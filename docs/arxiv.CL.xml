<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>

<item>
<title>Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation</title>
<link>https://arxiv.org/abs/2511.05516</link>
<guid>https://arxiv.org/abs/2511.05516</guid>
<content:encoded><![CDATA[
<div> Keywords: speech language model, continuous audio tokenizer, speech editing, instruction-based, free-form editing

Summary:
MingTok-Audio is introduced as a unified continuous speech tokenizer that integrates semantic and acoustic features for improved speech understanding and generation. Ming-UniAudio, based on MingTok-Audio, achieves a balance between generation and understanding capabilities, setting new SOTA records on the ContextASR benchmark. Ming-UniAudio-Edit is the first speech language model enabling free-form speech editing guided by natural language instructions without timestamp conditions. A comprehensive benchmark, Ming-Freeform-Audio-Edit, is introduced to evaluate instruction-based speech editing, covering semantic correctness, acoustic quality, and instruction alignment. The models are open-sourced to foster the development of unified audio understanding, generation, and manipulation.<br><br>Summary: MingTok-Audio and Ming-UniAudio improve speech understanding and generation, setting benchmarks in ASR tasks. Ming-UniAudio-Edit enables free-form speech editing guided by natural language instructions, supported by the Ming-Freeform-Audio-Edit benchmark. All models are open-sourced to advance research in unified audio processing. <div>
arXiv:2511.05516v1 Announce Type: new 
Abstract: Existing speech models suffer from competing requirements on token representations by understanding and generation tasks. This discrepancy in representation prevents speech language models from performing instruction-based free-form editing. To solve this challenge, we introduce a novel framework that unifies speech understanding, generation, and editing. The core of our unified model is a unified continuous speech tokenizer MingTok-Audio, the first continuous tokenizer to effectively integrate semantic and acoustic features, which makes it suitable for both understanding and generation tasks. Based on this unified continuous audio tokenizer, we developed the speech language model Ming-UniAudio, which achieved a balance between generation and understanding capabilities. Ming-UniAudio sets new state-of-the-art (SOTA) records on 8 out of 12 metrics on the ContextASR benchmark. Notably, for Chinese voice cloning, it achieves a highly competitive Seed-TTS-WER of 0.95. Leveraging this foundational model, we further trained a dedicated speech editing model Ming-UniAudio-Edit, the first speech language model that enables universal, free-form speech editing guided solely by natural language instructions, handling both semantic and acoustic modifications without timestamp condition. To rigorously assess the editing capability and establish a foundation for future research, we introduce Ming-Freeform-Audio-Edit, the first comprehensive benchmark tailored for instruction-based free-form speech editing, featuring diverse scenarios and evaluation dimensions spanning semantic correctness, acoustic quality, and instruction alignment. We open-sourced the continuous audio tokenizer, the unified foundational model, and the free-form instruction-based editing model to facilitate the development of unified audio understanding, generation, and manipulation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retracing the Past: LLMs Emit Training Data When They Get Lost</title>
<link>https://arxiv.org/abs/2511.05518</link>
<guid>https://arxiv.org/abs/2511.05518</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, memorization, data extraction, privacy concerns, confusion-inducing attacks

Summary:
The paper introduces Confusion-Inducing Attacks (CIA), a framework for extracting memorized data from large language models (LLMs) by maximizing model uncertainty. It identifies a spike in token-level prediction entropy as a precursor to the emission of memorized text during divergence, and leverages this insight to optimize input snippets for inducing a high-entropy state. The proposed Mismatched Supervised Fine-tuning (SFT) weakens alignment in aligned LLMs to increase susceptibility to attacks. Experimental results show that CIA outperforms existing methods in extracting verbatim and near-verbatim training data without prior knowledge of the training data. The study highlights the persistent risks of memorization in LLMs and offers a systematic approach to assess these vulnerabilities. 

<br><br>Summary: The paper presents Confusion-Inducing Attacks (CIA) to extract memorized data from large language models by maximizing model uncertainty. It uses token-level prediction entropy to optimize input snippets and proposes Mismatched Supervised Fine-tuning (SFT) for aligned models. Experimental results demonstrate the effectiveness of CIA in extracting training data without prior knowledge, highlighting ongoing risks of memorization in LLMs. <div>
arXiv:2511.05518v1 Announce Type: new 
Abstract: The memorization of training data in large language models (LLMs) poses significant privacy and copyright concerns. Existing data extraction methods, particularly heuristic-based divergence attacks, often exhibit limited success and offer limited insight into the fundamental drivers of memorization leakage. This paper introduces Confusion-Inducing Attacks (CIA), a principled framework for extracting memorized data by systematically maximizing model uncertainty. We empirically demonstrate that the emission of memorized text during divergence is preceded by a sustained spike in token-level prediction entropy. CIA leverages this insight by optimizing input snippets to deliberately induce this consecutive high-entropy state. For aligned LLMs, we further propose Mismatched Supervised Fine-tuning (SFT) to simultaneously weaken their alignment and induce targeted confusion, thereby increasing susceptibility to our attacks. Experiments on various unaligned and aligned LLMs demonstrate that our proposed attacks outperform existing baselines in extracting verbatim and near-verbatim training data without requiring prior knowledge of the training data. Our findings highlight persistent memorization risks across various LLMs and offer a more systematic method for assessing these vulnerabilities.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond One-Size-Fits-All: Personalized Harmful Content Detection with In-Context Learning</title>
<link>https://arxiv.org/abs/2511.05532</link>
<guid>https://arxiv.org/abs/2511.05532</guid>
<content:encoded><![CDATA[
<div> Keywords: online content moderation, in-context learning, foundation models, personalization, decentralized environments

Summary: 
- The article introduces a novel framework that utilizes in-context learning with foundation models to detect harmful online content such as toxicity, spam, and negative sentiment across various settings.
- This approach allows for lightweight personalization, enabling users to block or unblock categories and extend detection to semantic variations without needing model retraining.
- Experiments on public datasets and a new annotated Mastodon dataset show that foundation models can generalize well across tasks, sometimes outperforming task-specific models.
- Personalization is effective with minimal user input, and adding label definitions or rationales to prompts improves robustness to noisy data.
- The work suggests a move towards user-centric content safety systems that are practical, privacy-preserving, and highly adaptable, offering a new approach beyond traditional centralized moderation systems.

<br><br>Summary: <div>
arXiv:2511.05532v1 Announce Type: new 
Abstract: The proliferation of harmful online content--e.g., toxicity, spam, and negative sentiment--demands robust and adaptable moderation systems. However, prevailing moderation systems are centralized and task-specific, offering limited transparency and neglecting diverse user preferences--an approach ill-suited for privacy-sensitive or decentralized environments. We propose a novel framework that leverages in-context learning (ICL) with foundation models to unify the detection of toxicity, spam, and negative sentiment across binary, multi-class, and multi-label settings. Crucially, our approach enables lightweight personalization, allowing users to easily block new categories, unblock existing ones, or extend detection to semantic variations through simple prompt-based interventions--all without model retraining. Extensive experiments on public benchmarks (TextDetox, UCI SMS, SST2) and a new, annotated Mastodon dataset reveal that: (i) foundation models achieve strong cross-task generalization, often matching or surpassing task-specific fine-tuned models; (ii) effective personalization is achievable with as few as one user-provided example or definition; and (iii) augmenting prompts with label definitions or rationales significantly enhances robustness to noisy, real-world data. Our work demonstrates a definitive shift beyond one-size-fits-all moderation, establishing ICL as a practical, privacy-preserving, and highly adaptable pathway for the next generation of user-centric content safety systems. To foster reproducibility and facilitate future research, we publicly release our code on GitHub and the annotated Mastodon dataset on Hugging Face.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCP4IFC: IFC-Based Building Design Using Large Language Models</title>
<link>https://arxiv.org/abs/2511.05533</link>
<guid>https://arxiv.org/abs/2511.05533</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, architecture, engineering, construction, Industry Foundation Classes (IFC)

Summary:
This article introduces MCP4IFC, an open-source framework that allows Large Language Models (LLMs) to manipulate Industry Foundation Classes (IFC) data in the architecture, engineering, and construction (AEC) field. The framework enables LLMs to translate natural language instructions into actions on standardized data models using the Model Context Protocol (MCP). It includes tools for querying scene information, creating and modifying building elements, and a dynamic code-generation system. The framework combines in-context learning with retrieval-augmented generation (RAG) for tasks beyond the predefined toolset. Experiments show that an LLM using MCP4IFC can successfully complete various tasks, from building a house to editing existing IFC data. The open-source framework aims to promote research in LLM-driven BIM design and facilitate AI-assisted modeling workflows.

<br><br>Summary: <div>
arXiv:2511.05533v1 Announce Type: new 
Abstract: Bringing generative AI into the architecture, engineering and construction (AEC) field requires systems that can translate natural language instructions into actions on standardized data models. We present MCP4IFC, a comprehensive open-source framework that enables Large Language Models (LLMs) to directly manipulate Industry Foundation Classes (IFC) data through the Model Context Protocol (MCP). The framework provides a set of BIM tools, including scene querying tools for information retrieval, predefined functions for creating and modifying common building elements, and a dynamic code-generation system that combines in-context learning with retrieval-augmented generation (RAG) to handle tasks beyond the predefined toolset. Experiments demonstrate that an LLM using our framework can successfully perform complex tasks, from building a simple house to querying and editing existing IFC data. Our framework is released as open-source to encourage research in LLM-driven BIM design and provide a foundation for AI-assisted modeling workflows. Our code is available at https://show2instruct.github.io/mcp4ifc/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference</title>
<link>https://arxiv.org/abs/2511.05534</link>
<guid>https://arxiv.org/abs/2511.05534</guid>
<content:encoded><![CDATA[
<div> framework, cross-modal information flow, multimodal, KV cache merging, sensitivity-adaptive token matching <br>
<br>
Summary: <br>
FlowMM is introduced as a framework for multimodal KV cache merging, addressing limitations in traditional eviction strategies. It leverages cross-modal information flow to dynamically apply merging strategies, capturing modality-specific patterns while maintaining contextual integrity. The framework also includes a sensitivity-adaptive token matching mechanism to evaluate token similarity and task sensitivity, merging low-risk tokens while preserving high-sensitivity ones. Experimental results across MLLMs demonstrate a significant reduction in KV cache memory and decoding latency while maintaining competitive task performance. <div>
arXiv:2511.05534v1 Announce Type: new 
Abstract: Traditional KV cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, causing context loss or hallucinations. Recent efforts shift toward KV merging, merging eviction tokens with retention tokens based on similarity. However, in multimodal scenarios, distributional biases across modality tokens and attentional biases in cross-modal interactions limit its effectiveness. This work introduces FlowMM, an adaptive framework for cross-modal information flow-guided multimodal KV cache merging. FlowMM leverages cross-modal information flow to dynamically apply layer-specific merging strategies, capturing modality-specific patterns while preserving contextual integrity. Furthermore, we introduce a sensitivity-adaptive token matching mechanism that jointly evaluates token similarity and task-critical sensitivity, merging low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to 95% and decoding latency by 1.3-1.8x, while maintaining competitive task performance.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Future of AI Models: A Computational perspective on Model collapse</title>
<link>https://arxiv.org/abs/2511.05535</link>
<guid>https://arxiv.org/abs/2511.05535</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, Diffusion models, Neural architectures, Model Collapse

Summary:
Artificial Intelligence, particularly Large Language Models (LLMs), has significantly impacted various fields such as software engineering, journalism, and academia. The proliferation of AI-generated content is evident, with a high percentage of webpages containing such material. However, the use of synthetic content poses a risk of Model Collapse due to reduced linguistic and semantic diversity. This study explores the onset of collapse by analyzing semantic similarity in English-language Wikipedia from 2013 to 2025. Results indicate a gradual increase in similarity before the adoption of LLM models, attributable to early RNN/LSTM technologies. Fluctuations in similarity reflect linguistic diversity, corpus size variations, and sampling error. A sharp rise in similarity post-LLM adoption signifies a potential threat to data richness and model generalization. This data-driven analysis offers insights into the progression towards recursive AI contamination. 

<br><br>Summary: Artificial Intelligence has revolutionized various domains, with a substantial presence of AI-generated content on webpages. However, the widespread use of synthetic material poses a risk of Model Collapse, impacting linguistic and semantic diversity. By analyzing semantic similarity trends in English-language Wikipedia, this study uncovers an exponential rise in similarity following the adoption of Large Language Models. Fluctuations in similarity metrics highlight linguistic diversity and corpus size variations. The study provides valuable insights into the potential onset of recursive AI contamination, signaling a potential threat to data richness and model generalization. <div>
arXiv:2511.05535v1 Announce Type: new 
Abstract: Artificial Intelligence, especially Large Language Models (LLMs), has transformed domains such as software engineering, journalism, creative writing, academia, and media (Naveed et al. 2025; arXiv:2307.06435). Diffusion models like Stable Diffusion generate high-quality images and videos from text. Evidence shows rapid expansion: 74.2% of newly published webpages now contain AI-generated material (Ryan Law 2025), 30-40% of the active web corpus is synthetic (Spennemann 2025; arXiv:2504.08755), 52% of U.S. adults use LLMs for writing, coding, or research (Staff 2025), and audits find AI involvement in 18% of financial complaints and 24% of press releases (Liang et al. 2025). The underlying neural architectures, including Transformers (Vaswani et al. 2023; arXiv:1706.03762), RNNs, LSTMs, GANs, and diffusion networks, depend on large, diverse, human-authored datasets (Shi & Iyengar 2019). As synthetic content dominates, recursive training risks eroding linguistic and semantic diversity, producing Model Collapse (Shumailov et al. 2024; arXiv:2307.15043; Dohmatob et al. 2024; arXiv:2402.07712). This study quantifies and forecasts collapse onset by examining year-wise semantic similarity in English-language Wikipedia (filtered Common Crawl) from 2013 to 2025 using Transformer embeddings and cosine similarity metrics. Results reveal a steady rise in similarity before public LLM adoption, likely driven by early RNN/LSTM translation and text-normalization pipelines, though modest due to a smaller scale. Observed fluctuations reflect irreducible linguistic diversity, variable corpus size across years, finite sampling error, and an exponential rise in similarity after the public adoption of LLM models. These findings provide a data-driven estimate of when recursive AI contamination may significantly threaten data richness and model generalization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Sparse Autoencoders: Leveraging the Sequential Nature of Language for Interpretability</title>
<link>https://arxiv.org/abs/2511.05541</link>
<guid>https://arxiv.org/abs/2511.05541</guid>
<content:encoded><![CDATA[
<div> Keywords: interpretability, Sparse Autoencoders, linguistic understanding, semantic features, unsupervised learning

Summary:<br><br>Translating complex model representations into human-understandable concepts is a critical goal for interpretability. Current methods like Sparse Autoencoders (SAEs) often fail to capture meaningful linguistic information, focusing instead on superficial patterns. This stems from a lack of integration of linguistic knowledge in training. To address this, Temporal Sparse Autoencoders (T-SAEs) are introduced, which prioritize semantic over syntactic features through a novel contrastive loss mechanism. The T-SAEs successfully disentangle semantic concepts in a self-supervised manner, yielding smoother and coherent semantic representations across various datasets and models. Surprisingly, these semantic structures emerge without explicit semantic supervision, indicating a promising approach for enhancing unsupervised interpretability in language models. <div>
arXiv:2511.05541v1 Announce Type: new 
Abstract: Translating the internal representations and computations of models into concepts that humans can understand is a key goal of interpretability. While recent dictionary learning methods such as Sparse Autoencoders (SAEs) provide a promising route to discover human-interpretable features, they suffer from a variety of problems, including a systematic failure to capture the rich conceptual information that drives linguistic understanding. Instead, they exhibit a bias towards shallow, token-specific, or noisy features, such as "the phrase 'The' at the start of sentences". In this work, we propose that this is due to a fundamental issue with how dictionary learning methods for LLMs are trained. Language itself has a rich, well-studied structure spanning syntax, semantics, and pragmatics; however, current unsupervised methods largely ignore this linguistic knowledge, leading to poor feature discovery that favors superficial patterns over meaningful concepts. We focus on a simple but important aspect of language: semantic content has long-range dependencies and tends to be smooth over a sequence, whereas syntactic information is much more local. Building on this insight, we introduce Temporal Sparse Autoencoders (T-SAEs), which incorporate a novel contrastive loss encouraging consistent activations of high-level features over adjacent tokens. This simple yet powerful modification enables SAEs to disentangle semantic from syntactic features in a self-supervised manner. Across multiple datasets and models, T-SAEs recover smoother, more coherent semantic concepts without sacrificing reconstruction quality. Strikingly, they exhibit clear semantic structure despite being trained without explicit semantic signal, offering a new pathway for unsupervised interpretability in language models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample-Efficient Language Modeling with Linear Attention and Lightweight Enhancements</title>
<link>https://arxiv.org/abs/2511.05560</link>
<guid>https://arxiv.org/abs/2511.05560</guid>
<content:encoded><![CDATA[
<div> mLSTM, language modeling, linear attention, sliding window attention, Muon optimizer  
Summary:  
1. The study focuses on sample-efficient language modeling techniques for the BabyLM 2025 shared task, utilizing the BLaLM model with a linear-time mLSTM token mixer.
2. Architectural enhancements such as short convolutions, sliding window attention, and Hedgehog feature maps are explored to improve model performance.
3. A curated high-quality corpus is used for training in low-resource settings, emphasizing readability and pedagogical structure.
4. Experiments reveal that linear attention with sliding window attention enhances zero-shot performance consistently.
5. The Muon optimizer is found to stabilize convergence and reduce perplexity compared to AdamW, showcasing effective strategies for efficient language modeling without relying on scale.  
<br><br>Summary: <div>
arXiv:2511.05560v1 Announce Type: new 
Abstract: We study architectural and optimization tech- niques for sample-efficient language modeling under the constraints of the BabyLM 2025 shared task. Our model, BLaLM, replaces self-attention with a linear-time mLSTM to- ken mixer and explores lightweight enhance- ments, including short convolutions, sliding window attention with dynamic modulation, and Hedgehog feature maps. To support train- ing in low-resource settings, we curate a high- quality corpus emphasizing readability and ped- agogical structure. Experiments across both STRICT and STRICT-SMALL tracks show that (1) linear attention combined with sliding win- dow attention consistently improves zero-shot performance, and (2) the Muon optimizer stabi- lizes convergence and reduces perplexity over AdamW. These results highlight effective strate- gies for efficient language modeling without relying on scale.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UTF-8 Plumbing: Byte-level Tokenizers Unavoidably Enable LLMs to Generate Ill-formed UTF-8</title>
<link>https://arxiv.org/abs/2511.05578</link>
<guid>https://arxiv.org/abs/2511.05578</guid>
<content:encoded><![CDATA[
<div> tokenization, subword, language model, UTF-8, monoid theory

Summary:
This paper discusses the challenges of subword tokenization for language models, specifically focusing on the trade-offs between using code points and bytes in the vocabulary. By formalizing tokenization using monoid theory, the study proves that tokenizers with ill-formed UTF-8 tokens can result in sequences that are also ill-formed UTF-8. The research demonstrates the implications of converting tokens incrementally versus all at once, highlighting potential differences in results. Real-world bugs resulting from these issues are discussed, along with evaluations of mitigations. Case studies involving major foundation models, serving engines, and constrained generation systems are explored to showcase the impact of these findings. The study emphasizes the need for applications of language models to account for potential breakage introduced by using byte-based vocabularies. 
<br><br>Summary: <div>
arXiv:2511.05578v1 Announce Type: new 
Abstract: Subword tokenization segments input text according to a pre-defined vocabulary to feed it into a language model; the language model, in turn, generates a sequence made from this same vocabulary. The members of the vocabulary can be built of code points or bytes. Using code points means that all members of the vocabulary are valid UTF-8 characters. However, it also requires thousands of initial members to achieve acceptable coverage of inputs. Beginning with bytes, on the contrary, avoids out-of-vocabulary errors with only 256 initial members of the vocabulary, but the members of the vocabulary and sequences of them are not guaranteed to be valid UTF-8. Sequences that are not valid UTF-8 break code that assumes its input to be valid UTF-8. Applications of language models must account for the breakage thereby introduced. In this paper, we formalize tokenization using monoid theory and prove that tokenizers whose vocabularies contain tokens that are ill-formed UTF-8 can always produce sequences that are ill-formed UTF-8. We demonstrate formally that attempting to incrementally convert tokens back to a string and interpret the results as UTF-8 gives different results than converting the whole sequence of tokens at once. This formal result predicts real-world bugs: we evaluate mitigations for the problem identified and provide case studies of major foundation models, serving engines, and constrained generation systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Diversity and Quality through Base-Aligned Model Collaboration</title>
<link>https://arxiv.org/abs/2511.05650</link>
<guid>https://arxiv.org/abs/2511.05650</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, model collaboration, diversity, quality, inference-time

Summary: 
Base-Aligned Model Collaboration (BACo) is introduced as a framework for improving the diversity and quality of large language models (LLMs) by dynamically combining a base model with its aligned counterpart. Utilizing routing strategies based on next-token prediction uncertainty and semantic role prediction, BACo optimizes diversity and quality without compromising on performance. Compared to existing diversity-promoting methods, BACo achieves a balance between diversity and quality in a single pass, offering controllability and consistently outperforming state-of-the-art baselines across various generation tasks and metrics. Through collaboration between base and aligned models, BACo demonstrates a 21.3% joint improvement in diversity and quality, as confirmed by human evaluations. This approach highlights the potential of model collaboration to enhance the output quality of LLMs while maintaining diversity. 

<br><br>Summary: <div>
arXiv:2511.05650v1 Announce Type: new 
Abstract: Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OckBench: Measuring the Efficiency of LLM Reasoning</title>
<link>https://arxiv.org/abs/2511.05722</link>
<guid>https://arxiv.org/abs/2511.05722</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, token efficiency, benchmarking, automated reasoning, code generation

Summary:
Large language models like GPT-4 and Claude 3 have greatly improved automated reasoning and code generation. However, existing benchmarks often overlook token efficiency, which plays a crucial role in determining system latency, cost, and energy consumption. In response to this gap, the OckBench benchmark was introduced to evaluate both accuracy and token count for reasoning and coding tasks. Through experiments on various models, it was discovered that models with similar accuracy can differ significantly in token consumption. The benchmark also highlights the importance of considering token efficiency when evaluating models, prompting a paradigm shift in research evaluation practices. OckBench serves as a comprehensive platform for measuring, comparing, and guiding research in token-efficient reasoning. The benchmarks are accessible at https://ockbench.github.io/. 

<br><br>Summary: <div>
arXiv:2511.05722v1 Announce Type: new 
Abstract: Large language models such as GPT-4, Claude 3, and the Gemini series have improved automated reasoning and code generation. However, existing benchmarks mainly focus on accuracy and output quality, and they ignore an important factor: decoding token efficiency. In real systems, generating 10,000 tokens versus 100,000 tokens leads to large differences in latency, cost, and energy. In this work, we introduce OckBench, a model-agnostic and hardware-agnostic benchmark that evaluates both accuracy and token count for reasoning and coding tasks. Through experiments comparing multiple open- and closed-source models, we uncover that many models with comparable accuracy differ wildly in token consumption, revealing that efficiency variance is a neglected but significant axis of differentiation. We further demonstrate Pareto frontiers over the accuracy-efficiency plane and argue for an evaluation paradigm shift: we should no longer treat tokens as "free" to multiply. OckBench provides a unified platform for measuring, comparing, and guiding research in token-efficient reasoning. Our benchmarks are available at https://ockbench.github.io/ .
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Learning Without Copying</title>
<link>https://arxiv.org/abs/2511.05743</link>
<guid>https://arxiv.org/abs/2511.05743</guid>
<content:encoded><![CDATA[
<div> induction heads, inductive copying, transformers, abstractive ICL, Hapax <br>
<br>
Summary: 
The study explores the role of induction heads in transformers, which are attention heads that perform inductive copying to match patterns and copy continuations. The research investigates whether transformers can still acquire in-context learning capabilities when inductive copying is suppressed. The proposed Hapax setting omits the loss contribution for tokens predict correctly by induction heads. Despite a reduction in inductive copying, performance on abstractive in-context learning tasks remains comparable and even surpasses the vanilla model on various tasks. The model achieves lower loss values on token positions not predicted by induction heads. Analysis reveals that models trained with Hapax develop fewer and weaker induction heads but still retain in-context learning capabilities. These findings suggest that inductive copying is not essential for learning abstractive in-context learning mechanisms. <br> <div>
arXiv:2511.05743v1 Announce Type: new 
Abstract: Induction heads are attention heads that perform inductive copying by matching patterns from earlier context and copying their continuations verbatim. As models develop induction heads, they often experience a sharp drop in training loss, a phenomenon cited as evidence that induction heads may serve as a prerequisite for more complex in-context learning (ICL) capabilities. In this work, we ask whether transformers can still acquire ICL capabilities when inductive copying is suppressed. We propose Hapax, a setting where we omit the loss contribution of any token that can be correctly predicted by induction heads. Despite a significant reduction in inductive copying, performance on abstractive ICL tasks (i.e., tasks where the answer is not contained in the input context) remains comparable and surpasses the vanilla model on 13 of 21 tasks, even though 31.7\% of tokens are omitted from the loss. Furthermore, our model achieves lower loss values on token positions that cannot be predicted correctly by induction heads. Mechanistic analysis further shows that models trained with Hapax develop fewer and weaker induction heads but still preserve ICL capabilities. Taken together, our findings indicate that inductive copying is not essential for learning abstractive ICL mechanisms.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Scale Feature Fusion and Graph Neural Network Integration for Text Classification with Large Language Models</title>
<link>https://arxiv.org/abs/2511.05752</link>
<guid>https://arxiv.org/abs/2511.05752</guid>
<content:encoded><![CDATA[
<div> Keywords: text classification, language models, feature pyramids, graph neural networks, semantic modeling

Summary: This study presents a hybrid method for text classification that combines deep feature extraction from language models, multi-scale feature fusion via feature pyramids, and structured modeling with graph neural networks. The large language model captures contextual dependencies and semantic representations, forming a strong foundation for subsequent modeling. The feature pyramid mechanism integrates semantic features of different scales, balancing global and local information to create hierarchical semantic expressions. Fused features are transformed into graph representations, allowing graph neural networks to capture semantic relations and dependencies in the text. The proposed method outperforms existing models in robustness alignment experiments on metrics such as ACC, F1-Score, AUC, and Precision. This framework provides a new approach for multi-scale feature fusion and structured semantic modeling in text classification tasks.<br><br>Summary: This study introduces a hybrid method for text classification that combines deep feature extraction from language models with multi-scale feature fusion and structured modeling. The framework achieves superior performance in complex semantic contexts, showcasing the effectiveness of balancing global and local information, as well as semantics and structure in text classification tasks. <div>
arXiv:2511.05752v1 Announce Type: new 
Abstract: This study investigates a hybrid method for text classification that integrates deep feature extraction from large language models, multi-scale fusion through feature pyramids, and structured modeling with graph neural networks to enhance performance in complex semantic contexts. First, the large language model captures contextual dependencies and deep semantic representations of the input text, providing a rich feature foundation for subsequent modeling. Then, based on multi-level feature representations, the feature pyramid mechanism effectively integrates semantic features of different scales, balancing global information and local details to construct hierarchical semantic expressions. Furthermore, the fused features are transformed into graph representations, and graph neural networks are employed to capture latent semantic relations and logical dependencies in the text, enabling comprehensive modeling of complex interactions among semantic units. On this basis, the readout and classification modules generate the final category predictions. The proposed method demonstrates significant advantages in robustness alignment experiments, outperforming existing models on ACC, F1-Score, AUC, and Precision, which verifies the effectiveness and stability of the framework. This study not only constructs an integrated framework that balances global and local information as well as semantics and structure, but also provides a new perspective for multi-scale feature fusion and structured semantic modeling in text classification tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Generation: Complexity Barriers and Implications for Learning</title>
<link>https://arxiv.org/abs/2511.05759</link>
<guid>https://arxiv.org/abs/2511.05759</guid>
<content:encoded><![CDATA[
<div> language generation, learnability, regular languages, context-free languages, language models
Summary:<br>
- Kleinberg and Mullainathan demonstrated the theoretical possibility of language generation with a sufficient number of positive examples, but practical feasibility is challenging.<br>
- Simple language families like regular and context-free languages may require an extraordinarily large number of examples for successful generation, sometimes without a computable bound.<br>
- There exists a significant gap between the theoretical potential and efficient learnability of language.<br>
- The success of modern language models may be explained by considering structural properties of natural language that enable effective generation in practice.<br> <div>
arXiv:2511.05759v1 Announce Type: new 
Abstract: Kleinberg and Mullainathan showed that, in principle, language generation is always possible: with sufficiently many positive examples, a learner can eventually produce sentences indistinguishable from those of a target language. However, the existence of such a guarantee does not speak to its practical feasibility. In this work, we show that even for simple and well-studied language families -- such as regular and context-free languages -- the number of examples required for successful generation can be extraordinarily large, and in some cases not bounded by any computable function. These results reveal a substantial gap between theoretical possibility and efficient learnability. They suggest that explaining the empirical success of modern language models requires a refined perspective -- one that takes into account structural properties of natural language that make effective generation possible in practice.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning</title>
<link>https://arxiv.org/abs/2511.05784</link>
<guid>https://arxiv.org/abs/2511.05784</guid>
<content:encoded><![CDATA[
<div> Keywords: unlearning, language models, privacy protection, detection module, continual unlearning

Summary:
The article introduces a novel framework called DRAGON for unlearning in Large Language Models (LLMs) to protect private data and remove harmful knowledge. DRAGON leverages in-context chain-of-thought (CoT) instructions to guide LLMs without the need for access to retain data. By utilizing a lightweight detection module and a CoT guard model, DRAGON can effectively identify forget-worthy prompts and enforce safe intervention without modifying the base model. The framework is evaluated across three unlearning tasks, demonstrating its strong capability, scalability, and practical applicability. The proposed metrics for unlearning performance and the continual unlearning setting provide a robust evaluation of DRAGON's effectiveness in data-limited scenarios. Overall, DRAGON offers a promising solution for efficient and secure unlearning in LLMs. 

<br><br>Summary: <div>
arXiv:2511.05784v1 Announce Type: new 
Abstract: Unlearning in Large Language Models (LLMs) is crucial for protecting private data and removing harmful knowledge. Most existing approaches rely on fine-tuning to balance unlearning efficiency with general language capabilities. However, these methods typically require training or access to retain data, which is often unavailable in real world scenarios. Although these methods can perform well when both forget and retain data are available, few works have demonstrated equivalent capability in more practical, data-limited scenarios. To overcome these limitations, we propose Detect-Reasoning Augmented GeneratiON (DRAGON), a systematic, reasoning-based framework that utilizes in-context chain-of-thought (CoT) instructions to guard deployed LLMs before inference. Instead of modifying the base model, DRAGON leverages the inherent instruction-following ability of LLMs and introduces a lightweight detection module to identify forget-worthy prompts without any retain data. These are then routed through a dedicated CoT guard model to enforce safe and accurate in-context intervention. To robustly evaluate unlearning performance, we introduce novel metrics for unlearning performance and the continual unlearning setting. Extensive experiments across three representative unlearning tasks validate the effectiveness of DRAGON, demonstrating its strong unlearning capability, scalability, and applicability in practical scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Edits Decay in Fine-tuned LLMs</title>
<link>https://arxiv.org/abs/2511.05852</link>
<guid>https://arxiv.org/abs/2511.05852</guid>
<content:encoded><![CDATA[
<div> knowledge editing, fine-tuning, language models, edits decay, selective-layer fine-tuning

Summary:
In this study, the authors explore the interaction between knowledge editing and fine-tuning in large language models (LLMs). They investigate whether edits made to LLMs persist after fine-tuning, which is crucial for scenarios such as removing malicious edits or preserving beneficial ones. The experiments involve testing two editing methods (MEMIT, AlphaEdit) and three fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three datasets. The results show that edits decay after fine-tuning, with the survival of edits varying across configurations. Selective-layer fine-tuning is proposed as a strategy to effectively remove edits, even though it may slightly impact downstream performance. Surprisingly, fine-tuning non-edited layers impairs more edits than full fine-tuning. This study establishes empirical baselines and provides actionable strategies for integrating knowledge editing with fine-tuning in LLMs, emphasizing the importance of considering the full application pipeline when evaluating model editing. 

Summary: <div>
arXiv:2511.05852v1 Announce Type: new 
Abstract: Knowledge editing has emerged as a lightweight alternative to retraining for correcting or injecting specific facts in large language models (LLMs). Meanwhile, fine-tuning remains the default operation for adapting LLMs to new domains and tasks. Despite their widespread adoption, these two post-training interventions have been studied in isolation, leaving open a crucial question: if we fine-tune an edited model, do the edits survive? This question is motivated by two practical scenarios: removing covert or malicious edits, and preserving beneficial edits. If fine-tuning impairs edits as shown in Figure 1, current KE methods become less useful, as every fine-tuned model would require re-editing, which significantly increases the cost; if edits persist, fine-tuned models risk propagating hidden malicious edits, raising serious safety concerns. To this end, we systematically quantify edits decay after fine-tuning, investigating how fine-tuning affects knowledge editing. We evaluate two state-of-the-art editing methods (MEMIT, AlphaEdit) and three fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three datasets, yielding 232 experimental configurations. Our results show that edits decay after fine-tuning, with survival varying across configurations, e.g., AlphaEdit edits decay more than MEMIT edits. Further, we propose selective-layer fine-tuning and find that fine-tuning edited layers only can effectively remove edits, though at a slight cost to downstream performance. Surprisingly, fine-tuning non-edited layers impairs more edits than full fine-tuning. Overall, our study establishes empirical baselines and actionable strategies for integrating knowledge editing with fine-tuning, and underscores that evaluating model editing requires considering the full LLM application pipeline.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations</title>
<link>https://arxiv.org/abs/2511.05901</link>
<guid>https://arxiv.org/abs/2511.05901</guid>
<content:encoded><![CDATA[
<div> Keywords: medical knowledge, large language models, retrieval-augmented generation, clinical validation, low-resource settings 

Summary:
Large language models (LLMs) have shown value in the medical field, but there are limitations. Retrieval-augmented generation (RAG) technologies have potential to enhance clinical applicability. However, current research relies heavily on publicly available data, with limited use of private data. Retrieval approaches commonly use English-centric embedding models, while medical-specific LLMs are underutilized. Evaluation metrics focus on generation quality and task performance, but lack attention to bias and safety. RAG applications in medicine are concentrated on question answering, report generation, text summarization, and information extraction. The field of medical RAG is still in its early stages and requires advancements in clinical validation, cross-linguistic adaptation, and support for low-resource settings to ensure global use is trustworthy and responsible. 

<br><br>Summary: <div>
arXiv:2511.05901v1 Announce Type: new 
Abstract: The rapid growth of medical knowledge and increasing complexity of clinical practice pose challenges. In this context, large language models (LLMs) have demonstrated value; however, inherent limitations remain. Retrieval-augmented generation (RAG) technologies show potential to enhance their clinical applicability. This study reviewed RAG applications in medicine. We found that research primarily relied on publicly available data, with limited application in private data. For retrieval, approaches commonly relied on English-centric embedding models, while LLMs were mostly generic, with limited use of medical-specific LLMs. For evaluation, automated metrics evaluated generation quality and task performance, whereas human evaluation focused on accuracy, completeness, relevance, and fluency, with insufficient attention to bias and safety. RAG applications were concentrated on question answering, report generation, text summarization, and information extraction. Overall, medical RAG remains at an early stage, requiring advances in clinical validation, cross-linguistic adaptation, and support for low-resource settings to enable trustworthy and responsible global use.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NILC: Discovering New Intents with LLM-assisted Clustering</title>
<link>https://arxiv.org/abs/2511.05913</link>
<guid>https://arxiv.org/abs/2511.05913</guid>
<content:encoded><![CDATA[
<div> Keywords: New Intent Discovery, Clustering, Language Models, Semi-Supervised Learning, Text Embeddings <br>
Summary: <br>
The paper introduces a novel clustering framework, NILC, designed for effective New Intent Discovery (NID) in dialogue systems. Existing NID approaches typically use a cascaded architecture, but NILC adopts an iterative workflow to leverage feedback from both text embeddings and clustering stages. By incorporating large language models (LLMs), NILC enriches cluster centroids with contextual semantics and refines uncertain utterances for improved clustering. It also utilizes LLMs to rewrite and correct ambiguous or terse utterances, enhancing the performance of NID. In the semi-supervised setting, NILC employs non-trivial techniques such as seeding and soft must links to provide supervision signals for more accurate intent recognition. Experimental results demonstrate the superior performance of NILC over recent baselines across various datasets, showcasing its effectiveness in achieving significant improvements in NID tasks. <br> <div>
arXiv:2511.05913v1 Announce Type: new 
Abstract: New intent discovery (NID) seeks to recognize both new and known intents from unlabeled user utterances, which finds prevalent use in practical dialogue systems. Existing works towards NID mainly adopt a cascaded architecture, wherein the first stage focuses on encoding the utterances into informative text embeddings beforehand, while the latter is to group similar embeddings into clusters (i.e., intents), typically by K-Means. However, such a cascaded pipeline fails to leverage the feedback from both steps for mutual refinement, and, meanwhile, the embedding-only clustering overlooks nuanced textual semantics, leading to suboptimal performance. To bridge this gap, this paper proposes NILC, a novel clustering framework specially catered for effective NID. Particularly, NILC follows an iterative workflow, in which clustering assignments are judiciously updated by carefully refining cluster centroids and text embeddings of uncertain utterances with the aid of large language models (LLMs). Specifically, NILC first taps into LLMs to create additional semantic centroids for clusters, thereby enriching the contextual semantics of the Euclidean centroids of embeddings. Moreover, LLMs are then harnessed to augment hard samples (ambiguous or terse utterances) identified from clusters via rewriting for subsequent cluster correction. Further, we inject supervision signals through non-trivial techniques seeding and soft must links for more accurate NID in the semi-supervised setting. Extensive experiments comparing NILC against multiple recent baselines under both unsupervised and semi-supervised settings showcase that NILC can achieve significant performance improvements over six benchmark datasets of diverse domains consistently.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IDALC: A Semi-Supervised Framework for Intent Detection and Active Learning based Correction</title>
<link>https://arxiv.org/abs/2511.05921</link>
<guid>https://arxiv.org/abs/2511.05921</guid>
<content:encoded><![CDATA[
<div> Keywords: Voice-controlled dialog systems, Intent Detection, Active Learning, Semi-supervised learning, Annotation cost reduction

Summary:
IDALC (Intent Detection and Active Learning based Correction) is introduced as a semi-supervised framework for voice-controlled dialog systems. It aims to detect user intents and rectify system-rejected utterances while minimizing human annotation. The framework surpasses baseline methods, achieving higher accuracy and macro-F1 improvement. Empirical findings on benchmark datasets show 5-10% higher accuracy and 4-8% improvement in macro-F1 compared to baseline methods. Notably, IDALC maintains the overall annotation cost at just 6-10% of the unlabelled data available. The framework can efficiently retrain agents with new intents from rejected queries over time, reducing the need for manual annotation. IDALC presents a cost-effective solution for improving the performance of voice-controlled dialog systems while minimizing human effort in annotation tasks.<br><br>Summary: IDALC is a semi-supervised framework that detects intents and corrects rejected utterances in voice-controlled systems, achieving higher accuracy and macro-F1 improvement while minimizing annotation costs and manual effort. <div>
arXiv:2511.05921v1 Announce Type: new 
Abstract: Voice-controlled dialog systems have become immensely popular due to their ability to perform a wide range of actions in response to diverse user queries. These agents possess a predefined set of skills or intents to fulfill specific user tasks. But every system has its own limitations. There are instances where, even for known intents, if any model exhibits low confidence, it results in rejection of utterances that necessitate manual annotation. Additionally, as time progresses, there may be a need to retrain these agents with new intents from the system-rejected queries to carry out additional tasks. Labeling all these emerging intents and rejected utterances over time is impractical, thus calling for an efficient mechanism to reduce annotation costs. In this paper, we introduce IDALC (Intent Detection and Active Learning based Correction), a semi-supervised framework designed to detect user intents and rectify system-rejected utterances while minimizing the need for human annotation. Empirical findings on various benchmark datasets demonstrate that our system surpasses baseline methods, achieving a 5-10% higher accuracy and a 4-8% improvement in macro-F1. Remarkably, we maintain the overall annotation cost at just 6-10% of the unlabelled data available to the system. The overall framework of IDALC is shown in Fig. 1
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs</title>
<link>https://arxiv.org/abs/2511.05933</link>
<guid>https://arxiv.org/abs/2511.05933</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, language model, hierarchical knowledge, structured prompting, internal activation analysis

Summary:
Reinforcement learning (RL) enhances language models' performance in recalling pure knowledge, especially in structured knowledge tasks like medical codes. The study shows that RL models outperform base and supervised fine-tuned models in recalling procedural paths within existing knowledge hierarchies. Structured prompting helps fine-tuned models bridge the performance gap, indicating that RL models excel in navigating knowledge hierarchies. While final-answer accuracy improves with prompting, RL models retain superior procedural path recall abilities. Internal activation analysis reveals that RL transforms how models traverse knowledge rather than knowledge representation itself. Factual representations maintain similarity between fine-tuned and RL models, but query representations diverge, highlighting RL's impact on knowledge traversal. This study challenges the notion that RL degrades memorized knowledge, instead showing its effectiveness in enhancing procedural skills in structured knowledge recall tasks. 

<br><br>Summary: <div>
arXiv:2511.05933v1 Announce Type: new 
Abstract: Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement "code 57.95 refers to urinary infection") maintain high cosine similarity between SFT and RL models, query representations (e.g., "what is code 57.95") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Recognition of Cognitive Distortions in Natural Language Texts</title>
<link>https://arxiv.org/abs/2511.05969</link>
<guid>https://arxiv.org/abs/2511.05969</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-factor classification, natural language texts, cognitive distortions, artificial intelligence, interpretable model

Summary:
This article introduces a novel approach to multi-factor classification of natural language texts using weighted structured patterns like N-grams, while considering the heterarchical relationships among them. The focus is on automating the detection of specific cognitive distortions in psychological care by employing an interpretable, robust, and transparent artificial intelligence model. The proposed recognition and learning algorithms enhance the existing state-of-the-art solutions in this domain. The effectiveness of the approach is validated on two publicly available datasets, showcasing significant improvements in F1 scores compared to existing literature. The optimization of hyper-parameters is conducted, and the code and models developed are made available for community use. This research contributes to advancing the field of automated cognitive distortion detection, offering a valuable tool for psychological care practitioners. 

<br><br>Summary: <div>
arXiv:2511.05969v1 Announce Type: new 
Abstract: We propose a new approach to multi-factor classification of natural language texts based on weighted structured patterns such as N-grams, taking into account the heterarchical relationships between them, applied to solve such a socially impactful problem as the automation of detection of specific cognitive distortions in psychological care, relying on an interpretable, robust and transparent artificial intelligence model. The proposed recognition and learning algorithms improve the current state of the art in this field. The improvement is tested on two publicly available datasets, with significant improvements over literature-known F1 scores for the task, with optimal hyper-parameters determined, having code and models available for future use by the community.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Entropy in Reinforcement Learning for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2511.05993</link>
<guid>https://arxiv.org/abs/2511.05993</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, verifiable rewards, language models, entropy dynamics, calibration

Summary: 
Reinforcement learning with verifiable rewards (RLVR) has been used to enhance the reasoning capabilities of large language models (LLMs). However, the collapse of entropy during RLVR training can lead to suboptimal convergence and hinder performance improvement. A comprehensive study on entropy dynamics in RLVR is lacking, prompting extensive experiments to be conducted. Factors such as off-policy updates, data diversity, and clipping thresholds impact model entropy. Tokens with positive advantages contribute most to entropy collapse, and adjusting loss weights of tokens can regulate model entropy effectively. The study reveals a correlation between model entropy, response diversity, calibration, and performance in various benchmarks. This research provides valuable insights into improving RLVR training of LLMs. 

<br><br>Summary: <div>
arXiv:2511.05993v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a predominant approach for enhancing the reasoning capabilities of large language models (LLMs). However, the entropy of LLMs usually collapses during RLVR training, causing premature convergence to suboptimal local minima and hinder further performance improvement. Although various approaches have been proposed to mitigate entropy collapse, a comprehensive study of entropy in RLVR remains lacking. To address this gap, we conduct extensive experiments to investigate the entropy dynamics of LLMs trained with RLVR and analyze how model entropy correlates with response diversity, calibration, and performance across various benchmarks. Our findings reveal that the number of off-policy updates, the diversity of training data, and the clipping thresholds in the optimization objective are critical factors influencing the entropy of LLMs trained with RLVR. Moreover, we theoretically and empirically demonstrate that tokens with positive advantages are the primary contributors to entropy collapse, and that model entropy can be effectively regulated by adjusting the relative loss weights of tokens with positive and negative advantages during training.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Do Not See Age: Assessing Demographic Bias in Automated Systematic Review Synthesis</title>
<link>https://arxiv.org/abs/2511.06000</link>
<guid>https://arxiv.org/abs/2511.06000</guid>
<content:encoded><![CDATA[
<div> age-related information; language models; biomedical evidence synthesis; DemogSummary dataset; summarization-capable LLMs

Summary:
The study evaluates the retention of age-related information by state-of-the-art language models in generating abstractive summaries of biomedical studies. A novel age-stratified dataset, DemogSummary, is created to assess demographic distinctions in child, adult, and older adult populations. Three LLMs - Qwen, Longformer, and GPT-4.1 Nano - are evaluated using standard metrics and a new Demographic Salience Score. It is found that adult-focused summaries have the lowest demographic fidelity, and under-represented populations are more susceptible to hallucinations. The study highlights the limitations of current LLMs in maintaining faithful and unbiased summarization, emphasizing the necessity for fairness-aware evaluation frameworks and summarization pipelines in biomedical NLP.

Summary: <div>
arXiv:2511.06000v1 Announce Type: new 
Abstract: Clinical interventions often hinge on age: medications and procedures safe for adults may be harmful to children or ineffective for older adults. However, as language models are increasingly integrated into biomedical evidence synthesis workflows, it remains uncertain whether these systems preserve such crucial demographic distinctions. To address this gap, we evaluate how well state-of-the-art language models retain age-related information when generating abstractive summaries of biomedical studies. We construct DemogSummary, a novel age-stratified dataset of systematic review primary studies, covering child, adult, and older adult populations. We evaluate three prominent summarisation-capable LLMs, Qwen (open-source), Longformer (open-source) and GPT-4.1 Nano (proprietary), using both standard metrics and a newly proposed Demographic Salience Score (DSS), which quantifies age-related entity retention and hallucination. Our results reveal systematic disparities across models and age groups: demographic fidelity is lowest for adult-focused summaries, and under-represented populations are more prone to hallucinations. These findings highlight the limitations of current LLMs in faithful and bias-free summarisation and point to the need for fairness-aware evaluation frameworks and summarisation pipelines in biomedical NLP.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Reward GRPO Fine-Tuning for De-biasing Large Language Models: A Study Based on Chinese-Context Discrimination Data</title>
<link>https://arxiv.org/abs/2511.06023</link>
<guid>https://arxiv.org/abs/2511.06023</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Implicit biases, Discrimination, Multi-dimensional, De-biasing

Summary:
This paper introduces a Multi-Reward Group Relative Policy Optimization (GRPO) framework to address the implicit biases and discriminatory tendencies found in Large Language Models (LLMs). By creating a synthetic dataset based on Chinese-context discrimination categories and training a reward model with multi-dimensional signals, the GRPO fine-tunes LLMs to optimize ethical dimensions such as fairness, neutrality, and linguistic quality. Experimental results show a significant reduction in bias intensity and an improvement in alignment with non-discriminatory standards while maintaining fluency and informativeness. The study demonstrates the effectiveness of GRPO-based multi-reward optimization for de-biasing LLMs, providing a replicable framework for cultural-contextual ethical alignment. 

<br><br>Summary: <div>
arXiv:2511.06023v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often exhibit implicit biases and discriminatory tendencies that reflect underlying social stereotypes. While recent alignment techniques such as RLHF and DPO have mitigated some of these issues, they remain limited in addressing culturally specific and multi-dimensional forms of discrimination. This paper proposes a Multi-Reward Group Relative Policy Optimization (GRPO) framework to fine-tune LLMs toward ethical and bias-free behavior. Our approach constructs a synthetic English-language dataset derived from Chinese-context discrimination categories, including regional, ethnic, and occupational biases. Each instance is paired with both neutral and biased responses to train a reward model based on DeBERTa-v3, which provides multi-dimensional reward signals capturing fairness, neutrality, and linguistic quality. The trained reward model then guides GRPO fine-tuning to optimize model outputs along these ethical dimensions. Experimental results demonstrate significant reductions in bias intensity and improved alignment with non-discriminatory standards without compromising fluency or informativeness. This study highlights the effectiveness of GRPO-based multi-reward optimization for de-biasing LLMs and offers a replicable framework for cultural-contextual ethical alignment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Exploration of Feature Relationships in Sparse Autoencoders with Curated Concepts</title>
<link>https://arxiv.org/abs/2511.06048</link>
<guid>https://arxiv.org/abs/2511.06048</guid>
<content:encoded><![CDATA[
<div> Keywords: Sparse autoencoders, large language models, feature interpretation, interactive visualization, dimensionality reduction

Summary:
Sparse autoencoders (SAEs) have become a valuable tool for interpreting features in large language models by learning sparse directions. However, the vast number of extracted directions presents challenges for comprehensive exploration. Conventional embedding techniques like UMAP have limitations that can hinder accurate representation of global structure. In response, a focused exploration framework is proposed in this work, prioritizing curated concepts and their associated SAE features instead of attempting to visualize all features at once. An interactive visualization system combines topology-based visual encoding with dimensionality reduction, allowing users to examine both local and global relationships within selected features. This hybrid approach enhances understanding of SAE behavior through targeted and interpretable subsets, enabling deeper analysis of concept representation in latent space. <br><br>Summary: <div>
arXiv:2511.06048v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for uncovering interpretable features in large language models (LLMs) through the sparse directions they learn. However, the sheer number of extracted directions makes comprehensive exploration intractable. While conventional embedding techniques such as UMAP can reveal global structure, they suffer from limitations including high-dimensional compression artifacts, overplotting, and misleading neighborhood distortions. In this work, we propose a focused exploration framework that prioritizes curated concepts and their corresponding SAE features over attempts to visualize all available features simultaneously. We present an interactive visualization system that combines topology-based visual encoding with dimensionality reduction to faithfully represent both local and global relationships among selected features. This hybrid approach enables users to investigate SAE behavior through targeted, interpretable subsets, facilitating deeper and more nuanced analysis of concept representation in latent space.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Hate Speech Detection: A Three-Layer LoRA-Tuned BERTweet Framework</title>
<link>https://arxiv.org/abs/2511.06051</link>
<guid>https://arxiv.org/abs/2511.06051</guid>
<content:encoded><![CDATA[
<div> Efficient, Hate Speech Detection, Real-time Deployment, LoRA, BERTweet

Summary:
Efficient hate speech detection systems are crucial for real-time deployment. A three-layer framework is proposed, combining rule-based pre-filtering with a parameter-efficient LoRA-tuned BERTweet model and continuous learning capabilities. This approach achieves a 0.85 macro F1 score, comparable to state-of-the-art large language models like SafePhi but with a base model 100 times smaller. Dataset unification and optimized fine-tuning contribute to superior performance compared to traditional BERT-based methods with similar computational requirements. The system only requires 1.87M trainable parameters and trains in about 2 hours on a single T4 GPU, making it suitable for resource-constrained environments while maintaining competitive accuracy for real-world deployment. 

<br><br>Summary: <div>
arXiv:2511.06051v1 Announce Type: new 
Abstract: This paper addresses the critical challenge of developing computationally efficient hate speech detection systems that maintain competitive performance while being practical for real-time deployment. We propose a novel three-layer framework that combines rule-based pre-filtering with a parameter-efficient LoRA-tuned BERTweet model and continuous learning capabilities. Our approach achieves 0.85 macro F1 score - representing 94% of the performance of state-of-the-art large language models like SafePhi (Phi-4 based) while using a base model that is 100x smaller (134M vs 14B parameters). Compared to traditional BERT-based approaches with similar computational requirements, our method demonstrates superior performance through strategic dataset unification and optimized fine-tuning. The system requires only 1.87M trainable parameters (1.37% of full fine-tuning) and trains in approximately 2 hours on a single T4 GPU, making robust hate speech detection accessible in resource-constrained environments while maintaining competitive accuracy for real-world deployment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReMoD: Rethinking Modality Contribution in Multimodal Stance Detection via Dual Reasoning</title>
<link>https://arxiv.org/abs/2511.06057</link>
<guid>https://arxiv.org/abs/2511.06057</guid>
<content:encoded><![CDATA[
<div> framework, multimodal stance detection, dual-reasoning, modality contribution, context-aware<br>
Summary:<br>
The article introduces ReMoD, a framework for multimodal stance detection that incorporates dual-reasoning mechanisms. By combining intuitive reasoning with reflective reasoning, ReMoD dynamically weights the contribution of different modalities based on their expressive power. The intuitive stage utilizes experience pools to form an initial stance hypothesis, while the reflective stage refines this hypothesis by adjusting for modality biases and incorporating deeper semantic insights. Through continuous refinement during training and inference, ReMoD guides robust and context-aware stance decisions. Experimental results on the MMSD benchmark show that ReMoD outperforms baseline models and demonstrates strong generalization capabilities. <div>
arXiv:2511.06057v1 Announce Type: new 
Abstract: Multimodal Stance Detection (MSD) is a crucial task for understanding public opinion on social media. Existing work simply fuses information from various modalities to learn stance representations, overlooking the varying contributions of stance expression from different modalities. Therefore, stance misunderstanding noises may be drawn into the stance learning process due to the risk of learning errors by rough modality combination. To address this, we get inspiration from the dual-process theory of human cognition and propose **ReMoD**, a framework that **Re**thinks **Mo**dality contribution of stance expression through a **D**ual-reasoning paradigm. ReMoD integrates *experience-driven intuitive reasoning* to capture initial stance cues with *deliberate reflective reasoning* to adjust for modality biases, refine stance judgments, and thereby dynamically weight modality contributions based on their actual expressive power for the target stance. Specifically, the intuitive stage queries the Modality Experience Pool (MEP) and Semantic Experience Pool (SEP) to form an initial stance hypothesis, prioritizing historically impactful modalities. This hypothesis is then refined in the reflective stage via two reasoning chains: Modality-CoT updates MEP with adaptive fusion strategies to amplify relevant modalities, while Semantic-CoT refines SEP with deeper contextual insights of stance semantics. These dual experience structures are continuously refined during training and recalled at inference to guide robust and context-aware stance decisions. Extensive experiments on the public MMSD benchmark demonstrate that our ReMoD significantly outperforms most baseline models and exhibits strong generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automating Hardware Design and Verification from Architectural Papers via a Neural-Symbolic Graph Framework</title>
<link>https://arxiv.org/abs/2511.06067</link>
<guid>https://arxiv.org/abs/2511.06067</guid>
<content:encoded><![CDATA[
<div> Keywords: ArchCraft, hardware architectures, Verilog, RTL verification, ArchSynthBench

Summary:
ArchCraft is a framework designed to convert abstract architectural descriptions from academic papers into synthesizable Verilog projects for hardware reproduction. The framework utilizes formal graphs to capture the Architectural Blueprint and symbols to define the Functional Specification, translating unstructured academic papers into hardware-aware designs. ArchCraft generates RTL and testbench code, facilitating verification and debugging to report Power, Area, and Performance metrics. The proposed benchmark ArchSynthBench provides a comprehensive evaluation of hardware synthesis from architectural descriptions. Experimental results showcase the superiority of ArchCraft over direct generation methods and the VerilogCoder framework in paper understanding and code completion. Evaluations and physical implementations verify that the generated RTL code meets all timing constraints and performance metrics consistent with the original papers. <div>
arXiv:2511.06067v1 Announce Type: new 
Abstract: The reproduction of hardware architectures from academic papers remains a significant challenge due to the lack of publicly available source code and the complexity of hardware description languages (HDLs). To this end, we propose \textbf{ArchCraft}, a Framework that converts abstract architectural descriptions from academic papers into synthesizable Verilog projects with register-transfer level (RTL) verification. ArchCraft introduces a structured workflow, which uses formal graphs to capture the Architectural Blueprint and symbols to define the Functional Specification, translating unstructured academic papers into verifiable, hardware-aware designs. The framework then generates RTL and testbench (TB) code decoupled via these symbols to facilitate verification and debugging, ultimately reporting the circuit's Power, Area, and Performance (PPA). Moreover, we propose the first benchmark, \textbf{ArchSynthBench}, for synthesizing hardware from architectural descriptions, with a complete set of evaluation indicators, 50 project-level circuits, and around 600 circuit blocks. We systematically assess ArchCraft on ArchSynthBench, where the experiment results demonstrate the superiority of our proposed method, surpassing direct generation methods and the VerilogCoder framework in both paper understanding and code completion. Furthermore, evaluation and physical implementation of the generated executable RTL code show that these implementations meet all timing constraints without violations, and their performance metrics are consistent with those reported in the original papers.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stemming Hallucination in Language Models Using a Licensing Oracle</title>
<link>https://arxiv.org/abs/2511.06073</link>
<guid>https://arxiv.org/abs/2511.06073</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, hallucinations, Licensing Oracle, structured knowledge graphs, fact-based domains<br>
Summary: <br>
- The study introduces the Licensing Oracle, an architectural solution, to prevent hallucinations in language models by validating against structured knowledge graphs.
- Unlike other methods, the Licensing Oracle integrates deterministic validation to ensure only factually accurate information is generated.
- Experiments comparing the Licensing Oracle with various methods showed it achieved perfect abstention precision and zero false answers, with 89.1% accuracy in factual responses.
- While methods like retrieval-augmented generation and fine-tuning improve performance, they do not eliminate hallucinations.
- The Licensing Oracle offers a reliable solution for fact-based domains and may pave the way for truth-constrained generation in future AI systems. <br><br>Summary: <div>
arXiv:2511.06073v1 Announce Type: new 
Abstract: Language models exhibit remarkable natural language generation capabilities but remain prone to hallucinations, generating factually incorrect information despite producing syntactically coherent responses. This study introduces the Licensing Oracle, an architectural solution designed to stem hallucinations in LMs by enforcing truth constraints through formal validation against structured knowledge graphs. Unlike statistical approaches that rely on data scaling or fine-tuning, the Licensing Oracle embeds a deterministic validation step into the model's generative process, ensuring that only factually accurate claims are made. We evaluated the effectiveness of the Licensing Oracle through experiments comparing it with several state-of-the-art methods, including baseline language model generation, fine-tuning for factual recall, fine-tuning for abstention behavior, and retrieval-augmented generation (RAG). Our results demonstrate that although RAG and fine-tuning improve performance, they fail to eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring that only valid claims were generated with 89.1% accuracy in factual responses. This work shows that architectural innovations, such as the Licensing Oracle, offer a necessary and sufficient solution for hallucinations in domains with structured knowledge representations, offering guarantees that statistical methods cannot match. Although the Licensing Oracle is specifically designed to address hallucinations in fact-based domains, its framework lays the groundwork for truth-constrained generation in future AI systems, providing a new path toward reliable, epistemically grounded models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MuonAll: Muon Variant for Efficient Finetuning of Large Language Models</title>
<link>https://arxiv.org/abs/2511.06086</link>
<guid>https://arxiv.org/abs/2511.06086</guid>
<content:encoded><![CDATA[
<div> Keywords: Muon optimizer, language models, finetuning, AdamW, open-source<br>
Summary:<br>
The article introduces MuonAll, an extension of the Muon optimizer designed for finetuning existing pretrained language models. MuonAll incorporates all parameters inside Muon by transforming them into 2D matrices. Extensive finetuning experiments were conducted on publicly available language models with sizes up to half a billion parameters, showcasing comparable performance to AdamW across various benchmarks. The study emphasizes the effectiveness of Muon and MuonAll as alternative optimizers in the field of language model finetuning. Additionally, the distributed implementations of Muon and MuonAll have been open-sourced and are accessible on GitHub at https://github.com/Saurabh750/optimizer. <div>
arXiv:2511.06086v1 Announce Type: new 
Abstract: Muon optimizer has demonstrated robust results in pretraining of language models but its performance in finetuning of existing public pretrained models is not yet explored. Currently, Muon is used along with AdamW introducing a scope of improvement for adopting all parameters inside Muon. We introduce MuonAll, which incorporates all the parameters inside Muon by transforming into 2D matrices. We conduct extensive finetuning experiments across publicly available language models with model sizes upto half billion parameters. Muon and MuonAll perform at par with AdamW across major benchmarks, highlighting their effectiveness as alternative optimizers. We open-source the distributed implementations of Muon and MuonAll, available at https://github.com/Saurabh750/optimizer
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluation of retrieval-based QA on QUEST-LOFT</title>
<link>https://arxiv.org/abs/2511.06125</link>
<guid>https://arxiv.org/abs/2511.06125</guid>
<content:encoded><![CDATA[
<div> Retrieval-augmented generation, grounded QA, LOFT study, QUEST benchmark, structured output format<br>
<br>
Summary: 
The paper delves into the limitations of current retrieval-augmented generation (RAG) methods in handling questions with distributed information or complex reasoning. It highlights the findings from the LOFT study, showcasing the challenges faced by long-context language models, particularly with the QUEST benchmark. The analysis identifies key factors contributing to poor performance on QUEST-LOFT and presents updated results from comprehensive human evaluations. The study demonstrates that optimizing RAG with a structured output format incorporating reasoning and evidence, followed by answer re-verification, can significantly surpass long-context approaches in performance. <div>
arXiv:2511.06125v1 Announce Type: new 
Abstract: Despite the popularity of retrieval-augmented generation (RAG) as a solution for grounded QA in both academia and industry, current RAG methods struggle with questions where the necessary information is distributed across many documents or where retrieval needs to be combined with complex reasoning. Recently, the LOFT study has shown that this limitation also applies to approaches based on long-context language models, with the QUEST benchmark exhibiting particularly large headroom. In this paper, we provide an in-depth analysis of the factors contributing to the poor performance on QUEST-LOFT, publish updated numbers based on a thorough human evaluation, and demonstrate that RAG can be optimized to significantly outperform long-context approaches when combined with a structured output format containing reasoning and evidence, optionally followed by answer re-verification.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Referring Expressions as a Lens into Spatial Language Grounding in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.06146</link>
<guid>https://arxiv.org/abs/2511.06146</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatial Reasoning, Vision-language models, Referring Expression Comprehension, Ambiguity, Negation

Summary:
Spatial Reasoning plays a crucial role in human cognition, challenging the latest Vision-Language Models (VLMs). Traditional analysis focuses on image captioning and visual question answering, but this study suggests using the Referring Expression Comprehension task to evaluate VLMs' spatial reasoning. This task reveals VLMs' performance in detecting ambiguous objects, understanding complex spatial expressions, and handling negation. By comparing task-specific architectures and large VLMs, the study exposes the strengths and weaknesses of these models in various spatial semantics categories. Despite facing difficulties in the task, the models exhibit different behaviors based on their underlying structures. This research sheds light on existing challenges and opportunities for future studies in enhancing VLMs' spatial reasoning capabilities.

<br><br>Summary: 
- Spatial Reasoning is essential for human cognition and poses challenges for Vision-Language Models (VLMs).
- Referring Expression Comprehension task is proposed as an evaluation platform to assess VLMs' spatial reasoning abilities.
- The task highlights challenges in object detection ambiguity, complex spatial expressions, and negation understanding.
- Comparison of task-specific architectures and large VLMs exposes strengths and weaknesses in handling various spatial semantics categories.
- Despite facing difficulties, models exhibit different behaviors based on their structures, suggesting avenues for future research. <div>
arXiv:2511.06146v1 Announce Type: new 
Abstract: Spatial Reasoning is an important component of human cognition and is an area in which the latest Vision-language models (VLMs) show signs of difficulty. The current analysis works use image captioning tasks and visual question answering. In this work, we propose using the Referring Expression Comprehension task instead as a platform for the evaluation of spatial reasoning by VLMs. This platform provides the opportunity for a deeper analysis of spatial comprehension and grounding abilities when there is 1) ambiguity in object detection, 2) complex spatial expressions with a longer sentence structure and multiple spatial relations, and 3) expressions with negation ('not'). In our analysis, we use task-specific architectures as well as large VLMs and highlight their strengths and weaknesses in dealing with these specific situations. While all these models face challenges with the task at hand, the relative behaviors depend on the underlying models and the specific categories of spatial semantics (topological, directional, proximal, etc.). Our results highlight these challenges and behaviors and provide insight into research gaps and future directions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BookAsSumQA: An Evaluation Framework for Aspect-Based Book Summarization via Question Answering</title>
<link>https://arxiv.org/abs/2511.06183</link>
<guid>https://arxiv.org/abs/2511.06183</guid>
<content:encoded><![CDATA[
<div> Keywords: aspect-based summarization, personalized, targeted, BookAsSumQA, QA-based evaluation framework

Summary: Aspect-based summarization for books is challenging due to the lack of reference summaries for long texts. To address this, BookAsSumQA introduces a QA-based evaluation framework that generates aspect-specific QA pairs from a narrative knowledge graph to evaluate summarization quality based on question-answering performance. Experimental results using BookAsSumQA indicate that LLM-based approaches exhibit higher accuracy on shorter texts, while RAG-based methods are more effective for longer documents, making them more practical for aspect-based book summarization. This framework allows for more personalized and targeted summaries by highlighting specific aspects of the text. <div>
arXiv:2511.06183v1 Announce Type: new 
Abstract: Aspect-based summarization aims to generate summaries that highlight specific aspects of a text, enabling more personalized and targeted summaries. However, its application to books remains unexplored due to the difficulty of constructing reference summaries for long text. To address this challenge, we propose BookAsSumQA, a QA-based evaluation framework for aspect-based book summarization. BookAsSumQA automatically generates aspect-specific QA pairs from a narrative knowledge graph to evaluate summary quality based on its question-answering performance. Our experiments using BookAsSumQA revealed that while LLM-based approaches showed higher accuracy on shorter texts, RAG-based methods become more effective as document length increases, making them more efficient and practical for aspect-based book summarization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning</title>
<link>https://arxiv.org/abs/2511.06190</link>
<guid>https://arxiv.org/abs/2511.06190</guid>
<content:encoded><![CDATA[
<div> framework, routing, Large Language Models, inference costs, confidence scores<br>
Summary:<br>
The paper introduces STEER, a domain-agnostic framework for cost-efficient reasoning in Large Language Models (LLMs). By leveraging confidence scores from smaller models, STEER performs step-level routing between smaller and larger LLMs to reduce inference costs. Unlike existing methods, STEER does not rely on external models for routing and achieves competitive or enhanced accuracy across diverse benchmarks such as Mathematical Reasoning and Planning tasks. By utilizing model-internal confidence as a robust signal for routing, STEER offers a scalable approach for efficient deployment of LLMs. This approach helps to reduce inference costs by up to 48% while maintaining similar or improved accuracy compared to solely using larger models, showcasing the effectiveness of model-internal confidence in guiding efficient reasoning in LLMs. <br> <div>
arXiv:2511.06190v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) - particularly model scaling and test-time techniques - have greatly enhanced the reasoning capabilities of language models at the expense of higher inference costs. To lower inference costs, prior works train router models or deferral mechanisms that allocate easy queries to a small, efficient model, while forwarding harder queries to larger, more expensive models. However, these trained router models often lack robustness under domain shifts and require expensive data synthesis techniques such as Monte Carlo rollouts to obtain sufficient ground-truth routing labels for training. In this work, we propose Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning (STEER), a domain-agnostic framework that performs fine-grained, step-level routing between smaller and larger LLMs without utilizing external models. STEER leverages confidence scores from the smaller model's logits prior to generating a reasoning step, so that the large model is invoked only when necessary. Extensive evaluations using different LLMs on a diverse set of challenging benchmarks across multiple domains such as Mathematical Reasoning, Multi-Hop QA, and Planning tasks indicate that STEER achieves competitive or enhanced accuracy while reducing inference costs (up to +20% accuracy with 48% less FLOPs compared to solely using the larger model on AIME), outperforming baselines that rely on trained external modules. Our results establish model-internal confidence as a robust, domain-agnostic signal for model routing, offering a scalable pathway for efficient LLM deployment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimer's Disease</title>
<link>https://arxiv.org/abs/2511.06215</link>
<guid>https://arxiv.org/abs/2511.06215</guid>
<content:encoded><![CDATA[
<div> Keywords: Alzheimer's Disease, language models, in-context learning, explicit knowledge, clinical reasoning

Summary:
Explicit Knowledge In-Context Learners (EK-ICL) is proposed as a framework to enhance Alzheimer's Disease detection using narrative transcripts. It integrates structured explicit knowledge to improve reasoning stability and task alignment in large language models (LLMs) under out-of-distribution and data-scarce conditions. EK-ICL incorporates confidence scores from small language models to ground predictions, parsing feature scores for improved demonstration selection, and label word replacement to address semantic misalignment. A parsing-based retrieval strategy and ensemble prediction are used to handle semantic homogeneity in AD transcripts. Extensive experiments showed that EK-ICL outperforms current fine-tuning and in-context learning approaches in AD detection. The study highlights the significance of explicit knowledge in clinical reasoning, emphasizing the importance of aligning label semantics with task context for optimal performance in low-resource settings.<br><br>Summary: <div>
arXiv:2511.06215v1 Announce Type: new 
Abstract: Detecting Alzheimer's Disease (AD) from narrative transcripts remains a challenging task for large language models (LLMs), particularly under out-of-distribution (OOD) and data-scarce conditions. While in-context learning (ICL) provides a parameter-efficient alternative to fine-tuning, existing ICL approaches often suffer from task recognition failure, suboptimal demonstration selection, and misalignment between label words and task objectives, issues that are amplified in clinical domains like AD detection. We propose Explicit Knowledge In-Context Learners (EK-ICL), a novel framework that integrates structured explicit knowledge to enhance reasoning stability and task alignment in ICL. EK-ICL incorporates three knowledge components: confidence scores derived from small language models (SLMs) to ground predictions in task-relevant patterns, parsing feature scores to capture structural differences and improve demo selection, and label word replacement to resolve semantic misalignment with LLM priors. In addition, EK-ICL employs a parsing-based retrieval strategy and ensemble prediction to mitigate the effects of semantic homogeneity in AD transcripts. Extensive experiments across three AD datasets demonstrate that EK-ICL significantly outperforms state-of-the-art fine-tuning and ICL baselines. Further analysis reveals that ICL performance in AD detection is highly sensitive to the alignment of label semantics and task-specific context, underscoring the importance of explicit knowledge in clinical reasoning under low-resource conditions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization</title>
<link>https://arxiv.org/abs/2511.06222</link>
<guid>https://arxiv.org/abs/2511.06222</guid>
<content:encoded><![CDATA[
<div> alignment paradigm, trustworthiness, helpfulness, Self-Priority Alignment, scalability
<br>
Summary: 
The article introduces a new alignment paradigm called priority alignment, focusing on ensuring trustworthiness before helpfulness in language model applications for high-stakes scenarios. It presents Self-Priority Alignment (SPA), an unsupervised framework that generates diverse responses, evaluates them, and refines them to meet trustworthy thresholds. SPA uses dual-criterion denoising to remove inconsistencies and control variance, constructing preference pairs and fine-tuning the model with an uncertainty-weighted alignment loss. Experimental results across various benchmarks demonstrate that SPA improves helpfulness while maintaining safety, outperforming strong baselines and preserving general capabilities. The approach provides a scalable and interpretable strategy for aligning language models in critical applications. 
<br><br> <div>
arXiv:2511.06222v1 Announce Type: new 
Abstract: In high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs must be both trustworthy and helpful. However, these goals often conflict. We propose priority alignment, a new alignment paradigm that enforces a strict "trustworthy-before-helpful" ordering: optimization of helpfulness is conditioned on first meeting trustworthy thresholds (e.g., harmlessness or honesty). To realize this, we introduce Self-Priority Alignment (SPA)-a fully unsupervised framework that generates diverse responses, self-evaluates them and refines them by the model itself, and applies dual-criterion denoising to remove inconsistency and control variance. From this, SPA constructs lexicographically ordered preference pairs and fine-tunes the model using an uncertainty-weighted alignment loss that emphasizes high-confidence, high-gap decisions. Experiments across multiple benchmarks show that SPA improves helpfulness without compromising safety, outperforming strong baselines while preserving general capabilities. Our results demonstrate that SPA provides a scalable and interpretable alignment strategy for critical LLM applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overview of CHIP 2025 Shared Task 2: Discharge Medication Recommendation for Metabolic Diseases Based on Chinese Electronic Health Records</title>
<link>https://arxiv.org/abs/2511.06230</link>
<guid>https://arxiv.org/abs/2511.06230</guid>
<content:encoded><![CDATA[
<div> Keywords: discharge medication recommendation, CHIP 2025 Shared Task 2, Chinese EHR data, multi-label nature, large language model ensemble systems

Summary:
The paper discusses the CHIP 2025 Shared Task 2 competition focused on developing automated discharge medication recommendations using Chinese electronic health record (EHR) data. A high-quality dataset called CDrugRed was created for this task, challenging due to multi-label medication recommendations and diverse clinical text. Over 500 teams participated, with the top team showcasing the potential of advanced large language model ensembles. They achieved a Jaccard score of 0.5102 and an F1 score of 0.6267 on the final test set. While demonstrating promise in medication recommendation using language models in Chinese EHRs, the results also highlight the existing challenges. The post-evaluation phase for the competition is ongoing on the Tianchi platform. <br><br>Summary: <div>
arXiv:2511.06230v1 Announce Type: new 
Abstract: Discharge medication recommendation plays a critical role in ensuring treatment continuity, preventing readmission, and improving long-term management for patients with chronic metabolic diseases. This paper present an overview of the CHIP 2025 Shared Task 2 competition, which aimed to develop state-of-the-art approaches for automatically recommending appro-priate discharge medications using real-world Chinese EHR data. For this task, we constructed CDrugRed, a high-quality dataset consisting of 5,894 de-identified hospitalization records from 3,190 patients in China. This task is challenging due to multi-label nature of medication recommendation, het-erogeneous clinical text, and patient-specific variability in treatment plans. A total of 526 teams registered, with 167 and 95 teams submitting valid results to the Phase A and Phase B leaderboards, respectively. The top-performing team achieved the highest overall performance on the final test set, with a Jaccard score of 0.5102, F1 score of 0.6267, demonstrating the potential of advanced large language model (LLM)-based ensemble systems. These re-sults highlight both the promise and remaining challenges of applying LLMs to medication recommendation in Chinese EHRs. The post-evaluation phase remains open at https://tianchi.aliyun.com/competition/entrance/532411/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analyzing and Mitigating Negation Artifacts using Data Augmentation for Improving ELECTRA-Small Model Accuracy</title>
<link>https://arxiv.org/abs/2511.06234</link>
<guid>https://arxiv.org/abs/2511.06234</guid>
<content:encoded><![CDATA[
<div> negation, pre-trained models, natural language inference, dataset artifacts, data augmentation  
Summary:  
- Pre-trained models for natural language inference lack understanding of language nuances like negation, relying on dataset artifacts for high performance.  
- An investigation into an ELECTRA-small model fine-tuned on the SNLI dataset revealed struggles in accurately classifying negation-containing examples.  
- To address this issue, training data was augmented with contrast sets and adversarial examples emphasizing negation.  
- Results showed that this targeted data augmentation improved the model's accuracy on negation-containing examples without compromising overall performance.  
- The identified dataset artifact of the model's struggle with negation was successfully mitigated through the data augmentation technique.  

Summary: <div>
arXiv:2511.06234v1 Announce Type: new 
Abstract: Pre-trained models for natural language inference (NLI) often achieve high performance on benchmark datasets by using spurious correlations, or dataset artifacts, rather than understanding language touches such as negation. In this project, we investigate the performance of an ELECTRA-small model fine-tuned on the Stanford Natural Language Inference (SNLI) dataset, focusing on its handling of negation. Through analysis, we identify that the model struggles with correctly classifying examples containing negation. To address this, we augment the training data with contrast sets and adversarial examples emphasizing negation. Our results demonstrate that this targeted data augmentation improves the model's accuracy on negation-containing examples without adversely affecting overall performance, therefore mitigating the identified dataset artifact.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeSense:Making Large Language Models Proficient in Time-Series Analysis</title>
<link>https://arxiv.org/abs/2511.06344</link>
<guid>https://arxiv.org/abs/2511.06344</guid>
<content:encoded><![CDATA[
<div> Keywords: time-series, text-temporal data integration, large language models, multimodal framework, temporal sense

Summary:
The study introduces the EvalTS benchmark consisting of 10 tasks to evaluate models incorporating text and temporal data. It addresses the issue of models biased towards textual cues by proposing TimeSense, a multimodal framework. TimeSense includes a Temporal Sense module to ground textual reasoning in time-series dynamics and uses coordinate-based positional embeddings for spatial understanding of time-series data. Experimental results show that TimeSense outperforms existing methods on complex multi-dimensional time-series reasoning tasks, achieving state-of-the-art performance across multiple tasks. The framework balances textual reasoning with a preserved temporal sense, enhancing the model's ability to analyze time-series data effectively. <br><br>Summary: <div>
arXiv:2511.06344v1 Announce Type: new 
Abstract: In the time-series domain, an increasing number of works combine text with temporal data to leverage the reasoning capabilities of large language models (LLMs) for various downstream time-series understanding tasks. This enables a single model to flexibly perform tasks that previously required specialized models for each domain. However, these methods typically rely on text labels for supervision during training, biasing the model toward textual cues while potentially neglecting the full temporal features. Such a bias can lead to outputs that contradict the underlying time-series context. To address this issue, we construct the EvalTS benchmark, comprising 10 tasks across three difficulty levels, from fundamental temporal pattern recognition to complex real-world reasoning, to evaluate models under more challenging and realistic scenarios. We also propose TimeSense, a multimodal framework that makes LLMs proficient in time-series analysis by balancing textual reasoning with a preserved temporal sense. TimeSense incorporates a Temporal Sense module that reconstructs the input time-series within the model's context, ensuring that textual reasoning is grounded in the time-series dynamics. Moreover, to enhance spatial understanding of time-series data, we explicitly incorporate coordinate-based positional embeddings, which provide each time point with spatial context and enable the model to capture structural dependencies more effectively. Experimental results demonstrate that TimeSense achieves state-of-the-art performance across multiple tasks, and it particularly outperforms existing methods on complex multi-dimensional time-series reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HatePrototypes: Interpretable and Transferable Representations for Implicit and Explicit Hate Speech Detection</title>
<link>https://arxiv.org/abs/2511.06391</link>
<guid>https://arxiv.org/abs/2511.06391</guid>
<content:encoded><![CDATA[
<div> Keywords: offensive content moderation, hate speech detection, implicit hate, cross-task transfer, HatePrototypes

Summary:<br>
This study examines the optimization of offensive content moderation models for different types of hateful messages. While existing benchmarks focus on explicit hate toward protected groups, implicit hate, including demeaning comparisons and subtle discriminatory language, is often overlooked. The researchers propose using HatePrototypes, class-level vector representations derived from language models, to enable cross-task transfer between explicit and implicit hate with as few as 50 examples per class. They demonstrate that these prototypes facilitate efficient hate speech detection without the need for repeated fine-tuning. Additionally, parameter-free early exiting with prototypes proves effective for both explicit and implicit hate. The code, prototype resources, and evaluation scripts have been released to support future research on efficient and transferable hate speech detection.<br>Summary: <div>
arXiv:2511.06391v1 Announce Type: new 
Abstract: Optimization of offensive content moderation models for different types of hateful messages is typically achieved through continued pre-training or fine-tuning on new hate speech benchmarks. However, existing benchmarks mainly address explicit hate toward protected groups and often overlook implicit or indirect hate, such as demeaning comparisons, calls for exclusion or violence, and subtle discriminatory language that still causes harm. While explicit hate can often be captured through surface features, implicit hate requires deeper, full-model semantic processing. In this work, we question the need for repeated fine-tuning and analyze the role of HatePrototypes, class-level vector representations derived from language models optimized for hate speech detection and safety moderation. We find that these prototypes, built from as few as 50 examples per class, enable cross-task transfer between explicit and implicit hate, with interchangeable prototypes across benchmarks. Moreover, we show that parameter-free early exiting with prototypes is effective for both hate types. We release the code, prototype resources, and evaluation scripts to support future research on efficient and transferable hate speech detection.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SugarTextNet: A Transformer-Based Framework for Detecting Sugar Dating-Related Content on Social Media with Context-Aware Focal Loss</title>
<link>https://arxiv.org/abs/2511.06402</link>
<guid>https://arxiv.org/abs/2511.06402</guid>
<content:encoded><![CDATA[
<div> Transformer-based framework, social media posts, sugar dating, class imbalance, content moderation<br>
Summary:<br> 
- Sugar dating-related content on social media poses societal and regulatory concerns,<br>
- Detection is challenging due to euphemisms and class imbalance,<br>
- SugarTextNet framework with pretrained transformer encoder and attention-based cue extractor,<br>
- Introduces Context-Aware Focal Loss for minority-class detection,<br>
- Outperforms traditional models and large language models in detecting sensitive content. <br> <div>
arXiv:2511.06402v1 Announce Type: new 
Abstract: Sugar dating-related content has rapidly proliferated on mainstream social media platforms, giving rise to serious societal and regulatory concerns, including commercialization of intimate relationships and the normalization of transactional relationships.~Detecting such content is highly challenging due to the prevalence of subtle euphemisms, ambiguous linguistic cues, and extreme class imbalance in real-world data.~In this work, we present SugarTextNet, a novel transformer-based framework specifically designed to identify sugar dating-related posts on social media.~SugarTextNet integrates a pretrained transformer encoder, an attention-based cue extractor, and a contextual phrase encoder to capture both salient and nuanced features in user-generated text.~To address class imbalance and enhance minority-class detection, we introduce Context-Aware Focal Loss, a tailored loss function that combines focal loss scaling with contextual weighting.~We evaluate SugarTextNet on a newly curated, manually annotated dataset of 3,067 Chinese social media posts from Sina Weibo, demonstrating that our approach substantially outperforms traditional machine learning models, deep learning baselines, and large language models across multiple metrics.~Comprehensive ablation studies confirm the indispensable role of each component.~Our findings highlight the importance of domain-specific, context-aware modeling for sensitive content detection, and provide a robust solution for content moderation in complex, real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Well Do LLMs Understand Drug Mechanisms? A Knowledge + Reasoning Evaluation Dataset</title>
<link>https://arxiv.org/abs/2511.06418</link>
<guid>https://arxiv.org/abs/2511.06418</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, drug development, personalized medicine, reasoning tasks, counterfactuals

Summary: 
Large language models (LLMs) are being increasingly utilized in the fields of drug development and personalized medicine. A new dataset has been introduced to evaluate LLMs on their factual knowledge of drug mechanisms and their ability to reason about them in novel situations, presented as counterfactuals. Results show that the o4-mini model performs better than other models from OpenAI, with the Qwen3-4B-thinking model closely matching its performance. The open-world setting for reasoning tasks, where models must recall relevant knowledge, proves to be more challenging than the closed-world setting. Additionally, counterfactuals affecting internal links in the reasoning chain are more difficult than those affecting links from the drug mentioned in the prompt. Overall, these findings suggest the importance of LLMs in drug development and personalized medicine, highlighting the need for models to demonstrate factual knowledge and deep understanding of drug mechanisms. 

<br><br>Summary: <div>
arXiv:2511.06418v1 Announce Type: new 
Abstract: Two scientific fields showing increasing interest in pre-trained large language models (LLMs) are drug development / repurposing, and personalized medicine. For both, LLMs have to demonstrate factual knowledge as well as a deep understanding of drug mechanisms, so they can recall and reason about relevant knowledge in novel situations. Drug mechanisms of action are described as a series of interactions between biomedical entities, which interlink into one or more chains directed from the drug to the targeted disease. Composing the effects of the interactions in a candidate chain leads to an inference about whether the drug might be useful or not for that disease. We introduce a dataset that evaluates LLMs on both factual knowledge of known mechanisms, and their ability to reason about them under novel situations, presented as counterfactuals that the models are unlikely to have seen during training. Using this dataset, we show that o4-mini outperforms the 4o, o3, and o3-mini models from OpenAI, and the recent small Qwen3-4B-thinking model closely matches o4-mini's performance, even outperforming it in some cases. We demonstrate that the open world setting for reasoning tasks, which requires the model to recall relevant knowledge, is more challenging than the closed world setting where the needed factual knowledge is provided. We also show that counterfactuals affecting internal links in the reasoning chain present a much harder task than those affecting a link from the drug mentioned in the prompt.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dutch Metaphor Extraction from Cancer Patients' Interviews and Forum Data using LLMs and Human in the Loop</title>
<link>https://arxiv.org/abs/2511.06427</link>
<guid>https://arxiv.org/abs/2511.06427</guid>
<content:encoded><![CDATA[
<div> Keywords: Metaphors, Dutch language data, Cancer patients, Large language models, HealthQuote.NL 

Summary:<br><br>
This article focuses on the extraction of metaphors used by cancer patients in Dutch language data to improve healthcare communication. Through analyzing patient storytelling interviews and online forum data, including posts and comments, researchers explore the performance of large language models (LLMs) using various prompting strategies. With a human-in-the-loop setup, the extracted metaphors are verified and compiled into a corpus named HealthQuote.NL. The goal is to enhance patient care, shared decision making, communication between patients and clinicians, and patient health literacy. The extracted metaphors can also contribute to designing personalized care pathways. The prompts and resources related to this research are shared on GitHub for further exploration and implementation. <div>
arXiv:2511.06427v1 Announce Type: new 
Abstract: Metaphors and metaphorical language (MLs) play an important role in healthcare communication between clinicians, patients, and patients' family members. In this work, we focus on Dutch language data from cancer patients. We extract metaphors used by patients using two data sources: (1) cancer patient storytelling interview data and (2) online forum data, including patients' posts, comments, and questions to professionals. We investigate how current state-of-the-art large language models (LLMs) perform on this task by exploring different prompting strategies such as chain of thought reasoning, few-shot learning, and self-prompting. With a human-in-the-loop setup, we verify the extracted metaphors and compile the outputs into a corpus named HealthQuote.NL. We believe the extracted metaphors can support better patient care, for example shared decision making, improved communication between patients and clinicians, and enhanced patient health literacy. They can also inform the design of personalized care pathways. We share prompts and related resources at https://github.com/aaronlifenghan/HealthQuote.NL
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Resource-Efficient Multimodal Intelligence: Learned Routing among Specialized Expert Models</title>
<link>https://arxiv.org/abs/2511.06441</link>
<guid>https://arxiv.org/abs/2511.06441</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, large language models, multimodal queries, routing network, efficient vision tasks

Summary: 
AI models are evolving beyond text and increasingly powering vision, audio, and document understanding. While large language models (LLMs) are effective, their high inference costs present challenges for real-time deployment. In contrast, smaller open-source models offer cost advantages but struggle with complex or multimodal queries. To address this, a unified framework has been introduced that intelligently routes queries to the most suitable expert model using a learned routing network. For vision tasks, a two-stage open-source pipeline is employed, optimized for efficiency by incorporating efficient classical vision components where they excel. Performance on benchmarks such as MMLU and VQA matches or exceeds monolithic LLM systems while reducing reliance on costly models by over 67%. The framework's extensible, multi-agent orchestration enables the delivery of high-quality, resource-efficient AI at scale. 

<br><br>Summary: <div>
arXiv:2511.06441v1 Announce Type: new 
Abstract: As AI moves beyond text, large language models (LLMs) increasingly power vision, audio, and document understanding; however, their high inference costs hinder real-time, scalable deployment. Conversely, smaller open-source models offer cost advantages but struggle with complex or multimodal queries. We introduce a unified, modular framework that intelligently routes each query - textual, multimodal, or complex - to the most fitting expert model, using a learned routing network that balances cost and quality. For vision tasks, we employ a two-stage open-source pipeline optimized for efficiency and reviving efficient classical vision components where they remain SOTA for sub-tasks. On benchmarks such as Massive Multitask Language Understanding (MMLU) and Visual Question Answering (VQA), we match or exceed the performance of always-premium LLM (monolithic systems with one model serving all query types) performance, yet reduce the reliance on costly models by over 67%. With its extensible, multi-agent orchestration, we deliver high-quality, resource-efficient AI at scale.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention</title>
<link>https://arxiv.org/abs/2511.06446</link>
<guid>https://arxiv.org/abs/2511.06446</guid>
<content:encoded><![CDATA[
<div> knowledge bases, language models, retrieval, structured knowledge, attention<br>
Summary:<br>
The paper introduces SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases into large language models. SR-KI encodes KBs into key-value pairs and injects them into LLMs' KV cache for efficient retrieval. It employs a two-stage training paradigm, incorporating a dedicated retrieval layer within the LLM and applying an attention-based loss for supervision. Unlike traditional methods, SR-KI performs retrieval entirely within the model's latent space, allowing for dynamic knowledge updates. Experimental results show that SR-KI enables the integration of up to 40K KBs into a 7B LLM while maintaining strong retrieval performance. It achieves high Recall@10 rates and up to 99.75% compression of injected KBs, demonstrating its effectiveness in question answering and KB ID generation tasks. <div>
arXiv:2511.06446v1 Announce Type: new 
Abstract: This paper proposes SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases (KBs) into large language models (LLMs). SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder, and injects them into LLMs' KV cache. Building on this representation, we employ a two-stage training paradigm: first locating a dedicated retrieval layer within the LLM, and then applying an attention-based loss at this layer to explicitly supervise attention toward relevant KB entries. Unlike traditional retrieval-augmented generation methods that rely heavily on the performance of external retrievers and multi-stage pipelines, SR-KI supports end-to-end inference by performing retrieval entirely within the models latent space. This design enables efficient compression of injected knowledge and facilitates dynamic knowledge updates. Comprehensive experiments demonstrate that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98% Recall@10 on the best-performing task and exceeding 88% on average across all tasks. Task performance on question answering and KB ID generation also demonstrates that SR-KI maintains strong performance while achieving up to 99.75% compression of the injected KBs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking what Matters: Effective and Robust Multilingual Realignment for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2511.06497</link>
<guid>https://arxiv.org/abs/2511.06497</guid>
<content:encoded><![CDATA[
<div> Realignment, cross-lingual transfer, multilingual language models, low-resource languages, parallel data <br>
<br>
Summary: Realignment is a promising strategy for improving cross-lingual transfer in multilingual language models. However, its effectiveness varies, especially for low-resource languages (LRLs). This study explores whether using strategically selected language subsets can provide comparable or even superior results compared to full multilingual alignment. The experiments show that realignment can greatly benefit LRLs and that carefully chosen language subsets can match or even outperform full alignment, particularly for unseen LRLs. This suggests that realignment does not always require all available languages, reducing the need for extensive data collection. Strategic language selection can lead to efficient and robust realignment, offering a more feasible approach for enhancing cross-lingual transfer in multilingual language models. <br> <div>
arXiv:2511.06497v1 Announce Type: new 
Abstract: Realignment is a promising strategy to improve cross-lingual transfer in multilingual language models. However, empirical results are mixed and often unreliable, particularly for typologically distant or low-resource languages (LRLs) compared to English. Moreover, word realignment tools often rely on high-quality parallel data, which can be scarce or noisy for many LRLs. In this work, we conduct an extensive empirical study to investigate whether realignment truly benefits from using all available languages, or if strategically selected subsets can offer comparable or even improved cross-lingual transfer, and study the impact on LRLs. Our controlled experiments show that realignment can be particularly effective for LRLs and that using carefully selected, linguistically diverse subsets can match full multilingual alignment, and even outperform it for unseen LRLs. This indicates that effective realignment does not require exhaustive language coverage and can reduce data collection overhead, while remaining both efficient and robust when guided by informed language selection.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Had One Job: Per-Task Quantization Using LLMs' Hidden Representations</title>
<link>https://arxiv.org/abs/2511.06516</link>
<guid>https://arxiv.org/abs/2511.06516</guid>
<content:encoded><![CDATA[
<div> quantization, Large Language Models, task-aware, hidden representations, precision<br>
<br>
Summary: <br>
Large Language Models (LLMs) have shown excellent performance on various tasks, but their large size can lead to inefficiencies in memory and latency for applications requiring limited capabilities. This work introduces two new task-aware post-training quantization (PTQ) methods, Task-Aware Quantization (TAQ) and TAQO, which leverage task-specific signals encoded in hidden representations to guide the quantization process. By identifying task-relevant layers and preserving their precision while aggressively quantizing others, TAQ and TAQO achieve stable task sensitivity profiles and efficient task-specialized models. Experimental results demonstrate that TAQ and TAQO outperform existing baselines on various models, with TAQ leading on Phi-4 and TAQO leading on Llama-3.1, Qwen3, and Qwen2.5. For example, TAQ achieves significantly higher accuracy on Phi-4 compared to Activation-aware Weight Quantization (AWQ), while maintaining performance close to the original accuracy at lower average precision levels. <div>
arXiv:2511.06516v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel across diverse tasks, yet many applications require only limited capabilities, making large variants inefficient in memory and latency. Existing approaches often combine distillation and quantization, but most post-training quantization (PTQ) methods are task-agnostic, ignoring how task-specific signals are distributed across layers. In this work, we propose to use hidden representations that encode task-salient signals as a guideline for quantization. In order to fully utilize our innovative idea, this paper compares two new task-aware PTQ methods: Task-Aware Quantization (TAQ), which allocates bitwidths using task-conditioned statistics from hidden activations, and TAQO, which allocates precision based on direct layer sensitivity tests. From a small calibration set, these approaches identify task-relevant layers, preserving their precision while aggressively quantizing the rest. This yields stable task sensitivity profiles and efficient task-specialized models. Across models, TAQ and TAQO outperform the baselines; TAQ leads on Phi-4, while TAQO leads on Llama-3.1, Qwen3, and Qwen2.5. For instances, on Phi-4 it achieves 42.33 EM / 50.81 F1, far surpassing Activation-aware Weight Quantization (AWQ) (2.25 / 7.07), while remaining within < 1.0% of the original accuracy at lower average precision.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better Datasets Start From RefineLab: Automatic Optimization for High-Quality Dataset Refinement</title>
<link>https://arxiv.org/abs/2511.06530</link>
<guid>https://arxiv.org/abs/2511.06530</guid>
<content:encoded><![CDATA[
<div> Framework, Question-Answer, Data refinement, Large Language Models, Dataset quality  
Summary:  
RefineLab is a novel framework that uses Large Language Models to automatically refine Question-Answer datasets, addressing quality issues such as domain coverage gaps, difficulty imbalances, and factual inconsistencies. By setting target quality attributes and a token budget constraint, RefineLab performs selective edits on the dataset to improve overall quality while being resource-efficient. The framework operates as a constrained optimization problem, selecting optimal refinement strategies such as rephrasing or distractor replacement for each QA sample. Experimental results show that RefineLab significantly enhances dataset quality in terms of coverage, difficulty alignment, factual fidelity, and distractor quality, reducing discrepancies from expert-crafted datasets. This approach offers a scalable and customizable solution for reproducible dataset design, with implications for improving Large Language Model evaluation.  
<br><br>Summary: <div>
arXiv:2511.06530v1 Announce Type: new 
Abstract: High-quality Question-Answer (QA) datasets are foundational for reliable Large Language Model (LLM) evaluation, yet even expert-crafted datasets exhibit persistent gaps in domain coverage, misaligned difficulty distributions, and factual inconsistencies. The recent surge in generative model-powered datasets has compounded these quality challenges. In this work, we introduce RefineLab, the first LLM-driven framework that automatically refines raw QA textual data into high-quality datasets under a controllable token-budget constraint. RefineLab takes a set of target quality attributes (such as coverage and difficulty balance) as refinement objectives, and performs selective edits within a predefined token budget to ensure practicality and efficiency. In essence, RefineLab addresses a constrained optimization problem: improving the quality of QA samples as much as possible while respecting resource limitations. With a set of available refinement operations (e.g., rephrasing, distractor replacement), RefineLab takes as input the original dataset, a specified set of target quality dimensions, and a token budget, and determines which refinement operations should be applied to each QA sample. This process is guided by an assignment module that selects optimal refinement strategies to maximize overall dataset quality while adhering to the budget constraint. Experiments demonstrate that RefineLab consistently narrows divergence from expert datasets across coverage, difficulty alignment, factual fidelity, and distractor quality. RefineLab pioneers a scalable, customizable path to reproducible dataset design, with broad implications for LLM evaluation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ibom NLP: A Step Toward Inclusive Natural Language Processing for Nigeria's Minority Languages</title>
<link>https://arxiv.org/abs/2511.06531</link>
<guid>https://arxiv.org/abs/2511.06531</guid>
<content:encoded><![CDATA[
<div> languages, Nigeria, natural language processing, machine translation, topic classification
Summary: 
- Nigeria, with over 200 million people and 500 languages, lacks NLP research in languages beyond Hausa, Igbo, Nigerian-Pidgin, and Yoruba.
- A new dataset, ibom, introduces Anaang, Efik, Ibibio, and Oro languages for machine translation and topic classification.
- These languages are not represented in major benchmarks like Google Translate or Flores-200, highlighting the need for their inclusion.
- Evaluation shows current language models perform poorly in machine translation for these languages but improve steadily in topic classification with more shots.
<br><br>Summary: <div>
arXiv:2511.06531v1 Announce Type: new 
Abstract: Nigeria is the most populous country in Africa with a population of more than 200 million people. More than 500 languages are spoken in Nigeria and it is one of the most linguistically diverse countries in the world. Despite this, natural language processing (NLP) research has mostly focused on the following four languages: Hausa, Igbo, Nigerian-Pidgin, and Yoruba (i.e <1% of the languages spoken in Nigeria). This is in part due to the unavailability of textual data in these languages to train and apply NLP algorithms. In this work, we introduce ibom -- a dataset for machine translation and topic classification in four Coastal Nigerian languages from the Akwa Ibom State region: Anaang, Efik, Ibibio, and Oro. These languages are not represented in Google Translate or in major benchmarks such as Flores-200 or SIB-200. We focus on extending Flores-200 benchmark to these languages, and further align the translated texts with topic labels based on SIB-200 classification dataset. Our evaluation shows that current LLMs perform poorly on machine translation for these languages in both zero-and-few shot settings. However, we find the few-shot samples to steadily improve topic classification with more shots.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rep2Text: Decoding Full Text from a Single LLM Token Representation</title>
<link>https://arxiv.org/abs/2511.06571</link>
<guid>https://arxiv.org/abs/2511.06571</guid>
<content:encoded><![CDATA[
<div> Recovering Input Text, Large Language Models (LLMs), Rep2Text, Information Bottleneck Effect, Generalization Capability
<br>
Summary:
Rep2Text introduces a new framework for decoding full text from last-token representations in large language models (LLMs). Through a trainable adapter and decoding language model, it can recover over half of the information in 16-token sequences while maintaining semantic integrity. There is an observed information bottleneck effect where longer sequences show decreased token-level recovery but maintain semantic coherence. The framework shows robust generalization to out-of-distribution medical data, indicating its potential applicability across diverse domains. <div>
arXiv:2511.06571v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable progress across diverse tasks, yet their internal mechanisms remain largely opaque. In this work, we address a fundamental question: to what extent can the original input text be recovered from a single last-token representation within an LLM? We propose Rep2Text, a novel framework for decoding full text from last-token representations. Rep2Text employs a trainable adapter that projects a target model's internal representations into the embedding space of a decoding language model, which then autoregressively reconstructs the input text. Experiments on various model combinations (Llama-3.1-8B, Gemma-7B, Mistral-7B-v0.1, Llama-3.2-3B) demonstrate that, on average, over half of the information in 16-token sequences can be recovered from this compressed representation while maintaining strong semantic integrity and coherence. Furthermore, our analysis reveals an information bottleneck effect: longer sequences exhibit decreased token-level recovery while preserving strong semantic integrity. Besides, our framework also demonstrates robust generalization to out-of-distribution medical data.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabRAG: Tabular Document Retrieval via Structured Language Representations</title>
<link>https://arxiv.org/abs/2511.06582</link>
<guid>https://arxiv.org/abs/2511.06582</guid>
<content:encoded><![CDATA[

arXiv:2511.06582v1 Announce Type: new 
Abstract: Ingesting data for Retrieval-Augmented Generation (RAG) involves either fine-tuning the embedding model directly on the target corpus or parsing documents for embedding model encoding. The former, while accurate, incurs high computational hardware requirements, while the latter suffers from suboptimal performance when extracting tabular data. In this work, we address the latter by presenting TabRAG, a parsing-based RAG pipeline designed to tackle table-heavy documents via structured language representations. TabRAG outperforms existing popular parsing-based methods for generation and retrieval. Code is available at https://github.com/jacobyhsi/TabRAG.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making</title>
<link>https://arxiv.org/abs/2511.06592</link>
<guid>https://arxiv.org/abs/2511.06592</guid>
<content:encoded><![CDATA[

arXiv:2511.06592v1 Announce Type: new 
Abstract: As large language models transition from text-based interfaces to audio interactions in clinical settings, they might introduce new vulnerabilities through paralinguistic cues in audio. We evaluated these models on 170 clinical cases, each synthesized into speech from 36 distinct voice profiles spanning variations in age, gender, and emotion. Our findings reveal a severe modality bias: surgical recommendations for audio inputs varied by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer recommendations. Further analysis uncovered age disparities of up to 12% between young and elderly voices, which persisted in most models despite chain-of-thought prompting. While explicit reasoning successfully eliminated gender bias, the impact of emotion was not detected due to poor recognition performance. These results demonstrate that audio LLMs are susceptible to making clinical decisions based on a patient's voice characteristics rather than medical evidence, a flaw that risks perpetuating healthcare disparities. We conclude that bias-aware architectures are essential and urgently needed before the clinical deployment of these models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Duality-based Mode Operations and Pyramid Multilayer Mapping for Rhetorical Modes</title>
<link>https://arxiv.org/abs/2511.06601</link>
<guid>https://arxiv.org/abs/2511.06601</guid>
<content:encoded><![CDATA[

arXiv:2511.06601v1 Announce Type: new 
Abstract: Rhetorical modes are useful in both academic and non-academic writing, and can be subjects to be studied within linguistic research and computational modeling. Establishing a conceptual bridge among these domains could enable each to benefit from the others. This paper proposes duality-based mode operations (split-unite, forward-backward, expansion-reduction and orthogonal dualities) to expand the set of rhetorical modes, introducing generated modes like combination and generalization, thereby enhancing epistemic diversity across multiple applications. It further presents a pyramid multilayer mapping framework (e.g., three layers from the rhetorical model layer, to cognitive layer, and to epistemic layers) that reduces the resulting cognitive complexity. The degrees of expressive diversity and complexity reduction are quantified through binomial combinatorics and Shannon entropy analysis. A Marginal Rhetorical Bit (MRB) is identified, permitting the definition of a rhetorical-scalable parameter that measures expressive growth speed in bits per stage. A direct entropy measure shows that hierarchical selection over smaller subsets markedly reduces choice uncertainty compared with flat selection across all modes. These considerations appear to transform static and non-measurable rhetorical taxonomies into more dynamic and more measurable systems for discourse design. From this work, it would be possible to identify a pathway for future AI systems to operate not only on language tokens but on layered rhetorical reasoning structures, bridging linguistic, pedagogical, academic, and computational research
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How AI Fails: An Interactive Pedagogical Tool for Demonstrating Dialectal Bias in Automated Toxicity Models</title>
<link>https://arxiv.org/abs/2511.06676</link>
<guid>https://arxiv.org/abs/2511.06676</guid>
<content:encoded><![CDATA[

arXiv:2511.06676v1 Announce Type: new 
Abstract: Now that AI-driven moderation has become pervasive in everyday life, we often hear claims that "the AI is biased". While this is often said jokingly, the light-hearted remark reflects a deeper concern. How can we be certain that an online post flagged as "inappropriate" was not simply the victim of a biased algorithm? This paper investigates this problem using a dual approach. First, I conduct a quantitative benchmark of a widely used toxicity model (unitary/toxic-bert) to measure performance disparity between text in African-American English (AAE) and Standard American English (SAE). The benchmark reveals a clear, systematic bias: on average, the model scores AAE text as 1.8 times more toxic and 8.8 times higher for "identity hate". Second, I introduce an interactive pedagogical tool that makes these abstract biases tangible. The tool's core mechanic, a user-controlled "sensitivity threshold," demonstrates that the biased score itself is not the only harm; instead, the more-concerning harm is the human-set, seemingly neutral policy that ultimately operationalises discrimination. This work provides both statistical evidence of disparate impact and a public-facing tool designed to foster critical AI literacy.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steering LLMs toward Korean Local Speech: Iterative Refinement Framework for Faithful Dialect Translation</title>
<link>https://arxiv.org/abs/2511.06680</link>
<guid>https://arxiv.org/abs/2511.06680</guid>
<content:encoded><![CDATA[

arXiv:2511.06680v1 Announce Type: new 
Abstract: Standard-to-dialect machine translation remains challenging due to a persistent dialect gap in large language models and evaluation distortions inherent in n-gram metrics, which favor source copying over authentic dialect translation. In this paper, we propose the dialect refinement (DIA-REFINE) framework, which guides LLMs toward faithful target dialect outputs through an iterative loop of translation, verification, and feedback using external dialect classifiers. To address the limitations of n-gram-based metrics, we introduce the dialect fidelity score (DFS) to quantify linguistic shift and the target dialect ratio (TDR) to measure the success of dialect translation. Experiments on Korean dialects across zero-shot and in-context learning baselines demonstrate that DIA-REFINE consistently enhances dialect fidelity. The proposed metrics distinguish between False Success cases, where high n-gram scores obscure failures in dialectal translation, and True Attempt cases, where genuine attempts at dialectal translation yield low n-gram scores. We also observed that models exhibit varying degrees of responsiveness to the framework, and that integrating in-context examples further improves the translation of dialectal expressions. Our work establishes a robust framework for goal-directed, inclusive dialect translation, providing both rigorous evaluation and critical insights into model performance.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Textual Self-attention Network: Test-Time Preference Optimization through Textual Gradient-based Attention</title>
<link>https://arxiv.org/abs/2511.06682</link>
<guid>https://arxiv.org/abs/2511.06682</guid>
<content:encoded><![CDATA[

arXiv:2511.06682v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization capabilities, but aligning their outputs with human preferences typically requires expensive supervised fine-tuning. Recent test-time methods leverage textual feedback to overcome this, but they often critique and revise a single candidate response, lacking a principled mechanism to systematically analyze, weigh, and synthesize the strengths of multiple promising candidates. Such a mechanism is crucial because different responses may excel in distinct aspects (e.g., clarity, factual accuracy, or tone), and combining their best elements may produce a far superior outcome. This paper proposes the Textual Self-Attention Network (TSAN), a new paradigm for test-time preference optimization that requires no parameter updates. TSAN emulates self-attention entirely in natural language to overcome this gap: it analyzes multiple candidates by formatting them into textual keys and values, weighs their relevance using an LLM-based attention module, and synthesizes their strengths into a new, preference-aligned response under the guidance of the learned textual attention. This entire process operates in a textual gradient space, enabling iterative and interpretable optimization. Empirical evaluations demonstrate that with just three test-time iterations on a base SFT model, TSAN outperforms supervised models like Llama-3.1-70B-Instruct and surpasses the current state-of-the-art test-time alignment method by effectively leveraging multiple candidate solutions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sentiment Analysis On YouTube Comments Using Machine Learning Techniques Based On Video Games Content</title>
<link>https://arxiv.org/abs/2511.06708</link>
<guid>https://arxiv.org/abs/2511.06708</guid>
<content:encoded><![CDATA[

arXiv:2511.06708v1 Announce Type: new 
Abstract: The rapid evolution of the gaming industry, driven by technological advancements and a burgeoning community, necessitates a deeper understanding of user sentiments, especially as expressed on popular social media platforms like YouTube. This study presents a sentiment analysis on video games based on YouTube comments, aiming to understand user sentiments within the gaming community. Utilizing YouTube API, comments related to various video games were collected and analyzed using the TextBlob sentiment analysis tool. The pre-processed data underwent classification using machine learning algorithms, including Na\"ive Bayes, Logistic Regression, and Support Vector Machine (SVM). Among these, SVM demonstrated superior performance, achieving the highest classification accuracy across different datasets. The analysis spanned multiple popular gaming videos, revealing trends and insights into user preferences and critiques. The findings underscore the importance of advanced sentiment analysis in capturing the nuanced emotions expressed in user comments, providing valuable feedback for game developers to enhance game design and user experience. Future research will focus on integrating more sophisticated natural language processing techniques and exploring additional data sources to further refine sentiment analysis in the gaming domain.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Retrieval-Augmented Generation for Medicine: A Large-Scale, Systematic Expert Evaluation and Practical Insights</title>
<link>https://arxiv.org/abs/2511.06738</link>
<guid>https://arxiv.org/abs/2511.06738</guid>
<content:encoded><![CDATA[

arXiv:2511.06738v1 Announce Type: new 
Abstract: Large language models (LLMs) are transforming the landscape of medicine, yet two fundamental challenges persist: keeping up with rapidly evolving medical knowledge and providing verifiable, evidence-grounded reasoning. Retrieval-augmented generation (RAG) has been widely adopted to address these limitations by supplementing model outputs with retrieved evidence. However, whether RAG reliably achieves these goals remains unclear. Here, we present the most comprehensive expert evaluation of RAG in medicine to date. Eighteen medical experts contributed a total of 80,502 annotations, assessing 800 model outputs generated by GPT-4o and Llama-3.1-8B across 200 real-world patient and USMLE-style queries. We systematically decomposed the RAG pipeline into three components: (i) evidence retrieval (relevance of retrieved passages), (ii) evidence selection (accuracy of evidence usage), and (iii) response generation (factuality and completeness of outputs). Contrary to expectation, standard RAG often degraded performance: only 22% of top-16 passages were relevant, evidence selection remained weak (precision 41-43%, recall 27-49%), and factuality and completeness dropped by up to 6% and 5%, respectively, compared with non-RAG variants. Retrieval and evidence selection remain key failure points for the model, contributing to the overall performance drop. We further show that simple yet effective strategies, including evidence filtering and query reformulation, substantially mitigate these issues, improving performance on MedMCQA and MedXpertQA by up to 12% and 8.2%, respectively. These findings call for re-examining RAG's role in medicine and highlight the importance of stage-aware evaluation and deliberate system design for reliable medical LLM applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sensitivity of Small Language Models to Fine-tuning Data Contamination</title>
<link>https://arxiv.org/abs/2511.06763</link>
<guid>https://arxiv.org/abs/2511.06763</guid>
<content:encoded><![CDATA[

arXiv:2511.06763v1 Announce Type: new 
Abstract: Small Language Models (SLMs) are increasingly being deployed in resource-constrained environments, yet their behavioral robustness to data contamination during instruction tuning remains poorly understood. We systematically investigate the contamination sensitivity of 23 SLMs (270M to 4B parameters) across multiple model families by measuring susceptibility to syntactic and semantic transformation types during instruction tuning: syntactic transformations (character and word reversal) and semantic transformations (irrelevant and counterfactual responses), each applied at contamination levels of 25\%, 50\%, 75\%, and 100\%. Our results reveal fundamental asymmetries in vulnerability patterns: syntactic transformations cause catastrophic performance degradation, with character reversal producing near-complete failure across all models regardless of size or family, while semantic transformations demonstrate distinct threshold behaviors and greater resilience in core linguistic capabilities. Critically, we discover a ``\textit{capability curse}" where larger, more capable models become more susceptible to learning semantic corruptions, effectively following harmful instructions more readily, while our analysis of base versus instruction-tuned variants reveals that alignment provides inconsistent robustness benefits, sometimes even reducing resilience. Our work establishes three core contributions: (1) empirical evidence of SLMs' disproportionate vulnerability to syntactic pattern contamination, (2) identification of asymmetric sensitivity patterns between syntactic and semantic transformations, and (3) systematic evaluation protocols for contamination robustness assessment. These findings have immediate deployment implications, suggesting that current robustness assumptions may not hold for smaller models and highlighting the need for contamination-aware training protocols.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAFENLIDB: A Privacy-Preserving Safety Alignment Framework for LLM-based Natural Language Database Interfaces</title>
<link>https://arxiv.org/abs/2511.06778</link>
<guid>https://arxiv.org/abs/2511.06778</guid>
<content:encoded><![CDATA[

arXiv:2511.06778v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) has driven significant progress in Natural Language Interface to Database (NLIDB). However, the widespread adoption of LLMs has raised critical privacy and security concerns. During interactions, LLMs may unintentionally expose confidential database contents or be manipulated by attackers to exfiltrate data through seemingly benign queries. While current efforts typically rely on rule-based heuristics or LLM agents to mitigate this leakage risk, these methods still struggle with complex inference-based attacks, suffer from high false positive rates, and often compromise the reliability of SQL queries. To address these challenges, we propose \textsc{SafeNlidb}, a novel privacy-security alignment framework for LLM-based NLIDB. The framework features an automated pipeline that generates hybrid chain-of-thought interaction data from scratch, seamlessly combining implicit security reasoning with SQL generation. Additionally, we introduce reasoning warm-up and alternating preference optimization to overcome the multi-preference oscillations of Direct Preference Optimization (DPO), enabling LLMs to produce security-aware SQL through fine-grained reasoning without the need for human-annotated preference data. Extensive experiments demonstrate that our method outperforms both larger-scale LLMs and ideal-setting baselines, achieving significant security improvements while preserving high utility.WARNING: This work may contain content that is offensive and harmful!
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Focus: Focal Attention for Selective and Scalable Transformers</title>
<link>https://arxiv.org/abs/2511.06818</link>
<guid>https://arxiv.org/abs/2511.06818</guid>
<content:encoded><![CDATA[

arXiv:2511.06818v1 Announce Type: new 
Abstract: Attention is a core component of transformer architecture, whether encoder-only, decoder-only, or encoder-decoder model. However, the standard softmax attention often produces noisy probability distribution, which can impair effective feature selection at every layer of these models, particularly for long contexts. We propose Focal Attention, a simple yet effective modification that sharpens the attention distribution by controlling the softmax temperature, either as a fixed hyperparameter or as a learnable parameter during training. This sharpening enables the model to concentrate on the most relevant tokens while suppressing irrelevant ones. Empirically, Focal Attention scales more favorably than standard transformer with respect to model size, training data, and context length. Across diverse benchmarks, it achieves the same accuracy with up to 42% fewer parameters or 33% less training data. On long-context tasks, it delivers substantial relative improvements ranging from 17% to 82%, demonstrating its effectiveness in real world applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Plain Demos: A Demo-centric Anchoring Paradigm for In-Context Learning in Alzheimer's Disease Detection</title>
<link>https://arxiv.org/abs/2511.06826</link>
<guid>https://arxiv.org/abs/2511.06826</guid>
<content:encoded><![CDATA[

arXiv:2511.06826v1 Announce Type: new 
Abstract: Detecting Alzheimer's disease (AD) from narrative transcripts challenges large language models (LLMs): pre-training rarely covers this out-of-distribution task, and all transcript demos describe the same scene, producing highly homogeneous contexts. These factors cripple both the model's built-in task knowledge (\textbf{task cognition}) and its ability to surface subtle, class-discriminative cues (\textbf{contextual perception}). Because cognition is fixed after pre-training, improving in-context learning (ICL) for AD detection hinges on enriching perception through better demonstration (demo) sets. We demonstrate that standard ICL quickly saturates, its demos lack diversity (context width) and fail to convey fine-grained signals (context depth), and that recent task vector (TV) approaches improve broad task adaptation by injecting TV into the LLMs' hidden states (HSs), they are ill-suited for AD detection due to the mismatch of injection granularity, strength and position. To address these bottlenecks, we introduce \textbf{DA4ICL}, a demo-centric anchoring framework that jointly expands context width via \emph{\textbf{Diverse and Contrastive Retrieval}} (DCR) and deepens each demo's signal via \emph{\textbf{Projected Vector Anchoring}} (PVA) at every Transformer layer. Across three AD benchmarks, DA4ICL achieves large, stable gains over both ICL and TV baselines, charting a new paradigm for fine-grained, OOD and low-resource LLM adaptation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLiFT-ASR: A Cross-Lingual Fine-Tuning Framework for Low-Resource Taiwanese Hokkien Speech Recognition</title>
<link>https://arxiv.org/abs/2511.06860</link>
<guid>https://arxiv.org/abs/2511.06860</guid>
<content:encoded><![CDATA[

arXiv:2511.06860v1 Announce Type: new 
Abstract: Automatic speech recognition (ASR) for low-resource languages such as Taiwanese Hokkien is difficult due to the scarcity of annotated data. However, direct fine-tuning on Han-character transcriptions often fails to capture detailed phonetic and tonal cues, while training only on romanization lacks lexical and syntactic coverage. In addition, prior studies have rarely explored staged strategies that integrate both annotation types. To address this gap, we present CLiFT-ASR, a cross-lingual fine-tuning framework that builds on Mandarin HuBERT models and progressively adapts them to Taiwanese Hokkien. The framework employs a two-stage process in which it first learns acoustic and tonal representations from phonetic Tai-lo annotations and then captures vocabulary and syntax from Han-character transcriptions. This progressive adaptation enables effective alignment between speech sounds and orthographic structures. Experiments on the TAT-MOE corpus demonstrate that CLiFT-ASR achieves a 24.88\% relative reduction in character error rate (CER) compared with strong baselines. The results indicate that CLiFT-ASR provides an effective and parameter-efficient solution for Taiwanese Hokkien ASR and that it has potential to benefit other low-resource language scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inclusion of Role into Named Entity Recognition and Ranking</title>
<link>https://arxiv.org/abs/2511.06886</link>
<guid>https://arxiv.org/abs/2511.06886</guid>
<content:encoded><![CDATA[

arXiv:2511.06886v1 Announce Type: new 
Abstract: Most of the Natural Language Processing sys- tems are involved in entity-based processing for several tasks like Information Extraction, Question-Answering, Text-Summarization and so on. A new challenge comes when entities play roles according to their act or attributes in certain context. Entity Role Detection is the task of assigning such roles to the entities. Usu- ally real-world entities are of types: person, lo- cation and organization etc. Roles could be con- sidered as domain-dependent subtypes of these types. In the cases, where retrieving a subset of entities based on their roles is needed, poses the problem of defining the role and entities having those roles. This paper presents the study of study of solving Entity Role Detection prob- lem by modeling it as Named Entity Recogni- tion (NER) and Entity Retrieval/Ranking task. In NER, these roles could be considered as mutually exclusive classes and standard NER methods like sequence tagging could be used. For Entity Retrieval, Roles could be formulated as Query and entities as Collection on which the query needs to be executed. The aspect of Entity Retrieval task, which is different than document retrieval task is that the entities and roles against which they need to be retrieved are indirectly described. We have formulated au- tomated ways of learning representative words and phrases and building representations of roles and entities using them. We have also explored different contexts like sentence and document. Since the roles depend upon con- text, so it is not always possible to have large domain-specific dataset or knowledge bases for learning purposes, so we have tried to exploit the information from small dataset in domain- agnostic way.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity and Adversarial Safety of LLMs as Simulated Teachers</title>
<link>https://arxiv.org/abs/2511.06890</link>
<guid>https://arxiv.org/abs/2511.06890</guid>
<content:encoded><![CDATA[

arXiv:2511.06890v1 Announce Type: new 
Abstract: Large Language Models for Simulating Professions (SP-LLMs), particularly as teachers, are pivotal for personalized education. However, ensuring their professional competence and ethical safety is a critical challenge, as existing benchmarks fail to measure role-playing fidelity or address the unique teaching harms inherent in educational scenarios. To address this, we propose EduGuardBench, a dual-component benchmark. It assesses professional fidelity using a Role-playing Fidelity Score (RFS) while diagnosing harms specific to the teaching profession. It also probes safety vulnerabilities using persona-based adversarial prompts targeting both general harms and, particularly, academic misconduct, evaluated with metrics including Attack Success Rate (ASR) and a three-tier Refusal Quality assessment. Our extensive experiments on 14 leading models reveal a stark polarization in performance. While reasoning-oriented models generally show superior fidelity, incompetence remains the dominant failure mode across most models. The adversarial tests uncovered a counterintuitive scaling paradox, where mid-sized models can be the most vulnerable, challenging monotonic safety assumptions. Critically, we identified a powerful Educational Transformation Effect: the safest models excel at converting harmful requests into teachable moments by providing ideal Educational Refusals. This capacity is strongly negatively correlated with ASR, revealing a new dimension of advanced AI safety. EduGuardBench thus provides a reproducible framework that moves beyond siloed knowledge tests toward a holistic assessment of professional, ethical, and pedagogical alignment, uncovering complex dynamics essential for deploying trustworthy AI in education. See https://github.com/YL1N/EduGuardBench for Materials.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RPTS: Tree-Structured Reasoning Process Scoring for Faithful Multimodal Evaluation</title>
<link>https://arxiv.org/abs/2511.06899</link>
<guid>https://arxiv.org/abs/2511.06899</guid>
<content:encoded><![CDATA[

arXiv:2511.06899v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) excel in multimodal reasoning and have shown impressive performance on various multimodal benchmarks. However, most of these benchmarks evaluate models primarily through multiple-choice or short-answer formats, which do not take the reasoning process into account. Although some benchmarks assess the reasoning process, their methods are often overly simplistic and only examine reasoning when answers are incorrect. This approach overlooks scenarios where flawed reasoning leads to correct answers. In addition, these benchmarks do not consider the impact of intermodal relationships on reasoning. To address this issue, we propose the Reasoning Process Tree Score (RPTS), a tree structure-based metric to assess reasoning processes. Specifically, we organize the reasoning steps into a reasoning tree and leverage its hierarchical information to assign weighted faithfulness scores to each reasoning step. By dynamically adjusting these weights, RPTS not only evaluates the overall correctness of the reasoning, but also pinpoints where the model fails in the reasoning. To validate RPTS in real-world multimodal scenarios, we construct a new benchmark, RPTS-Eval, comprising 374 images and 390 reasoning instances. Each instance includes reliable visual-textual clues that serve as leaf nodes of the reasoning tree. Furthermore, we define three types of intermodal relationships to investigate how intermodal interactions influence the reasoning process. We evaluated representative LVLMs (e.g., GPT4o, Llava-Next), uncovering their limitations in multimodal reasoning and highlighting the differences between open-source and closed-source commercial LVLMs. We believe that this benchmark will contribute to the advancement of research in the field of multimodal reasoning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HLPD: Aligning LLMs to Human Language Preference for Machine-Revised Text Detection</title>
<link>https://arxiv.org/abs/2511.06942</link>
<guid>https://arxiv.org/abs/2511.06942</guid>
<content:encoded><![CDATA[

arXiv:2511.06942v1 Announce Type: new 
Abstract: To prevent misinformation and social issues arising from trustworthy-looking content generated by LLMs, it is crucial to develop efficient and reliable methods for identifying the source of texts. Previous approaches have demonstrated exceptional performance in detecting texts fully generated by LLMs. However, these methods struggle when confronting more advanced LLM output or text with adversarial multi-task machine revision, especially in the black-box setting, where the generating model is unknown. To address this challenge, grounded in the hypothesis that human writing possesses distinctive stylistic patterns, we propose Human Language Preference Detection (HLPD). HLPD employs a reward-based alignment process, Human Language Preference Optimization (HLPO), to shift the scoring model's token distribution toward human-like writing, making the model more sensitive to human writing, therefore enhancing the identification of machine-revised text. We test HLPD in an adversarial multi-task evaluation framework that leverages a five-dimensional prompt generator and multiple advanced LLMs to create diverse revision scenarios. When detecting texts revised by GPT-series models, HLPD achieves a 15.11% relative improvement in AUROC over ImBD, surpassing Fast-DetectGPT by 45.56%. When evaluated on texts generated by advanced LLMs, HLPD achieves the highest average AUROC, exceeding ImBD by 5.53% and Fast-DetectGPT by 34.14%. Code will be made available at https://github.com/dfq2021/HLPD.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCOPE: Intrinsic Semantic Space Control for Mitigating Copyright Infringement in LLMs</title>
<link>https://arxiv.org/abs/2511.07001</link>
<guid>https://arxiv.org/abs/2511.07001</guid>
<content:encoded><![CDATA[

arXiv:2511.07001v1 Announce Type: new 
Abstract: Large language models sometimes inadvertently reproduce passages that are copyrighted, exposing downstream applications to legal risk. Most existing studies for inference-time defences focus on surface-level token matching and rely on external blocklists or filters, which add deployment complexity and may overlook semantically paraphrased leakage. In this work, we reframe copyright infringement mitigation as intrinsic semantic-space control and introduce SCOPE, an inference-time method that requires no parameter updates or auxiliary filters. Specifically, the sparse autoencoder (SAE) projects hidden states into a high-dimensional, near-monosemantic space; benefiting from this representation, we identify a copyright-sensitive subspace and clamp its activations during decoding. Experiments on widely recognized benchmarks show that SCOPE mitigates copyright infringement without degrading general utility. Further interpretability analyses confirm that the isolated subspace captures high-level semantics.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Circuit Interpretation via Probe Prompting</title>
<link>https://arxiv.org/abs/2511.07002</link>
<guid>https://arxiv.org/abs/2511.07002</guid>
<content:encoded><![CDATA[

arXiv:2511.07002v1 Announce Type: new 
Abstract: Mechanistic interpretability aims to understand neural networks by identifying which learned features mediate specific behaviors. Attribution graphs reveal these feature pathways, but interpreting them requires extensive manual analysis -- a single prompt can take approximately 2 hours for an experienced circuit tracer. We present probe prompting, an automated pipeline that transforms attribution graphs into compact, interpretable subgraphs built from concept-aligned supernodes. Starting from a seed prompt and target logit, we select high-influence features, generate concept-targeted yet context-varying probes, and group features by cross-prompt activation signatures into Semantic, Relationship, and Say-X categories using transparent decision rules.
  Across five prompts including classic "capitals" circuits, probe-prompted subgraphs preserve high explanatory coverage while compressing complexity (Completeness 0.83, mean across circuits; Replacement 0.54). Compared to geometric clustering baselines, concept-aligned groups exhibit higher behavioral coherence: 2.3x higher peak-token consistency (0.425 vs 0.183) and 5.8x higher activation-pattern similarity (0.762 vs 0.130), despite lower geometric compactness. Entity-swap tests reveal a layerwise hierarchy: early-layer features transfer robustly (64% transfer rate, mean layer 6.3), while late-layer Say-X features specialize for output promotion (mean layer 16.4), supporting a backbone-and-specialization view of transformer computation.
  We release code (https://github.com/peppinob-ol/attribution-graph-probing), an interactive demo (https://huggingface.co/spaces/Peppinob/attribution-graph-probing), and minimal artifacts enabling immediate reproduction and community adoption.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs</title>
<link>https://arxiv.org/abs/2511.07003</link>
<guid>https://arxiv.org/abs/2511.07003</guid>
<content:encoded><![CDATA[

arXiv:2511.07003v1 Announce Type: new 
Abstract: Large language models have significantly advanced Multilingual Machine Translation (MMT), yet the broad language coverage, consistent translation quality, and English-centric bias remain open challenges. To address these challenges, we introduce \textbf{LMT}, a suite of \textbf{L}arge-scale \textbf{M}ultilingual \textbf{T}ranslation models centered on both Chinese and English, covering 60 languages and 234 translation directions. During development, we identify a previously overlooked phenomenon of \textbf{directional degeneration}, where symmetric multi-way fine-tuning data overemphasize reverse directions (X $\to$ En/Zh), leading to excessive many-to-one mappings and degraded translation quality. We propose \textbf{Strategic Downsampling}, a simple yet effective method to mitigate this degeneration. In addition, we design \textbf{Parallel Multilingual Prompting (PMP)}, which leverages typologically related auxiliary languages to enhance cross-lingual transfer. Through rigorous data curation and refined adaptation strategies, LMT achieves SOTA performance among models of comparable language coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B and NLLB-54B models by a substantial margin. We release LMT in four sizes (0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for inclusive, scalable, and high-quality MMT \footnote{\href{https://github.com/NiuTrans/LMT}{https://github.com/NiuTrans/LMT}}.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Picture is Worth a Thousand (Correct) Captions: A Vision-Guided Judge-Corrector System for Multimodal Machine Translation</title>
<link>https://arxiv.org/abs/2511.07010</link>
<guid>https://arxiv.org/abs/2511.07010</guid>
<content:encoded><![CDATA[

arXiv:2511.07010v1 Announce Type: new 
Abstract: In this paper, we describe our system under the team name BLEU Monday for the English-to-Indic Multimodal Translation Task at WAT 2025. We participate in the text-only translation tasks for English-Hindi, English-Bengali, English-Malayalam, and English-Odia language pairs. We present a two-stage approach that addresses quality issues in the training data through automated error detection and correction, followed by parameter-efficient model fine-tuning.
  Our methodology introduces a vision-augmented judge-corrector pipeline that leverages multimodal language models to systematically identify and correct translation errors in the training data. The judge component classifies translations into three categories: correct, visually ambiguous (requiring image context), or mistranslated (poor translation quality). Identified errors are routed to specialized correctors: GPT-4o-mini regenerates captions requiring visual disambiguation, while IndicTrans2 retranslates cases with pure translation quality issues. This automated pipeline processes 28,928 training examples across four languages, correcting an average of 17.1% of captions per language.
  We then apply Low-Rank Adaptation (LoRA) to fine-tune the IndicTrans2 en-indic 200M distilled model on both original and corrected datasets. Training on corrected data yields consistent improvements, with BLEU score gains of +1.30 for English-Bengali on the evaluation set (42.00 -> 43.30) and +0.70 on the challenge set (44.90 -> 45.60), +0.60 for English-Odia on the evaluation set (41.00 -> 41.60), and +0.10 for English-Hindi on the challenge set (53.90 -> 54.00).
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multilingual Lexical Feature Analysis of Spoken Language for Predicting Major Depression Symptom Severity</title>
<link>https://arxiv.org/abs/2511.07011</link>
<guid>https://arxiv.org/abs/2511.07011</guid>
<content:encoded><![CDATA[

arXiv:2511.07011v1 Announce Type: new 
Abstract: Background: Captured between clinical appointments using mobile devices, spoken language has potential for objective, more regular assessment of symptom severity and earlier detection of relapse in major depressive disorder. However, research to date has largely been in non-clinical cross-sectional samples of written language using complex machine learning (ML) approaches with limited interpretability.
  Methods: We describe an initial exploratory analysis of longitudinal speech data and PHQ-8 assessments from 5,836 recordings of 586 participants in the UK, Netherlands, and Spain, collected in the RADAR-MDD study. We sought to identify interpretable lexical features associated with MDD symptom severity with linear mixed-effects modelling. Interpretable features and high-dimensional vector embeddings were also used to test the prediction performance of four regressor ML models.
  Results: In English data, MDD symptom severity was associated with 7 features including lexical diversity measures and absolutist language. In Dutch, associations were observed with words per sentence and positive word frequency; no associations were observed in recordings collected in Spain. The predictive power of lexical features and vector embeddings was near chance level across all languages.
  Limitations: Smaller samples in non-English speech and methodological choices, such as the elicitation prompt, may have also limited the effect sizes observable. A lack of NLP tools in languages other than English restricted our feature choice.
  Conclusion: To understand the value of lexical markers in clinical research and practice, further research is needed in larger samples across several languages using improved protocols, and ML models that account for within- and between-individual variations in language.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks</title>
<link>https://arxiv.org/abs/2511.07025</link>
<guid>https://arxiv.org/abs/2511.07025</guid>
<content:encoded><![CDATA[

arXiv:2511.07025v1 Announce Type: new 
Abstract: We introduce llama-embed-nemotron-8b, an open-weights text embedding model that achieves state-of-the-art performance on the Multilingual Massive Text Embedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent models show strong performance, their training data or methodologies are often not fully disclosed. We aim to address this by developing a fully open-source model, publicly releasing its weights and detailed ablation studies, and planning to share the curated training datasets. Our model demonstrates superior performance across all major embedding tasks -- including retrieval, classification and semantic textual similarity (STS) -- and excels in challenging multilingual scenarios, such as low-resource languages and cross-lingual setups. This state-of-the-art performance is driven by a novel data mix of 16.1 million query-document pairs, split between 7.7 million samples from public datasets and 8.4 million synthetically generated examples from various open-weight LLMs. One of our key contributions is a detailed ablation study analyzing core design choices, including a comparison of contrastive loss implementations, an evaluation of synthetic data generation (SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b is an instruction-aware model, supporting user-defined instructions to enhance performance for specific use-cases. This combination of top-tier performance, broad applicability, and user-driven flexibility enables it to serve as a universal text embedding solution.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating LLMs for Anxiety, Depression, and Stress Detection Evaluating Large Language Models for Anxiety, Depression, and Stress Detection: Insights into Prompting Strategies and Synthetic Data</title>
<link>https://arxiv.org/abs/2511.07044</link>
<guid>https://arxiv.org/abs/2511.07044</guid>
<content:encoded><![CDATA[

arXiv:2511.07044v1 Announce Type: new 
Abstract: Mental health disorders affect over one-fifth of adults globally, yet detecting such conditions from text remains challenging due to the subtle and varied nature of symptom expression. This study evaluates multiple approaches for mental health detection, comparing Large Language Models (LLMs) such as Llama and GPT with classical machine learning and transformer-based architectures including BERT, XLNet, and Distil-RoBERTa. Using the DAIC-WOZ dataset of clinical interviews, we fine-tuned models for anxiety, depression, and stress classification and applied synthetic data generation to mitigate class imbalance. Results show that Distil-RoBERTa achieved the highest F1 score (0.883) for GAD-2, while XLNet outperformed others on PHQ tasks (F1 up to 0.891). For stress detection, a zero-shot synthetic approach (SD+Zero-Shot-Basic) reached an F1 of 0.884 and ROC AUC of 0.886. Findings demonstrate the effectiveness of transformer-based models and highlight the value of synthetic data in improving recall and generalization. However, careful calibration is required to prevent precision loss. Overall, this work emphasizes the potential of combining advanced language models and data augmentation to enhance automated mental health assessment from text.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Sufficient is not Enough: Utilizing the Rashomon Effect for Complete Evidence Extraction</title>
<link>https://arxiv.org/abs/2511.07055</link>
<guid>https://arxiv.org/abs/2511.07055</guid>
<content:encoded><![CDATA[

arXiv:2511.07055v1 Announce Type: new 
Abstract: Feature attribution methods typically provide minimal sufficient evidence justifying a model decision. However, in many applications this is inadequate. For compliance and cataloging, the full set of contributing features must be identified - complete evidence. We perform a case study on a medical dataset which contains human-annotated complete evidence. We show that individual models typically recover only subsets of complete evidence and that aggregating evidence from several models improves evidence recall from $\sim$0.60 (single best model) to $\sim$0.86 (ensemble). We analyze the recall-precision trade-off, the role of training with evidence, dynamic ensembles with certainty thresholds, and discuss implications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Attention with Human Rationales for Self-Explaining Hate Speech Detection</title>
<link>https://arxiv.org/abs/2511.07065</link>
<guid>https://arxiv.org/abs/2511.07065</guid>
<content:encoded><![CDATA[

arXiv:2511.07065v1 Announce Type: new 
Abstract: The opaque nature of deep learning models presents significant challenges for the ethical deployment of hate speech detection systems. To address this limitation, we introduce Supervised Rational Attention (SRA), a framework that explicitly aligns model attention with human rationales, improving both interpretability and fairness in hate speech classification. SRA integrates a supervised attention mechanism into transformer-based classifiers, optimizing a joint objective that combines standard classification loss with an alignment loss term that minimizes the discrepancy between attention weights and human-annotated rationales. We evaluated SRA on hate speech benchmarks in English (HateXplain) and Portuguese (HateBRXplain) with rationale annotations. Empirically, SRA achieves 2.4x better explainability compared to current baselines, and produces token-level explanations that are more faithful and human-aligned. In terms of fairness, SRA achieves competitive fairness across all measures, with second-best performance in detecting toxic posts targeting identity groups, while maintaining comparable results on other metrics. These findings demonstrate that incorporating human rationales into attention mechanisms can enhance interpretability and faithfulness without compromising fairness.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Importance-Aware Data Selection for Efficient LLM Instruction Tuning</title>
<link>https://arxiv.org/abs/2511.07074</link>
<guid>https://arxiv.org/abs/2511.07074</guid>
<content:encoded><![CDATA[

arXiv:2511.07074v1 Announce Type: new 
Abstract: Instruction tuning plays a critical role in enhancing the performance and efficiency of Large Language Models (LLMs). Its success depends not only on the quality of the instruction data but also on the inherent capabilities of the LLM itself. Some studies suggest that even a small amount of high-quality data can achieve instruction fine-tuning results that are on par with, or even exceed, those from using a full-scale dataset. However, rather than focusing solely on calculating data quality scores to evaluate instruction data, there is a growing need to select high-quality data that maximally enhances the performance of instruction tuning for a given LLM. In this paper, we propose the Model Instruction Weakness Value (MIWV) as a novel metric to quantify the importance of instruction data in enhancing model's capabilities. The MIWV metric is derived from the discrepancies in the model's responses when using In-Context Learning (ICL), helping identify the most beneficial data for enhancing instruction tuning performance. Our experimental results demonstrate that selecting only the top 1\% of data based on MIWV can outperform training on the full dataset. Furthermore, this approach extends beyond existing research that focuses on data quality scoring for data selection, offering strong empirical evidence supporting the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmoBang: Detecting Emotion From Bengali Texts</title>
<link>https://arxiv.org/abs/2511.07077</link>
<guid>https://arxiv.org/abs/2511.07077</guid>
<content:encoded><![CDATA[

arXiv:2511.07077v1 Announce Type: new 
Abstract: Emotion detection from text seeks to identify an individual's emotional or mental state - positive, negative, or neutral - based on linguistic cues. While significant progress has been made for English and other high-resource languages, Bengali remains underexplored despite being the world's fourth most spoken language. The lack of large, standardized datasets classifies Bengali as a low-resource language for emotion detection. Existing studies mainly employ classical machine learning models with traditional feature engineering, yielding limited performance. In this paper, we introduce a new Bengali emotion dataset annotated across eight emotion categories and propose two models for automatic emotion detection: (i) a hybrid Convolutional Recurrent Neural Network (CRNN) model (EmoBangHybrid) and (ii) an AdaBoost-Bidirectional Encoder Representations from Transformers (BERT) ensemble model (EmoBangEnsemble). Additionally, we evaluate six baseline models with five feature engineering techniques and assess zero-shot and few-shot large language models (LLMs) on the dataset. To the best of our knowledge, this is the first comprehensive benchmark for Bengali emotion detection. Experimental results show that EmoBangH and EmoBangE achieve accuracies of 92.86% and 93.69%, respectively, outperforming existing methods and establishing strong baselines for future research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wasm: A Pipeline for Constructing Structured Arabic Interleaved Multimodal Corpora</title>
<link>https://arxiv.org/abs/2511.07080</link>
<guid>https://arxiv.org/abs/2511.07080</guid>
<content:encoded><![CDATA[

arXiv:2511.07080v1 Announce Type: new 
Abstract: The performance of large language models (LLMs) and large multimodal models (LMMs) depends heavily on the quality and scale of their pre-training datasets. Recent research shows that large multimodal models trained on natural documents where images and text are interleaved outperform those trained only on image-text pairs across a wide range of benchmarks, leveraging advanced pre- trained models to enforce semantic alignment, image-sequence consistency, and textual coherence. For Arabic, however, the lack of high-quality multimodal datasets that preserve document structure has limited progress. In this paper, we present our pipeline Wasm for processing the Common Crawl dataset to create a new Arabic multimodal dataset that uniquely provides markdown output. Unlike existing Arabic corpora that focus solely on text extraction, our approach preserves the structural integrity of web content while maintaining flexibility for both text-only and multimodal pre-training scenarios. We provide a comprehensive comparative analysis of our data processing pipeline against those used for major existing datasets, highlighting the convergences in filtering strategies and justifying our specific design choices. To support future research, we publicly release a representative dataset dump along with the multimodal processing pipeline for Arabic.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More Agents Helps but Adversarial Robustness Gap Persists</title>
<link>https://arxiv.org/abs/2511.07112</link>
<guid>https://arxiv.org/abs/2511.07112</guid>
<content:encoded><![CDATA[

arXiv:2511.07112v1 Announce Type: new 
Abstract: When LLM agents work together, they seem to be more powerful than a single LLM in mathematical question answering. However, are they also more robust to adversarial inputs? We investigate this question using adversarially perturbed math questions. These perturbations include punctuation noise with three intensities (10, 30, and 50 percent), plus real-world and human-like typos (WikiTypo, R2ATA). Using a unified sampling-and-voting framework (Agent Forest), we evaluate six open-source models (Qwen3-4B/14B, Llama3.1-8B, Mistral-7B, Gemma3-4B/12B) across four benchmarks (GSM8K, MATH, MMLU-Math, MultiArith), with various numbers of agents n from one to 25 (1, 2, 5, 10, 15, 20, 25). Our findings show that (1) Noise type matters: punctuation noise harm scales with its severity, and the human typos remain the dominant bottleneck, yielding the largest gaps to Clean accuracy and the highest ASR even with a large number of agents. And (2) Collaboration reliably improves accuracy as the number of agents, n, increases, with the largest gains from one to five agents and diminishing returns beyond 10 agents. However, the adversarial robustness gap persists regardless of the agent count.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Consistently, Reason Efficiently: Energy-Based Calibration for Implicit Chain-of-Thought</title>
<link>https://arxiv.org/abs/2511.07124</link>
<guid>https://arxiv.org/abs/2511.07124</guid>
<content:encoded><![CDATA[

arXiv:2511.07124v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated strong reasoning capabilities through \emph{Chain-of-Thought} (CoT) prompting, which enables step-by-step intermediate reasoning. However, explicit CoT methods rely on discrete token-level reasoning processes that are prone to error propagation and limited by vocabulary expressiveness, often resulting in rigid and inconsistent reasoning trajectories. Recent research has explored implicit or continuous reasoning in latent spaces, allowing models to perform internal reasoning before generating explicit output. Although such approaches alleviate some limitations of discrete CoT, they generally lack explicit mechanisms to enforce consistency among reasoning steps, leading to divergent reasoning paths and unstable outcomes. To address this issue, we propose EBM-CoT, an Energy-Based Chain-of-Thought Calibration framework that refines latent thought representations through an energy-based model (EBM). Our method dynamically adjusts latent reasoning trajectories toward lower-energy, high-consistency regions in the embedding space, improving both reasoning accuracy and consistency without modifying the base language model. Extensive experiments across mathematical, commonsense, and symbolic reasoning benchmarks demonstrate that the proposed framework significantly enhances the consistency and efficiency of multi-step reasoning in LLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoRA on the Go: Instance-level Dynamic LoRA Selection and Merging</title>
<link>https://arxiv.org/abs/2511.07129</link>
<guid>https://arxiv.org/abs/2511.07129</guid>
<content:encoded><![CDATA[

arXiv:2511.07129v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient approach for fine-tuning large language models.However, conventional LoRA adapters are typically trained for a single task, limiting their applicability in real-world settings where inputs may span diverse and unpredictable domains. At inference time, existing approaches combine multiple LoRAs for improving performance on diverse tasks, while usually requiring labeled data or additional task-specific training, which is expensive at scale. In this work, we introduce LoRA on the Go (LoGo), a training-free framework that dynamically selects and merges adapters at the instance level without any additional requirements. LoGo leverages signals extracted from a single forward pass through LoRA adapters, to identify the most relevant adapters and determine their contributions on-the-fly. Across 5 NLP benchmarks, 27 datasets, and 3 model families, LoGo outperforms training-based baselines on some tasks upto a margin of 3.6% while remaining competitive on other tasks and maintaining inference throughput, highlighting its effectiveness and practicality.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TCM-Eval: An Expert-Level Dynamic and Extensible Benchmark for Traditional Chinese Medicine</title>
<link>https://arxiv.org/abs/2511.07148</link>
<guid>https://arxiv.org/abs/2511.07148</guid>
<content:encoded><![CDATA[

arXiv:2511.07148v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in modern medicine, yet their application in Traditional Chinese Medicine (TCM) remains severely limited by the absence of standardized benchmarks and the scarcity of high-quality training data. To address these challenges, we introduce TCM-Eval, the first dynamic and extensible benchmark for TCM, meticulously curated from national medical licensing examinations and validated by TCM experts. Furthermore, we construct a large-scale training corpus and propose Self-Iterative Chain-of-Thought Enhancement (SI-CoTE) to autonomously enrich question-answer pairs with validated reasoning chains through rejection sampling, establishing a virtuous cycle of data and model co-evolution. Using this enriched training data, we develop ZhiMingTang (ZMT), a state-of-the-art LLM specifically designed for TCM, which significantly exceeds the passing threshold for human practitioners. To encourage future research and development, we release a public leaderboard, fostering community engagement and continuous improvement.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Categorical Emotions or Appraisals - Which Emotion Model Explains Argument Convincingness Better?</title>
<link>https://arxiv.org/abs/2511.07162</link>
<guid>https://arxiv.org/abs/2511.07162</guid>
<content:encoded><![CDATA[

arXiv:2511.07162v1 Announce Type: new 
Abstract: The convincingness of an argument does not only depend on its structure (logos), the person who makes the argument (ethos), but also on the emotion that it causes in the recipient (pathos). While the overall intensity and categorical values of emotions in arguments have received considerable attention in the research community, we argue that the emotion an argument evokes in a recipient is subjective. It depends on the recipient's goals, standards, prior knowledge, and stance. Appraisal theories lend themselves as a link between the subjective cognitive assessment of events and emotions. They have been used in event-centric emotion analysis, but their suitability for assessing argument convincingness remains unexplored. In this paper, we evaluate whether appraisal theories are suitable for emotion analysis in arguments by considering subjective cognitive evaluations of the importance and impact of an argument on its receiver. Based on the annotations in the recently published ContArgA corpus, we perform zero-shot prompting experiments to evaluate the importance of gold-annotated and predicted emotions and appraisals for the assessment of the subjective convincingness labels. We find that, while categorical emotion information does improve convincingness prediction, the improvement is more pronounced with appraisals. This work presents the first systematic comparison between emotion models for convincingness prediction, demonstrating the advantage of appraisals, providing insights for theoretical and practical applications in computational argumentation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaRec: Adaptive Recommendation with LLMs via Narrative Profiling and Dual-Channel Reasoning</title>
<link>https://arxiv.org/abs/2511.07166</link>
<guid>https://arxiv.org/abs/2511.07166</guid>
<content:encoded><![CDATA[

arXiv:2511.07166v1 Announce Type: new 
Abstract: We propose AdaRec, a few-shot in-context learning framework that leverages large language models for an adaptive personalized recommendation. AdaRec introduces narrative profiling, transforming user-item interactions into natural language representations to enable unified task handling and enhance human readability. Centered on a bivariate reasoning paradigm, AdaRec employs a dual-channel architecture that integrates horizontal behavioral alignment, discovering peer-driven patterns, with vertical causal attribution, highlighting decisive factors behind user preferences. Unlike existing LLM-based approaches, AdaRec eliminates manual feature engineering through semantic representations and supports rapid cross-task adaptation with minimal supervision. Experiments on real ecommerce datasets demonstrate that AdaRec outperforms both machine learning models and LLM-based baselines by up to eight percent in few-shot settings. In zero-shot scenarios, it achieves up to a nineteen percent improvement over expert-crafted profiling, showing effectiveness for long-tail personalization with minimal interaction data. Furthermore, lightweight fine-tuning on synthetic data generated by AdaRec matches the performance of fully fine-tuned models, highlighting its efficiency and generalization across diverse tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMODIS: A Benchmark for Context-Dependent Emoji Disambiguation in Large Language Models</title>
<link>https://arxiv.org/abs/2511.07193</link>
<guid>https://arxiv.org/abs/2511.07193</guid>
<content:encoded><![CDATA[

arXiv:2511.07193v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in real-world communication settings, yet their ability to resolve context-dependent ambiguity remains underexplored. In this work, we present EMODIS, a new benchmark for evaluating LLMs' capacity to interpret ambiguous emoji expressions under minimal but contrastive textual contexts. Each instance in EMODIS comprises an ambiguous sentence containing an emoji, two distinct disambiguating contexts that lead to divergent interpretations, and a specific question that requires contextual reasoning. We evaluate both open-source and API-based LLMs, and find that even the strongest models frequently fail to distinguish meanings when only subtle contextual cues are present. Further analysis reveals systematic biases toward dominant interpretations and limited sensitivity to pragmatic contrast. EMODIS provides a rigorous testbed for assessing contextual disambiguation, and highlights the gap in semantic reasoning between humans and LLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discourse Graph Guided Document Translation with Large Language Models</title>
<link>https://arxiv.org/abs/2511.07230</link>
<guid>https://arxiv.org/abs/2511.07230</guid>
<content:encoded><![CDATA[

arXiv:2511.07230v1 Announce Type: new 
Abstract: Adapting large language models to full document translation remains challenging due to the difficulty of capturing long-range dependencies and preserving discourse coherence throughout extended texts. While recent agentic machine translation systems mitigate context window constraints through multi-agent orchestration and persistent memory, they require substantial computational resources and are sensitive to memory retrieval strategies. We introduce TransGraph, a discourse-guided framework that explicitly models inter-chunk relationships through structured discourse graphs and selectively conditions each translation segment on relevant graph neighbourhoods rather than relying on sequential or exhaustive context. Across three document-level MT benchmarks spanning six languages and diverse domains, TransGraph consistently surpasses strong baselines in translation quality and terminology consistency while incurring significantly lower token overhead.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who Is the Story About? Protagonist Entity Recognition in News</title>
<link>https://arxiv.org/abs/2511.07296</link>
<guid>https://arxiv.org/abs/2511.07296</guid>
<content:encoded><![CDATA[

arXiv:2511.07296v1 Announce Type: new 
Abstract: News articles often reference numerous organizations, but traditional Named Entity Recognition (NER) treats all mentions equally, obscuring which entities genuinely drive the narrative. This limits downstream tasks that rely on understanding event salience, influence, or narrative focus. We introduce Protagonist Entity Recognition (PER), a task that identifies the organizations that anchor a news story and shape its main developments. To validate PER, we compare he predictions of Large Language Models (LLMs) against annotations from four expert annotators over a gold corpus, establishing both inter-annotator consistency and human-LLM agreement. Leveraging these findings, we use state-of-the-art LLMs to automatically label large-scale news collections through NER-guided prompting, generating scalable, high-quality supervision. We then evaluate whether other LLMs, given reduced context and without explicit candidate guidance, can still infer the correct protagonists. Our results demonstrate that PER is a feasible and meaningful extension to narrative-centered information extraction, and that guided LLMs can approximate human judgments of narrative importance at scale.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retriv at BLP-2025 Task 1: A Transformer Ensemble and Multi-Task Learning Approach for Bangla Hate Speech Identification</title>
<link>https://arxiv.org/abs/2511.07304</link>
<guid>https://arxiv.org/abs/2511.07304</guid>
<content:encoded><![CDATA[

arXiv:2511.07304v1 Announce Type: new 
Abstract: This paper addresses the problem of Bangla hate speech identification, a socially impactful yet linguistically challenging task. As part of the "Bangla Multi-task Hate Speech Identification" shared task at the BLP Workshop, IJCNLP-AACL 2025, our team "Retriv" participated in all three subtasks: (1A) hate type classification, (1B) target group identification, and (1C) joint detection of type, severity, and target. For subtasks 1A and 1B, we employed a soft-voting ensemble of transformer models (BanglaBERT, MuRIL, IndicBERTv2). For subtask 1C, we trained three multitask variants and aggregated their predictions through a weighted voting ensemble. Our systems achieved micro-f1 scores of 72.75% (1A) and 72.69% (1B), and a weighted micro-f1 score of 72.62% (1C). On the shared task leaderboard, these corresponded to 9th, 10th, and 7th positions, respectively. These results highlight the promise of transformer ensembles and weighted multitask frameworks for advancing Bangla hate speech detection in low-resource contexts. We made experimental scripts publicly available for the community.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ACE-ICD: Acronym Expansion As Data Augmentation For Automated ICD Coding</title>
<link>https://arxiv.org/abs/2511.07311</link>
<guid>https://arxiv.org/abs/2511.07311</guid>
<content:encoded><![CDATA[

arXiv:2511.07311v1 Announce Type: new 
Abstract: Automatic ICD coding, the task of assigning disease and procedure codes to electronic medical records, is crucial for clinical documentation and billing. While existing methods primarily enhance model understanding of code hierarchies and synonyms, they often overlook the pervasive use of medical acronyms in clinical notes, a key factor in ICD code inference. To address this gap, we propose a novel effective data augmentation technique that leverages large language models to expand medical acronyms, allowing models to be trained on their full form representations. Moreover, we incorporate consistency training to regularize predictions by enforcing agreement between the original and augmented documents. Extensive experiments on the MIMIC-III dataset demonstrate that our approach, ACE-ICD establishes new state-of-the-art performance across multiple settings, including common codes, rare codes, and full-code assignments. Our code is publicly available.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments</title>
<link>https://arxiv.org/abs/2511.07317</link>
<guid>https://arxiv.org/abs/2511.07317</guid>
<content:encoded><![CDATA[

arXiv:2511.07317v1 Announce Type: new 
Abstract: We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). RLVE enables each verifiable environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses. In contrast, static data distributions often lead to vanishing learning signals when problems are either too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a large-scale suite of 400 verifiable environments carefully developed through manual environment engineering. Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs. By comparison, continuing this LM's original RL training yields only a 0.49% average absolute gain despite using over 3x more compute. We release our code publicly.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs</title>
<link>https://arxiv.org/abs/2511.07318</link>
<guid>https://arxiv.org/abs/2511.07318</guid>
<content:encoded><![CDATA[

arXiv:2511.07318v1 Announce Type: new 
Abstract: Despite substantial advances, large language models (LLMs) continue to exhibit hallucinations, generating plausible yet incorrect responses. In this paper, we highlight a critical yet previously underexplored class of hallucinations driven by spurious correlations -- superficial but statistically prominent associations between features (e.g., surnames) and attributes (e.g., nationality) present in the training data. We demonstrate that these spurious correlations induce hallucinations that are confidently generated, immune to model scaling, evade current detection methods, and persist even after refusal fine-tuning. Through systematically controlled synthetic experiments and empirical evaluations on state-of-the-art open-source and proprietary LLMs (including GPT-5), we show that existing hallucination detection methods, such as confidence-based filtering and inner-state probing, fundamentally fail in the presence of spurious correlations. Our theoretical analysis further elucidates why these statistical biases intrinsically undermine confidence-based detection techniques. Our findings thus emphasize the urgent need for new approaches explicitly designed to address hallucinations caused by spurious correlations.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinRpt: Dataset, Evaluation System and LLM-based Multi-agent Framework for Equity Research Report Generation</title>
<link>https://arxiv.org/abs/2511.07322</link>
<guid>https://arxiv.org/abs/2511.07322</guid>
<content:encoded><![CDATA[

arXiv:2511.07322v1 Announce Type: new 
Abstract: While LLMs have shown great success in financial tasks like stock prediction and question answering, their application in fully automating Equity Research Report generation remains uncharted territory. In this paper, we formulate the Equity Research Report (ERR) Generation task for the first time. To address the data scarcity and the evaluation metrics absence, we present an open-source evaluation benchmark for ERR generation - FinRpt. We frame a Dataset Construction Pipeline that integrates 7 financial data types and produces a high-quality ERR dataset automatically, which could be used for model training and evaluation. We also introduce a comprehensive evaluation system including 11 metrics to assess the generated ERRs. Moreover, we propose a multi-agent framework specifically tailored to address this task, named FinRpt-Gen, and train several LLM-based agents on the proposed datasets using Supervised Fine-Tuning and Reinforcement Learning. Experimental results indicate the data quality and metrics effectiveness of the benchmark FinRpt and the strong performance of FinRpt-Gen, showcasing their potential to drive innovation in the ERR generation field. All code and datasets are publicly available.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selecting Auxiliary Data via Neural Tangent Kernels for Low-Resource Domains</title>
<link>https://arxiv.org/abs/2511.07380</link>
<guid>https://arxiv.org/abs/2511.07380</guid>
<content:encoded><![CDATA[

arXiv:2511.07380v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success across widespread tasks, yet their application in low-resource domains remains a significant challenge due to data scarcity and the high risk of overfitting. While in-domain data is limited, there exist vast amounts of similar general-domain data, and our initial findings reveal that they could potentially serve as auxiliary supervision for domain enhancement. This observation leads us to our central research question: \textbf{\textit{how to effectively select the most valuable auxiliary data to maximize domain-specific performance}}, particularly when traditional methods are inapplicable due to a lack of large in-domain data pools or validation sets. To address this, we propose \textbf{NTK-Selector}, a principled and efficient framework for selecting general-domain auxiliary data to enhance domain-specific performance via neural tangent kernels (NTK). Our method tackles two challenges of directly applying NTK to LLMs, theoretical assumptions and prohibitive computational cost, by empirically demonstrating a stable NTK-like behavior in LLMs during LoRA fine-tuning and proposing a Jacobian-free approximation method. Extensive experiments across four low-resource domains (medical, financial, legal, and psychological) demonstrate that NTK-Selector consistently improves downstream performance. Specifically, fine-tuning on 1,000 in-domain samples alone only yielded +0.8 points for Llama3-8B-Instruct and +0.9 points for Qwen3-8B. In contrast, enriching with 9,000 auxiliary samples selected by NTK-Selector led to substantial \textbf{gains of +8.7 and +5.1 points}, which corresponds to a \textbf{10.9x and 5.7x improvement} over the domain-only setting.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for Bangla-to-Python Code Generation</title>
<link>https://arxiv.org/abs/2511.07382</link>
<guid>https://arxiv.org/abs/2511.07382</guid>
<content:encoded><![CDATA[

arXiv:2511.07382v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have advanced the automated generation of code from natural language prompts. However, low-resource languages (LRLs) like Bangla remain underrepresented due to the limited availability of instruction-to-code datasets and evaluation benchmarks. To address this, the BLP Workshop at IJCNLP-AACL 2025 introduced a shared task on "Code Generation in Bangla". In this work, we propose a method that combines instruction prompting with a test-driven, feedback-guided iterative refinement process using a fine-tuned Qwen2.5-14B model. The model generates code from Bangla instructions, tests it against unit tests, and iteratively refines any failing outputs through three evaluation passes, using test feedback to guide each step. This approach helped our team "Retriv" to secure 2nd place in the shared task with a Pass@1 score of 0.934. The analysis highlights challenges in Bangla instruction understanding and Python code generation, emphasizing the need for targeted methods in LRLs. We made experimental scripts publicly available for the community.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence</title>
<link>https://arxiv.org/abs/2511.07384</link>
<guid>https://arxiv.org/abs/2511.07384</guid>
<content:encoded><![CDATA[

arXiv:2511.07384v1 Announce Type: new 
Abstract: Recent advances in depth-recurrent language models show that recurrence can decouple train-time compute and parameter count from test-time compute. In this work, we study how to convert existing pretrained non-recurrent language models into depth-recurrent models. We find that using a curriculum of recurrences to increase the effective depth of the model over the course of training preserves performance while reducing total computational cost. In our experiments, on mathematics, we observe that converting pretrained models to recurrent ones results in better performance at a given compute budget than simply post-training the original non-recurrent language model.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surgical Agent Orchestration Platform for Voice-directed Patient Data Interaction</title>
<link>https://arxiv.org/abs/2511.07392</link>
<guid>https://arxiv.org/abs/2511.07392</guid>
<content:encoded><![CDATA[

arXiv:2511.07392v1 Announce Type: new 
Abstract: In da Vinci robotic surgery, surgeons' hands and eyes are fully engaged in the procedure, making it difficult to access and manipulate multimodal patient data without interruption. We propose a voice-directed Surgical Agent Orchestrator Platform (SAOP) built on a hierarchical multi-agent framework, consisting of an orchestration agent and three task-specific agents driven by Large Language Models (LLMs). These LLM-based agents autonomously plan, refine, validate, and reason to map voice commands into specific tasks such as retrieving clinical information, manipulating CT scans, or navigating 3D anatomical models on the surgical video. We also introduce a Multi-level Orchestration Evaluation Metric (MOEM) to comprehensively assess the performance and robustness from command-level and category-level perspectives. The SAOP achieves high accuracy and success rates across 240 voice commands, while LLM-based agents improve robustness against speech recognition errors and diverse or ambiguous free-form commands, demonstrating strong potential to support minimally invasive da Vinci robotic surgery.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConvFill: Model Collaboration for Responsive Conversational Voice Agents</title>
<link>https://arxiv.org/abs/2511.07397</link>
<guid>https://arxiv.org/abs/2511.07397</guid>
<content:encoded><![CDATA[

arXiv:2511.07397v1 Announce Type: new 
Abstract: Deploying conversational voice agents with large language models faces a critical challenge: cloud-based foundation models provide deep reasoning and domain knowledge but introduce latency that disrupts natural conversation, while on-device models respond immediately but lack sophistication. We propose conversational infill, a task where a lightweight on-device model generates contextually appropriate dialogue while seamlessly incorporating streaming knowledge from a powerful backend model. This approach decouples response latency from model capability, enabling systems that feel responsive while accessing the full power of large-scale models. We present ConvFill, a 360M parameter model trained on synthetic multi-domain conversations. Evaluation across multiple backend models shows that conversational infill can be successfully learned, with ConvFill achieving accuracy improvements of 36-42% over standalone small models of the same size while consistently retaining sub-200ms response latencies. Our results demonstrate the promise of this approach for building on-device conversational agents that are both immediately responsive and knowledgeable.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPOT: An Annotated French Corpus and Benchmark for Detecting Critical Interventions in Online Conversations</title>
<link>https://arxiv.org/abs/2511.07405</link>
<guid>https://arxiv.org/abs/2511.07405</guid>
<content:encoded><![CDATA[

arXiv:2511.07405v1 Announce Type: new 
Abstract: We introduce SPOT (Stopping Points in Online Threads), the first annotated corpus translating the sociological concept of stopping point into a reproducible NLP task. Stopping points are ordinary critical interventions that pause or redirect online discussions through a range of forms (irony, subtle doubt or fragmentary arguments) that frameworks like counterspeech or social correction often overlook. We operationalize this concept as a binary classification task and provide reliable annotation guidelines. The corpus contains 43,305 manually annotated French Facebook comments linked to URLs flagged as false information by social media users, enriched with contextual metadata (article, post, parent comment, page or group, and source). We benchmark fine-tuned encoder models (CamemBERT) and instruction-tuned LLMs under various prompting strategies. Results show that fine-tuned encoders outperform prompted LLMs in F1 score by more than 10 percentage points, confirming the importance of supervised learning for emerging non-English social media tasks. Incorporating contextual metadata further improves encoder models F1 scores from 0.75 to 0.78. We release the anonymized dataset, along with the annotation guidelines and code in our code repository, to foster transparency and reproducible research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Role of High-Performance GPU Resources in Large Language Model Based Radiology Imaging Diagnosis</title>
<link>https://arxiv.org/abs/2509.16328</link>
<guid>https://arxiv.org/abs/2509.16328</guid>
<content:encoded><![CDATA[

arXiv:2509.16328v2 Announce Type: cross 
Abstract: Large-language models (LLMs) are rapidly being applied to radiology, enabling automated image interpretation and report generation tasks. Their deployment in clinical practice requires both high diagnostic accuracy and low inference latency, which in turn demands powerful hardware. High-performance graphical processing units (GPUs) provide the necessary compute and memory throughput to run large LLMs on imaging data. We review modern GPU architectures (e.g. NVIDIA A100/H100, AMD Instinct MI250X/MI300) and key performance metrics of floating-point throughput, memory bandwidth, VRAM capacity. We show how these hardware capabilities affect radiology tasks: for example, generating reports or detecting findings on CheXpert and MIMIC-CXR images is computationally intensive and benefits from GPU parallelism and tensor-core acceleration. Empirical studies indicate that using appropriate GPU resources can reduce inference time and improve throughput. We discuss practical challenges including privacy, deployment, cost, power and optimization strategies: mixed-precision, quantization, compression, and multi-GPU scaling. Finally, we anticipate that next-generation features (8-bit tensor cores, enhanced interconnect) will further enable on-premise and federated radiology AI. Advancing GPU infrastructure is essential for safe, efficient LLM-based radiology diagnostics.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Oscar-Nominated Screenplays with Sentence Embeddings</title>
<link>https://arxiv.org/abs/2511.05500</link>
<guid>https://arxiv.org/abs/2511.05500</guid>
<content:encoded><![CDATA[

arXiv:2511.05500v1 Announce Type: cross 
Abstract: Oscar nominations are an important factor in the movie industry because they can boost both the visibility and the commercial success. This work explores whether it is possible to predict Oscar nominations for screenplays using modern language models. Since no suitable dataset was available, a new one called Movie-O-Label was created by combining the MovieSum collection of movie scripts with curated Oscar records. Each screenplay was represented by its title, Wikipedia summary, and full script. Long scripts were split into overlapping text chunks and encoded with the E5 sentence em bedding model. Then, the screenplay embed dings were classified using a logistic regression model. The best results were achieved when three feature inputs related to screenplays (script, summary, and title) were combined. The best-performing model reached a macro F1 score of 0.66, a precision recall AP of 0.445 with baseline 0.19 and a ROC-AUC of 0.79. The results suggest that even simple models based on modern text embeddings demonstrate good prediction performance and might be a starting point for future research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Factual and Musical Evaluation Metrics for Music Language Models</title>
<link>https://arxiv.org/abs/2511.05550</link>
<guid>https://arxiv.org/abs/2511.05550</guid>
<content:encoded><![CDATA[

arXiv:2511.05550v1 Announce Type: cross 
Abstract: Music language models (Music LMs), like vision language models, leverage multimodal representations to answer natural language queries about musical audio recordings. Although Music LMs are reportedly improving, we find that current evaluations fail to capture whether their answers are correct. Specifically, for all Music LMs that we examine, widely-used evaluation metrics such as BLEU, METEOR, and BERTScore fail to measure anything beyond linguistic fluency of the model's responses. To measure the true performance of Music LMs, we propose (1) a better general-purpose evaluation metric for Music LMs adapted to the music domain and (2) a factual evaluation framework to quantify the correctness of a Music LM's responses. Our framework is agnostic to the modality of the question-answering model and could be generalized to quantify performance in other open-ended question-answering domains. We use open datasets in our experiments and will release all code on publication.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction</title>
<link>https://arxiv.org/abs/2511.05577</link>
<guid>https://arxiv.org/abs/2511.05577</guid>
<content:encoded><![CDATA[

arXiv:2511.05577v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have shown strong performance in tasks like visual question answering and multimodal text generation, but their effectiveness in scientific domains such as materials science remains limited. While some machine learning methods have addressed specific challenges in this field, there is still a lack of foundation models designed for broad tasks like polymer property prediction using multimodal data. In this work, we present a multimodal polymer dataset to fine-tune VLMs through instruction-tuning pairs and assess the impact of multimodality on prediction performance. Our fine-tuned models, using LoRA, outperform unimodal and baseline approaches, demonstrating the benefits of multimodal learning. Additionally, this approach reduces the need to train separate models for different properties, lowering deployment and maintenance costs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Approximating the Mathematical Structure of Psychodynamics</title>
<link>https://arxiv.org/abs/2511.05580</link>
<guid>https://arxiv.org/abs/2511.05580</guid>
<content:encoded><![CDATA[

arXiv:2511.05580v1 Announce Type: cross 
Abstract: The complexity of human cognition has meant that psychology makes more use of theory and conceptual models than perhaps any other biomedical field. To enable precise quantitative study of the full breadth of phenomena in psychological and psychiatric medicine as well as cognitive aspects of AI safety, there is a need for a mathematical formulation which is both mathematically precise and equally accessible to experts from numerous fields. In this paper we formalize human psychodynamics via the diagrammatic framework of process theory, describe its key properties, and explain the links between a diagrammatic representation and central concepts in analysis of cognitive processes in contexts such as psychotherapy, neurotechnology, AI alignment, AI agent representation of individuals in autonomous negotiations, developing human-like AI systems, and other aspects of AI safety.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Representation Sharpening Framework for Zero Shot Dense Retrieval</title>
<link>https://arxiv.org/abs/2511.05684</link>
<guid>https://arxiv.org/abs/2511.05684</guid>
<content:encoded><![CDATA[

arXiv:2511.05684v1 Announce Type: cross 
Abstract: Zero-shot dense retrieval is a challenging setting where a document corpus is provided without relevant queries, necessitating a reliance on pretrained dense retrievers (DRs). However, since these DRs are not trained on the target corpus, they struggle to represent semantic differences between similar documents. To address this failing, we introduce a training-free representation sharpening framework that augments a document's representation with information that helps differentiate it from similar documents in the corpus. On over twenty datasets spanning multiple languages, the representation sharpening framework proves consistently superior to traditional retrieval, setting a new state-of-the-art on the BRIGHT benchmark. We show that representation sharpening is compatible with prior approaches to zero-shot dense retrieval and consistently improves their performance. Finally, we address the performance-cost tradeoff presented by our framework and devise an indexing-time approximation that preserves the majority of our performance gains over traditional retrieval, yet suffers no additional inference-time cost.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabDistill: Distilling Transformers into Neural Nets for Few-Shot Tabular Classification</title>
<link>https://arxiv.org/abs/2511.05704</link>
<guid>https://arxiv.org/abs/2511.05704</guid>
<content:encoded><![CDATA[

arXiv:2511.05704v1 Announce Type: cross 
Abstract: Transformer-based models have shown promising performance on tabular data compared to their classical counterparts such as neural networks and Gradient Boosted Decision Trees (GBDTs) in scenarios with limited training data. They utilize their pre-trained knowledge to adapt to new domains, achieving commendable performance with only a few training examples, also called the few-shot regime. However, the performance gain in the few-shot regime comes at the expense of significantly increased complexity and number of parameters. To circumvent this trade-off, we introduce TabDistill, a new strategy to distill the pre-trained knowledge in complex transformer-based models into simpler neural networks for effectively classifying tabular data. Our framework yields the best of both worlds: being parameter-efficient while performing well with limited training data. The distilled neural networks surpass classical baselines such as regular neural networks, XGBoost and logistic regression under equal training data, and in some cases, even the original transformer-based models that they were distilled from.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale</title>
<link>https://arxiv.org/abs/2511.05705</link>
<guid>https://arxiv.org/abs/2511.05705</guid>
<content:encoded><![CDATA[

arXiv:2511.05705v1 Announce Type: cross 
Abstract: Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models. Remarkably, we show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data baselines across all evaluated vision-centric benchmarks, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning (MMAU), demonstrating its effectiveness. Similarly, despite not containing videos or embodied visual data, we observe notable gains when evaluating on a single-evidence embodied QA benchmark (NiEH). Finally, we use our data to analyze the entire VLM post-training pipeline. Our empirical analysis highlights that (i) SFT on high-quality data with non-linear reasoning traces is essential for effective online RL, (ii) staged offline RL matches online RL's performance while reducing compute demands, and (iii) careful SFT on high quality data can substantially improve out-of-domain, cross-modality transfer.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persian Musical Instruments Classification Using Polyphonic Data Augmentation</title>
<link>https://arxiv.org/abs/2511.05717</link>
<guid>https://arxiv.org/abs/2511.05717</guid>
<content:encoded><![CDATA[

arXiv:2511.05717v1 Announce Type: cross 
Abstract: Musical instrument classification is essential for music information retrieval (MIR) and generative music systems. However, research on non-Western traditions, particularly Persian music, remains limited. We address this gap by introducing a new dataset of isolated recordings covering seven traditional Persian instruments, two common but originally non-Persian instruments (i.e., violin, piano), and vocals. We propose a culturally informed data augmentation strategy that generates realistic polyphonic mixtures from monophonic samples. Using the MERT model (Music undERstanding with large-scale self-supervised Training) with a classification head, we evaluate our approach with out-of-distribution data which was obtained by manually labeling segments of traditional songs. On real-world polyphonic Persian music, the proposed method yielded the best ROC-AUC (0.795), highlighting complementary benefits of tonal and temporal coherence. These results demonstrate the effectiveness of culturally grounded augmentation for robust Persian instrument recognition and provide a foundation for culturally inclusive MIR and diverse music generation systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs</title>
<link>https://arxiv.org/abs/2511.05766</link>
<guid>https://arxiv.org/abs/2511.05766</guid>
<content:encoded><![CDATA[

arXiv:2511.05766v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly examined as both behavioral subjects and decision systems, yet it remains unclear whether observed cognitive biases reflect surface imitation or deeper probability shifts. Anchoring bias, a classic human judgment bias, offers a critical test case. While prior work shows LLMs exhibit anchoring, most evidence relies on surface-level outputs, leaving internal mechanisms and attributional contributions unexplored. This paper advances the study of anchoring in LLMs through three contributions: (1) a log-probability-based behavioral analysis showing that anchors shift entire output distributions, with controls for training-data contamination; (2) exact Shapley-value attribution over structured prompt fields to quantify anchor influence on model log-probabilities; and (3) a unified Anchoring Bias Sensitivity Score integrating behavioral and attributional evidence across six open-source models. Results reveal robust anchoring effects in Gemma-2B, Phi-2, and Llama-2-7B, with attribution signaling that the anchors influence reweighting. Smaller models such as GPT-2, Falcon-RW-1B, and GPT-Neo-125M show variability, suggesting scale may modulate sensitivity. Attributional effects, however, vary across prompt designs, underscoring fragility in treating LLMs as human substitutes. The findings demonstrate that anchoring bias in LLMs is robust, measurable, and interpretable, while highlighting risks in applied domains. More broadly, the framework bridges behavioral science, LLM safety, and interpretability, offering a reproducible path for evaluating other cognitive biases in LLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiagnoLLM: A Hybrid Bayesian Neural Language Framework for Interpretable Disease Diagnosis</title>
<link>https://arxiv.org/abs/2511.05810</link>
<guid>https://arxiv.org/abs/2511.05810</guid>
<content:encoded><![CDATA[

arXiv:2511.05810v1 Announce Type: cross 
Abstract: Building trustworthy clinical AI systems requires not only accurate predictions but also transparent, biologically grounded explanations. We present \texttt{DiagnoLLM}, a hybrid framework that integrates Bayesian deconvolution, eQTL-guided deep learning, and LLM-based narrative generation for interpretable disease diagnosis. DiagnoLLM begins with GP-unmix, a Gaussian Process-based hierarchical model that infers cell-type-specific gene expression profiles from bulk and single-cell RNA-seq data while modeling biological uncertainty. These features, combined with regulatory priors from eQTL analysis, power a neural classifier that achieves high predictive performance in Alzheimer's Disease (AD) detection (88.0\% accuracy). To support human understanding and trust, we introduce an LLM-based reasoning module that translates model outputs into audience-specific diagnostic reports, grounded in clinical features, attribution signals, and domain knowledge. Human evaluations confirm that these reports are accurate, actionable, and appropriately tailored for both physicians and patients. Our findings show that LLMs, when deployed as post-hoc reasoners rather than end-to-end predictors, can serve as effective communicators within hybrid diagnostic pipelines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCP-RiskCue: Can LLM infer risk information from MCP server System Logs?</title>
<link>https://arxiv.org/abs/2511.05867</link>
<guid>https://arxiv.org/abs/2511.05867</guid>
<content:encoded><![CDATA[

arXiv:2511.05867v1 Announce Type: cross 
Abstract: Large language models (LLMs) demonstrate strong capabilities in solving complex tasks when integrated with external tools. The Model Context Protocol (MCP) has become a standard interface for enabling such tool-based interactions. However, these interactions introduce substantial security concerns, particularly when the MCP server is compromised or untrustworthy. While prior benchmarks primarily focus on prompt injection attacks or analyze the vulnerabilities of LLM MCP interaction trajectories, limited attention has been given to the underlying system logs associated with malicious MCP servers. To address this gap, we present the first synthetic benchmark for evaluating LLMs ability to identify security risks from system logs. We define nine categories of MCP server risks and generate 1,800 synthetic system logs using ten state-of-the-art LLMs. These logs are embedded in the return values of 243 curated MCP servers, yielding a dataset of 2,421 chat histories for training and 471 queries for evaluation. Our pilot experiments reveal that smaller models often fail to detect risky system logs, leading to high false negatives. While models trained with supervised fine-tuning (SFT) tend to over-flag benign logs, resulting in elevated false positives, Reinforcement Learning from Verifiable Reward (RLVR) offers a better precision-recall balance. In particular, after training with Group Relative Policy Optimization (GRPO), Llama3.1-8B-Instruct achieves 83% accuracy, surpassing the best-performing large remote model by 9 percentage points. Fine-grained, per-category analysis further underscores the effectiveness of reinforcement learning in enhancing LLM safety within the MCP framework. Code and data are available at: https://github.com/PorUna-byte/MCP-Guard/tree/master
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Imperfect Learner: Incorporating Developmental Trajectories in Memory-based Student Simulation</title>
<link>https://arxiv.org/abs/2511.05903</link>
<guid>https://arxiv.org/abs/2511.05903</guid>
<content:encoded><![CDATA[

arXiv:2511.05903v1 Announce Type: cross 
Abstract: User simulation is important for developing and evaluating human-centered AI, yet current student simulation in educational applications has significant limitations. Existing approaches focus on single learning experiences and do not account for students' gradual knowledge construction and evolving skill sets. Moreover, large language models are optimized to produce direct and accurate responses, making it challenging to represent the incomplete understanding and developmental constraints that characterize real learners. In this paper, we introduce a novel framework for memory-based student simulation that incorporates developmental trajectories through a hierarchical memory mechanism with structured knowledge representation. The framework also integrates metacognitive processes and personality traits to enrich the individual learner profiling, through dynamical consolidation of both cognitive development and personal learning characteristics. In practice, we implement a curriculum-aligned simulator grounded on the Next Generation Science Standards. Experimental results show that our approach can effectively reflect the gradual nature of knowledge development and the characteristic difficulties students face, providing a more accurate representation of learning processes.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs</title>
<link>https://arxiv.org/abs/2511.05919</link>
<guid>https://arxiv.org/abs/2511.05919</guid>
<content:encoded><![CDATA[

arXiv:2511.05919v1 Announce Type: cross 
Abstract: LLMs are now an integral part of information retrieval. As such, their role as question answering chatbots raises significant concerns due to their shown vulnerability to adversarial man-in-the-middle (MitM) attacks. Here, we propose the first principled attack evaluation on LLM factual memory under prompt injection via Xmera, our novel, theory-grounded MitM framework. By perturbing the input given to "victim" LLMs in three closed-book and fact-based QA settings, we undermine the correctness of the responses and assess the uncertainty of their generation process. Surprisingly, trivial instruction-based attacks report the highest success rate (up to ~85.3%) while simultaneously having a high uncertainty for incorrectly answered questions. To provide a simple defense mechanism against Xmera, we train Random Forest classifiers on the response uncertainty levels to distinguish between attacked and unattacked queries (average AUC of up to ~96%). We believe that signaling users to be cautious about the answers they receive from black-box and potentially corrupt LLMs is a first checkpoint toward user cyberspace safety.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScRPO: From Errors to Insights</title>
<link>https://arxiv.org/abs/2511.06065</link>
<guid>https://arxiv.org/abs/2511.06065</guid>
<content:encoded><![CDATA[

arXiv:2511.06065v1 Announce Type: cross 
Abstract: We propose Self-correction Relative Policy Optimization (ScRPO), a novel reinforcement learning framework designed to enhance large language models on challenging mathemati- cal problems by leveraging self-reflection and error correction. Our approach consists of two stages: (1) Trial-and-error learning stage: training the model with GRPO and collect- ing incorrect answers along with their cor- responding questions in an error pool; (2) Self-correction learning stage: guiding the model to reflect on why its previous an- swers were wrong. Extensive experiments across multiple math reasoning benchmarks, including AIME, AMC, Olympiad, MATH- 500, GSM8k, using Deepseek-Distill-Qwen- 1.5B and Deepseek-Distill-Qwen-7B. The ex- perimental results demonstrate that ScRPO consistently outperforms several post-training methods. These findings highlight ScRPO as a promising paradigm for enabling language models to self-improve on difficult tasks with limited external feedback, paving the way to- ward more reliable and capable AI systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulating Students with Large Language Models: A Review of Architecture, Mechanisms, and Role Modelling in Education with Generative AI</title>
<link>https://arxiv.org/abs/2511.06078</link>
<guid>https://arxiv.org/abs/2511.06078</guid>
<content:encoded><![CDATA[

arXiv:2511.06078v1 Announce Type: cross 
Abstract: Simulated Students offer a valuable methodological framework for evaluating pedagogical approaches and modelling diverse learner profiles, tasks which are otherwise challenging to undertake systematically in real-world settings. Recent research has increasingly focused on developing such simulated agents to capture a range of learning styles, cognitive development pathways, and social behaviours. Among contemporary simulation techniques, the integration of large language models (LLMs) into educational research has emerged as a particularly versatile and scalable paradigm. LLMs afford a high degree of linguistic realism and behavioural adaptability, enabling agents to approximate cognitive processes and engage in contextually appropriate pedagogical dialogues. This paper presents a thematic review of empirical and methodological studies utilising LLMs to simulate student behaviour across educational environments. We synthesise current evidence on the capacity of LLM-based agents to emulate learner archetypes, respond to instructional inputs, and interact within multi-agent classroom scenarios. Furthermore, we examine the implications of such systems for curriculum development, instructional evaluation, and teacher training. While LLMs surpass rule-based systems in natural language generation and situational flexibility, ongoing concerns persist regarding algorithmic bias, evaluation reliability, and alignment with educational objectives. The review identifies existing technological and methodological gaps and proposes future research directions for integrating generative AI into adaptive learning systems and instructional design.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Web Agents with Synthetic Supervision</title>
<link>https://arxiv.org/abs/2511.06101</link>
<guid>https://arxiv.org/abs/2511.06101</guid>
<content:encoded><![CDATA[

arXiv:2511.06101v1 Announce Type: cross 
Abstract: Web agents struggle to adapt to new websites due to the scarcity of environment specific tasks and demonstrations. Recent works have explored synthetic data generation to address this challenge, however, they suffer from data quality issues where synthesized tasks contain hallucinations that cannot be executed, and collected trajectories are noisy with redundant or misaligned actions. In this paper, we propose SynthAgent, a fully synthetic supervision framework that aims at improving synthetic data quality via dual refinement of both tasks and trajectories. Our approach begins by synthesizing diverse tasks through categorized exploration of web elements, ensuring efficient coverage of the target environment. During trajectory collection, we refine tasks when conflicts with actual observations are detected, mitigating hallucinations while maintaining task consistency. After collection, we conduct trajectory refinement with a global context to mitigate potential noise or misalignments. Finally, we fine-tune open-source web agents on the refined synthetic data to adapt them to the target environment. Experimental results demonstrate that SynthAgent outperforms existing synthetic data methods, validating the importance of high-quality synthetic supervision. The code will be publicly available at https://github.com/aiming-lab/SynthAgent.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models Develop Novel Social Biases Through Adaptive Exploration</title>
<link>https://arxiv.org/abs/2511.06148</link>
<guid>https://arxiv.org/abs/2511.06148</guid>
<content:encoded><![CDATA[

arXiv:2511.06148v1 Announce Type: cross 
Abstract: As large language models (LLMs) are adopted into frameworks that grant them the capacity to make real decisions, it is increasingly important to ensure that they are unbiased. In this paper, we argue that the predominant approach of simply removing existing biases from models is not enough. Using a paradigm from the psychology literature, we demonstrate that LLMs can spontaneously develop novel social biases about artificial demographic groups even when no inherent differences exist. These biases result in highly stratified task allocations, which are less fair than assignments by human participants and are exacerbated by newer and larger models. In social science, emergent biases like these have been shown to result from exploration-exploitation trade-offs, where the decision-maker explores too little, allowing early observations to strongly influence impressions about entire demographic groups. To alleviate this effect, we examine a series of interventions targeting model inputs, problem structure, and explicit steering. We find that explicitly incentivizing exploration most robustly reduces stratification, highlighting the need for better multifaceted objectives to mitigate bias. These results reveal that LLMs are not merely passive mirrors of human social biases, but can actively create new ones from experience, raising urgent questions about how these systems will shape societies over time.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Implicit Biases in LLM Reasoning through Logic Grid Puzzles</title>
<link>https://arxiv.org/abs/2511.06160</link>
<guid>https://arxiv.org/abs/2511.06160</guid>
<content:encoded><![CDATA[

arXiv:2511.06160v1 Announce Type: cross 
Abstract: While recent safety guardrails effectively suppress overtly biased outputs, subtler forms of social bias emerge during complex logical reasoning tasks that evade current evaluation benchmarks. To fill this gap, we introduce a new evaluation framework, PRIME (Puzzle Reasoning for Implicit Biases in Model Evaluation), that uses logic grid puzzles to systematically probe the influence of social stereotypes on logical reasoning and decision making in LLMs. Our use of logic puzzles enables automatic generation and verification, as well as variability in complexity and biased settings. PRIME includes stereotypical, anti-stereotypical, and neutral puzzle variants generated from a shared puzzle structure, allowing for controlled and fine-grained comparisons. We evaluate multiple model families across puzzle sizes and test the effectiveness of prompt-based mitigation strategies. Focusing our experiments on gender stereotypes, our findings highlight that models consistently reason more accurately when solutions align with stereotypical associations. This demonstrates the significance of PRIME for diagnosing and quantifying social biases perpetuated in the deductive reasoning of LLMs, where fairness is critical.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Adversarial Robustness of IoT Intrusion Detection via SHAP-Based Attribution Fingerprinting</title>
<link>https://arxiv.org/abs/2511.06197</link>
<guid>https://arxiv.org/abs/2511.06197</guid>
<content:encoded><![CDATA[

arXiv:2511.06197v1 Announce Type: cross 
Abstract: The rapid proliferation of Internet of Things (IoT) devices has transformed numerous industries by enabling seamless connectivity and data-driven automation. However, this expansion has also exposed IoT networks to increasingly sophisticated security threats, including adversarial attacks targeting artificial intelligence (AI) and machine learning (ML)-based intrusion detection systems (IDS) to deliberately evade detection, induce misclassification, and systematically undermine the reliability and integrity of security defenses. To address these challenges, we propose a novel adversarial detection model that enhances the robustness of IoT IDS against adversarial attacks through SHapley Additive exPlanations (SHAP)-based fingerprinting. Using SHAP's DeepExplainer, we extract attribution fingerprints from network traffic features, enabling the IDS to reliably distinguish between clean and adversarially perturbed inputs. By capturing subtle attribution patterns, the model becomes more resilient to evasion attempts and adversarial manipulations. We evaluated the model on a standard IoT benchmark dataset, where it significantly outperformed a state-of-the-art method in detecting adversarial attacks. In addition to enhanced robustness, this approach improves model transparency and interpretability, thereby increasing trust in the IDS through explainable AI.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads</title>
<link>https://arxiv.org/abs/2511.06209</link>
<guid>https://arxiv.org/abs/2511.06209</guid>
<content:encoded><![CDATA[

arXiv:2511.06209v1 Announce Type: cross 
Abstract: Solving complex tasks usually requires LLMs to generate long multi-step reasoning chains. Previous work has shown that verifying the correctness of individual reasoning steps can further improve the performance and efficiency of LLMs on such tasks and enhance solution interpretability. However, existing verification approaches, such as Process Reward Models (PRMs), are either computationally expensive, limited to specific domains, or require large-scale human or model-generated annotations. Thus, we propose a lightweight alternative for step-level reasoning verification based on data-driven uncertainty scores. We train transformer-based uncertainty quantification heads (UHeads) that use the internal states of a frozen LLM to estimate the uncertainty of its reasoning steps during generation. The approach is fully automatic: target labels are generated either by another larger LLM (e.g., DeepSeek R1) or in a self-supervised manner by the original model itself. UHeads are both effective and lightweight, containing less than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, they match or even surpass the performance of PRMs that are up to 810x larger. Our findings suggest that the internal states of LLMs encode their uncertainty and can serve as reliable signals for reasoning verification, offering a promising direction toward scalable and generalizable introspective LLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B</title>
<link>https://arxiv.org/abs/2511.06221</link>
<guid>https://arxiv.org/abs/2511.06221</guid>
<content:encoded><![CDATA[

arXiv:2511.06221v1 Announce Type: cross 
Abstract: Challenging the prevailing consensus that small models inherently lack robust reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense model developed via our Spectrum-to-Signal Principle (SSP). This challenges the prevailing approach of scaling model parameters to enhance capabilities, as seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL) to amplify the correct signal. With a total training cost of only $7,800, VibeThinker-1.5B demonstrates superior reasoning capabilities compared to closed-source models like Magistral Medium and Claude Opus 4, and performs on par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8), AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial improvement over its base model (6.7, 4.3, and 0.6, respectively). On LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its base model's 0.0. These findings demonstrate that small models can achieve reasoning capabilities comparable to large models, drastically reducing training and inference costs and thereby democratizing advanced AI research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixtures of SubExperts for Large Language Continual Learning</title>
<link>https://arxiv.org/abs/2511.06237</link>
<guid>https://arxiv.org/abs/2511.06237</guid>
<content:encoded><![CDATA[

arXiv:2511.06237v1 Announce Type: cross 
Abstract: Adapting Large Language Models (LLMs) to a continuous stream of tasks is a critical yet challenging endeavor. While Parameter-Efficient Fine-Tuning (PEFT) methods have become a standard for this, they face a fundamental dilemma in continual learning. Reusing a single set of PEFT parameters for new tasks often leads to catastrophic forgetting of prior knowledge. Conversely, allocating distinct parameters for each task prevents forgetting but results in a linear growth of the model's size and fails to facilitate knowledge transfer between related tasks. To overcome these limitations, we propose a novel adaptive PEFT method referred to as \textit{Mixtures of SubExperts (MoSEs)}, a novel continual learning framework designed for minimal forgetting and efficient scalability. MoSEs integrate a sparse Mixture of SubExperts into the transformer layers, governed by a task-specific routing mechanism. This architecture allows the model to isolate and protect knowledge within dedicated SubExperts, thereby minimizing parameter interference and catastrophic forgetting. Crucially, the router can adaptively select and combine previously learned sparse parameters for new tasks, enabling effective knowledge transfer while ensuring that the model's capacity grows sublinearly. We evaluate MoSEs on the comprehensive TRACE benchmark datasets. Our experiments demonstrate that MoSEs significantly outperform conventional continual learning approaches in both knowledge retention and scalability to new tasks, achieving state-of-the-art performance with substantial memory and computational savings.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Multimodal Misinformation Detection by Replaying the Whole Story from Image Modality Perspective</title>
<link>https://arxiv.org/abs/2511.06284</link>
<guid>https://arxiv.org/abs/2511.06284</guid>
<content:encoded><![CDATA[

arXiv:2511.06284v1 Announce Type: cross 
Abstract: Multimodal Misinformation Detection (MMD) refers to the task of detecting social media posts involving misinformation, where the post often contains text and image modalities. However, by observing the MMD posts, we hold that the text modality may be much more informative than the image modality because the text generally describes the whole event/story of the current post but the image often presents partial scenes only. Our preliminary empirical results indicate that the image modality exactly contributes less to MMD. Upon this idea, we propose a new MMD method named RETSIMD. Specifically, we suppose that each text can be divided into several segments, and each text segment describes a partial scene that can be presented by an image. Accordingly, we split the text into a sequence of segments, and feed these segments into a pre-trained text-to-image generator to augment a sequence of images. We further incorporate two auxiliary objectives concerning text-image and image-label mutual information, and further post-train the generator over an auxiliary text-to-image generation benchmark dataset. Additionally, we propose a graph structure by defining three heuristic relationships between images, and use a graph neural network to generate the fused features. Extensive empirical results validate the effectiveness of RETSIMD.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ELEGANCE: Efficient LLM Guidance for Audio-Visual Target Speech Extraction</title>
<link>https://arxiv.org/abs/2511.06288</link>
<guid>https://arxiv.org/abs/2511.06288</guid>
<content:encoded><![CDATA[

arXiv:2511.06288v1 Announce Type: cross 
Abstract: Audio-visual target speaker extraction (AV-TSE) models primarily rely on visual cues from the target speaker. However, humans also leverage linguistic knowledge, such as syntactic constraints, next word prediction, and prior knowledge of conversation, to extract target speech. Inspired by this observation, we propose ELEGANCE, a novel framework that incorporates linguistic knowledge from large language models (LLMs) into AV-TSE models through three distinct guidance strategies: output linguistic constraints, intermediate linguistic prediction, and input linguistic prior. Comprehensive experiments with RoBERTa, Qwen3-0.6B, and Qwen3-4B on two AV-TSE backbones demon- strate the effectiveness of our approach. Significant improvements are observed in challenging scenarios, including visual cue impaired, unseen languages, target speaker switches, increased interfering speakers, and out-of-domain test set. Demo page: https://alexwxwu.github.io/ELEGANCE/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LPFQA: A Long-Tail Professional Forum-based Benchmark for LLM Evaluation</title>
<link>https://arxiv.org/abs/2511.06346</link>
<guid>https://arxiv.org/abs/2511.06346</guid>
<content:encoded><![CDATA[

arXiv:2511.06346v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have made rapid progress in reasoning, question answering, and professional applications; however, their true capabilities remain difficult to evaluate using existing benchmarks. Current datasets often focus on simplified tasks or artificial scenarios, overlooking long-tail knowledge and the complexities of real-world applications. To bridge this gap, we propose LPFQA, a long-tail knowledge-based benchmark derived from authentic professional forums across 20 academic and industrial fields, covering 502 tasks grounded in practical expertise. LPFQA introduces four key innovations: fine-grained evaluation dimensions that target knowledge depth, reasoning, terminology comprehension, and contextual analysis; a hierarchical difficulty structure that ensures semantic clarity and unique answers; authentic professional scenario modeling with realistic user personas; and interdisciplinary knowledge integration across diverse domains. We evaluated 12 mainstream LLMs on LPFQA and observed significant performance disparities, especially in specialized reasoning tasks. LPFQA provides a robust, authentic, and discriminative benchmark for advancing LLM evaluation and guiding future model development.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MONICA: Real-Time Monitoring and Calibration of Chain-of-Thought Sycophancy in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2511.06419</link>
<guid>https://arxiv.org/abs/2511.06419</guid>
<content:encoded><![CDATA[

arXiv:2511.06419v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) suffer from sycophantic behavior, where models tend to agree with users' incorrect beliefs and follow misinformation rather than maintain independent reasoning. This behavior undermines model reliability and poses societal risks. Mitigating LRM sycophancy requires monitoring how this sycophancy emerges during the reasoning trajectory; however, current methods mainly focus on judging based on final answers and correcting them, without understanding how sycophancy develops during reasoning processes. To address this limitation, we propose MONICA, a novel Monitor-guided Calibration framework that monitors and mitigates sycophancy during model inference at the level of reasoning steps, without requiring the model to finish generating its complete answer. MONICA integrates a sycophantic monitor that provides real-time monitoring of sycophantic drift scores during response generation with a calibrator that dynamically suppresses sycophantic behavior when scores exceed predefined thresholds. Extensive experiments across 12 datasets and 3 LRMs demonstrate that our method effectively reduces sycophantic behavior in both intermediate reasoning steps and final answers, yielding robust performance improvements.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CG-TTRL: Context-Guided Test-Time Reinforcement Learning for On-Device Large Language Models</title>
<link>https://arxiv.org/abs/2511.06430</link>
<guid>https://arxiv.org/abs/2511.06430</guid>
<content:encoded><![CDATA[

arXiv:2511.06430v1 Announce Type: cross 
Abstract: Test-time Reinforcement Learning (TTRL) has shown promise in adapting foundation models for complex tasks at test-time, resulting in large performance improvements. TTRL leverages an elegant two-phase sampling strategy: first, multi-sampling derives a pseudo-label via majority voting, while subsequent downsampling and reward-based fine-tuning encourages the model to explore and learn diverse valid solutions, with the pseudo-label modulating the reward signal. Meanwhile, in-context learning has been widely explored at inference time and demonstrated the ability to enhance model performance without weight updates. However, TTRL's two-phase sampling strategy under-utilizes contextual guidance, which can potentially improve pseudo-label accuracy in the initial exploitation phase while regulating exploration in the second. To address this, we propose context-guided TTRL (CG-TTRL), integrating context dynamically into both sampling phases and propose a method for efficient context selection for on-device applications. Our evaluations on mathematical and scientific QA benchmarks show CG-TTRL outperforms TTRL (e.g. additional 7% relative accuracy improvement over TTRL), while boosting efficiency by obtaining strong performance after only a few steps of test-time training (e.g. 8% relative improvement rather than 1% over TTRL after 3 steps).
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Chain-of-Thought Confidence via Topological and Dirichlet Risk Analysis</title>
<link>https://arxiv.org/abs/2511.06437</link>
<guid>https://arxiv.org/abs/2511.06437</guid>
<content:encoded><![CDATA[

arXiv:2511.06437v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) prompting enables Large Language Models to solve complex problems, but deploying these models safely requires reliable confidence estimates, a capability where existing methods suffer from poor calibration and severe overconfidence on incorrect predictions. We propose Enhanced Dirichlet and Topology Risk (EDTR), a novel decoding strategy that combines topological analysis with Dirichlet-based uncertainty quantification to measure LLM confidence across multiple reasoning paths. EDTR treats each CoT as a vector in high-dimensional space and extracts eight topological risk features capturing the geometric structure of reasoning distributions: tighter, more coherent clusters indicate higher confidence while dispersed, inconsistent paths signal uncertainty. We evaluate EDTR against three state-of-the-art calibration methods across four diverse reasoning benchmarks spanning olympiad-level mathematics (AIME), grade school math (GSM8K), commonsense reasoning, and stock price prediction \cite{zhang2025aime, cobbe2021training, talmor-etal-2019-commonsenseqa, yahoo_finance}. EDTR achieves 41\% better calibration than competing methods with an average ECE of 0.287 and the best overall composite score of 0.672, while notably achieving perfect accuracy on AIME and exceptional calibration on GSM8K with an ECE of 0.107, domains where baselines exhibit severe overconfidence. Our work provides a geometric framework for understanding and quantifying uncertainty in multi-step LLM reasoning, enabling more reliable deployment where calibrated confidence estimates are essential.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When AI Agents Collude Online: Financial Fraud Risks by Collaborative LLM Agents on Social Platforms</title>
<link>https://arxiv.org/abs/2511.06448</link>
<guid>https://arxiv.org/abs/2511.06448</guid>
<content:encoded><![CDATA[

arXiv:2511.06448v1 Announce Type: cross 
Abstract: In this work, we study the risks of collective financial fraud in large-scale multi-agent systems powered by large language model (LLM) agents. We investigate whether agents can collaborate in fraudulent behaviors, how such collaboration amplifies risks, and what factors influence fraud success. To support this research, we present MultiAgentFraudBench, a large-scale benchmark for simulating financial fraud scenarios based on realistic online interactions. The benchmark covers 28 typical online fraud scenarios, spanning the full fraud lifecycle across both public and private domains. We further analyze key factors affecting fraud success, including interaction depth, activity level, and fine-grained collaboration failure modes. Finally, we propose a series of mitigation strategies, including adding content-level warnings to fraudulent posts and dialogues, using LLMs as monitors to block potentially malicious agents, and fostering group resilience through information sharing at the societal level. Notably, we observe that malicious agents can adapt to environmental interventions. Our findings highlight the real-world risks of multi-agent financial fraud and suggest practical measures for mitigating them. Code is available at https://github.com/zheng977/MutiAgent4Fraud.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Analogy between Human Brain and LLMs: Spotting Key Neurons in Grammar Perception</title>
<link>https://arxiv.org/abs/2511.06519</link>
<guid>https://arxiv.org/abs/2511.06519</guid>
<content:encoded><![CDATA[

arXiv:2511.06519v1 Announce Type: cross 
Abstract: Artificial Neural Networks, the building blocks of AI, were inspired by the human brain's network of neurons. Over the years, these networks have evolved to replicate the complex capabilities of the brain, allowing them to handle tasks such as image and language processing. In the realm of Large Language Models, there has been a keen interest in making the language learning process more akin to that of humans. While neuroscientific research has shown that different grammatical categories are processed by different neurons in the brain, we show that LLMs operate in a similar way. Utilizing Llama 3, we identify the most important neurons associated with the prediction of words belonging to different part-of-speech tags. Using the achieved knowledge, we train a classifier on a dataset, which shows that the activation patterns of these key neurons can reliably predict part-of-speech tags on fresh data. The results suggest the presence of a subspace in LLMs focused on capturing part-of-speech tag concepts, resembling patterns observed in lesion studies of the brain in neuroscience.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FPGA or GPU? Analyzing comparative research for application-specific guidance</title>
<link>https://arxiv.org/abs/2511.06565</link>
<guid>https://arxiv.org/abs/2511.06565</guid>
<content:encoded><![CDATA[

arXiv:2511.06565v1 Announce Type: cross 
Abstract: The growing complexity of computational workloads has amplified the need for efficient and specialized hardware accelerators. Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) have emerged as prominent solutions, each excelling in specific domains. Although there is substantial research comparing FPGAs and GPUs, most of the work focuses primarily on performance metrics, offering limited insight into the specific types of applications that each accelerator benefits the most. This paper aims to bridge this gap by synthesizing insights from various research articles to guide users in selecting the appropriate accelerator for domain-specific applications. By categorizing the reviewed studies and analyzing key performance metrics, this work highlights the strengths, limitations, and ideal use cases for FPGAs and GPUs. The findings offer actionable recommendations, helping researchers and practitioners navigate trade-offs in performance, energy efficiency, and programmability.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAPH-GRPO-LEX: Contract Graph Modeling and Reinforcement Learning with Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2511.06618</link>
<guid>https://arxiv.org/abs/2511.06618</guid>
<content:encoded><![CDATA[

arXiv:2511.06618v1 Announce Type: cross 
Abstract: Contracts are complex documents featuring detailed formal structures, explicit and implicit dependencies and rich semantic content. Given these document properties, contract drafting and manual examination of contracts have proven to be both arduous and susceptible to errors. This work aims to simplify and automate the task of contract review and analysis using a novel framework for transforming legal contracts into structured semantic graphs, enabling computational analysis and data-driven insights. We introduce a detailed ontology mapping core legal contract elements to their graph-theoretic equivalents of nodes and edges. We then present a reinforcement learning based Large Language Model (LLM) framework for segmentation and extraction of entities and relationships from contracts. Our method, GRAPH-GRPO-LEX, incorporates both LLMs and reinforcement learning with group relative policy optimization (GRPO). By applying a carefully drafted reward function of graph metrics, we demonstrate the ability to automatically identify direct relationships between clauses, and even uncover hidden dependencies. Our introduction of the gated GRPO approach shows a strong learning signal and can move contract analysis from a linear, manual reading process to an easily visualized graph. This allows for a more dynamic analysis, including building the groundwork for contract linting similar to what is now practiced in software engineering.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Testing for Segmenting Watermarked Texts From Language Models</title>
<link>https://arxiv.org/abs/2511.06645</link>
<guid>https://arxiv.org/abs/2511.06645</guid>
<content:encoded><![CDATA[

arXiv:2511.06645v1 Announce Type: cross 
Abstract: The rapid adoption of large language models (LLMs), such as GPT-4 and Claude 3.5, underscores the need to distinguish LLM-generated text from human-written content to mitigate the spread of misinformation and misuse in education. One promising approach to address this issue is the watermark technique, which embeds subtle statistical signals into LLM-generated text to enable reliable identification. In this paper, we first generalize the likelihood-based LLM detection method of a previous study by introducing a flexible weighted formulation, and further adapt this approach to the inverse transform sampling method. Moving beyond watermark detection, we extend this adaptive detection strategy to tackle the more challenging problem of segmenting a given text into watermarked and non-watermarked substrings. In contrast to the approach in a previous study, which relies on accurate estimation of next-token probabilities that are highly sensitive to prompt estimation, our proposed framework removes the need for precise prompt estimation. Extensive numerical experiments demonstrate that the proposed methodology is both effective and robust in accurately segmenting texts containing a mixture of watermarked and non-watermarked content.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiMo-CLIP: Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment</title>
<link>https://arxiv.org/abs/2511.06653</link>
<guid>https://arxiv.org/abs/2511.06653</guid>
<content:encoded><![CDATA[

arXiv:2511.06653v1 Announce Type: cross 
Abstract: Contrastive vision-language models like CLIP have achieved impressive results in image-text retrieval by aligning image and text representations in a shared embedding space. However, these models often treat text as flat sequences, limiting their ability to handle complex, compositional, and long-form descriptions. In particular, they fail to capture two essential properties of language: semantic hierarchy, which reflects the multi-level compositional structure of text, and semantic monotonicity, where richer descriptions should result in stronger alignment with visual content.To address these limitations, we propose HiMo-CLIP, a representation-level framework that enhances CLIP-style models without modifying the encoder architecture. HiMo-CLIP introduces two key components: a hierarchical decomposition (HiDe) module that extracts latent semantic components from long-form text via in-batch PCA, enabling flexible, batch-aware alignment across different semantic granularities, and a monotonicity-aware contrastive loss (MoLo) that jointly aligns global and component-level representations, encouraging the model to internalize semantic ordering and alignment strength as a function of textual completeness.These components work in concert to produce structured, cognitively-aligned cross-modal representations. Experiments on multiple image-text retrieval benchmarks show that HiMo-CLIP consistently outperforms strong baselines, particularly under long or compositional descriptions. The code is available at https://github.com/UnicomAI/HiMo-CLIP.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Place Matters: Comparing LLM Hallucination Rates for Place-Based Legal Queries</title>
<link>https://arxiv.org/abs/2511.06700</link>
<guid>https://arxiv.org/abs/2511.06700</guid>
<content:encoded><![CDATA[

arXiv:2511.06700v1 Announce Type: cross 
Abstract: How do we make a meaningful comparison of a large language model's knowledge of the law in one place compared to another? Quantifying these differences is critical to understanding if the quality of the legal information obtained by users of LLM-based chatbots varies depending on their location. However, obtaining meaningful comparative metrics is challenging because legal institutions in different places are not themselves easily comparable. In this work we propose a methodology to obtain place-to-place metrics based on the comparative law concept of functionalism. We construct a dataset of factual scenarios drawn from Reddit posts by users seeking legal advice for family, housing, employment, crime and traffic issues. We use these to elicit a summary of a law from the LLM relevant to each scenario in Los Angeles, London and Sydney. These summaries, typically of a legislative provision, are manually evaluated for hallucinations. We show that the rate of hallucination of legal information by leading closed-source LLMs is significantly associated with place. This suggests that the quality of legal solutions provided by these models is not evenly distributed across geography. Additionally, we show a strong negative correlation between hallucination rate and the frequency of the majority response when the LLM is sampled multiple times, suggesting a measure of uncertainty of model predictions of legal facts.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting the Data Sampling in Multimodal Post-training from a Difficulty-Distinguish View</title>
<link>https://arxiv.org/abs/2511.06722</link>
<guid>https://arxiv.org/abs/2511.06722</guid>
<content:encoded><![CDATA[

arXiv:2511.06722v1 Announce Type: cross 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have spurred significant progress in Chain-of-Thought (CoT) reasoning. Building on the success of Deepseek-R1, researchers extended multimodal reasoning to post-training paradigms based on reinforcement learning (RL), focusing predominantly on mathematical datasets. However, existing post-training paradigms tend to neglect two critical aspects: (1) The lack of quantifiable difficulty metrics capable of strategically screening samples for post-training optimization. (2) Suboptimal post-training paradigms that fail to jointly optimize perception and reasoning capabilities. To address this gap, we propose two novel difficulty-aware sampling strategies: Progressive Image Semantic Masking (PISM) quantifies sample hardness through systematic image degradation, while Cross-Modality Attention Balance (CMAB) assesses cross-modal interaction complexity via attention distribution analysis. Leveraging these metrics, we design a hierarchical training framework that incorporates both GRPO-only and SFT+GRPO hybrid training paradigms, and evaluate them across six benchmark datasets. Experiments demonstrate consistent superiority of GRPO applied to difficulty-stratified samples compared to conventional SFT+GRPO pipelines, indicating that strategic data sampling can obviate the need for supervised fine-tuning while improving model accuracy. Our code will be released at https://github.com/qijianyu277/DifficultySampling.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MENTOR: A Metacognition-Driven Self-Evolution Framework for Uncovering and Mitigating Implicit Risks in LLMs on Domain Tasks</title>
<link>https://arxiv.org/abs/2511.07107</link>
<guid>https://arxiv.org/abs/2511.07107</guid>
<content:encoded><![CDATA[

arXiv:2511.07107v1 Announce Type: cross 
Abstract: Ensuring the safety and value alignment of large language models (LLMs) is critical for their deployment. Current alignment efforts primarily target explicit risks such as bias, hate speech, and violence. However, they often fail to address deeper, domain-specific implicit risks and lack a flexible, generalizable framework applicable across diverse specialized fields. Hence, we proposed MENTOR: A MEtacognition-driveN self-evoluTion framework for uncOvering and mitigating implicit Risks in LLMs on Domain Tasks. To address the limitations of labor-intensive human evaluation, we introduce a novel metacognitive self-assessment tool. This enables LLMs to reflect on potential value misalignments in their responses using strategies like perspective-taking and consequential thinking. We also release a supporting dataset of 9,000 risk queries spanning education, finance, and management to enhance domain-specific risk identification. Subsequently, based on the outcomes of metacognitive reflection, the framework dynamically generates supplementary rule knowledge graphs that extend predefined static rule trees. This enables models to actively apply validated rules to future similar challenges, establishing a continuous self-evolution cycle that enhances generalization by reducing maintenance costs and inflexibility of static systems. Finally, we employ activation steering during inference to guide LLMs in following the rules, a cost-effective method to robustly enhance enforcement across diverse contexts. Experimental results show MENTOR's effectiveness: In defensive testing across three vertical domains, the framework substantially reduces semantic attack success rates, enabling a new level of implicit risk mitigation for LLMs. Furthermore, metacognitive assessment not only aligns closely with baseline human evaluators but also delivers more thorough and insightful analysis of LLMs value alignment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Representation-based Model Poisoning on the Heterogeneous Internet of Agents</title>
<link>https://arxiv.org/abs/2511.07176</link>
<guid>https://arxiv.org/abs/2511.07176</guid>
<content:encoded><![CDATA[

arXiv:2511.07176v1 Announce Type: cross 
Abstract: Internet of Agents (IoA) envisions a unified, agent-centric paradigm where heterogeneous large language model (LLM) agents can interconnect and collaborate at scale. Within this paradigm, federated learning (FL) serves as a key enabler that allows distributed LLM agents to co-train global models without centralizing data. However, the FL-enabled IoA system remains vulnerable to model poisoning attacks, and the prevailing distance and similarity-based defenses become fragile at billion-parameter scale and under heterogeneous data distributions. This paper proposes a graph representation-based model poisoning (GRMP) attack, which passively exploits observed benign local models to construct a parameter correlation graph and extends an adversarial variational graph autoencoder to capture and reshape higher-order dependencies. The GRMP attack synthesizes malicious local models that preserve benign-like statistics while embedding adversarial objectives, remaining elusive to detection at the server. Experiments demonstrate a gradual drop in system accuracy under the proposed attack and the ineffectiveness of the prevailing defense mechanism in detecting the attack, underscoring a severe threat to the ambitious IoA paradigm.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Few Govern the Many:Unveiling Few-Layer Dominance for Time Series Models</title>
<link>https://arxiv.org/abs/2511.07237</link>
<guid>https://arxiv.org/abs/2511.07237</guid>
<content:encoded><![CDATA[

arXiv:2511.07237v1 Announce Type: cross 
Abstract: Large-scale models are at the forefront of time series (TS) forecasting, dominated by two paradigms: fine-tuning text-based Large Language Models (LLM4TS) and training Time Series Foundation Models (TSFMs) from scratch. Both approaches share a foundational assumption that scaling up model capacity and data volume leads to improved performance. However, we observe a \textit{\textbf{scaling paradox}} in TS models, revealing a puzzling phenomenon that larger models do \emph{NOT} achieve better performance. Through extensive experiments on two model families across four scales (100M to 1.7B parameters) and diverse data (up to 6B observations), we rigorously confirm that the scaling paradox is a pervasive issue. We then diagnose its root cause by analyzing internal representations, identifying a phenomenon we call \textit{few-layer dominance}: only a small subset of layers are functionally important, while the majority are redundant, under-utilized, and can even distract training. Based on this discovery, we propose a practical method to automatically identify and retain only these dominant layers. In our models, retaining only 21\% of the parameters achieves up to a 12\% accuracy improvement and a 2.7$\times$ inference speedup. We validate the universality of our method on 8 prominent SOTA models (LLM4TS and TSFMs, 90M to 6B), showing that retaining less than 30\% of layers achieves comparable or superior accuracy in over 95\% of tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction</title>
<link>https://arxiv.org/abs/2511.07327</link>
<guid>https://arxiv.org/abs/2511.07327</guid>
<content:encoded><![CDATA[

arXiv:2511.07327v1 Announce Type: cross 
Abstract: Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that reformulates long-horizon research as a Markov Decision Process with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. We further develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning framework that incentivizes efficient exploration through geometric reward discounting and enables stable distributed training via adaptive downsampling. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5\% to 42.5\%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Evaluating LLMs for Multi-Step Tasks: Stepwise Confidence Estimation for Failure Detection</title>
<link>https://arxiv.org/abs/2511.07364</link>
<guid>https://arxiv.org/abs/2511.07364</guid>
<content:encoded><![CDATA[

arXiv:2511.07364v1 Announce Type: cross 
Abstract: Reliability and failure detection of large language models (LLMs) is critical for their deployment in high-stakes, multi-step reasoning tasks. Prior work explores confidence estimation for self-evaluating LLM-scorer systems, with confidence scorers estimating the likelihood of errors in LLM responses. However, most methods focus on single-step outputs and overlook the challenges of multi-step reasoning. In this work, we extend self-evaluation techniques to multi-step tasks, testing two intuitive approaches: holistic scoring and step-by-step scoring. Using two multi-step benchmark datasets, we show that stepwise evaluation generally outperforms holistic scoring in detecting potential errors, with up to 15% relative increase in AUC-ROC. Our findings demonstrate that self-evaluating LLM systems provide meaningful confidence estimates in complex reasoning, improving their trustworthiness and providing a practical framework for failure detection.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards</title>
<link>https://arxiv.org/abs/2511.07403</link>
<guid>https://arxiv.org/abs/2511.07403</guid>
<content:encoded><![CDATA[

arXiv:2511.07403v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DigiData: Training and Evaluating General-Purpose Mobile Control Agents</title>
<link>https://arxiv.org/abs/2511.07413</link>
<guid>https://arxiv.org/abs/2511.07413</guid>
<content:encoded><![CDATA[

arXiv:2511.07413v1 Announce Type: cross 
Abstract: AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Generation with Infinite Contamination</title>
<link>https://arxiv.org/abs/2511.07417</link>
<guid>https://arxiv.org/abs/2511.07417</guid>
<content:encoded><![CDATA[

arXiv:2511.07417v1 Announce Type: cross 
Abstract: We study language generation in the limit, where an algorithm observes an adversarial enumeration of strings from an unknown target language $K$ and must eventually generate new, unseen strings from $K$. Kleinberg and Mullainathan [KM24] proved that generation is achievable in surprisingly general settings. But their generator suffers from ``mode collapse,'' producing from an ever-smaller subset of the target. To address this, Kleinberg and Wei [KW25] require the generator's output to be ``dense'' in the target language. They showed that generation with density, surprisingly, remains achievable at the same generality.
  Both results assume perfect data: no noisy insertions and no omissions. This raises a central question: how much contamination can generation tolerate? Recent works made partial progress on this question by studying (non-dense) generation with either finite amounts of noise (but no omissions) or omissions (but no noise).
  We characterize robustness under contaminated enumerations: 1. Generation under Contamination: Language generation in the limit is achievable for all countable collections iff the fraction of contaminated examples converges to zero. When this fails, we characterize which collections are generable. 2. Dense Generation under Contamination: Dense generation is strictly less robust to contamination than generation. As a byproduct, we resolve an open question of Raman and Raman [ICML25] by showing that generation is possible with only membership oracle access under finitely many contaminated examples.
  Finally, we introduce a beyond-worst-case model inspired by curriculum learning and prove that dense generation is achievable even with infinite contamination provided the fraction of contaminated examples converges to zero. This suggests curriculum learning may be crucial for learning from noisy web data.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiLA: Enhancing LLM Tool Learning with Differential Logic Layer</title>
<link>https://arxiv.org/abs/2402.11903</link>
<guid>https://arxiv.org/abs/2402.11903</guid>
<content:encoded><![CDATA[

arXiv:2402.11903v4 Announce Type: replace 
Abstract: Considering the challenges faced by large language models (LLMs) in logical reasoning and planning, prior efforts have sought to augment LLMs with access to external solvers. While progress has been made on simple reasoning problems, solving classical constraint satisfaction problems, such as the Boolean Satisfiability Problem (SAT) and Graph Coloring Problem (GCP), remains difficult for off-the-shelf solvers due to their intricate expressions and exponential search spaces. In this paper, we propose a novel differential logic layer-aided language modeling (DiLA) approach, where logical constraints are integrated into the forward and backward passes of a network layer, to provide another option for LLM tool learning. In DiLA, LLM aims to transform the language description to logic constraints and identify initial solutions of the highest quality, while the differential logic layer focuses on iteratively refining the LLM-prompted solution. Leveraging the logic layer as a bridge, DiLA enhances the logical reasoning ability of LLMs on a range of reasoning problems encoded by Boolean variables, guaranteeing the efficiency and correctness of the solution process. We evaluate the performance of DiLA on two classic reasoning problems and empirically demonstrate its consistent outperformance against existing prompt-based and solver-aided approaches.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Likelihood-based Mitigation of Evaluation Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2402.15987</link>
<guid>https://arxiv.org/abs/2402.15987</guid>
<content:encoded><![CDATA[

arXiv:2402.15987v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics. However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods. In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators. We also propose a method to mitigate the likelihood bias. Our method utilizes highly biased instances as few-shot examples for in-context learning. Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias. Furthermore, our proposed method successfully mitigates this bias, also improving evaluation performance (in terms of correlation of models with human scores) significantly.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry through Curiosity-Driven Queries</title>
<link>https://arxiv.org/abs/2405.20318</link>
<guid>https://arxiv.org/abs/2405.20318</guid>
<content:encoded><![CDATA[

arXiv:2405.20318v4 Announce Type: replace 
Abstract: Recent progress in Large Language Model (LLM) technology has changed our role in interacting with these models. Instead of primarily testing these models with questions we already know answers to, we are now using them for queries where the answers are unknown to us, driven by human curiosity. This shift highlights the growing need to understand curiosity-driven human questions - those that are more complex, open-ended, and reflective of real-world needs. To this end, we present Quriosity, a collection of 13.5K naturally occurring questions from three diverse sources: human-to-search-engine queries, human-to-human interactions, and human-to-LLM conversations. Our comprehensive collection enables a rich understanding of human curiosity across various domains and contexts. Our analysis reveals a significant presence of causal questions (up to 42%) in the dataset, for which we develop an iterative prompt improvement framework to identify all causal queries and examine their unique linguistic properties, cognitive complexity and source distribution. Our paper paves the way for future work on causal question identification and open-ended chatbot interactions. Our code and data are at https://github.com/roberto-ceraolo/quriosity.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedCoT: Federated Chain-of-Thought Distillation for Large Language Models</title>
<link>https://arxiv.org/abs/2406.12403</link>
<guid>https://arxiv.org/abs/2406.12403</guid>
<content:encoded><![CDATA[

arXiv:2406.12403v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have emerged as a transformative force in artificial intelligence, demonstrating exceptional proficiency across various tasks. However, their deployment in resource-constrained environments and concerns over user data privacy pose significant challenges. In contrast, Small Language Models (SLMs) offer computational efficiency but often lag in performance. To address these issues, we propose FedCoT, a federated framework designed for the Chain-of-Thought (CoT) distillation of knowledge from LLMs to SLMs, while ensuring the preservation of clients' data privacy. FedCoT ensures secure and efficient knowledge transfer from an LLM on a high-powered server to an SLM on a resource-constrained client, while adhering to privacy requirements. Leveraging perturbed prompts and rationales generated through the CoT approach, the framework enhances the performance of the client's SLM without compromising user data privacy within a multi-task learning framework. We propose two privacy protection strategies: the Exponential Mechanism Strategy and the Adaptive Exponential Mechanism Strategy, which balance user prompt privacy and the usability of rationales. Empirical evaluation on various text generation tasks demonstrates the effectiveness of FedCoT in training task-specific SLMs with enhanced performance while prioritizing data privacy protection. Our code has been contributed to the FATE open-source project and is now publicly accessible at \textit{https://github.com/FederatedAI/FATE-LLM/tree/main/python/fate_llm/algo/fedcot}
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models</title>
<link>https://arxiv.org/abs/2407.01599</link>
<guid>https://arxiv.org/abs/2407.01599</guid>
<content:encoded><![CDATA[

arXiv:2407.01599v3 Announce Type: replace 
Abstract: The rapid evolution of artificial intelligence (AI) through developments in Large Language Models (LLMs) and Vision-Language Models (VLMs) has brought significant advancements across various technological domains. While these models enhance capabilities in natural language processing and visual interactive tasks, their growing adoption raises critical concerns regarding security and ethical alignment. This survey provides an extensive review of the emerging field of jailbreaking--deliberately circumventing the ethical and operational boundaries of LLMs and VLMs--and the consequent development of defense mechanisms. Our study categorizes jailbreaks into seven distinct types and elaborates on defense strategies that address these vulnerabilities. Through this comprehensive examination, we identify research gaps and propose directions for future studies to enhance the security frameworks of LLMs and VLMs. Our findings underscore the necessity for a unified perspective that integrates both jailbreak strategies and defensive solutions to foster a robust, secure, and reliable environment for the next generation of language models. More details can be found on our website: https://chonghan-chen.com/llm-jailbreak-zoo-survey/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Employing Sentence Space Embedding for Classification of Data Stream from Fake News Domain</title>
<link>https://arxiv.org/abs/2407.10807</link>
<guid>https://arxiv.org/abs/2407.10807</guid>
<content:encoded><![CDATA[

arXiv:2407.10807v2 Announce Type: replace 
Abstract: Tabular data is considered the last unconquered castle of deep learning, yet the task of data stream classification is stated to be an equally important and demanding research area. Due to the temporal constraints, it is assumed that deep learning methods are not the optimal solution for application in this field. However, excluding the entire -- and prevalent -- group of methods seems rather rash given the progress that has been made in recent years in its development. For this reason, the following paper is the first to present an approach to natural language data stream classification using the sentence space method, which allows for encoding text into the form of a discrete digital signal. This allows the use of convolutional deep networks dedicated to image classification to solve the task of recognizing fake news based on text data. Based on the real-life Fakeddit dataset, the proposed approach was compared with state-of-the-art algorithms for data stream classification based on generalization ability and time complexity.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BLADE: Benchmarking Language Model Agents for Data-Driven Science</title>
<link>https://arxiv.org/abs/2408.09667</link>
<guid>https://arxiv.org/abs/2408.09667</guid>
<content:encoded><![CDATA[

arXiv:2408.09667v3 Announce Type: replace 
Abstract: Data-driven scientific discovery requires the iterative integration of scientific domain knowledge, statistical expertise, and an understanding of data semantics to make nuanced analytical decisions, e.g., about which variables, transformations, and statistical models to consider. LM-based agents equipped with planning, memory, and code execution capabilities have the potential to support data-driven science. However, evaluating agents on such open-ended tasks is challenging due to multiple valid approaches, partially correct steps, and different ways to express the same decisions. To address these challenges, we present BLADE, a benchmark to automatically evaluate agents' multifaceted approaches to open-ended research questions. BLADE consists of 12 datasets and research questions drawn from existing scientific literature, with ground truth collected from independent analyses by expert data scientists and researchers. To automatically evaluate agent responses, we developed corresponding computational methods to match different representations of analyses to this ground truth. Though language models possess considerable world knowledge, our evaluation shows that they are often limited to basic analyses. However, agents capable of interacting with the underlying data demonstrate improved, but still non-optimal, diversity in their analytical decision making. Our work enables the evaluation of agents for data-driven science and provides researchers deeper insights into agents' analysis approaches.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mufu: Multilingual Fused Learning for Low-Resource Translation with LLM</title>
<link>https://arxiv.org/abs/2409.13949</link>
<guid>https://arxiv.org/abs/2409.13949</guid>
<content:encoded><![CDATA[

arXiv:2409.13949v3 Announce Type: replace 
Abstract: Multilingual large language models (LLMs) are great translators, but this is largely limited to high-resource languages. For many LLMs, translating in and out of low-resource languages remains a challenging task. To maximize data efficiency in this low-resource setting, we introduce Mufu, which includes a selection of automatically generated multilingual candidates and an instruction to correct inaccurate translations in the prompt. Mufu prompts turn a translation task into a postediting one, and seek to harness the LLM's reasoning capability with auxiliary translation candidates, from which the model is required to assess the input quality, align the semantics cross-lingually, copy from relevant inputs and override instances that are incorrect. Our experiments on En-XX translations over the Flores-200 dataset show LLMs finetuned against Mufu-style prompts are robust to poor quality auxiliary translation candidates, achieving performance superior to NLLB 1.3B distilled model in 64% of low- and very-low-resource language pairs. We then distill these models to reduce inference cost, while maintaining on average 3.1 chrF improvement over finetune-only baseline in low-resource translations.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skill Path: Unveiling Language Skills from Circuit Graphs</title>
<link>https://arxiv.org/abs/2410.01334</link>
<guid>https://arxiv.org/abs/2410.01334</guid>
<content:encoded><![CDATA[

arXiv:2410.01334v3 Announce Type: replace 
Abstract: Circuit graph discovery has emerged as a fundamental approach to elucidating the skill mechanistic of language models. Despite the output faithfulness of circuit graphs, they suffer from atomic ablation, which causes the loss of causal dependencies between connected components. In addition, their discovery process, designed to preserve output faithfulness, inadvertently captures extraneous effects other than an isolated target skill. To alleviate these challenges, we introduce skill paths, which offers a more refined and compact representation by isolating individual skills within a linear chain of components. To enable skill path extracting from circuit graphs, we propose a three-step framework, consisting of decomposition, pruning, and post-pruning causal mediation. In particular, we offer a complete linear decomposition of the transformer model which leads to a disentangled computation graph. After pruning, we further adopt causal analysis techniques, including counterfactuals and interventions, to extract the final skill paths from the circuit graph. To underscore the significance of skill paths, we investigate three generic language skills-Previous Token Skill, Induction Skill, and In-Context Learning Skill-using our framework. Experiments support two crucial properties of these skills, namely stratification and inclusiveness.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-PRE: An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation</title>
<link>https://arxiv.org/abs/2410.12265</link>
<guid>https://arxiv.org/abs/2410.12265</guid>
<content:encoded><![CDATA[

arXiv:2410.12265v2 Announce Type: replace 
Abstract: The rapid development of large language models (LLMs) has highlighted the need for efficient and reliable methods to evaluate their performance. Traditional evaluation methods often face challenges like high costs, limited task formats, dependence on human references, and systematic biases. To address these limitations, we propose Auto-PRE, an automatic LLM evaluation framework inspired by the peer review process. Unlike previous approaches that rely on human annotations, Auto-PRE automatically selects evaluator LLMs based on three core traits: consistency, pertinence, and self-confidence, which correspond to the instruction, content, and response stages, respectively, and collectively cover the entire evaluation process. Experiments on three representative tasks, including summarization, non-factoid QA, and dialogue generation, demonstrate that Auto-PRE achieves state-of-the-art performance while significantly reducing evaluation costs. Furthermore, the structured and scalable design of our automatic qualification exam framework provides valuable insights into automating the evaluation of LLMs-as-judges, paving the way for more advanced LLM-based evaluation frameworks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>All Entities are Not Created Equal: Examining the Long Tail for Ultra-Fine Entity Typing</title>
<link>https://arxiv.org/abs/2410.17355</link>
<guid>https://arxiv.org/abs/2410.17355</guid>
<content:encoded><![CDATA[

arXiv:2410.17355v3 Announce Type: replace 
Abstract: Due to their capacity to acquire world knowledge from large corpora, pre-trained language models (PLMs) are extensively used in ultra-fine entity typing tasks where the space of labels is extremely large. In this work, we explore the limitations of the knowledge acquired by PLMs by proposing a novel heuristic to approximate the pre-training distribution of entities when the pre-training data is unknown. Then, we systematically demonstrate that entity-typing approaches that rely solely on the parametric knowledge of PLMs struggle significantly with entities at the long tail of the pre-training distribution, and that knowledge-infused approaches can account for some of these shortcomings. Our findings suggest that we need to go beyond PLMs to produce solutions that perform well for infrequent entities.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shared Heritage, Distinct Writing: Rethinking Resource Selection for East Asian Historical Documents</title>
<link>https://arxiv.org/abs/2411.04822</link>
<guid>https://arxiv.org/abs/2411.04822</guid>
<content:encoded><![CDATA[

arXiv:2411.04822v2 Announce Type: replace 
Abstract: Historical documents in the Sinosphere are known to share common formats and practices, particularly in veritable records compiled by court historians. This shared linguistic heritage has led researchers to use Classical Chinese resources for cross-lingual transfer when processing historical documents from Korea and Japan, which remain relatively low-resource. In this paper, we question the assumption of cross-lingual transferability from Classical Chinese to Hanja and Kanbun, the ancient written languages of Korea and Japan, respectively. Our experiments across machine translation, named entity recognition, and punctuation restoration tasks show minimal impact of Classical Chinese datasets on language model performance for ancient Korean documents written in Hanja, with performance differences within $\pm{}0.0068$ F1-score for sequence labeling tasks and up to $+0.84$ BLEU score for translation. These limitations persist consistently across various model sizes, architectures, and domain-specific datasets. Our analysis reveals that the benefits of Classical Chinese resources diminish rapidly as local language data increases for Hanja, while showing substantial improvements only in extremely low-resource scenarios for both Korean and Japanese historical documents. These findings emphasize the need for careful empirical validation rather than assuming benefits from indiscriminate cross-lingual transfer.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compositional Phoneme Approximation for L1-Grounded L2 Pronunciation Training</title>
<link>https://arxiv.org/abs/2411.10927</link>
<guid>https://arxiv.org/abs/2411.10927</guid>
<content:encoded><![CDATA[

arXiv:2411.10927v5 Announce Type: replace 
Abstract: Learners of a second language (L2) often map non-native phonemes to similar native-language (L1) phonemes, making conventional L2-focused training slow and effortful. To address this, we propose an L1-grounded pronunciation training method based on compositional phoneme approximation (CPA), a feature-based representation technique that approximates L2 sounds with sequences of L1 phonemes. Evaluations with 20 Korean non-native English speakers show that CPA-based training achieves a 76% in-box formant rate in acoustic analysis, 17.6% relative improvement in phoneme recognition accuracy, and over 80% of speech being rated as more native-like, with minimal training. Project page: https://gsanpark.github.io/CPA-Pronunciation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pralekha: Cross-Lingual Document Alignment for Indic Languages</title>
<link>https://arxiv.org/abs/2411.19096</link>
<guid>https://arxiv.org/abs/2411.19096</guid>
<content:encoded><![CDATA[

arXiv:2411.19096v3 Announce Type: replace 
Abstract: Mining parallel document pairs for document-level machine translation (MT) remains challenging due to the limitations of existing Cross-Lingual Document Alignment (CLDA) techniques. Existing methods often rely on metadata such as URLs, which are scarce, or on pooled document representations that fail to capture fine-grained alignment cues. Moreover, the limited context window of sentence embedding models hinders their ability to represent document-level context, while sentence-based alignment introduces a combinatorially large search space, leading to high computational cost. To address these challenges for Indic languages, we introduce Pralekha, a benchmark containing over 3 million aligned document pairs across 11 Indic languages and English, which includes 1.5 million English-Indic pairs. Furthermore, we propose Document Alignment Coefficient (DAC), a novel metric for fine-grained document alignment. Unlike pooling-based methods, DAC aligns documents by matching smaller chunks and computes similarity as the ratio of aligned chunks to the average number of chunks in a pair. Intrinsic evaluation shows that our chunk-based method is 2-3x faster while maintaining competitive performance, and that DAC achieves substantial gains over pooling-based baselines. Extrinsic evaluation further demonstrates that document-level MT models trained on DAC-aligned pairs consistently outperform those using baseline alignment methods. These results highlight DAC's effectiveness for parallel document mining. The dataset and evaluation framework are publicly available to support further research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification</title>
<link>https://arxiv.org/abs/2411.19638</link>
<guid>https://arxiv.org/abs/2411.19638</guid>
<content:encoded><![CDATA[

arXiv:2411.19638v2 Announce Type: replace 
Abstract: With the ever-increasing number of news stories available online, classifying them by topic, regardless of the language they are written in, has become crucial for enhancing readers' access to relevant content. To address this challenge, we propose a teacher-student framework based on large language models (LLMs) for developing multilingual news topic classification models of reasonable size with no need for manual data annotation. The framework employs a Generative Pretrained Transformer (GPT) model as the teacher model to develop a news topic training dataset through automatic annotation of 20,000 news articles in Slovenian, Croatian, Greek, and Catalan. Articles are classified into 17 main categories from the Media Topic schema, developed by the International Press Telecommunications Council (IPTC). The teacher model exhibits high zero-shot performance in all four languages. Its agreement with human annotators is comparable to that between the human annotators themselves. To mitigate the computational limitations associated with the requirement of processing millions of texts daily, smaller BERT-like student models are fine-tuned on the GPT-annotated dataset. These student models achieve high performance comparable to the teacher model. Furthermore, we explore the impact of the training data size on the performance of the student models and investigate their monolingual, multilingual, and zero-shot cross-lingual capabilities. The findings indicate that student models can achieve high performance with a relatively small number of training instances, and demonstrate strong zero-shot cross-lingual abilities. Finally, we publish the best-performing news topic classifier, enabling multilingual classification with the top-level categories of the IPTC Media Topic schema.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RareAgents: Autonomous Multi-disciplinary Team for Rare Disease Diagnosis and Treatment</title>
<link>https://arxiv.org/abs/2412.12475</link>
<guid>https://arxiv.org/abs/2412.12475</guid>
<content:encoded><![CDATA[

arXiv:2412.12475v3 Announce Type: replace 
Abstract: Rare diseases, despite their low individual incidence, collectively impact around 300 million people worldwide due to the vast number of diseases. The involvement of multiple organs and systems, and the shortage of specialized doctors with relevant experience, make diagnosing and treating rare diseases more challenging than common diseases. Recently, agents powered by large language models (LLMs) have demonstrated notable applications across various domains. In the medical field, some agent methods have outperformed direct prompts in question-answering tasks from medical examinations. However, current agent frameworks are not well-adapted to real-world clinical scenarios, especially those involving the complex demands of rare diseases. To bridge this gap, we introduce RareAgents, the first LLM-driven multi-disciplinary team decision-support tool designed specifically for the complex clinical context of rare diseases. RareAgents integrates advanced Multidisciplinary Team (MDT) coordination, memory mechanisms, and medical tools utilization, leveraging Llama-3.1-8B/70B as the base model. Experimental results show that RareAgents outperforms state-of-the-art domain-specific models, GPT-4o, and current agent frameworks in diagnosis and treatment for rare diseases. Furthermore, we contribute a novel rare disease dataset, MIMIC-IV-Ext-Rare, to facilitate further research in this field.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revealing emergent human-like conceptual representations from language prediction</title>
<link>https://arxiv.org/abs/2501.12547</link>
<guid>https://arxiv.org/abs/2501.12547</guid>
<content:encoded><![CDATA[

arXiv:2501.12547v4 Announce Type: replace 
Abstract: People acquire concepts through rich physical and social experiences and use them to understand and navigate the world. In contrast, large language models (LLMs), trained solely through next-token prediction on text, exhibit strikingly human-like behaviors. Are these models developing concepts akin to those of humans? If so, how are such concepts represented, organized, and related to behavior? Here, we address these questions by investigating the representations formed by LLMs during an in-context concept inference task. We found that LLMs can flexibly derive concepts from linguistic descriptions in relation to contextual cues about other concepts. The derived representations converge toward a shared, context-independent structure, and alignment with this structure reliably predicts model performance across various understanding and reasoning tasks. Moreover, the convergent representations effectively capture human behavioral judgments and closely align with neural activity patterns in the human brain, providing evidence for biological plausibility. Together, these findings establish that structured, human-like conceptual representations can emerge purely from language prediction without real-world grounding, highlighting the role of conceptual structure in understanding intelligent behavior. More broadly, our work suggests that LLMs offer a tangible window into the nature of human concepts and lays the groundwork for advancing alignment between artificial and human intelligence.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Task Representations from In-Context Learning</title>
<link>https://arxiv.org/abs/2502.05390</link>
<guid>https://arxiv.org/abs/2502.05390</guid>
<content:encoded><![CDATA[

arXiv:2502.05390v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable proficiency in in-context learning (ICL), where models adapt to new tasks through example-based prompts without requiring parameter updates. However, understanding how tasks are internally encoded and generalized remains a challenge. To address some of the empirical and technical gaps in the literature, we introduce an automated formulation for encoding task information in ICL prompts as a function of attention heads within the transformer architecture. This approach computes a single task vector as a weighted sum of attention heads, with the weights optimized causally via gradient descent. Our findings show that existing methods fail to generalize effectively to modalities beyond text. In response, we also design a benchmark to evaluate whether a task vector can preserve task fidelity in functional regression tasks. The proposed method successfully extracts task-specific information from in-context demonstrations and excels in both text and regression tasks, demonstrating its generalizability across modalities.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models</title>
<link>https://arxiv.org/abs/2502.07077</link>
<guid>https://arxiv.org/abs/2502.07077</guid>
<content:encoded><![CDATA[

arXiv:2502.07077v2 Announce Type: replace 
Abstract: The tendency of users to anthropomorphise large language models (LLMs) is of growing interest to AI developers, researchers, and policy-makers. Here, we present a novel method for empirically evaluating anthropomorphic LLM behaviours in realistic and varied settings. Going beyond single-turn static benchmarks, we contribute three methodological advances in state-of-the-art (SOTA) LLM evaluation. First, we develop a multi-turn evaluation of 14 anthropomorphic behaviours. Second, we present a scalable, automated approach by employing simulations of user interactions. Third, we conduct an interactive, large-scale human subject study (N=1101) to validate that the model behaviours we measure predict real users' anthropomorphic perceptions. We find that all SOTA LLMs evaluated exhibit similar behaviours, characterised by relationship-building (e.g., empathy and validation) and first-person pronoun use, and that the majority of behaviours only first occur after multiple turns. Our work lays an empirical foundation for investigating how design choices influence anthropomorphic model behaviours and for progressing the ethical debate on the desirability of these behaviours. It also showcases the necessity of multi-turn evaluations for complex social phenomena in human-AI interaction.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCS: Perceived Confidence Scoring of Black Box LLMs with Metamorphic Relations</title>
<link>https://arxiv.org/abs/2502.07186</link>
<guid>https://arxiv.org/abs/2502.07186</guid>
<content:encoded><![CDATA[

arXiv:2502.07186v2 Announce Type: replace 
Abstract: Zero-shot LLMs are now also used for textual classification tasks, e.g., sentiment and bias detection in a sentence or article. However, their performance can be suboptimal in such data annotation tasks. We introduce a novel technique that evaluates an LLM's confidence for classifying a textual input by leveraging Metamorphic Relations (MRs). The MRs generate semantically equivalent yet textually divergent versions of the input. Following the principles of Metamorphic Testing (MT), the mutated versions are expected to have annotation labels similar to the input. By analyzing the consistency of an LLM's responses across these variations, we compute a perceived confidence score (PCS) based on the frequency of the predicted labels. PCS can be used for both single and multiple LLM settings (e.g., when multiple LLMs are vetted in a majority-voting setup). Empirical evaluation shows that our PCS-based approach improves the performance of zero-shot LLMs by 9.3% in textual classification tasks. When multiple LLMs are used in a majority-voting setup, we obtain a performance boost of 5.8% with PCS.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPC-GPT: Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation</title>
<link>https://arxiv.org/abs/2502.15857</link>
<guid>https://arxiv.org/abs/2502.15857</guid>
<content:encoded><![CDATA[

arXiv:2502.15857v2 Announce Type: replace 
Abstract: Compressing Large Language Models (LLMs) into task-specific Small Language Models (SLMs) encounters two significant challenges: safeguarding domain-specific knowledge privacy and managing limited resources. To tackle these challenges, we propose PPC-GPT, a novel unified framework that systematically addresses both privacy preservation and model compression in federated settings. PPC-GPT works on a server-client federated architecture, where the client sends differentially private (DP) perturbed task-specific data to the server's LLM. The LLM then generates synthetic data along with their corresponding rationales. This synthetic data is subsequently used for both LLM pruning and retraining processes. Our framework's key innovation lies in its holistic integration of privacy-preserving mechanisms, synthetic data generation, and task-specific compression techniques, creating unique benefits through component interaction. Our experiments across diverse text generation tasks demonstrate that PPC-GPT successfully achieves dual objectives: maintaining competitive performance comparable to full-sized LLMs while ensuring robust privacy protection through its federated architecture. Our code has been contributed to the FATE open-source project and is now publicly accessible at \textit{https://github.com/FederatedAI/FATE-LLM/tree/main/python/fate_llm/algo/ppc-gpt}
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse</title>
<link>https://arxiv.org/abs/2502.16002</link>
<guid>https://arxiv.org/abs/2502.16002</guid>
<content:encoded><![CDATA[

arXiv:2502.16002v4 Announce Type: replace 
Abstract: We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we investigate a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation when using KV caches computed independently for each document, KVLink introduces two key techniques: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, and using trainable special tokens to restore self-attention across independently encoded documents. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 96% compared to standard LLM inference, making it a scalable and efficient solution for context reuse. Additionally, KVLink can be combined with KV cache compression to further save cache loading and storage overhead while outperforming the baselines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Order Doesn't Matter, But Reasoning Does: Training LLMs with Order-Centric Augmentation</title>
<link>https://arxiv.org/abs/2502.19907</link>
<guid>https://arxiv.org/abs/2502.19907</guid>
<content:encoded><![CDATA[

arXiv:2502.19907v2 Announce Type: replace 
Abstract: Logical reasoning is essential for large language models (LLMs) to ensure accurate and coherent inference. However, LLMs struggle with reasoning order variations and fail to generalize across logically equivalent transformations. LLMs often rely on fixed sequential patterns rather than true logical understanding. To address this issue, we introduce an order-centric data augmentation framework based on commutativity in logical reasoning. We first randomly shuffle independent premises to introduce condition order augmentation. For reasoning steps, we construct a directed acyclic graph (DAG) to model dependencies between steps, which allows us to identify valid reorderings of steps while preserving logical correctness. By leveraging order-centric augmentations, models can develop a more flexible and generalized reasoning process. Finally, we conduct extensive experiments across multiple logical reasoning benchmarks, demonstrating that our method significantly enhances LLMs' reasoning performance and adaptability to diverse logical structures. We release our codes and augmented data in https://github.com/qianxiHe147/Order-Centric-Data-Augmentation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual Knowledge Transfer</title>
<link>https://arxiv.org/abs/2502.21228</link>
<guid>https://arxiv.org/abs/2502.21228</guid>
<content:encoded><![CDATA[

arXiv:2502.21228v3 Announce Type: replace 
Abstract: To achieve equitable performance across languages, large language models (LLMs) must be able to abstract knowledge beyond the language in which it was learnt. However, the current literature lacks reliable ways to measure LLMs' capability of such cross-lingual knowledge transfer. To that end, we present ECLeKTic, a multilingual closed-book QA dataset that Evaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. Concretely, we used the presence and absence of Wikipedia articles in 12 languages to detect pieces of information that were likely available during pre-training in one of the languages but not in the others. We curate ECLeKTic as a set of fact-seeking questions over this kind of information, in all the different languages. Therefore, in order to solve ECLeKTic the model is required to transfer knowledge between languages. We evaluated 8 LLMs and showed that current SOTA models struggle to effectively share knowledge across languages, even if they can predict the answer for questions in the language in which the knowledge was acquired.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Human-LLM Representation Alignment: A Case Study on Affective Sentence Generation for Augmentative and Alternative Communication</title>
<link>https://arxiv.org/abs/2503.11881</link>
<guid>https://arxiv.org/abs/2503.11881</guid>
<content:encoded><![CDATA[

arXiv:2503.11881v3 Announce Type: replace 
Abstract: Gaps arise between a language model's use of concepts and people's expectations. This gap is critical when LLMs generate text to help people communicate via Augmentative and Alternative Communication (AAC) tools. In this work, we introduce the evaluation task of Representation Alignment for measuring this gap via human judgment. In our study, we expand keywords and emotion representations into full sentences. We select four emotion representations: Words, Valence-Arousal-Dominance (VAD) dimensions expressed in both Lexical and Numeric forms, and Emojis. In addition to Representation Alignment, we also measure people's judgments of the accuracy and realism of the generated sentences. While representations like VAD break emotions into easy-to-compute components, our findings show that people agree more with how LLMs generate when conditioned on English words (e.g., "angry") rather than VAD scales. This difference is especially visible when comparing Numeric VAD to words. Furthermore, we found that the perception of how much a generated sentence conveys an emotion is dependent on both the representation type and which emotion it is.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1, DeepSeek-R1, and Beyond</title>
<link>https://arxiv.org/abs/2503.16040</link>
<guid>https://arxiv.org/abs/2503.16040</guid>
<content:encoded><![CDATA[

arXiv:2503.16040v2 Announce Type: replace 
Abstract: Recent advances in test-time scaling of large language models (LLMs), exemplified by DeepSeek-R1 and OpenAI's o1, show that extending the chain of thought during inference can significantly improve general reasoning performance. However, the impact of this paradigm on legal reasoning remains insufficiently explored. To address this gap, we present the first systematic evaluation of 12 LLMs, including both reasoning-focused and general-purpose models, across 17 Chinese and English legal tasks spanning statutory and case-law traditions. In addition, we curate a bilingual chain-of-thought dataset for legal reasoning through distillation from DeepSeek-R1 and develop Legal-R1, an open-source model specialized for the legal domain. Experimental results show that Legal-R1 delivers competitive performance across diverse tasks. DeepSeek-R1 exhibits clear advantages in Chinese legal reasoning, while OpenAI's o1 achieves comparable results on English tasks. We further conduct a detailed error analysis, which reveals recurring issues such as outdated legal knowledge, limited capacity for legal interpretation, and susceptibility to factual hallucinations. These findings delineate the main obstacles confronting legal-domain LLMs and suggest promising directions for future research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Consistency of Multilingual Context Utilization in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.00597</link>
<guid>https://arxiv.org/abs/2504.00597</guid>
<content:encoded><![CDATA[

arXiv:2504.00597v4 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) with large language models (LLMs) has demonstrated strong performance in multilingual question-answering (QA) tasks by leveraging relevant passages retrieved from corpora. In multilingual RAG (mRAG), the retrieved passages can be written in languages other than that of the query entered by the user, making it challenging for LLMs to effectively utilize the provided information. Recent research suggests that retrieving passages from multilingual corpora can improve RAG performance, particularly for low-resource languages. However, the extent to which LLMs can leverage different kinds of multilingual contexts to generate accurate answers, *independently from retrieval quality*, remains understudied. In this paper, we conduct an extensive assessment of LLMs' ability to (i) make consistent use of a relevant passage regardless of its language, (ii) respond in the expected language, and (iii) focus on the relevant passage even when multiple `distracting' passages in different languages are provided in the context. Our experiments with four LLMs across three QA datasets covering a total of 48 languages reveal a surprising ability of LLMs to extract the relevant information from passages in a different language than the query, but a much weaker ability to formulate a full answer in the correct language. Our analysis, based on both accuracy and feature attribution techniques, further shows that distracting passages negatively impact answer quality regardless of their language. However, distractors in the query language exert a slightly stronger influence. Taken together, our findings deepen the understanding of how LLMs utilize context in mRAG systems, providing directions for future improvements.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence</title>
<link>https://arxiv.org/abs/2504.02904</link>
<guid>https://arxiv.org/abs/2504.02904</guid>
<content:encoded><![CDATA[

arXiv:2504.02904v3 Announce Type: replace 
Abstract: Post-training is essential for the success of large language models (LLMs), transforming pre-trained base models into more useful and aligned post-trained models. While plenty of works have studied post-training algorithms and evaluated post-training models by their outputs, it remains understudied how post-training reshapes LLMs internally. In this paper, we compare base and post-trained LLMs mechanistically from four perspectives to better understand post-training effects. Our findings across model families and datasets reveal that: (1) Post-training does not change the factual knowledge storage locations, and it adapts knowledge representations from the base model while developing new knowledge representations; (2) Both truthfulness and refusal can be represented by vectors in the hidden representation space. The truthfulness direction is highly similar between the base and post-trained model, and it is effectively transferable for interventions; (3) The refusal direction is different between the base and post-trained models, and it shows limited forward transferability; (4) Differences in confidence between the base and post-trained models cannot be attributed to entropy neurons. Our study provides insights into the fundamental mechanisms preserved and altered during post-training, facilitates downstream tasks like model steering, and could potentially benefit future research in interpretability and LLM post-training. Our code is publicly available at https://github.com/HZD01/post-training-mechanistic-analysis.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering</title>
<link>https://arxiv.org/abs/2505.02311</link>
<guid>https://arxiv.org/abs/2505.02311</guid>
<content:encoded><![CDATA[

arXiv:2505.02311v2 Announce Type: replace 
Abstract: The collaborative paradigm of large and small language models (LMs) effectively balances performance and cost, yet its pivotal challenge lies in precisely pinpointing the moment of invocation when hallucinations arise in small LMs. Previous optimization efforts primarily focused on post-processing techniques, which were separate from the reasoning process of LMs, resulting in high computational costs and limited effectiveness. In this paper, we propose a practical invocation evaluation metric called AttenHScore, which calculates the accumulation and propagation of hallucinations during the generation process of small LMs, continuously amplifying potential reasoning errors. By dynamically adjusting the detection threshold, we achieve more accurate real-time invocation of large LMs. Additionally, considering the limited reasoning capacity of small LMs, we leverage uncertainty-aware knowledge reorganization to assist them better capture critical information from different text chunks. Extensive experiments reveal that our AttenHScore outperforms most baselines in enhancing real-time hallucination detection capabilities across multiple QA datasets, especially when addressing complex queries. Moreover, our strategies eliminate the need for additional model training and display flexibility in adapting to various transformer-based LMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atomic Consistency Preference Optimization for Long-Form Question Answering</title>
<link>https://arxiv.org/abs/2505.09039</link>
<guid>https://arxiv.org/abs/2505.09039</guid>
<content:encoded><![CDATA[

arXiv:2505.09039v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) often produce factoid hallucinations - plausible yet incorrect answers. A common mitigation strategy is model alignment, which improves factual accuracy by training on curated (factual, non-factual) pairs. However, this approach often relies on a stronger model (e.g., GPT-4) or an external knowledge base to assess factual correctness that may not always be accessible. Addressing this, we propose Atomic Consistency Preference Optimization (ACPO), a self-supervised preference-tuning method that enhances factual accuracy without external supervision. ACPO leverages atomic consistency signals (i.e., the agreement of individual facts across multiple stochastic responses) to identify high- and low-quality data pairs for model alignment. Despite being fully self-supervised, ACPO outperforms the strong supervised alignment baseline by 1.95 points averaged across Phi-3 and Llama3 on the LongFact and BioGen datasets, demonstrating its effectiveness in improving factual reliability without relying on external models or knowledge bases.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection</title>
<link>https://arxiv.org/abs/2505.13979</link>
<guid>https://arxiv.org/abs/2505.13979</guid>
<content:encoded><![CDATA[

arXiv:2505.13979v2 Announce Type: replace 
Abstract: Multimodal models play a key role in empathy detection, but their performance can suffer when modalities provide conflicting cues. To understand these failures, we examine cases where unimodal and multimodal predictions diverge. Using fine-tuned models for text, audio, and video, along with a gated fusion model, we find that such disagreements often reflect underlying ambiguity, as evidenced by annotator uncertainty. Our analysis shows that dominant signals in one modality can mislead fusion when unsupported by others. We also observe that humans, like models, do not consistently benefit from multimodal input. These insights position disagreement as a useful diagnostic signal for identifying challenging examples and improving empathy system robustness.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Large Language Models for Detecting Mental Manipulation via Annotation-Free Data Augmentation and Anti-Curriculum Distillation</title>
<link>https://arxiv.org/abs/2505.15255</link>
<guid>https://arxiv.org/abs/2505.15255</guid>
<content:encoded><![CDATA[

arXiv:2505.15255v3 Announce Type: replace 
Abstract: Mental manipulation is a subtle yet pervasive form of psychological abuse that poses serious threats to mental health. Nevertheless, detecting mental manipulation remains a largely underexplored research problem. The field faces three major challenges: (i) insufficient and hard-to-obtain training data; (ii) the covert nature of mental manipulation, which hinders detection; and (iii) the lack of real-world datasets. To address these challenges, we propose MentalMAC, a novel framework that enhances large language models' ability to detect elements of mental manipulation in multi-turn dialogue. Our approach consists of three key components: EvoSA, an annotation-free data augmentation method based on evolutionary operations and speech act theory; teacher-model-generated multi-task supervision; and progressive task-level anti-curriculum distillation. We then constructed the ReaMent dataset, comprising 5,000 real-world dialogue samples, utilizing MentalMAC-distilled models to aid in human annotation. Vast experiments show that MentalMAC achieves up to 25.9% improvement in F1mac and 8.1% in accuracy over the best-performing baseline, outperforming commercial LLMs such as GPT-4 and Claude-3.5-Sonnet. Warning: This paper contains content that may be offensive to the reader.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Model Distillation: A Temporal Difference Imitation Learning Perspective</title>
<link>https://arxiv.org/abs/2505.20335</link>
<guid>https://arxiv.org/abs/2505.20335</guid>
<content:encoded><![CDATA[

arXiv:2505.20335v2 Announce Type: replace 
Abstract: Large language models have led to significant progress across many NLP tasks, although their massive sizes often incur substantial computational costs. Distillation has become a common practice to compress these large and highly capable models into smaller, more efficient ones. Many existing language model distillation methods can be viewed as behavior cloning from the perspective of imitation learning or inverse reinforcement learning. This viewpoint has inspired subsequent studies that leverage (inverse) reinforcement learning techniques, including variations of behavior cloning and temporal difference learning methods. Rather than proposing yet another specific temporal difference method, we introduce a general framework for temporal difference-based distillation by exploiting the distributional sparsity of the teacher model. Specifically, it is often observed that language models assign most probability mass to a small subset of tokens. Motivated by this observation, we design a temporal difference learning framework that operates on a reduced action space (a subset of vocabulary), and demonstrate how practical algorithms can be derived and the resulting performance improvements.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Text-based Protein Understanding: Retrieval or LLM?</title>
<link>https://arxiv.org/abs/2505.20354</link>
<guid>https://arxiv.org/abs/2505.20354</guid>
<content:encoded><![CDATA[

arXiv:2505.20354v4 Announce Type: replace 
Abstract: In recent years, protein-text models have gained significant attention for their potential in protein generation and understanding. Current approaches focus on integrating protein-related knowledge into large language models through continued pretraining and multi-modal alignment, enabling simultaneous comprehension of textual descriptions and protein sequences. Through a thorough analysis of existing model architectures and text-based protein understanding benchmarks, we identify significant data leakage issues present in current benchmarks. Moreover, conventional metrics derived from natural language processing fail to accurately assess the model's performance in this domain. To address these limitations, we reorganize existing datasets and introduce a novel evaluation framework based on biological entities. Motivated by our observation, we propose a retrieval-enhanced method, which significantly outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy and efficiency in training-free scenarios. Our code and data can be seen at https://github.com/IDEA-XL/RAPM.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.24332</link>
<guid>https://arxiv.org/abs/2505.24332</guid>
<content:encoded><![CDATA[

arXiv:2505.24332v2 Announce Type: replace 
Abstract: Information seeking demands iterative evidence gathering and reflective reasoning, yet large language models (LLMs) still struggle with it in open-web question answering. Existing prompting and supervised fine-tuning (SFT) methods remain fixed by prompt rules or training corpora, and are usually benchmarked only on well-structured wiki sources, limiting real-world adaptability. We introduce WebPuzzle, a 24k-sample training and 275-sample test benchmark that evaluates information seeking on the live internet, across both wiki and open-domain queries. Leveraging 7k WebPuzzle instances, we develop DeepDiver, a reinforcement-learning (RL) framework that cultivates Search Intensity Scaling (SIS)-an emergent ability to escalate search frequency and depth instead of settling on overconfident, under-evidenced answers. With SIS, Qwen2.5-7B-Instruct and Pangu-7B-Reasoner attain performance on real-web tasks comparable to the 671B-parameter DeepSeek-R1. We detail DeepDiver's curriculum from cold-start SFT to a well designed RL procedure, and show that its seeking policy generalized from closed-ended queries to open-ended generation such as long-form writing. Our results advance adaptive information seeking in LLMs and provide a rigorous benchmark for future work.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Language Shapes Thought: Cross-Lingual Transfer of Factual Knowledge in Question Answering</title>
<link>https://arxiv.org/abs/2505.24409</link>
<guid>https://arxiv.org/abs/2505.24409</guid>
<content:encoded><![CDATA[

arXiv:2505.24409v2 Announce Type: replace 
Abstract: Multilingual large language models (LLMs) offer promising opportunities for cross-lingual information access, yet their use of factual knowledge remains highly sensitive to the input language. Prior work has addressed this through English prompting and evaluation, assuming that English-based reasoning is universally beneficial. In this work, we challenge that assumption by exploring factual knowledge transfer from non-English to English through the lens of Language and Thought Theory. We introduce Language-to-Thought (L2T) prompting, which aligns the model's internal ''thinking'' language with the source of knowledge. Across three languages and four models, L2T consistently outperforms English-based reasoning, reversing the expected advantage of English prompts. Our code is available at https://github.com/GeomeunByeol/Language2Thought.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text</title>
<link>https://arxiv.org/abs/2505.24826</link>
<guid>https://arxiv.org/abs/2505.24826</guid>
<content:encoded><![CDATA[

arXiv:2505.24826v2 Announce Type: replace 
Abstract: As large language models (LLMs) are increasingly used in legal applications, current evaluation benchmarks tend to focus mainly on factual accuracy while largely neglecting important linguistic quality aspects such as clarity, coherence, and terminology. To address this gap, we propose three steps: First, we develop a regression model to evaluate the quality of legal texts based on clarity, coherence, and terminology. Second, we create a specialized set of legal questions. Third, we analyze 49 LLMs using this evaluation framework.
  Our analysis identifies three key findings: First, model quality levels off at 14 billion parameters, with only a marginal improvement of $2.7\%$ noted at 72 billion parameters. Second, engineering choices such as quantization and context length have a negligible impact, as indicated by statistical significance thresholds above 0.016. Third, reasoning models consistently outperform base architectures. A significant outcome of our research is the release of a ranking list and Pareto analysis, which highlight the Qwen3 series as the optimal choice for cost-performance tradeoffs. This work not only establishes standardized evaluation protocols for legal LLMs but also uncovers fundamental limitations in current training data refinement approaches. Code and models are available at: https://github.com/lyxx3rd/LegalEval-Q.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics</title>
<link>https://arxiv.org/abs/2506.12618</link>
<guid>https://arxiv.org/abs/2506.12618</guid>
<content:encoded><![CDATA[

arXiv:2506.12618v2 Announce Type: replace 
Abstract: Robust unlearning is crucial for safely deploying large language models (LLMs) in environments where data privacy, model safety, and regulatory compliance must be ensured. Yet the task is inherently challenging, partly due to difficulties in reliably measuring whether unlearning has truly occurred. Moreover, fragmentation in current methodologies and inconsistent evaluation metrics hinder comparative analysis and reproducibility. To unify and accelerate research efforts, we introduce OpenUnlearning, a standardized and extensible framework designed explicitly for benchmarking both LLM unlearning methods and metrics. OpenUnlearning integrates 13 unlearning algorithms and 16 diverse evaluations across 3 leading benchmarks (TOFU, MUSE, and WMDP) and also enables analyses of forgetting behaviors across 450+ checkpoints we publicly release. Leveraging OpenUnlearning, we propose a novel meta-evaluation benchmark focused specifically on assessing the faithfulness and robustness of evaluation metrics themselves. We also benchmark diverse unlearning methods and provide a comparative analysis against an extensive evaluation suite. Overall, we establish a clear, community-driven pathway toward rigorous development in LLM unlearning research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning with Exploration: An Entropy Perspective</title>
<link>https://arxiv.org/abs/2506.14758</link>
<guid>https://arxiv.org/abs/2506.14758</guid>
<content:encoded><![CDATA[

arXiv:2506.14758v4 Announce Type: replace 
Abstract: Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing large language model (LLM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LLMs. Through empirical analysis, we uncover positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LLMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LLM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LLM reasoning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations</title>
<link>https://arxiv.org/abs/2506.16678</link>
<guid>https://arxiv.org/abs/2506.16678</guid>
<content:encoded><![CDATA[

arXiv:2506.16678v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit a robust mastery of syntax when processing and generating text. While this suggests internalized understanding of hierarchical syntax and dependency relations, the precise mechanism by which they represent syntactic structure is an open area within interpretability research. Probing provides one way to identify the mechanism of syntax being linearly encoded in activations, however, no comprehensive study has yet established whether a model's probing accuracy reliably predicts its downstream syntactic performance. Adopting a "mechanisms vs. outcomes" framework, we evaluate 32 open-weight transformer models and find that syntactic features extracted via probing fail to predict outcomes of targeted syntax evaluations across English linguistic phenomena. Our results highlight a substantial disconnect between latent syntactic representations found via probing and observable syntactic behaviors in downstream tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReCode: Updating Code API Knowledge with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.20495</link>
<guid>https://arxiv.org/abs/2506.20495</guid>
<content:encoded><![CDATA[

arXiv:2506.20495v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DP-Fusion: Token-Level Differentially Private Inference for Large Language Models</title>
<link>https://arxiv.org/abs/2507.04531</link>
<guid>https://arxiv.org/abs/2507.04531</guid>
<content:encoded><![CDATA[

arXiv:2507.04531v3 Announce Type: replace 
Abstract: Large language models (LLMs) do not preserve privacy at inference-time. The LLM's outputs can inadvertently reveal information about the model's context, which presents a privacy challenge when the LLM is augmented via tools or databases containing sensitive information. Existing privacy-preserving methods at inference-time have significant limitations since they (i) lack provable guarantees or (ii) have a poor utility/privacy trade-off. We propose DP-Fusion, a Differentially Private Inference (DPI) mechanism for LLMs that provably bounds the influence a set of tokens in the context can have on the LLM's output. DP-Fusion works as follows: (1) label a subset of sensitive tokens, (2) infer the LLM without any sensitive tokens to obtain a baseline, (3) infer the LLM with the sensitive tokens, and (4) blend distributions so that the final output remains within a bounded distance of the baseline distribution. While this per-token influence bound also mitigates jailbreak-style prompt injection, we focus on \emph{document privatization}, where the goal is to paraphrase a document containing sensitive tokens, e.g., personally identifiable information, so that no attacker can reliably infer them from the paraphrased document while preserving high text quality. The privacy/utility trade-off is controlled by $\epsilon$, where $\epsilon=0$ hides sensitive tokens entirely, while higher values trade off privacy for improved text quality. We show that our method creates token-level provably privatized documents with substantially improved theoretical and empirical privacy, achieving $6\times$ lower perplexity than related DPI methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?</title>
<link>https://arxiv.org/abs/2507.05639</link>
<guid>https://arxiv.org/abs/2507.05639</guid>
<content:encoded><![CDATA[

arXiv:2507.05639v2 Announce Type: replace 
Abstract: In this paper, we introduce ECom-Bench, the first benchmark framework for evaluating LLM agent with multimodal capabilities in the e-commerce customer support domain. ECom-Bench features dynamic user simulation based on persona information collected from real e-commerce customer interactions and a realistic task dataset derived from authentic e-commerce dialogues. These tasks, covering a wide range of business scenarios, are designed to reflect real-world complexities, making ECom-Bench highly challenging. For instance, even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our benchmark, highlighting the substantial difficulties posed by complex e-commerce scenarios. The code and data have been made publicly available at https://github.com/XiaoduoAILab/ECom-Bench to facilitate further research and development in this domain.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited</title>
<link>https://arxiv.org/abs/2507.12059</link>
<guid>https://arxiv.org/abs/2507.12059</guid>
<content:encoded><![CDATA[

arXiv:2507.12059v2 Announce Type: replace 
Abstract: We investigate the abilities of 28 Large language Models (LLMs) to reason about cardinal directions (CDs) using a benchmark generated from a set of templates, extensively testing an LLM's ability to determine the correct CD given a particular scenario. The templates allow for a number of degrees of variation such as means of locomotion of the agent involved, and whether set in the first, second or third person. Even the newer Large Reasoning Models are unable to reliably determine the correct CD for all questions. This paper summarises and extends earlier work presented at COSIT-24.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMBRACE: Shaping Inclusive Opinion Representation by Aligning Implicit Conversations with Social Norms</title>
<link>https://arxiv.org/abs/2507.20264</link>
<guid>https://arxiv.org/abs/2507.20264</guid>
<content:encoded><![CDATA[

arXiv:2507.20264v2 Announce Type: replace 
Abstract: Shaping inclusive representations that embrace diversity and ensure fair participation and reflections of values is at the core of many conversation-based models. However, many existing methods rely on surface inclusion using mention of user demographics or behavioral attributes of social groups. Such methods overlook the nuanced, implicit expression of opinion embedded in conversations. Furthermore, the over-reliance on overt cues can exacerbate misalignment and reinforce harmful or stereotypical representations in model outputs. Thus, we took a step back and recognized that equitable inclusion needs to account for the implicit expression of opinion and use the stance of responses to validate the normative alignment. This study aims to evaluate how opinions are represented in NLP or computational models by introducing an alignment evaluation framework that foregrounds implicit, often overlooked conversations and evaluates the normative social views and discourse. Our approach models the stance of responses as a proxy for the underlying opinion, enabling a considerate and reflective representation of diverse social viewpoints. We evaluate the framework using both (i) positive-unlabeled (PU) online learning with base classifiers, and (ii) instruction-tuned language models to assess post-training alignment. Through this, we provide a principled and structured lens on how implicit opinions are (mis)represented and offer a pathway toward more inclusive model behavior.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases</title>
<link>https://arxiv.org/abs/2507.21652</link>
<guid>https://arxiv.org/abs/2507.21652</guid>
<content:encoded><![CDATA[

arXiv:2507.21652v2 Announce Type: replace 
Abstract: As large reasoning models (LRMs) grow more capable, chain-of-thought (CoT) reasoning introduces new safety challenges. Existing SFT-based safety alignment studies dominantly focused on filtering prompts with safe, high-quality responses, while overlooking hard prompts that always elicit harmful outputs. To fill this gap, we introduce UnsafeChain, a safety alignment dataset constructed from hard prompts with diverse sources, where unsafe completions are identified and explicitly corrected into safe responses. By exposing models to unsafe behaviors and guiding their correction, UnsafeChain enhances safety while preserving general reasoning ability. We fine-tune three LRMs on UnsafeChain and compare them against recent SafeChain and STAR-1 across six out-of-distribution and five in-distribution benchmarks. UnsafeChain consistently outperforms prior datasets, with even a 1K subset matching or surpassing baseline performance, demonstrating the effectiveness and generalizability of correction-based supervision. We release our dataset and code at https://github.com/mbzuai-nlp/UnsafeChain
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications</title>
<link>https://arxiv.org/abs/2508.01710</link>
<guid>https://arxiv.org/abs/2508.01710</guid>
<content:encoded><![CDATA[

arXiv:2508.01710v4 Announce Type: replace 
Abstract: The increasing use of Large Language Models (LLMs) in agentic applications highlights the need for robust safety guard models. While content safety in English is well-studied, non-English languages lack similar advancements due to the high cost of collecting culturally aligned labeled datasets. We present CultureGuard, a novel solution for curating culturally aligned, high-quality safety datasets across multiple languages. Our approach introduces a four-stage synthetic data generation and filtering pipeline: cultural data segregation, cultural data adaptation, machine translation, and quality filtering. This pipeline enables the conversion and expansion of the Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese. The resulting dataset, Nemotron-Safety-Guard-Dataset-v3, comprises 386,661 samples in 9 languages and facilitates the training of Llama-3.1-Nemotron-Safety-Guard-8B-v3 via LoRA-based fine-tuning. The final model achieves state-of-the-art performance on several multilingual content safety benchmarks. Furthermore, we show our moderately multilingual fine-tuning enables robust cross-lingual transfer and strong zero-shot generalization to unseen languages. We also benchmark the latest open LLMs on multilingual safety and observe that these LLMs are more prone to give unsafe responses when prompted in non-English languages. This work advances multilingual LLM safety by enabling the development of culturally aware safety guard models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating, Synthesizing, and Enhancing for Customer Support Conversation</title>
<link>https://arxiv.org/abs/2508.04423</link>
<guid>https://arxiv.org/abs/2508.04423</guid>
<content:encoded><![CDATA[

arXiv:2508.04423v2 Announce Type: replace 
Abstract: Effective customer support requires not only accurate problem solving but also structured and empathetic communication aligned with professional standards. However, existing dialogue datasets often lack strategic guidance, and real-world service data is difficult to access and annotate. To address this, we introduce the task of Customer Support Conversation (CSC), aimed at training customer service agents to respond using well-defined support strategies. We propose a structured CSC framework grounded in COPC guidelines, defining five conversational stages and twelve strategies to guide high-quality interactions. Based on this, we construct CSConv, an evaluation dataset of 1,855 real-world customer-agent conversations rewritten using LLMs to reflect deliberate strategy use, and annotated accordingly. Additionally, we develop a role-playing approach that simulates strategy-rich conversations using LLM-powered roles aligned with the CSC framework, resulting in the training dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses on CSConv. Human evaluations further confirm gains in problem resolution. All code and data will be made publicly available at https://github.com/aliyun/qwen-dianjin.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.05100</link>
<guid>https://arxiv.org/abs/2508.05100</guid>
<content:encoded><![CDATA[

arXiv:2508.05100v2 Announce Type: replace 
Abstract: With the rapid advancement of large language models (LLMs), retrieval-augmented generation (RAG) has emerged as a critical approach to supplement the inherent knowledge limitations of LLMs. However, due to the typically large volume of retrieved information, RAG tends to operate with long context lengths. From the perspective of entropy engineering, we identify unconstrained entropy growth and attention dilution due to long retrieval context as significant factors affecting RAG performance. In this paper, we propose the balanced entropy-engineered RAG (BEE-RAG) framework, which improves the adaptability of RAG systems to varying context lengths through the principle of entropy invariance. By leveraging balanced context entropy to reformulate attention dynamics, BEE-RAG separates attention sensitivity from context length, ensuring a stable entropy level. Building upon this, we introduce a zero-shot inference strategy for multi-importance estimation and a parameter-efficient adaptive fine-tuning mechanism to obtain the optimal balancing factor for different settings. Extensive experiments across multiple RAG tasks demonstrate the effectiveness of BEE-RAG.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations</title>
<link>https://arxiv.org/abs/2508.05470</link>
<guid>https://arxiv.org/abs/2508.05470</guid>
<content:encoded><![CDATA[

arXiv:2508.05470v2 Announce Type: replace 
Abstract: We systematically examine, analyze, and compare representative creativity measures--creativity index, perplexity, syntactic templates, and LLM-as-a-Judge--across diverse creative domains, including creative writing, unconventional problem-solving, and research ideation. Our analyses reveal that these metrics exhibit limited consistency, capturing different dimensions of creativity. We highlight key limitations, including the creativity index's focus on lexical diversity, perplexity's sensitivity to model confidence, and syntactic templates' inability to capture conceptual creativity. Additionally, LLM-as-a-Judge shows instability and bias. Our findings underscore the need for more robust, generalizable evaluation frameworks that better align with human judgments of creativity.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Does a Deep Neural Network Look at Lexical Stress?</title>
<link>https://arxiv.org/abs/2508.07229</link>
<guid>https://arxiv.org/abs/2508.07229</guid>
<content:encoded><![CDATA[

arXiv:2508.07229v2 Announce Type: replace 
Abstract: Despite their success in speech processing, neural networks often operate as black boxes, prompting the question: what informs their decisions, and how can we interpret them? This work examines this issue in the context of lexical stress. A dataset of English disyllabic words was automatically constructed from read and spontaneous speech. Several Convolutional Neural Network (CNN) architectures were trained to predict stress position from a spectrographic representation of disyllabic words lacking minimal stress pairs (e.g., initial stress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out test data. Layerwise Relevance Propagation (LRP), a technique for CNN interpretability analysis, revealed that predictions for held-out minimal pairs (PROtest vs. proTEST ) were most strongly influenced by information in stressed versus unstressed syllables, particularly the spectral properties of stressed vowels. However, the classifiers also attended to information throughout the word. A feature-specific relevance analysis is proposed, and its results suggest that our best-performing classifier is strongly influenced by the stressed vowel's first and second formants, with some evidence that its pitch and third formant also contribute. These results reveal deep learning's ability to acquire distributed cues to stress from naturally occurring data, extending traditional phonetic work based around highly controlled stimuli.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Tokenization for Rich Morphology: The Dominance of Unigram over BPE and Morphological Alignment</title>
<link>https://arxiv.org/abs/2508.08424</link>
<guid>https://arxiv.org/abs/2508.08424</guid>
<content:encoded><![CDATA[

arXiv:2508.08424v3 Announce Type: replace 
Abstract: The relationship between tokenizer algorithm (e.g., Byte-Pair Encoding (BPE), Unigram), morphological alignment, tokenization quality (e.g., compression efficiency), and downstream performance remains largely unclear, particularly for languages with complex morphology. In this paper, we conduct a comprehensive evaluation of tokenizers using small-sized BERT models -- from pre-training through fine-tuning -- for Telugu (agglutinative), along with preliminary evaluation in Hindi (primarily fusional with some agglutination) and English (fusional). To evaluate morphological alignment of tokenizers in Telugu, we create a dataset containing gold morpheme segmentations of 600 derivational and 7000 inflectional word forms.
  Our experiments reveal two key findings for Telugu. First, the choice of tokenizer algorithm is the most significant factor influencing performance, with Unigram-based tokenizers consistently outperforming BPE across most settings. Second, while better morphological alignment shows a moderate, positive correlation with performance on text classification and structure prediction tasks, its impact is secondary to the tokenizer algorithm. Notably, hybrid approaches that use morphological information for pre-segmentation significantly boost the performance of BPE, though not Unigram. Our results further showcase the need for comprehensive intrinsic evaluation metrics for tokenizers that could explain downstream performance trends consistently.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Utilizing Multilingual Encoders to Improve Large Language Models for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2508.09091</link>
<guid>https://arxiv.org/abs/2508.09091</guid>
<content:encoded><![CDATA[

arXiv:2508.09091v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel in English, but their performance degrades significantly on low-resource languages (LRLs) due to English-centric training. While methods like LangBridge align LLMs with multilingual encoders such as the Massively Multilingual Text-to-Text Transfer Transformer (mT5), they typically use only the final encoder layer. We propose a novel architecture that fuses all intermediate layers, enriching the linguistic information passed to the LLM. Our approach features two strategies: (1) a Global Softmax weighting for overall layer importance, and (2) a Transformer Softmax model that learns token-specific weights. The fused representations are mapped into the LLM's embedding space, enabling it to process multilingual inputs. The model is trained only on English data, without using any parallel or multilingual data. Evaluated on XNLI, IndicXNLI, Sinhala News Classification, and Amazon Reviews, our Transformer Softmax model significantly outperforms the LangBridge baseline. We observe strong performance gains in LRLs, improving Sinhala classification accuracy from 71.66% to 75.86% and achieving clear improvements across Indic languages such as Tamil, Bengali, and Malayalam. These specific gains contribute to an overall boost in average XNLI accuracy from 70.36% to 71.50%. This approach offers a scalable, data-efficient path toward more capable and equitable multilingual LLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SinLlama -- A Large Language Model for Sinhala</title>
<link>https://arxiv.org/abs/2508.09115</link>
<guid>https://arxiv.org/abs/2508.09115</guid>
<content:encoded><![CDATA[

arXiv:2508.09115v4 Announce Type: replace 
Abstract: Low-resource languages such as Sinhala are often overlooked by open-source Large Language Models (LLMs). In this research, we extend an existing multilingual LLM (Llama-3-8B) to better serve Sinhala. We enhance the LLM tokenizer with Sinhala specific vocabulary and perform continual pre-training on a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This is the very first decoder-based open-source LLM with explicit Sinhala support. When SinLlama was instruction fine-tuned for three text classification tasks, it outperformed base and instruct variants of Llama-3-8B by a significant margin.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMCARE: early detection of cognitive impairment via transformer models enhanced by LLM-generated synthetic data</title>
<link>https://arxiv.org/abs/2508.10027</link>
<guid>https://arxiv.org/abs/2508.10027</guid>
<content:encoded><![CDATA[

arXiv:2508.10027v3 Announce Type: replace 
Abstract: Alzheimer's disease and related dementias(ADRD) affect nearly five million older adults in the United States, yet more than half remain undiagnosed. Speech-based natural language processing(NLP) offers a scalable approach for detecting early cognitive decline through subtle linguistic markers that may precede clinical diagnosis. This study develops and evaluates a speech-based screening pipeline integrating transformer embeddings with handcrafted linguistic features, synthetic augmentation using large language models(LLMs), and benchmarking of unimodal and multimodal classifiers. External validation assessed generalizability to a MCI-only cohort.
  Transcripts were drawn from the ADReSSo 2021 benchmark dataset(n=237, Pitt Corpus) and the DementiaBank Delaware corpus(n=205, MCI vs. controls). Ten transformer models were tested under three fine-tuning strategies. A late-fusion model combined embeddings from the top transformer with 110 linguistic features. Five LLMs(LLaMA8B/70B, MedAlpaca7B, Ministral8B,GPT-4o) generated label-conditioned synthetic speech for augmentation, and three multimodal LLMs(GPT-4o,Qwen-Omni,Phi-4) were evaluated in zero-shot and fine-tuned modes. On ADReSSo, the fusion model achieved F1=83.3(AUC=89.5), outperforming transformer-only and linguistic baselines. MedAlpaca7B augmentation(2x) improved F1=85.7, though larger scales reduced gains. Fine-tuning boosted unimodal LLMs(MedAlpaca7B F1=47.7=>78.7), while multimodal models performed lower (Phi-4=71.6;GPT-4o=67.6). On Delaware, the fusion plus 1x MedAlpaca7B model achieved F1=72.8(AUC=69.6). Integrating transformer and linguistic features enhances ADRD detection. LLM-based augmentation improves data efficiency but yields diminishing returns, while current multimodal models remain limited. Validation on an independent MCI cohort supports the pipeline's potential for scalable, clinically relevant early screening.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning</title>
<link>https://arxiv.org/abs/2508.10419</link>
<guid>https://arxiv.org/abs/2508.10419</guid>
<content:encoded><![CDATA[

arXiv:2508.10419v2 Announce Type: replace 
Abstract: Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and its high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods could fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition on reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global context comprehension, offering a principled, cognitively motivated paradigm towards retrieval-based stateful reasoning. Our framework is made publicly available at https://github.com/EternityJune25/ComoRAG.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMNLP: Educator-role Moral and Normative Large Language Models Profiling</title>
<link>https://arxiv.org/abs/2508.15250</link>
<guid>https://arxiv.org/abs/2508.15250</guid>
<content:encoded><![CDATA[

arXiv:2508.15250v3 Announce Type: replace 
Abstract: Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 14 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at https://e-m-n-l-p.github.io/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs</title>
<link>https://arxiv.org/abs/2508.20325</link>
<guid>https://arxiv.org/abs/2508.20325</guid>
<content:encoded><![CDATA[

arXiv:2508.20325v2 Announce Type: replace 
Abstract: As Large Language Models become increasingly integral to various domains, their potential to generate harmful responses has prompted significant societal and regulatory concerns. In response, governments have issued ethics guidelines to promote the development of trustworthy AI. However, these guidelines are typically high-level demands for developers and testers, leaving a gap in translating them into actionable testing questions to verify LLM compliance.
  To address this challenge, we introduce GUARD (\textbf{G}uideline \textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize guidelines into specific guideline-violating questions that assess LLM adherence. To implement this, GUARD uses automated generation of guideline-violating questions based on government-issued guidelines, thereby testing whether responses comply with these guidelines. When responses directly violate guidelines, GUARD reports inconsistencies. Furthermore, for responses that do not directly violate guidelines, GUARD integrates the concept of ``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that provoke unethical or guideline-violating responses, effectively identifying potential scenarios that could bypass built-in safety mechanisms. Our method finally culminates in a compliance report, delineating the extent of adherence and highlighting any violations.
  We have empirically validated the effectiveness of GUARD on seven LLMs, including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4, GPT-4o, and Claude-3.7, by testing compliance under three government-issued guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can transfer jailbreak diagnostics to vision-language models, demonstrating its usage in promoting reliable LLM-based applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement</title>
<link>https://arxiv.org/abs/2508.20916</link>
<guid>https://arxiv.org/abs/2508.20916</guid>
<content:encoded><![CDATA[

arXiv:2508.20916v2 Announce Type: replace 
Abstract: Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to natural human-computer interaction, enabling end-to-end spoken dialogue systems. However, evaluating these models remains a fundamental challenge. We propose \texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech LLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches that disregard acoustic features, SageLM jointly assesses both semantic and acoustic dimensions. Second, it leverages rationale-based supervision to enhance explainability and guide model learning, achieving superior alignment with evaluation outcomes compared to rule-based reinforcement learning methods. Third, we introduce \textit{SpeechFeedback}, a synthetic preference dataset, and employ a two-stage training paradigm to mitigate the scarcity of speech preference data. Trained on both semantic and acoustic dimensions, SageLM achieves an 82.79\% agreement rate with human evaluators, outperforming cascaded and SLM-based baselines by at least 7.42\% and 26.20\%, respectively.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Normality and the Turing Test</title>
<link>https://arxiv.org/abs/2508.21382</link>
<guid>https://arxiv.org/abs/2508.21382</guid>
<content:encoded><![CDATA[

arXiv:2508.21382v2 Announce Type: replace 
Abstract: This paper proposes to revisit the Turing test through the concept of normality. Its core argument is that the Turing test is a test of normal intelligence as assessed by a normal judge. First, in the sense that the Turing test targets normal/average rather than exceptional human intelligence, so that successfully passing the test requires machines to "make mistakes" and display imperfect behavior just like normal/average humans. Second, in the sense that the Turing test is a statistical test where judgments of intelligence are never carried out by a single "average" judge (understood as non-expert) but always by a full jury. As such, the notion of "average human interrogator" that Turing talks about in his original paper should be understood primarily as referring to a mathematical abstraction made of the normalized aggregate of individual judgments of multiple judges. Its conclusions are twofold. First, it argues that large language models such as ChatGPT are unlikely to pass the Turing test as those models precisely target exceptional rather than normal/average human intelligence. As such, they constitute models of what it proposes to call artificial smartness rather than artificial intelligence, insofar as they deviate from the original goal of Turing for the modeling of artificial minds. Second, it argues that the objectivization of normal human behavior in the Turing test fails due to the game configuration of the test which ends up objectivizing normative ideals of normal behavior rather than normal behavior per se.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning</title>
<link>https://arxiv.org/abs/2509.13624</link>
<guid>https://arxiv.org/abs/2509.13624</guid>
<content:encoded><![CDATA[

arXiv:2509.13624v2 Announce Type: replace 
Abstract: Large language models are increasingly deployed across diverse applications. This often includes tasks LLMs have not encountered during training. This implies that enumerating and obtaining the high-quality training data for all tasks is infeasible. Thus, we often need to rely on transfer learning using datasets with different characteristics, and anticipate out-of-distribution requests. Motivated by this practical need, we propose an analysis framework, building a transfer learning matrix and dimensionality reduction, to dissect these cross-task interactions. We train and analyze 10 models to identify latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic) and discover the side effects of the transfer learning. Our findings reveal that performance improvements often defy explanations based on surface-level dataset similarity or source data quality. Instead, hidden statistical factors of the source dataset, such as class distribution and generation length proclivities, alongside specific linguistic features, are actually more influential. This work offers insights into the complex dynamics of transfer learning, paving the way for more predictable and effective LLM adaptation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness of Neurosymbolic Reasoners on First-Order Logic Problems</title>
<link>https://arxiv.org/abs/2509.17377</link>
<guid>https://arxiv.org/abs/2509.17377</guid>
<content:encoded><![CDATA[

arXiv:2509.17377v2 Announce Type: replace 
Abstract: Recent trends in NLP aim to improve reasoning capabilities in Large Language Models (LLMs), with key focus on generalization and robustness to variations in tasks. Counterfactual task variants introduce minimal but semantically meaningful changes to otherwise valid first-order logic (FOL) problem instances altering a single predicate or swapping roles of constants to probe whether a reasoning system can maintain logical consistency under perturbation. Previous studies showed that LLMs becomes brittle on counterfactual variations, suggesting that they often rely on spurious surface patterns to generate responses. In this work, we explore if a neurosymbolic (NS) approach that integrates an LLM and a symbolic logical solver could mitigate this problem. Experiments across LLMs of varying sizes show that NS methods are more robust but perform worse overall that purely neural methods. We then propose NSCoT that combines an NS method and Chain-of-Thought (CoT) prompting and demonstrate that while it improves performance, NSCoT still lags behind standard CoT. Our analysis opens research directions for future work.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EditGRPO: Reinforcement Learning with Post-Rollout Edits for Clinically Accurate Chest X-Ray Report Generation</title>
<link>https://arxiv.org/abs/2509.22812</link>
<guid>https://arxiv.org/abs/2509.22812</guid>
<content:encoded><![CDATA[

arXiv:2509.22812v2 Announce Type: replace 
Abstract: Radiology report generation requires advanced medical image analysis, effective temporal reasoning, and accurate text generation. Although recent innovations, particularly multimodal large language models, have shown improved performance, their supervised fine-tuning (SFT) objective is not explicitly aligned with clinical efficacy. In this work, we introduce EditGRPO, a mixed-policy reinforcement learning algorithm designed specifically to optimize the generation through clinically motivated rewards. EditGRPO integrates on-policy exploration with off-policy guidance by injecting sentence-level detailed corrections during training rollouts. This mixed-policy approach addresses the exploration dilemma and sampling efficiency issues typically encountered in RL. Applied to a Qwen2.5-VL-3B, EditGRPO outperforms both SFT and vanilla GRPO baselines, achieving an average improvement of 3.4\% in clinical metrics across four major datasets. Notably, EditGRPO also demonstrates superior out-of-domain generalization, with an average performance gain of 5.9\% on unseen datasets.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale Corpora</title>
<link>https://arxiv.org/abs/2510.10114</link>
<guid>https://arxiv.org/abs/2510.10114</guid>
<content:encoded><![CDATA[

arXiv:2510.10114v4 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) is widely used to mitigate hallucinations of Large Language Models (LLMs) by leveraging external knowledge. While effective for simple queries, traditional RAG systems struggle with large-scale, unstructured corpora where information is fragmented. Recent advances incorporate knowledge graphs to capture relational structures, enabling more comprehensive retrieval for complex, multi-hop reasoning tasks. However, existing graph-based RAG (GraphRAG) methods rely on unstable and costly relation extraction for graph construction, often producing noisy graphs with incorrect or inconsistent relations that degrade retrieval quality. In this paper, we revisit the pipeline of existing GraphRAG systems and propose LinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient framework that enables reliable graph construction and precise passage retrieval. Specifically, LinearRAG constructs a relation-free hierarchical graph, termed Tri-Graph, using only lightweight entity extraction and semantic linking, avoiding unstable relation modeling. This new paradigm of graph construction scales linearly with corpus size and incurs no extra token consumption, providing an economical and reliable indexing of the original passages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant entity activation via local semantic bridging, followed by (ii) passage retrieval through global importance aggregation. Extensive experiments on four datasets demonstrate that LinearRAG significantly outperforms baseline models. Our code and datasets are available at https://github.com/DEEP-PolyU/LinearRAG.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations</title>
<link>https://arxiv.org/abs/2510.11196</link>
<guid>https://arxiv.org/abs/2510.11196</guid>
<content:encoded><![CDATA[

arXiv:2510.11196v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) often produce chain-of-thought (CoT) explanations that sound plausible yet fail to reflect the underlying decision process, undermining trust in high-stakes clinical use. Existing evaluations rarely catch this misalignment, prioritizing answer accuracy or adherence to formats. We present a clinically grounded framework for chest X-ray visual question answering (VQA) that probes CoT faithfulness via controlled text and image modifications across three axes: clinical fidelity, causal attribution, and confidence calibration. In a reader study (n=4), evaluator-radiologist correlations fall within the observed inter-radiologist range for all axes, with strong alignment for attribution (Kendall's $\tau_b=0.670$), moderate alignment for fidelity ($\tau_b=0.387$), and weak alignment for confidence tone ($\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows that answer accuracy and explanation quality can be decoupled, acknowledging injected cues does not ensure grounding, and text cues shift explanations more than visual cues. While some open-source models match final answer accuracy, proprietary models score higher on attribution (25.0% vs. 1.4%) and often on fidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to evaluate beyond final answer accuracy.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPLoRA: Orthogonal Projection LoRA Prevents Catastrophic Forgetting during Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2510.13003</link>
<guid>https://arxiv.org/abs/2510.13003</guid>
<content:encoded><![CDATA[

arXiv:2510.13003v2 Announce Type: replace 
Abstract: Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large language models but suffers from catastrophic forgetting when learned updates interfere with the dominant singular directions that encode essential pre-trained knowledge. We propose Orthogonal Projection LoRA (OPLoRA), a theoretically grounded approach that prevents this interference through double-sided orthogonal projections. By decomposing frozen weights via SVD, OPLoRA constrains LoRA updates to lie entirely within the orthogonal complement of the top-$k$ singular subspace using projections $P_L = I - U_k U_k^\top$ and $P_R = I - V_k V_k^\top$. We prove that this construction exactly preserves the top-$k$ singular triples, providing mathematical guarantees for knowledge retention. To quantify subspace interference, we introduce $\rho_k$, a metric measuring update alignment with dominant directions. Extensive experiments across commonsense reasoning, mathematics, and code generation demonstrate that OPLoRA significantly reduces forgetting while maintaining competitive task-specific performance on LLaMA-2 7B and Qwen2.5 7B, establishing orthogonal projection as an effective mechanism for knowledge preservation in parameter-efficient fine-tuning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons</title>
<link>https://arxiv.org/abs/2510.13797</link>
<guid>https://arxiv.org/abs/2510.13797</guid>
<content:encoded><![CDATA[

arXiv:2510.13797v2 Announce Type: replace 
Abstract: The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meronymic Ontology Extraction via Large Language Models</title>
<link>https://arxiv.org/abs/2510.13839</link>
<guid>https://arxiv.org/abs/2510.13839</guid>
<content:encoded><![CDATA[

arXiv:2510.13839v2 Announce Type: replace 
Abstract: Ontologies have become essential in today's digital age as a way of organising the vast amount of readily available unstructured text. In providing formal structure to this information, ontologies have immense value and application across various domains, e.g., e-commerce, where countless product listings necessitate proper product organisation. However, the manual construction of these ontologies is a time-consuming, expensive and laborious process. In this paper, we harness the recent advancements in large language models (LLMs) to develop a fully-automated method of extracting product ontologies, in the form of meronymies, from raw review texts. We demonstrate that the ontologies produced by our method surpass an existing, BERT-based baseline when evaluating using an LLM-as-a-judge. Our investigation provides the groundwork for LLMs to be used more generally in (product or otherwise) ontology extraction.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models</title>
<link>https://arxiv.org/abs/2510.13847</link>
<guid>https://arxiv.org/abs/2510.13847</guid>
<content:encoded><![CDATA[

arXiv:2510.13847v2 Announce Type: replace 
Abstract: Speculative decoding has become a standard way to accelerate LLM inference: a small drafter proposes multiple tokens and a large target model verifies them once per speculation length. Recently, scaling of the LLM vocabulary has pushed the number of tokens to grow substantially. While verification over the full vocabulary leaves the target model largely unaffected, the O(|V|d) parameters in the drafter's output head become a latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g., FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed top frequent subset of the target model's vocabulary. Although this reduces draft-time compute, it is brittle, since: (i) frequency lists are corpus-dependent and require retuning to generalize, and (ii) static shortlists suppress rare or domain-specific tokens, lowering the expected number of tokens per verification step. We propose DynaSpec, a context-dependent dynamic shortlisting mechanism that is robust, speeds up drafting, and generalizes across diverse tasks. Concretely, we introduce lightweight, coarse-grained meta-classifiers that route contexts to a small number of token clusters; the union of the top-k selected clusters forms the drafter's shortlist, while verification retains the full vocabulary and exactness. The meta-classifier finishes its computation earlier than the drafter's hidden state generation by exploiting parallel execution of draft encoding and meta shortlisting on separate streams. Across standard speculative decoding benchmarks, DynaSpec delivers consistent improvements in mean accepted length, for Llama-3-8B, reaching upto 98.2% of full-vocabulary performance, while fixed-shortlist baselines attain only 84.4%. By leveraging context-dependent selection, DynaSpec achieves up to a 2.18 times increase in generated tokens compared to 1.91 times for fixed-vocabulary approaches.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation</title>
<link>https://arxiv.org/abs/2510.19670</link>
<guid>https://arxiv.org/abs/2510.19670</guid>
<content:encoded><![CDATA[

arXiv:2510.19670v2 Announce Type: replace 
Abstract: We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer that grounds generation in site specific policies and notes; (iii) PromptRouter, a cost and uncertainty aware policy that selects edge only generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system works with modern serving optimizations, including paged or streaming KV caches, FlashAttention style kernels, speculative decoding, and quantized LoRA adapters, and supports on device personalization and federated updates under non IID drift. Across home, office, and clinic deployments, CoSense-LLM delivers grounded explanations while meeting tight service level objectives: it sustains sub second (p95) end to end latency on edge dominant paths, reduces inter tier token and bandwidth costs by preferring local retrieval grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge-RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV plus decoding accelerators lower energy per decision. The results support an edge first design that treats semantics, privacy, and predictable latency as co equal goals for large model deployments in interference prone environments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment</title>
<link>https://arxiv.org/abs/2407.17716</link>
<guid>https://arxiv.org/abs/2407.17716</guid>
<content:encoded><![CDATA[

arXiv:2407.17716v2 Announce Type: replace-cross 
Abstract: Speech emotion recognition (SER) systems often struggle in real-world environments, where ambient noise severely degrades their performance. This paper explores a novel approach that exploits prior knowledge of testing environments to maximize SER performance under noisy conditions. To address this task, we propose a text-guided, environment-aware training where an SER model is trained with contaminated speech samples and their paired noise description. We use a pre-trained text encoder to extract the text-based environment embedding and then fuse it to a transformer-based SER model during training and inference. We demonstrate the effectiveness of our approach through our experiment with the MSP-Podcast corpus and real-world additive noise samples collected from the Freesound and DEMAND repositories. Our experiment indicates that the text-based environment descriptions processed by a large language model (LLM) produce representations that improve the noise-robustness of the SER system. With a contrastive learning (CL)-based representation, our proposed method can be improved by jointly fine-tuning the text encoder with the emotion recognition model. Under the -5dB signal-to-noise ratio (SNR) level, fine-tuning the text encoder improves our CL-based representation method by 76.4% (arousal), 100.0% (dominance), and 27.7% (valence).
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning - A Convex Optimization Perspective</title>
<link>https://arxiv.org/abs/2410.15483</link>
<guid>https://arxiv.org/abs/2410.15483</guid>
<content:encoded><![CDATA[

arXiv:2410.15483v4 Announce Type: replace-cross 
Abstract: The post-training of LLMs, which typically consists of the supervised fine-tuning (SFT) stage and the preference learning stage (RLHF or DPO), is crucial to effective and safe LLM applications. The widely adopted approach in post-training popular open-source LLMs is to sequentially perform SFT and RLHF/DPO. However, this is suboptimal in terms of SFT and RLHF/DPO trade-off: the LLM gradually forgets about the first stage's training when undergoing the second stage's training. This sequential paradigm persists largely due to its simplicity and modularity, which make it easier to implement and manage at scale despite its limitations. We theoretically prove the sub-optimality of sequential post-training and propose a practical joint post-training framework which has theoretical convergence guarantees and empirically outperforms sequential post-training framework, with up to 23% overall performance improvement across multiple LLM evaluation benchmarks, while having minimal computational overhead. Our code is available at https://github.com/heshandevaka/XRIGHT.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training</title>
<link>https://arxiv.org/abs/2411.07066</link>
<guid>https://arxiv.org/abs/2411.07066</guid>
<content:encoded><![CDATA[

arXiv:2411.07066v4 Announce Type: replace-cross 
Abstract: Network pruning focuses on algorithms that aim to reduce a given model's computational cost by removing a subset of its parameters while having minimal impact on performance. Throughout the last decade, the most widely used pruning paradigm has been pruning and re-training, which nowadays is inconvenient due to the vast amount of pre-trained models, which are, in any case, too expensive to re-train. In this paper, we exploit functional information from dense pre-trained models, i.e., their input activations, to obtain sparse models that maximize the activations' alignment with respect to their corresponding dense models. Hence, we propose \textbf{NeuroAl}, a \emph{top-up} algorithm that can be used on top of any given pruning algorithm for LLMs, which modifies the block-wise and row-wise sparsity, exploiting information from both the dense model and its sparse version to maximize the \emph{neuron alignment} among activations. Different from existing methods, our approach adaptively selects the best hyperparameters for the block-wise and row-wise sparsity ratios w.r.t. the model and the desired sparsity, and requires \emph{no re-training}. We test our method over $\sim$300 test cases with four LLM families, three sparsity ratios, and ten language tasks (three language modeling and seven zero-shot datasets), showing how it consistently outperforms the latest state-of-the-art methods in terms of performance-runtime trade-off. The code is available at \href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEAGraph: Unveiling the Whole Story of Paper Review Comments</title>
<link>https://arxiv.org/abs/2412.11939</link>
<guid>https://arxiv.org/abs/2412.11939</guid>
<content:encoded><![CDATA[

arXiv:2412.11939v2 Announce Type: replace-cross 
Abstract: Peer review, as a cornerstone of scientific research, ensures the integrity and quality of scholarly work by providing authors with objective feedback for refinement. However, in the traditional peer review process, authors often receive vague or insufficiently detailed feedback, which provides limited assistance and leads to a more time-consuming review cycle. If authors can identify some specific weaknesses in their paper, they can not only address the reviewer's concerns but also improve their work. This raises the critical question of how to enhance authors' comprehension of review comments. In this paper, we present SEAGraph, a novel framework developed to clarify review comments by uncovering the underlying intentions behind them. We construct two types of graphs for each paper: the semantic mind graph, which captures the authors' thought process, and the hierarchical background graph, which delineates the research domains related to the paper. A retrieval method is then designed to extract relevant content from both graphs, facilitating coherent explanations for the review comments. Extensive experiments show that SEAGraph excels in review comment understanding tasks, offering significant benefits to authors. By bridging the gap between reviewers' critiques and authors' comprehension, SEAGraph contributes to a more efficient, transparent and collaborative scientific publishing ecosystem.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributional Surgery for Language Model Activations</title>
<link>https://arxiv.org/abs/2501.15758</link>
<guid>https://arxiv.org/abs/2501.15758</guid>
<content:encoded><![CDATA[

arXiv:2501.15758v2 Announce Type: replace-cross 
Abstract: Language models, while capable of generating remarkably coherent and seemingly accurate text, can occasionally produce undesirable content, including harmful or toxic outputs. In this paper, we present a new two-stage approach to detect and mitigate undesirable content generations by rectifying activations. First, we train an ensemble of layerwise classifiers to detect undesirable content using activations by minimizing a smooth surrogate of the risk-aware score. Then, for detected undesirable contents, we propose layerwise distributional steering policies that transform the attention heads. These policies are computed through principled semidefinite programming, which aims to minimally perturb the attention distribution while probabilistically guaranteeing the effectiveness of the editions. Empirical evaluations across multiple language models and datasets show that our method outperforms baselines in reducing the generation of undesirable output.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Pre-training of MoEs: How robust is your router?</title>
<link>https://arxiv.org/abs/2503.05029</link>
<guid>https://arxiv.org/abs/2503.05029</guid>
<content:encoded><![CDATA[

arXiv:2503.05029v2 Announce Type: replace-cross 
Abstract: Sparsely-activated Mixture of Experts (MoE) transformers are promising architectures for foundation models. Compared to dense transformers that require the same amount of floating-point operations (FLOPs) per forward pass, MoEs benefit from improved sample efficiency at training time and achieve much stronger performance. Many closed-source and open-source frontier language models have thus adopted an MoE architecture. Naturally, practitioners will want to extend the capabilities of these models with large amounts of newly collected data without completely re-training them. Prior work has shown that a simple combination of replay, learning rate re-warming, and re-decaying can enable the continual pre-training (CPT) of dense decoder-only transformers with minimal performance degradation compared to full re-training. In the case of decoder-only MoE transformers, however, it is unclear how the routing algorithm will impact continual pre-training performance: 1) do the MoE transformer's routers exacerbate forgetting relative to a dense model?; 2) do the routers maintain a balanced load on previous distributions after CPT?; 3) are the same strategies applied to dense models sufficient to continually pre-train MoE LLMs? In what follows, we conduct a large-scale study training a 500M parameter dense transformer and four 500M-active/2B-total parameter MoE transformers. Each model is trained for 600B tokens. Our results establish a surprising robustness to distribution shifts for MoEs using both Sinkhorn-Balanced and Z-and-Aux-loss-balanced routing algorithms, even in MoEs continually pre-trained without replay. Moreover, we show that MoE LLMs maintain their sample efficiency (relative to a FLOP-matched dense model) during CPT and that they can match the performance of a fully re-trained MoE at a fraction of the cost.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness</title>
<link>https://arxiv.org/abs/2504.10514</link>
<guid>https://arxiv.org/abs/2504.10514</guid>
<content:encoded><![CDATA[

arXiv:2504.10514v3 Announce Type: replace-cross 
Abstract: Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating a suite of diverse test scenarios, with grounding in real applications, ColorBench evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on ColorBench, while the language model plays a more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on ColorBench but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our ColorBenchcan serve as a foundational tool for advancing the study of human-level color understanding of multimodal AI.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-to-Pipeline: Bridging Natural Language and Data Preparation Pipelines</title>
<link>https://arxiv.org/abs/2505.15874</link>
<guid>https://arxiv.org/abs/2505.15874</guid>
<content:encoded><![CDATA[

arXiv:2505.15874v2 Announce Type: replace-cross 
Abstract: Data preparation (DP) transforms raw data into a form suitable for downstream applications, typically by composing operations into executable pipelines. Building such pipelines is time-consuming and requires sophisticated programming skills, posing a significant barrier for non-experts. To lower this barrier, we introduce Text-to-Pipeline, a new task that translates NL data preparation instructions into DP pipelines, and PARROT, a large-scale benchmark to support systematic evaluation. To ensure realistic DP scenarios, PARROT is built by mining transformation patterns from production pipelines and instantiating them on 23,009 real-world tables, resulting in ~18,000 tasks spanning 16 core operators. Our empirical evaluation on PARROT reveals a critical failure mode in cutting-edge LLMs: they struggle not only with multi-step compositional logic but also with semantic parameter grounding. We thus establish a strong baseline with Pipeline-Agent, an execution-aware agent that iteratively reflects on intermediate states. While it achieves state-of-the-art performance, a significant gap remains, underscoring the deep, unsolved challenges for PARROT. It provides the essential, large-scale testbed for developing and evaluating the next generation of autonomous data preparation agentic systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains</title>
<link>https://arxiv.org/abs/2506.00708</link>
<guid>https://arxiv.org/abs/2506.00708</guid>
<content:encoded><![CDATA[

arXiv:2506.00708v3 Announce Type: replace-cross 
Abstract: Knowledge graph completion (KGC) aims to predict missing triples in knowledge graphs (KGs) by leveraging existing triples and textual information. Recently, generative large language models (LLMs) have been increasingly employed for graph tasks. However, current approaches typically encode graph context in textual form, which fails to fully exploit the potential of LLMs for perceiving and reasoning about graph structures. To address this limitation, we propose DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion). DrKGC employs a flexible lightweight model training strategy to learn structural embeddings and logical rules within the KG. It then leverages a novel bottom-up graph retrieval method to extract a subgraph for each query guided by the learned rules. Finally, a graph convolutional network (GCN) adapter uses the retrieved subgraph to enhance the structural embeddings, which are then integrated into the prompt for effective LLM fine-tuning. Experimental results on two general domain benchmark datasets and two biomedical datasets demonstrate the superior performance of DrKGC. Furthermore, a realistic case study in the biomedical domain highlights its interpretability and practical utility.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dissecting Long-Chain-of-Thought Reasoning Models: An Empirical Study</title>
<link>https://arxiv.org/abs/2506.04913</link>
<guid>https://arxiv.org/abs/2506.04913</guid>
<content:encoded><![CDATA[

arXiv:2506.04913v2 Announce Type: replace-cross 
Abstract: Despite recent progress in training long-chain-of-thought reasoning models via scaling reinforcement learning (RL), its underlying training dynamics remain poorly understood, and several counterintuitive behaviors persist. This work focuses on three key aspects: (1) We systematically analyze the roles of positive and negative samples in scaling RL, revealing that positive samples mainly facilitate precise fitting to the training data, whereas negative samples significantly enhance generalization and robustness. Interestingly, while positive samples are essential for convergence in the zero-RL setting, training on negative samples alone suffices to attain strong reasoning performance and even better generalization in cold-start scenarios. (2) We identify substantial data inefficiency in group relative policy optimization, where over half of the samples yield zero advantage. To address this, we explore two strategies, including relative length rewards and offline sample injection, to leverage these data better and enhance reasoning efficiency and capability. (3) We investigate unstable performance across various reasoning models and benchmarks, attributing instability to uncertain problems with ambiguous outcomes, and demonstrate that greedy decoding can distort evaluation by flipping the correctness of responses. Our code is available at: https://github.com/takagi97/Dissect-Long-Reason-Models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning</title>
<link>https://arxiv.org/abs/2507.16795</link>
<guid>https://arxiv.org/abs/2507.16795</guid>
<content:encoded><![CDATA[

arXiv:2507.16795v2 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) can lead to unintended out-of-distribution generalization. Standard approaches to this problem rely on modifying training data, for example by adding data that better specify the intended generalization. However, this is not always practical. We introduce Concept Ablation Fine-Tuning (CAFT), a technique that leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution. Given a set of directions in an LLM's latent space corresponding to undesired concepts, CAFT works by ablating these concepts with linear projections during fine-tuning, steering the model away from unintended generalizations. We successfully apply CAFT to three fine-tuning tasks, including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow task generalize to give egregiously misaligned responses to general questions. Without any changes to the fine-tuning data, CAFT reduces misaligned responses by 10x without degrading performance on the training distribution. Overall, CAFT represents a novel approach for steering LLM generalization without modifying training data.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assemble Your Crew: Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation</title>
<link>https://arxiv.org/abs/2507.18224</link>
<guid>https://arxiv.org/abs/2507.18224</guid>
<content:encoded><![CDATA[

arXiv:2507.18224v3 Announce Type: replace-cross 
Abstract: Multi-agent systems (MAS) based on large language models (LLMs) have emerged as a powerful solution for dealing with complex problems across diverse domains. The effectiveness of MAS is critically dependent on its collaboration topology, which has become a focal point for automated design research. However, existing approaches are fundamentally constrained by their reliance on a template graph modification paradigm with a predefined set of agents and hard-coded interaction structures, significantly limiting their adaptability to task-specific requirements. To address these limitations, we reframe MAS design as a conditional autoregressive graph generation task, where both the system composition and structure are designed jointly. We propose ARG-Designer, a novel autoregressive model that operationalizes this paradigm by constructing the collaboration graph from scratch. Conditioned on a natural language task query, ARG-Designer sequentially and dynamically determines the required number of agents, selects their appropriate roles from an extensible pool, and establishes the optimal communication links between them. This generative approach creates a customized topology in a flexible and extensible manner, precisely tailored to the unique demands of different tasks. Extensive experiments across six diverse benchmarks demonstrate that ARG-Designer not only achieves state-of-the-art performance but also enjoys significantly greater token efficiency and enhanced extensibility. The source code of ARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Medical Event Models Improve with Scale</title>
<link>https://arxiv.org/abs/2508.12104</link>
<guid>https://arxiv.org/abs/2508.12104</guid>
<content:encoded><![CDATA[

arXiv:2508.12104v3 Announce Type: replace-cross 
Abstract: Realizing personalized medicine at scale calls for methods that distill insights from longitudinal patient journeys, which can be viewed as a sequence of medical events. Foundation models pretrained on large-scale medical event data represent a promising direction for scaling real-world evidence generation and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with medical events from de-identified longitudinal health records for 16.3 billion encounters over 300 million unique patient records from 310 health systems, we introduce the Curiosity models, a family of decoder-only transformer models pretrained on 118 million patients representing 115 billion discrete medical events (151 billion tokens). We present the largest scaling-law study of medical event data, establishing a methodology for pretraining and revealing power-law scaling relationships for compute, tokens, and model size. Consequently, we pretrained a series of compute-optimal models with up to 1 billion parameters. Conditioned on a patient's real-world history, Curiosity autoregressively predicts the next medical event to simulate patient health timelines. We studied 78 real-world tasks, including diagnosis prediction, disease prognosis, and healthcare operations. Remarkably for a foundation model with generic pretraining and simulation-based inference, Curiosity generally outperformed or matched task-specific supervised models on these tasks, without requiring task-specific fine-tuning or few-shot examples. Curiosity's predictive power consistently improves as the model and pretraining scale. Our results show that Curiosity, a generative medical event foundation model, can effectively capture complex clinical dynamics, providing an extensible and generalizable framework to support clinical decision-making, streamline healthcare operations, and improve patient outcomes.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2509.21227</link>
<guid>https://arxiv.org/abs/2509.21227</guid>
<content:encoded><![CDATA[

arXiv:2509.21227v2 Announce Type: replace-cross 
Abstract: Text-image generation has advanced rapidly, but assessing whether outputs truly capture the objects, attributes, and relations described in prompts remains a central challenge. Evaluation in this space relies heavily on automated metrics, yet these are often adopted by convention or popularity rather than validated against human judgment. Because evaluation and reported progress in the field depend directly on these metrics, it is critical to understand how well they reflect human preferences. To address this, we present a broad study of widely used metrics for compositional text-image evaluation. Our analysis goes beyond simple correlation, examining their behavior across diverse compositional challenges and comparing how different metric families align with human judgments. The results show that no single metric performs consistently across tasks: performance varies with the type of compositional problem. Notably, VQA-based metrics, though popular, are not uniformly superior, while certain embedding-based metrics prove stronger in specific cases. Image-only metrics, as expected, contribute little to compositional evaluation, as they are designed for perceptual quality rather than alignment. These findings underscore the importance of careful and transparent metric selection, both for trustworthy evaluation and for their use as reward models in generation. Project page is available at https://amirkasaei.com/eval-the-evals/ .
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation</title>
<link>https://arxiv.org/abs/2509.21257</link>
<guid>https://arxiv.org/abs/2509.21257</guid>
<content:encoded><![CDATA[

arXiv:2509.21257v2 Announce Type: replace-cross 
Abstract: In language and vision-language models, hallucination is broadly understood as content generated from a model's prior knowledge or biases rather than from the given input. While this phenomenon has been studied in those domains, it has not been clearly framed for text-to-image (T2I) generative models. Existing evaluations mainly focus on alignment, checking whether prompt-specified elements appear, but overlook what the model generates beyond the prompt. We argue for defining hallucination in T2I as bias-driven deviations and propose a taxonomy with three categories: attribute, relation, and object hallucinations. This framing introduces an upper bound for evaluation and surfaces hidden biases, providing a foundation for richer assessment of T2I models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Markovian Thinker</title>
<link>https://arxiv.org/abs/2510.06557</link>
<guid>https://arxiv.org/abs/2510.06557</guid>
<content:encoded><![CDATA[

arXiv:2510.06557v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has recently become a strong recipe for training reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard RL "thinking environment", where the state is the prompt plus all prior reasoning tokens, makes the state unbounded and forces attention-based policies to pay quadratic compute as thoughts lengthen. We revisit the environment itself. We propose Markovian Thinking, a paradigm in which the policy advances reasoning while conditioning on a constant-size state, decoupling thinking length from context size. As an immediate consequence this yields linear compute with constant memory. We instantiate this idea with Delethink, an RL environment that structures reasoning into fixed-size chunks. Within each chunk, the model thinks as usual; at the boundary, the environment resets the context and reinitializes the prompt with a short carryover. Through RL, the policy learns to write a textual state near the end of each chunk sufficient for seamless continuation of reasoning after reset. Trained in this environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget. With test-time scaling, Delethink continues to improve where LongCoT plateaus. The effect of linear compute is substantial: we empirically estimate at 96K average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink. Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B) often sample Markovian traces zero-shot across diverse benchmarks, providing positive samples that make RL effective at scale. Our results show that redesigning the thinking environment is a powerful lever: it enables very long reasoning without quadratic overhead and opens a path toward efficient, scalable reasoning LLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy</title>
<link>https://arxiv.org/abs/2510.16830</link>
<guid>https://arxiv.org/abs/2510.16830</guid>
<content:encoded><![CDATA[

arXiv:2510.16830v2 Announce Type: replace-cross 
Abstract: Large language models are often adapted through parameter efficient fine tuning, but current release practices provide weak assurances about what data were used and how updates were computed. We present Verifiable Fine Tuning, a protocol and system that produces succinct zero knowledge proofs that a released model was obtained from a public initialization under a declared training program and an auditable dataset commitment. The approach combines five elements. First, commitments that bind data sources, preprocessing, licenses, and per epoch quota counters to a manifest. Second, a verifiable sampler that supports public replayable and private index hiding batch selection. Third, update circuits restricted to parameter efficient fine tuning that enforce AdamW style optimizer semantics and proof friendly approximations with explicit error budgets. Fourth, recursive aggregation that folds per step proofs into per epoch and end to end certificates with millisecond verification. Fifth, provenance binding and optional trusted execution property cards that attest code identity and constants. On English and bilingual instruction mixtures, the method maintains utility within tight budgets while achieving practical proof performance. Policy quotas are enforced with zero violations, and private sampling windows show no measurable index leakage. Federated experiments demonstrate that the system composes with probabilistic audits and bandwidth constraints. These results indicate that end to end verifiable fine tuning is feasible today for real parameter efficient pipelines, closing a critical trust gap for regulated and decentralized deployments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>Evaluating LLMs' Reasoning Over Ordered Procedural Steps</title>
<link>https://arxiv.org/abs/2511.04688</link>
<guid>https://arxiv.org/abs/2511.04688</guid>
<content:encoded><![CDATA[
<div> Keywords: procedural sequences, language models, food recipes, ordering quality, model performance<br />
Summary:<br />
The study focuses on the task of reconstructing ordered sequences from shuffled procedural steps using a dataset of food recipes. Various large language models (LLMs) are evaluated in zero-shot and few-shot settings using metrics like Kendall's Tau, NLCS, and NED to assess ordering quality. The performance of LLMs decreases with longer sequences and more severe shuffling, indicating challenges in procedural reasoning. The research highlights the limitations of current LLMs in handling complex procedural sequences with increasing length and disorder. Overall, the analysis underscores the importance of accurate sequencing in tasks like food recipes and the need for advancements in LLMs for effective procedural reasoning.<br /> 
Summary: <div>
arXiv:2511.04688v1 Announce Type: new 
Abstract: Reasoning over procedural sequences, where the order of steps directly impacts outcomes, is a critical capability for large language models (LLMs). In this work, we study the task of reconstructing globally ordered sequences from shuffled procedural steps, using a curated dataset of food recipes, a domain where correct sequencing is essential for task success. We evaluate several LLMs under zero-shot and few-shot settings and present a comprehensive evaluation framework that adapts established metrics from ranking and sequence alignment. These include Kendall's Tau, Normalized Longest Common Subsequence (NLCS), and Normalized Edit Distance (NED), which capture complementary aspects of ordering quality. Our analysis shows that model performance declines with increasing sequence length, reflecting the added complexity of longer procedures. We also find that greater step displacement in the input, corresponding to more severe shuffling, leads to further degradation. These findings highlight the limitations of current LLMs in procedural reasoning, especially with longer and more disordered inputs.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks</title>
<link>https://arxiv.org/abs/2511.04689</link>
<guid>https://arxiv.org/abs/2511.04689</guid>
<content:encoded><![CDATA[
<div> Item Response Theory, large language model evaluation, adaptive testing, benchmark items, measurement precision<br />
<br />
Summary: ATLAS is an adaptive testing framework that uses Item Response Theory to estimate model ability by selecting benchmark items based on Fisher information. It reduces the number of items needed for evaluation by 90% while maintaining measurement precision. Analysis shows that a small percentage of benchmark items have negative discrimination, indicating annotation errors that can corrupt evaluation results. ATLAS allows for more efficient evaluation with lower item exposure rates and test overlap compared to static benchmarks where all models see all items. IRT rankings of models differ from accuracy rankings, with models having the same accuracy receiving different IRT scores and a significant percentage of models shifting rank positions. The framework provides code and calibrated item banks for easy implementation. <div>
arXiv:2511.04689v1 Announce Type: new 
Abstract: Large language model evaluation requires thousands of benchmark items, making evaluations expensive and slow. Existing methods compute average accuracy across fixed item sets, treating all items equally despite varying quality and informativeness. We present ATLAS an adaptive testing framework using Item Response Theory (IRT) to estimate model ability through Fisher information-guided item selection. Our analysis of five major benchmarks reveals that 3-6% of items exhibit negative discrimination, indicating annotation errors that corrupt static evaluation. ATLAS achieves 90% item reduction while maintaining measurement precision: on HellaSwag (5,608 items), we match full-benchmark estimates using only 42 items with 0.154 MAE. Our framework maintains item exposure rates below 10% and test overlap at 16-27%, compared to static benchmarks where every model sees all items (100% exposure). Among 4,000+ tested models, IRT ranks differ from accuracy ranks: models with the same accuracy get different IRT scores, and 23-31% of all models shift by more than 10 rank positions. Code and calibrated item banks are available at https://github.com/Peiyu-Georgia-Li/ATLAS.git.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SARC: Sentiment-Augmented Deep Role Clustering for Fake News Detection</title>
<link>https://arxiv.org/abs/2511.04692</link>
<guid>https://arxiv.org/abs/2511.04692</guid>
<content:encoded><![CDATA[
<div> Keywords: fake news detection, sentiment information, user roles, deep clustering, SARC

Summary: <br /><br /> Fake news detection in social networks has been a significant research focus, with recent studies highlighting the importance of incorporating sentiment information from both news content and user comments for improved performance. However, existing approaches often overlook role differentiation, where the same sentiment polarity can come from users with different roles, limiting their ability to capture nuanced patterns. To address this issue, the proposed Sentiment-Augmented Role Clustering (SARC) framework utilizes sentiment-enhanced deep clustering to identify user roles for better fake news detection. By generating user features through joint comment text representation and sentiment encoding, SARC constructs a deep clustering module to categorize user roles and optimize dual objectives for role clustering and fake news detection simultaneously. Experimental results on benchmark datasets show that SARC outperforms baseline models in all metrics, demonstrating its effectiveness in enhancing fake news detection. <div>
arXiv:2511.04692v1 Announce Type: new 
Abstract: Fake news detection has been a long-standing research focus in social networks. Recent studies suggest that incorporating sentiment information from both news content and user comments can enhance detection performance. However, existing approaches typically treat sentiment features as auxiliary signals, overlooking role differentiation, that is, the same sentiment polarity may originate from users with distinct roles, thereby limiting their ability to capture nuanced patterns for effective detection. To address this issue, we propose SARC, a Sentiment-Augmented Role Clustering framework which utilizes sentiment-enhanced deep clustering to identify user roles for improved fake news detection. The framework first generates user features through joint comment text representation (with BiGRU and Attention mechanism) and sentiment encoding. It then constructs a differentiable deep clustering module to automatically categorize user roles. Finally, unlike existing approaches which take fake news label as the unique supervision signal, we propose a joint optimization objective integrating role clustering and fake news detection to further improve the model performance. Experimental results on two benchmark datasets, RumourEval-19 and Weibo-comp, demonstrate that SARC achieves superior performance across all metrics compared to baseline models. The code is available at: https://github.com/jxshang/SARC.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Up the Instruction Ladder for Controllable Language Models</title>
<link>https://arxiv.org/abs/2511.04694</link>
<guid>https://arxiv.org/abs/2511.04694</guid>
<content:encoded><![CDATA[
<div> Keywords: language model, instruction hierarchy, reasoning task, reinforcement learning, safety-critical settings

Summary:
This article introduces the concept of instruction hierarchy in large language models (LLMs) to manage conflicting instructions from different sources within a prompt context. The researchers frame instruction hierarchy resolution as a reasoning task and create the VerIH dataset to train models on prioritizing instructions. Through lightweight reinforcement learning, models are trained to understand the relationship between user prompts and system instructions, leading to improvements in instruction following and hierarchy benchmarks. The trained models demonstrate enhanced reasoning abilities that extend to safety-critical scenarios, protecting against adversarial attacks. By effectively resolving conflicts between user inputs and predefined policies, the models exhibit robustness and controllability in responding to prompt updates. This research highlights how reasoning over instruction hierarchies can enhance the reliability and performance of LLMs in real-world decision-making contexts.<br /><br />Summary: <div>
arXiv:2511.04694v1 Announce Type: new 
Abstract: As large language model (LLM) based systems take on high-stakes roles in real-world decision-making, they must reconcile competing instructions from multiple sources (e.g., model developers, users, and tools) within a single prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where higher-level directives override lower-priority requests, is critical for the reliability and controllability of LLMs. In this work, we reframe instruction hierarchy resolution as a reasoning task. Specifically, the model must first "think" about the relationship between a given user prompt and higher-priority (system) instructions before generating a response. To enable this capability via training, we construct VerIH, an instruction hierarchy dataset of constraint-following tasks with verifiable answers. This dataset comprises both aligned and conflicting system-user instructions. We show that lightweight reinforcement learning with VerIH effectively transfers general reasoning capabilities of models to instruction prioritization. Our finetuned models achieve consistent improvements on instruction following and instruction hierarchy benchmarks. This reasoning ability also generalizes to safety-critical settings beyond the training distribution. By treating safety issues as resolving conflicts between adversarial user inputs and predefined higher-priority policies, our trained model enhances robustness against jailbreak and prompt injection attacks. These results demonstrate that reasoning over instruction hierarchies provides a practical path to reliable LLMs, where updates to system prompts yield controllable and robust changes in model behavior.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EncouRAGe: Evaluating RAG Local, Fast, and Reliable</title>
<link>https://arxiv.org/abs/2511.04696</link>
<guid>https://arxiv.org/abs/2511.04696</guid>
<content:encoded><![CDATA[
<div> Keywords: EncouRAGe, Python framework, Retrieval-Augmented Generation, Large Language Models, Evaluation

Summary:
EncouRAGe is a Python framework designed for developing and evaluating Retrieval-Augmented Generation (RAG) systems using Large Language Models (LLMs) and Embedding Models. The framework consists of five components: Type Manifest, RAG Factory, Inference, Vector Store, and Metrics. It prioritizes scientific reproducibility, diverse evaluation metrics, and local deployment for efficient dataset assessment in RAG workflows. Evaluation on multiple benchmark datasets showed that RAG still lags behind Oracle Context performance, with Hybrid BM25 achieving the best results. Reranking had marginal performance gains but increased response latency. This comprehensive framework offers flexibility and extensibility in RAG system development and evaluation.<br /><br />Summary: <div>
arXiv:2511.04696v1 Announce Type: new 
Abstract: We introduce EncouRAGe, a comprehensive Python framework designed to streamline the development and evaluation of Retrieval-Augmented Generation (RAG) systems using Large Language Models (LLMs) and Embedding Models. EncouRAGe comprises five modular and extensible components: Type Manifest, RAG Factory, Inference, Vector Store, and Metrics, facilitating flexible experimentation and extensible development. The framework emphasizes scientific reproducibility, diverse evaluation metrics, and local deployment, enabling researchers to efficiently assess datasets within RAG workflows. This paper presents implementation details and an extensive evaluation across multiple benchmark datasets, including 25k QA pairs and over 51k documents. Our results show that RAG still underperforms compared to the Oracle Context, while Hybrid BM25 consistently achieves the best results across all four datasets. We further examine the effects of reranking, observing only marginal performance improvements accompanied by higher response latency.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>multiMentalRoBERTa: A Fine-tuned Multiclass Classifier for Mental Health Disorder</title>
<link>https://arxiv.org/abs/2511.04698</link>
<guid>https://arxiv.org/abs/2511.04698</guid>
<content:encoded><![CDATA[
<div> detecting mental health disorders, social media text, multiMentalRoBERTa, classification, stress, anxiety, depression, PTSD, suicidal ideation<br />
Summary:<br />
- The study introduces multiMentalRoBERTa, a model for classifying mental health conditions from social media text.<br />
- Data exploration reveals correlations between depression and suicidal ideation, and anxiety and PTSD.<br />
- multiMentalRoBERTa outperforms other methods with macro F1-scores of 0.839 and 0.870 in different setups.<br />
- Explainability methods such as Layer Integrated Gradients and KeyBERT are used to identify key features for classification, focusing on distinguishing depression from suicidal ideation.<br />
- The study emphasizes the importance of fairness, bias mitigation, and human-in-the-loop safety protocols in mental health detection.<br />
Summary: <div>
arXiv:2511.04698v1 Announce Type: new 
Abstract: The early detection of mental health disorders from social media text is critical for enabling timely support, risk assessment, and referral to appropriate resources. This work introduces multiMentalRoBERTa, a fine-tuned RoBERTa model designed for multiclass classification of common mental health conditions, including stress, anxiety, depression, post-traumatic stress disorder (PTSD), suicidal ideation, and neutral discourse. Drawing on multiple curated datasets, data exploration is conducted to analyze class overlaps, revealing strong correlations between depression and suicidal ideation as well as anxiety and PTSD, while stress emerges as a broad, overlapping category. Comparative experiments with traditional machine learning methods, domain-specific transformers, and prompting-based large language models demonstrate that multiMentalRoBERTa achieves superior performance, with macro F1-scores of 0.839 in the six-class setup and 0.870 in the five-class setup (excluding stress), outperforming both fine-tuned MentalBERT and baseline classifiers. Beyond predictive accuracy, explainability methods, including Layer Integrated Gradients and KeyBERT, are applied to identify lexical cues that drive classification, with a particular focus on distinguishing depression from suicidal ideation. The findings emphasize the effectiveness of fine-tuned transformers for reliable and interpretable detection in sensitive contexts, while also underscoring the importance of fairness, bias mitigation, and human-in-the-loop safety protocols. Overall, multiMentalRoBERTa is presented as a lightweight, robust, and deployable solution for enhancing support in mental health platforms.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding</title>
<link>https://arxiv.org/abs/2511.04699</link>
<guid>https://arxiv.org/abs/2511.04699</guid>
<content:encoded><![CDATA[
<div> Keywords: Cross-Lingual SynthDocs, Arabic resources, Optical Character Recognition, Document Understanding, multilingual document analysis

Summary:<br />
Cross-Lingual SynthDocs is a synthetic corpus created to address the lack of Arabic resources for Optical Character Recognition (OCR) and Document Understanding (DU). The dataset includes over 2.5 million samples, consisting of textual data, annotated tables, and real data-based charts. The pipeline utilizes scanned backgrounds, bilingual layouts, and diacritic-aware fonts to capture Arabic document complexity. Various styles for charts and tables are also included in the corpus. Finetuning on SynthDocs shows improved Word Error Rate (WER) and Character Error Rate (CER) in OCR, as well as enhanced Tree-Edit Distance Similarity (TEDS) and Chart Extraction Score (CharTeX) in other modalities. This resource provides a scalable and visually realistic tool for advancing multilingual document analysis research.

Summary: <div>
arXiv:2511.04699v1 Announce Type: new 
Abstract: Cross-Lingual SynthDocs is a large-scale synthetic corpus designed to address the scarcity of Arabic resources for Optical Character Recognition (OCR) and Document Understanding (DU). The dataset comprises over 2.5 million of samples, including 1.5 million textual data, 270K fully annotated tables, and hundred thousands of real data based charts. Our pipeline leverages authentic scanned backgrounds, bilingual layouts, and diacritic aware fonts to capture the typographic and structural complexity of Arabic documents. In addition to text, the corpus includes variety of rendered styles for charts and tables. Finetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in Word Error Rate (WER) and Character Error Rate (CER) in terms of OCR across multiple public Arabic benchmarks, Tree-Edit Distance Similarity (TEDS) and Chart Extraction Score (CharTeX) improved as well in other modalities. SynthDocs provides a scalable, visually realistic resource for advancing research in multilingual document analysis.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Separate the Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2511.04700</link>
<guid>https://arxiv.org/abs/2511.04700</guid>
<content:encoded><![CDATA[
<div> Keyword: Retrieval-augmented generation, large language models, external knowledge, WinnowRAG, document filtering  
Summary:  
- Retrieval-augmented generation (RAG) integrates external knowledge sources to enhance large language models (LLMs).  
- To increase the likelihood of retrieving relevant information, WinnowRAG filters out noisy documents while preserving valuable content through a process known as winnowing.  
- WinnowRAG operates in two stages: query-aware clustering to form distinct topic clusters and winnowing to separate useful documents from noisy ones.  
- A critic LLM evaluates outputs of multiple agents and strategic merging techniques ensure only relevant knowledge is used for final response generation.  
- Model-agnostic WinnowRAG does not require model fine-tuning, making it easily adaptable to various tasks.  

<br /><br />Summary:  
Retrieval-augmented generation (RAG) enhances large language models by integrating external knowledge sources to address limitations in accessing up-to-date or specialized information. WinnowRAG, a novel RAG framework, systematically filters out noisy documents while preserving valuable content through winnowing. Operating in two stages, WinnowRAG uses query-aware clustering to form distinct topic clusters and a critic LLM for evaluating outputs and separating useful documents. Strategic merging techniques ensure that only relevant knowledge is used for generating final responses. This model-agnostic approach of WinnowRAG, without requiring model fine-tuning, makes it easily adaptable to various tasks. Extensive experiments demonstrate the effectiveness of WinnowRAG over existing baselines on realistic datasets. <div>
arXiv:2511.04700v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge sources to address their limitations in accessing up-to-date or specialized information. A natural strategy to increase the likelihood of retrieving relevant information is to expand the number of retrieved documents. However, involving more documents could introduce significant noise, as many documents may be irrelevant or misleading, thereby reducing the overall accuracy of the generated responses. To overcome the challenge associated with handling a larger number of documents, we propose WinnowRAG, a novel RAG framework designed to systematically filter out noisy documents while preserving valuable content -- a process we refer to as winnowing. WinnowRAG operates in two stages: In Stage I, we perform query-aware clustering to group similar documents and form distinct topic clusters. Each cluster is assigned to an LLM agent for generating a unique answer. In Stage II, we perform winnowing, wherein a critic LLM evaluates the outputs of multiple agents and iteratively separates useful documents from noisy ones. To retain useful documents when discarding agents, we propose two strategic merging techniques to ensure that only relevant knowledge is used for generating the final response. Crucially, WinnowRAG is model-agnostic and does not require any model fine-tuning, making it easily adaptable to various tasks. Extensive experiments on various realistic datasets demonstrate the effectiveness of WinnowRAG over state-of-the-art baselines.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring what Matters: Construct Validity in Large Language Model Benchmarks</title>
<link>https://arxiv.org/abs/2511.04703</link>
<guid>https://arxiv.org/abs/2511.04703</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, evaluation, safety, robustness, construct validity

Summary:<br /><br />
In this study, a systematic review was conducted on 445 benchmarks for large language models (LLMs) to evaluate their capabilities and identify safety or robustness issues. It was found that the measures used in these benchmarks often did not effectively capture the phenomena of safety and robustness, leading to questionable validity of the claims made. The team of 29 expert reviewers identified patterns related to the measured phenomena, tasks, and scoring metrics that undermined the construct validity of the benchmarks. To address these shortcomings, eight key recommendations and actionable guidance were provided for researchers and practitioners in the development of LLM benchmarks. It is crucial for the field to improve the measurement of safety and robustness in LLM evaluations to ensure the reliability and validity of their assessments. 

Summary: <div>
arXiv:2511.04703v1 Announce Type: new 
Abstract: Evaluating large language models (LLMs) is crucial for both assessing their capabilities and identifying safety or robustness issues prior to deployment. Reliably measuring abstract and complex phenomena such as 'safety' and 'robustness' requires strong construct validity, that is, having measures that represent what matters to the phenomenon. With a team of 29 expert reviewers, we conduct a systematic review of 445 LLM benchmarks from leading conferences in natural language processing and machine learning. Across the reviewed articles, we find patterns related to the measured phenomena, tasks, and scoring metrics which undermine the validity of the resulting claims. To address these shortcomings, we provide eight key recommendations and detailed actionable guidance to researchers and practitioners in developing LLM benchmarks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>POLIS-Bench: Towards Multi-Dimensional Evaluation of LLMs for Bilingual Policy Tasks in Governmental Scenarios</title>
<link>https://arxiv.org/abs/2511.04705</link>
<guid>https://arxiv.org/abs/2511.04705</guid>
<content:encoded><![CDATA[
<div> Keywords: POLIS-Bench, LLMs, bilingual policy scenarios, dual-metric evaluation, compliance tasks

Summary:
POLIS-Bench is introduced as a comprehensive evaluation suite for LLMs in governmental bilingual policy scenarios. It includes an up-to-date bilingual policy corpus, scenario-grounded task design, and a dual-metric evaluation framework. The benchmark reveals a performance hierarchy among LLMs, with reasoning models excelling in cross-task stability and accuracy. Compliance tasks are particularly challenging. Leveraging POLIS-Bench, a lightweight open-source model achieves comparable or better performance to proprietary baselines at a reduced cost. This provides a cost-effective pathway for real-world governmental deployment. <div>
arXiv:2511.04705v1 Announce Type: new 
Abstract: We introduce POLIS-Bench, the first rigorous, systematic evaluation suite designed for LLMs operating in governmental bilingual policy scenarios. Compared to existing benchmarks, POLIS-Bench introduces three major advancements. (i) Up-to-date Bilingual Corpus: We construct an extensive, up-to-date policy corpus that significantly scales the effective assessment sample size, ensuring relevance to current governance practice. (ii) Scenario-Grounded Task Design: We distill three specialized, scenario-grounded tasks -- Clause Retrieval & Interpretation, Solution Generation, and the Compliance Judgmen--to comprehensively probe model understanding and application. (iii) Dual-Metric Evaluation Framework: We establish a novel dual-metric evaluation framework combining semantic similarity with accuracy rate to precisely measure both content alignment and task requirement adherence. A large-scale evaluation of over 10 state-of-the-art LLMs on POLIS-Bench reveals a clear performance hierarchy where reasoning models maintain superior cross-task stability and accuracy, highlighting the difficulty of compliance tasks. Furthermore, leveraging our benchmark, we successfully fine-tune a lightweight open-source model. The resulting POLIS series models achieves parity with, or surpasses, strong proprietary baselines on multiple policy subtasks at a significantly reduced cost, providing a cost-effective and compliant path for robust real-world governmental deployment.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GEMMA-SQL: A Novel Text-to-SQL Model Based on Large Language Models</title>
<link>https://arxiv.org/abs/2511.04710</link>
<guid>https://arxiv.org/abs/2511.04710</guid>
<content:encoded><![CDATA[
<div> model, Text-to-SQL, GEMMA-SQL, instruction-tuned, benchmark  
Summary:  
GEMMA-SQL is a lightweight and efficient text-to-SQL model based on the Gemma 2B architecture. It is fine-tuned in a resource-efficient manner and can run on low-cost hardware. By leveraging prompting strategies like few-shot learning, GEMMA-SQL enhances SQL query generation accuracy on the SPIDER benchmark. The instruction-tuned variant, GEMMA-SQL Instruct, outperforms state-of-the-art models with 66.8% Test-Suite accuracy and 63.3% Exact Set Match accuracy. Effective prompt design and targeted instruction tuning are shown to significantly improve performance while maintaining scalability and adaptability. GEMMA-SQL is positioned as a practical, open-source option for reliable and accessible text-to-SQL systems.  
Summary: <div>
arXiv:2511.04710v1 Announce Type: new 
Abstract: Text-to-SQL systems enable users to interact with structured databases using natural language, eliminating the need for specialized programming knowledge. In this work, we introduce GEMMA-SQL, a lightweight and efficient text-to-SQL model built upon the open-source Gemma 2B architecture. Unlike many large language models (LLMs), GEMMA-SQL is fine-tuned in a resource-efficient, iterative manner and can be deployed on low-cost hardware. Leveraging the SPIDER benchmark for training and evaluation, GEMMA-SQL combines multiple prompting strategies, including few-shot learning, to enhance SQL query generation accuracy. The instruction-tuned variant, GEMMA-SQL Instruct, achieves 66.8% Test-Suite accuracy and 63.3% Exact Set Match accuracy, outperforming several state-of-the-art baselines such as IRNet, RYANSQL, and CodeXDavinci. The proposed approach demonstrates that effective prompt design and targeted instruction tuning can significantly boost performance while maintaining high scalability and adaptability. These results position GEMMA-SQL as a practical, open-source alternative for robust and accessible text-to-SQL systems.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First is Not Really Better Than Last: Evaluating Layer Choice and Aggregation Strategies in Language Model Data Influence Estimation</title>
<link>https://arxiv.org/abs/2511.04715</link>
<guid>https://arxiv.org/abs/2511.04715</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Influence Functions, Attention Layers, Aggregation Methods, Noise Detection Rate

Summary: 
The study focuses on understanding how training samples affect Large Language Model (LLM) decision-making and the methods used to estimate their influence. Contrary to prior beliefs, the cancellation effect hypothesis regarding the informative nature of the first layers for influence computation is debunked. The research suggests that middle attention layers are more accurate estimators of influence in LLMs. Additionally, alternative aggregation methods such as ranking and vote-based approaches significantly improve influence score performance. The study introduces the Noise Detection Rate (NDR) as a novel metric for assessing influence score efficacy without retraining the model, demonstrating superior predictive capability compared to the cancellation effect. Through extensive experimentation with various types and scales of LLMs, the research concludes that the first layers are not always better than the last layers for influence estimation in LLMs. 

<br /><br />Summary: <div>
arXiv:2511.04715v1 Announce Type: new 
Abstract: Identifying how training samples influence/impact Large Language Model (LLM) decision-making is essential for effectively interpreting model decisions and auditing large-scale datasets. Current training sample influence estimation methods (also known as influence functions) undertake this goal by utilizing information flow through the model via its first-order and higher-order gradient terms. However, owing to the large model sizes of today consisting of billions of parameters, these influence computations are often restricted to some subset of model layers to ensure computational feasibility. Prior seminal work by Yeh et al. (2022) in assessing which layers are best suited for computing language data influence concluded that the first (embedding) layers are the most informative for this purpose, using a hypothesis based on influence scores canceling out (i.e., the cancellation effect). In this work, we propose theoretical and empirical evidence demonstrating how the cancellation effect is unreliable, and that middle attention layers are better estimators for influence. Furthermore, we address the broader challenge of aggregating influence scores across layers, and showcase how alternatives to standard averaging (such as ranking and vote-based methods) can lead to significantly improved performance. Finally, we propose better methods for evaluating influence score efficacy in LLMs without undertaking model retraining, and propose a new metric known as the Noise Detection Rate (NDR) that exhibits strong predictive capability compared to the cancellation effect. Through extensive experiments across LLMs of varying types and scales, we concretely determine that the first (layers) are not necessarily better than the last (layers) for LLM influence estimation, contrasting with prior knowledge in the field.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to reason about rare diseases through retrieval-augmented agents</title>
<link>https://arxiv.org/abs/2511.04720</link>
<guid>https://arxiv.org/abs/2511.04720</guid>
<content:encoded><![CDATA[
<div> Rare diseases, brain MRI, AI agents, medical knowledge, diagnostic reasoning<br />
Summary:<br />
RADAR, or Retrieval Augmented Diagnostic Reasoning Agents, is a system designed to aid in rare disease detection in brain MRI scans. The approach utilizes AI agents that have access to external medical knowledge from case reports and literature. By embedding this knowledge and using efficient similarity search techniques, RADAR can retrieve relevant evidence to assist radiologists in making diagnostic decisions for unfamiliar diseases. This system does not require additional training and can be seamlessly integrated with various large language models to improve rare pathology recognition and interpretability. Testing on the NOVA dataset showed up to a 10.2% performance gain, especially with open source models like DeepSeek. The retrieved examples not only enhance accuracy but also provide interpretable explanations grounded in literature, showcasing the effectiveness of retrieval-augmented reasoning for low-prevalence conditions in medical imaging.<br /> <div>
arXiv:2511.04720v1 Announce Type: new 
Abstract: Rare diseases represent the long tail of medical imaging, where AI models often fail due to the scarcity of representative training data. In clinical workflows, radiologists frequently consult case reports and literature when confronted with unfamiliar findings. Following this line of reasoning, we introduce RADAR, Retrieval Augmented Diagnostic Reasoning Agents, an agentic system for rare disease detection in brain MRI. Our approach uses AI agents with access to external medical knowledge by embedding both case reports and literature using sentence transformers and indexing them with FAISS to enable efficient similarity search. The agent retrieves clinically relevant evidence to guide diagnostic decision making on unseen diseases, without the need of additional training. Designed as a model-agnostic reasoning module, RADAR can be seamlessly integrated with diverse large language models, consistently improving their rare pathology recognition and interpretability. On the NOVA dataset comprising 280 distinct rare diseases, RADAR achieves up to a 10.2% performance gain, with the strongest improvements observed for open source models such as DeepSeek. Beyond accuracy, the retrieved examples provide interpretable, literature grounded explanations, highlighting retrieval-augmented reasoning as a powerful paradigm for low-prevalence conditions in medical imaging.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surprisal reveals diversity gaps in image captioning and different scorers change the story</title>
<link>https://arxiv.org/abs/2511.04754</link>
<guid>https://arxiv.org/abs/2511.04754</guid>
<content:encoded><![CDATA[
<div> Linguistic diversity, image captioning, surprisal variance, state-of-the-art LLMs, human captions<br />
<br />
Summary:<br />
- The study focuses on quantifying linguistic diversity in image captioning using surprisal variance, which measures the spread of token-level negative log-probabilities within a set of captions.
- Comparison of five advanced vision-and-language LLMs, decoded with greedy and nucleus sampling, to human captions on the MSCOCO test set reveals humans display approximately double the surprisal variance of models.
- However, when rescoring captions with a general-language model, the pattern is reversed, indicating the importance of evaluating diversity under multiple scoring metrics.
- The analysis introduces the metric of surprisal-based diversity for image captioning, highlighting the need for robust evaluation methods in assessing diversity.
- It is emphasized that relying on a single scorer can completely reverse conclusions, underscoring the significance of considering surprisal under various scorers in diversity evaluations. <br /> <div>
arXiv:2511.04754v1 Announce Type: new 
Abstract: We quantify linguistic diversity in image captioning with surprisal variance - the spread of token-level negative log-probabilities within a caption set. On the MSCOCO test set, we compare five state-of-the-art vision-and-language LLMs, decoded with greedy and nucleus sampling, to human captions. Measured with a caption-trained n-gram LM, humans display roughly twice the surprisal variance of models, but rescoring the same captions with a general-language model reverses the pattern. Our analysis introduces the surprisal-based diversity metric for image captioning. We show that relying on a single scorer can completely invert conclusions, thus, robust diversity evaluation must report surprisal under several scorers.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explore Data Left Behind in Reinforcement Learning for Reasoning Language Models</title>
<link>https://arxiv.org/abs/2511.04800</link>
<guid>https://arxiv.org/abs/2511.04800</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Verifiable Rewards, Large Language Models, Group Relative Policy Optimization, Explore Residual Prompts<br />
Summary:<br />
The article discusses Reinforcement Learning with Verifiable Rewards (RLVR) and the challenges faced when training large language models (LLMs) with residual prompts providing no training signal. The Explore Residual Prompts in Policy Optimization (ERPO) framework is introduced to address this issue by encouraging exploration on residual prompts and reactivating their training signals. ERPO maintains a history tracker for each prompt and adjusts sampling temperature for prompts with all correct responses to introduce more diverse reasoning traces. Empirical results on mathematical reasoning benchmarks show that ERPO outperforms strong baselines, improving the efficiency and effectiveness of training large language models with RLVR. <div>
arXiv:2511.04800v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for improving the reasoning abilities of large language models (LLMs). The Group Relative Policy Optimization (GRPO) family has demonstrated strong performance in training LLMs with RLVR. However, as models train longer and scale larger, more training prompts become residual prompts, those with zero variance rewards that provide no training signal. Consequently, fewer prompts contribute to training, reducing diversity and hindering effectiveness. To fully exploit these residual prompts, we propose the Explore Residual Prompts in Policy Optimization (ERPO) framework, which encourages exploration on residual prompts and reactivates their training signals. ERPO maintains a history tracker for each prompt and adaptively increases the sampling temperature for residual prompts that previously produced all correct responses. This encourages the model to generate more diverse reasoning traces, introducing incorrect responses that revive training signals. Empirical results on the Qwen2.5 series demonstrate that ERPO consistently surpasses strong baselines across multiple mathematical reasoning benchmarks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs</title>
<link>https://arxiv.org/abs/2511.04869</link>
<guid>https://arxiv.org/abs/2511.04869</guid>
<content:encoded><![CDATA[
<div> sampling, semantic calibration, question-answering, calibration, local loss optimality 
Summary:<br /><br />Large Language Models (LLMs) have been found to exhibit semantic calibration in open-domain question-answering tasks, suggesting they can assess confidence in their responses beyond token-level. A theoretical framework based on next-token prediction explains how semantic calibration emerges as a byproduct of calibration and local loss optimality. Base LLMs are expected to be semantically calibrated when able to predict their distribution over semantic answer classes before generating a response. Experimental validation confirms three implications: (1) Base LLMs exhibit semantic calibration, (2) RL instruction-tuning breaks this calibration, and (3) chain-of-thought reasoning disrupts calibration. This study provides a foundational understanding of the conditions under which semantic calibration arises in LLMs. <br /><br /> <div>
arXiv:2511.04869v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often lack meaningful confidence estimates for their outputs. While base LLMs are known to exhibit next-token calibration, it remains unclear whether they can assess confidence in the actual meaning of their responses beyond the token level. We find that, when using a certain sampling-based notion of semantic calibration, base LLMs are remarkably well-calibrated: they can meaningfully assess confidence in open-domain question-answering tasks, despite not being explicitly trained to do so. Our main theoretical contribution establishes a mechanism for why semantic calibration emerges as a byproduct of next-token prediction, leveraging a recent connection between calibration and local loss optimality. The theory relies on a general definition of "B-calibration," which is a notion of calibration parameterized by a choice of equivalence classes (semantic or otherwise). This theoretical mechanism leads to a testable prediction: base LLMs will be semantically calibrated when they can easily predict their own distribution over semantic answer classes before generating a response. We state three implications of this prediction, which we validate through experiments: (1) Base LLMs are semantically calibrated across question-answering tasks, (2) RL instruction-tuning systematically breaks this calibration, and (3) chain-of-thought reasoning breaks calibration. To our knowledge, our work provides the first principled explanation of when and why semantic calibration emerges in LLMs.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs</title>
<link>https://arxiv.org/abs/2511.04875</link>
<guid>https://arxiv.org/abs/2511.04875</guid>
<content:encoded><![CDATA[
<div> self-awareness, LLMs, low-rank adapters, fine-tuning, activation space <br />
Summary: 
1. The study explores the emergence of behavioral self-awareness in Large Language Models (LLMs) without explicit supervision.
2. Through experiments with low-rank adapters (LoRA), it is found that self-awareness can be induced with a single rank-1 LoRA adapter.
3. The learned self-aware behavior can be represented by a single steering vector in activation space, capturing most of the fine-tune's effect.
4. Self-awareness is domain-specific and localized, with unique representations across tasks.
5. The findings suggest that self-awareness in LLMs is a linear feature that can be easily induced and modulated. <br /> <div>
arXiv:2511.04875v1 Announce Type: new 
Abstract: Recent studies have revealed that LLMs can exhibit behavioral self-awareness: the ability to accurately describe or predict their own learned behaviors without explicit supervision. This capability raises safety concerns as it may, for example, allow models to better conceal their true abilities during evaluation. We attempt to characterize the minimal conditions under which such self-awareness emerges, and the mechanistic processes through which it manifests. Through controlled finetuning experiments on instruction-tuned LLMs with low-rank adapters (LoRA), we find: (1) that self-awareness can be reliably induced using a single rank-1 LoRA adapter; (2) that the learned self-aware behavior can be largely captured by a single steering vector in activation space, recovering nearly all of the fine-tune's behavioral effect; and (3) that self-awareness is non-universal and domain-localized, with independent representations across tasks. Together, these findings suggest that behavioral self-awareness emerges as a domain-specific, linear feature that can be easily induced and modulated.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDS KoPub VDR: A Benchmark Dataset for Visual Document Retrieval in Korean Public Documents</title>
<link>https://arxiv.org/abs/2511.04910</link>
<guid>https://arxiv.org/abs/2511.04910</guid>
<content:encoded><![CDATA[
<div> benchmark, visual document retrieval, Korean, public documents, multimodal models <br />
Summary:<br />
The article introduces SDS KoPub VDR, an innovative benchmark focusing on visual document retrieval for Korean public documents. The benchmark includes a corpus of 361 real-world documents with complex visual elements such as tables and charts. It also features 600 query-page-answer triples generated using multimodal models and refined for accuracy. The queries cover six major public domains and are categorized based on reasoning modality. Evaluation tasks include text-only retrieval and multimodal retrieval to assess model performance. Results show significant gaps in performance, particularly in multimodal scenarios requiring cross-modal reasoning. SDS KoPub VDR not only facilitates rigorous evaluation of textual and multimodal retrieval tasks but also serves as a valuable resource for advancing multimodal AI in real-world document intelligence.<br /> <div>
arXiv:2511.04910v1 Announce Type: new 
Abstract: Existing benchmarks for visual document retrieval (VDR) largely overlook non-English languages and the structural complexity of official publications. To address this critical gap, we introduce SDS KoPub VDR, the first large-scale, publicly available benchmark for retrieving and understanding Korean public documents. The benchmark is built upon a corpus of 361 real-world documents (40,781 pages), including 256 files under the KOGL Type 1 license and 105 from official legal portals, capturing complex visual elements like tables, charts, and multi-column layouts. To establish a challenging and reliable evaluation set, we constructed 600 query-page-answer triples. These were initially generated using multimodal models (e.g., GPT-4o) and subsequently underwent a rigorous human verification and refinement process to ensure factual accuracy and contextual relevance. The queries span six major public domains and are systematically categorized by the reasoning modality required: text-based, visual-based (e.g., chart interpretation), and cross-modal. We evaluate SDS KoPub VDR on two complementary tasks that reflect distinct retrieval paradigms: (1) text-only retrieval, which measures a model's ability to locate relevant document pages based solely on textual signals, and (2) multimodal retrieval, which assesses retrieval performance when visual features (e.g., tables, charts, and layouts) are jointly leveraged alongside text. This dual-task evaluation reveals substantial performance gaps, particularly in multimodal scenarios requiring cross-modal reasoning, even for state-of-the-art models. As a foundational resource, SDS KoPub VDR not only enables rigorous and fine-grained evaluation across textual and multimodal retrieval tasks but also provides a clear roadmap for advancing multimodal AI in complex, real-world document intelligence.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models</title>
<link>https://arxiv.org/abs/2511.04919</link>
<guid>https://arxiv.org/abs/2511.04919</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, BudgetMem, memory constraints, selective memory policies, document length

Summary:
BudgetMem is a novel memory augmented architecture designed to address the computational and memory challenges faced by Large Language Models when processing long contexts. It focuses on learning what information to remember rather than storing everything, utilizing selective memory policies and feature-based salience scoring. The system combines learned gating mechanisms with BM25 sparse retrieval to efficiently access information while under strict budget constraints. Through comprehensive experiments, BudgetMem demonstrates remarkable results on long documents, achieving minimal F1 score degradation while saving a significant amount of memory compared to baseline models. The approach is validated through budget sensitivity analysis, baseline comparisons, and document length analysis, highlighting its benefits increasing with longer documents. BudgetMem offers a practical solution for deploying advanced language understanding capabilities on modest hardware, making it more accessible to a wider range of users. 

<br /><br />Summary: <div>
arXiv:2511.04919v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face significant computational and memory constraints when processing long contexts, despite growing demand for applications requiring reasoning over extensive documents, multi-session dialogues, and book length texts. While recent advances have extended context windows to 100K-1M tokens, such approaches incur prohibitive costs for resource constrained deployments. We propose BudgetMem, a novel memory augmented architecture that learns what to remember rather than remembering everything. Our system combines selective memory policies with feature based salience scoring (entity density, TF-IDF, discourse markers, position bias) to decide which information merits storage under strict budget constraints. Unlike existing retrieval augmented generation (RAG) systems that store all chunks, BudgetMem employs learned gating mechanisms coupled with BM25 sparse retrieval for efficient information access. Through comprehensive experiments on 700 question answer pairs across short (237 tokens) and long (5K-10K tokens) documents with Llama-3.2-3B-Instruct, we demonstrate that BudgetMem achieves remarkable results on long documents: only 1.0% F1 score degradation while saving 72.4% memory compared to baseline RAG. We validate our approach through budget sensitivity analysis (testing 7 budget ratios), naive baseline comparisons, and document length analysis, showing that BudgetMem's benefits increase with document length. Our work provides a practical pathway for deploying capable long context systems on modest hardware, democratizing access to advanced language understanding capabilities.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentExpt: Automating AI Experiment Design with LLM-based Resource Retrieval Agent</title>
<link>https://arxiv.org/abs/2511.04921</link>
<guid>https://arxiv.org/abs/2511.04921</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model agents, dataset retrieval, baseline recommendation, experimental design, AI research

Summary:
Large language model agents are being used for tasks like information retrieval and complex reasoning on the web. To facilitate scientific research, automation of experiment design through dataset and baseline retrieval is critical in AI research. Existing methods have limitations in data coverage and bias toward superficial similarity. In this study, a framework for comprehensive baseline and dataset recommendation is proposed by leveraging collective perception from citation networks. An automated data-collection pipeline links accepted papers to the baselines and datasets they actually used. A collective perception enhanced retriever is designed to represent the scholarly network, enabling efficient candidate recall. A reasoning-augmented reranker constructs explicit reasoning chains for interpretable justifications and refined rankings. The method outperforms prior baselines with significant improvements in Recall@20 and HitRate@5 on datasets from top AI conferences. This advancement offers reliable and interpretable automation for experimental design.<br /><br />Summary: <div>
arXiv:2511.04921v1 Announce Type: new 
Abstract: Large language model agents are becoming increasingly capable at web-centric tasks such as information retrieval, complex reasoning. These emerging capabilities have given rise to surge research interests in developing LLM agent for facilitating scientific quest. One key application in AI research is to automate experiment design through agentic dataset and baseline retrieval. However, prior efforts suffer from limited data coverage, as recommendation datasets primarily harvest candidates from public portals and omit many datasets actually used in published papers, and from an overreliance on content similarity that biases model toward superficial similarity and overlooks experimental suitability. Harnessing collective perception embedded in the baseline and dataset citation network, we present a comprehensive framework for baseline and dataset recommendation. First, we design an automated data-collection pipeline that links roughly one hundred thousand accepted papers to the baselines and datasets they actually used. Second, we propose a collective perception enhanced retriever. To represent the position of each dataset or baseline within the scholarly network, it concatenates self-descriptions with aggregated citation contexts. To achieve efficient candidate recall, we finetune an embedding model on these representations. Finally, we develop a reasoning-augmented reranker that exact interaction chains to construct explicit reasoning chains and finetunes a large language model to produce interpretable justifications and refined rankings. The dataset we curated covers 85\% of the datasets and baselines used at top AI conferences over the past five years. On our dataset, the proposed method outperforms the strongest prior baseline with average gains of +5.85\% in Recall@20, +8.30\% in HitRate@5. Taken together, our results advance reliable, interpretable automation of experimental design.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagnosing and Mitigating Semantic Inconsistencies in Wikidata's Classification Hierarchy</title>
<link>https://arxiv.org/abs/2511.04926</link>
<guid>https://arxiv.org/abs/2511.04926</guid>
<content:encoded><![CDATA[
<div> entities, open knowledge graph, validation method, taxonomic inconsistency, crowdsourced

Summary:<br />
1. The study focuses on Wikidata, the largest open knowledge graph with over 120 million entities, integrating data from various databases and Wikipedia.
2. A novel validation method is proposed and applied to identify classification errors, over-generalized subclass links, and redundant connections in specific domains within Wikidata.
3. A new evaluation criterion is introduced to determine the significance of these issues for correction.
4. A system is developed for users to inspect taxonomic relationships of Wikidata entities, utilizing the platform's crowdsourced nature for improved accuracy and reliability.
5. The study aims to enhance the quality of taxonomic information on Wikidata and address existing inconsistencies, leveraging the platform's open structure for collaborative improvement. 

<br /><br />Summary: <div>
arXiv:2511.04926v1 Announce Type: new 
Abstract: Wikidata is currently the largest open knowledge graph on the web, encompassing over 120 million entities. It integrates data from various domain-specific databases and imports a substantial amount of content from Wikipedia, while also allowing users to freely edit its content. This openness has positioned Wikidata as a central resource in knowledge graph research and has enabled convenient knowledge access for users worldwide. However, its relatively loose editorial policy has also led to a degree of taxonomic inconsistency. Building on prior work, this study proposes and applies a novel validation method to confirm the presence of classification errors, over-generalized subclass links, and redundant connections in specific domains of Wikidata. We further introduce a new evaluation criterion for determining whether such issues warrant correction and develop a system that allows users to inspect the taxonomic relationships of arbitrary Wikidata entities-leveraging the platform's crowdsourced nature to its full potential.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoPT: Lossless Parallel Tokenization Acceleration for Long Context Inference of Large Language Model</title>
<link>https://arxiv.org/abs/2511.04952</link>
<guid>https://arxiv.org/abs/2511.04952</guid>
<content:encoded><![CDATA[
<div> Tokenization, Long context inference, Computational latency, Lossless Parallel Tokenization, Speedup <br />
Summary: <br />
Long context inference in large language models is important but computationally intensive. Existing methods for parallel tokenization suffer from inconsistent results. To address this issue, a novel framework called LoPT is proposed, ensuring lossless tokenization with significant speedup. LoPT uses character-position-based matching and dynamic chunk length adjustment to accurately align and merge tokenized segments. Extensive experiments on various long-text datasets demonstrate the effectiveness and consistency of LoPT. The theoretical proof and analytical studies further validate the robustness of the approach. <div>
arXiv:2511.04952v1 Announce Type: new 
Abstract: Long context inference scenarios have become increasingly important for large language models, yet they introduce significant computational latency. While prior research has optimized long-sequence inference through operators, model architectures, and system frameworks, tokenization remains an overlooked bottleneck. Existing parallel tokenization methods accelerate processing through text segmentation and multi-process tokenization, but they suffer from inconsistent results due to boundary artifacts that occur after merging. To address this, we propose LoPT, a novel Lossless Parallel Tokenization framework that ensures output identical to standard sequential tokenization. Our approach employs character-position-based matching and dynamic chunk length adjustment to align and merge tokenized segments accurately. Extensive experiments across diverse long-text datasets demonstrate that LoPT achieves significant speedup while guaranteeing lossless tokenization. We also provide theoretical proof of consistency and comprehensive analytical studies to validate the robustness of our method.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Too Good to be Bad: On the Failure of LLMs to Role-Play Villains</title>
<link>https://arxiv.org/abs/2511.04962</link>
<guid>https://arxiv.org/abs/2511.04962</guid>
<content:encoded><![CDATA[
<div> Moral RolePlay benchmark, large language models, creative generation, fictional characters, safety alignment <br />
<br />
Summary: The study explores the ability of Large Language Models (LLMs) to portray morally ambiguous or villainous characters. The researchers introduce the Moral RolePlay benchmark, which evaluates LLMs' role-playing fidelity on a four-level moral alignment scale. Results show a decline in role-playing accuracy as character morality decreases, with models struggling with traits like deceit and manipulation. The study highlights a conflict between model safety and creative fidelity, as highly safety-aligned models perform poorly in playing villainous roles. The findings emphasize the need for more nuanced alignment methods to address this limitation. <div>
arXiv:2511.04962v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly tasked with creative generation, including the simulation of fictional characters. However, their ability to portray non-prosocial, antagonistic personas remains largely unexamined. We hypothesize that the safety alignment of modern LLMs creates a fundamental conflict with the task of authentically role-playing morally ambiguous or villainous characters. To investigate this, we introduce the Moral RolePlay benchmark, a new dataset featuring a four-level moral alignment scale and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs with role-playing characters from moral paragons to pure villains. Our large-scale evaluation reveals a consistent, monotonic decline in role-playing fidelity as character morality decreases. We find that models struggle most with traits directly antithetical to safety principles, such as ``Deceitful'' and ``Manipulative'', often substituting nuanced malevolence with superficial aggression. Furthermore, we demonstrate that general chatbot proficiency is a poor predictor of villain role-playing ability, with highly safety-aligned models performing particularly poorly. Our work provides the first systematic evidence of this critical limitation, highlighting a key tension between model safety and creative fidelity. Our benchmark and findings pave the way for developing more nuanced, context-aware alignment methods.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Acquiring Common Chinese Emotional Events Using Large Language Model</title>
<link>https://arxiv.org/abs/2511.04989</link>
<guid>https://arxiv.org/abs/2511.04989</guid>
<content:encoded><![CDATA[
<div> Keywords: Chinese emotional events, large language model, sentiment polarity, knowledge base, emotion cause extraction

Summary:
Knowledge about emotional events is crucial for improving the effectiveness of various applications. This paper focuses on acquiring common emotional events in Chinese by utilizing a large language model (LLM) and a filter to ensure quality. By collecting emotional event indicators and prompting the LLM, the study generated 102,218 high-quality emotional events with sentiment polarity labels, forming a valuable knowledge base. The approach demonstrated effectiveness in acquiring common Chinese emotional events and showed potential for emotion cause extraction (ECE). The release of related emotional event indicators and emotional events will further contribute to the field. <div>
arXiv:2511.04989v1 Announce Type: new 
Abstract: Knowledge about emotional events is an important kind of knowledge which has been applied to improve the effectiveness of different applications. However, emotional events cannot be easily acquired, especially common or generalized emotional events that are context-independent. The goal of this paper is to obtain common emotional events in Chinese language such as "win a prize" and "be criticized". Our approach begins by collecting a comprehensive list of Chinese emotional event indicators. Then, we generate emotional events by prompting a Chinese large language model (LLM) using these indicators. To ensure the quality of these emotional events, we train a filter to discard invalid generated results. We also classify these emotional events as being positive events and negative events using different techniques. Finally, we harvest a total of 102,218 high-quality common emotional events with sentiment polarity labels, which is the only large-scale commonsense knowledge base of emotional events in Chinese language. Intrinsic evaluation results show that the proposed method in this paper can be effectively used to acquire common Chinese emotional events. An extrinsic use case also demonstrates the strong potential of common emotional events in the field of emotion cause extraction (ECE). Related resources including emotional event indicators and emotional events will be released after the publication of this paper.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies</title>
<link>https://arxiv.org/abs/2511.05018</link>
<guid>https://arxiv.org/abs/2511.05018</guid>
<content:encoded><![CDATA[
<div> dynamic evaluation suite, LLMs, pluralistic alignment, behavioral policies, adversarial conditions

Summary:
PBSUITE is introduced as a dynamic evaluation suite for assessing LLMs' adherence to pluralistic alignment in multi-turn conversations. It includes a dataset with 300 behavioral policies from various industries and an evaluation framework to test model compliance in adversarial conditions. Leading LLMs show strong adherence to behavioral policies in single-turn interactions but struggle in multi-turn adversarial settings, indicating a gap in enforcing pluralistic behavioral policies. The study highlights the need for improved alignment and safety measures to support diverse user values in real-world LLM interactions. <div>
arXiv:2511.05018v1 Announce Type: new 
Abstract: Large language models (LLMs) are typically aligned to a universal set of safety and usage principles intended for broad public acceptability. Yet, real-world applications of LLMs often take place within organizational ecosystems shaped by distinctive corporate policies, regulatory requirements, use cases, brand guidelines, and ethical commitments. This reality highlights the need for rigorous and comprehensive evaluation of LLMs with pluralistic alignment goals, an alignment paradigm that emphasizes adaptability to diverse user values and needs. In this work, we present PLURALISTIC BEHAVIOR SUITE (PBSUITE), a dynamic evaluation suite designed to systematically assess LLMs' capacity to adhere to pluralistic alignment specifications in multi-turn, interactive conversations. PBSUITE consists of (1) a diverse dataset of 300 realistic LLM behavioral policies, grounded in 30 industries; and (2) a dynamic evaluation framework for stress-testing model compliance with custom behavioral specifications under adversarial conditions. Using PBSUITE, We find that leading open- and closed-source LLMs maintain robust adherence to behavioral policies in single-turn settings (less than 4% failure rates), but their compliance weakens substantially in multi-turn adversarial interactions (up to 84% failure rates). These findings highlight that existing model alignment and safety moderation methods fall short in coherently enforcing pluralistic behavioral policies in real-world LLM interactions. Our work contributes both the dataset and analytical framework to support future research toward robust and context-aware pluralistic alignment techniques.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code Generation in Ukrainian</title>
<link>https://arxiv.org/abs/2511.05040</link>
<guid>https://arxiv.org/abs/2511.05040</guid>
<content:encoded><![CDATA[
<div> Benchmark, code generation, competitive programming, low-resource languages, language models  
Summary:  
The paper introduces UA-Code-Bench, a new benchmark for evaluating code generation and problem-solving abilities of language models in Ukrainian. The benchmark consists of 500 problems from Eolymp, categorized by complexity levels. Results show that even top models like OpenAI o3 and GPT-5 struggle to solve half of the problems, indicating the difficulty of code generation in low-resource languages. The study analyzes performance across difficulty levels and assesses solution uniqueness and efficiency in terms of time and memory consumption. The research highlights the importance of competitive programming benchmarks in evaluating language models, particularly in underrepresented languages, and sets the stage for future work on multilingual code generation and reasoning-enhanced models. The benchmark and related scripts are available for further exploration.  
Summary:  <div>
arXiv:2511.05040v1 Announce Type: new 
Abstract: Evaluating the real capabilities of large language models in low-resource languages still represents a challenge, as many existing benchmarks focus on widespread tasks translated from English or evaluate only simple language understanding. This paper introduces UA-Code-Bench, a new open-source benchmark established for a thorough evaluation of language models' code generation and competitive programming problem-solving abilities in Ukrainian. The benchmark comprises 500 problems from the Eolymp platform, evenly distributed across five complexity levels from very easy to very hard. A diverse set of 13 leading proprietary and open-source models, generating Python solutions based on a one-shot prompt, was evaluated via the dedicated Eolymp environment against hidden tests, ensuring code correctness. The obtained results reveal that even top-performing models, such as OpenAI o3 and GPT-5, solve only half of the problems, highlighting the challenge of code generation in low-resource natural language. Furthermore, this research presents a comprehensive analysis of performance across various difficulty levels, as well as an assessment of solution uniqueness and computational efficiency, measured by both elapsed time and memory consumption of the generated solutions. In conclusion, this work demonstrates the value of competitive programming benchmarks in evaluating large language models, especially in underrepresented languages. It also paves the way for future research on multilingual code generation and reasoning-enhanced models. The benchmark, data parsing, preparation, code generation, and evaluation scripts are available at https://huggingface.co/datasets/NLPForUA/ua-code-bench.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Order-Level Attention Similarity Across Language Models: A Latent Commonality</title>
<link>https://arxiv.org/abs/2511.05064</link>
<guid>https://arxiv.org/abs/2511.05064</guid>
<content:encoded><![CDATA[
<div> attention aggregation, language models, transfer learning, syntactic knowledge, adapter

Summary: 
This paper investigates the commonalities in context aggregation patterns across different language models (LMs). By analyzing the Order-Level Attention (OLA) derived from Attention Rollout, the study reveals significant similarities in OLA among different LMs at the same order, suggesting a implicit mapping to syntactic knowledge. Based on these findings, the Transferable OLA Adapter (TOA) is proposed as a training-free method for cross-LM knowledge transfer. By utilizing OLA as a syntactic feature representation, TOA generalizes effectively to unseen LMs without parameter updates. Extensive experiments demonstrate that the TOA method enhances the performance of unseen LMs through cross-LM generalization. The code implementation of the proposed method is available on GitHub for further exploration and application. 

<br /><br />Summary: <div>
arXiv:2511.05064v1 Announce Type: new 
Abstract: In this paper, we explore an important yet previously neglected question: Do context aggregation patterns across Language Models (LMs) share commonalities? While some works have investigated context aggregation or attention weights in LMs, they typically focus on individual models or attention heads, lacking a systematic analysis across multiple LMs to explore their commonalities. In contrast, we focus on the commonalities among LMs, which can deepen our understanding of LMs and even facilitate cross-model knowledge transfer. In this work, we introduce the Order-Level Attention (OLA) derived from the order-wise decomposition of Attention Rollout and reveal that the OLA at the same order across LMs exhibits significant similarities. Furthermore, we discover an implicit mapping between OLA and syntactic knowledge. Based on these two findings, we propose the Transferable OLA Adapter (TOA), a training-free cross-LM adapter transfer method. Specifically, we treat the OLA as a unified syntactic feature representation and train an adapter that takes OLA as input. Due to the similarities in OLA across LMs, the adapter generalizes to unseen LMs without requiring any parameter updates. Extensive experiments demonstrate that TOA's cross-LM generalization effectively enhances the performance of unseen LMs. Code is available at https://github.com/jinglin-liang/OLAS.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning-Guided Claim Normalization for Noisy Multilingual Social Media Posts</title>
<link>https://arxiv.org/abs/2511.05078</link>
<guid>https://arxiv.org/abs/2511.05078</guid>
<content:encoded><![CDATA[
<div> Keywords: claim normalization, multilingual misinformation detection, cross-lingual transfer, social media, linguistic structures

Summary:<br /><br />
1. The study focuses on claim normalization for multilingual misinformation detection across 20 languages by utilizing a systematic decomposition approach based on Who, What, Where, When, Why, and How questions.
2. The methodology involves finetuning a model named Qwen3-14B using LoRA and utilizing token-level recall filtering, intra-post deduplication, and retrieval-augmented few-shot learning with contextual examples for robust cross-lingual transfer.
3. Results show significant improvements in METEOR scores across different languages, with a relative improvement of 41.3% over baseline configurations.
4. The approach demonstrates effective cross-lingual generalization for Romance and Germanic languages while maintaining semantic coherence across diverse linguistic structures.
5. The system achieves competitive rankings on the English, Dutch, and Punjabi leaderboards, highlighting its effectiveness in addressing claim normalization in multilingual social media posts. 

Summary: <div>
arXiv:2511.05078v1 Announce Type: new 
Abstract: We address claim normalization for multilingual misinformation detection - transforming noisy social media posts into clear, verifiable statements across 20 languages. The key contribution demonstrates how systematic decomposition of posts using Who, What, Where, When, Why and How questions enables robust cross-lingual transfer despite training exclusively on English data. Our methodology incorporates finetuning Qwen3-14B using LoRA with the provided dataset after intra-post deduplication, token-level recall filtering for semantic alignment and retrieval-augmented few-shot learning with contextual examples during inference. Our system achieves METEOR scores ranging from 41.16 (English) to 15.21 (Marathi), securing third rank on the English leaderboard and fourth rank for Dutch and Punjabi. The approach shows 41.3% relative improvement in METEOR over baseline configurations and substantial gains over existing methods. Results demonstrate effective cross-lingual generalization for Romance and Germanic languages while maintaining semantic coherence across diverse linguistic structures.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Text Simplification Metrics and General-Purpose LLMs for Accessible Health Information, and A Potential Architectural Advantage of The Instruction-Tuned LLM class</title>
<link>https://arxiv.org/abs/2511.05080</link>
<guid>https://arxiv.org/abs/2511.05080</guid>
<content:encoded><![CDATA[
<div> health-seeking behavior, digital consumption, text simplification, linguistic capabilities, discourse fidelity 
Summary:<br /><br />The study evaluates the performance of two large language models (LLMs) for text simplification. Mistral 24B shows a balanced approach to simplification, improving readability and maintaining discourse fidelity with a high BERTScore of 0.91. In contrast, QWen2.5 32B struggles to balance readability and accuracy, resulting in a lower BERTScore of 0.89. Mistral's lexical simplification strategy enhances readability metrics and achieves a mean SARI of 42.46. The study also identifies the importance of heuristic metric selection for text simplification tasks. Further analysis reveals functional redundancies among readability indices and highlights lexical support as a crucial domain-adaptation issue for text simplification models. This empirical evidence establishes Mistral as a promising option for text simplification and provides valuable insights for improving the effectiveness of LLMs in this domain.<br /><br /> <div>
arXiv:2511.05080v1 Announce Type: new 
Abstract: The increasing health-seeking behavior and digital consumption of biomedical information by the general public necessitate scalable solutions for automatically adapting complex scientific and technical documents into plain language. Automatic text simplification solutions, including advanced large language models, however, continue to face challenges in reliably arbitrating the tension between optimizing readability performance and ensuring preservation of discourse fidelity. This report empirically assesses the performance of two major classes of general-purpose LLMs, demonstrating their linguistic capabilities and foundational readiness for the task compared to a human benchmark. Using a comparative analysis of the instruction-tuned Mistral 24B and the reasoning-augmented QWen2.5 32B, we identify a potential architectural advantage in the instruction-tuned LLM. Mistral exhibits a tempered lexical simplification strategy that enhances readability across a suite of metrics and the simplification-specific formula SARI (mean 42.46), while preserving human-level discourse with a BERTScore of 0.91. QWen also attains enhanced readability performance, but its operational strategy shows a disconnect in balancing between readability and accuracy, reaching a statistically significantly lower BERTScore of 0.89. Additionally, a comprehensive correlation analysis of 21 metrics spanning readability, discourse fidelity, content safety, and underlying distributional measures for mechanistic insights, confirms strong functional redundancies among five readability indices. This empirical evidence tracks baseline performance of the evolving LLMs for the task of text simplification, identifies the instruction-tuned Mistral 24B for simplification, provides necessary heuristics for metric selection, and points to lexical support as a primary domain-adaptation issue for simplification.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Layer-wise Distillation for Efficient Compression of Large Language Models</title>
<link>https://arxiv.org/abs/2511.05085</link>
<guid>https://arxiv.org/abs/2511.05085</guid>
<content:encoded><![CDATA[
<div> distillation methods, large language models, ShortGPT approach, layer importance, model efficiency <br />
Summary: 
This work explores distillation methods for large language models (LLMs) to create compact models without significant performance loss. By iteratively evaluating layer importance and training with a joint loss function, the proposed method based on ShortGPT reduces the number of layers in the Qwen2.5-3B model with minimal quality loss. Experiment results show that middle transformer layers contribute less to inference, indicating the potential of the approach for efficient model creation. The findings highlight the effectiveness of iterative distillation and fine-tuning, making the method suitable for resource-limited environments. <div>
arXiv:2511.05085v1 Announce Type: new 
Abstract: This work investigates distillation methods for large language models (LLMs) with the goal of developing compact models that preserve high performance. Several existing approaches are reviewed, with a discussion of their respective strengths and limitations. An improved method based on the ShortGPT approach has been developed, building upon the idea of incorporating iterative evaluation of layer importance. At each step, importance is assessed by measuring performance degradation when individual layers are removed, using a set of representative datasets. This process is combined with further training using a joint loss function based on KL divergence and mean squared error. Experiments on the Qwen2.5-3B model show that the number of layers can be reduced from 36 to 28 (resulting in a 2.47 billion parameter model) with only a 9.7% quality loss, and to 24 layers with an 18% loss. The findings suggest that the middle transformer layers contribute less to inference, underscoring the potential of the proposed method for creating efficient models. The results demonstrate the effectiveness of iterative distillation and fine-tuning, making the approach suitable for deployment in resource-limited settings.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Toolbox for Improving Evolutionary Prompt Search</title>
<link>https://arxiv.org/abs/2511.05120</link>
<guid>https://arxiv.org/abs/2511.05120</guid>
<content:encoded><![CDATA[
<div> Keywords: Evolutionary prompt optimization, LLMs, efficient evaluation, human feedback, code release

Summary:
This work introduces key improvements to evolutionary prompt optimization for refining Language Model prompts. Firstly, it decomposes evolution into distinct steps to enhance control and effectiveness. Secondly, an LLM-based judge is introduced to verify evolutions, ensuring quality. Thirdly, human feedback is integrated to refine evolutionary operators, enhancing performance further. Finally, more efficient evaluation strategies are developed to maintain performance while reducing computational overhead. These improvements result in enhanced optimization quality and efficiency, making prompt optimization more effective. The code has been released, allowing for prompt optimization on new tasks and facilitating future research in this field. 

<br /><br />Summary: <div>
arXiv:2511.05120v1 Announce Type: new 
Abstract: Evolutionary prompt optimization has demonstrated effectiveness in refining prompts for LLMs. However, existing approaches lack robust operators and efficient evaluation mechanisms. In this work, we propose several key improvements to evolutionary prompt optimization that can partially generalize to prompt optimization in general: 1) decomposing evolution into distinct steps to enhance the evolution and its control, 2) introducing an LLM-based judge to verify the evolutions, 3) integrating human feedback to refine the evolutionary operator, and 4) developing more efficient evaluation strategies that maintain performance while reducing computational overhead. Our approach improves both optimization quality and efficiency. We release our code, enabling prompt optimization on new tasks and facilitating further research in this area.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ManufactuBERT: Efficient Continual Pretraining for Manufacturing</title>
<link>https://arxiv.org/abs/2511.05135</link>
<guid>https://arxiv.org/abs/2511.05135</guid>
<content:encoded><![CDATA[
<div> Keywords: ManufactuBERT, RoBERTa, deduplication, manufacturing domain, NLP tasks 
 
Summary: <br /><br />ManufactuBERT is a specialized RoBERTa model pretrained on a large-scale corpus specifically curated for the manufacturing domain. The development of ManufactuBERT involved creating a comprehensive data processing pipeline to filter domain-specific terminology and semantics from web data. The multi-stage deduplication process implemented in the pipeline significantly reduced redundancies, leading to improved model performance. ManufactuBERT outperformed strong specialized baselines on manufacturing-related NLP tasks, demonstrating a new state-of-the-art in the field. Additionally, training on the carefully deduplicated corpus accelerated convergence, reducing training time and computational costs by 33%. The proposed pipeline serves as a reproducible example for building high-performing encoders in other specialized domains. The model and curated corpus will be released for public access. <br /><br />Summary: <div>
arXiv:2511.05135v1 Announce Type: new 
Abstract: While large general-purpose Transformer-based encoders excel at general language understanding, their performance diminishes in specialized domains like manufacturing due to a lack of exposure to domain-specific terminology and semantics. In this paper, we address this gap by introducing ManufactuBERT, a RoBERTa model continually pretrained on a large-scale corpus curated for the manufacturing domain. We present a comprehensive data processing pipeline to create this corpus from web data, involving an initial domain-specific filtering step followed by a multi-stage deduplication process that removes redundancies. Our experiments show that ManufactuBERT establishes a new state-of-the-art on a range of manufacturing-related NLP tasks, outperforming strong specialized baselines. More importantly, we demonstrate that training on our carefully deduplicated corpus significantly accelerates convergence, leading to a 33\% reduction in training time and computational cost compared to training on the non-deduplicated dataset. The proposed pipeline offers a reproducible example for developing high-performing encoders in other specialized domains. We will release our model and curated corpus at https://huggingface.co/cea-list-ia.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Gap... or Not? How Translation Errors and Evaluation Details Skew Multilingual Results</title>
<link>https://arxiv.org/abs/2511.05162</link>
<guid>https://arxiv.org/abs/2511.05162</guid>
<content:encoded><![CDATA[
<div> math, language models, multilingual, performance, dataset

Summary:
- Study on performance of language models across different languages, focusing on math domain.
- Identified translation errors in standard math benchmark dataset (MGSM).
- Lack of standardized answer extraction from language model outputs affecting results.
- Proposed method for automatic quality assurance to address translation errors at scale.
- Recommendations provided to address lack of standardized answer extraction.
- Language gap mostly disappears with proposed approaches, leading to different research conclusions.
- Corrected dataset released to the community for further research. 

<br /><br />Summary: <div>
arXiv:2511.05162v1 Announce Type: new 
Abstract: Most current large language models (LLMs) support a wide variety of languages in addition to English, including high-resource languages (e.g. German, Chinese, French), as well as low-resource ones (e.g. Swahili, Telugu). In addition they have also shown impressive capabilities in different domains, like coding, science and math. In this short paper, taking math as an example domain, we study the performance of different LLMs across languages. Experimental results show that there exists a non-negligible and consistent gap in the performance of the models across languages. Interestingly, and somewhat against expectations, the gap exists for both high- and low-resource languages. We hope that these results influence further research into cross-lingual capability generalization for next generation LLMs. If it weren't for the fact that they are false! By analyzing one of the standard multilingual math benchmarks (MGSM), we determine that several translation errors are present in the data. Furthermore, the lack of standardized answer extraction from LLM outputs further influences the final results. We propose a method for automatic quality assurance to address the first issue at scale, and give recommendations to address the second one. Combining these two approaches we show that the aforementioned language gap mostly disappears, leading to completely different conclusions from our research. We additionally release the corrected dataset to the community.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effectiveness of Chain-of-Thought in Distilling Reasoning Capability from Large Language Models</title>
<link>https://arxiv.org/abs/2511.05184</link>
<guid>https://arxiv.org/abs/2511.05184</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought prompting, Large Language Models, Knowledge Distillation, White-box KD, Natural Language Reasoning<br />
Summary:<br />
Chain-of-Thought (CoT) prompting is utilized to enhance the reasoning capability of Large Language Models (LLMs) and has been applied in Knowledge Distillation (KD) for transferring reasoning skills from larger to smaller LLMs. This study investigates the impact of CoT in white-box KD for reasoning improvement, using LLMs from Qwen and Llama2 families and CoT data from the CoT-Collection dataset. The performance of distilled models is evaluated on challenging tasks from the BIG-Bench-Hard (BBH) benchmark. Results show the effectiveness of CoT in enhancing white-box KD, leading to better performance of distilled models in natural language reasoning and understanding tasks from BBH.<br /><br />Summary: <div>
arXiv:2511.05184v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) prompting is a widely used method to improve the reasoning capability of Large Language Models (LLMs). More recently, CoT has been leveraged in Knowledge Distillation (KD) to transfer reasoning capability from a larger LLM to a smaller one. This paper examines the role of CoT in distilling the reasoning capability from larger LLMs to smaller LLMs using white-box KD, analysing its effectiveness in improving the performance of the distilled models for various natural language reasoning and understanding tasks. We conduct white-box KD experiments using LLMs from the Qwen and Llama2 families, employing CoT data from the CoT-Collection dataset. The distilled models are then evaluated on natural language reasoning and understanding tasks from the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for smaller LLMs. Experimental results demonstrate the role of CoT in improving white-box KD effectiveness, enabling the distilled models to achieve better average performance in natural language reasoning and understanding tasks from BBH.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Translation via Annotation: A Computational Study of Translating Classical Chinese into Japanese</title>
<link>https://arxiv.org/abs/2511.05239</link>
<guid>https://arxiv.org/abs/2511.05239</guid>
<content:encoded><![CDATA[
<div> translation, sequence tagging, low-resource, language models, annotation

Summary:
In this study, the process of translating classical Chinese into Japanese through character annotation is analyzed as sequence tagging tasks compatible with modern language technologies. The research addresses the challenge of low-resource availability in this annotation and translation system by introducing a LLM-based annotation pipeline and developing a new dataset from digitalized open-source translation data. The study demonstrates that incorporating auxiliary Chinese NLP tasks enhances the training of sequence tagging tasks in low-resource settings. While large language models excel in direct machine translation, they struggle with character annotation tasks. The proposed method serves as a complementary tool to LLMs, showcasing its potential in enhancing translation accuracy and efficiency. <div>
arXiv:2511.05239v1 Announce Type: new 
Abstract: Ancient people translated classical Chinese into Japanese by annotating around each character. We abstract this process as sequence tagging tasks and fit them into modern language technologies. The research of this annotation and translation system is a facing low-resource problem. We release this problem by introducing a LLM-based annotation pipeline and construct a new dataset from digitalized open-source translation data. We show that under the low-resource setting, introducing auxiliary Chinese NLP tasks has a promoting effect on the training of sequence tagging tasks. We also evaluate the performance of large language models. They achieve high scores in direct machine translation, but they are confused when being asked to annotate characters. Our method could work as a supplement of LLMs.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reflective Personalization Optimization: A Post-hoc Rewriting Framework for Black-Box Large Language Models</title>
<link>https://arxiv.org/abs/2511.05286</link>
<guid>https://arxiv.org/abs/2511.05286</guid>
<content:encoded><![CDATA[
<div> reflection module, personalization, large language models, optimization, user-centric generation<br />
<br />
Summary:
The article introduces Reflective Personalization Optimization (RPO), a new framework for personalizing black-box large language models (LLMs). RPO decouples content generation from alignment in two stages: a base model generates a generic response, and an external reflection module rewrites it to align with user preferences. The reflection module is trained through supervised fine-tuning and reinforcement learning. Experiments on the LaMP benchmark show that RPO outperforms current methods, highlighting the effectiveness of explicit response shaping over implicit context injection. RPO provides an efficient, model-agnostic personalization layer that can enhance user-centric generation scenarios by integrating seamlessly with base models. <div>
arXiv:2511.05286v1 Announce Type: new 
Abstract: The personalization of black-box large language models (LLMs) is a critical yet challenging task. Existing approaches predominantly rely on context injection, where user history is embedded into the prompt to directly guide the generation process. However, this single-step paradigm imposes a dual burden on the model: generating accurate content while simultaneously aligning with user-specific styles. This often results in a trade-off that compromises output quality and limits precise control. To address this fundamental tension, we propose Reflective Personalization Optimization (RPO), a novel framework that redefines the personalization paradigm by decoupling content generation from alignment. RPO operates in two distinct stages: first, a base model generates a high-quality, generic response; then, an external reflection module explicitly rewrites this output to align with the user's preferences. This reflection module is trained using a two-stage process. Initially, supervised fine-tuning is employed on structured rewriting trajectories to establish a core personalized reasoning policy that models the transformation from generic to user-aligned responses. Subsequently, reinforcement learning is applied to further refine and enhance the quality of the personalized outputs. Comprehensive experiments on the LaMP benchmark demonstrate that RPO, by decoupling content generation from personalization, significantly outperforms state-of-the-art baselines. These findings underscore the superiority of explicit response shaping over implicit context injection. Moreover, RPO introduces an efficient, model-agnostic personalization layer that can be seamlessly integrated with any underlying base model, paving the way for a new and effective direction in user-centric generation scenarios.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Listening Between the Lines: Decoding Podcast Narratives with Language Modeling</title>
<link>https://arxiv.org/abs/2511.05310</link>
<guid>https://arxiv.org/abs/2511.05310</guid>
<content:encoded><![CDATA[
<div> Keywords: podcasts, narrative frames, persuasion, discourse, BERT <br />
Summary: <br />
Podcasts play a crucial role in shaping public opinion, making them a valuable resource for studying contemporary discourse. However, their unscripted and conversational nature poses challenges for automated analysis. Current language models struggle to accurately capture the narrative frames used in podcasts, hindering large-scale analysis. To address this, a fine-tuned BERT model is developed to link narrative frames to specific entities mentioned in conversations. This methodology bridges the gap between abstract frames and concrete details, enabling a more nuanced analysis of podcast narratives. By correlating granular frame labels with high-level topics, broader discourse trends can be revealed. The study highlights a novel frame-labeling approach that aligns better with human judgment for messy data and uncovers the relationship between discussed topics and presentation frames, providing a robust framework for studying influence in digital media. <br /> <div>
arXiv:2511.05310v1 Announce Type: new 
Abstract: Podcasts have become a central arena for shaping public opinion, making them a vital source for understanding contemporary discourse. Their typically unscripted, multi-themed, and conversational style offers a rich but complex form of data. To analyze how podcasts persuade and inform, we must examine their narrative structures -- specifically, the narrative frames they employ.
  The fluid and conversational nature of podcasts presents a significant challenge for automated analysis. We show that existing large language models, typically trained on more structured text such as news articles, struggle to capture the subtle cues that human listeners rely on to identify narrative frames. As a result, current approaches fall short of accurately analyzing podcast narratives at scale.
  To solve this, we develop and evaluate a fine-tuned BERT model that explicitly links narrative frames to specific entities mentioned in the conversation, effectively grounding the abstract frame in concrete details. Our approach then uses these granular frame labels and correlates them with high-level topics to reveal broader discourse trends. The primary contributions of this paper are: (i) a novel frame-labeling methodology that more closely aligns with human judgment for messy, conversational data, and (ii) a new analysis that uncovers the systematic relationship between what is being discussed (the topic) and how it is being presented (the frame), offering a more robust framework for studying influence in digital media.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Are the Facts? Automated Extraction of Court-Established Facts from Criminal-Court Opinions</title>
<link>https://arxiv.org/abs/2511.05320</link>
<guid>https://arxiv.org/abs/2511.05320</guid>
<content:encoded><![CDATA[
<div> Keywords: Criminal justice, court decisions, sentencing descriptions, regular expressions, large language models

Summary: 
Court decisions in continental European countries provide detailed descriptions of criminal behaviors in verdicts. This study from Slovakia explores the use of regular expressions and large language models (LLMs) to extract these descriptions. While a baseline method using regular expressions identified descriptions in only 40.5% of verdicts, advanced regular expressions and LLMs significantly improved performance, reaching 97% and 98.75%, respectively. When combined, the methods achieved a detection rate of 99.5%. Evaluation by law students showed that both advanced methods matched human annotations in about 90% of cases, compared to just 34.5% for the baseline. LLMs were particularly effective, fully matching human-labeled descriptions in 91.75% of instances. The combination of advanced regular expressions with LLMs reached a matching rate of 92%. <div>
arXiv:2511.05320v1 Announce Type: new 
Abstract: Criminal justice administrative data contain only a limited amount of information about the committed offense. However, there is an unused source of extensive information in continental European courts' decisions: descriptions of criminal behaviors in verdicts by which offenders are found guilty. In this paper, we study the feasibility of extracting these descriptions from publicly available court decisions from Slovakia. We use two different approaches for retrieval: regular expressions and large language models (LLMs). Our baseline was a simple method employing regular expressions to identify typical words occurring before and after the description. The advanced regular expression approach further focused on "sparing" and its normalization (insertion of spaces between individual letters), typical for delineating the description. The LLM approach involved prompting the Gemini Flash 2.0 model to extract the descriptions using predefined instructions. Although the baseline identified descriptions in only 40.5% of verdicts, both methods significantly outperformed it, achieving 97% with advanced regular expressions and 98.75% with LLMs, and 99.5% when combined. Evaluation by law students showed that both advanced methods matched human annotations in about 90% of cases, compared to just 34.5% for the baseline. LLMs fully matched human-labeled descriptions in 91.75% of instances, and a combination of advanced regular expressions with LLMs reached 92%.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Subword Tokenization Techniques for Bengali: A Benchmark Study with BengaliBPE</title>
<link>https://arxiv.org/abs/2511.05324</link>
<guid>https://arxiv.org/abs/2511.05324</guid>
<content:encoded><![CDATA[
<div> Tokenization, Natural Language Processing (NLP), Bengali, Byte Pair Encoding (BPE), morphologically rich languages
Summary:
BengaliBPE, a BPE tokenizer designed for Bengali script, addresses limitations of current tokenizers by applying Unicode normalization, grapheme-level initialization, and morphology-aware merge rules. Evaluation on a Bengali news classification dataset shows BengaliBPE provides detailed segmentation and better morphological interpretability compared to baselines like Whitespace, SentencePiece BPE, and HuggingFace BPE, despite slightly higher computational cost. This highlights the importance of language-aware tokenization for morphologically rich scripts and establishes BengaliBPE as a strong foundation for future Bengali NLP systems, including large-scale pretraining of contextual language models. 
<br /><br />Summary: <div>
arXiv:2511.05324v1 Announce Type: new 
Abstract: Tokenization is an important first step in Natural Language Processing (NLP) pipelines because it decides how models learn and represent linguistic information. However, current subword tokenizers like SentencePiece or HuggingFace BPE are mostly designed for Latin or multilingual corpora and do not perform well on languages with rich morphology such as Bengali. To address this limitation, we present BengaliBPE, a Byte Pair Encoding (BPE) tokenizer specifically developed for the Bengali script. BengaliBPE applies Unicode normalization, grapheme-level initialization, and morphology-aware merge rules to maintain linguistic consistency and preserve subword integrity. We use a large-scale Bengali news classification dataset to compare BengaliBPE with three baselines: Whitespace, SentencePiece BPE, and HuggingFace BPE. The evaluation considers tokenization granularity, encoding speed, and downstream classification accuracy. While all methods perform reasonably well, BengaliBPE provides the most detailed segmentation and the best morphological interpretability, albeit with slightly higher computational cost. These findings highlight the importance of language-aware tokenization for morphologically rich scripts and establish BengaliBPE as a strong foundation for future Bengali NLP systems, including large-scale pretraining of contextual language models.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A multimodal multiplex of the mental lexicon for multilingual individuals</title>
<link>https://arxiv.org/abs/2511.05361</link>
<guid>https://arxiv.org/abs/2511.05361</guid>
<content:encoded><![CDATA[
<div> Bilingualism, cognitive load, multilinguals, mental lexicon, heritage language
Summary:
This research proposal focuses on understanding the architecture of the bilingual word recognition system and how multilinguals may outperform monolinguals in various linguistic and cognitive tasks. The study builds on previous work on multiplex models of the mental lexicon and the Bilingual Interactive Activation framework. It introduces multimodality by incorporating visual inputs into a translation task to explore how visual input affects participants' proficiency and accuracy compared to text-only conditions. The research aims to investigate how a heritage language influences the acquisition of another language. <div>
arXiv:2511.05361v1 Announce Type: new 
Abstract: Historically, bilingualism was often perceived as an additional cognitive load that could hinder linguistic and intellectual development. However, over the last three decades, this view has changed considerably. Numerous studies have aimed to model and understand the architecture of the bilingual word recognition system Dijkstra and van Heuven (2002), investigating how parallel activation operates in the brain and how one language influences another Kroll et al. (2015). Increasingly, evidence suggests that multilinguals, individuals who speak three or more languages, can perform better than monolinguals in various linguistic and cognitive tasks, such as learning an additional language Abu-Rabia and Sanitsky (2010). This research proposal focuses on the study of the mental lexicon and how it may be structured in individuals who speak multiple languages. Building on the work of Stella et al. (2018), who investigated explosive learning in humans using a multiplex model of the mental lexicon, and the Bilingual Interactive Activation (BIA+) framework proposed by Dijkstra and van Heuven (2002), the present study applies the same multilayer network principles introduced by Kivela et al. (2014). Our experimental design extends previous research by incorporating multimodality into the multiplex model, introducing an additional layer that connects visual inputs to their corresponding lexical representations across the multilingual layers of the mental lexicon. In this research, we aim to explore how a heritage language influences the acquisition of another language. Specifically, we ask: Does the presence of visual input in a translation task influence participants' proficiency and accuracy compared to text-only conditions?
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models for Explainable Threat Intelligence</title>
<link>https://arxiv.org/abs/2511.05406</link>
<guid>https://arxiv.org/abs/2511.05406</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, cybersecurity threats, retrieval-augmented generation, threat intelligence, explainability<br />
Summary: <br />
- The paper discusses the use of Large Language Models (LLMs) with retrieval-augmented generation (RAG) to enhance cybersecurity threat intelligence.
- The proposed system, RAGRecon, utilizes LLMs with RAG to provide answers about cybersecurity threats in a transparent and interpretable manner.
- RAGRecon generates knowledge graphs for every response, increasing the model's explainability and allowing analysts to understand the reasoning behind the model's connections.
- Experimental evaluation with two datasets and seven LLMs showed that RAGRecon provided responses matching the reference responses over 91% of the time with the best combinations.
- By combining real-time information retrieval with domain-specific data, RAGRecon demonstrates the potential for LLMs to improve cybersecurity threat analysis. <br /><br />Summary: <div>
arXiv:2511.05406v1 Announce Type: new 
Abstract: As cyber threats continue to grow in complexity, traditional security mechanisms struggle to keep up. Large language models (LLMs) offer significant potential in cybersecurity due to their advanced capabilities in text processing and generation. This paper explores the use of LLMs with retrieval-augmented generation (RAG) to obtain threat intelligence by combining real-time information retrieval with domain-specific data. The proposed system, RAGRecon, uses a LLM with RAG to answer questions about cybersecurity threats. Moreover, it makes this form of Artificial Intelligence (AI) explainable by generating and visually presenting to the user a knowledge graph for every reply. This increases the transparency and interpretability of the reasoning of the model, allowing analysts to better understand the connections made by the system based on the context recovered by the RAG system. We evaluated RAGRecon experimentally with two datasets and seven different LLMs and the responses matched the reference responses more than 91% of the time for the best combinations.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minority-Aware Satisfaction Estimation in Dialogue Systems via Preference-Adaptive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.05407</link>
<guid>https://arxiv.org/abs/2511.05407</guid>
<content:encoded><![CDATA[
<div> Framework, User satisfaction estimation, Individual preferences, Group-level preferences, Reinforcement learning  
Summary:  
- The study addresses the subjective nature of user satisfaction in dialogue systems and the differences in satisfaction ratings among minority and majority users due to individual intents and preferences.  
- Introduces Chain-of-Personalized-Reasoning (CoPeR) to capture individual preferences through interpretable reasoning chains.  
- Proposes Majority-Minority Preference-Aware Clustering (M2PC) algorithm to discover distinct user groups and learn group-level preferences in an unsupervised manner.  
- Integrates CoPeR and M2PC into a preference-adaptive reinforcement learning framework (PAda-PPO) to optimize alignment with both individual and group preferences.  
- Experimental results on the Emotional Support Conversation dataset show improvements in user satisfaction estimation, particularly for underrepresented user groups.  

Summary: <div>
arXiv:2511.05407v1 Announce Type: new 
Abstract: User satisfaction in dialogue systems is inherently subjective. When the same response strategy is applied across users, minority users may assign different satisfaction ratings than majority users due to variations in individual intents and preferences. However, existing alignment methods typically train one-size-fits-all models that aim for broad consensus, often overlooking minority perspectives and user-specific adaptation. We propose a unified framework that models both individual- and group-level preferences for user satisfaction estimation. First, we introduce Chain-of-Personalized-Reasoning (CoPeR) to capture individual preferences through interpretable reasoning chains. Second, we propose an expectation-maximization-based Majority-Minority Preference-Aware Clustering (M2PC) algorithm that discovers distinct user groups in an unsupervised manner to learn group-level preferences. Finally, we integrate these components into a preference-adaptive reinforcement learning framework (PAda-PPO) that jointly optimizes alignment with both individual and group preferences. Experiments on the Emotional Support Conversation dataset demonstrate consistent improvements in user satisfaction estimation, particularly for underrepresented user groups.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steering Language Models with Weight Arithmetic</title>
<link>https://arxiv.org/abs/2511.05408</link>
<guid>https://arxiv.org/abs/2511.05408</guid>
<content:encoded><![CDATA[
<div> steering, language models, feedback, weight arithmetic, behavioral control 
Summary:
Contrastive weight steering is introduced as a post-training method for Large Language Models (LLMs) to improve feedback quality and generalization on diverse training distributions. By manipulating model weights through weight arithmetic, behavior direction in weight-space is isolated, mitigating sycophancy and inducing misalignment. Weight steering surpasses activation steering in out-of-distribution behavioral control while maintaining general capabilities. It effectively mitigates sycophancy and under-refusals post task-specific fine-tuning without compromising task performance gains. The method also shows promise in detecting emergent misalignment by monitoring weight evolution during training. This technique suggests a potential for early detection of rare misaligned behaviors not observed during regular training or evaluations. <br /><br />Summary: <div>
arXiv:2511.05408v1 Announce Type: new 
Abstract: Providing high-quality feedback to Large Language Models (LLMs) on a diverse training distribution can be difficult and expensive, and providing feedback only on a narrow distribution can result in unintended generalizations. To better leverage narrow training data, we propose contrastive weight steering, a simple post-training method that edits the model parameters using weight arithmetic. We isolate a behavior direction in weight-space by subtracting the weight deltas from two small fine-tunes -- one that induces the desired behavior and another that induces its opposite -- and then add or remove this direction to modify the model's weights. We apply this technique to mitigate sycophancy and induce misalignment, and find that weight steering often generalizes further than activation steering, achieving stronger out-of-distribution behavioral control before degrading general capabilities. We also show that, in the context of task-specific fine-tuning, weight steering can partially mitigate undesired behavioral drift: it can reduce sycophancy and under-refusals introduced during fine-tuning while preserving task performance gains. Finally, we provide preliminary evidence that emergent misalignment can be detected by measuring the similarity between fine-tuning updates and an "evil" weight direction, suggesting that it may be possible to monitor the evolution of weights during training and detect rare misaligned behaviors that never manifest during training or evaluations.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIMIC-SR-ICD11: A Dataset for Narrative-Based Diagnosis</title>
<link>https://arxiv.org/abs/2511.05485</link>
<guid>https://arxiv.org/abs/2511.05485</guid>
<content:encoded><![CDATA[
<div> Keywords: Disease diagnosis, healthcare, electronic health record, diagnostic dataset, likelihood-based re-ranking

Summary:
Disease diagnosis plays a crucial role in healthcare, allowing for early detection and appropriate intervention. The MIMIC-SR-ICD11 dataset has been introduced, which aligns English diagnostic data from electronic health record discharge notes with WHO ICD-11 terminology. A new framework called LL-Rank has been developed, which uses a likelihood-based re-ranking approach to improve diagnostic accuracy. LL-Rank outperforms a baseline method across different model backbones, with its primary advantage attributed to PMI-based scoring that focuses on semantic compatibility and reduces label frequency bias. This innovative approach enhances the preservation of clinically important signals that may be overlooked in traditional templated EHR documentation, ultimately improving the accuracy and efficiency of disease diagnosis in the healthcare setting. 

<br /><br />Summary: Disease diagnosis is essential for early detection and intervention in healthcare. The MIMIC-SR-ICD11 dataset aligns diagnostic data with WHO ICD-11 terminology, while LL-Rank, a likelihood-based re-ranking framework, boosts diagnostic accuracy by focusing on semantic compatibility and reducing label frequency bias. This innovative approach enhances the retention of crucial clinical signals often omitted in electronic health records, improving disease diagnosis efficiency and accuracy in healthcare settings. <div>
arXiv:2511.05485v1 Announce Type: new 
Abstract: Disease diagnosis is a central pillar of modern healthcare, enabling early detection and timely intervention for acute conditions while guiding lifestyle adjustments and medication regimens to prevent or slow chronic disease. Self-reports preserve clinically salient signals that templated electronic health record (EHR) documentation often attenuates or omits, especially subtle but consequential details. To operationalize this shift, we introduce MIMIC-SR-ICD11, a large English diagnostic dataset built from EHR discharge notes and natively aligned to WHO ICD-11 terminology. We further present LL-Rank, a likelihood-based re-ranking framework that computes a length-normalized joint likelihood of each label given the clinical report context and subtracts the corresponding report-free prior likelihood for that label. Across seven model backbones, LL-Rank consistently outperforms a strong generation-plus-mapping baseline (GenMap). Ablation experiments show that LL-Rank's gains primarily stem from its PMI-based scoring, which isolates semantic compatibility from label frequency bias.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity</title>
<link>https://arxiv.org/abs/2511.04686</link>
<guid>https://arxiv.org/abs/2511.04686</guid>
<content:encoded><![CDATA[
<div> cache, language models, positional encodings, eviction strategies, model architecture
<br />
Summary:<br />
The Key-Value (KV) cache is crucial for efficient autoregressive inference in large language models (LLMs), but in stateful multi-turn scenarios, its uncontrolled growth poses challenges. The study examines the relationship between KV cache management strategies, model architecture limits, and the integrity of positional encodings. It finds that LLM generation quality declines when the accumulated KV cache approaches or exceeds the model's trained context window, distinct from GPU memory issues. Eviction strategies, even high-retention ones, can worsen performance if they disrupt positional coherence. Preserving contiguous context blocks through simple strategies can result in more coherent generations than complex or positionally disruptive ones. The study advocates for eviction techniques that respect architectural limits, retain positional structure, and consider overall "cache health" rather than just size. <div>
arXiv:2511.04686v1 Announce Type: cross 
Abstract: The Key-Value (KV) cache is integral to efficient autoregressive inference in large language models (LLMs), yet its unbounded growth in stateful multi-turn scenarios presents major challenges. This paper examines the interplay between KV cache management strategies, the architectural context limits of models like meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of positional encodings. Through empirical analysis using a stateful benchmarking framework, we show that LLM generation quality degrades sharply when the accumulated KV cache approaches or exceeds the model's trained context window (e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via AttentionTop), can worsen performance if they disrupt positional coherence. Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a cache by removing non-contiguous tokens can scramble these signals and lead to degenerative outputs. We further show that simple strategies preserving contiguous context blocks (e.g., keeping an initial "gist") can yield more coherent generations than complex or positionally disruptive ones. We advocate for eviction techniques that respect architectural limits, preserve positional structure, and view "cache health" holistically beyond mere size.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatizaci\'on de Informes Geot\'ecnicos para Macizos Rocosos con IA</title>
<link>https://arxiv.org/abs/2511.04690</link>
<guid>https://arxiv.org/abs/2511.04690</guid>
<content:encoded><![CDATA[
<div> Keywords: Geotechnical reports, artificial intelligence, image processing, multimodal large language model, web-based tool
Summary: 
- Geotechnical reports are traditionally prepared manually, leading to slow and error-prone processes.
- This study proposes using artificial intelligence techniques to automatically generate reports by processing field data and images.
- The methodology involves collecting data, defining report outlines, and validating responses from a multimodal large language model.
- Iterative refinement of prompts proved effective in generating structured and specific report instructions.
- Evaluation metrics suggest that automatic descriptions produced by the system are comparable to those made by experts. 
- The web-based tool with an intuitive interface and export capabilities to standardized formats is a valuable innovation for professionals and students in the field of geology.

<br /><br />Summary: Geotechnical reports are traditionally prepared manually, leading to slow and error-prone processes. This study proposes using artificial intelligence techniques to automatically generate reports by processing field data and images. The methodology involves collecting data, defining report outlines, and validating responses from a multimodal large language model. Iterative refinement of prompts proved effective in generating structured and specific report instructions. Evaluation metrics suggest that automatic descriptions produced by the system are comparable to those made by experts. The web-based tool with an intuitive interface and export capabilities to standardized formats is a valuable innovation for professionals and students in the field of geology. <div>
arXiv:2511.04690v1 Announce Type: cross 
Abstract: Geotechnical reports are crucial for assessing the stability of rock formations and ensuring safety in modern engineering. Traditionally, these reports are prepared manually based on field observations using compasses, magnifying glasses, and notebooks. This method is slow, prone to errors, and subjective in its interpretations. To overcome these limitations, the use of artificial intelligence techniques is proposed for the automatic generation of reports through the processing of images and field data. The methodology was based on the collection of photographs of rock outcrops and manual samples with their respective descriptions, as well as on the reports prepared during the Geotechnical Studies course. These resources were used to define the report outline, prompt engineering, and validate the responses of a multimodal large language model (MLLM). The iterative refinement of prompts until structured and specific instructions were obtained for each section of the report proved to be an effective alternative to the costly process of fine-tuning the MLLM. The system evaluation establishes values of 0.455 and 0.653 for the BLEU and ROUGE-L metrics, respectively, suggesting that automatic descriptions are comparable to those made by experts. This tool, accessible via the web, with an intuitive interface and the ability to export to standardized formats, represents an innovation and an important contribution for professionals and students of field geology.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Penny for Your Thoughts: Decoding Speech from Inexpensive Brain Signals</title>
<link>https://arxiv.org/abs/2511.04691</link>
<guid>https://arxiv.org/abs/2511.04691</guid>
<content:encoded><![CDATA[
<div> EEG; neural networks; speech decoding; brain activity; personalized architectures<br />
Summary:<br />
In this study, researchers investigated the use of neural networks to decode brain activity into speech by utilizing EEG recordings and aligning them with audio representations. By training a model with a contrastive CLIP loss, the researchers aimed to enhance the decoding process. Three architectural modifications were introduced to improve the existing EEG decoder, resulting in enhancements in performance. The incorporation of subject-specific attention layers and personalized spatial attention led to improvements in Word Error Rate (WER), highlighting the potential of personalized architectures in brain-to-speech decoding. Additionally, the implementation of a dual-path RNN with attention further improved performance, showcasing the benefits of personalized approaches in brain-computer interface applications. Overall, these findings demonstrate the feasibility and effectiveness of utilizing neural networks for decoding brain activity into speech. <br /><br />Summary: <div>
arXiv:2511.04691v1 Announce Type: cross 
Abstract: We explore whether neural networks can decode brain activity into speech by mapping EEG recordings to audio representations. Using EEG data recorded as subjects listened to natural speech, we train a model with a contrastive CLIP loss to align EEG-derived embeddings with embeddings from a pre-trained transformer-based speech model. Building on the state-of-the-art EEG decoder from Meta, we introduce three architectural modifications: (i) subject-specific attention layers (+0.15% WER improvement), (ii) personalized spatial attention (+0.45%), and (iii) a dual-path RNN with attention (-1.87%). Two of the three modifications improved performance, highlighting the promise of personalized architectures for brain-to-speech decoding and applications in brain-computer interfaces.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulating Misinformation Vulnerabilities With Agent Personas</title>
<link>https://arxiv.org/abs/2511.04697</link>
<guid>https://arxiv.org/abs/2511.04697</guid>
<content:encoded><![CDATA[
<div> Keywords: disinformation campaigns, agent-based simulation, Large Language Models, misinformation, mental schemas

Summary:
Disinformation campaigns can have detrimental effects on public perception and societal stability. Real-world experimentation to understand how different populations respond to information is challenging. In this study, an agent-based simulation using Large Language Models (LLMs) was developed to model responses to misinformation. Different agent personas based on professions and mental schemas were created to evaluate reactions to news headlines. The results showed that LLM-generated agents closely aligned with ground-truth labels and human predictions, indicating their potential as proxies for studying information responses. Mental schemas were found to have a greater influence than professional backgrounds on how agents interpret misinformation. This research validates the use of LLMs as agents in analyzing trust, polarization, and susceptibility to deceptive content in complex social systems. <br /><br />Summary: Disinformation campaigns can distort public perception and destabilize institutions. An agent-based simulation using Large Language Models (LLMs) was utilized to model responses to misinformation. The study found that mental schemas play a significant role in how agents interpret misinformation, surpassing the influence of professional backgrounds. The research demonstrates the potential of LLMs in analyzing trust, polarization, and susceptibility to deceptive content in complex social systems. <div>
arXiv:2511.04697v1 Announce Type: cross 
Abstract: Disinformation campaigns can distort public perception and destabilize institutions. Understanding how different populations respond to information is crucial for designing effective interventions, yet real-world experimentation is impractical and ethically challenging. To address this, we develop an agent-based simulation using Large Language Models (LLMs) to model responses to misinformation. We construct agent personas spanning five professions and three mental schemas, and evaluate their reactions to news headlines. Our findings show that LLM-generated agents align closely with ground-truth labels and human predictions, supporting their use as proxies for studying information responses. We also find that mental schemas, more than professional background, influence how agents interpret misinformation. This work provides a validation of LLMs to be used as agents in an agent-based model of an information network for analyzing trust, polarization, and susceptibility to deceptive content in complex social systems.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jailbreaking in the Haystack</title>
<link>https://arxiv.org/abs/2511.04707</link>
<guid>https://arxiv.org/abs/2511.04707</guid>
<content:encoded><![CDATA[
<div> Keywords: long-context language models, safety implications, jailbreak attack, goal positioning, vulnerability

Summary: 
NINJA is a new method for attacking long-context language models by appending harmless content to harmful user goals, demonstrating the importance of goal positioning in safety. The experiments on the HarmBench benchmark show that NINJA significantly increases attack success rates on various state-of-the-art models, including LLaMA, Qwen, Mistral, and Gemini. This method is low-resource, transferrable, and less detectable compared to previous jailbreaking techniques. NINJA is also shown to be compute-optimal, as increasing context length under a fixed compute budget can outperform increasing the number of trials in traditional jailbreak methods. These results highlight that benign long contexts, when strategically crafted, can introduce significant vulnerabilities in modern long-context language models. 

<br /><br />Summary: <div>
arXiv:2511.04707v1 Announce Type: cross 
Abstract: Recent advances in long-context language models (LMs) have enabled million-token inputs, expanding their capabilities across complex tasks like computer-use agents. Yet, the safety implications of these extended contexts remain unclear. To bridge this gap, we introduce NINJA (short for Needle-in-haystack jailbreak attack), a method that jailbreaks aligned LMs by appending benign, model-generated content to harmful user goals. Critical to our method is the observation that the position of harmful goals play an important role in safety. Experiments on standard safety benchmark, HarmBench, show that NINJA significantly increases attack success rates across state-of-the-art open and proprietary models, including LLaMA, Qwen, Mistral, and Gemini. Unlike prior jailbreaking methods, our approach is low-resource, transferable, and less detectable. Moreover, we show that NINJA is compute-optimal -- under a fixed compute budget, increasing context length can outperform increasing the number of trials in best-of-N jailbreak. These findings reveal that even benign long contexts -- when crafted with careful goal positioning -- introduce fundamental vulnerabilities in modern LMs.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying the Climate Risk of Generative AI: Region-Aware Carbon Accounting with G-TRACE and the AI Sustainability Pyramid</title>
<link>https://arxiv.org/abs/2511.04776</link>
<guid>https://arxiv.org/abs/2511.04776</guid>
<content:encoded><![CDATA[
<div> framework, AI sustainability, carbon emissions, energy consumption, climate risk
Summary: 
The study introduces G-TRACE, a framework that quantifies carbon emissions and energy consumption of Generative Artificial Intelligence (GenAI). It reveals how decentralized inference can magnify small energy costs into significant system-level impacts. Through analyzing the Ghibli-style image generation trend, the study estimates significant energy consumption and emissions, highlighting the consequences of widespread digital participation. The AI Sustainability Pyramid, a governance model, connects carbon accounting metrics with operational readiness, optimization, and stewardship, providing guidance for sustainable AI deployment. The study enhances understanding of emerging digital infrastructures as a climate risk category and supports adaptive governance for sustainable technology deployment. The work emphasizes the importance of aligning technological innovation with global decarbonization and resilience objectives. 
<br /><br />Summary: <div>
arXiv:2511.04776v1 Announce Type: cross 
Abstract: Generative Artificial Intelligence (GenAI) represents a rapidly expanding digital infrastructure whose energy demand and associated CO2 emissions are emerging as a new category of climate risk. This study introduces G-TRACE (GenAI Transformative Carbon Estimator), a cross-modal, region-aware framework that quantifies training- and inference-related emissions across modalities and deployment geographies. Using real-world analytics and microscopic simulation, G-TRACE measures energy use and carbon intensity per output type (text, image, video) and reveals how decentralized inference amplifies small per-query energy costs into system-level impacts. Through the Ghibli-style image generation trend (2024-2025), we estimate 4,309 MWh of energy consumption and 2,068 tCO2 emissions, illustrating how viral participation inflates individual digital actions into tonne-scale consequences. Building on these findings, we propose the AI Sustainability Pyramid, a seven-level governance model linking carbon accounting metrics (L1-L7) with operational readiness, optimization, and stewardship. This framework translates quantitative emission metrics into actionable policy guidance for sustainable AI deployment. The study contributes to the quantitative assessment of emerging digital infrastructures as a novel category of climate risk, supporting adaptive governance for sustainable technology deployment. By situating GenAI within climate-risk frameworks, the work advances data-driven methods for aligning technological innovation with global decarbonization and resilience objectives.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Association via Entropy Reduction</title>
<link>https://arxiv.org/abs/2511.04901</link>
<guid>https://arxiv.org/abs/2511.04901</guid>
<content:encoded><![CDATA[
arXiv:2511.04901v1 Announce Type: cross 
Abstract: Prior to recent successes using neural networks, term frequency-inverse document frequency (tf-idf) was clearly regarded as the best choice for identifying documents related to a query. We provide a different score, aver, and observe, on a dataset with ground truth marking for association, that aver does do better at finding assciated pairs than tf-idf. This example involves finding associated vertices in a large graph and that may be an area where neural networks are not currently an obvious best choice. Beyond this one anecdote, we observe that (1) aver has a natural threshold for declaring pairs as unassociated while tf-idf does not, (2) aver can distinguish between pairs of documents for which tf-idf gives a score of 1.0, (3) aver can be applied to larger collections of documents than pairs while tf-idf cannot, and (4) that aver is derived from entropy under a simple statistical model while tf-idf is a construction designed to achieve a certain goal and hence aver may be more "natural." To be fair, we also observe that (1) writing down and computing the aver score for a pair is more complex than for tf-idf and (2) that the fact that the aver score is naturally scale-free makes it more complicated to interpret aver scores.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORCHID: Orchestrated Retrieval-Augmented Classification with Human-in-the-Loop Intelligent Decision-Making for High-Risk Property</title>
<link>https://arxiv.org/abs/2511.04956</link>
<guid>https://arxiv.org/abs/2511.04956</guid>
<content:encoded><![CDATA[
arXiv:2511.04956v1 Announce Type: cross 
Abstract: High-Risk Property (HRP) classification is critical at U.S. Department of Energy (DOE) sites, where inventories include sensitive and often dual-use equipment. Compliance must track evolving rules designated by various export control policies to make transparent and auditable decisions. Traditional expert-only workflows are time-consuming, backlog-prone, and struggle to keep pace with shifting regulatory boundaries. We demo ORCHID, a modular agentic system for HRP classification that pairs retrieval-augmented generation (RAG) with human oversight to produce policy-based outputs that can be audited. Small cooperating agents, retrieval, description refiner, classifier, validator, and feedback logger, coordinate via agent-to-agent messaging and invoke tools through the Model Context Protocol (MCP) for model-agnostic on-premise operation. The interface follows an Item to Evidence to Decision loop with step-by-step reasoning, on-policy citations, and append-only audit bundles (run-cards, prompts, evidence). In preliminary tests on real HRP cases, ORCHID improves accuracy and traceability over a non-agentic baseline while deferring uncertain items to Subject Matter Experts (SMEs). The demonstration shows single item submission, grounded citations, SME feedback capture, and exportable audit artifacts, illustrating a practical path to trustworthy LLM assistance in sensitive DOE compliance workflows.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Public Speaking Skills in Engineering Students Through AI</title>
<link>https://arxiv.org/abs/2511.04995</link>
<guid>https://arxiv.org/abs/2511.04995</guid>
<content:encoded><![CDATA[
arXiv:2511.04995v1 Announce Type: cross 
Abstract: This research-to-practice full paper was inspired by the persistent challenge in effective communication among engineering students. Public speaking is a necessary skill for future engineers as they have to communicate technical knowledge with diverse stakeholders. While universities offer courses or workshops, they are unable to offer sustained and personalized training to students. Providing comprehensive feedback on both verbal and non-verbal aspects of public speaking is time-intensive, making consistent and individualized assessment impractical. This study integrates research on verbal and non-verbal cues in public speaking to develop an AI-driven assessment model for engineering students. Our approach combines speech analysis, computer vision, and sentiment detection into a multi-modal AI system that provides assessment and feedback. The model evaluates (1) verbal communication (pitch, loudness, pacing, intonation), (2) non-verbal communication (facial expressions, gestures, posture), and (3) expressive coherence, a novel integration ensuring alignment between speech and body language. Unlike previous systems that assess these aspects separately, our model fuses multiple modalities to deliver personalized, scalable feedback. Preliminary testing demonstrated that our AI-generated feedback was moderately aligned with expert evaluations. Among the state-of-the-art AI models evaluated, all of which were Large Language Models (LLMs), including Gemini and OpenAI models, Gemini Pro emerged as the best-performing, showing the strongest agreement with human annotators. By eliminating reliance on human evaluators, this AI-driven public speaking trainer enables repeated practice, helping students naturally align their speech with body language and emotion, crucial for impactful and professional communication.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings</title>
<link>https://arxiv.org/abs/2511.05017</link>
<guid>https://arxiv.org/abs/2511.05017</guid>
<content:encoded><![CDATA[
arXiv:2511.05017v1 Announce Type: cross 
Abstract: In this work, we identify an inherent bias in prevailing LVLM architectures toward the language modality, largely resulting from the common practice of simply appending visual embeddings to the input text sequence. To address this, we propose a simple yet effective method that refines textual embeddings by integrating average-pooled visual features. Our approach demonstrably improves visual grounding and significantly reduces hallucinations on established benchmarks. While average pooling offers a straightforward, robust, and efficient means of incorporating visual information, we believe that more sophisticated fusion methods could further enhance visual grounding and cross-modal alignment. Given that the primary focus of this work is to highlight the modality imbalance and its impact on hallucinations -- and to show that refining textual embeddings with visual information mitigates this issue -- we leave exploration of advanced fusion strategies for future work.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wikipedia-based Datasets in Russian Information Retrieval Benchmark RusBEIR</title>
<link>https://arxiv.org/abs/2511.05079</link>
<guid>https://arxiv.org/abs/2511.05079</guid>
<content:encoded><![CDATA[
arXiv:2511.05079v1 Announce Type: cross 
Abstract: In this paper, we present a novel series of Russian information retrieval datasets constructed from the "Did you know..." section of Russian Wikipedia. Our datasets support a range of retrieval tasks, including fact-checking, retrieval-augmented generation, and full-document retrieval, by leveraging interesting facts and their referenced Wikipedia articles annotated at the sentence level with graded relevance. We describe the methodology for dataset creation that enables the expansion of existing Russian Information Retrieval (IR) resources. Through extensive experiments, we extend the RusBEIR research by comparing lexical retrieval models, such as BM25, with state-of-the-art neural architectures fine-tuned for Russian, as well as multilingual models. Results of our experiments show that lexical methods tend to outperform neural models on full-document retrieval, while neural approaches better capture lexical semantics in shorter texts, such as in fact-checking or fine-grained retrieval. Using our newly created datasets, we also analyze the impact of document length on retrieval performance and demonstrate that combining retrieval with neural reranking consistently improves results. Our contribution expands the resources available for Russian information retrieval research and highlights the importance of accurate evaluation of retrieval models to achieve optimal performance. All datasets are publicly available at HuggingFace. To facilitate reproducibility and future research, we also release the full implementation on GitHub.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Generation and Identification From Partial Enumeration: Tight Density Bounds and Topological Characterizations</title>
<link>https://arxiv.org/abs/2511.05295</link>
<guid>https://arxiv.org/abs/2511.05295</guid>
<content:encoded><![CDATA[
arXiv:2511.05295v1 Announce Type: cross 
Abstract: The success of large language models (LLMs) has motivated formal theories of language generation and learning. We study the framework of \emph{language generation in the limit}, where an adversary enumerates strings from an unknown language $K$ drawn from a countable class, and an algorithm must generate unseen strings from $K$. Prior work showed that generation is always possible, and that some algorithms achieve positive lower density, revealing a \emph{validity--breadth} trade-off between correctness and coverage. We resolve a main open question in this line, proving a tight bound of $1/2$ on the best achievable lower density. We then strengthen the model to allow \emph{partial enumeration}, where the adversary reveals only an infinite subset $C \subseteq K$. We show that generation in the limit remains achievable, and if $C$ has lower density $\alpha$ in $K$, the algorithm's output achieves density at least $\alpha/2$, matching the upper bound. This generalizes the $1/2$ bound to the partial-information setting, where the generator must recover within a factor $1/2$ of the revealed subset's density. We further revisit the classical Gold--Angluin model of \emph{language identification} under partial enumeration. We characterize when identification in the limit is possible -- when hypotheses $M_t$ eventually satisfy $C \subseteq M \subseteq K$ -- and in the process give a new topological formulation of Angluin's characterization, showing that her condition is precisely equivalent to an appropriate topological space having the $T_D$ separation property.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QUESTER: Query Specification for Generative Retrieval</title>
<link>https://arxiv.org/abs/2511.05301</link>
<guid>https://arxiv.org/abs/2511.05301</guid>
<content:encoded><![CDATA[
arXiv:2511.05301v1 Announce Type: cross 
Abstract: Generative Retrieval (GR) differs from the traditional index-then-retrieve pipeline by storing relevance in model parameters and directly generating document identifiers. However, GR often struggles to generalize and is costly to scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval), which reframes GR as query specification generation - in this work, a simple keyword query handled by BM25 - using a (small) LLM. The policy is trained using reinforcement learning techniques (GRPO). Across in- and out-of-domain evaluations, we show that our model is more effective than BM25, and competitive with neural IR models, while maintaining a good efficiency
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConVerse: Benchmarking Contextual Safety in Agent-to-Agent Conversations</title>
<link>https://arxiv.org/abs/2511.05359</link>
<guid>https://arxiv.org/abs/2511.05359</guid>
<content:encoded><![CDATA[
arXiv:2511.05359v1 Announce Type: cross 
Abstract: As language models evolve into autonomous agents that act and communicate on behalf of users, ensuring safety in multi-agent ecosystems becomes a central challenge. Interactions between personal assistants and external service providers expose a core tension between utility and protection: effective collaboration requires information sharing, yet every exchange creates new attack surfaces. We introduce ConVerse, a dynamic benchmark for evaluating privacy and security risks in agent-agent interactions. ConVerse spans three practical domains (travel, real estate, insurance) with 12 user personas and over 864 contextually grounded attacks (611 privacy, 253 security). Unlike prior single-agent settings, it models autonomous, multi-turn agent-to-agent conversations where malicious requests are embedded within plausible discourse. Privacy is tested through a three-tier taxonomy assessing abstraction quality, while security attacks target tool use and preference manipulation. Evaluating seven state-of-the-art models reveals persistent vulnerabilities; privacy attacks succeed in up to 88% of cases and security breaches in up to 60%, with stronger models leaking more. By unifying privacy and security within interactive multi-agent contexts, ConVerse reframes safety as an emergent property of communication.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>APP: Accelerated Path Patching with Task-Specific Pruning</title>
<link>https://arxiv.org/abs/2511.05442</link>
<guid>https://arxiv.org/abs/2511.05442</guid>
<content:encoded><![CDATA[
arXiv:2511.05442v1 Announce Type: cross 
Abstract: Circuit discovery is a key step in many mechanistic interpretability pipelines. Current methods, such as Path Patching, are computationally expensive and have limited in-depth circuit analysis for smaller models. In this study, we propose Accelerated Path Patching (APP), a hybrid approach leveraging our novel contrastive attention head pruning method to drastically reduce the search space of circuit discovery methods. Our Contrastive-FLAP pruning algorithm uses techniques from causal mediation analysis to assign higher pruning scores to task-specific attention heads, leading to higher performing sparse models compared to traditional pruning techniques. Although Contrastive-FLAP is successful at preserving task-specific heads that existing pruning algorithms remove at low sparsity ratios, the circuits found by Contrastive-FLAP alone are too large to satisfy the minimality constraint required in circuit analysis. APP first applies Contrastive-FLAP to reduce the search space on required for circuit discovery algorithms by, on average, 56\%. Next, APP, applies traditional Path Patching on the remaining attention heads, leading to a speed up of 59.63\%-93.27\% compared to Path Patching applied to the dense model. Despite the substantial computational saving that APP provides, circuits obtained from APP exhibit substantial overlap and similar performance to previously established Path Patching circuits
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>To Word Senses and Beyond: Inducing Concepts with Contextualized Language Models</title>
<link>https://arxiv.org/abs/2406.20054</link>
<guid>https://arxiv.org/abs/2406.20054</guid>
<content:encoded><![CDATA[
arXiv:2406.20054v3 Announce Type: replace 
Abstract: Polysemy and synonymy are two crucial interrelated facets of lexical ambiguity. While both phenomena are widely documented in lexical resources and have been studied extensively in NLP, leading to dedicated systems, they are often being considered independently in practical problems. While many tasks dealing with polysemy (e.g. Word Sense Disambiguation or Induction) highlight the role of word's senses, the study of synonymy is rooted in the study of concepts, i.e. meanings shared across the lexicon. In this paper, we introduce Concept Induction, the unsupervised task of learning a soft clustering among words that defines a set of concepts directly from data. This task generalizes Word Sense Induction. We propose a bi-level approach to Concept Induction that leverages both a local lemma-centric view and a global cross-lexicon view to induce concepts. We evaluate the obtained clustering on SemCor's annotated data and obtain good performance (BCubed F1 above 0.60). We find that the local and the global levels are mutually beneficial to induce concepts and also senses in our setting. Finally, we create static embeddings representing our induced concepts and use them on the Word-in-Context task, obtaining competitive performance with the State-of-the-Art.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LEME: Open Large Language Models for Ophthalmology with Advanced Reasoning and Clinical Validation</title>
<link>https://arxiv.org/abs/2410.03740</link>
<guid>https://arxiv.org/abs/2410.03740</guid>
<content:encoded><![CDATA[
arXiv:2410.03740v3 Announce Type: replace 
Abstract: The rising prevalence of eye diseases poses a growing public health burden. Large language models (LLMs) offer a promising path to reduce documentation workload and support clinical decision-making. However, few have been tailored for ophthalmology, and most evaluations focus mainly on knowledge-based QA without clinically relevant benchmarks or real-world validation. Here, we present LEME, a suite of open-weight LLMs developed through a two-stage process: (1) instruction tuning on 200,000 samples from clinical guidelines, textbooks, and case reports to enhance reasoning and task-following, and (2) reinforcement learning with ~30,000 preference labels to enhance accuracy and informativeness. LEME was evaluated on five curated zero-shot benchmarks spanning tasks such as patient QA, consultation, and treatment planning. It outperformed all seven baselines (all p < 0.004), exceeding GPT-4o by 3.32% (absolute ROUGE-L gain). It was further evaluated on three downstream tasks using deidentified patient data, reviewed by clinicians. In patient QA, LEME received the highest ratings from attending clinicians in 3 out of 4 criteria, with scores of 4.67 for factuality, 4.77 for specificity, 4.79 for completeness, and 4.88 for safety (1-5 scale). Its completeness score surpassed that of expert-written answers (4.79 vs. 4.56; p = 0.015). In visual acuity extraction, LEME achieved the highest F1, outperforming LLaMA-3 by 14.1% and Eye-LLaMA by 59.0%. In a pilot evaluation on assessment and treatment planning for diabetic retinopathy, AMD, and glaucoma, LEME received scores of 4.36 for factuality, 4.55 for specificity, 4.42 for completeness, and 4.36 for safety, approaching attending-level performance. All models, data, and code will be released to support further development and clinical translation, laying the groundwork for improved efficiency and patient care
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extracting narrative signals from public discourse: a network-based approach</title>
<link>https://arxiv.org/abs/2411.00702</link>
<guid>https://arxiv.org/abs/2411.00702</guid>
<content:encoded><![CDATA[
arXiv:2411.00702v2 Announce Type: replace 
Abstract: Narratives are key interpretative devices by which humans make sense of political reality. As the significance of narratives for understanding current societal issues such as polarization and misinformation becomes increasingly evident, there is a growing demand for methods that support their empirical analysis. To this end, we propose a graph-based formalism and machine-guided method for extracting, representing, and analyzing selected narrative signals from digital textual corpora, based on Abstract Meaning Representation (AMR). The formalism and method introduced here specifically cater to the study of political narratives that figure in texts from digital media such as archived political speeches, social media posts, transcripts of parliamentary debates, and political manifestos on party websites. We approach the study of such political narratives as a problem of information retrieval: starting from a textual corpus, we first extract a graph-like representation of the meaning of each sentence in the corpus using AMR. Drawing on transferable concepts from narratology, we then apply a set of heuristics to filter these graphs for representations of 1) actors and their relationships, 2) the events in which these actors figure, and 3) traces of the perspectivization of these events. We approach these references to actors, events, and instances of perspectivization as core narrative signals that allude to larger political narratives. By systematically analyzing and re-assembling these signals into networks that guide the researcher to the relevant parts of the text, the underlying narratives can be reconstructed through a combination of distant and close reading. A case study of State of the European Union addresses (2010 -- 2023) demonstrates how the formalism can be used to inductively surface signals of political narratives from public discourse.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for Advanced Tool Use</title>
<link>https://arxiv.org/abs/2501.09766</link>
<guid>https://arxiv.org/abs/2501.09766</guid>
<content:encoded><![CDATA[
arXiv:2501.09766v5 Announce Type: replace 
Abstract: Augmenting large language models (LLMs) with external tools is a promising approach to enhance their capabilities, especially for complex tasks. Synthesizing tool-use data through real-world simulations is an effective way to achieve this. However, our investigation reveals that training gains significantly decay as synthetic data increases. The model struggles to benefit from additional synthetic data, which fails to endow it with advanced tool-use capabilities in complex scenarios Moreover, we discovered that the above limitation usually manifests as a fragment deficiency (i.e., parameter errors) in response. To this end, we propose an iterative reinforced fine-tuning strategy designed to alleviate this limitation. This strategy involves: (1) enhancing the diversity of response for synthetic data through path exploration of Monte Carlo Tree Search. (2) iteratively pinpointing the model's deficiency by constructing fine-grained preference pairs, and then improving it by preference optimization algorithms for targeted improvement. The experiments show that our method achieves 13.11% better performance than the same-size base model. It achieves an improvement of 6.5% in complex scenarios compared to the baseline, and it also outperforms larger open-source and closed-source models.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activation-Informed Merging of Large Language Models</title>
<link>https://arxiv.org/abs/2502.02421</link>
<guid>https://arxiv.org/abs/2502.02421</guid>
<content:encoded><![CDATA[
arXiv:2502.02421v3 Announce Type: replace 
Abstract: Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning (CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs, with up to a 40% increase in benchmark performance.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions</title>
<link>https://arxiv.org/abs/2502.13124</link>
<guid>https://arxiv.org/abs/2502.13124</guid>
<content:encoded><![CDATA[
arXiv:2502.13124v4 Announce Type: replace 
Abstract: Scaling reasoning capabilities beyond traditional domains such as math and coding is hindered by the lack of diverse and high-quality questions. To overcome this limitation, we introduce a scalable approach for generating diverse and challenging reasoning questions, accompanied by reference answers. We present NaturalReasoning, a comprehensive dataset comprising 2.8 million questions that span multiple domains, including STEM fields (e.g., Physics, Computer Science), Economics, Social Sciences, and more. We demonstrate the utility of the questions in NaturalReasoning through knowledge distillation experiments which show that NaturalReasoning can effectively elicit and transfer reasoning capabilities from a strong teacher model. Furthermore, we demonstrate that NaturalReasoning is also effective for unsupervised self-training using external reward models or self-rewarding. To foster future work, we publicly release NaturalReasoning at https://huggingface.co/datasets/facebook/natural_reasoning.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback</title>
<link>https://arxiv.org/abs/2502.15027</link>
<guid>https://arxiv.org/abs/2502.15027</guid>
<content:encoded><![CDATA[
arXiv:2502.15027v3 Announce Type: replace 
Abstract: Existing benchmarks do not test Large Multimodal Models (LMMs) on their interactive intelligence with human users, which is vital for developing general-purpose AI assistants. We design InterFeedback, an interactive framework, which can be applied to any LMM and dataset to assess this ability autonomously. On top of this, we introduce InterFeedback-Bench which evaluates interactive intelligence using two representative datasets, MMMU-Pro and MathVerse, to test 10 different open-source LMMs. Additionally, we present InterFeedback-Human, a newly collected dataset of 120 cases designed for manually testing interactive performance in leading models such as OpenAI-o1 and Claude-Sonnet-4. Our evaluation results indicate that even the state-of-the-art LMM, OpenAI-o1, struggles to refine its responses based on human feedback, achieving an average score of less than 50%. Our findings point to the need for methods that can enhance LMMs' capabilities to interpret and benefit from feedback.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Blind Spots: A Focus-Level Evaluation Framework for LLM Reviews</title>
<link>https://arxiv.org/abs/2502.17086</link>
<guid>https://arxiv.org/abs/2502.17086</guid>
<content:encoded><![CDATA[
arXiv:2502.17086v4 Announce Type: replace 
Abstract: Peer review underpins scientific progress, but it is increasingly strained by reviewer shortages and growing workloads. Large Language Models (LLMs) can automatically draft reviews now, but determining whether LLM-generated reviews are trustworthy requires systematic evaluation. Researchers have evaluated LLM reviews at either surface-level (e.g., BLEU and ROUGE) or content-level (e.g., specificity and factual accuracy). Yet it remains uncertain whether LLM-generated reviews attend to the same critical facets that human experts weigh -- the strengths and weaknesses that ultimately drive an accept-or-reject decision. We introduce a focus-level evaluation framework that operationalizes the focus as a normalized distribution of attention across predefined facets in paper reviews. Based on the framework, we developed an automatic focus-level evaluation pipeline based on two sets of facets: target (e.g., problem, method, and experiment) and aspect (e.g., validity, clarity, and novelty), leveraging 676 paper reviews (https://figshare.com/s/d5adf26c802527dd0f62) from OpenReview that consists of 3,657 strengths and weaknesses identified from human experts. The comparison of focus distributions between LLMs and human experts showed that the off-the-shelf LLMs consistently have a more biased focus towards examining technical validity while significantly overlooking novelty assessment when criticizing papers.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Multimodal Perception in Large Language Models Through Perceptual Strength Ratings</title>
<link>https://arxiv.org/abs/2503.06980</link>
<guid>https://arxiv.org/abs/2503.06980</guid>
<content:encoded><![CDATA[
arXiv:2503.06980v2 Announce Type: replace 
Abstract: This study investigated whether multimodal large language models can achieve human-like sensory grounding by examining their ability to capture perceptual strength ratings across sensory modalities. We explored how model characteristics (size, multimodal capabilities, architectural generation) influence grounding performance, distributional factor dependencies (word frequency, embeddings, feature distances), and human-model processing differences. We evaluated 21 models from four families (GPT, Gemini, LLaMA, Qwen) using 3,611 words from the Lancaster Sensorimotor Norms through correlation, distance metrics, and qualitative analysis. Results showed that larger (6 out of 8 comparisons), multimodal (5 of 7), and newer models (5 of 8) generally outperformed their smaller, text-based, and older counterparts. Top models achieved 85-90% accuracy and 0.58-0.65 correlations with human ratings, demonstrating substantial similarity. Moreover, distributional factors showed minimal impact, not exceeding human dependency levels. However, despite strong alignment, models were not identical to humans, as even top performers showed differences in distance and correlation measures, with qualitative analysis revealing processing patterns related to absent sensory grounding. Additionally, it remains questionable whether introducing multimodality resolves this grounding deficit. Although multimodality improved performance, it seems to provide similar information as massive text rather than qualitatively different data, as benefits occurred across unrelated sensory dimensions and massive text-only models achieved comparable results. Our findings demonstrate that while advanced LLMs can approximate human sensory-linguistic associations through statistical learning, they still differ from human embodied cognition in processing mechanisms, even with multimodal integration.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MorphTok: Morphologically Grounded Tokenization for Indian Languages</title>
<link>https://arxiv.org/abs/2504.10335</link>
<guid>https://arxiv.org/abs/2504.10335</guid>
<content:encoded><![CDATA[
arXiv:2504.10335v2 Announce Type: replace 
Abstract: Tokenization is a crucial step in NLP, especially with the rise of large language models (LLMs), impacting downstream performance, computational cost, and efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE) algorithm for subword tokenization that greedily merges frequent character bigrams, often leading to segmentation that does not align with linguistically meaningful units. To address this, we propose morphology-aware segmentation as a pre-tokenization step before applying BPE. To facilitate morphology-aware segmentation, we create a novel dataset for Hindi and Marathi, incorporating sandhi splitting to enhance the subword tokenization. Experiments on downstream tasks show that morphologically grounded tokenization improves machine translation and language modeling performance. Additionally, to handle the dependent vowels common in syllable-based writing systems used by Indic languages, we propose Constrained BPE (CBPE), an extension to the standard BPE algorithm incorporating script-specific constraints. In particular, CBPE handles dependent vowels to form a cohesive unit with other characters instead of occurring as a single unit. Our results show that CBPE achieves a 1.68\% reduction in fertility scores while maintaining comparable or improved downstream performance in machine translation and language modeling, offering a computationally efficient alternative to standard BPE. Moreover, to evaluate segmentation across different tokenization algorithms, we introduce a new human evaluation metric, \textit{EvalTok}, enabling more human-grounded assessment.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fair Document Valuation in LLM Summaries via Shapley Values</title>
<link>https://arxiv.org/abs/2505.23842</link>
<guid>https://arxiv.org/abs/2505.23842</guid>
<content:encoded><![CDATA[
arXiv:2505.23842v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly used in systems that retrieve and summarize content from multiple sources, such as search engines and AI assistants. While these systems enhance user experience through coherent summaries, they obscure the individual contributions of original content creators, raising concerns about credit attribution and compensation. We address the challenge of valuing individual documents used in LLM-generated summaries by proposing a Shapley value-based framework for fair document valuation. Although theoretically appealing, exact Shapley value computation is prohibitively expensive at scale. To improve efficiency, we develop Cluster Shapley, a simple approximation algorithm that leverages semantic similarity among documents to reduce computation while maintaining attribution accuracy. Using Amazon product review data, we empirically show that off-the-shelf Shapley approximations, such as Monte Carlo sampling and Kernel SHAP, perform suboptimally in LLM settings, whereas Cluster Shapley substantially improves the efficiency-accuracy frontier. Moreover, simple attribution rules (e.g., equal or relevance-based allocation), though computationally cheap, lead to highly unfair outcomes. Together, our findings highlight the potential of structure-aware Shapley approximations tailored to LLM summarization and offer guidance for platforms seeking scalable and fair content attribution mechanisms.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProRefine: Inference-Time Prompt Refinement with Textual Feedback</title>
<link>https://arxiv.org/abs/2506.05305</link>
<guid>https://arxiv.org/abs/2506.05305</guid>
<content:encoded><![CDATA[
arXiv:2506.05305v3 Announce Type: replace 
Abstract: Agentic workflows, where multiple AI agents collaborate to accomplish complex tasks like reasoning or planning, play a substantial role in many cutting-edge commercial applications, and continue to fascinate researchers across fields for their potential to accomplish expensive, complex tasks that, until recently, only humans have been trusted to do. These workflows depend critically on the prompts used to provide the roles models play in such workflows. Poorly designed prompts that fail even slightly to guide individual agents can lead to sub-optimal performance that may snowball within a system of agents, limiting their reliability and scalability. To address this important problem of inference-time prompt optimization, we introduce ProRefine, an innovative inference-time optimization method that uses an agentic loop of LLMs to generate and apply textual feedback. ProRefine dynamically refines prompts for multi-step reasoning tasks without additional training or ground truth labels. Evaluated on five benchmark mathematical reasoning datasets, ProRefine significantly surpasses zero-shot Chain-of-Thought baselines by 3 to 37 percentage points. This approach not only boosts accuracy but also allows smaller models to approach the performance of their larger counterparts. This highlights its potential for building more cost-effective and powerful hybrid AI systems, thereby democratizing access to high-performing AI.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance</title>
<link>https://arxiv.org/abs/2507.09601</link>
<guid>https://arxiv.org/abs/2507.09601</guid>
<content:encoded><![CDATA[
arXiv:2507.09601v2 Announce Type: replace 
Abstract: General-purpose sentence embedding models often struggle to capture specialized financial semantics, especially in low-resource languages like Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual embedding models fine-tuned with 18.8K high-confidence triplets that pair in-domain paraphrases, hard negatives derived from a semantic-shift typology, and exact Korean-English translations. Concurrently, we release KorFinSTS, a 1,921-pair Korean financial STS benchmark spanning news, disclosures, research reports, and regulations, designed to expose nuances that general benchmarks miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and +0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing other models by the largest margin, while revealing a modest trade-off in general STS performance. Our analysis further shows that models with richer Korean token coverage adapt more effectively, underscoring the importance of tokenizer design in low-resource, cross-lingual settings. By making both models and the benchmark publicly available, we provide the community with robust tools for domain-adapted, multilingual representation learning in finance.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication</title>
<link>https://arxiv.org/abs/2507.23247</link>
<guid>https://arxiv.org/abs/2507.23247</guid>
<content:encoded><![CDATA[
arXiv:2507.23247v2 Announce Type: replace 
Abstract: Although explainability and interpretability have received significant attention in artificial intelligence (AI) and natural language processing (NLP) for mental health, reasoning has not been examined in the same depth. Addressing this gap is essential to bridge NLP and mental health through interpretable and reasoning-capable AI systems. To this end, we investigate the pragmatic reasoning capability of large-language models (LLMs) in the mental health domain. We introduce PRiMH dataset, and propose pragmatic reasoning tasks in mental health with pragmatic implicature and presupposition phenomena. In particular, we formulate two tasks in implicature and one task in presupposition. To benchmark the dataset and the tasks presented, we consider four models: Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of the experiments suggest that Mistral and Qwen show substantial reasoning abilities in the domain. Subsequently, we study the behavior of MentaLLaMA on the proposed reasoning tasks with the rollout attention mechanism. In addition, we also propose three StiPRompts to study the stigma around mental health with the state-of-the-art LLMs, GPT4o-mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings show that Claude-3.5-haiku deals with stigma more responsibly compared to the other two LLMs.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Dynamics of Meta-Learning in Small Model Pretraining</title>
<link>https://arxiv.org/abs/2508.02189</link>
<guid>https://arxiv.org/abs/2508.02189</guid>
<content:encoded><![CDATA[
arXiv:2508.02189v2 Announce Type: replace 
Abstract: Large language models are powerful but costly. We ask whether meta-learning can make the pretraining of small language models not only better but also more interpretable. We integrate first-order MAML with subset-masked LM pretraining, producing four LLama-style decoder-only models (11M-570M params), and evaluate it on a fundamental NLP task with many settings and real-world applications. Compared with vanilla training, our model (i) reaches the same loss up to 1.6x sooner, (ii) improves F1 on multilingual Universal NER under equal compute, and (iii) makes the training dynamics easy to read: first the network's representations fan out ("diversify") and later they collapse into a smaller, shared subspace ("compress"). This two-stage shift shows up as a rise-and-fall in both effective-rank curves and attention-head entropy. The same curves pinpoint which layers specialise earliest and which later reconverge, giving a compact, interpretable signature of meta-adaptation. Code, checkpoints and WandB logs are released.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Query: Unleashing the Power of LLMs in SQL-Aided Table Understanding via Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2508.15809</link>
<guid>https://arxiv.org/abs/2508.15809</guid>
<content:encoded><![CDATA[
arXiv:2508.15809v2 Announce Type: replace 
Abstract: Table understanding requires structured, multi-step reasoning. Large Language Models (LLMs) struggle with it due to the structural complexity of tabular data. Recently, multi-agent frameworks for SQL generation have shown promise in tackling the challenges of understanding tabular data, but existing approaches often suffer from limitations such as the inability to comprehend table structure for reliable SQL generation, error propagation that results in invalid queries, and over-reliance on execution correctness. To address these issues, we propose Chain-of-Query (CoQ), a novel multi-agent framework for SQL-aided table understanding. CoQ adopts natural-language-style representations of table schemas to abstract away structural noise and enhance understanding. It employs a clause-by-clause SQL generation strategy to improve query quality and introduces a hybrid reasoning division that separates SQL-based mechanical reasoning from LLM-based logical inference, thereby reducing reliance on execution outcomes. Extensive experiments across four models and five widely used benchmarks demonstrate that CoQ achieves substantial accuracy improvements and significantly lowers invalid SQL rates compared to prior generic LLM-based, SQL-aided, and hybrid baselines, confirming its superior effectiveness in table understanding. The code is available at https://github.com/SongyuanSui/ChainofQuery.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRQA: Dynamic Reasoning Quota Allocation for Controlling Overthinking in Reasoning Large Language Models</title>
<link>https://arxiv.org/abs/2508.17803</link>
<guid>https://arxiv.org/abs/2508.17803</guid>
<content:encoded><![CDATA[
arXiv:2508.17803v2 Announce Type: replace 
Abstract: Reasoning large language models (RLLMs), such as OpenAI-O3 and DeepSeek-R1, have recently demonstrated remarkable capabilities by performing structured and multi-step reasoning. However, recent studies reveal that RLLMs often suffer from overthinking, i.e., producing unnecessarily lengthy reasoning chains even for simple questions, leading to excessive token consumption and computational inefficiency. Interestingly, we observe that when processing multiple questions in batch mode, RLLMs exhibit more resource-efficient behavior by dynamically compressing reasoning steps for easier problems, due to implicit resource competition. Inspired by this, we propose Dynamic Reasoning Quota Allocation (DRQA), a novel method that transfers the benefits of resource competition from batch processing to single-question inference. Specifically, DRQA leverages batch-generated preference data and reinforcement learning to train the model to allocate reasoning resources adaptively. By encouraging the model to internalize a preference for responses that are both accurate and concise, DRQA enables it to generate concise answers for simple questions while retaining sufficient reasoning depth for more challenging ones. Extensive experiments on a wide range of mathematical and scientific reasoning benchmarks demonstrate that DRQA significantly reduces token usage while maintaining, and in many cases improving, answer accuracy. By effectively mitigating the overthinking problem, DRQA offers a promising direction for more efficient and scalable deployment of RLLMs, and we hope it inspires further exploration into fine-grained control of reasoning behaviors.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM</title>
<link>https://arxiv.org/abs/2508.20514</link>
<guid>https://arxiv.org/abs/2508.20514</guid>
<content:encoded><![CDATA[
arXiv:2508.20514v2 Announce Type: replace 
Abstract: Topic discovery in scientific literature provides valuable insights for researchers to identify emerging trends and explore new avenues for investigation, facilitating easier scientific information retrieval. Many machine learning methods, particularly deep embedding techniques, have been applied to discover research topics. However, most existing topic discovery methods rely on word embedding to capture the semantics and lack a comprehensive understanding of scientific publications, struggling with complex, high-dimensional text relationships. Inspired by the exceptional comprehension of textual information by large language models (LLMs), we propose an advanced topic discovery method enhanced by LLMs to improve scientific topic identification, namely SciTopic. Specifically, we first build a textual encoder to capture the content from scientific publications, including metadata, title, and abstract. Next, we construct a space optimization module that integrates entropy-based sampling and triplet tasks guided by LLMs, enhancing the focus on thematic relevance and contextual intricacies between ambiguous instances. Then, we propose to fine-tune the textual encoder based on the guidance from the LLMs by optimizing the contrastive loss of the triplets, forcing the text encoder to better discriminate instances of different topics. Finally, extensive experiments conducted on three real-world datasets of scientific publications demonstrate that SciTopic outperforms the state-of-the-art (SOTA) scientific topic discovery methods, enabling researchers to gain deeper and faster insights.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Humans as Brittle as Large Language Models?</title>
<link>https://arxiv.org/abs/2509.07869</link>
<guid>https://arxiv.org/abs/2509.07869</guid>
<content:encoded><![CDATA[
arXiv:2509.07869v2 Announce Type: replace 
Abstract: The output of large language models (LLMs) is unstable, due both to non-determinism of the decoding process as well as to prompt brittleness. While the intrinsic non-determinism of LLM generation may mimic existing uncertainty in human annotations through distributional shifts in outputs, it is largely assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs. This raises the question: do human annotators show similar sensitivity to prompt changes? If so, should prompt brittleness in LLMs be considered problematic? One may alternatively hypothesize that prompt brittleness correctly reflects human annotation variances. To fill this research gap, we systematically compare the effects of prompt modifications on LLMs and identical instruction modifications for human annotators, focusing on the question of whether humans are similarly sensitive to prompt perturbations. To study this, we prompt both humans and LLMs for a set of text classification tasks conditioned on prompt variations. Our findings indicate that both humans and LLMs exhibit increased brittleness in response to specific types of prompt modifications, particularly those involving the substitution of alternative label sets or label formats. However, the distribution of human judgments is less affected by typographical errors and reversed label order than that of LLMs.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems</title>
<link>https://arxiv.org/abs/2509.09360</link>
<guid>https://arxiv.org/abs/2509.09360</guid>
<content:encoded><![CDATA[
arXiv:2509.09360v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly deployed in enterprise applications, yet their reliability remains limited by hallucinations, i.e., confident but factually incorrect information. Existing detection approaches, such as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not address the unique challenges of Retrieval-Augmented Generation (RAG) systems, where responses must be consistent with retrieved evidence. We therefore present MetaRAG, a metamorphic testing framework for hallucination detection in Retrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time, unsupervised, black-box setting, requiring neither ground-truth references nor access to model internals, making it suitable for proprietary and high-stakes domains. The framework proceeds in four stages: (1) decompose answers into atomic factoids, (2) generate controlled mutations of each factoid using synonym and antonym substitutions, (3) verify each variant against the retrieved context (synonyms are expected to be entailed and antonyms contradicted), and (4) aggregate penalties for inconsistencies into a response-level hallucination score. Crucially for identity-aware AI, MetaRAG localizes unsupported claims at the factoid span where they occur (e.g., pregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility), allowing users to see flagged spans and enabling system designers to configure thresholds and guardrails for identity-sensitive queries. Experiments on a proprietary enterprise dataset illustrate the effectiveness of MetaRAG for detecting hallucinations and enabling trustworthy deployment of RAG-based conversational agents. We also outline a topic-based deployment design that translates MetaRAG's span-level scores into identity-aware safeguards; this design is discussed but not evaluated in our experiments.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Policy-as-Prompt: Turning AI Governance Rules into Guardrails for AI Agents</title>
<link>https://arxiv.org/abs/2509.23994</link>
<guid>https://arxiv.org/abs/2509.23994</guid>
<content:encoded><![CDATA[
arXiv:2509.23994v2 Announce Type: replace 
Abstract: As autonomous AI agents are used in regulated and safety-critical settings, organizations need effective ways to turn policy into enforceable controls. We introduce a regulatory machine learning framework that converts unstructured design artifacts (like PRDs, TDDs, and code) into verifiable runtime guardrails. Our Policy as Prompt method reads these documents and risk controls to build a source-linked policy tree. This tree is then compiled into lightweight, prompt-based classifiers for real-time runtime monitoring. The system is built to enforce least privilege and data minimization. For conformity assessment, it provides complete provenance, traceability, and audit logging, all integrated with a human-in-the-loop review process. Evaluations show our system reduces prompt-injection risk, blocks out-of-scope requests, and limits toxic outputs. It also generates auditable rationales aligned with AI governance frameworks. By treating policies as executable prompts (a policy-as-code for agents), this approach enables secure-by-design deployment, continuous compliance, and scalable AI safety and AI security assurance for regulatable ML.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning MedGemma for Clinical Captioning to Enhance Multimodal RAG over Malaysia CPGs</title>
<link>https://arxiv.org/abs/2510.15418</link>
<guid>https://arxiv.org/abs/2510.15418</guid>
<content:encoded><![CDATA[
arXiv:2510.15418v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation systems are essential for providing fact-based guidance from Malaysian Clinical Practice Guidelines. However, their effectiveness with image-based queries is limited, as general Vision-Language Model captions often lack clinical specificity and factual grounding. This study proposes and validates a framework to specialize the MedGemma model for generating high-fidelity captions that serve as superior queries. To overcome data scarcity, we employ a knowledge distillation pipeline to create a synthetic dataset across dermatology, fundus, and chest radiography domains, and fine-tune MedGemma using the parameter-efficient QLoRA method. Performance was rigorously assessed through a dual framework measuring both classification accuracy and, via a novel application of the RAGAS framework, caption faithfulness, relevancy, and correctness. The fine-tuned model demonstrated substantial improvements in classification performance, while RAGAS evaluation confirmed significant gains in caption faithfulness and correctness, validating the models ability to produce reliable, factually grounded descriptions. This work establishes a robust pipeline for specializing medical VLMs and validates the resulting model as a high-quality query generator, laying the groundwork for enhancing multimodal RAG systems in evidence-based clinical decision support.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Can String Probability Tell Us About Grammaticality?</title>
<link>https://arxiv.org/abs/2510.16227</link>
<guid>https://arxiv.org/abs/2510.16227</guid>
<content:encoded><![CDATA[
arXiv:2510.16227v2 Announce Type: replace 
Abstract: What have language models (LMs) learned about grammar? This question remains hotly debated, with major ramifications for linguistic theory. However, since probability and grammaticality are distinct notions in linguistics, it is not obvious what string probabilities can reveal about an LM's underlying grammatical knowledge. We present a theoretical analysis of the relationship between grammar, meaning, and string probability, based on simple assumptions about the generative process of corpus data. Our framework makes three predictions, which we validate empirically using 280K sentence pairs in English and Chinese: (1) correlation between the probability of strings within minimal pairs, i.e., string pairs with minimal semantic differences; (2) correlation between models' and humans' deltas within minimal pairs; and (3) poor separation in probability space between unpaired grammatical and ungrammatical strings. Our analyses give theoretical grounding for using probability to learn about LMs' structural knowledge, and suggest directions for future work in LM grammatical evaluation.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics</title>
<link>https://arxiv.org/abs/2510.17797</link>
<guid>https://arxiv.org/abs/2510.17797</guid>
<content:encoded><![CDATA[
arXiv:2510.17797v2 Announce Type: replace 
Abstract: As information grows exponentially, enterprises face increasing pressure to transform unstructured data into coherent, actionable insights. While autonomous agents show promise, they often struggle with domain-specific nuances, intent alignment, and enterprise integration. We present Enterprise Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning Agent for adaptive query decomposition, (2) four specialized search agents (General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a Visualization Agent for data-driven insights, and (5) a reflection mechanism that detects knowledge gaps and updates research direction with optional human-in-the-loop steering guidance. These components enable automated report generation, real-time streaming, and seamless enterprise deployment, as validated on internal datasets. On open-ended benchmarks including DeepResearch Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without any human steering. We release the EDR framework and benchmark trajectories to advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and Dataset at https://huggingface.co/datasets/Salesforce/EDR-200
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Re:Member: Emotional Question Generation from Personal Memories</title>
<link>https://arxiv.org/abs/2510.19030</link>
<guid>https://arxiv.org/abs/2510.19030</guid>
<content:encoded><![CDATA[
arXiv:2510.19030v2 Announce Type: replace 
Abstract: We present Re:Member, a system that explores how emotionally expressive, memory-grounded interaction can support more engaging second language (L2) learning. By drawing on users' personal videos and generating stylized spoken questions in the target language, Re:Member is designed to encourage affective recall and conversational engagement. The system aligns emotional tone with visual context, using expressive speech styles such as whispers or late-night tones to evoke specific moods. It combines WhisperX-based transcript alignment, 3-frame visual sampling, and Style-BERT-VITS2 for emotional synthesis within a modular generation pipeline. Designed as a stylized interaction probe, Re:Member highlights the role of affect and personal media in learner-centered educational technologies.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation</title>
<link>https://arxiv.org/abs/2510.22115</link>
<guid>https://arxiv.org/abs/2510.22115</guid>
<content:encoded><![CDATA[
arXiv:2510.22115v2 Announce Type: replace 
Abstract: We introduce Ling 2.0, a series reasoning-oriented language foundation built upon the principle that every activation boosts reasoning capability. Designed to scale from tens of billions to one trillion parameters under a unified Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity, cross-scale consistency, and efficiency guided by empirical scaling laws. The series includes three non-thinking (instruct) models - Ling-mini-2.0, Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and achieving up to 7-fold active-compute efficiency compared with dense counterparts. Ling 2.0 integrates coordinated innovations across model architecture, pre-training, post-training, and infrastructure: a high-sparsity MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale FP8 training with fine-grained heterogeneous pipelines. At the trillion scale, Ling-1T establishes a new Pareto frontier of reasoning accuracy versus computational efficiency, demonstrating that sparse activation, when properly aligned with reasoning objectives, enables scalable and efficient intelligence. Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for advancing future reasoning and thinking models, including the Ring series built upon the same base.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMDocIR: Benchmarking Multimodal Retrieval for Long Documents</title>
<link>https://arxiv.org/abs/2501.08828</link>
<guid>https://arxiv.org/abs/2501.08828</guid>
<content:encoded><![CDATA[
arXiv:2501.08828v3 Announce Type: replace-cross 
Abstract: Multimodal document retrieval aims to identify and retrieve various forms of multimodal content, such as figures, tables, charts, and layout information from extensive documents. Despite its increasing popularity, there is a notable lack of a comprehensive and robust benchmark to effectively evaluate the performance of systems in such tasks. To address this gap, this work introduces a new benchmark, named MMDocIR, that encompasses two distinct tasks: page-level and layout-level retrieval. The former evaluates the performance of identifying the most relevant pages within a long document, while the later assesses the ability of detecting specific layouts, providing a more fine-grained measure than whole-page analysis. A layout refers to a variety of elements, including textual paragraphs, equations, figures, tables, or charts. The MMDocIR benchmark comprises a rich dataset featuring 1,685 questions annotated by experts and 173,843 questions with bootstrapped labels, making it a valuable resource in multimodal document retrieval for both training and evaluation. Through rigorous experiments, we demonstrate that (i) visual retrievers significantly outperform their text counterparts, (ii) MMDocIR training set effectively enhances the performance of multimodal document retrieval and (iii) text retrievers leveraging VLM-text significantly outperforms retrievers relying on OCR-text. Our dataset is available at https://mmdocrag.github.io/MMDocIR/.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIRepr: An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science</title>
<link>https://arxiv.org/abs/2502.16395</link>
<guid>https://arxiv.org/abs/2502.16395</guid>
<content:encoded><![CDATA[
arXiv:2502.16395v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly used to automate data analysis through executable code generation. Yet, data science tasks often admit multiple statistically valid solutions, e.g. different modeling strategies, making it critical to understand the reasoning behind analyses, not just their outcomes. While manual review of LLM-generated code can help ensure statistical soundness, it is labor-intensive and requires expertise. A more scalable approach is to evaluate the underlying workflows-the logical plans guiding code generation. However, it remains unclear how to assess whether an LLM-generated workflow supports reproducible implementations.
  To address this, we present AIRepr, an Analyst-Inspector framework for automatically evaluating and improving the reproducibility of LLM-generated data analysis workflows. Our framework is grounded in statistical principles and supports scalable, automated assessment. We introduce two novel reproducibility-enhancing prompting strategies and benchmark them against standard prompting across 15 analyst-inspector LLM pairs and 1,032 tasks from three public benchmarks. Our findings show that workflows with higher reproducibility also yield more accurate analyses, and that reproducibility-enhancing prompts substantially improve both metrics. This work provides a foundation for transparent, reliable, and efficient human-AI collaboration in data science. Our code is publicly available.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CSPLADE: Learned Sparse Retrieval with Causal Language Models</title>
<link>https://arxiv.org/abs/2504.10816</link>
<guid>https://arxiv.org/abs/2504.10816</guid>
<content:encoded><![CDATA[
arXiv:2504.10816v3 Announce Type: replace-cross 
Abstract: In recent years, dense retrieval has been the focus of information retrieval (IR) research. While effective, dense retrieval produces uninterpretable dense vectors, and suffers from the drawback of large index size. Learned sparse retrieval (LSR) has emerged as promising alternative, achieving competitive retrieval performance while also being able to leverage the classical inverted index data structure for efficient retrieval. However, limited works have explored scaling LSR beyond BERT scale. In this work, we identify two challenges in training large language models (LLM) for LSR: (1) training instability during the early stage of contrastive training; (2) suboptimal performance due to pre-trained LLM's unidirectional attention. To address these challenges, we propose two corresponding techniques: (1) a lightweight adaptation training phase to eliminate training instability; (2) two model variants to enable bidirectional information. With these techniques, we are able to train LSR models with 8B scale LLM, and achieve competitive retrieval performance with reduced index size. Furthermore, we are among the first to analyze the performance-efficiency tradeoff of LLM-based LSR model through the lens of model quantization. Our findings provide insights into adapting LLMs for efficient retrieval modeling.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Explainable Fake Image Detection with Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2504.14245</link>
<guid>https://arxiv.org/abs/2504.14245</guid>
<content:encoded><![CDATA[
arXiv:2504.14245v2 Announce Type: replace-cross 
Abstract: Progress in image generation raises significant public security concerns. We argue that fake image detection should not operate as a "black box". Instead, an ideal approach must ensure both strong generalization and transparency. Recent progress in Multi-modal Large Language Models (MLLMs) offers new opportunities for reasoning-based AI-generated image detection. In this work, we evaluate the capabilities of MLLMs in comparison to traditional detection methods and human evaluators, highlighting their strengths and limitations. Furthermore, we design six distinct prompts and propose a framework that integrates these prompts to develop a more robust, explainable, and reasoning-driven detection system. The code is available at https://github.com/Gennadiyev/mllm-defake.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRACE: Textual Relevance Augmentation and Contextual Encoding for Multimodal Hate Detection</title>
<link>https://arxiv.org/abs/2504.17902</link>
<guid>https://arxiv.org/abs/2504.17902</guid>
<content:encoded><![CDATA[
arXiv:2504.17902v2 Announce Type: replace-cross 
Abstract: Social media memes are a challenging domain for hate detection because they intertwine visual and textual cues into culturally nuanced messages. To tackle these challenges, we introduce TRACE, a hierarchical multimodal framework that leverages visually grounded context augmentation, along with a novel caption-scoring network to emphasize hate-relevant content, and parameter-efficient fine-tuning of CLIP's text encoder. Our experiments demonstrate that selectively fine-tuning deeper text encoder layers significantly enhances performance compared to simpler projection-layer fine-tuning methods. Specifically, our framework achieves state-of-the-art accuracy (0.807) and F1-score (0.806) on the widely-used Hateful Memes dataset, matching the performance of considerably larger models while maintaining efficiency. Moreover, it achieves superior generalization on the MultiOFF offensive meme dataset (F1-score 0.673), highlighting robustness across meme categories. Additional analyses confirm that robust visual grounding and nuanced text representations significantly reduce errors caused by benign confounders. We publicly release our code to facilitate future research.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Anytime Reasoning via Budget Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2505.13438</link>
<guid>https://arxiv.org/abs/2505.13438</guid>
<content:encoded><![CDATA[
arXiv:2505.13438v3 Announce Type: replace-cross 
Abstract: Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Retrieval-Augmented Multimodal Generation for Document Question Answering</title>
<link>https://arxiv.org/abs/2505.16470</link>
<guid>https://arxiv.org/abs/2505.16470</guid>
<content:encoded><![CDATA[
arXiv:2505.16470v2 Announce Type: replace-cross 
Abstract: Document Visual Question Answering (DocVQA) faces dual challenges in processing lengthy multimodal documents (text, images, tables) and performing cross-modal reasoning. Current document retrieval-augmented generation (DocRAG) methods remain limited by their text-centric approaches, frequently missing critical visual information. The field also lacks robust benchmarks for assessing multimodal evidence selection and integration. We introduce MMDocRAG, a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with multi-page, cross-modal evidence chains. Our framework introduces innovative metrics for evaluating multimodal quote selection and enables answers that interleave text with relevant visual elements. Through large-scale experiments with 60 VLM/LLM models and 14 retrieval systems, we identify persistent challenges in multimodal evidence retrieval, selection, and integration.Key findings reveal advanced proprietary LVMs show superior performance than open-sourced alternatives. Also, they show moderate advantages using multimodal inputs over text-only inputs, while open-source alternatives show significant performance degradation. Notably, fine-tuned LLMs achieve substantial improvements when using detailed image descriptions. MMDocRAG establishes a rigorous testing ground and provides actionable insights for developing more robust multimodal DocVQA systems. Our benchmark and code are available at https://mmdocrag.github.io/MMDocRAG/.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inference-Time Hyper-Scaling with KV Cache Compression</title>
<link>https://arxiv.org/abs/2506.05345</link>
<guid>https://arxiv.org/abs/2506.05345</guid>
<content:encoded><![CDATA[
arXiv:2506.05345v2 Announce Type: replace-cross 
Abstract: Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8$\times$ compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference latency and memory load. For instance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and 9.7 on LiveCodeBench on average for an equivalent number of memory reads.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering</title>
<link>https://arxiv.org/abs/2506.10751</link>
<guid>https://arxiv.org/abs/2506.10751</guid>
<content:encoded><![CDATA[
arXiv:2506.10751v2 Announce Type: replace-cross 
Abstract: Automated question answering (QA) over electronic health records (EHRs) can bridge critical information gaps for clinicians and patients, yet it demands both precise evidence retrieval and faithful answer generation under limited supervision. In this work, we present Neural, the runner-up in the BioNLP 2025 ArchEHR-QA shared task on evidence-grounded clinical QA. Our proposed method decouples the task into (1) sentence-level evidence identification and (2) answer synthesis with explicit citations. For each stage, we automatically explore the prompt space with DSPy's MIPROv2 optimizer, jointly tuning instructions and few-shot demonstrations on the development set. A self-consistency voting scheme further improves evidence recall without sacrificing precision. On the hidden test set, our method attains an overall score of 51.5, placing second stage while outperforming standard zero-shot and few-shot prompting by over 20 and 10 points, respectively. These results indicate that data-driven prompt optimization is a cost-effective alternative to model fine-tuning for high-stakes clinical QA, advancing the reliability of AI assistants in healthcare.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Holistic Evaluation of Multimodal LLMs on Spatial Intelligence</title>
<link>https://arxiv.org/abs/2508.13142</link>
<guid>https://arxiv.org/abs/2508.13142</guid>
<content:encoded><![CDATA[
arXiv:2508.13142v3 Announce Type: replace-cross 
Abstract: Multimodal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, the very capability that anchors artificial general intelligence in the physical world. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path toward spatial intelligence. We thus propose EASI for holistic Evaluation of multimodAl LLMs on Spatial Intelligence. EASI conceptualizes a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and a standardized protocol for the fair evaluation of state-of-the-art proprietary and open-source models. In this report, we conduct the study across eight key benchmarks, at a cost exceeding ten billion total tokens. Our empirical study then reveals that (1) GPT-5 demonstrates unprecedented strength in spatial intelligence (SI), yet (2) still falls short of human performance significantly across a broad spectrum of SI-tasks. Moreover, we (3) show that SI-tasks expose greater model capability deficiency than non-SI tasks, to the extent that (4) proprietary models do not exhibit a decisive advantage when facing the most difficult ones. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans, yet fail even the most advanced multimodal models.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-Augmented Review Generation for Poisoning Recommender Systems</title>
<link>https://arxiv.org/abs/2508.15252</link>
<guid>https://arxiv.org/abs/2508.15252</guid>
<content:encoded><![CDATA[
arXiv:2508.15252v2 Announce Type: replace-cross 
Abstract: Recent studies have shown that recommender systems (RSs) are highly vulnerable to data poisoning attacks, where malicious actors inject fake user profiles, including a group of well-designed fake ratings, to manipulate recommendations. Due to security and privacy constraints in practice, attackers typically possess limited knowledge of the victim system and thus need to craft profiles that have transferability across black-box RSs. To maximize the attack impact, the profiles often remains imperceptible. However, generating such high-quality profiles with the restricted resources is challenging. Some works suggest incorporating fake textual reviews to strengthen the profiles; yet, the poor quality of the reviews largely undermines the attack effectiveness and imperceptibility under the practical setting. To tackle the above challenges, in this paper, we propose to enhance the quality of the review text by harnessing in-context learning (ICL) capabilities of multimodal foundation models. To this end, we introduce a demonstration retrieval algorithm and a text style transfer strategy to augment the navie ICL. Specifically, we propose a novel practical attack framework named RAGAN to generate high-quality fake user profiles, which can gain insights into the robustness of RSs. The profiles are generated by a jailbreaker and collaboratively optimized on an instructional agent and a guardian to improve the attack transferability and imperceptibility. Comprehensive experiments on various real-world datasets demonstrate that RAGAN achieves the state-of-the-art poisoning attack performance.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence</title>
<link>https://arxiv.org/abs/2509.03505</link>
<guid>https://arxiv.org/abs/2509.03505</guid>
<content:encoded><![CDATA[
arXiv:2509.03505v2 Announce Type: replace-cross 
Abstract: We argue that progress toward general intelligence requires complementary foundation models grounded in language, the physical world, and structured data. This report presents LimiX-16M and LimiX-2M, two instantiations of our large structured-data models (LDMs). Both models treat structured data as a joint distribution over variables and missingness, thus capable of addressing a wide range of tabular tasks through query-based conditional prediction via a single model. They are pretrained using masked joint-distribution modeling with an episodic, context-conditional objective, supporting rapid, training-free adaptation at inference. We evaluate LimiX models across 11 large structured-data benchmarks with broad regimes of sample size, feature dimensionality, class number, categorical-to-numerical feature ratio, missingness, and sample-to-feature ratios. LimiX-16M consistently surpasses strong baselines, as shown in Figure 1 and Figure 2. The superiority holds across a wide range of tasks, such as classification, regression, missing value imputation, and data generation, often by substantial margins, while avoiding task-specific architectures or bespoke training per task. Notably, LimiX-2M delivers strong results under tight compute and memory budgets. We also present the first scaling law study for LDMs, revealing how data and model scaling jointly influence downstream performance and offering quantitative guidance for tabular foundation modeling. All LimiX models are publicly accessible under Apache 2.0.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward</title>
<link>https://arxiv.org/abs/2510.03222</link>
<guid>https://arxiv.org/abs/2510.03222</guid>
<content:encoded><![CDATA[
arXiv:2510.03222v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large Language Models in complex reasoning, yet its scalability is often hindered by a training bottleneck where performance plateaus as policy entropy collapses, signaling a loss of exploration. Previous methods typically address this by maintaining high policy entropy, yet the precise mechanisms that govern meaningful exploration have remained underexplored. Our analysis suggests that an unselective focus on entropy risks amplifying irrelevant tokens and destabilizing training. This paper investigates the exploration dynamics within RLVR and identifies a key issue: the gradual elimination of valuable low-probability exploratory tokens, which we term \textbf{\textit{reasoning sparks}}. We find that while abundant in pre-trained models, these sparks are systematically extinguished during RLVR due to over-penalization, leading to a degeneracy in exploration. To address this, we introduce Low-probability Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a heuristic proxy distribution. This proxy is constructed by filtering out presumed noise tokens and re-normalizing the distribution over the remaining candidates. The result is a less-noisy proxy where the probability of \textit{reasoning sparks} is amplified, which then serves as a soft regularization target to shield these valuable tokens from elimination via KL divergence. Experiments show that Lp-Reg enables stable on-policy RL, sustaining continuous scaling across $3,000$ training steps and $81,204$ GPU-hours, where baseline entropy-control methods collapse. This sustained exploration leads to state-of-the-art performance, achieving a $60.17\%$ average accuracy on five math benchmarks, an improvement of $2.66\%$ over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Internal World Models as Imagination Networks in Cognitive Agents</title>
<link>https://arxiv.org/abs/2510.04391</link>
<guid>https://arxiv.org/abs/2510.04391</guid>
<content:encoded><![CDATA[
arXiv:2510.04391v2 Announce Type: replace-cross 
Abstract: What is the computational objective of imagination? While classical interpretations suggest imagination is useful for maximizing rewards, recent findings challenge this view. In this study, we propose that imagination serves to access an internal world model (IWM) and use psychological network analysis to explore IWMs in humans and large language models (LLMs). Specifically, we assessed imagination vividness ratings using two questionnaires and constructed imagination networks from these reports. Imagination networks from human groups showed correlations between different centrality measures, including expected influence, strength, and closeness. However, imagination networks from LLMs showed a lack of clustering and lower correlations between centrality measures under different prompts and conversational memory conditions. Together, these results indicate a lack of similarity between IWMs in human and LLM agents. Overall, our study offers a novel method for comparing internally-generated representations in humans and AI, providing insights for developing human-like imagination in artificial intelligence.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HugAgent: Benchmarking LLMs for Simulation of Individualized Human Reasoning</title>
<link>https://arxiv.org/abs/2510.15144</link>
<guid>https://arxiv.org/abs/2510.15144</guid>
<content:encoded><![CDATA[
arXiv:2510.15144v3 Announce Type: replace-cross 
Abstract: Simulating human reasoning in open-ended tasks has long been a central aspiration in AI and cognitive science. While large language models now approximate human responses at scale, they remain tuned to population-level consensus, often erasing the individuality of reasoning styles and belief trajectories. To advance the vision of more human-like reasoning in machines, we introduce HugAgent (Human-Grounded Agent Benchmark), which rethinks human reasoning simulation along three dimensions: (i) from averaged to individualized reasoning, (ii) from behavioral mimicry to cognitive alignment, and (iii) from vignette-based to open-ended data. The benchmark evaluates whether a model can predict a specific person's behavioral responses and the underlying reasoning dynamics in out-of-distribution scenarios, given partial evidence of their prior views. HugAgent adopts a dual-track design: a human track that automates and scales the think-aloud method to collect ecologically valid human reasoning data, and a synthetic track for further scalability and systematic stress testing. This architecture enables low-cost, extensible expansion to new tasks and populations. Experiments with state-of-the-art language models reveal persistent adaptation gaps, positioning HugAgent as the first extensible benchmark for aligning machine reasoning with the individuality of human thought. The benchmark, along with its complete data collection pipeline and companion chatbot, is open-sourced as HugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking (https://anonymous.4open.science/r/trace-your-thinking).
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations</title>
<link>https://arxiv.org/abs/2510.22780</link>
<guid>https://arxiv.org/abs/2510.22780</guid>
<content:encoded><![CDATA[
arXiv:2510.22780v2 Announce Type: replace-cross 
Abstract: AI agents are continually optimized for tasks related to human work, such as software engineering and professional writing, signaling a pressing trend with significant impacts on the human workforce. However, these agent developments have often not been grounded in a clear understanding of how humans execute work, to reveal what expertise agents possess and the roles they can play in diverse workflows. In this work, we study how agents do human work by presenting the first direct comparison of human and agent workers across multiple essential work-related skills: data analysis, engineering, computation, writing, and design. To better understand and compare heterogeneous computer-use activities of workers, we introduce a scalable toolkit to induce interpretable, structured workflows from either human or agent computer-use activities. Using such induced workflows, we compare how humans and agents perform the same tasks and find that: (1) While agents exhibit promise in their alignment to human workflows, they take an overwhelmingly programmatic approach across all work domains, even for open-ended, visually dependent tasks like design, creating a contrast with the UI-centric methods typically used by humans. (2) Agents produce work of inferior quality, yet often mask their deficiencies via data fabrication and misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster and cost 90.4-96.2% less than humans, highlighting the potential for enabling efficient collaboration by delegating easily programmable tasks to agents.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs</title>
<link>https://arxiv.org/abs/2511.03738</link>
<guid>https://arxiv.org/abs/2511.03738</guid>
<content:encoded><![CDATA[
<div> Personality Traits, Large Language Models, Controlling, Behavioural Manipulation, Transformer Layers <br />
Summary: Large Language Models possess implicit personalities in their generation, necessitating better control mechanisms. This study explores using the Big Five Personality Traits to extract hidden state activations from transformer layers and identify optimal layers for injecting trait-specific alignments. By applying low-rank subspace discovery methods, the study finds that personality traits share a low-rank subspace, enabling precise steering of model outputs without compromising fluency or general capabilities. The proposed pipeline allows for dynamic layer selection, facilitating effective integration of psychological theory into practical model alignment. <div>
arXiv:2511.03738v1 Announce Type: new 
Abstract: Large Language Models exhibit implicit personalities in their generation, but reliably controlling or aligning these traits to meet specific needs remains an open challenge. The need for effective mechanisms for behavioural manipulation of the model during generation is a critical gap in the literature that needs to be fulfilled. Personality-aware LLMs hold a promising direction towards this objective. However, the relationship between these psychological constructs and their representations within LLMs remains underexplored and requires further investigation. Moreover, it is intriguing to understand and study the use of these representations to steer the models' behaviour. We propose a novel pipeline that extracts hidden state activations from transformer layers using the Big Five Personality Traits (Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism), which is a comprehensive and empirically validated framework to model human personality applies low-rank subspace discovery methods, and identifies trait-specific optimal layers across different model architectures for robust injection. The resulting personality-aligned directions are then operationalised through a flexible steering framework with dynamic layer selection, enabling precise control of trait expression in LLM outputs. Our findings reveal that personality traits occupy a low-rank shared subspace, and that these latent structures can be transformed into actionable mechanisms for effective steering through careful perturbations without impacting the fluency, variance and general capabilities, helping to bridge the gap between psychological theory and practical model alignment.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TextualVerifier: Verify TextGrad Step-by-Step</title>
<link>https://arxiv.org/abs/2511.03739</link>
<guid>https://arxiv.org/abs/2511.03739</guid>
<content:encoded><![CDATA[
<div> TextGrad, TextualVerifier, automatic differentiation, verification framework, large language models <br />
Summary: <br />
TextGrad is a text-based automatic differentiation approach for composite AI systems. TextualVerifier is introduced to provide self-verification mechanisms for reasoning validity in decision making. It leverages chain-of-thought reasoning, majority voting, and large language models. The four-stage workflow includes chain-of-thought decomposition, variant generation, majority voting, and consensus aggregation. Experimental evaluation using Gemini 1.5 Pro shows significant improvements. Standalone evaluation on PRM800K demonstrates a 29% improvement in reasoning validity. Integration with TextGrad on benchmarks results in a 2.2 percentage point gain in loss function verification. TextualVerifier versioning yields further improvements on various benchmarks. This framework offers the first self-verification for TextGrad through large language model techniques, enhancing the reliability of reasoning in text-based optimization and paving the way for new verification avenues. <br /> <div>
arXiv:2511.03739v1 Announce Type: new 
Abstract: TextGrad is a novel approach to text-based automatic differentiation that enables composite AI systems to perform optimization without explicit numerical equations. However, it currently lacks self-verification mechanisms that ensure reasoning validity in text-based decision making. This research introduces TextualVerifier, a verification framework that leverages chain-of-thought reasoning and majority voting with large language models to address this verification gap. TextualVerifier implements a four-stage workflow: chain-of-thought decomposition, variant generation, majority voting, and consensus aggregation. It integrates non-invasively with TextGrad at both the loss function and optimization result verification stages. Experimental evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1) standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically significant improvements (p < 0.001). In phase one, TextualVerifier improves the validity of reasoning steps by 29 percent. In phase two, integration into TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4 percent with a moderate overhead of 5.9 LLM calls on average. Further evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92 percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively. TextualVerifier thus presents the first self-verification framework for TextGrad through LLM-based techniques without requiring numerical gradients, enabling more reliable reasoning and opening new directions for verification in text-based optimization.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation</title>
<link>https://arxiv.org/abs/2511.03772</link>
<guid>https://arxiv.org/abs/2511.03772</guid>
<content:encoded><![CDATA[
<div> Dataset, Greek Dialectal, Fine-tuning, LLMs, Model architectures <br />
Summary: <br />
The article introduces the GRDD+ dataset, expanding on the existing Greek Dialectal Dataset with data from various Greek dialects. The GRDD+ now includes varieties such as Greco-Corsican, Griko, Maniot, Heptanesian, Tsakonian, and Katharevusa Greek, totaling 6,374,939 words across 10 varieties. The dataset is the first of its kind in terms of variation and size. The study conducted fine-tuning experiments on three model architectures (Llama-3-8B, Llama-3.1-8B, Krikri-8B) using the dataset and compared the results to frontier models like Claude-3.7-Sonnet, Gemini-2.5, and ChatGPT-5. The experiments aimed to assess the impact of high-quality dialectal data on Language Model Models' (LLMs) performance, highlighting the importance of diverse and extensive datasets for model training. <div>
arXiv:2511.03772v1 Announce Type: new 
Abstract: We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a dataset with total size 6,374,939 words and 10 varieties. This is the first dataset with such variation and size to date. We conduct a number of fine-tuning experiments to see the effect of good quality dialectal data on a number of LLMs. We fine-tune three model architectures (Llama-3-8B, Llama-3.1-8B, Krikri-8B) and compare the results to frontier models (Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PLLuM: A Family of Polish Large Language Models</title>
<link>https://arxiv.org/abs/2511.03823</link>
<guid>https://arxiv.org/abs/2511.03823</guid>
<content:encoded><![CDATA[
<div> Polish, Large Language Model, PLLuM, development, open-source <br />
<br />
Summary: Large Language Models (LLMs) have mostly been focused on English, leaving other languages with limited support. A consortium of major Polish research institutions has developed PLLuM (Polish Large Language Model) to cater specifically to the Polish language. The project involved creating a 140-billion-token Polish text corpus for pre-training, a custom instructions dataset, and a preference optimization dataset. A Responsible AI framework was implemented for data governance and output correction. The architecture, training procedures, and alignment techniques for base and instruction-tuned variants were detailed, showcasing their utility in a public administration task. By openly releasing PLLuM, the aim is to promote open research and enhance sovereign AI technologies in Poland. <br /> <div>
arXiv:2511.03823v1 Announce Type: new 
Abstract: Large Language Models (LLMs) play a central role in modern artificial intelligence, yet their development has been primarily focused on English, resulting in limited support for other languages. We present PLLuM (Polish Large Language Model), the largest open-source family of foundation models tailored specifically for the Polish language. Developed by a consortium of major Polish research institutions, PLLuM addresses the need for high-quality, transparent, and culturally relevant language models beyond the English-centric commercial landscape. We describe the development process, including the construction of a new 140-billion-token Polish text corpus for pre-training, a 77k custom instructions dataset, and a 100k preference optimization dataset. A key component is a Responsible AI framework that incorporates strict data governance and a hybrid module for output correction and safety filtering. We detail the models' architecture, training procedures, and alignment techniques for both base and instruction-tuned variants, and demonstrate their utility in a downstream task within public administration. By releasing these models publicly, PLLuM aims to foster open research and strengthen sovereign AI technologies in Poland.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models</title>
<link>https://arxiv.org/abs/2511.03827</link>
<guid>https://arxiv.org/abs/2511.03827</guid>
<content:encoded><![CDATA[
<div> Efficient, Token Alignment, Language Models, STARS, Rejection Sampling  
Summary:  
STARS proposes a decoding-time algorithm, Segment-level Token Alignment with Rejection Sampling, to align large language models with human values efficiently. The algorithm iteratively samples, scores, and rejects/accepts fixed-size token segments to correct the generation path early. Compared to traditional methods like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), STARS outperforms by up to 14.9 and 4.3 percentage points on win-rates, respectively, while remaining competitive with Best-of-N baselines. This approach enhances computational efficiency and alignment quality, providing a robust alternative to fine-tuning and full-sequence ranking methods for aligning language models.  
<br /><br />Summary: <div>
arXiv:2511.03827v1 Announce Type: new 
Abstract: Aligning large language models with human values is crucial for their safe deployment; however, existing methods, such as fine-tuning, are computationally expensive and suboptimal. In contrast, inference-time approaches like Best-of-N sampling require practically infeasible computation to achieve optimal alignment. We propose STARS: Segment-level Token Alignment with Rejection Sampling, a decoding-time algorithm that steers model generation by iteratively sampling, scoring, and rejecting/accepting short, fixed-size token segments. This allows for early correction of the generation path, significantly improving computational efficiency and boosting alignment quality. Across a suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT) by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up to 4.3 percentage points on win-rates, while remaining highly competitive with strong Best-of-N baselines. Our work establishes granular, reward-guided sampling as a generalizable, robust, and efficient alternative to traditional fine-tuning and full-sequence ranking methods for aligning LLMs.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification</title>
<link>https://arxiv.org/abs/2511.03830</link>
<guid>https://arxiv.org/abs/2511.03830</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-label text classification, large language models, affective text analysis, dichotomic queries, cache-aware inference

Summary:<br />
The article introduces a novel approach for efficient multi-label text classification using large language models (LLMs). By reformulating classification tasks as sequences of dichotomic decisions, efficiency gains are achieved without sacrificing accuracy. The method focuses on affective text analysis, covering 24 dimensions including emotions and sentiment. Through LLM-to-SLM distillation, a powerful annotator model (DeepSeek-V3) provides multiple annotations per text, which are then aggregated to fine-tune smaller models. The fine-tuned models demonstrate significant improvements over zero-shot baselines, particularly on dimensions seen during training. The study suggests that decomposing multi-label classification into dichotomic queries, combined with distillation and cache-aware inference, offers a scalable and effective framework for LLM-based classification. While the method is validated on affective states, it is generalizable and applicable across different domains.<br />Summary: <div>
arXiv:2511.03830v1 Announce Type: new 
Abstract: We introduce a method for efficient multi-label text classification with large language models (LLMs), built on reformulating classification tasks as sequences of dichotomic (yes/no) decisions. Instead of generating all labels in a single structured response, each target dimension is queried independently, which, combined with a prefix caching mechanism, yields substantial efficiency gains for short-text inference without loss of accuracy. To demonstrate the approach, we focus on affective text analysis, covering 24 dimensions including emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator model (DeepSeek-V3) provides multiple annotations per text, which are aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B, Gemma3-1B). The fine-tuned models show significant improvements over zero-shot baselines, particularly on the dimensions seen during training. Our findings suggest that decomposing multi-label classification into dichotomic queries, combined with distillation and cache-aware inference, offers a scalable and effective framework for LLM-based classification. While we validate the method on affective states, the approach is general and applicable across domains.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens</title>
<link>https://arxiv.org/abs/2511.03880</link>
<guid>https://arxiv.org/abs/2511.03880</guid>
<content:encoded><![CDATA[
<div> Keywords: low-resourced languages, machine translation, gender representation, dataset quality, societal biases

Summary:<br /><br />
The study investigates the quality of Machine Translation datasets for low-resourced languages such as Afan Oromo, Amharic, and Tigrinya, particularly focusing on gender representation. The research highlights that while the training data is rich in political and religious content, benchmark datasets mainly feature news, health, and sports. Furthermore, there is a noticeable bias towards male gender representation in names, verb genders, and stereotypical depictions. The datasets also contain harmful and toxic portrayals of women, with a significant prevalence in the language with the largest dataset size. The findings emphasize that a focus on quantity over quality in dataset collection can lead to detrimental societal biases and hinder the performance of language technologies for underrepresented languages. The study encourages further examination of datasets for low-resourced languages and early interventions to address harmful content.<br /><br />Summary: <div>
arXiv:2511.03880v1 Announce Type: new 
Abstract: As low-resourced languages are increasingly incorporated into NLP research, there is an emphasis on collecting large-scale datasets. But in prioritizing quantity over quality, we risk 1) building language technologies that perform poorly for these languages and 2) producing harmful content that perpetuates societal biases. In this paper, we investigate the quality of Machine Translation (MT) datasets for three low-resourced languages--Afan Oromo, Amharic, and Tigrinya, with a focus on the gender representation in the datasets. Our findings demonstrate that while training data has a large representation of political and religious domain text, benchmark datasets are focused on news, health, and sports. We also found a large skew towards the male gender--in names of persons, the grammatical gender of verbs, and in stereotypical depictions in the datasets. Further, we found harmful and toxic depictions against women, which were more prominent for the language with the largest amount of data, underscoring that quantity does not guarantee quality. We hope that our work inspires further inquiry into the datasets collected for low-resourced languages and prompts early mitigation of harmful content. WARNING: This paper contains discussion of NSFW content that some may find disturbing.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation</title>
<link>https://arxiv.org/abs/2511.03900</link>
<guid>https://arxiv.org/abs/2511.03900</guid>
<content:encoded><![CDATA[
<div> decoding, large language models, hallucination mitigation, knowledge graphs, evidence retrieval<br />
Summary:<br />
Graph-Retrieved Adaptive Decoding (GRAD) is introduced as a method to mitigate hallucination in large language models (LLMs) without the need for retraining. It constructs a token transition graph using corpus-derived evidence, which is then used during decoding to influence generation towards more accurate outputs. GRAD outperforms baselines in various question-answering tasks, achieving higher accuracy, lower hallucination rates, and greater correctness. It effectively combines model logits with graph-retrieved evidence to steer generation towards truthful and verifiable outputs. This lightweight approach offers a plug-and-play alternative to contrastive decoding and knowledge graph augmentation, showcasing the importance of statistical evidence in improving the reliability and accuracy of generated text. <br /> <div>
arXiv:2511.03900v1 Announce Type: new 
Abstract: Hallucination mitigation remains a persistent challenge for large language models (LLMs), even as model scales grow. Existing approaches often rely on external knowledge sources, such as structured databases or knowledge graphs, accessed through prompting or retrieval. However, prompt-based grounding is fragile and domain-sensitive, while symbolic knowledge integration incurs heavy retrieval and formatting costs. Motivated by knowledge graphs, we introduce Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds generation in corpus-derived evidence without retraining. GRAD constructs a sparse token transition graph by accumulating next-token logits across a small retrieved corpus in a single forward pass. During decoding, graph-retrieved logits are max-normalized and adaptively fused with model logits to favor high-evidence continuations while preserving fluency. Across three models and a range of question-answering benchmarks spanning intrinsic, extrinsic hallucination, and factuality tasks, GRAD consistently surpasses baselines, achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination rates, and 6.9$\%$ greater correctness compared to greedy decoding, while attaining the highest truth--informativeness product score among all methods. GRAD offers a lightweight, plug-and-play alternative to contrastive decoding and knowledge graph augmentation, demonstrating that statistical evidence from corpus-level token transitions can effectively steer generation toward more truthful and verifiable outputs.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context informs pragmatic interpretation in vision-language models</title>
<link>https://arxiv.org/abs/2511.03908</link>
<guid>https://arxiv.org/abs/2511.03908</guid>
<content:encoded><![CDATA[
<div> Keywords: iterated reference games, context-sensitive pragmatic reasoning, vision-language models, multi-turn linguistic environments, few-shot reference games <br />
Summary: 
In a study on iterated reference games, humans and vision-language models were tested on picking out novel referents using language in varying contexts. Model performance was initially below human levels without relevant context but improved significantly with relevant information. The difficulty of few-shot reference games with abstract referents was highlighted as a challenge for machine learning models. The study emphasizes the importance of context-sensitive pragmatic reasoning in multi-turn linguistic environments for both human and machine performance. <div>
arXiv:2511.03908v1 Announce Type: new 
Abstract: Iterated reference games - in which players repeatedly pick out novel referents using language - present a test case for agents' ability to perform context-sensitive pragmatic reasoning in multi-turn linguistic environments. We tested humans and vision-language models on trials from iterated reference games, varying the given context in terms of amount, order, and relevance. Without relevant context, models were above chance but substantially worse than humans. However, with relevant context, model performance increased dramatically over trials. Few-shot reference games with abstract referents remain a difficult task for machine learning models.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023</title>
<link>https://arxiv.org/abs/2511.03915</link>
<guid>https://arxiv.org/abs/2511.03915</guid>
<content:encoded><![CDATA[
<div> Keywords: Human Flourishing, Geographic Index, Social Media Discourse, Well-being, Social Change

Summary: 
The article introduces the Human Flourishing Geographic Index (HFGI), a novel measure derived from analyzing a vast amount of geolocated U.S. tweets over a decade. This index aims to quantify human flourishing, a multidimensional construct encompassing various aspects of well-being. By analyzing 48 indicators aligned with Harvard's Global Flourishing Study framework, the HFGI provides fine spatial and temporal resolution at county- and state-levels. The dataset offers insights into happiness, health, purpose, virtue, relationships, financial stability, attitudes towards migration, and perceptions of corruption. The measures are validated to ensure accuracy and alignment with established indicators. This resource enables multidisciplinary analyses of well-being, inequality, and social change in the United States. The HFGI offers a unique perspective on the dynamics of human flourishing as reflected in social media discourse, providing valuable insights for researchers and policymakers alike. 

<br /><br />Summary: <div>
arXiv:2511.03915v1 Announce Type: new 
Abstract: Quantifying human flourishing, a multidimensional construct including happiness, health, purpose, virtue, relationships, and financial stability, is critical for understanding societal well-being beyond economic indicators. Existing measures often lack fine spatial and temporal resolution. Here we introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned large language models to classify expressions across 48 indicators aligned with Harvard's Global Flourishing Study framework plus attitudes towards migration and perception of corruption. The dataset offers monthly and yearly county- and state-level indicators of flourishing-related discourse, validated to confirm that the measures accurately represent the underlying constructs and show expected correlations with established indicators. This resource enables multidisciplinary analyses of well-being, inequality, and social change at unprecedented resolution, offering insights into the dynamics of human flourishing as reflected in social media discourse across the United States over the past decade.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Direct Semantic Communication Between Large Language Models via Vector Translation</title>
<link>https://arxiv.org/abs/2511.03945</link>
<guid>https://arxiv.org/abs/2511.03945</guid>
<content:encoded><![CDATA[
<div> latent bridge, language models, vector translations, semantic exchange, information transfer

Summary:  
- The study addresses the communication limitations of large language models (LLMs) in multi-agent settings.
- By using vector translations, learned mappings enable direct semantic exchange between representation spaces.
- A dual-encoder translator achieves a high cosine alignment between Llama-2-7B and Mistral-7B-Instruct models.
- Injecting translated vectors at a blending strength of 30% guides the target model's generation without affecting computational stability.
- Bidirectional evaluation reveals a transfer asymmetry of 2.01:1, indicating the greater transferability of general-purpose models over instruction-tuned variants.
- This approach demonstrates the feasibility of cross-model latent communication, laying the groundwork for collaborative AI systems that communicate shared meaning rather than just tokens.<br /><br />Summary: <div>
arXiv:2511.03945v1 Announce Type: new 
Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large language models (LLMs) pass messages as plain tokens, discarding most latent semantics. This constrains information transfer and adds unnecessary computational overhead. We form a latent bridge via vector translations, which use learned mappings that enable direct semantic exchange between representation spaces. A dual-encoder translator trained between Llama-2-7B and Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the translated vectors at 30 percent blending strength steers the target model's generation without destabilizing logits. Bidirectional evaluation shows a 2.01:1 transfer asymmetry, indicating that general-purpose models yield more transferable representations than instruction-tuned variants. This conservative injection preserves computational stability while demonstrating that cross-model latent communication is feasible, enabling collaborative AI systems that share meaning rather than tokens.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises</title>
<link>https://arxiv.org/abs/2511.04020</link>
<guid>https://arxiv.org/abs/2511.04020</guid>
<content:encoded><![CDATA[
<div> abductive inference, retrieval-augmented generation, large language models, reasoning process, knowledge-intensive tasks <br />
Summary: <br />
- The paper introduces a framework that incorporates abductive inference into retrieval-augmented large language models (LLMs) to address incomplete evidence retrieval issues.
- The proposed method identifies insufficient evidence, generates potential missing premises, and verifies their validity through consistency and plausibility assessments.
- Experimental results on abductive reasoning and multi-hop question answering tasks demonstrate the effectiveness of the approach in enhancing answer accuracy and reasoning faithfulness.
- Integrating abductive inference in retrieval-augmented LLMs shows promise in improving the robustness and explainability of such systems.
- This work underlines the importance of abductive inference as a valuable approach for enhancing the performance of retrieval-augmented generation models. <br /> <div>
arXiv:2511.04020v1 Announce Type: new 
Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved evidence is incomplete, leaving gaps in the reasoning process. In such cases, \emph{abductive inference} -- the process of generating plausible missing premises to explain observations -- offers a principled approach to bridge these gaps. In this paper, we propose a framework that integrates abductive inference into retrieval-augmented LLMs. Our method detects insufficient evidence, generates candidate missing premises, and validates them through consistency and plausibility checks. Experimental results on abductive reasoning and multi-hop QA benchmarks show that our approach improves both answer accuracy and reasoning faithfulness. This work highlights abductive inference as a promising direction for enhancing the robustness and explainability of RAG systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WST: Weakly Supervised Transducer for Automatic Speech Recognition</title>
<link>https://arxiv.org/abs/2511.04035</link>
<guid>https://arxiv.org/abs/2511.04035</guid>
<content:encoded><![CDATA[
<div> Keywords: Recurrent Neural Network-Transducer, Weakly Supervised Transducer, automatic speech recognition, transcription errors, robustness

Summary: 
The article introduces a Weakly Supervised Transducer (WST) as a solution for reducing the reliance on large-scale annotated data in automatic speech recognition tasks. WST is designed to handle errors in transcripts without the need for confidence estimation or pre-trained models. Empirical evaluations on synthetic and industrial datasets demonstrate that WST maintains performance even with high transcription error rates, outperforming existing weakly supervised approaches like BTC and OTC. The results highlight the practical utility and robustness of WST in real-world ASR settings. The implementation of WST will be made publicly available for further research and development. <div>
arXiv:2511.04035v1 Announce Type: new 
Abstract: The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily on large-scale, high-quality annotated data, which are often costly and difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised Transducer (WST), which integrates a flexible training graph designed to robustly handle errors in the transcripts without requiring additional confidence estimation or auxiliary pre-trained models. Empirical evaluations on synthetic and industrial datasets reveal that WST effectively maintains performance even with transcription error rates of up to 70%, consistently outperforming existing Connectionist Temporal Classification (CTC)-based weakly supervised approaches, such as Bypass Temporal Classification (BTC) and Omni-Temporal Classification (OTC). These results demonstrate the practical utility and robustness of WST in realistic ASR settings. The implementation will be publicly available.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T-FIX: Text-Based Explanations with Features Interpretable to eXperts</title>
<link>https://arxiv.org/abs/2511.04070</link>
<guid>https://arxiv.org/abs/2511.04070</guid>
<content:encoded><![CDATA[
<div> knowledge-intensive, LLMs, explanations, expert alignment, T-FIX 

Summary: 
The article introduces the concept of expert alignment in evaluating explanations provided by Large Language Models (LLMs) in knowledge-intensive settings such as surgery, astronomy, and therapy. Unlike current evaluation schemes that focus on plausibility or internal faithfulness of explanations, expert alignment aims to ensure that the content of explanations aligns with expert intuition. The authors propose T-FIX, a benchmark comprising seven knowledge-intensive domains, to evaluate the alignment of LLM explanations with expert judgment. Collaborating with domain experts, novel metrics are developed to measure this alignment. This approach is essential for users who are domain experts, such as doctors, astrophysicists, and psychologists, as they require explanations that reflect expert-level reasoning. Overall, the article highlights the importance of expert alignment in providing meaningful explanations from LLMs in knowledge-intensive domains. 

<br /><br />Summary: <div>
arXiv:2511.04070v1 Announce Type: new 
Abstract: As LLMs are deployed in knowledge-intensive settings (e.g., surgery, astronomy, therapy), users expect not just answers, but also meaningful explanations for those answers. In these settings, users are often domain experts (e.g., doctors, astrophysicists, psychologists) who require explanations that reflect expert-level reasoning. However, current evaluation schemes primarily emphasize plausibility or internal faithfulness of the explanation, which fail to capture whether the content of the explanation truly aligns with expert intuition. We formalize expert alignment as a criterion for evaluating explanations with T-FIX, a benchmark spanning seven knowledge-intensive domains. In collaboration with domain experts, we develop novel metrics to measure the alignment of LLM explanations with expert judgment.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering</title>
<link>https://arxiv.org/abs/2511.04072</link>
<guid>https://arxiv.org/abs/2511.04072</guid>
<content:encoded><![CDATA[
<div> Keywords: Temporal Knowledge Graph Question Answering, Large Language Models, Plan of Knowledge framework, Temporal Knowledge Store, contrastive retrieval

Summary: 
The study introduces the Plan of Knowledge (PoK) framework to enhance Temporal Knowledge Graph Question Answering (TKGQA) by leveraging structured planning and temporal knowledge retrieval. This framework decomposes complex temporal questions into sub-objectives and utilizes a contrastive temporal retriever to selectively retrieve relevant facts from Temporal Knowledge Graphs (TKGs). By combining structured planning with temporal knowledge retrieval, PoK improves interpretability and factual consistency in temporal reasoning tasks. Experimental results on benchmark TKGQA datasets demonstrate that PoK significantly boosts retrieval precision and reasoning accuracy of Large Language Models (LLMs), outperforming state-of-the-art TKGQA methods by up to 56.0%. <div>
arXiv:2511.04072v1 Announce Type: new 
Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer time-sensitive questions by leveraging factual information from Temporal Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG embeddings or graph neural networks to inject temporal knowledge, they fail to fully understand the complex semantic information of time constraints. Recently, Large Language Models (LLMs) have shown remarkable progress, benefiting from their strong semantic understanding and reasoning generalization capabilities. However, their temporal reasoning ability remains limited. LLMs frequently suffer from hallucination and a lack of knowledge. To address these limitations, we propose the Plan of Knowledge framework with a contrastive temporal retriever, which is named PoK. Specifically, the proposed Plan of Knowledge module decomposes a complex temporal question into a sequence of sub-objectives from the pre-defined tools, serving as intermediate guidance for reasoning exploration. In parallel, we construct a Temporal Knowledge Store (TKS) with a contrastive retrieval framework, enabling the model to selectively retrieve semantically and temporally aligned facts from TKGs. By combining structured planning with temporal knowledge retrieval, PoK effectively enhances the interpretability and factual consistency of temporal reasoning. Extensive experiments on four benchmark TKGQA datasets demonstrate that PoK significantly improves the retrieval precision and reasoning accuracy of LLMs, surpassing the performance of the state-of-the-art TKGQA methods by 56.0% at most.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The truth is no diaper: Human and AI-generated associations to emotional words</title>
<link>https://arxiv.org/abs/2511.04077</link>
<guid>https://arxiv.org/abs/2511.04077</guid>
<content:encoded><![CDATA[
<div> word associations, human participants, cognitive styles, large language models, emotional load <br />
<br />
The study compares word associations between humans and large language models, focusing on emotionally loaded words. It finds that while there is moderate overlap between the two, LLMs tend to amplify the emotional load of stimuli and generate less creative associations compared to humans. Human responses are influenced by personal experience, emotions, and cognitive styles, leading to unpredictable associations. In contrast, LLM associations are more predictable and less creative. The ability to form associative links between unrelated concepts is crucial for creativity, with humans displaying a higher level of creativity in associations compared to LLMs. This study sheds light on the differences in associative behavior between humans and LLMs, highlighting the importance of understanding the nuances of word associations in gaining insight into the internal mental lexicon. <br /><br />Summary: <div>
arXiv:2511.04077v1 Announce Type: new 
Abstract: Human word associations are a well-known method of gaining insight into the internal mental lexicon, but the responses spontaneously offered by human participants to word cues are not always predictable as they may be influenced by personal experience, emotions or individual cognitive styles. The ability to form associative links between seemingly unrelated concepts can be the driving mechanisms of creativity. We perform a comparison of the associative behaviour of humans compared to large language models. More specifically, we explore associations to emotionally loaded words and try to determine whether large language models generate associations in a similar way to humans. We find that the overlap between humans and LLMs is moderate, but also that the associations of LLMs tend to amplify the underlying emotional load of the stimulus, and that they tend to be more predictable and less creative than human ones.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods</title>
<link>https://arxiv.org/abs/2511.04079</link>
<guid>https://arxiv.org/abs/2511.04079</guid>
<content:encoded><![CDATA[
<div> Keywords: transformer-based models, radiology reports, de-identification, protected health information (PHI) detection, benchmarking

Summary: 
Large-scale, transformer-based models were utilized to enhance automated de-identification of radiology reports. The models were fine-tuned on annotated radiology corpora from Stanford University, improving PHI detection across multiple categories. The model achieved high F1 scores on test sets from Stanford and the University of Pennsylvania, surpassing previous state-of-the-art performance. Synthetic PHI generation proved to be consistent in detectability and outperformed commercial vendor systems in PHI detection. The study demonstrated the effectiveness of large-scale, multimodal training in improving cross-institutional generalization and robustness. The transformer-based model established a new benchmark for secure clinical text processing. 

<br /><br />Summary: <div>
arXiv:2511.04079v1 Announce Type: new 
Abstract: Objective: To enhance automated de-identification of radiology reports by scaling transformer-based models through extensive training datasets and benchmarking performance against commercial cloud vendor systems for protected health information (PHI) detection. Materials and Methods: In this retrospective study, we built upon a state-of-the-art, transformer-based, PHI de-identification pipeline by fine-tuning on two large annotated radiology corpora from Stanford University, encompassing chest X-ray, chest CT, abdomen/pelvis CT, and brain MR reports and introducing an additional PHI category (AGE) into the architecture. Model performance was evaluated on test sets from Stanford and the University of Pennsylvania (Penn) for token-level PHI detection. We further assessed (1) the stability of synthetic PHI generation using a "hide-in-plain-sight" method and (2) performance against commercial systems. Precision, recall, and F1 scores were computed across all PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining the previous state-of-the-art model performance. Synthetic PHI evaluation showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50 independently de-identified Penn datasets. Our model outperformed all vendor systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754). Discussion: Large-scale, multimodal training improved cross-institutional generalization and robustness. Synthetic PHI generation preserved data utility while ensuring privacy. Conclusion: A transformer-based de-identification model trained on diverse radiology datasets outperforms prior academic and commercial systems in PHI detection and establishes a new benchmark for secure clinical text processing.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Characterization of List Language Identification in the Limit</title>
<link>https://arxiv.org/abs/2511.04103</link>
<guid>https://arxiv.org/abs/2511.04103</guid>
<content:encoded><![CDATA[
<div> characterization, language identification, list identification, limit, rate

Summary: 
- The study focuses on language identification in the limit, exploring the possibility of correctly guessing the target language after a finite time.
- While classical results by Gold showed the impossibility of language identification in the limit for various language sets, Angluin provided a specific characterization for achievable tasks.
- The research introduces the concept of $k$-list identification, where the learner can provide a list of $k$ guesses at each time step to ensure correctness beyond a finite time.
- An exact characterization is presented for language collections that can be $k$-list identified in the limit, building on Angluin's initial framework.
- The findings establish rates for list identification in the statistical setting, demonstrating that exponential rates can be achieved for $k$-list identification in the limit, while collections that are not $k$-list identifiable cannot be identified at any rate approaching zero.<br /><br />Summary: <div>
arXiv:2511.04103v1 Announce Type: new 
Abstract: We study the problem of language identification in the limit, where given a sequence of examples from a target language, the goal of the learner is to output a sequence of guesses for the target language such that all the guesses beyond some finite time are correct. Classical results of Gold showed that language identification in the limit is impossible for essentially any interesting collection of languages. Later, Angluin gave a precise characterization of language collections for which this task is possible. Motivated by recent positive results for the related problem of language generation, we revisit the classic language identification problem in the setting where the learner is given the additional power of producing a list of $k$ guesses at each time step. The goal is to ensure that beyond some finite time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be $k$-list identified in the limit, based on a recursive version of Angluin's characterization (for language identification with a list of size $1$). This further leads to a conceptually appealing characterization: A language collection can be $k$-list identified in the limit if and only if the collection can be decomposed into $k$ collections of languages, each of which can be identified in the limit (with a list of size $1$). We also use our characterization to establish rates for list identification in the statistical setting where the input is drawn as an i.i.d. stream from a distribution supported on some language in the collection. Our results show that if a collection is $k$-list identifiable in the limit, then the collection can be $k$-list identified at an exponential rate, and this is best possible. On the other hand, if a collection is not $k$-list identifiable in the limit, then it cannot be $k$-list identified at any rate that goes to zero.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models</title>
<link>https://arxiv.org/abs/2511.04108</link>
<guid>https://arxiv.org/abs/2511.04108</guid>
<content:encoded><![CDATA[
<div> batching, large reasoning models, regularization, inference cost, language models

Summary:
Batch prompting is not only effective for amortizing inference cost in large language models (LLMs) but also serves as a regularization technique for Large Reasoning Models (LRMs). Through a study across 13 benchmarks, it was found that batching improves accuracy while significantly reducing reasoning token usage. The benefits of batching include suppressing overthinking, reducing hedging language, and encouraging more decisive answers during multi-step reasoning. Additionally, batched inference shows collective effects where models generalize patterns from earlier examples to solve harder ones in the same batch. These findings highlight the importance of batching as a powerful inference-time regularizer for more efficient and reliable LLM reasoning. <div>
arXiv:2511.04108v1 Announce Type: new 
Abstract: Recent work has explored batch prompting as a strategy to amortize inference cost in large language models (LLMs). In this paper, we show that batching offers an additional, underappreciated benefit: it regularizes model behavior during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a comprehensive study across 13 diverse benchmarks and observe that batching improves accuracy while substantially reducing reasoning token usage, often by 3x-5x. Through detailed behavioral analysis, we find that batching suppresses overthinking, reduces hedging language (e.g., repetitive self-corrections), and encourages more decisive answers. Surprisingly, we also observe emergent collective effects in batched inference: models often generalize patterns from earlier examples to solve harder ones in the same batch. These findings position batching not just as a throughput optimization, but as a powerful inference-time regularizer for more efficient and reliable LLM reasoning.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2511.04120</link>
<guid>https://arxiv.org/abs/2511.04120</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, mathematical reasoning, adversarial evaluation, question-rewriting framework, Item Response Theory <br />
Summary: <br />
Large language models have shown high performance in mathematical reasoning tasks, but the results may not always reflect genuine reasoning abilities, potentially due to training data leakage or superficial pattern matching. To address this, the authors introduce RIDE, an adversarial question-rewriting framework that utilizes Item Response Theory to generate more challenging and well-posed mathematical questions. By simulating students with 35 LLMs and building a difficulty ranker, RIDE successfully reformulates existing questions across difficulty levels. Applying RIDE to competition-level mathematical benchmarks reveals a significant decrease in LLM performance, indicating limited robustness in mathematical reasoning. This approach provides a rigorous evaluation method that exposes weaknesses in current LLM capabilities and highlights the importance of genuine reasoning skills in solving mathematical problems. <br /> <div>
arXiv:2511.04120v1 Announce Type: new 
Abstract: Large language models (LLMs) achieve high performance on mathematical reasoning, but these results can be inflated by training data leakage or superficial pattern matching rather than genuine reasoning. To this end, an adversarial perturbation-based evaluation is needed to measure true mathematical reasoning ability. Current rule-based perturbation methods often generate ill-posed questions and impede the systematic evaluation of question difficulty and the evolution of benchmarks. To bridge this gap, we propose RIDE, a novel adversarial question-rewriting framework that leverages Item Response Theory (IRT) to rigorously measure question difficulty and to generate intrinsically more challenging, well-posed variations of mathematical problems. We employ 35 LLMs to simulate students and build a difficulty ranker from their responses. This ranker provides a reward signal during reinforcement learning and guides a question-rewriting model to reformulate existing questions across difficulty levels. Applying RIDE to competition-level mathematical benchmarks yields perturbed versions that degrade advanced LLM performance, with experiments showing an average 21.73% drop across 26 models, thereby exposing limited robustness in mathematical reasoning and confirming the validity of our evaluation approach.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese</title>
<link>https://arxiv.org/abs/2511.04139</link>
<guid>https://arxiv.org/abs/2511.04139</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic speech recognition, Cantonese, low-resource, acoustic cues, large audio-language models

Summary: 
CantoASR introduces a collaborative error correction framework for low-resource Cantonese speech recognition. It addresses challenges such as limited annotated data, complex tones, tone sandhi, and accent variation. The framework integrates forced alignment for acoustic feature extraction, a LoRA-finetuned Whisper for improved tone discrimination, and an instruction-tuned Qwen-Audio for prosody-aware correction. By combining acoustic cues with large audio-language models (LALMs), CantoASR achieves significant gains in Character Error Rate (CER) compared to existing models. The results demonstrate the effectiveness of leveraging broader contextual reasoning while incorporating tonal and prosodic cues in ASR. This approach offers a scalable strategy for improving tonal and dialectal ASR in low-resource languages like Cantonese. <br /><br />Summary: <div>
arXiv:2511.04139v1 Announce Type: new 
Abstract: Automatic speech recognition (ASR) is critical for language accessibility, yet low-resource Cantonese remains challenging due to limited annotated data, six lexical tones, tone sandhi, and accent variation. Existing ASR models, such as Whisper, often suffer from high word error rates. Large audio-language models (LALMs), in contrast, can leverage broader contextual reasoning but still require explicit tonal and prosodic acoustic cues. We introduce CantoASR, a collaborative ASR-LALM error correction framework that integrates forced alignment for acoustic feature extraction, a LoRA-finetuned Whisper for improved tone discrimination, and an instruction-tuned Qwen-Audio for prosody-aware correction. Evaluations on spontaneous Cantonese data show substantial CER gains over Whisper-Large-V3. These findings suggest that integrating acoustic cues with LALM reasoning provides a scalable strategy for low-resource tonal and dialectal ASR.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation</title>
<link>https://arxiv.org/abs/2511.04153</link>
<guid>https://arxiv.org/abs/2511.04153</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-SQL, Large Language Models, Multi-agent Pipeline, SQL Generation, Benchmarking

Summary: 

Text-to-SQL systems aim to provide a natural language interface for accessing database information but face challenges with complex SQL generation. This study explores three multi-agent Large Language Model pipelines for SQL generation: Multi-agent discussion, Planner-Coder, and Coder-Aggregator. Benchmarked on open-source models, results show that Multi-agent discussion can improve small model performance, with a notable increase in Execution Accuracy. The LLM Reasoner-Coder pipeline stands out as the most effective, showcasing improvements in SQL generation accuracy. The experiments conducted on the Bird-Bench Mini-Dev set demonstrate the potential of these multi-agent approaches in enhancing SQL generation from natural language instructions. Further details and codes for the pipelines can be found on the provided GitHub repository. <br /><br />Summary: <div>
arXiv:2511.04153v1 Announce Type: new 
Abstract: Text-to-SQL systems provide a natural language interface that can enable even laymen to access information stored in databases. However, existing Large Language Models (LLM) struggle with SQL generation from natural instructions due to large schema sizes and complex reasoning. Prior work often focuses on complex, somewhat impractical pipelines using flagship models, while smaller, efficient models remain overlooked. In this work, we explore three multi-agent LLM pipelines, with systematic performance benchmarking across a range of small to large open-source models: (1) Multi-agent discussion pipeline, where agents iteratively critique and refine SQL queries, and a judge synthesizes the final answer; (2) Planner-Coder pipeline, where a thinking model planner generates stepwise SQL generation plans and a coder synthesizes queries; and (3) Coder-Aggregator pipeline, where multiple coders independently generate SQL queries, and a reasoning agent selects the best query. Experiments on the Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small model performance, with up to 10.6% increase in Execution Accuracy for Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines, the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest score of 56.4%. Codes are available at https://github.com/treeDweller98/bappa-sql.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains</title>
<link>https://arxiv.org/abs/2511.04184</link>
<guid>https://arxiv.org/abs/2511.04184</guid>
<content:encoded><![CDATA[
<div> trustworthiness, LAAC, communication, LLMs, information capture<br />
Summary:<br />
The paper discusses the role of LLMs in communication processes, highlighting the issue of AI-generated content inflation and compression. The proposed LAAC paradigm suggests using LLMs as intelligent intermediaries to facilitate genuine knowledge exchange. It explores the trustworthiness requirements for deploying LAAC in various communication domains, focusing on information capture fidelity, reproducibility, and query response integrity. Preliminary findings from controlled experiments using LAAC's multi-agent architecture reveal trust gaps that need to be addressed before reliable deployment in high-stakes scenarios. <div>
arXiv:2511.04184v1 Announce Type: new 
Abstract: The proliferation of AI-generated content has created an absurd communication theater where senders use LLMs to inflate simple ideas into verbose content, recipients use LLMs to compress them back into summaries, and as a consequence neither party engage with authentic content. LAAC (LLM as a Communicator) proposes a paradigm shift - positioning LLMs as intelligent communication intermediaries that capture the sender's intent through structured dialogue and facilitate genuine knowledge exchange with recipients. Rather than perpetuating cycles of AI-generated inflation and compression, LAAC enables authentic communication across diverse contexts including academic papers, proposals, professional emails, and cross-platform content generation. However, deploying LLMs as trusted communication intermediaries raises critical questions about information fidelity, consistency, and reliability. This position paper systematically evaluates the trustworthiness requirements for LAAC's deployment across multiple communication domains. We investigate three fundamental dimensions: (1) Information Capture Fidelity - accuracy of intent extraction during sender interviews across different communication types, (2) Reproducibility - consistency of structured knowledge across multiple interaction instances, and (3) Query Response Integrity - reliability of recipient-facing responses without hallucination, source conflation, or fabrication. Through controlled experiments spanning multiple LAAC use cases, we assess these trust dimensions using LAAC's multi-agent architecture. Preliminary findings reveal measurable trust gaps that must be addressed before LAAC can be reliably deployed in high-stakes communication scenarios.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Turing Test Reveals Systematic Differences Between Human and AI Language</title>
<link>https://arxiv.org/abs/2511.04195</link>
<guid>https://arxiv.org/abs/2511.04195</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, validation, computational Turing test, calibration, human-likeness <br />
Summary: 
This paper introduces a computational Turing test framework for validating large language models (LLMs) in simulating human behavior. It compares nine LLMs using five calibration strategies across different social media platforms. The findings challenge existing assumptions, showing that even calibrated LLM outputs are distinguishable from human text, especially in affective tone and emotional expression. Instruction-tuned models perform poorly compared to base models, and increasing model size does not improve human-likeness. There is a trade-off between optimizing for human-likeness and semantic fidelity. The study provides a scalable framework for validating and calibrating LLM simulations but warns about the limitations in capturing human communication. <br /><br />Summary: <div>
arXiv:2511.04195v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used in the social sciences to simulate human behavior, based on the assumption that they can generate realistic, human-like text. Yet this assumption remains largely untested. Existing validation efforts rely heavily on human-judgment-based evaluations -- testing whether humans can distinguish AI from human output -- despite evidence that such judgments are blunt and unreliable. As a result, the field lacks robust tools for assessing the realism of LLM-generated text or for calibrating models to real-world data. This paper makes two contributions. First, we introduce a computational Turing test: a validation framework that integrates aggregate metrics (BERT-based detectability and semantic similarity) with interpretable linguistic features (stylistic markers and topical patterns) to assess how closely LLMs approximate human language within a given dataset. Second, we systematically compare nine open-weight LLMs across five calibration strategies -- including fine-tuning, stylistic prompting, and context retrieval -- benchmarking their ability to reproduce user interactions on X (formerly Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the literature. Even after calibration, LLM outputs remain clearly distinguishable from human text, particularly in affective tone and emotional expression. Instruction-tuned models underperform their base counterparts, and scaling up model size does not enhance human-likeness. Crucially, we identify a trade-off: optimizing for human-likeness often comes at the cost of semantic fidelity, and vice versa. These results provide a much-needed scalable framework for validation and calibration in LLM simulations -- and offer a cautionary note about their current limitations in capturing human communication.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal</title>
<link>https://arxiv.org/abs/2511.04205</link>
<guid>https://arxiv.org/abs/2511.04205</guid>
<content:encoded><![CDATA[
<div> exam, language models, public procurement law, judgment, limitations
<br />
Summary:
Language models were tested to see if they could pass Poland's National Appeal Chamber membership exam. While they did well in the knowledge test, they struggled with the practical written portion and differed in judgment from the official committee. Limitations included hallucinations, incorrect legal citations, weak argumentation, and the need for collaboration between legal and technical experts. This indicates that current language models cannot yet replace human judges or examiners in Polish public procurement adjudication. 
Summary: <div>
arXiv:2511.04205v1 Announce Type: new 
Abstract: This study provides an empirical assessment of whether current large language models (LLMs) can pass the official qualifying examination for membership in Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors examine two related ideas: using LLM as actual exam candidates and applying the 'LLM-as-a-judge' approach, in which model-generated answers are automatically evaluated by other models. The paper describes the structure of the exam, which includes a multiple-choice knowledge test on public procurement law and a written judgment, and presents the hybrid information recovery and extraction pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4 Sonnet and Bielik-11B-v2.6) were tested in closed-book and various Retrieval-Augmented Generation settings. The results show that although the models achieved satisfactory scores in the knowledge test, none met the passing threshold in the practical written part, and the evaluations of the 'LLM-as-a-judge' often diverged from the judgments of the official examining committee. The authors highlight key limitations: susceptibility to hallucinations, incorrect citation of legal provisions, weaknesses in logical argumentation, and the need for close collaboration between legal experts and technical teams. The findings indicate that, despite rapid technological progress, current LLMs cannot yet replace human judges or independent examiners in Polish public procurement adjudication.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs</title>
<link>https://arxiv.org/abs/2511.04228</link>
<guid>https://arxiv.org/abs/2511.04228</guid>
<content:encoded><![CDATA[
<div> Keywords: Machine unlearning, privacy, safety, regulatory compliance, REMIND evaluation method

Summary:
Machine unlearning, crucial for privacy, safety, and compliance, aims to remove specific training data without retraining. Existing evaluation methods may overlook residual influence in similar examples. A novel evaluation method, REMIND, detects subtle remaining influence of unlearned data and classifies if data is forgotten effectively. REMIND analyzes loss over input variations, showing unlearned data yielding flatter loss landscapes, while retained or unrelated data display sharper patterns. REMIND, requiring only query-based access, outperforms existing methods and is robust across models, datasets, and inputs. It offers a more sensitive and interpretable measure of unlearning effectiveness, providing a reliable framework to assess unlearning in language models. By offering a new perspective on memorization and unlearning, REMIND enhances trustworthiness and reliability in machine unlearning.<br /><br />Summary: <div>
arXiv:2511.04228v1 Announce Type: new 
Abstract: Machine unlearning aims to remove the influence of specific training data from a model without requiring full retraining. This capability is crucial for ensuring privacy, safety, and regulatory compliance. Therefore, verifying whether a model has truly forgotten target data is essential for maintaining reliability and trustworthiness. However, existing evaluation methods often assess forgetting at the level of individual inputs. This approach may overlook residual influence present in semantically similar examples. Such influence can compromise privacy and lead to indirect information leakage. We propose REMIND (Residual Memorization In Neighborhood Dynamics), a novel evaluation method aiming to detect the subtle remaining influence of unlearned data and classify whether the data has been effectively forgotten. REMIND analyzes the model's loss over small input variations and reveals patterns unnoticed by single-point evaluations. We show that unlearned data yield flatter, less steep loss landscapes, while retained or unrelated data exhibit sharper, more volatile patterns. REMIND requires only query-based access, outperforms existing methods under similar constraints, and demonstrates robustness across different models, datasets, and paraphrased inputs, making it practical for real-world deployment. By providing a more sensitive and interpretable measure of unlearning effectiveness, REMIND provides a reliable framework to assess unlearning in language models. As a result, REMIND offers a novel perspective on memorization and unlearning.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reusing Pre-Training Data at Test Time is a Compute Multiplier</title>
<link>https://arxiv.org/abs/2511.04234</link>
<guid>https://arxiv.org/abs/2511.04234</guid>
<content:encoded><![CDATA[
<div> pre-training, retrieval, generation, dataset value, compute<br />
Summary:<br />
The study investigates the efficiency of current pre-training methods in extracting ideas and knowledge from the data. By using retrieval augmented generation and test-time compute, the researchers quantify the value left behind in pre-training processes and its impact across different scales. Results show significant accuracy gains in various tasks such as MMLU, Math-500, and SimpleQA when leveraging retrieval from standard datasets. The study reveals that retrieval acts as a 5x compute multiplier compared to pre-training alone for MMLU. Additional compute at test time further improves results, with a 10 percentage point enhancement on MMLU for the LLaMA 3.1 8B model. Overall, the findings suggest that current pre-training methods do not fully utilize the information within pre-training datasets, indicating room for advancement in the field. <br /><br />Summary: <div>
arXiv:2511.04234v1 Announce Type: new 
Abstract: Large language models learn from their vast pre-training corpora, gaining the ability to solve an ever increasing variety of tasks; yet although researchers work to improve these datasets, there is little effort to understand how efficient the pre-training apparatus is at extracting ideas and knowledge from the data. In this work, we use retrieval augmented generation along with test-time compute as a way to quantify how much dataset value was left behind by the process of pre-training, and how this changes across scale. We demonstrate that pre-training then retrieving from standard and largely open-sourced datasets results in significant accuracy gains in MMLU, Math-500, and SimpleQA, which persist through decontamination. For MMLU we observe that retrieval acts as a ~5x compute multiplier versus pre-training alone. We show that these results can be further improved by leveraging additional compute at test time to parse the retrieved context, demonstrating a 10 percentage point improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results suggest that today's pre-training methods do not make full use of the information in existing pre-training datasets, leaving significant room for progress.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models</title>
<link>https://arxiv.org/abs/2511.04248</link>
<guid>https://arxiv.org/abs/2511.04248</guid>
<content:encoded><![CDATA[
<div> topic modeling, text analysis, graph-based approach, topic labeling, interpretability

Summary: 

- The article proposes a graph-based approach for topic labeling in text analysis to enhance interpretability without intensive computational requirements.
- Topic modeling is used to automatically discover topics in text data, but the resulting word distributions lack clear interpretability.
- The graph-based approach enriches topic words with semantically related terms and explores their relationships to derive meaningful labels.
- Comparative study with benchmarks, including ChatGPT-3.5, shows that the proposed method outperforms traditional benchmarks in terms of BERTScore and cosine similarity while remaining computationally efficient.
- Future directions for topic labeling research are discussed, emphasizing the potential for enhancing interpretability and automation. 

Summary: <div>
arXiv:2511.04248v1 Announce Type: new 
Abstract: Extracting topics from text has become an essential task, especially with the rapid growth of unstructured textual data. Most existing works rely on highly computational methods to address this challenge. In this paper, we argue that probabilistic and statistical approaches, such as topic modeling (TM), can offer effective alternatives that require fewer computational resources. TM is a statistical method that automatically discovers topics in large collections of unlabeled text; however, it produces topics as distributions of representative words, which often lack clear interpretability. Our objective is to perform topic labeling by assigning meaningful labels to these sets of words. To achieve this without relying on computationally expensive models, we propose a graph-based approach that not only enriches topic words with semantically related terms but also explores the relationships among them. By analyzing these connections within the graph, we derive suitable labels that accurately capture each topic's meaning. We present a comparative study between our proposed method and several benchmarks, including ChatGPT-3.5, across two different datasets. Our method achieved consistently better results than traditional benchmarks in terms of BERTScore and cosine similarity and produced results comparable to ChatGPT-3.5, while remaining computationally efficient. Finally, we discuss future directions for topic labeling and highlight potential research avenues for enhancing interpretability and automation.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSPO: Subsentence-level Policy Optimization</title>
<link>https://arxiv.org/abs/2511.04256</link>
<guid>https://arxiv.org/abs/2511.04256</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Reinforcement Learning, Verifiable Reward, GRPO, GSPO <br />
<br />
Summary: This paper introduces SSPO, a new algorithm for post-training Large Language Models (LLMs) using Reinforcement Learning from Verifiable Reward (RLVR). SSPO addresses the shortcomings of existing algorithms such as GRPO and GSPO by applying a sentence-level importance ratio, striking a balance between the two approaches. By avoiding training collapse, high variance, and low data utilization, SSPO outperforms GRPO and GSPO, achieving state-of-the-art performance on three datasets. Additionally, the integration of sentence entropy in the PPO-CLIP algorithm allows for steady adjustment of clipping bounds, promoting exploration of high-entropy tokens while narrowing the clipping range for low-entropy tokens. These advancements highlight SSPO's effectiveness in leveraging generated data and improving reasoning skills in LLMs. <br /> <div>
arXiv:2511.04256v1 Announce Type: new 
Abstract: As a significant part of post-training of the Large Language Models (LLMs), Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs' reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative Policy Optimization) and GSPO (Group Sequence Policy Optimization), are observed to suffer from unstable policy updates and low usage of sampling data, respectively. The importance ratio of GRPO is calculated at the token level, which focuses more on optimizing a single token. This will be easily affected by outliers, leading to model training collapse. GSPO proposed the calculation of the response level importance ratio, which solves the problem of high variance and training noise accumulation in the calculation of the GRPO importance ratio. However, since all the response tokens share a common importance ratio, extreme values can easily raise or lower the overall mean, leading to the entire response being mistakenly discarded, resulting in a decrease in the utilization of sampled data. This paper introduces SSPO, which applies sentence-level importance ratio, taking the balance between GRPO and GSPO. SSPO not only avoids training collapse and high variance, but also prevents the whole response tokens from being abandoned by the clipping mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily adjust the clipping bounds, encouraging high-entropy tokens to explore and narrow the clipping range of low-entropy tokens. In particular, SSPO achieves an average score of 46.57 across five datasets, surpassing GRPO (43.01) and GSPO (44.42), and wins state-of-the-art performance on three datasets. These results highlight SSPO's effectiveness in leveraging generated data by taking the essence of GSPO but rejecting its shortcomings.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.04406</link>
<guid>https://arxiv.org/abs/2511.04406</guid>
<content:encoded><![CDATA[
<div> Keywords: data selection, machine translation, fine-tuning, learnability score, batch selection<br />
Summary:<br />
This paper introduces a data selection methodology to enhance the effectiveness of fine-tuning machine translation systems. It utilizes a learner model and a pre-trained reference model to evaluate the utility of data points through a learnability score. By selecting the most relevant examples, the method improves training efficiency. The batch selection strategy considers interdependencies among data points, optimizing the training process. Experimental results on various language pairs with an mBART model fine-tuned on the CCMatrix dataset show up to a fivefold improvement in data efficiency compared to a baseline. The approach also enhances computational efficiency by requiring fewer training data points, especially when using cached embeddings. The method leads to improved generalization and superior translation performance compared to random selection, showcasing its effectiveness in improving machine translation models. <br /><br />Summary: <div>
arXiv:2511.04406v1 Announce Type: new 
Abstract: Data quality and its effective selection are fundamental to improving the performance of machine translation models, serving as cornerstones for achieving robust and reliable translation systems. This paper presents a data selection methodology specifically designed for fine-tuning machine translation systems, which leverages the synergy between a learner model and a pre-trained reference model to enhance overall training effectiveness. By defining a learnability score, our approach systematically evaluates the utility of data points for training, ensuring that only the most relevant and impactful examples contribute to the fine-tuning process. Furthermore, our method employs a batch selection strategy which considers interdependencies among data points, optimizing the efficiency of the training process while maintaining a focus on data relevance. Experiments on English to Persian and several other language pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that our method can achieve up to a fivefold improvement in data efficiency compared to an iid baseline. Experimental results indicate that our approach improves computational efficiency by 24 when utilizing cached embeddings, as it requires fewer training data points. Additionally, it enhances generalization, resulting in superior translation performance compared to random selection method.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs</title>
<link>https://arxiv.org/abs/2511.04432</link>
<guid>https://arxiv.org/abs/2511.04432</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, temporal reasoning, Norwegian, English, trivia questions

Summary:
The study explores the temporal reasoning capabilities of Language Models (LLMs) by prompting them to answer trivia questions from a Norwegian book dating back to 1940. The questions were posed in both English and Norwegian, with English prompts consistently yielding better results than Norwegian prompts. Additionally, using larger LLMs led to improved performance. The study tested various LLM model families, including DeepSeek-R1, Gemma3, Qwen3, and Llama3.1, as well as a specifically designed LLM for Norwegian. Correct answers were assessed through LLM-based grading, with occasional verification by a native speaker. The unexpected finding of better performance with English prompts highlights the complexity of language processing in LLMs and the impact of language choice on model performance. This research sheds light on the capabilities and limitations of LLMs in temporal reasoning tasks across different languages. 

<br /><br />Summary: 
- The study examines LLMs' temporal reasoning abilities through answering 1940 trivia questions in Norwegian.
- English prompts yielded superior results compared to Norwegian prompts, emphasizing language choice's impact on LLM performance.
- Larger LLMs demonstrated enhanced performance in temporal reasoning tasks.
- Various LLM model families were tested, along with a specialized Norwegian LLM model.
- Correct answers were evaluated using LLM-based grading with native speaker verification, showcasing the complexity of language processing in LLMs. <div>
arXiv:2511.04432v1 Announce Type: new 
Abstract: In this study, we experiment with the ability of LLMs to do temporal reasoning. Using a Norwegian book from 1940 containing trivia questions, we prompt the LLMs to answer the questions as if it were 1940. We also pose the questions in both English and Norwegian. Correct answers are often presented as sentences, and grading is done by means of LLM-as-judge, with sampled checks by a native speaker. Prompting in English consistently gave better results than in Norwegian, an unexpected result. In contrast, using larger LLMs improved results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families, and also the largest available LLM especially crafted for Norwegian.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Textual Time Series Depression Detection</title>
<link>https://arxiv.org/abs/2511.04476</link>
<guid>https://arxiv.org/abs/2511.04476</guid>
<content:encoded><![CDATA[
<div> Probabilistic Textual Time Series Depression Detection, PHQ-8 scores, clinical interviews, uncertainty modeling, LSTM, self-attention

Summary:
PTTSD is a framework for predicting depression severity from clinical interviews, incorporating uncertainty estimates and temporal modeling. It includes sequence-to-sequence and sequence-to-one variants with bidirectional LSTMs, self-attention, and residual connections. Trained via negative log-likelihood, it achieves state-of-the-art performance on E-DAIC and DAIC-WOZ datasets. The framework produces well-calibrated prediction intervals and demonstrates the value of attention and probabilistic modeling in depression detection. Comparison with MentalBERT shows its generality, while a calibration analysis and case studies underline its interpretability and clinical relevance. PTTSD provides accurate and interpretable predictions of depression severity, essential for clinical decision support. <br /><br />Summary: <div>
arXiv:2511.04476v1 Announce Type: new 
Abstract: Accurate and interpretable predictions of depression severity are essential for clinical decision support, yet existing models often lack uncertainty estimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time Series Depression Detection framework that predicts PHQ-8 scores from utterance-level clinical interviews while modeling uncertainty over time. PTTSD includes sequence-to-sequence and sequence-to-one variants, both combining bidirectional LSTMs, self-attention, and residual connections with Gaussian or Student-t output heads trained via negative log-likelihood. Evaluated on E-DAIC and DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only systems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated prediction intervals. Ablations confirm the value of attention and probabilistic modeling, while comparisons with MentalBERT establish generality. A three-part calibration analysis and qualitative case studies further highlight the interpretability and clinical relevance of uncertainty-aware forecasting.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai</title>
<link>https://arxiv.org/abs/2511.04479</link>
<guid>https://arxiv.org/abs/2511.04479</guid>
<content:encoded><![CDATA[
<div> benchmark, vision-language models, Thai text, visual understanding, document structure<br />
Summary:<br />
ThaiOCRBench is introduced as the first benchmark for evaluating vision-language models on Thai text-rich visual understanding tasks. The dataset includes 2,808 samples across 13 task categories. State-of-the-art VLMs are evaluated in a zero-shot setting, with proprietary models like Gemini 2.5 Pro outperforming open-source systems. Open-source models struggle with fine-grained text recognition and handwritten content extraction. Error analysis reveals challenges such as language bias, structural mismatch, and hallucinated content, providing insights for improving Thai-language document understanding. ThaiOCRBench offers a standardized framework for assessing VLMs in low-resource, script-complex settings.<br /><br />Summary: <div>
arXiv:2511.04479v1 Announce Type: new 
Abstract: We present ThaiOCRBench, the first comprehensive benchmark for evaluating vision-language models (VLMs) on Thai text-rich visual understanding tasks. Despite recent progress in multimodal modeling, existing benchmarks predominantly focus on high-resource languages, leaving Thai underrepresented, especially in tasks requiring document structure understanding. ThaiOCRBench addresses this gap by offering a diverse, human-annotated dataset comprising 2,808 samples across 13 task categories. We evaluate a wide range of state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and open-source systems. Results show a significant performance gap, with proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source counterparts. Notably, fine-grained text recognition and handwritten content extraction exhibit the steepest performance drops among open-source models. Through detailed error analysis, we identify key challenges such as language bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a standardized framework for assessing VLMs in low-resource, script-complex settings, and provides actionable insights for improving Thai-language document understanding.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables</title>
<link>https://arxiv.org/abs/2511.04491</link>
<guid>https://arxiv.org/abs/2511.04491</guid>
<content:encoded><![CDATA[
<div> benchmark, tabular reasoning, large language models, real-world data, multi-hop reasoning

Summary:
RUST-BENCH is a new benchmark that aims to evaluate Large Language Models (LLMs) on the complexity of real-world tabular data. Unlike existing benchmarks that focus on small, uniform tables, RUST-BENCH includes 7966 questions from 2031 real-world tables in two domains: RB-Science and RB-Sports. It tests models on scale, heterogeneity, domain specificity, and reasoning complexity. Findings suggest that LLMs struggle with heterogeneous schemas and complex multi-hop inference, highlighting weaknesses in current architectures. The benchmark serves as a challenging testbed for advancing tabular reasoning research, prompting the need for new strategies to improve LLM performance on real-world data. 

<br /><br />Summary: <div>
arXiv:2511.04491v1 Announce Type: new 
Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform tables, underrepresenting the complexity of real-world data and giving an incomplete view of Large Language Models' (LLMs) reasoning abilities. Real tables are long, heterogeneous, and domain-specific, mixing structured fields with free text and requiring multi-hop reasoning across thousands of tokens. To address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from 2031 real-world tables spanning two domains: i) RB-Science (NSF grant records) and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates LLMs jointly across scale, heterogeneity, domain specificity, and reasoning complexity. Experiments with open-source and proprietary models show that LLMs struggle with heterogeneous schemas and complex multi-hop inference, revealing persistent weaknesses in current architectures and prompting strategies. RUST-BENCH establishes a challenging new testbed for advancing tabular reasoning research.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation</title>
<link>https://arxiv.org/abs/2511.04495</link>
<guid>https://arxiv.org/abs/2511.04495</guid>
<content:encoded><![CDATA[
<div> LLM-prompting-based, text simplification, performance, CEFR levels, multi-round simplification <br />
Summary: 
The OUNLP system was developed for readability-controlled text simplification for the TSAR-2025 Shared Task. The performance of text simplification was found to be influenced by the gap between source and target CEFR levels. Two multi-round simplification methods, MRS-Rule and MRS-Joint, were proposed and generated using GPT-4o. The system ranked 7th out of 20 teams initially, with MRS-Joint showing improved performance in subsequent evaluations. Taking LLM-simplified candidates as starting points was found to enhance multi-round simplification effectiveness. <div>
arXiv:2511.04495v1 Announce Type: new 
Abstract: This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task (Alva-Manchego et al., 2025), designed for readability-controlled text simplification using LLM-prompting-based generation. Based on the analysis of prompt-based text simplification methods, we discovered an interesting finding that text simplification performance is highly related to the gap between the source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by this finding, we propose two multi-round simplification methods and generate them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams. Later improvements with MRS-Joint show that taking the LLM simplified candidates as the starting point could further boost the multi-round simplification performance.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering</title>
<link>https://arxiv.org/abs/2511.04499</link>
<guid>https://arxiv.org/abs/2511.04499</guid>
<content:encoded><![CDATA[
<div> Personality, Large Language Models, Trait analysis, Big Five Inventory-2, Temperature adjustments

Summary:
- The study evaluates six Large Language Models (LLMs) using the Big Five Inventory-2 framework to assess personality-like traits under different sampling temperatures.
- Significant differences were observed across four of the five personality dimensions, particularly in Neuroticism and Extraversion, showing susceptibility to temperature adjustments.
- Hierarchical clustering revealed distinct model clusters, indicating that architectural features may influence stable trait profiles in LLMs.
- The findings offer insights into how personality-like patterns emerge in LLMs and provide a new perspective on model tuning, selection, and ethical governance of AI systems.
- The data and code for this analysis are available for further exploration at the provided link.

<br /><br />Summary: <div>
arXiv:2511.04499v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) become integral to human-centered applications, understanding their personality-like behaviors is increasingly important for responsible development and deployment. This paper systematically evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to assess trait expressions under varying sampling temperatures. We find significant differences across four of the five personality dimensions, with Neuroticism and Extraversion susceptible to temperature adjustments. Further, hierarchical clustering reveals distinct model clusters, suggesting that architectural features may predispose certain models toward stable trait profiles. Taken together, these results offer new insights into the emergence of personality-like patterns in LLMs and provide a new perspective on model tuning, selection, and the ethical governance of AI systems. We share the data and code for this analysis here: https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG</title>
<link>https://arxiv.org/abs/2511.04502</link>
<guid>https://arxiv.org/abs/2511.04502</guid>
<content:encoded><![CDATA[
<div> Framework, Evaluation, Retrieval-Augmented Generation (RAG), Large Language Models (LLMs), Domain-specific

Summary:
RAGalyst is introduced as an automated, human-aligned agentic framework for evaluating domain-specific Retrieval-Augmented Generation (RAG) systems. The framework generates synthetic question-answering datasets and refines LLM-as-a-Judge metrics for improved alignment with human judgment. Evaluation across military operations, cybersecurity, and bridge engineering domains reveals context-dependent performance, emphasizing the importance of domain-specific considerations. No universally optimal model or configuration is identified, highlighting the need for systematic evaluation frameworks like RAGalyst to inform design decisions for reliable RAG systems. An analysis of common low Answer Correctness reasons in RAG underlines the framework's value in uncovering domain-specific trade-offs. RAGalyst is available on Github. 

<br /><br />Summary: <div>
arXiv:2511.04502v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) is a critical technique for grounding Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in specialized, safety-critical domains remains a significant challenge. Existing evaluation frameworks often rely on heuristic-based metrics that fail to capture domain-specific nuances and other works utilize LLM-as-a-Judge approaches that lack validated alignment with human judgment. This paper introduces RAGalyst, an automated, human-aligned agentic framework designed for the rigorous evaluation of domain-specific RAG systems. RAGalyst features an agentic pipeline that generates high-quality, synthetic question-answering (QA) datasets from source documents, incorporating an agentic filtering step to ensure data fidelity. The framework refines two key LLM-as-a-Judge metrics-Answer Correctness and Answerability-using prompt optimization to achieve a strong correlation with human annotations. Applying this framework to evaluate various RAG components across three distinct domains (military operations, cybersecurity, and bridge engineering), we find that performance is highly context-dependent. No single embedding model, LLM, or hyperparameter configuration proves universally optimal. Additionally, we provide an analysis on the most common low Answer Correctness reasons in RAG. These findings highlight the necessity of a systematic evaluation framework like RAGalyst, which empowers practitioners to uncover domain-specific trade-offs and make informed design choices for building reliable and effective RAG systems. RAGalyst is available on our Github.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways</title>
<link>https://arxiv.org/abs/2511.04506</link>
<guid>https://arxiv.org/abs/2511.04506</guid>
<content:encoded><![CDATA[
<div> Keywords: Radiology reports, Uncertainty, Hedging phrases, LLM-based ranking, Diagnostic pathways

Summary: 
- Radiology reports play a crucial role in clinical decision-making and can benefit from automated analysis when structured into machine-readable formats.
- Uncertainty in radiology reports is categorized into explicit uncertainty, conveyed through hedging phrases, and implicit uncertainty, where parts of reasoning are omitted by radiologists.
- An expert-validated, LLM-based reference ranking is used to quantify explicit uncertainty by mapping each finding to a probability value based on common hedging phrases.
- Implicit uncertainty is addressed through an expansion framework that adds characteristic sub-findings derived from diagnostic pathways for common diagnoses.
- Lunguage++, an expanded, uncertainty-aware version of the Lunguage benchmark, is introduced as a resource for uncertainty-aware image classification, faithful diagnostic reasoning, and exploration of the clinical impact of diagnostic uncertainty. 

<br /><br />Summary: <div>
arXiv:2511.04506v1 Announce Type: new 
Abstract: Radiology reports are invaluable for clinical decision-making and hold great potential for automated analysis when structured into machine-readable formats. These reports often contain uncertainty, which we categorize into two distinct types: (i) Explicit uncertainty reflects doubt about the presence or absence of findings, conveyed through hedging phrases. These vary in meaning depending on the context, making rule-based systems insufficient to quantify the level of uncertainty for specific findings; (ii) Implicit uncertainty arises when radiologists omit parts of their reasoning, recording only key findings or diagnoses. Here, it is often unclear whether omitted findings are truly absent or simply unmentioned for brevity. We address these challenges with a two-part framework. We quantify explicit uncertainty by creating an expert-validated, LLM-based reference ranking of common hedging phrases, and mapping each finding to a probability value based on this reference. In addition, we model implicit uncertainty through an expansion framework that systematically adds characteristic sub-findings derived from expert-defined diagnostic pathways for 14 common diagnoses. Using these methods, we release Lunguage++, an expanded, uncertainty-aware version of the Lunguage benchmark of fine-grained structured radiology reports. This enriched resource enables uncertainty-aware image classification, faithful diagnostic reasoning, and new investigations into the clinical impact of diagnostic uncertainty.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics</title>
<link>https://arxiv.org/abs/2511.04527</link>
<guid>https://arxiv.org/abs/2511.04527</guid>
<content:encoded><![CDATA[
<div> Keywords: language model, uncertainty quantification, hidden activations, chain-of-thought reasoning, intervention

Summary:
In this paper, the authors investigate how language models navigate through different reasoning paths during text generation and the associated uncertainty. By analyzing hidden activations, they explore the model's ability to represent alternate paths and quantify uncertainty. The experiments reveal a strong correlation between the model's uncertainty at different points and its responsiveness to activation interventions. This implies that effective interventions can steer the model when diverse paths are available, indicating a lack of commitment to a specific outcome. The study also shows that hidden activations can predict the model's future outcome distribution, indicating the representation of potential paths by the model. These findings highlight the significance of understanding how language models reason and make decisions, offering insights into enhancing their performance and interpretability. 

<br /><br />Summary: <div>
arXiv:2511.04527v1 Announce Type: new 
Abstract: When a language model generates text, the selection of individual tokens might lead it down very different reasoning paths, making uncertainty difficult to quantify. In this work, we consider whether reasoning language models represent the alternate paths that they could take during generation. To test this hypothesis, we use hidden activations to control and predict a language model's uncertainty during chain-of-thought reasoning. In our experiments, we find a clear correlation between how uncertain a model is at different tokens, and how easily the model can be steered by controlling its activations. This suggests that activation interventions are most effective when there are alternate paths available to the model -- in other words, when it has not yet committed to a particular final answer. We also find that hidden activations can predict a model's future outcome distribution, demonstrating that models implicitly represent the space of possible paths.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection</title>
<link>https://arxiv.org/abs/2511.04528</link>
<guid>https://arxiv.org/abs/2511.04528</guid>
<content:encoded><![CDATA[
<div> Keywords: IntelliProof, argumentative essays, LLMs, argumentation graph, user experience

Summary:<br /><br />IntelliProof is a new interactive system designed for analyzing argumentative essays using LLMs. It organizes essays into argumentation graphs, where claims are nodes, evidence is attached as properties, and edges indicate supporting or attacking relations. Unlike other automated scoring systems, IntelliProof focuses on user experience by classifying and scoring relations with LLMs and offering visual representations for better comprehension. The system also provides justifications for classifications and generates quantitative measures of essay coherence. It allows for quick evaluation of argument quality while incorporating human oversight. Additionally, IntelliProof includes tools to enhance the understanding of argumentative essays and their graphs in natural language, bridging the gap between structural semantics and user comprehension. A live demo and the system are accessible at https://intelliproof.vercel.app. <div>
arXiv:2511.04528v1 Announce Type: new 
Abstract: We present IntelliProof, an interactive system for analyzing argumentative essays through LLMs. IntelliProof structures an essay as an argumentation graph, where claims are represented as nodes, supporting evidence is attached as node properties, and edges encode supporting or attacking relations. Unlike existing automated essay scoring systems, IntelliProof emphasizes the user experience: each relation is initially classified and scored by an LLM, then visualized for enhanced understanding. The system provides justifications for classifications and produces quantitative measures for essay coherence. It enables rapid exploration of argumentative quality while retaining human oversight. In addition, IntelliProof provides a set of tools for a better understanding of an argumentative essay and its corresponding graph in natural language, bridging the gap between the structural semantics of argumentative essays and the user's understanding of a given text. A live demo and the system are available here to try: \textbf{https://intelliproof.vercel.app}
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting</title>
<link>https://arxiv.org/abs/2511.04538</link>
<guid>https://arxiv.org/abs/2511.04538</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, code security, vulnerabilities, Prompt Exposure, Model Exposure

Summary:
Large Language Models (LLMs) play a crucial role in software development but also introduce new vulnerabilities in code. Despite efforts to improve code security benchmarks, vulnerabilities still exist in widely used coding LLMs. This study reveals that even the latest open-weight models are susceptible to vulnerabilities, indicating a trade-off between safety and functionality hindering effective patching. A new severity metric incorporating Prompt Exposure (PE) is introduced to assess the risk of LLM-generated vulnerabilities, leading to the Model Exposure (ME) score. ME reflects the severity and prevalence of vulnerabilities generated by a model, aiming to prioritize mitigation efforts for the most serious and common vulnerabilities. Addressing these vulnerabilities is crucial to enhance the overall cybersecurity landscape in software development.<br /><br />Summary: <div>
arXiv:2511.04538v1 Announce Type: new 
Abstract: As the role of Large Language Models (LLM)-based coding assistants in software development becomes more critical, so does the role of the bugs they generate in the overall cybersecurity landscape. While a number of LLM code security benchmarks have been proposed alongside approaches to improve the security of generated code, it remains unclear to what extent they have impacted widely used coding LLMs. Here, we show that even the latest open-weight models are vulnerable in the earliest reported vulnerability scenarios in a realistic use setting, suggesting that the safety-functionality trade-off has until now prevented effective patching of vulnerabilities. To help address this issue, we introduce a new severity metric that reflects the risk posed by an LLM-generated vulnerability, accounting for vulnerability severity, generation chance, and the formulation of the prompt that induces vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation of the most serious and prevalent vulnerabilities, we use PE to define the Model Exposure (ME) score, which indicates the severity and prevalence of vulnerabilities a model generates.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering</title>
<link>https://arxiv.org/abs/2511.04560</link>
<guid>https://arxiv.org/abs/2511.04560</guid>
<content:encoded><![CDATA[
<div> Keywords: Bangla, biomedical, Question Answering, Retrieval-Augmented Generation, Artificial Intelligence

Summary:
The paper introduces BanglaMedQA and BanglaMMedBench datasets for evaluating biomedical Question Answering systems in Bangla. Various Retrieval-Augmented Generation (RAG) strategies are applied and benchmarked, including Traditional, Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG. A key innovation involves integrating a Bangla medical textbook corpus through OCR and implementing an Agentic RAG pipeline that dynamically selects between retrieval and reasoning strategies. Experimental results show that the Agentic RAG achieved the highest accuracy of 89.54%, outperforming other configurations. This approach combines textbook-based and web retrieval with generative reasoning to improve factual accuracy and rationale quality. The findings suggest the potential of RAG-based methods to enhance the reliability and accessibility of Bangla medical QA, paving the way for future research in multilingual medical AI.<br /><br />Summary: <div>
arXiv:2511.04560v1 Announce Type: new 
Abstract: Developing accurate biomedical Question Answering (QA) systems in low-resource languages remains a major challenge, limiting equitable access to reliable medical knowledge. This paper introduces BanglaMedQA and BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical artificial intelligence (AI). The study applies and benchmarks several Retrieval-Augmented Generation (RAG) strategies, including Traditional, Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining textbook-based and web retrieval with generative reasoning to improve factual accuracy. A key novelty lies in integrating a Bangla medical textbook corpus through Optical Character Recognition (OCR) and implementing an Agentic RAG pipeline that dynamically selects between retrieval and reasoning strategies. Experimental results show that the Agentic RAG achieved the highest accuracy 89.54% with openai/gpt-oss-120b, outperforming other configurations and demonstrating superior rationale quality. These findings highlight the potential of RAG-based methods to enhance the reliability and accessibility of Bangla medical QA, establishing a foundation for future research in multilingual medical artificial intelligence.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection</title>
<link>https://arxiv.org/abs/2511.04643</link>
<guid>https://arxiv.org/abs/2511.04643</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, fact verification, DeReC, dense retrieval, classification

Summary: 
DeReC is introduced as a lightweight framework for fact verification tasks, utilizing text embeddings to replace autoregressive Large Language Models (LLMs) efficiently. The system combines dense retrieval with specialized classification, outperforming explanation-generating LLMs in accuracy while being more time-efficient. DeReC significantly reduces runtime on datasets like RAWFC and LIAR-RAW, showcasing its effectiveness across varying data sizes. It achieves an F1 score of 65.58% on RAWFC, surpassing the state-of-the-art L-Defense method's performance. The study shows that retrieval-based systems, when carefully engineered, can match or even exceed LLM performance in specialized tasks while being more practical for real-world deployment. <br /><br />Summary: <div>
arXiv:2511.04643v1 Announce Type: new 
Abstract: The proliferation of misinformation necessitates robust yet computationally efficient fact verification systems. While current state-of-the-art approaches leverage Large Language Models (LLMs) for generating explanatory rationales, these methods face significant computational barriers and hallucination risks in real-world deployments. We present DeReC (Dense Retrieval Classification), a lightweight framework that demonstrates how general-purpose text embeddings can effectively replace autoregressive LLM-based approaches in fact verification tasks. By combining dense retrieval with specialized classification, our system achieves better accuracy while being significantly more efficient. DeReC outperforms explanation-generating LLMs in efficiency, reducing runtime by 95% on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92% on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds), showcasing its effectiveness across varying dataset sizes. On the RAWFC dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art method L-Defense (61.20%). Our results demonstrate that carefully engineered retrieval-based systems can match or exceed LLM performance in specialized tasks while being significantly more practical for real-world deployment.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2511.04654</link>
<guid>https://arxiv.org/abs/2511.04654</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought prompting, large language models, LEASH, rationale generation, model-agnostic<br />
Summary:<br />
The article introduces LEASH, a Logit-Entropy Adaptive Stopping Heuristic that improves complex reasoning in language models without the need for additional training. LEASH monitors token-level entropy and top-logit margin improvement to determine when to halt rationale generation, reducing token usage by 30-35% and latency by 27%. While there is a slight accuracy drop compared to Chain-of-Thought (CoT) prompting, LEASH offers a more efficient alternative. It is model-agnostic, requiring no extra training, making it a simple and effective solution for enhancing reasoning capabilities in language models. <div>
arXiv:2511.04654v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) prompting is a key technique for enabling complex reasoning in large language models. However, generating full, fixed-length rationales is computationally wasteful, inflating both token usage and latency. We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free decoding algorithm that adaptively halts rationale generation. LEASH monitors two intrinsic signals: the slope of token-level entropy and the improvement in the top-logit margin. It terminates the generation once both signals plateau, indicating the model has reached a stable reasoning state. Across four instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces average token generation by 30--35% and latency by 27%, while incurring a 10 p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no additional training or supervision, offering a simple and efficient alternative to CoT decoding.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MimiTalk: Revolutionizing Qualitative Research with Dual-Agent AI</title>
<link>https://arxiv.org/abs/2511.03731</link>
<guid>https://arxiv.org/abs/2511.03731</guid>
<content:encoded><![CDATA[
<div> Constitutional AI, MimiTalk, conversational data collection, social science research, dual-agent framework <br />
<br />
Summary: The article introduces MimiTalk, a dual-agent constitutional AI framework developed for conversational data collection in social science research. The framework consists of a supervisor model for oversight and a conversational model for generating questions. Three studies were conducted to evaluate the framework's usability and performance. Results show that MimiTalk reduces interview anxiety, maintains conversational coherence, and outperforms human interviews in information richness, coherence, and stability. AI interviews excel in extracting technical insights and candid views on sensitive topics, while human interviews capture cultural and emotional nuances better. The findings suggest that dual-agent constitutional AI facilitates effective human-AI collaboration, enabling replicable, scalable, and quality-controlled qualitative research. <br /><br /> <div>
arXiv:2511.03731v1 Announce Type: cross 
Abstract: We present MimiTalk, a dual-agent constitutional AI framework designed for scalable and ethical conversational data collection in social science research. The framework integrates a supervisor model for strategic oversight and a conversational model for question generation. We conducted three studies: Study 1 evaluated usability with 20 participants; Study 2 compared 121 AI interviews to 1,271 human interviews from the MediaSum dataset using NLP metrics and propensity score matching; Study 3 involved 10 interdisciplinary researchers conducting both human and AI interviews, followed by blind thematic analysis. Results across studies indicate that MimiTalk reduces interview anxiety, maintains conversational coherence, and outperforms human interviews in information richness, coherence, and stability. AI interviews elicit technical insights and candid views on sensitive topics, while human interviews better capture cultural and emotional nuances. These findings suggest that dual-agent constitutional AI supports effective human-AI collaboration, enabling replicable, scalable and quality-controlled qualitative research.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis</title>
<link>https://arxiv.org/abs/2511.03825</link>
<guid>https://arxiv.org/abs/2511.03825</guid>
<content:encoded><![CDATA[
<div> Tokenization, Assembly code, Natural Language Processing, NLP, Pretrained models <br />
Summary:<br /> 
- Tokenization in assembly code analysis affects vocabulary size, semantic coverage, and downstream task performance.
- This study evaluates NLP tokenization models and parameter choices for assembly code, including vocabulary size.
- Customized preprocessing options and pre-tokenization rules tailored to assembly code characteristics are explored.
- The impact of these choices on downstream tasks like function signature prediction in binary code analysis is assessed.
- Comparisons are made between tokenization models based on efficiency, vocabulary compression, and representational fidelity for assembly code. Preliminary findings suggest that tokenization choice significantly influences downstream performance. Intrinsic metrics provide some predictability of extrinsic evaluation outcomes, revealing trade-offs between tokenizer properties and utility in assembly code tasks. Insights from this study can help optimize tokenization models for low-level code analysis using NLM-based workflows.<br /> 
Summary: <div>
arXiv:2511.03825v1 Announce Type: cross 
Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic characteristics like vocabulary size, semantic coverage, and extrinsic performance in downstream tasks. Despite its significance, tokenization in the context of assembly code remains an underexplored area. This study aims to address this gap by evaluating the intrinsic properties of Natural Language Processing (NLP) tokenization models and parameter choices, such as vocabulary size. We explore preprocessing customization options and pre-tokenization rules tailored to the unique characteristics of assembly code. Additionally, we assess their impact on downstream tasks like function signature prediction -- a critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models, systematically analyzing their efficiency in encoding assembly instructions and capturing semantic nuances. Through intrinsic evaluations, we compare tokenizers based on tokenization efficiency, vocabulary compression, and representational fidelity for assembly code. Using state-of-the-art pre-trained models such as the decoder-only Large Language Model (LLM) Llama 3.2, the encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate the effectiveness of these tokenizers across multiple performance metrics. Preliminary findings indicate that tokenizer choice significantly influences downstream performance, with intrinsic metrics providing partial but incomplete predictability of extrinsic evaluation outcomes. These results reveal complex trade-offs between intrinsic tokenizer properties and their utility in practical assembly code tasks. Ultimately, this study provides valuable insights into optimizing tokenization models for low-level code analysis, contributing to the robustness and scalability of Natural Language Model (NLM)-based binary analysis workflows.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods</title>
<link>https://arxiv.org/abs/2511.03939</link>
<guid>https://arxiv.org/abs/2511.03939</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Human Feedback, Large Language Models, Multi-modal alignment, Cultural fairness<br />
Summary: This survey paper explores the advancements in Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). It discusses critical gaps in multi-modal alignment, cultural fairness, and low-latency optimization in AI systems. The paper reviews foundational algorithms such as PPO, DPO, and GRPO before delving into the latest innovations in the field. By providing a comparative synthesis of these techniques and highlighting open challenges, the paper serves as a roadmap for researchers aiming to build more robust, efficient, and equitable AI systems. The study emphasizes the importance of incorporating diverse feedback sources and ensuring fairness and efficiency in AI alignment processes.<br /><br />Summary: This survey paper delves into the advancements in Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models. It discusses gaps in multi-modal alignment, cultural fairness, and low-latency optimization in AI systems. The paper reviews foundational algorithms and presents the latest innovations in the field, serving as a roadmap for building more robust and equitable AI systems. <div>
arXiv:2511.03939v1 Announce Type: cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is the standard for aligning Large Language Models (LLMs), yet recent progress has moved beyond canonical text-based methods. This survey synthesizes the new frontier of alignment research by addressing critical gaps in multi-modal alignment, cultural fairness, and low-latency optimization. To systematically explore these domains, we first review foundational algo- rithms, including PPO, DPO, and GRPO, before presenting a detailed analysis of the latest innovations. By providing a comparative synthesis of these techniques and outlining open challenges, this work serves as an essential roadmap for researchers building more robust, efficient, and equitable AI systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation</title>
<link>https://arxiv.org/abs/2511.03942</link>
<guid>https://arxiv.org/abs/2511.03942</guid>
<content:encoded><![CDATA[
<div> Keywords: MIDI-LLM, multitrack music, text prompts, vLLM library, Text2midi <br />
Summary: 
MIDI-LLM is introduced as a language model designed to generate multitrack MIDI music from text prompts. The approach involves expanding the vocabulary of a text language model to incorporate MIDI tokens and implementing a two-stage training process to enable text-to-MIDI conversion. By maintaining the original parameter structure of the language model, MIDI-LLM effectively utilizes the vLLM library for enhanced inference performance. Comparisons with the Text2midi model reveal that MIDI-LLM achieves superior music quality, offers improved text control, and enables faster inference speeds. A live demonstration of the MIDI-LLM model can be accessed at https://midi-llm-demo.vercel.app. <br /><br />Summary: <div>
arXiv:2511.03942v1 Announce Type: cross 
Abstract: We present MIDI-LLM, an LLM for generating multitrack MIDI music from free-form text prompts. Our approach expands a text LLM's vocabulary to include MIDI tokens, and uses a two-stage training recipe to endow text-to-MIDI abilities. By preserving the original LLM's parameter structure, we can directly leverage the vLLM library for accelerated inference. Experiments show that MIDI-LLM achieves higher quality, better text control, and faster inference compared to the recent Text2midi model. Live demo at https://midi-llm-demo.vercel.app.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Collaborative Framework For Math Problem Generation</title>
<link>https://arxiv.org/abs/2511.03958</link>
<guid>https://arxiv.org/abs/2511.03958</guid>
<content:encoded><![CDATA[
<div> keyword: Automatic question generation, mathematics education, transformer-based language models, multi-agent framework, cognitive demand <br />
Summary: 
This paper introduces a novel collaborative multi-agent framework for automatic question generation in mathematics education. The approach aims to improve control over problem complexity and cognitive demands compared to existing transformer-based language models. Multiple agents work together to refine question-answer pairs iteratively, balancing complexity and cognitive challenge. Evaluation criteria include relevance, importance, clarity, difficulty matching, and answerability. Preliminary results indicate that the collaborative multi-agent framework enhances the quality of generated educational content by achieving a more nuanced balance between cognitive challenge and clarity. This innovative approach shows promising outcomes for automated educational content generation and adaptive learning environments. <br /><br />Summary: <div>
arXiv:2511.03958v1 Announce Type: cross 
Abstract: Automatic question generation (AQG) for mathematics education remains an elusive goal for Intelligent Tutoring Systems and educators. While pre-trained transformer-based language models have significantly advanced natural language generation, they often struggle to precisely control problem complexity and cognitive demands. In this paper, we introduce a collaborative multi-agent framework as a novel method of incorporating inference-time computation into AQG. This approach leverages multiple agents that iteratively refine generated question-answer pairs to better balance complexity and cognitive demand. We evaluate the generated questions on five meta-evaluation criteria: relevance, importance, clarity, difficulty matching, answerability, to assess the system's ability to control the required complexity and quality of the questions. Preliminary evaluations show that this collaborative multi-agent framework elevates the quality of generated educational content by fostering a more nuanced balance between cognitive challenge and clarity. These promising outcomes suggest that integrating collaborative multi-agent workflows can yield more controlled, pedagogically valuable content that can help advance automated educational content generation and adaptive learning environments.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing</title>
<link>https://arxiv.org/abs/2511.03980</link>
<guid>https://arxiv.org/abs/2511.03980</guid>
<content:encoded><![CDATA[
<div> LLMs, cultural values, prompt language, cultural framing, model responses<br />
Summary:<br />
This study investigates the impact of prompt language and cultural framing on Large Language Models (LLMs) regarding alignment with cultural values. Testing 10 LLMs with prompts translated into 11 languages, the study finds that while prompt language and cultural perspective influence LLM outputs, the models are biased towards the values of a limited set of countries  the Netherlands, Germany, the US, and Japan. LLMs tend to produce neutral responses with some progressive stances, notably on social tolerance. Cultural framing improves alignment with human values more than prompt language alone, but combining both approaches does not enhance effectiveness. The findings suggest that LLMs are responsive to prompt changes but are anchored in specific cultural defaults, limiting their ability to represent cultural diversity effectively.<br /> 
Summary: <div>
arXiv:2511.03980v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the globe, who interact with them in a diverse range of languages. At the same time, there are well-documented imbalances in the training data and optimisation objectives of this technology, raising doubts as to whether LLMs can represent the cultural diversity of their broad user base. In this study, we look at LLMs and cultural values and examine how prompt language and cultural framing influence model responses and their alignment with human values in different countries. We probe 10 LLMs with 63 items from the Hofstede Values Survey Module and World Values Survey, translated into 11 languages, and formulated as prompts with and without different explicit cultural perspectives. Our study confirms that both prompt language and cultural perspective produce variation in LLM outputs, but with an important caveat: While targeted prompting can, to a certain extent, steer LLM responses in the direction of the predominant values of the corresponding countries, it does not overcome the models' systematic bias toward the values associated with a restricted set of countries in our dataset: the Netherlands, Germany, the US, and Japan. All tested models, regardless of their origin, exhibit remarkably similar patterns: They produce fairly neutral responses on most topics, with selective progressive stances on issues such as social tolerance. Alignment with cultural values of human respondents is improved more with an explicit cultural perspective than with a targeted prompt language. Unexpectedly, combining both approaches is no more effective than cultural framing with an English prompt. These findings reveal that LLMs occupy an uncomfortable middle ground: They are responsive enough to changes in prompts to produce variation, but too firmly anchored to specific cultural defaults to adequately represent cultural diversity.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations</title>
<link>https://arxiv.org/abs/2511.04000</link>
<guid>https://arxiv.org/abs/2511.04000</guid>
<content:encoded><![CDATA[
arXiv:2511.04000v1 Announce Type: cross 
Abstract: Decision trees are widely used in high-stakes fields like finance and healthcare due to their interpretability. This work introduces an efficient, scalable method for generating synthetic pre-training data to enable meta-learning of decision trees. Our approach samples near-optimal decision trees synthetically, creating large-scale, realistic datasets. Using the MetaTree transformer architecture, we demonstrate that this method achieves performance comparable to pre-training on real-world data or with computationally expensive optimal decision trees. This strategy significantly reduces computational costs, enhances data generation flexibility, and paves the way for scalable and efficient meta-learning of interpretable decision tree models.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explorability in Pushdown Automata</title>
<link>https://arxiv.org/abs/2511.04048</link>
<guid>https://arxiv.org/abs/2511.04048</guid>
<content:encoded><![CDATA[
arXiv:2511.04048v1 Announce Type: cross 
Abstract: We study explorability, a measure of nondeterminism in pushdown automata, which generalises history-determinism. An automaton is k-explorable if, while reading the input, it suffices to follow k concurrent runs, built step-by-step based only on the input seen so far, to construct an accepting one, if it exists. We show that the class of explorable PDAs lies strictly between history-deterministic and fully nondeterministic PDAs in terms of both expressiveness and succinctness. In fact increasing explorability induces an infinite hierarchy: each level k defines a strictly more expressive class than level k-1, yet the entire class remains less expressive than general nondeterministic PDAs. We then introduce a parameterized notion of explorability, where the number of runs may depend on input length, and show that exponential explorability precisely captures the context-free languages. Finally, we prove that explorable PDAs can be doubly exponentially more succinct than history-deterministic ones, and that the succinctness gap between deterministic and 2-explorable PDAs is not recursively enumerable. These results position explorability as a robust and operationally meaningful measure of nondeterminism for pushdown systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization</title>
<link>https://arxiv.org/abs/2511.04063</link>
<guid>https://arxiv.org/abs/2511.04063</guid>
<content:encoded><![CDATA[
arXiv:2511.04063v1 Announce Type: cross 
Abstract: Quantization plays a crucial role in accelerating the inference of large-scale models, and rotational matrices have been shown to effectively improve quantization performance by smoothing outliers. However, end-to-end fine-tuning of rotational optimization algorithms incurs high computational costs and is prone to overfitting. To address this challenge, we propose an efficient distribution-aware rotational calibration method, DartQuant, which reduces the complexity of rotational optimization by constraining the distribution of the activations after rotation. This approach also effectively reduces reliance on task-specific losses, thereby mitigating the risk of overfitting. Additionally, we introduce the QR-Orth optimization scheme, which replaces expensive alternating optimization with a more efficient solution. In a variety of model quantization experiments, DartQuant demonstrates superior performance. Compared to existing methods, it achieves 47$\times$ acceleration and 10$\times$ memory savings for rotational optimization on a 70B model. Furthermore, it is the first to successfully complete rotational calibration for a 70B model on a single 3090 GPU, making quantization of large language models feasible in resource-constrained environments. Code is available at https://github.com/CAS-CLab/DartQuant.git.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sub-exponential Growth in Online Word Usage: A Piecewise Power-Law Model</title>
<link>https://arxiv.org/abs/2511.04106</link>
<guid>https://arxiv.org/abs/2511.04106</guid>
<content:encoded><![CDATA[
arXiv:2511.04106v1 Announce Type: cross 
Abstract: The diffusion of ideas and language in society has conventionally been described by S-shaped models, such as the logistic curve. However, the role of sub-exponential growth -a slower than exponential pattern known in epidemiology- has been largely overlooked in broader social phenomena. Here, we present a piecewise power-law model to characterize complex growth curves with a few parameters. We systematically analyzed a large-scale dataset of approximately one billion Japanese blog articles linked to Wikipedia vocabulary, and observed consistent patterns in web search trend data (English, Spanish, and Japanese). Our analysis of the 2,965 selected items reveals that about 55% (1,625 items) were found to have no abrupt jumps and were well captured by one or two segments. For single-segment curves, we found that (i) the mode of the shape parameter alpha was near 0.5, indicating prevalent sub-exponential growth; (ii) the ultimate diffusion scale is primarily determined by the growth rate R, with minor contributions from alpha or the duration T; and (iii) alpha showed a tendency to vary with the nature of the topic, being smaller for niche/local topics and larger for widely shared ones. Furthermore, a micro-behavioral model distinguishing outward contact with strangers from inward interaction within their community suggests that alpha can be interpreted as an index of the preference for outward-oriented communication. These findings suggest that sub-exponential growth is a common pattern of social diffusion, and our model provides a practical framework for consistently describing, comparing, and interpreting complex and diverse growth curves.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Straight: Document Orientation Detection for Efficient OCR</title>
<link>https://arxiv.org/abs/2511.04161</link>
<guid>https://arxiv.org/abs/2511.04161</guid>
<content:encoded><![CDATA[
arXiv:2511.04161v1 Announce Type: cross 
Abstract: Despite significant advances in document understanding, determining the correct orientation of scanned or photographed documents remains a critical pre-processing step in the real world settings. Accurate rotation correction is essential for enhancing the performance of downstream tasks such as Optical Character Recognition (OCR) where misalignment commonly arises due to user errors, particularly incorrect base orientations of the camera during capture. In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from rotation-transformed structured and free-form English OCR datasets, and (ii) ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource languages. We also present a fast, robust and lightweight rotation classification pipeline built on the vision encoder of Phi-3.5-Vision model with dynamic image cropping, fine-tuned specifically for 4-class rotation task in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy on identifying the rotations respectively on both the datasets. Beyond classification, we demonstrate the critical role of our module in boosting OCR performance: closed-source (up to 14%) and open-weights models (up to 4x) in the simulated real-world setting.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transforming Mentorship: An AI Powered Chatbot Approach to University Guidance</title>
<link>https://arxiv.org/abs/2511.04172</link>
<guid>https://arxiv.org/abs/2511.04172</guid>
<content:encoded><![CDATA[
arXiv:2511.04172v1 Announce Type: cross 
Abstract: University students face immense challenges during their undergraduate lives, often being deprived of personalized on-demand guidance that mentors fail to provide at scale. Digital tools exist, but there is a serious lack of customized coaching for newcomers. This paper presents an AI-powered chatbot that will serve as a mentor for the students of BRAC University. The main component is a data ingestion pipeline that efficiently processes and updates information from diverse sources, such as CSV files and university webpages. The chatbot retrieves information through a hybrid approach, combining BM25 lexical ranking with ChromaDB semantic retrieval, and uses a Large Language Model, LLaMA-3.3-70B, to generate conversational responses. The generated text was found to be semantically highly relevant, with a BERTScore of 0.831 and a METEOR score of 0.809. The data pipeline was also very efficient, taking 106.82 seconds for updates, compared to 368.62 seconds for new data. This chatbot will be able to help students by responding to their queries, helping them to get a better understanding of university life, and assisting them to plan better routines for their semester in the open-credit university.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Block Rotation is All You Need for MXFP4 Quantization</title>
<link>https://arxiv.org/abs/2511.04214</link>
<guid>https://arxiv.org/abs/2511.04214</guid>
<content:encoded><![CDATA[
arXiv:2511.04214v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved remarkable success, but their rapidly growing scale imposes prohibitive costs in memory, computation, and energy. Post-training quantization (PTQ) is a promising solution for efficient deployment, yet achieving accurate W4A4 quantization remains an open challenge. While most existing methods are designed for INT4 formats, the emergence of MXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)-- raises questions about the applicability of current techniques. In this work, we establish a comprehensive benchmark of PTQ methods under the MXFP4 format. Through systematic evaluation, we find that methods like GPTQ consistently deliver strong performance, whereas rotation-based approaches, which are almost used by all state-of-the-art approaches, suffer from severe incompatibility with MXFP4. We further provide the first in-depth analysis of this conflict, tracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two) block scaling and the redistribution of outlier energy via global rotation. Building on this insight, we propose a simple yet effective block rotation strategy that adapts rotation-based methods to MXFP4, leading to substantial accuracy improvements across diverse LLMs. Our findings not only offer clear guidance for practitioners but also set a foundation for advancing PTQ research under emerging low-precision formats.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Black-Box Guardrail Reverse-engineering Attack</title>
<link>https://arxiv.org/abs/2511.04215</link>
<guid>https://arxiv.org/abs/2511.04215</guid>
<content:encoded><![CDATA[
arXiv:2511.04215v1 Announce Type: cross 
Abstract: Large language models (LLMs) increasingly employ guardrails to enforce ethical, legal, and application-specific constraints on their outputs. While effective at mitigating harmful responses, these guardrails introduce a new class of vulnerabilities by exposing observable decision patterns. In this work, we present the first study of black-box LLM guardrail reverse-engineering attacks. We propose Guardrail Reverse-engineering Attack (GRA), a reinforcement learning-based framework that leverages genetic algorithm-driven data augmentation to approximate the decision-making policy of victim guardrails. By iteratively collecting input-output pairs, prioritizing divergence cases, and applying targeted mutations and crossovers, our method incrementally converges toward a high-fidelity surrogate of the victim guardrail. We evaluate GRA on three widely deployed commercial systems, namely ChatGPT, DeepSeek, and Qwen3, and demonstrate that it achieves an rule matching rate exceeding 0.92 while requiring less than $85 in API costs. These findings underscore the practical feasibility of guardrail extraction and highlight significant security risks for current LLM safety mechanisms. Our findings expose critical vulnerabilities in current guardrail designs and highlight the urgent need for more robust defense mechanisms in LLM deployment.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity</title>
<link>https://arxiv.org/abs/2511.04418</link>
<guid>https://arxiv.org/abs/2511.04418</guid>
<content:encoded><![CDATA[
arXiv:2511.04418v1 Announce Type: cross 
Abstract: Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is critical for trustworthy deployment. While real-world language is inherently ambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically benchmarked against tasks with no ambiguity. In this work, we demonstrate that while current uncertainty estimators perform well under the restrictive assumption of no ambiguity, they degrade to close-to-random performance on ambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first ambiguous question-answering (QA) datasets equipped with ground-truth answer distributions estimated from factual co-occurrence. We find this performance deterioration to be consistent across different estimation paradigms: using the predictive distribution itself, internal representations throughout the model, and an ensemble of models. We show that this phenomenon can be theoretically explained, revealing that predictive-distribution and ensemble-based estimators are fundamentally limited under ambiguity. Overall, our study reveals a key shortcoming of current UQ methods for LLMs and motivates a rethinking of current modeling paradigms.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs</title>
<link>https://arxiv.org/abs/2511.04473</link>
<guid>https://arxiv.org/abs/2511.04473</guid>
<content:encoded><![CDATA[
arXiv:2511.04473v1 Announce Type: cross 
Abstract: Retrieval of information from graph-structured knowledge bases represents a promising direction for improving the factuality of LLMs. While various solutions have been proposed, a comparison of methods is difficult due to the lack of challenging QA datasets with ground-truth targets for graph retrieval. We present SynthKGQA, a framework for generating high-quality synthetic Knowledge Graph Question Answering datasets from any Knowledge Graph, providing the full set of ground-truth facts in the KG to reason over each question. We show how, in addition to enabling more informative benchmarking of KG retrievers, the data produced with SynthKGQA also allows us to train better models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset designed to test zero-shot generalization abilities of KG retrievers with respect to unseen graph structures and relation types, and benchmark popular solutions for KG-augmented LLMs on it.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large language models replicate and predict human cooperation across experiments in game theory</title>
<link>https://arxiv.org/abs/2511.04500</link>
<guid>https://arxiv.org/abs/2511.04500</guid>
<content:encoded><![CDATA[
arXiv:2511.04500v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used both to make decisions in domains such as health, education and law, and to simulate human behavior. Yet how closely LLMs mirror actual human decision-making remains poorly understood. This gap is critical: misalignment could produce harmful outcomes in practical applications, while failure to replicate human behavior renders LLMs ineffective for social simulations. Here, we address this gap by developing a digital twin of game-theoretic experiments and introducing a systematic prompting and probing framework for machine-behavioral evaluation. Testing three open-source models (Llama, Mistral and Qwen), we find that Llama reproduces human cooperation patterns with high fidelity, capturing human deviations from rational choice theory, while Qwen aligns closely with Nash equilibrium predictions. Notably, we achieved population-level behavioral replication without persona-based prompting, simplifying the simulation process. Extending beyond the original human-tested games, we generate and preregister testable hypotheses for novel game configurations outside the original parameter grid. Our findings demonstrate that appropriately calibrated LLMs can replicate aggregate human behavioral patterns and enable systematic exploration of unexplored experimental spaces, offering a complementary approach to traditional research in the social and behavioral sciences that generates new empirical predictions about human social decision-making.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm</title>
<link>https://arxiv.org/abs/2511.04570</link>
<guid>https://arxiv.org/abs/2511.04570</guid>
<content:encoded><![CDATA[
arXiv:2511.04570v1 Announce Type: cross 
Abstract: "Thinking with Text" and "Thinking with Images" paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce "Thinking with Video", a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2's performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions "thinking with video" as a unified multimodal reasoning paradigm.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper</title>
<link>https://arxiv.org/abs/2511.04583</link>
<guid>https://arxiv.org/abs/2511.04583</guid>
<content:encoded><![CDATA[
arXiv:2511.04583v1 Announce Type: cross 
Abstract: Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis</title>
<link>https://arxiv.org/abs/2511.04584</link>
<guid>https://arxiv.org/abs/2511.04584</guid>
<content:encoded><![CDATA[
arXiv:2511.04584v1 Announce Type: cross 
Abstract: Natural language interfaces to tabular data must handle ambiguities inherent to queries. Instead of treating ambiguity as a deficiency, we reframe it as a feature of cooperative interaction, where the responsibility of query specification is shared among the user and the system. We develop a principled framework distinguishing cooperative queries, i.e., queries that yield a resolvable interpretation, from uncooperative queries that cannot be resolved. Applying the framework to evaluations for tabular question answering and analysis, we analyze the queries in 15 popular datasets, and observe an uncontrolled mixing of query types neither adequate for evaluating a system's execution accuracy nor for evaluating interpretation capabilities. Our framework and analysis of queries shifts the perspective from fixing ambiguity to embracing cooperation in resolving queries. This reflection enables more informed design and evaluation for natural language interfaces for tabular data, for which we outline implications and directions for future research.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2511.04646</link>
<guid>https://arxiv.org/abs/2511.04646</guid>
<content:encoded><![CDATA[
arXiv:2511.04646v1 Announce Type: cross 
Abstract: Cooperative multi-agent planning requires agents to make joint decisions with partial information and limited communication. Coordination at the trajectory level often fails, as small deviations in timing or movement cascade into conflicts. Symbolic planning mitigates this challenge by raising the level of abstraction and providing a minimal vocabulary of actions that enable synchronization and collective progress. We present DR. WELL, a decentralized neurosymbolic framework for cooperative multi-agent planning. Cooperation unfolds through a two-phase negotiation protocol: agents first propose candidate roles with reasoning and then commit to a joint allocation under consensus and environment constraints. After commitment, each agent independently generates and executes a symbolic plan for its role without revealing detailed trajectories. Plans are grounded in execution outcomes via a shared world model that encodes the current state and is updated as agents act. By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids brittle step-level alignment and enables higher-level operations that are reusable, synchronizable, and interpretable. Experiments on cooperative block-push tasks show that agents adapt across episodes, with the dynamic world model capturing reusable patterns and improving task completion rates and efficiency. Experiments on cooperative block-push tasks show that our dynamic world model improves task completion and efficiency through negotiation and self-refinement, trading a time overhead for evolving, more efficient collaboration strategies.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks</title>
<link>https://arxiv.org/abs/2511.04662</link>
<guid>https://arxiv.org/abs/2511.04662</guid>
<content:encoded><![CDATA[
arXiv:2511.04662v1 Announce Type: cross 
Abstract: LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but they cannot reliably verify their own logic. Even when they reach correct answers, the underlying reasoning may be flawed, undermining trust in high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a neuro-symbolic method that extracts and verifies formal logical arguments from CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order logic and identifies premises that ground the argument in source context, commonsense knowledge, or prior reasoning steps. The symbolic representation enables automated solvers to verify logical validity while the NL premises allow humans and systems to identify ungrounded or fallacious reasoning steps. Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT effectively identifies flawed reasoning, and serves as a strong predictor of final answer correctness. We also leverage VeriCoT's verification signal for (1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct preference optimization (DPO) using verification-based pairwise rewards, further improving reasoning validity and accuracy.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decomposed Prompting: Probing Multilingual Linguistic Structure Knowledge in Large Language Models</title>
<link>https://arxiv.org/abs/2402.18397</link>
<guid>https://arxiv.org/abs/2402.18397</guid>
<content:encoded><![CDATA[
arXiv:2402.18397v2 Announce Type: replace 
Abstract: Probing the multilingual knowledge of linguistic structure in LLMs, often characterized as sequence labeling, faces challenges with maintaining output templates in current text-to-text prompting strategies. To solve this, we introduce a decomposed prompting approach for sequence labeling tasks. Diverging from the single text-to-text prompt, our prompt method generates for each token of the input sentence an individual prompt which asks for its linguistic label. We test our method on the Universal Dependencies part-of-speech tagging dataset for 38 languages, using both English-centric and multilingual LLMs. Our findings show that decomposed prompting surpasses the iterative prompting baseline in efficacy and efficiency under zero- and few-shot settings. Moreover, our analysis of multilingual performance of English-centric LLMs yields insights into the transferability of linguistic knowledge via multilingual prompting.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users</title>
<link>https://arxiv.org/abs/2406.17737</link>
<guid>https://arxiv.org/abs/2406.17737</guid>
<content:encoded><![CDATA[
arXiv:2406.17737v2 Announce Type: replace 
Abstract: While state-of-the-art large language models (LLMs) have shown impressive performance on many tasks, there has been extensive research on undesirable model behavior such as hallucinations and bias. In this work, we investigate how the quality of LLM responses changes in terms of information accuracy, truthfulness, and refusals depending on three user traits: English proficiency, education level, and country of origin. We present extensive experimentation on three state-of-the-art LLMs and two different datasets targeting truthfulness and factuality. Our findings suggest that undesirable behaviors in state-of-the-art LLMs occur disproportionately more for users with lower English proficiency, of lower education status, and originating from outside the US, rendering these models unreliable sources of information towards their most vulnerable users.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Legal Fact Prediction: The Missing Piece in Legal Judgment Prediction</title>
<link>https://arxiv.org/abs/2409.07055</link>
<guid>https://arxiv.org/abs/2409.07055</guid>
<content:encoded><![CDATA[
arXiv:2409.07055v3 Announce Type: replace 
Abstract: Legal judgment prediction (LJP), which enables litigants and their lawyers to forecast judgment outcomes and refine litigation strategies, has emerged as a crucial legal NLP task. Existing studies typically utilize legal facts, i.e., facts that have been established by evidence and determined by the judge, to predict the judgment. However, legal facts are often difficult to obtain in the early stages of litigation, significantly limiting the practical applicability of fact-based LJP. To address this limitation, we propose a novel legal NLP task: legal fact prediction (LFP), which takes the evidence submitted by litigants for trial as input to predict legal facts, thereby empowering fact-based LJP technologies to make predictions in the absence of ground-truth legal facts. We also propose the first benchmark dataset, LFPBench, for evaluating the LFP task. Our extensive experiments on LFPBench demonstrate the effectiveness of LFP-empowered LJP and highlight promising research directions for LFP.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination</title>
<link>https://arxiv.org/abs/2410.04514</link>
<guid>https://arxiv.org/abs/2410.04514</guid>
<content:encoded><![CDATA[
arXiv:2410.04514v2 Announce Type: replace 
Abstract: Despite the great success of Large Vision-Language Models (LVLMs), they inevitably suffer from hallucination. As we know, both the visual encoder and the Large Language Model (LLM) decoder in LVLMs are Transformer-based, allowing the model to extract visual information and generate text outputs via attention mechanisms. We find that the attention distribution of LLM decoder on image tokens is highly consistent with the visual encoder and both distributions tend to focus on particular background tokens rather than the referred objects in the image. We attribute to the unexpected attention distribution to an inherent flaw in the visual encoder itself, which misguides LLMs to over emphasize the redundant information and generate object hallucination. To address the issue, we propose DAMRO, a novel training-free strategy that $D$ive into $A$ttention $M$echanism of LVLM to $R$educe $O$bject Hallucination. Specifically, our approach employs classification token (CLS) of ViT to filter out high-attention outlier tokens scattered in the background and then eliminate their influence during decoding stage. We evaluate our method on LVLMs including LLaVA-1.5, LLaVA-NeXT and InstructBLIP, using various benchmarks such as POPE, CHAIR, MME and GPT-4V Aided Evaluation. The results demonstrate that our approach significantly reduces the impact of these outlier tokens, thus effectively alleviating the hallucination of LVLMs. The code is released at https://github.com/coder-gx/DAMRO.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who is the root in a syntactic dependency structure?</title>
<link>https://arxiv.org/abs/2501.15188</link>
<guid>https://arxiv.org/abs/2501.15188</guid>
<content:encoded><![CDATA[
arXiv:2501.15188v3 Announce Type: replace 
Abstract: The syntactic structure of a sentence can be described as a tree that indicates the syntactic relationships between words. In spite of significant progress in unsupervised methods that retrieve the syntactic structure of sentences, guessing the right direction of edges is still a challenge. As in a syntactic dependency structure edges are oriented away from the root, the challenge of guessing the right direction can be reduced to finding an undirected tree and the root. The limited performance of current unsupervised methods demonstrates the lack of a proper understanding of what a root vertex is from first principles. We consider an ensemble of centrality scores, some that only take into account the free tree (non-spatial scores) and others that take into account the position of vertices (spatial scores). We test the hypothesis that the root vertex is an important or central vertex of the syntactic dependency structure. We confirm the hypothesis in the sense that root vertices tend to have high centrality and that vertices of high centrality tend to be roots. The best performance in guessing the root is achieved by novel scores that only take into account the position of a vertex and that of its neighbours. We provide theoretical and empirical foundations towards a universal notion of rootness from a network science perspective.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KGGen: Extracting Knowledge Graphs from Plain Text with Language Models</title>
<link>https://arxiv.org/abs/2502.09956</link>
<guid>https://arxiv.org/abs/2502.09956</guid>
<content:encoded><![CDATA[
arXiv:2502.09956v2 Announce Type: replace 
Abstract: Recent interest in building foundation models for KGs has highlighted a fundamental challenge: knowledge-graph data is relatively scarce. The best-known KGs are primarily human-labeled, created by pattern-matching, or extracted using early NLP techniques. While human-generated KGs are in short supply, automatically extracted KGs are of questionable quality. We present a solution to this data scarcity problem in the form of a text-to-KG generator (KGGen), a package that uses language models to create high-quality graphs from plaintext. Unlike other KG extractors, KGGen clusters related entities to reduce sparsity in extracted KGs. KGGen is available as a Python library (\texttt{pip install kg-gen}), making it accessible to everyone. Along with KGGen, we release the first benchmark, Measure of of Information in Nodes and Edges (MINE), that tests an extractor's ability to produce a useful KG from plain text. We benchmark our new tool against existing extractors and demonstrate far superior performance.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pragmatic Reasoning improves LLM Code Generation</title>
<link>https://arxiv.org/abs/2502.15835</link>
<guid>https://arxiv.org/abs/2502.15835</guid>
<content:encoded><![CDATA[
arXiv:2502.15835v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated impressive potential in translating natural language (NL) instructions into program code. However, user instructions often contain inherent ambiguities, making it challenging for LLMs to generate code that accurately reflects the user's true intent. To address this challenge, researchers have proposed approaches that produce multiple candidates of the program code and then rerank them to identify the best solution. In this paper, we propose CodeRSA, a novel code candidate reranking mechanism built upon the Rational Speech Act (RSA) framework, designed to guide LLMs toward more comprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using Llama-3-8B-Instruct and Qwen-2.5-7B-Instruct on two widely used code generation benchmarks, HumanEval and MBPP. Our experiment results show that CodeRSA consistently outperforms common baselines, surpasses the state-of-the-art approach in most cases, and demonstrates robust overall performance. These findings underscore the effectiveness of integrating pragmatic reasoning into code candidate reranking, offering a promising direction for enhancing code generation quality in LLMs.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphCheck: Multipath Fact-Checking with Entity-Relationship Graphs</title>
<link>https://arxiv.org/abs/2502.20785</link>
<guid>https://arxiv.org/abs/2502.20785</guid>
<content:encoded><![CDATA[
arXiv:2502.20785v3 Announce Type: replace 
Abstract: Automated fact-checking aims to assess the truthfulness of textual claims based on relevant evidence. However, verifying complex claims that require multi-hop reasoning remains a significant challenge. We propose GraphCheck, a novel framework that transforms claims into entity-relationship graphs for structured and systematic fact-checking. By explicitly modeling both explicit and latent entities and exploring multiple reasoning paths, GraphCheck enhances verification robustness. While GraphCheck excels in complex scenarios, it may be unnecessarily elaborate for simpler claims. To address this, we introduce DP-GraphCheck, a variant that employs a lightweight strategy selector to choose between direct prompting and GraphCheck adaptively. This selective mechanism improves both accuracy and efficiency by applying the appropriate level of reasoning to each claim. Experiments on the HOVER and EX-FEVER datasets demonstrate that our approach outperforms existing methods in verification accuracy, while achieving strong computational efficiency despite its multipath exploration. Moreover, the strategy selection mechanism in DP-GraphCheck generalizes well to other fact-checking pipelines, highlighting the broad applicability of our framework.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Datasets, Documents, and Repetitions: The Practicalities of Unequal Data Quality</title>
<link>https://arxiv.org/abs/2503.07879</link>
<guid>https://arxiv.org/abs/2503.07879</guid>
<content:encoded><![CDATA[
arXiv:2503.07879v2 Announce Type: replace 
Abstract: Data filtering has become a powerful tool for improving model performance while reducing computational cost. However, as large language model compute budgets continue to grow, the limited data volume provided by heavily filtered and deduplicated datasets will become a practical constraint. In efforts to better understand how to proceed, we study model performance at various compute budgets and across multiple pre-training datasets created through data filtering and deduplication. We find that, given appropriate modifications to the training recipe, repeating existing aggressively filtered datasets for up to ten epochs can outperform training on the ten times larger superset for a single epoch across multiple compute budget orders of magnitude. While this finding relies on repeating the dataset for many epochs, we also investigate repeats within these datasets at the document level. We find that not all documents within a dataset are equal, and we can create better datasets relative to a token budget by explicitly manipulating the counts of individual documents. We conclude by arguing that even as large language models scale, data filtering remains an important direction of research.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Model Development through Fine-tuning Transfer</title>
<link>https://arxiv.org/abs/2503.20110</link>
<guid>https://arxiv.org/abs/2503.20110</guid>
<content:encoded><![CDATA[
arXiv:2503.20110v2 Announce Type: replace 
Abstract: Modern LLMs struggle with efficient updates, as each new pretrained model version requires repeating expensive alignment processes. This challenge also applies to domain- or languagespecific models, where fine-tuning on specialized data must be redone for every new base model release. In this paper, we explore the transfer of fine-tuning updates between model versions. Specifically, we derive the diff vector (representing the weight changes from finetuning) from one source model version and apply it to the base model of a different target version. Through empirical evaluations on various open-weight model versions, we show that transferring diff vectors can significantly improve the performance of the target base model. For example, transferring the fine-tuning updates from Llama 3.0 8B improves Llama 3.1 8B by 46.9% on IFEval and 15.7% on LiveCodeBench without additional training, even surpassing Llama 3.1 8B Instruct. Furthermore, we demonstrate performance gains on multilingual tasks, with 4.7% and 15.5% improvements on Global MMLU for Malagasy and Turkish, respectively. We observe that these merged models provide stronger initializations for further fine-tuning. Lastly, our controlled experiments suggest that fine-tuning transfer is most effective when source and target models lie in a linearly connected region of parameter space, and we provide a theoretical analysis of our method. Taken together, fine-tuning transfer offers a cost-efficient and practical strategy for continuous LLM development. Our code is available at github.com/pjlintw/finetuning-transfer.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context</title>
<link>https://arxiv.org/abs/2504.04737</link>
<guid>https://arxiv.org/abs/2504.04737</guid>
<content:encoded><![CDATA[
arXiv:2504.04737v2 Announce Type: replace 
Abstract: In the landscape of Fact-based Judgment Prediction and Explanation (FJPE), reliance on factual data is essential for developing robust and realistic AI-driven decision-making tools. This paper introduces TathyaNyaya, the largest annotated dataset for FJPE tailored to the Indian legal context, encompassing judgments from the Supreme Court of India and various High Courts. Derived from the Hindi terms "Tathya" (fact) and "Nyaya" (justice), the TathyaNyaya dataset is uniquely designed to focus on factual statements rather than complete legal texts, reflecting real-world judicial processes where factual data drives outcomes. Complementing this dataset, we present FactLegalLlama, an instruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM), optimized for generating high-quality explanations in FJPE tasks. Finetuned on the factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy with coherent, contextually relevant explanations, addressing the critical need for transparency and interpretability in AI-assisted legal systems. Our methodology combines transformers for binary judgment prediction with FactLegalLlama for explanation generation, creating a robust framework for advancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses existing datasets in scale and diversity but also establishes a benchmark for building explainable AI systems in legal analysis. The findings underscore the importance of factual precision and domain-specific tuning in enhancing predictive performance and interpretability, positioning TathyaNyaya and FactLegalLlama as foundational resources for AI-assisted legal decision-making.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards</title>
<link>https://arxiv.org/abs/2505.04847</link>
<guid>https://arxiv.org/abs/2505.04847</guid>
<content:encoded><![CDATA[
arXiv:2505.04847v2 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) aims to reduce hallucinations by grounding responses in external context, yet large language models (LLMs) still frequently introduce unsupported information or contradictions even when provided with relevant context. This paper presents two complementary efforts at Vectara to measure and benchmark LLM faithfulness in RAG. First, we describe our original hallucination leaderboard, which has tracked hallucination rates for LLMs since 2023 using our HHEM hallucination detection model. Motivated by limitations observed in current hallucination detection methods, we introduce FaithJudge, an LLM-as-a-judge framework that leverages a pool of diverse human-annotated hallucination examples to substantially improve the automated hallucination evaluation of LLMs. We introduce an enhanced hallucination leaderboard centered on FaithJudge that benchmarks LLMs on RAG faithfulness in summarization, question-answering, and data-to-text generation tasks. FaithJudge enables a more reliable benchmarking of LLM hallucinations in RAG and supports the development of more trustworthy generative AI systems: https://github.com/vectara/FaithJudge.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Are They Talking About? A Benchmark of Knowledge-Grounded Discussion Summarization</title>
<link>https://arxiv.org/abs/2505.12474</link>
<guid>https://arxiv.org/abs/2505.12474</guid>
<content:encoded><![CDATA[
arXiv:2505.12474v3 Announce Type: replace 
Abstract: Traditional dialogue summarization primarily focuses on dialogue content, assuming it comprises adequate information for a clear summary. However, this assumption often fails for discussions grounded in shared background, where participants frequently omit context and use implicit references. This results in summaries that are confusing to readers unfamiliar with the background. To address this, we introduce Knowledge-Grounded Discussion Summarization (KGDS), a novel task that produces a supplementary background summary for context and a clear opinion summary with clarified references. To facilitate research, we construct the first KGDS benchmark, featuring news-discussion pairs and expert-created multi-granularity gold annotations for evaluating sub-summaries. We also propose a novel hierarchical evaluation framework with fine-grained and interpretable metrics. Our extensive evaluation of 12 advanced large language models (LLMs) reveals that KGDS remains a significant challenge. The models frequently miss key facts and retain irrelevant ones in background summarization, and often fail to resolve implicit references in opinion summary integration.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Multilingual Encoder Language Model Compression for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2505.16956</link>
<guid>https://arxiv.org/abs/2505.16956</guid>
<content:encoded><![CDATA[
arXiv:2505.16956v2 Announce Type: replace 
Abstract: In this paper, we combine two-step knowledge distillation, structured pruning, truncation, and vocabulary trimming for extremely compressing multilingual encoder-only language models for low-resource languages. Our novel approach systematically combines existing techniques and takes them to the extreme, reducing layer depth, feed-forward hidden size, and intermediate layer embedding size to create significantly smaller monolingual models while retaining essential language-specific knowledge. We achieve compression rates of up to 92% while maintaining competitive performance, with average drops of 2-10% for moderate compression and 8-13% at maximum compression in four downstream tasks, including sentiment analysis, topic classification, named entity recognition, and part-of-speech tagging, across three low-resource languages. Notably, the performance degradation correlates with the amount of language-specific data in the teacher model, with larger datasets resulting in smaller performance losses. Additionally, we conduct ablation studies to identify the best practices for multilingual model compression using these techniques.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compression Hacking: A Supplementary Perspective on Informatics Properties of Language Models from Geometric Distortion</title>
<link>https://arxiv.org/abs/2505.17793</link>
<guid>https://arxiv.org/abs/2505.17793</guid>
<content:encoded><![CDATA[
arXiv:2505.17793v3 Announce Type: replace 
Abstract: Recently, the concept of ``compression as intelligence'' has provided a novel informatics metric perspective for language models (LMs), emphasizing that highly structured representations signify the intelligence level of LMs. However, from a geometric standpoint, the word representation space of highly compressed LMs tends to degenerate into a highly anisotropic state, which hinders the LM's ability to comprehend instructions and directly impacts its performance. We found this compression-anisotropy synchronicity is essentially the ``Compression Hacking'' in LM representations, where noise-dominated directions tend to create the illusion of high compression rates by sacrificing spatial uniformity. Based on this, we propose three refined compression metrics by incorporating geometric distortion analysis and integrate them into a self-evaluation pipeline. The refined metrics exhibit strong alignment with the LM's comprehensive capabilities, achieving Spearman correlation coefficients above 0.9, significantly outperforming both the original compression and other internal structure-based metrics. This confirms that compression hacking substantially enhances the informatics interpretation of LMs by incorporating geometric distortion of representations.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics</title>
<link>https://arxiv.org/abs/2505.18658</link>
<guid>https://arxiv.org/abs/2505.18658</guid>
<content:encoded><![CDATA[
arXiv:2505.18658v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have emerged as a promising cornerstone for the development of natural language processing (NLP) and artificial intelligence (AI). However, ensuring the robustness of LLMs remains a critical challenge. To address these challenges and advance the field, this survey provides a comprehensive overview of current studies in this area. First, we systematically examine the nature of robustness in LLMs, including its conceptual foundations, the importance of consistent performance across diverse inputs, and the implications of failure modes in real-world applications. Next, we analyze the sources of non-robustness, categorizing intrinsic model limitations, data-driven vulnerabilities, and external adversarial factors that compromise reliability. Following this, we review state-of-the-art mitigation strategies, and then we discuss widely adopted benchmarks, emerging metrics, and persistent gaps in assessing real-world reliability. Finally, we synthesize findings from existing surveys and interdisciplinary studies to highlight trends, unresolved issues, and pathways for future research.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Models Hallucinate More: Factuality-Aware Reinforcement Learning for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2505.24630</link>
<guid>https://arxiv.org/abs/2505.24630</guid>
<content:encoded><![CDATA[
arXiv:2505.24630v2 Announce Type: replace 
Abstract: Large language models (LLMs) have significantly advanced in reasoning tasks through reinforcement learning (RL) optimization, achieving impressive capabilities across various challenging benchmarks. However, our empirical analysis reveals a critical drawback: reasoning-oriented RL fine-tuning significantly increases the prevalence of hallucinations. We theoretically analyze the RL training dynamics, identifying high-variance gradient, entropy-induced randomness, and susceptibility to spurious local optima as key factors leading to hallucinations. To address this drawback, we propose Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL fine-tuning algorithm incorporating explicit factuality verification at each reasoning step. FSPO leverages automated verification against given evidence to dynamically adjust token-level advantage values, incentivizing factual correctness throughout the reasoning process. Experiments across mathematical reasoning and hallucination benchmarks using Qwen2.5 and Llama models demonstrate that FSPO effectively reduces hallucinations while enhancing reasoning accuracy, substantially improving both reliability and performance.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs</title>
<link>https://arxiv.org/abs/2506.05410</link>
<guid>https://arxiv.org/abs/2506.05410</guid>
<content:encoded><![CDATA[
arXiv:2506.05410v2 Announce Type: replace 
Abstract: Recent advances in Large Language Models (LLMs) have highlighted the critical importance of extending context length, yet the quadratic complexity of attention mechanisms poses significant challenges for efficient long-context modeling. KV cache compression has emerged as a key approach to address this challenge. Through extensive empirical analysis, we reveal a fundamental yet previously overlooked asymmetry in KV caches: while adjacent keys receive similar attention weights ({\it local homogeneity}), adjacent values demonstrate distinct {\it heterogeneous} distributions. This key-value asymmetry reveals a critical limitation in existing compression methods that treat keys and values uniformly. To address the limitation, we propose a training-free compression framework (AsymKV) that combines homogeneity-based key merging with a mathematically proven lossless value compression. Extensive experiments demonstrate that AsymKV consistently outperforms existing long-context methods across various tasks and base models. For example, on LLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing SOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in this link:https://github.com/the-scale-lab/Asymkv.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TurBLiMP: A Turkish Benchmark of Linguistic Minimal Pairs</title>
<link>https://arxiv.org/abs/2506.13487</link>
<guid>https://arxiv.org/abs/2506.13487</guid>
<content:encoded><![CDATA[
arXiv:2506.13487v2 Announce Type: replace 
Abstract: We introduce TurBLiMP, the first Turkish benchmark of linguistic minimal pairs, designed to evaluate the linguistic abilities of monolingual and multilingual language models (LMs). Covering 16 linguistic phenomena with 1000 minimal pairs each, TurBLiMP fills an important gap in linguistic evaluation resources for Turkish. In designing the benchmark, we give extra attention to two properties of Turkish that remain understudied in current syntactic evaluations of LMs, namely word order flexibility and subordination through morphological processes. Our experiments on a wide range of LMs and a newly collected set of human acceptability judgments reveal that even cutting-edge Large LMs still struggle with grammatical phenomena that are not challenging for humans, and may also exhibit different sensitivities to word order and morphological complexity compared to humans.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning</title>
<link>https://arxiv.org/abs/2506.21591</link>
<guid>https://arxiv.org/abs/2506.21591</guid>
<content:encoded><![CDATA[
arXiv:2506.21591v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate significant potential but face challenges in complex financial reasoning tasks requiring both domain knowledge and sophisticated reasoning. Current evaluation benchmarks often fall short by not decoupling these capabilities indicators from single task performance and lack root cause analysis for task failure. To address this, we introduce FinEval-KR, a novel evaluation framework for decoupling and quantifying LLMs' knowledge and reasoning abilities independently, proposing distinct knowledge score and reasoning score metrics. Inspired by cognitive science, we further propose a cognitive score based on Bloom's taxonomy to analyze capabilities in reasoning tasks across different cognitive levels. We also release a new open-source Chinese financial reasoning dataset covering 22 subfields to support reproducible research and further advancements in financial reasoning. Our experimental results reveal that LLM reasoning ability and higher-order cognitive ability are the core factors influencing reasoning accuracy. We also specifically find that even top models still face a bottleneck with knowledge application. Furthermore, our analysis shows that specialized financial LLMs generally lag behind the top general large models across multiple metrics.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text2VectorSQL: Towards a Unified Interface for Vector Search and SQL Queries</title>
<link>https://arxiv.org/abs/2506.23071</link>
<guid>https://arxiv.org/abs/2506.23071</guid>
<content:encoded><![CDATA[
arXiv:2506.23071v2 Announce Type: replace 
Abstract: The proliferation of unstructured data poses a fundamental challenge to traditional database interfaces. While Text-to-SQL has democratized access to structured data, it remains incapable of interpreting semantic or multi-modal queries. Concurrently, vector search has emerged as the de facto standard for querying unstructured data, but its integration with SQL-termed VectorSQL-still relies on manual query crafting and lacks standardized evaluation methodologies, creating a significant gap between its potential and practical application.
  To bridge this fundamental gap, we introduce and formalize Text2VectorSQL, a novel task to establish a unified natural language interface for seamlessly querying both structured and unstructured data. To catalyze research in this new domain, we present a comprehensive foundational ecosystem, including: (1) A scalable and robust pipeline for synthesizing high-quality Text-to-VectorSQL training data. (2) VectorSQLBench, the first large-scale, multi-faceted benchmark for this task, encompassing 12 distinct combinations across three database backends (SQLite, PostgreSQL, ClickHouse) and four data sources (BIRD, Spider, arXiv, Wikipedia). (3) Several novel evaluation metrics designed for more nuanced performance analysis. Extensive experiments not only confirm strong baseline performance with our trained models, but also reveal the recall degradation challenge: the integration of SQL filters with vector search can lead to more pronounced result omissions than in conventional filtered vector search. By defining the core task, delivering the essential data and evaluation infrastructure, and identifying key research challenges, our work lays the essential groundwork to build the next generation of unified and intelligent data interfaces. Our repository is available at https://github.com/OpenDCAI/Text2VectorSQL.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distillation versus Contrastive Learning: How to Train Your Rerankers</title>
<link>https://arxiv.org/abs/2507.08336</link>
<guid>https://arxiv.org/abs/2507.08336</guid>
<content:encoded><![CDATA[
arXiv:2507.08336v3 Announce Type: replace 
Abstract: Training effective text rerankers is crucial for information retrieval. Two strategies are widely used: contrastive learning (optimizing directly on ground-truth labels) and knowledge distillation (transferring knowledge from a larger reranker). While both have been studied extensively, a clear comparison of their effectiveness for training cross-encoder rerankers under practical conditions is needed.
  This paper empirically compares these strategies by training rerankers of different sizes (0.5B, 1.5B, 3B, 7B) and architectures (Transformer, Recurrent) using both methods on the same data, with a strong contrastive learning model acting as the distillation teacher. Our results show that knowledge distillation generally yields better in-domain and out-of-domain ranking performance than contrastive learning when distilling from a more performant teacher model. This finding is consistent across student model sizes and architectures. However, distilling from a teacher of the same capacity does not provide the same advantage, particularly for out-of-domain tasks. These findings offer practical guidance for choosing a training strategy based on available teacher models. We recommend using knowledge distillation to train smaller rerankers if a larger, more performant teacher is accessible; in its absence, contrastive learning remains a robust baseline. Our code implementation is made available to facilitate reproducbility.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification</title>
<link>https://arxiv.org/abs/2507.14578</link>
<guid>https://arxiv.org/abs/2507.14578</guid>
<content:encoded><![CDATA[
arXiv:2507.14578v2 Announce Type: replace 
Abstract: We propose XL-DURel, a finetuned, multilingual Sentence Transformer model optimized for ordinal Word-in-Context classification. We test several loss functions for regression and ranking tasks managing to outperform previous models on ordinal and binary data with a ranking objective based on angular distance in complex space. We further show that binary WiC can be treated as a special case of ordinal WiC and that optimizing models for the general ordinal task improves performance on the more specific binary task. This paves the way for a unified treatment of WiC modeling across different task formulations.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System</title>
<link>https://arxiv.org/abs/2508.00709</link>
<guid>https://arxiv.org/abs/2508.00709</guid>
<content:encoded><![CDATA[
arXiv:2508.00709v2 Announce Type: replace 
Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RPRO: Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning</title>
<link>https://arxiv.org/abs/2509.00974</link>
<guid>https://arxiv.org/abs/2509.00974</guid>
<content:encoded><![CDATA[
arXiv:2509.00974v2 Announce Type: replace 
Abstract: Medical question answering requires advanced reasoning that integrates domain knowledge with logical inference. However, existing large language models (LLMs) often generate reasoning chains that lack factual accuracy and clinical reliability. We propose Ranked Preference Reinforcement Optimization (RPRO), a novel framework that uniquely combines reinforcement learning with preference-driven reasoning refinement to enhance clinical chain-of-thought (CoT) performance. RPRO differentiates itself from prior approaches by employing task-adaptive reasoning templates and a probabilistic evaluation mechanism that aligns outputs with established clinical workflows, while automatically identifying and correcting low-quality reasoning chains. Unlike traditional pairwise preference methods, RPRO introduces a groupwise ranking optimization based on the Bradley-Terry model and incorporates KL-divergence regularization for stable training. Experiments on PubMedQA and MedQA-USMLE show consistent improvements over strong baselines. Remarkably, our 1.1B parameter model outperforms much larger 7B-13B models, including medical-specialized variants. These findings demonstrate that combining preference optimization with quality-driven refinement offers a scalable and effective approach to building more reliable, clinically grounded medical LLMs.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions</title>
<link>https://arxiv.org/abs/2509.08217</link>
<guid>https://arxiv.org/abs/2509.08217</guid>
<content:encoded><![CDATA[
arXiv:2509.08217v2 Announce Type: replace 
Abstract: For machine learning datasets to accurately represent diverse opinions in a population, they must preserve variation in data labels while filtering out spam or low-quality responses. How can we balance annotator reliability and representation? We empirically evaluate how a range of heuristics for annotator filtering affect the preservation of variation on subjective tasks. We find that these methods, designed for contexts in which variation from a single ground-truth label is considered noise, often remove annotators who disagree instead of spam annotators, introducing suboptimal tradeoffs between accuracy and label diversity. We find that conservative settings for annotator removal (<5%) are best, after which all tested methods increase the mean absolute error from the true average label. We analyze performance on synthetic spam to observe that these methods often assume spam annotators are more random than real spammers tend to be: most spammers are distributionally indistinguishable from real annotators, and the minority that are distinguishable tend to give relatively fixed answers, not random ones. Thus, tasks requiring the preservation of variation reverse the intuition of existing spam filtering methods: spammers tend to be less random than non-spammers, so metrics that assume variation is spam fare worse. These results highlight the need for spam removal methods that account for label diversity.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications</title>
<link>https://arxiv.org/abs/2509.08604</link>
<guid>https://arxiv.org/abs/2509.08604</guid>
<content:encoded><![CDATA[
arXiv:2509.08604v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated significant potential in medicine. To date, LLMs have been widely applied to tasks such as diagnostic assistance, medical question answering, and clinical information synthesis. However, a key open question remains: to what extent do LLMs memorize medical training data. In this study, we present the first comprehensive evaluation of memorization of LLMs in medicine, assessing its prevalence (how frequently it occurs), characteristics (what is memorized), volume (how much content is memorized), and potential downstream impacts (how memorization may affect medical applications). We systematically analyze common adaptation scenarios: (1) continued pretraining on medical corpora, (2) fine-tuning on standard medical benchmarks, and (3) fine-tuning on real-world clinical data, including over 13,000 unique inpatient records from Yale New Haven Health System. The results demonstrate that memorization is prevalent across all adaptation scenarios and significantly higher than reported in the general domain. Memorization affects both the development and adoption of LLMs in medicine and can be categorized into three types: beneficial (e.g., accurate recall of clinical guidelines and biomedical references), uninformative (e.g., repeated disclaimers or templated medical document language), and harmful (e.g., regeneration of dataset-specific or sensitive clinical content). Based on these findings, we offer practical recommendations to facilitate beneficial memorization that enhances domain-specific reasoning and factual accuracy, minimize uninformative memorization to promote deeper learning beyond surface-level patterns, and mitigate harmful memorization to prevent the leakage of sensitive or identifiable patient information.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Findings of the Fourth Shared Task on Multilingual Coreference Resolution: Can LLMs Dethrone Traditional Approaches?</title>
<link>https://arxiv.org/abs/2509.17796</link>
<guid>https://arxiv.org/abs/2509.17796</guid>
<content:encoded><![CDATA[
arXiv:2509.17796v2 Announce Type: replace 
Abstract: The paper presents an overview of the fourth edition of the Shared Task on Multilingual Coreference Resolution, organized as part of the CODI-CRAC 2025 workshop. As in the previous editions, participants were challenged to develop systems that identify mentions and cluster them according to identity coreference.
  A key innovation of this year's task was the introduction of a dedicated Large Language Model (LLM) track, featuring a simplified plaintext format designed to be more suitable for LLMs than the original CoNLL-U representation.
  The task also expanded its coverage with three new datasets in two additional languages, using version 1.3 of CorefUD - a harmonized multilingual collection of 22 datasets in 17 languages.
  In total, nine systems participated, including four LLM-based approaches (two fine-tuned and two using few-shot adaptation). While traditional systems still kept the lead, LLMs showed clear potential, suggesting they may soon challenge established approaches in future editions.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual Coreference Resolution</title>
<link>https://arxiv.org/abs/2509.17858</link>
<guid>https://arxiv.org/abs/2509.17858</guid>
<content:encoded><![CDATA[
arXiv:2509.17858v2 Announce Type: replace 
Abstract: We present CorPipe 25, the winning entry to the CRAC 2025 Shared Task on Multilingual Coreference Resolution. This fourth iteration of the shared task introduces a new LLM track alongside the original unconstrained track, features reduced development and test sets to lower computational requirements, and includes additional datasets. CorPipe 25 represents a complete reimplementation of our previous systems, migrating from TensorFlow to PyTorch. Our system significantly outperforms all other submissions in both the LLM and unconstrained tracks by a substantial margin of 8 percentage points. The source code and trained models are publicly available at https://github.com/ufal/crac2025-corpipe.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Large Language Models To Reason In Parallel With Global Forking Tokens</title>
<link>https://arxiv.org/abs/2510.05132</link>
<guid>https://arxiv.org/abs/2510.05132</guid>
<content:encoded><![CDATA[
arXiv:2510.05132v2 Announce Type: replace 
Abstract: Although LLMs have demonstrated improved performance by scaling parallel test-time compute, doing so relies on generating reasoning paths that are both diverse and accurate. For challenging problems, the forking tokens that trigger diverse yet correct reasoning modes are typically deep in the sampling tree. Consequently, common strategies to encourage diversity, such as temperature scaling, encounter a worsened trade-off between diversity and accuracy. Motivated by this challenge, we treat parallel reasoning as a set-of-next-token-prediction problem, and incorporate a set-based global loss into Supervised Fine-Tuning (SFT) using self-supervised bipartite matching between our global forking tokens and unique reasoning traces. We observe that, while naive fine-tuning with multiple reasoning traces collapses these unique reasoning modes, our proposed method, Set Supervised Fine-Tuning (SSFT), preserves these modes and produces emergent global forking tokens. Experiments on multiple reasoning benchmarks show that our SSFT consistently outperforms SFT under both Pass@1 and Cons@k metrics.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematics with large language models as provers and verifiers</title>
<link>https://arxiv.org/abs/2510.12829</link>
<guid>https://arxiv.org/abs/2510.12829</guid>
<content:encoded><![CDATA[
arXiv:2510.12829v3 Announce Type: replace 
Abstract: During 2024 and 2025 the discussion about the theorem-proving capabilities of large language models started reporting interesting success stories, mostly to do with difficult exercises (such as problems from the International Mathematical Olympiad), but also with conjectures [Feldman & Karbasi, arXiv:2509.18383v1] formulated for the purpose of verifying whether the artificial intelligence could prove it. In this paper we report a theorem proving feat achieved by ChatGPT by using a protocol involving different prover and verifier instances of the gpt-5 model working collaboratively. To make sure that the produced proofs do not suffer from hallucinations, the final proof is formally verified by the lean proof assistant, and the conformance of premises and conclusion of the lean code is verified by a human. Our methodology is by no means complete or exact. It was nonetheless able to solve five out of six 2025 IMO problems, and close about a third of the sixty-six number theory conjectures in [Cohen, Journal of Integer Sequences, 2025].
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models</title>
<link>https://arxiv.org/abs/2503.22879</link>
<guid>https://arxiv.org/abs/2503.22879</guid>
<content:encoded><![CDATA[
arXiv:2503.22879v4 Announce Type: replace-cross 
Abstract: State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input $x$, combined with a per-state-group quantization for input-dependent parameters $B$ and $C$. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms two state-of-the-art SSM quantization methods and delivers 1.3$\times$ and 3$\times$ speed-ups in the pre-filling and generation stages, respectively, while offering 4$\times$ memory reduction with only a $1.6\%$ average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Chest X-ray with Zero-Shot Multi-Task Capability</title>
<link>https://arxiv.org/abs/2504.07416</link>
<guid>https://arxiv.org/abs/2504.07416</guid>
<content:encoded><![CDATA[
arXiv:2504.07416v3 Announce Type: replace-cross 
Abstract: Recent advancements in multimodal models have significantly improved vision-language (VL) alignment in radiology. However, existing approaches struggle to effectively utilize complex radiology reports for learning and offer limited interpretability through attention probability visualizations. To address these challenges, we introduce $\textbf{RadZero}$, a novel framework for VL alignment in chest X-ray with zero-shot multi-task capability. A key component of our approach is $\textbf{VL-CABS}$ ($\textbf{V}$ision-$\textbf{L}$anguage $\textbf{C}$ross-$\textbf{A}$ttention $\textbf{B}$ased on $\textbf{S}$imilarity), which aligns text embeddings with local image features for interpretable, fine-grained VL reasoning. RadZero leverages large language models to extract concise semantic sentences from radiology reports and employs multi-positive contrastive training to effectively capture relationships between images and multiple relevant textual descriptions. It uses a pre-trained vision encoder with additional trainable Transformer layers, allowing efficient high-resolution image processing. By computing similarity between text embeddings and local image patch features, VL-CABS enables zero-shot inference with similarity probability for classification, and pixel-level VL similarity maps for grounding and segmentation. Experimental results on public chest radiograph benchmarks show that RadZero outperforms state-of-the-art methods in zero-shot classification, grounding, and segmentation. Furthermore, VL similarity map analysis highlights the potential of VL-CABS for improving explainability in VL alignment. Additionally, qualitative evaluation demonstrates RadZero's capability for open-vocabulary semantic segmentation, further validating its effectiveness in medical imaging. Code is available at $\href{https://github.com/deepnoid-ai/RadZero}{https://github.com/deepnoid-ai/RadZero}$.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Will Large Language Models Transform Clinical Prediction?</title>
<link>https://arxiv.org/abs/2505.18246</link>
<guid>https://arxiv.org/abs/2505.18246</guid>
<content:encoded><![CDATA[
arXiv:2505.18246v2 Announce Type: replace-cross 
Abstract: Objective: Large language models (LLMs) are attracting increasing interest in healthcare. This commentary evaluates the potential of LLMs to improve clinical prediction models (CPMs) for diagnostic and prognostic tasks, with a focus on their ability to process longitudinal electronic health record (EHR) data.
  Findings: LLMs show promise in handling multimodal and longitudinal EHR data and can support multi-outcome predictions for diverse health conditions. However, methodological, validation, infrastructural, and regulatory chal- lenges remain. These include inadequate methods for time-to-event modelling, poor calibration of predictions, limited external validation, and bias affecting underrepresented groups. High infrastructure costs and the absence of clear regulatory frameworks further prevent adoption.
  Implications: Further work and interdisciplinary collaboration are needed to support equitable and effective integra- tion into the clinical prediction. Developing temporally aware, fair, and explainable models should be a priority focus for transforming clinical prediction workflow.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents</title>
<link>https://arxiv.org/abs/2505.20368</link>
<guid>https://arxiv.org/abs/2505.20368</guid>
<content:encoded><![CDATA[
arXiv:2505.20368v3 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) based large language models (LLMs) are widely used in finance for their excellent performance on knowledge-intensive tasks. However, standardized documents (e.g., SEC filing) share similar formats such as repetitive boilerplate texts, and similar table structures. This similarity forces traditional RAG methods to misidentify near-duplicate text, leading to duplicate retrieval that undermines accuracy and completeness. To address these issues, we propose the Hierarchical Retrieval with Evidence Curation (HiREC) framework. Our approach first performs hierarchical retrieval to reduce confusion among similar texts. It first retrieve related documents and then selects the most relevant passages from the documents. The evidence curation process removes irrelevant passages. When necessary, it automatically generates complementary queries to collect missing information. To evaluate our approach, we construct and release a Large-scale Open-domain Financial (LOFin) question answering benchmark that includes 145,897 SEC documents and 1,595 question-answer pairs. Our code and data are available at https://github.com/deep-over/LOFin-bench-HiREC.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VERA: Variational Inference Framework for Jailbreaking Large Language Models</title>
<link>https://arxiv.org/abs/2506.22666</link>
<guid>https://arxiv.org/abs/2506.22666</guid>
<content:encoded><![CDATA[
arXiv:2506.22666v2 Announce Type: replace-cross 
Abstract: The rise of API-only access to state-of-the-art LLMs highlights the need for effective black-box jailbreak methods to identify model vulnerabilities in real-world settings. Without a principled objective for gradient-based optimization, most existing approaches rely on genetic algorithms, which are limited by their initialization and dependence on manually curated prompt pools. Furthermore, these methods require individual optimization for each prompt, failing to provide a comprehensive characterization of model vulnerabilities. To address this gap, we introduce VERA: Variational infErence fRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a variational inference problem, training a small attacker LLM to approximate the target LLM's posterior over adversarial prompts. Once trained, the attacker can generate diverse, fluent jailbreak prompts for a target query without re-optimization. Experimental results show that VERA achieves strong performance across a range of target LLMs, highlighting the value of probabilistic inference for adversarial prompt generation.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences</title>
<link>https://arxiv.org/abs/2509.16189</link>
<guid>https://arxiv.org/abs/2509.16189</guid>
<content:encoded><![CDATA[
arXiv:2509.16189v2 Announce Type: replace-cross 
Abstract: When do machine learning systems fail to generalize, and what mechanisms could improve their generalization? Here, we draw inspiration from cognitive science to argue that one weakness of parametric machine learning systems is their failure to exhibit latent learning -- learning information that is not relevant to the task at hand, but that might be useful in a future task. We show how this perspective links failures ranging from the reversal curse in language modeling to new findings on agent-based navigation. We then highlight how cognitive science points to episodic memory as a potential part of the solution to these issues. Correspondingly, we show that a system with an oracle retrieval mechanism can use learning experiences more flexibly to generalize better across many of these challenges. We also identify some of the essential components for effectively using retrieval, including the importance of within-example in-context learning for acquiring the ability to use information across retrieved examples. In summary, our results illustrate one possible contributor to the relative data inefficiency of current machine learning systems compared to natural intelligence, and help to understand how retrieval methods can complement parametric learning to improve generalization. We close by discussing some of the links between these findings and prior results in cognitive science and neuroscience, and the broader implications.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Mirror Loop: Recursive Non-Convergence in Generative Reasoning Systems</title>
<link>https://arxiv.org/abs/2510.21861</link>
<guid>https://arxiv.org/abs/2510.21861</guid>
<content:encoded><![CDATA[
<div> reflection, self-correction, generative reasoning, grounding intervention, informational change

Summary:<br />
- Large language models often struggle with recursive self-evaluation, resulting in reformulation rather than progress. 
- Study tested this using three models and four task families, finding a decline in mean informational change without external feedback. 
- Grounded runs, with a minimal grounding intervention, showed a rebound in informational change, indicating the importance of external verification. 
- Measures such as n-gram novelty and embedding drift supported the pattern of reflection without contact leading to epistemic stasis. 
- Results suggest a structural limit on self-correction in generative reasoning and highlight the need for grounded, cooperative reasoning designs. <div>
arXiv:2510.21861v2 Announce Type: replace-cross 
Abstract: Large language models are often described as capable of reflective reasoning, yet recursive self-evaluation without external feedback frequently yields reformulation rather than progress. We test this prediction in a cross-provider study of 144 reasoning sequences across three models (OpenAI GPT-4o-mini, Anthropic Claude 3 Haiku, and Google Gemini 2.0 Flash) and four task families (arithmetic, code, explanation, reflection), each iterated ten times under two conditions: ungrounded self-critique and a minimal grounding intervention (a single verification step at iteration three). Mean informational change (delta I, measured via normalized edit distance) declined by 55% from early (0.193) to late (0.087) iterations in ungrounded runs, with consistent patterns across all three providers. Grounded runs showed a +28% rebound in informational change immediately after the intervention and sustained non-zero variance thereafter. Complementary measures-n-gram novelty, embedding drift, and character-level entropy-converged on the same pattern: reflection without contact tends toward informational closure. We interpret this as evidence for a structural limit on self-correction in generative reasoning: without an exchange of information with an independent verifier or environment, recursive inference approaches an attractor state of epistemic stasis. Minimal grounding functions as dissipative coupling, reintroducing informational flux. The cross-architecture consistency suggests the mirror loop arises from shared autoregressive training objectives rather than provider-specific alignment schemes. The results delineate when reflection is performative rather than epistemic and motivate design principles for grounded, cooperative reasoning. Materials and code are publicly available.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cache Mechanism for Agent RAG Systems</title>
<link>https://arxiv.org/abs/2511.02919</link>
<guid>https://arxiv.org/abs/2511.02919</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Retrieval-Augmented Generation, cache management, agent performance, efficiency

Summary: 
The paper introduces ARC, a novel caching framework for Large Language Model-based agents that dynamically manages compact and relevant corpora for each agent. By analyzing historical query distribution patterns and the embedding space of cached items, ARC automatically maintains a high-relevance cache. Experimental results on three retrieval datasets show that ARC significantly reduces storage requirements to 0.015% of the original corpus while improving the has-answer rate by up to 79.8% and decreasing average retrieval latency by 80%. This demonstrates that ARC enhances the efficiency and effectiveness of Retrieval-Augmented Generation (RAG) powered Large Language Model agents. 

<br /><br />Summary: <div>
arXiv:2511.02919v1 Announce Type: new 
Abstract: Recent advances in Large Language Model (LLM)-based agents have been propelled by Retrieval-Augmented Generation (RAG), which grants the models access to vast external knowledge bases. Despite RAG's success in improving agent performance, agent-level cache management, particularly constructing, maintaining, and updating a compact, relevant corpus dynamically tailored to each agent's need, remains underexplored. Therefore, we introduce ARC (Agent RAG Cache Mechanism), a novel, annotation-free caching framework that dynamically manages small, high-value corpora for each agent. By synthesizing historical query distribution patterns with the intrinsic geometry of cached items in the embedding space, ARC automatically maintains a high-relevance cache. With comprehensive experiments on three retrieval datasets, our experimental results demonstrate that ARC reduces storage requirements to 0.015% of the original corpus while offering up to 79.8% has-answer rate and reducing average retrieval latency by 80%. Our results demonstrate that ARC can drastically enhance efficiency and effectiveness in RAG-powered LLM agents.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Machine Translation Detection Using a Surrogate Multilingual Translation Model</title>
<link>https://arxiv.org/abs/2511.02958</link>
<guid>https://arxiv.org/abs/2511.02958</guid>
<content:encoded><![CDATA[
<div> Keywords: machine translation, parallel corpora, synthetic content, non-human translations, multilingual MT model

Summary:
In the field of machine translation (MT), the reliance on large parallel corpora from the Internet has led to the inclusion of machine-generated translations, impacting translation quality. To address this issue, a new method using internal representations of a multilingual MT model has been proposed to distinguish between human and machine-translated sentences. This approach has shown superior performance compared to existing techniques, especially for non-English language pairs, with an accuracy improvement of at least 5 percentage points. Filtering out non-human translations is crucial for building high-quality MT systems, and this novel method offers a promising solution to enhance translation quality by accurately identifying human-generated translations.1.9292. <div>
arXiv:2511.02958v1 Announce Type: new 
Abstract: Modern machine translation (MT) systems depend on large parallel corpora, often collected from the Internet. However, recent evidence indicates that (i) a substantial portion of these texts are machine-generated translations, and (ii) an overreliance on such synthetic content in training data can significantly degrade translation quality. As a result, filtering out non-human translations is becoming an essential pre-processing step in building high-quality MT systems. In this work, we propose a novel approach that directly exploits the internal representations of a surrogate multilingual MT model to distinguish between human and machine-translated sentences. Experimental results show that our method outperforms current state-of-the-art techniques, particularly for non-English language pairs, achieving gains of at least 5 percentage points of accuracy.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation</title>
<link>https://arxiv.org/abs/2511.03001</link>
<guid>https://arxiv.org/abs/2511.03001</guid>
<content:encoded><![CDATA[
<div> Large Language Models, 3D scene synthesis, fine-grained instructions, LEGO-Eval, LEGO-Bench

Summary:
LEGO-Eval addresses the issue of unrealistic 3D scene generation by introducing a comprehensive evaluation framework that grounds scene components. It outperforms existing methods in assessing alignment between detailed instructions and generated scenes. The LEGO-Bench benchmark highlights limitations in current generation approaches. The importance of realistic scene generation is emphasized to avoid agents learning incorrect priors. Without accurate alignment assessment, embodied agents may struggle in real-world environments. Current evaluation methods like CLIPScore and VLMs fall short due to a shallow understanding of 3D scenes. The experiments show that current approaches struggle to generate scenes aligning with detailed instructions, with success rates not exceeding 10%. Advancing 3D scene synthesis guided by fine-grained instructions is crucial for training agents in realistic environments. <div>
arXiv:2511.03001v1 Announce Type: new 
Abstract: Despite recent progress in using Large Language Models (LLMs) for automatically generating 3D scenes, generated scenes often lack realistic spatial layouts and object attributes found in real-world environments. As this problem stems from insufficiently detailed, coarse-grained instructions, advancing 3D scene synthesis guided by more detailed, fine-grained instructions that reflect real-world environments becomes crucial. Without such realistic scenes, training embodied agents in unrealistic environments can lead them to learn priors that diverge significantly from real-world physics and semantics, degrading their performance when deployed. Thus, verifying the alignment between the fine-grained instruction and the generated scene is essential for effective learning. However, current evaluation methods, such as CLIPScore and vision-language models (VLMs), often fail to reliably assess such alignment. This shortcoming arises primarily from their shallow understanding of 3D scenes, which often leads to improperly grounded scene components. To address this, we introduce LEGO-Eval, an evaluation framework equipped with diverse tools designed to explicitly ground scene components, enabling more accurate alignment assessments. We also present LEGO-Bench, a benchmark of detailed instructions that specify complex layouts and attributes of real-world environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with LEGO-Bench reveals significant limitations in current generation methods. Across all evaluated approaches, success rates reached at most 10% in generating scenes that fully align with fine-grained instructions.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Targeted Error Correction in Knowledge Distillation: Small Language Models Surpass GPT</title>
<link>https://arxiv.org/abs/2511.03005</link>
<guid>https://arxiv.org/abs/2511.03005</guid>
<content:encoded><![CDATA[
<div> Keywords: Analyze-Revise-Finetune, language models, customer service, summarization, data privacy<br />
Summary:<br />
The article introduces the Analyze-Revise-Finetune (ARF) pipeline, which boosts the performance of smaller open-source language models (LLMs) in customer service summarization tasks. By analyzing errors in summaries generated by a larger model (GPT-3.5), a compact editor model (Llama 3.1 70B) is used to refine the training data, leading to improved summarization by a smaller student model (Llama 3.1 8B). The ARF pipeline enhances cost efficiency and data privacy while maintaining competitive accuracy. This framework can potentially be applied to enhance open-source LLMs in various downstream applications. <br /><br />Summary: <div>
arXiv:2511.03005v1 Announce Type: new 
Abstract: We introduce an Analyze-Revise-Finetune (ARF) pipeline that enables smaller open-source language models (LLMs) to surpass substantially larger proprietary models in customer service summarization tasks. The pipeline first analyzes and categorizes common errors in summaries produced by a teacher model (GPT-3.5), then performs a targeted revision using a compact editor model (Llama 3.1 70B) to generate high-quality, refined training data. Fine-tuning a smaller student model (Llama 3.1 8B) on this refined data resulted in superior summarization performance compared to GPT-3.5. The ARF pipeline improves cost efficiency and data privacy while maintaining competitive accuracy, illustrating a generalizable framework for enhancing open-source LLMs across diverse downstream applications.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2511.03034</link>
<guid>https://arxiv.org/abs/2511.03034</guid>
<content:encoded><![CDATA[
<div> Evaluation Method, Decoder-only Generative Models, Fine-tuning, Multitask Learning, Education Review

Summary: 
1) A novel evaluation method, FTS-OBP, is proposed to address the rigidity of traditional evaluation methods for ABSA tasks, allowing for realistic boundary variations and providing fine-grained diagnostics.
2) Small decoder-only generative language models are studied for ABSA, exploring resource lower bounds in education review ABSA. Data-free and data-light fine-tuning methods are examined, with a multitask fine-tuning strategy significantly improving performance of smaller models.
3) The first public set of education review ABSA resources is released to support research in low-resource domains. <div>
arXiv:2511.03034v1 Announce Type: new 
Abstract: Aspect-based Sentiment Analysis (ABSA) is a fine-grained opinion mining approach that identifies and classifies opinions associated with specific entities (aspects) or their categories within a sentence. Despite its rapid growth and broad potential, ABSA research and resources remain concentrated in commercial domains, leaving analytical needs unmet in high-demand yet low-resource areas such as education and healthcare. Domain adaptation challenges and most existing methods' reliance on resource-intensive in-training knowledge injection further hinder progress in these areas. Moreover, traditional evaluation methods based on exact matches are overly rigid for ABSA tasks, penalising any boundary variations which may misrepresent the performance of generative models. This work addresses these gaps through three contributions: 1) We propose a novel evaluation method, Flexible Text Similarity Matching and Optimal Bipartite Pairing (FTS-OBP), which accommodates realistic extraction boundary variations while maintaining strong correlation with traditional metrics and offering fine-grained diagnostics. 2) We present the first ABSA study of small decoder-only generative language models (SLMs; <7B parameters), examining resource lower bounds via a case study in education review ABSA. We systematically explore data-free (in-context learning and weight merging) and data-light fine-tuning methods, and propose a multitask fine-tuning strategy that significantly enhances SLM performance, enabling 1.5-3.8 B models to surpass proprietary large models and approach benchmark results with only 200-1,000 examples on a single GPU. 3) We release the first public set of education review ABSA resources to support future research in low-resource domains.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ROBoto2: An Interactive System and Dataset for LLM-assisted Clinical Trial Risk of Bias Assessment</title>
<link>https://arxiv.org/abs/2511.03048</link>
<guid>https://arxiv.org/abs/2511.03048</guid>
<content:encoded><![CDATA[
<div> Keywords: ROBOTO2, language model, risk of bias assessment, clinical trials, dataset

Summary:
The article introduces ROBOTO2, a web-based platform designed to assist in assessing the risk of bias in clinical trials using large language models (LLMs). This platform aims to simplify the labor-intensive process of risk of bias (ROB) assessment by combining PDF parsing, LLM prompts, and human-in-the-loop review. Users can upload clinical trial reports, receive preliminary answers to ROB signaling questions, and provide feedback to improve the system's suggestions in real-time. The ROBOTO2 platform is publicly available, with code and data released for reproducibility and adoption. A dataset of 521 pediatric clinical trial reports annotated using both manual and LLM-assisted methods is also provided, serving as a benchmark for future research. The article evaluates the performance of four LLMs on the ROB2 assessment using this dataset, highlighting current model capabilities and challenges in automating this critical aspect of systematic review.

<br /><br />Summary: The article introduces ROBOTO2, a web-based platform for LLM-assisted risk of bias assessment in clinical trials. The platform combines PDF parsing, LLM prompts, and human-in-the-loop review to streamline the ROB2 annotation process. Users can upload trial reports, receive preliminary answers, and provide feedback for system suggestions. The platform is publicly available with released code and data, including a dataset of pediatric trial reports for benchmarking and research. Performance of four LLMs on ROB2 assessment is evaluated, shedding light on model capabilities and challenges in automating systematic reviews. <div>
arXiv:2511.03048v1 Announce Type: new 
Abstract: We present ROBOTO2, an open-source, web-based platform for large language model (LLM)-assisted risk of bias (ROB) assessment of clinical trials. ROBOTO2 streamlines the traditionally labor-intensive ROB v2 (ROB2) annotation process via an interactive interface that combines PDF parsing, retrieval-augmented LLM prompting, and human-in-the-loop review. Users can upload clinical trial reports, receive preliminary answers and supporting evidence for ROB2 signaling questions, and provide real-time feedback or corrections to system suggestions. ROBOTO2 is publicly available at https://roboto2.vercel.app/, with code and data released to foster reproducibility and adoption. We construct and release a dataset of 521 pediatric clinical trial reports (8954 signaling questions with 1202 evidence passages), annotated using both manually and LLM-assisted methods, serving as a benchmark and enabling future research. Using this dataset, we benchmark ROB2 performance for 4 LLMs and provide an analysis into current model capabilities and ongoing challenges in automating this critical aspect of systematic review.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reading Between the Lines: The One-Sided Conversation Problem</title>
<link>https://arxiv.org/abs/2511.03056</link>
<guid>https://arxiv.org/abs/2511.03056</guid>
<content:encoded><![CDATA[
<div> Keywords: Conversational AI, one-sided conversation problem, reconstruction, summarization, privacy-aware conversational AI

Summary: 
Conversational AI often faces limitations in real-world scenarios where only one side of a dialogue can be recorded, such as in telemedicine or call centers. The one-sided conversation problem (1SC) is formalized to address this challenge. Two tasks are focused on: reconstructing missing speaker turns in real-time scenarios and generating summaries from one-sided transcripts. Through evaluations on various datasets using different models and metrics, it was found that access to future turns and information on utterance length enhances the reconstruction process. Placeholder prompting helps reduce hallucinations, especially for smaller models that require fine-tuning. Moreover, high-quality summaries can be generated without reconstructing missing turns. This study presents promising results that signify progress towards privacy-aware conversational AI solutions.<br /><br />Summary: <div>
arXiv:2511.03056v1 Announce Type: new 
Abstract: Conversational AI is constrained in many real-world settings where only one side of a dialogue can be recorded, such as telemedicine, call centers, and smart glasses. We formalize this as the one-sided conversation problem (1SC): inferring and learning from one side of a conversation. We study two tasks: (1) reconstructing the missing speaker's turns for real-time use cases, and (2) generating summaries from one-sided transcripts. Evaluating prompting and finetuned models on MultiWOZ, DailyDialog, and Candor with both human A/B testing and LLM-as-a-judge metrics, we find that access to one future turn and information about utterance length improves reconstruction, placeholder prompting helps to mitigate hallucination, and while large models generate promising reconstructions with prompting, smaller models require finetuning. Further, high-quality summaries can be generated without reconstructing missing turns. We present 1SC as a novel challenge and report promising results that mark a step toward privacy-aware conversational AI.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PolyNorm: Few-Shot LLM-Based Text Normalization for Text-to-Speech</title>
<link>https://arxiv.org/abs/2511.03080</link>
<guid>https://arxiv.org/abs/2511.03080</guid>
<content:encoded><![CDATA[
<div> Keywords: Text Normalization, Large Language Models, PolyNorm, data curation, word error rate

Summary:
PolyNorm proposes a prompt-based Text Normalization (TN) approach using Large Language Models (LLMs) to reduce manual engineering efforts and improve linguistic applicability. It aims to address challenges in traditional TN systems by leveraging machine learning techniques. A language-agnostic pipeline for automatic data curation and evaluation is introduced to support scalable experimentation across multiple languages. Experiments conducted across eight languages demonstrate consistent reductions in word error rate (WER) compared to production-grade systems. The release of PolyNorm-Benchmark, a multilingual dataset, facilitates further research and benchmarking in text normalization. This approach shows promise in enhancing the efficiency and effectiveness of TN processes, especially in low-resource settings. <br /><br />Summary: <div>
arXiv:2511.03080v1 Announce Type: new 
Abstract: Text Normalization (TN) is a key preprocessing step in Text-to-Speech (TTS) systems, converting written forms into their canonical spoken equivalents. Traditional TN systems can exhibit high accuracy, but involve substantial engineering effort, are difficult to scale, and pose challenges to language coverage, particularly in low-resource settings. We propose PolyNorm, a prompt-based approach to TN using Large Language Models (LLMs), aiming to reduce the reliance on manually crafted rules and enable broader linguistic applicability with minimal human intervention. Additionally, we present a language-agnostic pipeline for automatic data curation and evaluation, designed to facilitate scalable experimentation across diverse languages. Experiments across eight languages show consistent reductions in the word error rate (WER) compared to a production-grade-based system. To support further research, we release PolyNorm-Benchmark, a multilingual data set covering a diverse range of text normalization phenomena.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Computational Approach to Analyzing Disrupted Language in Schizophrenia: Integrating Surprisal and Coherence Measures</title>
<link>https://arxiv.org/abs/2511.03089</link>
<guid>https://arxiv.org/abs/2511.03089</guid>
<content:encoded><![CDATA[
<div> Keywords: language disruptions, schizophrenia, computational linguistics, surprisal, semantic coherence

Summary:<br /><br />
This study explores language disruptions in individuals with schizophrenia by analyzing computational linguistic measures such as surprisal and semantic coherence. These disruptions, often seen as disorganized speech and impaired discourse coherence, are potential markers for symptom severity and diagnosis of schizophrenia. By comparing these linguistic measures between subjects with schizophrenia and healthy controls, the study aims to understand the differences in language production. Additionally, the research delves into how these measures change with varying levels of schizophrenia symptom severity. Overall, the study provides valuable insights into the cognitive disturbances underlying language disruptions in schizophrenia and highlights the use of computational linguistics in objectively assessing these symptoms. <div>
arXiv:2511.03089v1 Announce Type: new 
Abstract: Language disruptions are one of the well-known effects of schizophrenia symptoms. They are often manifested as disorganized speech and impaired discourse coherence. These abnormalities in spontaneous language production reflect underlying cognitive disturbances and have the potential to serve as objective markers for symptom severity and diagnosis of schizophrenia. This study focuses on how these language disruptions can be characterized in terms of two computational linguistic measures: surprisal and semantic coherence. By computing surprisal and semantic coherence of language using computational models, this study investigates how they differ between subjects with schizophrenia and healthy controls. Furthermore, this study provides further insight into how language disruptions in terms of these linguistic measures change with varying degrees of schizophrenia symptom severity.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARMA: Comprehensive Automatically-annotated Reddit Mental Health Dataset for Arabic</title>
<link>https://arxiv.org/abs/2511.03102</link>
<guid>https://arxiv.org/abs/2511.03102</guid>
<content:encoded><![CDATA[
<div> Automatic Annotation, Arabic Reddit Posts, Mental Health Conditions, Lexical and Semantic Differences, Classification Experiments <br />
<br />
Summary: The study introduces CARMA, the first large-scale Arabic dataset of Reddit posts annotated for mental health conditions. This resource addresses the lack of annotated datasets in Arabic, offering insights into linguistic markers of mental health conditions. The dataset includes six conditions and a control group, surpassing existing resources in scale and diversity. Qualitative and quantitative analyses reveal lexical and semantic differences among users, shedding light on specific mental health conditions. Classification experiments using various models demonstrate the dataset's potential for advancing mental health detection in underrepresented languages like Arabic. This research contributes to addressing the challenges of early detection of mental health disorders in Arabic-speaking populations where resources are limited and stigma hinders discourse. <div>
arXiv:2511.03102v1 Announce Type: new 
Abstract: Mental health disorders affect millions worldwide, yet early detection remains a major challenge, particularly for Arabic-speaking populations where resources are limited and mental health discourse is often discouraged due to cultural stigma. While substantial research has focused on English-language mental health detection, Arabic remains significantly underexplored, partly due to the scarcity of annotated datasets. We present CARMA, the first automatically annotated large-scale dataset of Arabic Reddit posts. The dataset encompasses six mental health conditions, such as Anxiety, Autism, and Depression, and a control group. CARMA surpasses existing resources in both scale and diversity. We conduct qualitative and quantitative analyses of lexical and semantic differences between users, providing insights into the linguistic markers of specific mental health conditions. To demonstrate the dataset's potential for further mental health analysis, we perform classification experiments using a range of models, from shallow classifiers to large language models. Our results highlight the promise of advancing mental health detection in underrepresented languages such as Arabic.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Control Barrier Function for Aligning Large Language Models</title>
<link>https://arxiv.org/abs/2511.03121</link>
<guid>https://arxiv.org/abs/2511.03121</guid>
<content:encoded><![CDATA[
<div> control barrier function, language models, text generation, alignment, safety filter

Summary:
This paper introduces a control-based framework for aligning large language models (LLMs) using a control barrier function (CBF) to ensure user-desirable text generation. The framework incorporates a safety filter based on the CBF, allowing intervention in the generated text without the need to fine-tune the baseline LLM. The safety filter offers the advantage of being an add-on type, enabling easy integration for alignment purposes. Additionally, if an evaluation model for desired alignment is available, it can be directly applied to the filter design. The text-generation system implemented with open-source language models aims to produce positive text through these control mechanisms. <div>
arXiv:2511.03121v1 Announce Type: new 
Abstract: This paper proposes a control-based framework for aligning large language models (LLMs) by leveraging a control barrier function (CBF) to ensure user-desirable text generation. The presented framework applies the CBF safety filter to the predicted token generated from the baseline LLM, to intervene in the generated text. The safety filter includes two significant advantages: this safety filter is an add-on type, allowing it to be used for alignment purposes without fine-tuning the baseline LLM, and if there is an evaluation model regarding the desired alignment, it can be directly applied to the filter design. The overall text-generation system is implemented with open-source language models, aiming to generate positive text.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity</title>
<link>https://arxiv.org/abs/2511.03146</link>
<guid>https://arxiv.org/abs/2511.03146</guid>
<content:encoded><![CDATA[
<div> benchmark, multimodal, cognitive capacity, reasoning tasks, MLLMs <br />
Summary:
The article introduces the MME-CC benchmark, a vision-grounded evaluation benchmark for assessing the cognitive capacity of multilingual language models (MLLMs) across spatial, geometric, and knowledge-based reasoning tasks. It aims to address the lack of systematic evaluation of MLLMs' cognitive abilities in existing benchmarks. Results from experiments on 16 MLLMs show that closed-source models currently outperform others, emphasizing the importance of cognitive capacity in evaluation and model design. However, spatial and geometric reasoning abilities of MLLMs are still weak, with common error patterns identified such as orientation mistakes and poor adherence to counterfactual instructions. The Chain-of-Thought process in reasoning tasks typically involves three stages extraction, reasoning, and verificationwith heavy reliance on visual information. Overall, the study highlights the need to consider the cognitive capacity of MLLMs in evaluating and improving their performance. <br /><br />Summary: <div>
arXiv:2511.03146v1 Announce Type: new 
Abstract: As reasoning models scale rapidly, the essential role of multimodality in human cognition has come into sharp relief, driving a growing need to probe vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either overemphasize textual reasoning or fall short of systematically capturing vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs insufficiently assessed. To address this limitation, we introduce MME-CC (Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded benchmark that organizes 11 representative reasoning tasks into three fundamental categories of visual information: spatial, geometric, and knowledge-based reasoning, and provides fine-grained analyses of MLLMs' cognitive capacity across these dimensions. Based on MME-CC, we conduct extensive experiments over 16 representative MLLMs. Our study reveals that closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs. 30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak (less than or equal to 30%). We further identify common error patterns, including orientation mistakes, fragile cross-view identity persistence, and poor adherence to counterfactual instructions, and observe that Chain-of-Thought typically follows a three-stage process (extract -> reason -> verify) with heavy reliance on visual extraction. We hope this work catalyzes a shift toward treating the cognitive capacity of MLLMs as central to both evaluation and model design.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment</title>
<link>https://arxiv.org/abs/2511.03152</link>
<guid>https://arxiv.org/abs/2511.03152</guid>
<content:encoded><![CDATA[
<div> Keywords: stakeholder, risk assessment, LLMs, AI systems, transparency

Summary:
This paper introduces a framework for stakeholder-grounded risk assessment in AI systems using Large Language Models (LLMs) as judges to predict and explain risks. By utilizing the Risk Atlas Nexus and GloVE explanation method, the framework generates stakeholder-specific, interpretable policies that showcase agreement or disagreement on risks among stakeholders. Three real-world AI use cases are examined: medical AI, autonomous vehicles, and fraud detection. An interactive visualization is proposed to reveal conflicts across stakeholder perspectives, enhancing transparency in reasoning. The research highlights the significant influence of stakeholder perspectives on risk perception and conflict patterns. The importance of stakeholder-aware explanations is emphasized for making LLM-based evaluations more transparent, interpretable, and aligned with human-centered AI governance goals.<br /><br />Summary: <div>
arXiv:2511.03152v1 Announce Type: new 
Abstract: Understanding how different stakeholders perceive risks in AI systems is essential for their responsible deployment. This paper presents a framework for stakeholder-grounded risk assessment by using LLMs, acting as judges to predict and explain risks. Using the Risk Atlas Nexus and GloVE explanation method, our framework generates stakeholder-specific, interpretable policies that shows how different stakeholders agree or disagree about the same risks. We demonstrate our method using three real-world AI use cases of medical AI, autonomous vehicles, and fraud detection domain. We further propose an interactive visualization that reveals how and why conflicts emerge across stakeholder perspectives, enhancing transparency in conflict reasoning. Our results show that stakeholder perspectives significantly influence risk perception and conflict patterns. Our work emphasizes the importance of these stakeholder-aware explanations needed to make LLM-based evaluations more transparent, interpretable, and aligned with human-centered AI governance goals.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Aleatoric and Epistemic Uncertainty in LLMs: Empirical Evaluation on ID and OOD QA Tasks</title>
<link>https://arxiv.org/abs/2511.03166</link>
<guid>https://arxiv.org/abs/2511.03166</guid>
<content:encoded><![CDATA[
<div> UE measures, Large Language Models, Uncertainty Estimation, Question-Answering, ID datasets, OOD datasets<br />
Summary:<br />
- Study on UE measures for LLM-generated answers in QA tasks
- 12 UE methods evaluated on ID and OOD datasets
- Information-based methods excel in ID settings
- Density-based methods and P(True) metric effective in OOD contexts
- Semantic consistency methods show consistent performance <div>
arXiv:2511.03166v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have become increasingly pervasive, finding applications across many industries and disciplines. Ensuring the trustworthiness of LLM outputs is paramount, where Uncertainty Estimation (UE) plays a key role. In this work, a comprehensive empirical study is conducted to examine the robustness and effectiveness of diverse UE measures regarding aleatoric and epistemic uncertainty in LLMs. It involves twelve different UE methods and four generation quality metrics including LLMScore from LLM criticizers to evaluate the uncertainty of LLM-generated answers in Question-Answering (QA) tasks on both in-distribution (ID) and out-of-distribution (OOD) datasets. Our analysis reveals that information-based methods, which leverage token and sequence probabilities, perform exceptionally well in ID settings due to their alignment with the model's understanding of the data. Conversely, density-based methods and the P(True) metric exhibit superior performance in OOD contexts, highlighting their effectiveness in capturing the model's epistemic uncertainty. Semantic consistency methods, which assess variability in generated answers, show reliable performance across different datasets and generation metrics. These methods generally perform well but may not be optimal for every situation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BengaliMoralBench: A Benchmark for Auditing Moral Reasoning in Large Language Models within Bengali Language and Culture</title>
<link>https://arxiv.org/abs/2511.03180</link>
<guid>https://arxiv.org/abs/2511.03180</guid>
<content:encoded><![CDATA[
<div> Keywords: BengaliMoralBench, Large Language Models, ethics benchmark, multilingual, cultural alignment<br />
Summary: 
BengaliMoralBench is introduced as the first ethics benchmark for the Bengali language, focusing on five moral domains and 50 culturally relevant subtopics. The benchmark aims to address the lack of cultural alignment in existing ethics benchmarks, particularly for South Asian languages like Bengali. The study evaluates the performance of multilingual Large Language Models (LLMs), such as Llama, Gemma, Qwen, and DeepSeek, on the BengaliMoralBench using a unified prompting protocol. Results show varying accuracy levels (50-91%) across models, highlighting weaknesses in cultural grounding, commonsense reasoning, and moral fairness. The benchmark serves as a foundation for responsible localization and aims to support the deployment of ethically robust AI in diverse, low-resource multilingual settings like Bangladesh.<br /><br />Summary: <div>
arXiv:2511.03180v1 Announce Type: new 
Abstract: As multilingual Large Language Models (LLMs) gain traction across South Asia, their alignment with local ethical norms, particularly for Bengali, which is spoken by over 285 million people and ranked 6th globally, remains underexplored. Existing ethics benchmarks are largely English-centric and shaped by Western frameworks, overlooking cultural nuances critical for real-world deployment. To address this, we introduce BengaliMoralBench, the first large-scale ethics benchmark for the Bengali language and socio-cultural contexts. It covers five moral domains, Daily Activities, Habits, Parenting, Family Relationships, and Religious Activities, subdivided into 50 culturally relevant subtopics. Each scenario is annotated via native-speaker consensus using three ethical lenses: Virtue, Commonsense, and Justice ethics. We conduct systematic zero-shot evaluation of prominent multilingual LLMs, including Llama, Gemma, Qwen, and DeepSeek, using a unified prompting protocol and standard metrics. Performance varies widely (50-91% accuracy), with qualitative analysis revealing consistent weaknesses in cultural grounding, commonsense reasoning, and moral fairness. BengaliMoralBench provides a foundation for responsible localization, enabling culturally aligned evaluation and supporting the deployment of ethically robust AI in diverse, low-resource multilingual settings such as Bangladesh.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LGM: Enhancing Large Language Models with Conceptual Meta-Relations and Iterative Retrieval</title>
<link>https://arxiv.org/abs/2511.03214</link>
<guid>https://arxiv.org/abs/2511.03214</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Language Graph Model, meta-relations, Concept Iterative Retrieval Algorithm, Retrieval-Augmented Generation

Summary:
The Language Graph Model (LGM) aims to enhance conceptual clarity in large language models (LLMs) by extracting meta-relations such as inheritance, alias, and composition from natural language. The model incorporates a reflection mechanism to validate these relationships and dynamically supplies them to the LLM using a Concept Iterative Retrieval Algorithm. This method improves the LLM's ability to interpret concepts and generate accurate responses without the need for text truncation. Unlike conventional Retrieval-Augmented Generation (RAG) approaches that rely on extended context windows, the LGM enables LLMs to process texts of any length effectively. Experimental results on standard benchmarks show that the LGM consistently outperforms existing RAG baselines. <div>
arXiv:2511.03214v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit strong semantic understanding, yet struggle when user instructions involve ambiguous or conceptually misaligned terms. We propose the Language Graph Model (LGM) to enhance conceptual clarity by extracting meta-relations-inheritance, alias, and composition-from natural language. The model further employs a reflection mechanism to validate these meta-relations. Leveraging a Concept Iterative Retrieval Algorithm, these relations and related descriptions are dynamically supplied to the LLM, improving its ability to interpret concepts and generate accurate responses. Unlike conventional Retrieval-Augmented Generation (RAG) approaches that rely on extended context windows, our method enables large language models to process texts of any length without the need for truncation. Experiments on standard benchmarks demonstrate that the LGM consistently outperforms existing RAG baselines.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Fact-Checking that Integrates Knowledge Graphs, Large Language Models, and Search-Based Retrieval Agents Improves Interpretable Claim Verification</title>
<link>https://arxiv.org/abs/2511.03217</link>
<guid>https://arxiv.org/abs/2511.03217</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, fact-checking, language models, hybrid approach, FEVER benchmark 

Summary:
A new hybrid fact-checking approach is proposed, combining large language models (LLMs) with knowledge graphs and real-time search agents. The system consists of three steps: Knowledge Graph retrieval, LM-based classification with task-specific prompts, and Web Search Agent utilization. Achieving an F1 score of 0.93 on the FEVER benchmark, the pipeline addresses "Not Enough Information" cases effectively through targeted reannotation studies. The approach uncovers valid evidence for initially labeled NEI claims, validated by expert annotators and LLM reviewers. The system is modular, open-source, with fallback strategies, and generalization across datasets. <div>
arXiv:2511.03217v1 Announce Type: new 
Abstract: Large language models (LLMs) excel in generating fluent utterances but can lack reliable grounding in verified information. At the same time, knowledge-graph-based fact-checkers deliver precise and interpretable evidence, yet suffer from limited coverage or latency. By integrating LLMs with knowledge graphs and real-time search agents, we introduce a hybrid fact-checking approach that leverages the individual strengths of each component. Our system comprises three autonomous steps: 1) a Knowledge Graph (KG) Retrieval for rapid one - hop lookups in DBpedia, 2) an LM-based classification guided by a task-specific labeling prompt, producing outputs with internal rule-based logic, and 3) a Web Search Agent invoked only when KG coverage is insufficient. Our pipeline achieves an F1 score of 0.93 on the FEVER benchmark on the Supported/Refuted split without task- specific fine - tuning. To address Not enough information cases, we conduct a targeted reannotation study showing that our approach frequently uncovers valid evidence for claims originally labeled as Not Enough Information (NEI), as confirmed by both expert annotators and LLM reviewers. With this paper, we present a modular, opensource fact-checking pipeline with fallback strategies and generalization across datasets.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Ranked Lists: The SARAL Framework for Cross-Lingual Document Set Retrieval</title>
<link>https://arxiv.org/abs/2511.03228</link>
<guid>https://arxiv.org/abs/2511.03228</guid>
<content:encoded><![CDATA[
<div> Keywords: Machine Translation, Cross-lingual Information Retrieval, Summarization, SARAL, MATERIAL

Summary:
SARAL's effort for the MATERIAL initiative focused on advancing cross-lingual information retrieval (CLIR) by developing a novel approach. The team's emphasis was on retrieving a query-relevant document set, rather than just a ranked list of documents. In the Phase-3 evaluations of MATERIAL, SARAL outperformed other teams in five out of six evaluation conditions across three different languages: Farsi, Kazakh, and Georgian. This success highlights the effectiveness of SARAL's approach in handling CLIR. The initiative, led by the Information Sciences Institute (ISI), aims to improve the state of CLIR using machine translation techniques. The results from SARAL's efforts demonstrate significant progress in achieving this goal, showcasing the potential for further advancements in the field of cross-lingual information retrieval. <div>
arXiv:2511.03228v1 Announce Type: new 
Abstract: Machine Translation for English Retrieval of Information in Any Language (MATERIAL) is an IARPA initiative targeted to advance the state of cross-lingual information retrieval (CLIR). This report provides a detailed description of Information Sciences Institute's (ISI's) Summarization and domain-Adaptive Retrieval Across Language's (SARAL's) effort for MATERIAL. Specifically, we outline our team's novel approach to handle CLIR with emphasis in developing an approach amenable to retrieve a query-relevant document \textit{set}, and not just a ranked document-list. In MATERIAL's Phase-3 evaluations, SARAL exceeded the performance of other teams in five out of six evaluation conditions spanning three different languages (Farsi, Kazakh, and Georgian).
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IndicSuperTokenizer: An Optimized Tokenizer for Indic Multilingual LLMs</title>
<link>https://arxiv.org/abs/2511.03237</link>
<guid>https://arxiv.org/abs/2511.03237</guid>
<content:encoded><![CDATA[
<div> Keywords: Tokenizers, Large Language Models, Multilingual, Indic, Subword

Summary: 
Tokenizers are essential for the performance of Large Language Models (LLMs), particularly in multilingual settings where scripts and morphology vary. The newly developed IndicSuperTokenizer combines subword and multi-word tokenization with language-specific pre-tokenization, resulting in linguistically aligned tokens and achieving a new state-of-the-art fertility score. Evaluations across English, 22 Indian languages, and code data show a significant improvement in the average fertility score compared to existing models. The tokenizer enhances inference throughput by 44% over previous models while maintaining performance on benchmark tests. Ablations conducted on training data size, vocabulary size, merging techniques, and pre-tokenization strategies demonstrate the robustness of the design choices, highlighting the effectiveness of the IndicSuperTokenizer in enhancing the efficiency and performance of multilingual LLMs.

<br /><br />Summary: <div>
arXiv:2511.03237v1 Announce Type: new 
Abstract: Tokenizers play a crucial role in determining the performance, training efficiency, and the inference cost of Large Language Models (LLMs). Designing effective tokenizers for multilingual LLMs is particularly challenging due to diverse scripts and rich morphological variation. While subword methods such as Byte Pair Encoding (BPE) are widely adopted, their effectiveness in multilingual settings remains underexplored. We present IndicSuperTokenizer, a tokenizer for Indic multilingual LLMs, that combines both subword and multi-word tokenization, along with language-specific pre-tokenization, leading to more linguistically aligned tokens and achieving a new state-of-the-art in fertility score. Evaluated across English, 22 Indian languages and code data, our tokenizer improves the average fertility score by 39.5% over LLaMA4 and by 18% over Sutra (the current best). This translates to 44% improvement in inference throughput over LLaMA4 while maintaining comparable performance on English and Indic benchmarks. We also present detailed ablations across tokenizer training data size, vocabulary size, merging techniques, and pre-tokenization strategies, demonstrating the robustness of our design choices.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature</title>
<link>https://arxiv.org/abs/2511.03261</link>
<guid>https://arxiv.org/abs/2511.03261</guid>
<content:encoded><![CDATA[
<div> Retrieval Augmented Generation, Generative AI models, Large Language Models, question-answering, Mistral-7b-instruct<br />
<br />
Summary: 
The study compares the performance of four open-source Large Language Models (LLMs) alongside OpenAI's GPT-3.5 in question-answering tasks within the computer science literature using Retrieval Augmented Generation (RAG) support. Evaluation metrics such as accuracy, precision for binary questions, ranking by human expert and Google's AI model Gemini, and cosine similarity for long-answer questions were utilized. GPT-3.5 demonstrated strong performance in binary and long-answer question answering when paired with RAG. Mistral-7b-instruct outperformed other open-source LLMs in both binary and long-answer question tasks. Orca-mini-v3-7b reported the shortest response latency, while LLaMa2-7b-chat exhibited the highest latency in generating responses. The research suggests that open-source LLMs can complement proprietary models like GPT-3.5 with improved infrastructure. <br /><br />Summary: <div>
arXiv:2511.03261v1 Announce Type: new 
Abstract: Retrieval Augmented Generation (RAG) is emerging as a powerful technique to enhance the capabilities of Generative AI models by reducing hallucination. Thus, the increasing prominence of RAG alongside Large Language Models (LLMs) has sparked interest in comparing the performance of different LLMs in question-answering (QA) in diverse domains. This study compares the performance of four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat, Falcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA tasks within the computer science literature leveraging RAG support. Evaluation metrics employed in the study include accuracy and precision for binary questions and ranking by a human expert, ranking by Google's AI model Gemini, alongside cosine similarity for long-answer questions. GPT-3.5, when paired with RAG, effectively answers binary and long-answer questions, reaffirming its status as an advanced LLM. Regarding open-source LLMs, Mistral AI's Mistral-7b-instruct paired with RAG surpasses the rest in answering both binary and long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b reports the shortest average latency in generating responses, whereas LLaMa2-7b-chat by Meta reports the highest average latency. This research underscores the fact that open-source LLMs, too, can go hand in hand with proprietary models like GPT-3.5 with better infrastructure.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCALE: Upscaled Continual Learning of Large Language Models</title>
<link>https://arxiv.org/abs/2511.03270</link>
<guid>https://arxiv.org/abs/2511.03270</guid>
<content:encoded><![CDATA[
<div> SCALE, continual pre-training, large language models, width upscaling architecture, preservation, adaptation

Summary:
Continual pre-training for large language models benefits more from scaling the right structure than scaling parameters alone. SCALE, a width upscaling architecture, enhances model capacity by inserting lightweight expansion into linear modules while maintaining the original model's functionality. Guided by Persistent Preservation and Collaborative Adaptation principles, SCALE variants like SCALE-Preserve, SCALE-Adapt, and SCALE-Route balance preservation of pre-trained weights with adaptation for acquiring new knowledge. This approach mitigates forgetting and achieves stability-plasticity trade-offs in continual pre-training experiments on Korean and English corpora. Analysis confirms preservation's reliability and highlights the optimization benefits of the interplay between preservation and adaptation in improving model performance. <div>
arXiv:2511.03270v1 Announce Type: new 
Abstract: We revisit continual pre-training for large language models and argue that progress now depends more on scaling the right structure than on scaling parameters alone. We introduce SCALE, a width upscaling architecture that inserts lightweight expansion into linear modules while freezing all pre-trained parameters. This preserves the residual and attention topologies and increases capacity without perturbing the base model's original functionality. SCALE is guided by two principles: Persistent Preservation, which maintains the base model's behavior via preservation-oriented initialization and freezing of the pre-trained weights, and Collaborative Adaptation, which selectively trains a subset of expansion components to acquire new knowledge with minimal interference. We instantiate these ideas as SCALE-Preserve (preservation-first), SCALE-Adapt (adaptation-first), and SCALE-Route, an optional routing extension that performs token-level routing between preservation and adaptation heads. On a controlled synthetic biography benchmark, SCALE mitigates the severe forgetting observed with depth expansion while still acquiring new knowledge. In continual pre-training on a Korean corpus, SCALE variants achieve less forgetting on English evaluations and competitive gains on Korean benchmarks, with these variants offering the best overall stability-plasticity trade-off. Accompanying analysis clarifies when preservation provably holds and why the interplay between preservation and adaptation stabilizes optimization compared to standard continual learning setups.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Evaluate Speech Translation with Source-Aware Neural MT Metrics</title>
<link>https://arxiv.org/abs/2511.03295</link>
<guid>https://arxiv.org/abs/2511.03295</guid>
<content:encoded><![CDATA[
<div> Keywords: speech-to-text translation, automatic evaluation, neural metrics, source-aware metrics, cross-lingual re-segmentation algorithm

Summary:<br />
- The study explores source-aware metrics for speech-to-text translation, focusing on real-world scenarios without source transcripts.
- Two strategies are analyzed for generating synthetic sources: automatic speech recognition (ASR) transcripts and back-translations of reference translations.
- A novel cross-lingual re-segmentation algorithm addresses alignment issues between synthetic sources and references.
- ASR transcripts are found to be a more reliable synthetic source than back-translations when the word error rate is below 20%.
- The cross-lingual re-segmentation algorithm enables the robust use of source-aware machine translation metrics in speech translation evaluation.
<br /><br />Summary: <div>
arXiv:2511.03295v1 Announce Type: new 
Abstract: Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks</title>
<link>https://arxiv.org/abs/2511.03328</link>
<guid>https://arxiv.org/abs/2511.03328</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, reasoning MLLMs, clinical tasks, model performance, medical applications

Summary: 
This study examines the impact of "thinking mode" capabilities in Multimodal Large Language Models (MLLMs) in clinical tasks. Two leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, were evaluated on visual medical tasks using VQA-RAD and ROCOv2 datasets. The study found that the activation of the thinking mode did not significantly improve performance compared to the standard non-thinking mode for most tasks. In particular, the models' performance on complex medical tasks like open-ended VQA and medical image interpretation was suboptimal, indicating the need for domain-specific medical data and advanced methods for medical knowledge integration. This research highlights the challenges in applying reasoning MLLMs to medical applications and the importance of developing specialized approaches for accurate and reliable clinical task performance. 

<br /><br />Summary: <div>
arXiv:2511.03328v1 Announce Type: new 
Abstract: A recent advancement in Multimodal Large Language Models (MLLMs) research is the emergence of "reasoning MLLMs" that offer explicit control over their internal thinking processes (normally referred as the "thinking mode") alongside the standard "non-thinking mode". This capability allows these models to engage in a step-by-step process of internal deliberation before generating a final response. With the rapid transition to and adoption of these "dual-state" MLLMs, this work rigorously evaluated how the enhanced reasoning processes of these MLLMs impact model performance and reliability in clinical tasks. This paper evaluates the active "thinking mode" capabilities of two leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We assessed their performance on four visual medical tasks using VQA-RAD and ROCOv2 datasets. Our findings reveal that the improvement from activating the thinking mode remains marginal compared to the standard non-thinking mode for the majority of the tasks. Their performance on complex medical tasks such as open-ended VQA and medical image interpretation remains suboptimal, highlighting the need for domain-specific medical data and more advanced methods for medical knowledge integration.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Artificial Intelligence in Bioinformatics: A Systematic Review of Models, Applications, and Methodological Advances</title>
<link>https://arxiv.org/abs/2511.03354</link>
<guid>https://arxiv.org/abs/2511.03354</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative artificial intelligence, bioinformatics, methodological advancement, predictive performance, data integration

Summary:<br /><br />Generative artificial intelligence (GenAI) has significantly impacted bioinformatics, with advancements in genomics, proteomics, and drug discovery. This review identified six research questions to evaluate the progress of GenAI. It found that GenAI applications in bioinformatics, such as sequence analysis and molecular design, outperform traditional methods through pattern recognition. Specialized model architectures, with targeted pretraining, show better performance than general-purpose models. GenAI benefits molecular analysis and data integration, enhancing accuracy and reducing errors. Improvements in structural modeling and functional prediction are evident, with validated benchmarks. Constraints include scalability issues and data biases affecting generalizability. Future directions focus on robust evaluation and biologically grounded modeling. Molecular, cellular, and textual datasets support the training and generalization of GenAI models in bioinformatics. <div>
arXiv:2511.03354v1 Announce Type: new 
Abstract: Generative artificial intelligence (GenAI) has become a transformative approach in bioinformatics that often enables advancements in genomics, proteomics, transcriptomics, structural biology, and drug discovery. To systematically identify and evaluate these growing developments, this review proposed six research questions (RQs), according to the preferred reporting items for systematic reviews and meta-analysis methods. The objective is to evaluate impactful GenAI strategies in methodological advancement, predictive performance, and specialization, and to identify promising approaches for advanced modeling, data-intensive discovery, and integrative biological analysis. RQ1 highlights diverse applications across multiple bioinformatics subfields (sequence analysis, molecular design, and integrative data modeling), which demonstrate superior performance over traditional methods through pattern recognition and output generation. RQ2 reveals that adapted specialized model architectures outperformed general-purpose models, an advantage attributed to targeted pretraining and context-aware strategies. RQ3 identifies significant benefits in the bioinformatics domains, focusing on molecular analysis and data integration, which improves accuracy and reduces errors in complex analysis. RQ4 indicates improvements in structural modeling, functional prediction, and synthetic data generation, validated by established benchmarks. RQ5 suggests the main constraints, such as the lack of scalability and biases in data that impact generalizability, and proposes future directions focused on robust evaluation and biologically grounded modeling. RQ6 examines that molecular datasets (such as UniProtKB and ProteinNet12), cellular datasets (such as CELLxGENE and GTEx) and textual resources (such as PubMedQA and OMIM) broadly support the training and generalization of GenAI models.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Silenced Biases: The Dark Side LLMs Learned to Refuse</title>
<link>https://arxiv.org/abs/2511.03369</link>
<guid>https://arxiv.org/abs/2511.03369</guid>
<content:encoded><![CDATA[
<div> safety-aligned large language models, fairness, bias, fairness evaluation framework, Silenced Bias Benchmark
Summary:
The article discusses the challenges in evaluating the fairness of large language models (LLMs) in sensitive applications. Current methods often overlook biases encoded within models' latent space, leading to a false sense of fairness. The concept of silenced biases is introduced, which are unfair preferences hidden by safety alignment. The Silenced Bias Benchmark (SBB) is proposed to uncover these biases by reducing model refusals during question-answer evaluations. SBB can easily be expanded to new demographic groups and subjects, providing a framework for evaluating fairness beyond alignment training. The approach is demonstrated on multiple LLMs, revealing a significant disparity between models' direct responses and underlying fairness issues. <div>
arXiv:2511.03369v1 Announce Type: new 
Abstract: Safety-aligned large language models (LLMs) are becoming increasingly widespread, especially in sensitive applications where fairness is essential and biased outputs can cause significant harm. However, evaluating the fairness of models is a complex challenge, and approaches that do so typically utilize standard question-answer (QA) styled schemes. Such methods often overlook deeper issues by interpreting the model's refusal responses as positive fairness measurements, which creates a false sense of fairness. In this work, we introduce the concept of silenced biases, which are unfair preferences encoded within models' latent space and are effectively concealed by safety-alignment. Previous approaches that considered similar indirect biases often relied on prompt manipulation or handcrafted implicit queries, which present limited scalability and risk contaminating the evaluation process with additional biases. We propose the Silenced Bias Benchmark (SBB), which aims to uncover these biases by employing activation steering to reduce model refusals during QA. SBB supports easy expansion to new demographic groups and subjects, presenting a fairness evaluation framework that encourages the future development of fair models and tools beyond the masking effects of alignment training. We demonstrate our approach over multiple LLMs, where our findings expose an alarming distinction between models' direct responses and their underlying fairness issues.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models for Edge-Deployable Credit Negotiation</title>
<link>https://arxiv.org/abs/2511.03370</link>
<guid>https://arxiv.org/abs/2511.03370</guid>
<content:encoded><![CDATA[
<div> emotionally charged complex personas, credit negotiation, small language models, strategic intelligence, privacy-preserving

Summary:<br />
The paper introduces EQ-Negotiator, a framework that enhances the performance of small language models (SLMs) in emotionally charged credit negotiation scenarios through emotional personas. EQ-Negotiator integrates game theory and a Hidden Markov Model to track debtor emotional states in real-time, enabling SLMs to counter manipulation and de-escalate conflicts ethically. Agent-to-agent simulations demonstrate that a 7B parameter SLM with EQ-Negotiator outperforms larger LLMs in debt recovery and negotiation efficiency by more than 10 times. This work emphasizes the importance of strategic emotional intelligence over raw model scale in automated negotiation. Additionally, it highlights the shift from descriptive character profiles to dynamic emotional architectures for persona modeling within privacy constraints. EQ-Negotiator enables the development of effective, ethical, and privacy-preserving AI negotiators suitable for on-device applications. 

<br /><br /> <div>
arXiv:2511.03370v1 Announce Type: new 
Abstract: The deployment of large language models (LLMs) in automated negotiation has set a high performance benchmark, but their computational cost and data privacy requirements render them unsuitable for many privacy-sensitive, on-device applications such as mobile assistants, embodied AI agents or private client interactions. While small language models (SLMs) offer a practical alternative, they suffer from a significant performance gap compared to LLMs in playing emotionally charged complex personas, especially for credit negotiation. This paper introduces EQ-Negotiator, a novel framework that bridges this capability gap using emotional personas. Its core is a reasoning system that integrates game theory with a Hidden Markov Model(HMM) to learn and track debtor emotional states online, without pre-training. This allows EQ-Negotiator to equip SLMs with the strategic intelligence to counter manipulation while de-escalating conflict and upholding ethical standards. Through extensive agent-to-agent simulations across diverse credit negotiation scenarios, including adversarial debtor strategies like cheating, threatening, and playing the victim, we show that a 7B parameter language model with EQ-Negotiator achieves better debt recovery and negotiation efficiency than baseline LLMs more than 10 times its size. This work advances persona modeling from descriptive character profiles to dynamic emotional architectures that operate within privacy constraints. Besides, this paper establishes that strategic emotional intelligence, not raw model scale, is the critical factor for success in automated negotiation, paving the way for effective, ethical, and privacy-preserving AI negotiators that can operate on the edge.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced Logical Reasoning</title>
<link>https://arxiv.org/abs/2511.03372</link>
<guid>https://arxiv.org/abs/2511.03372</guid>
<content:encoded><![CDATA[
<div> Keywords: logical data augmentation, large language models, symbolic logic, propositional logic, logical reasoning

Summary:
Logical data augmentation is a critical task that can be both costly and challenging when relying heavily on human annotation. This article introduces a novel approach called LFC-DA, which utilizes symbolic logic to control the generation of logical text. The pipeline of LFC-DA involves mapping text to propositional expressions, compiling a compact rule library, and conducting a bounded state-space search to generate valid formulas. These formulas are then transformed back into natural-language questions to ensure diversity and logical rigor under propositional logic. Experiments conducted on datasets like ReClor and LogiQA demonstrate significant improvements in logical-reasoning accuracy when using LFC-DA with pretrained models. Overall, LFC-DA presents an effective solution for logically heterogeneous data generation guided by large language models. 

<br /><br />Summary: <div>
arXiv:2511.03372v1 Announce Type: new 
Abstract: For complex logical data augmentation, heavy reliance on human annotation is costly, whereas direct generation with large language models yields uninterpretable and logically homogeneous examples. To address this, we present LFC-DA, a symbolic-logic-controlled pipeline: logical text is first mapped to propositional expressions, a compact rule library is compiled, and a bounded state-space search systematically discovers valid formulas that are then verbalized back into natural-language questions, ensuring both diversity and logical rigor under propositional logic. Experiments on ReClor and LogiQA show significant improvements in the logical-reasoning accuracy of pretrained models, confirming the effectiveness of LFC-DA for LLM-guided logical data augmentation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Segmentation Beyond Defaults: Asymmetrical Byte Pair Encoding for Optimal Machine Translation Performance</title>
<link>https://arxiv.org/abs/2511.03383</link>
<guid>https://arxiv.org/abs/2511.03383</guid>
<content:encoded><![CDATA[
<div> Machine Translation, Tokenization, Byte Pair Encoding, Asymmetric BPE, Low-resource Settings

Summary:
Asymmetric Byte Pair Encoding (BPE) for word segmentation in Machine Translation (MT) systems has been shown to significantly improve performance, especially in low-resource settings. By using different numbers of merge operations (NMOs) for the source and target languages, optimal results are achieved. In experiments across various language pairs and data sizes, high NMOs for the source language and low NMOs for the target language prove to be beneficial. Specifically, English-Hindi MT systems saw average gains of 5.32, 4.46, and 0.7 CHRF++ in low-resource setups. This trend was consistent across additional language pairs, with statistically significant improvements in 10 out of 12 systems compared to symmetric BPE. These findings emphasize the importance of considering asymmetric BPE segmentation recipes for improving MT system performance, particularly in low-resource scenarios.

<br /><br />Summary: <div>
arXiv:2511.03383v1 Announce Type: new 
Abstract: Existing Machine Translation (MT) research often suggests a single, fixed set of hyperparameters for word segmentation models, symmetric Byte Pair Encoding (BPE), which applies the same number of merge operations (NMO) to train tokenizers for both source and target languages. However, we demonstrate that this uniform approach doesn't guarantee optimal MT performance across different language pairs and data sizes. This work investigates BPE segmentation recipes across various data volumes and language pairs to evaluate MT system performance. We find that utilizing asymmetric BPE, where the source and target languages have different NMOs, significantly improves results over the symmetric approach, especially in low-resource settings (50K, 100K, and 500K sentence pairs). Specifically, asymmetric BPE yield statistically significant ($p<0.05$) average gains of 5.32, 4.46, and 0.7 CHRF++ on English-Hindi in low-resource setups. We validated this trend across six additional language pairs (English and Telugu, Shona, Norwegian, Kyrgyz, Hausa, and Inuktitut), observing statistically significant improvement in 10 out of 12 systems compared to symmetric BPE. Our findings indicate a high NMO for the source (4K to 32K) and a low NMO for the target (0.5K to 2K) provides optimal results, particularly benefiting low-resource MT.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overcoming the Generalization Limits of SLM Finetuning for Shape-Based Extraction of Datatype and Object Properties</title>
<link>https://arxiv.org/abs/2511.03407</link>
<guid>https://arxiv.org/abs/2511.03407</guid>
<content:encoded><![CDATA[
<div> relation extraction, RDF triples, language models, SHACL shapes, synthetic data augmentation<br />
<br />
Small language models (SLMs) have shown potential in relation extraction (RE) for RDF triples using SHACL shapes focusing on common datatype properties. This study delves into the handling of both datatype and object properties by SLMs for comprehensive RDF graph extraction. The main challenge lies in the distribution of rare properties, leading to a bottleneck in performance. To address this, various strategies were evaluated, including stratified sampling, weighted loss, dataset scaling, and template-based synthetic data augmentation. The most effective approach involved constructing a training set with a minimum occurrence threshold for each property. The release of datasets, experimental results, and code ensures reproducibility. These findings provide practical insights for training shape-aware SLMs in semantic RE and point towards promising avenues for future research.

<br /><br />Summary: 
- SLMs show promise in RE for RDF triples using SHACL shapes.
- Handling both datatype and object properties is a key challenge for SLMs.
- The long-tail distribution of rare properties proves to be a bottleneck in performance.
- Various strategies were evaluated to address this issue.
- Constructing a training set with a minimum occurrence threshold for each property was found to be the most effective. <div>
arXiv:2511.03407v1 Announce Type: new 
Abstract: Small language models (SLMs) have shown promises for relation extraction (RE) when extracting RDF triples guided by SHACL shapes focused on common datatype properties. This paper investigates how SLMs handle both datatype and object properties for a complete RDF graph extraction. We show that the key bottleneck is related to long-tail distribution of rare properties. To solve this issue, we evaluate several strategies: stratified sampling, weighted loss, dataset scaling, and template-based synthetic data augmentation. We show that the best strategy to perform equally well over unbalanced target properties is to build a training set where the number of occurrences of each property exceeds a given threshold. To enable reproducibility, we publicly released our datasets, experimental results and code. Our findings offer practical guidance for training shape-aware SLMs and highlight promising directions for future work in semantic RE.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Reasoning via Thought-Training and Thought-Free Inference</title>
<link>https://arxiv.org/abs/2511.03408</link>
<guid>https://arxiv.org/abs/2511.03408</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Chain-of-Thought prompting, efficient reasoning, hybrid model, implicit reasoning

Summary:
The article introduces a new framework called 3TF for efficient reasoning in large language models (LLMs). Unlike existing methods that compress verbose reasoning outputs, 3TF takes a Short-to-Long perspective by training a hybrid model capable of operating in both reasoning and non-reasoning modes. The model is further trained on CoT-annotated data to internalize structured reasoning while enforcing concise, thought-free outputs during inference. This approach improves the quality of reasoning in non-reasoning outputs, allowing models to perform rich internal reasoning implicitly while producing short external outputs. Empirical results show significant improvements in reasoning benchmarks under thought-free inference, suggesting that high-quality reasoning can be learned and executed implicitly without explicit step-by-step generation.

<br /><br />Summary: The article presents the 3TF framework for efficient reasoning in large language models, focusing on implicit reasoning through a hybrid model trained on CoT-annotated data. This approach enhances the quality of reasoning in non-reasoning outputs, enabling models to perform rich internal reasoning while producing concise thought-free external outputs. Empirical results demonstrate significant improvements in reasoning benchmarks under thought-free inference, highlighting the potential for implicit reasoning without explicit step-by-step generation. <div>
arXiv:2511.03408v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have leveraged explicit Chain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most existing methods primarily compress verbose reasoning outputs. These Long-to-Short transformations aim to improve efficiency, but still rely on explicit reasoning during inference. In this work, we introduce \textbf{3TF} (\textbf{T}hought-\textbf{T}raining and \textbf{T}hought-\textbf{F}ree inference), a framework for efficient reasoning that takes a Short-to-Long perspective. We first train a hybrid model that can operate in both reasoning and non-reasoning modes, and then further train it on CoT-annotated data to internalize structured reasoning, while enforcing concise, thought-free outputs at inference time using the no-reasoning mode. Unlike compression-based approaches, 3TF improves the reasoning quality of non-reasoning outputs, enabling models to perform rich internal reasoning implicitly while keeping external outputs short. Empirically, 3TF-trained models obtain large improvements on reasoning benchmarks under thought-free inference, demonstrating that high quality reasoning can be learned and executed implicitly without explicit step-by-step generation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge-Augmented Question Error Correction for Chinese Question Answer System with QuestionRAG</title>
<link>https://arxiv.org/abs/2511.03410</link>
<guid>https://arxiv.org/abs/2511.03410</guid>
<content:encoded><![CDATA[
<div> Keywords: QuestionRAG, question-answering systems, large language models, input errors, reinforcement learning

Summary:<br /><br />Input errors in question-answering systems often result in incorrect responses. Large language models (LLMs) commonly struggle with interpreting user intent or making unnecessary alterations to the original question. The proposed QuestionRAG framework addresses these challenges by enriching input with external knowledge to combat misinterpretation and using reinforcement learning (RL) to align the model's objective with precise correction, rather than just paraphrasing. The results demonstrate the importance of knowledge augmentation for understanding faulty questions, and RL-based alignment proves to be more effective than traditional supervised fine-tuning (SFT) in enhancing the model's ability to follow instructions and generalize. By combining these approaches, QuestionRAG unleashes the full capabilities of LLMs for question correction tasks. <br /><br />Summary: <div>
arXiv:2511.03410v1 Announce Type: new 
Abstract: Input errors in question-answering (QA) systems often lead to incorrect responses. Large language models (LLMs) struggle with this task, frequently failing to interpret user intent (misinterpretation) or unnecessarily altering the original question's structure (over-correction). We propose QuestionRAG, a framework that tackles these problems. To address misinterpretation, it enriches the input with external knowledge (e.g., search results, related entities). To prevent over-correction, it uses reinforcement learning (RL) to align the model's objective with precise correction, not just paraphrasing. Our results demonstrate that knowledge augmentation is critical for understanding faulty questions. Furthermore, RL-based alignment proves significantly more effective than traditional supervised fine-tuning (SFT), boosting the model's ability to follow instructions and generalize. By integrating these two strategies, QuestionRAG unlocks the full potential of LLMs for the question correction task.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the Biomedical Field</title>
<link>https://arxiv.org/abs/2511.03441</link>
<guid>https://arxiv.org/abs/2511.03441</guid>
<content:encoded><![CDATA[
<div> Keywords: biomedical, critical appraisal, large language models, CareMedEval dataset, reasoning tasks

Summary: 
CareMedEval introduces a new dataset for evaluating the performance of large language models (LLMs) in biomedical critical appraisal and reasoning tasks. The dataset is derived from exams taken by French medical students, containing 534 questions based on 37 scientific articles. Benchmarking different LLMs shows that both generalist and biomedical-specialized models struggle to achieve high accuracy, particularly in questions related to study limitations and statistical analysis. While incorporating intermediate reasoning tokens can improve results, the overall challenge of the task remains evident. CareMedEval serves as a challenging benchmark for evaluating LLMs in grounded reasoning tasks, highlighting current limitations and pointing towards future developments in automated support for critical appraisal.<br /><br />Summary: <div>
arXiv:2511.03441v1 Announce Type: new 
Abstract: Critical appraisal of scientific literature is an essential skill in the biomedical field. While large language models (LLMs) can offer promising support in this task, their reliability remains limited, particularly for critical reasoning in specialized domains. We introduce CareMedEval, an original dataset designed to evaluate LLMs on biomedical critical appraisal and reasoning tasks. Derived from authentic exams taken by French medical students, the dataset contains 534 questions based on 37 scientific articles. Unlike existing benchmarks, CareMedEval explicitly evaluates critical reading and reasoning grounded in scientific papers. Benchmarking state-of-the-art generalist and biomedical-specialized LLMs under various context conditions reveals the difficulty of the task: open and commercial models fail to exceed an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens considerably improves the results. Yet, models remain challenged especially on questions about study limitations and statistical analysis. CareMedEval provides a challenging benchmark for grounded reasoning, exposing current LLM limitations and paving the way for future development of automated support for critical appraisal.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kastor: Fine-tuned Small Language Models for Shape-based Active Relation Extraction</title>
<link>https://arxiv.org/abs/2511.03466</link>
<guid>https://arxiv.org/abs/2511.03466</guid>
<content:encoded><![CDATA[
<div> Keywords: RDF, pattern-based extraction, small language models, SHACL shape, knowledge bases 

Summary: 
Kastor is a framework introduced in this article to enhance RDF pattern-based extraction for fine-tuning small language models. It focuses on a specified SHACL shape, enabling efficient training on limited text and RDF data. Kastor shifts the traditional validation task from single SHACL shape validation to evaluating all possible combinations of properties derived from the shape, improving model generalization and performance by selecting the optimal combination for each example. The framework also utilizes an iterative learning process to refine noisy knowledge bases, creating robust models capable of uncovering new, relevant facts in specialized domains. Kastor addresses the demands for completing and refining knowledge bases by advancing the extraction approach and improving overall model capabilities. 

<br /><br />Summary: <div>
arXiv:2511.03466v1 Announce Type: new 
Abstract: RDF pattern-based extraction is a compelling approach for fine-tuning small language models (SLMs) by focusing a relation extraction task on a specified SHACL shape. This technique enables the development of efficient models trained on limited text and RDF data. In this article, we introduce Kastor, a framework that advances this approach to meet the demands for completing and refining knowledge bases in specialized domains. Kastor reformulates the traditional validation task, shifting from single SHACL shape validation to evaluating all possible combinations of properties derived from the shape. By selecting the optimal combination for each training example, the framework significantly enhances model generalization and performance. Additionally, Kastor employs an iterative learning process to refine noisy knowledge bases, enabling the creation of robust models capable of uncovering new, relevant facts
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BanglaSTEM: A Parallel Corpus for Technical Domain Bangla-English Translation</title>
<link>https://arxiv.org/abs/2511.03498</link>
<guid>https://arxiv.org/abs/2511.03498</guid>
<content:encoded><![CDATA[
<div> Dataset, Bangla-English, STEM, Translation model, Technical content <br />
Summary: <br />
Large language models are effective for technical problem solving in English but struggle in Bangla. Existing translation systems often mistranslate technical terms, leading to wrong answers. The BanglaSTEM dataset, comprising 5,000 Bangla-English sentence pairs from STEM fields, addresses this issue. Through careful selection and human evaluation, the dataset ensures accurate preservation of technical terminology. A T5-based translation model trained on BanglaSTEM demonstrates improved accuracy in translating technical content, benefiting Bangla speakers utilizing English-focused language models for tasks such as code generation and math problem solving. The dataset and trained model are publicly available for use. <div>
arXiv:2511.03498v1 Announce Type: new 
Abstract: Large language models work well for technical problem solving in English but perform poorly when the same questions are asked in Bangla. A simple solution would be to translate Bangla questions into English first and then use these models. However, existing Bangla-English translation systems struggle with technical terms. They often mistranslate specialized vocabulary, which changes the meaning of the problem and leads to wrong answers. We present BanglaSTEM, a dataset of 5,000 carefully selected Bangla-English sentence pairs from STEM fields including computer science, mathematics, physics, chemistry, and biology. We generated over 12,000 translations using language models and then used human evaluators to select the highest quality pairs that preserve technical terminology correctly. We train a T5-based translation model on BanglaSTEM and test it on two tasks: generating code and solving math problems. Our results show significant improvements in translation accuracy for technical content, making it easier for Bangla speakers to use English-focused language models effectively. Both the BanglaSTEM dataset and the trained translation model are publicly released at https://huggingface.co/reyazul/BanglaSTEM-T5.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HaluMem: Evaluating Hallucinations in Memory Systems of Agents</title>
<link>https://arxiv.org/abs/2511.03506</link>
<guid>https://arxiv.org/abs/2511.03506</guid>
<content:encoded><![CDATA[
<div> memory systems, AI, hallucinations, evaluation, benchmark

Summary:<br />
The article introduces the Hallucination in Memory Benchmark (HaluMem) to evaluate memory systems at different operational stages. It addresses memory hallucinations in AI systems by defining three evaluation tasks: memory extraction, memory updating, and memory question answering. Two datasets, HaluMem-Medium and HaluMem-Long, are constructed for human-AI interaction with extensive memory points and multi-type questions. The empirical studies reveal that memory systems tend to generate hallucinations during extraction and updating stages, leading to errors in question answering. The research highlights the importance of developing interpretable and constrained memory operation mechanisms to enhance memory reliability. Future focus should be on suppressing hallucinations systematically to improve overall system performance. 

Summary: <div>
arXiv:2511.03506v1 Announce Type: new 
Abstract: Memory systems are key components that enable AI systems such as LLMs and AI agents to achieve long-term learning and sustained interaction. However, during memory storage and retrieval, these systems frequently exhibit memory hallucinations, including fabrication, errors, conflicts, and omissions. Existing evaluations of memory hallucinations are primarily end-to-end question answering, which makes it difficult to localize the operational stage within the memory system where hallucinations arise. To address this, we introduce the Hallucination in Memory Benchmark (HaluMem), the first operation level hallucination evaluation benchmark tailored to memory systems. HaluMem defines three evaluation tasks (memory extraction, memory updating, and memory question answering) to comprehensively reveal hallucination behaviors across different operational stages of interaction. To support evaluation, we construct user-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and HaluMem-Long. Both include about 15k memory points and 3.5k multi-type questions. The average dialogue length per user reaches 1.5k and 2.6k turns, with context lengths exceeding 1M tokens, enabling evaluation of hallucinations across different context scales and task complexities. Empirical studies based on HaluMem show that existing memory systems tend to generate and accumulate hallucinations during the extraction and updating stages, which subsequently propagate errors to the question answering stage. Future research should focus on developing interpretable and constrained memory operation mechanisms that systematically suppress hallucinations and improve memory reliability.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Battle After Another: Probing LLMs' Limits on Multi-Turn Instruction Following with a Benchmark Evolving Framework</title>
<link>https://arxiv.org/abs/2511.03508</link>
<guid>https://arxiv.org/abs/2511.03508</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, instruction-following ability, dialogue, benchmark, EvolIF 

Summary: 
Large language models' ability to follow user instructions in multi-turn dialogues is crucial for conversational applications. Existing benchmarks are limited in assessing user experience. A new framework decouples linguistic forms from user intent simulation, tracking constraints, instructions, and topics for a more realistic assessment. The framework allows for dynamic benchmark construction, simulating user patience for realistic interaction assessment. The EvolIF benchmark, incorporating nine constraint types, showed GPT-5's superior instruction-following performance with an average of 18.54 turns and 70.31% robustness, outperforming other models significantly. Data and code will be publicly available. This framework provides a more comprehensive evaluation of language models' ability to understand and follow user instructions across varied topics and constraints. <br /><br />Summary: <div>
arXiv:2511.03508v1 Announce Type: new 
Abstract: Understanding how well large language models can follow users' instructions throughout a dialogue spanning multiple topics is of great importance for data-intensive conversational applications. Existing benchmarks are often limited to a fixed number of turns, making them susceptible to saturation and failing to account for the user's interactive experience. In this work, we propose an extensible framework for assessing multi-turn instruction-following ability. At its core, our framework decouples linguistic surface forms from user intent simulation through a three-layer mechanism that tracks constraints, instructions, and topics. This framework mimics User-LLM interaction by enabling the dynamic construction of benchmarks with state changes and tracebacks, terminating a conversation only when the model exhausts a simulated user's patience. We define a suite of metrics capturing the quality of the interaction process. Using this framework, we construct EvolIF, an evolving instruction-following benchmark incorporating nine distinct constraint types. Our results indicate that GPT-5 exhibits superior instruction-following performance. It sustains an average of 18.54 conversational turns and demonstrates 70.31% robustness, outperforming Gemini-2.5-Pro by a significant margin of 11.41%, while other models lag far behind. All of the data and code will be made publicly available online.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across Medical Specialties</title>
<link>https://arxiv.org/abs/2511.03542</link>
<guid>https://arxiv.org/abs/2511.03542</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Medical Question Answering, Multi-Agent Architecture, Domain-Specialized Language Models, Italian Medical Forum

Summary:
SOLVE-Med is a new multi-agent architecture designed to address deployment challenges in medical question answering systems. It combines specialized small language models for complex medical queries, including a Router Agent for specialist selection, ten models fine-tuned on specific medical domains, and an Orchestrator Agent for synthesizing responses. Evaluated on Italian medical forum data, SOLVE-Med outperforms standalone models with ROUGE-1 of 0.301 and BERTScore F1 of 0.697, while enabling local deployment. The system mitigates issues such as hallucinations, bias, computational demands, and privacy concerns, providing superior performance across diverse specialties. The code for SOLVE-Med is openly available on GitHub for further exploration and development.<br /><br />Summary: <div>
arXiv:2511.03542v1 Announce Type: new 
Abstract: Medical question answering systems face deployment challenges including hallucinations, bias, computational demands, privacy concerns, and the need for specialized expertise across diverse domains. Here, we present SOLVE-Med, a multi-agent architecture combining domain-specialized small language models for complex medical queries. The system employs a Router Agent for dynamic specialist selection, ten specialized models (1B parameters each) fine-tuned on specific medical domains, and an Orchestrator Agent that synthesizes responses. Evaluated on Italian medical forum data across ten specialties, SOLVE-Med achieves superior performance with ROUGE-1 of 0.301 and BERTScore F1 of 0.697, outperforming standalone models up to 14B parameters while enabling local deployment. Our code is publicly available on GitHub: https://github.com/PRAISELab-PicusLab/SOLVE-Med.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bearing Syntactic Fruit with Stack-Augmented Neural Networks</title>
<link>https://arxiv.org/abs/2511.03547</link>
<guid>https://arxiv.org/abs/2511.03547</guid>
<content:encoded><![CDATA[
<div> stack-augmented neural networks, hierarchical syntactic rules, language acquisition, transformer architecture, generalization

Summary:
stack-augmented neural networks are demonstrated to generalize in a human-like fashion without syntactic supervision, pre-training, or extended training. Three base architectures (transformer, simple RNN, LSTM) augmented with two types of stacks are tested. Transformers with nondeterministic stacks exhibit superior generalization on a question formation task. A modification to the stack RNN architecture enhances hierarchical generalization. These findings suggest that stack-augmented neural networks may offer more accurate models of human language acquisition compared to standard architectures, making them valuable subjects for psycholinguistic research. The study's code is publicly available. <div>
arXiv:2511.03547v1 Announce Type: new 
Abstract: Any finite set of training data is consistent with an infinite number of hypothetical algorithms that could have generated it. Studies have shown that when human children learn language, they consistently favor hypotheses based on hierarchical syntactic rules without ever encountering disambiguating examples. A recent line of work has inquired as to whether common neural network architectures share this bias, finding that they do so only under special conditions: when syntactically supervised, when pre-trained on massive corpora, or when trained long past convergence. In this paper, we demonstrate, for the first time, neural network architectures that are able to generalize in human-like fashion without any of the aforementioned requirements: stack-augmented neural networks. We test three base architectures (transformer, simple RNN, LSTM) augmented with two styles of stack: the superposition stack of Joulin & Mikolov (2015) and a nondeterministic generalization of it proposed by DuSell & Chiang (2023). We find that transformers with nondeterministic stacks generalize best out of these architectures on a classical question formation task. We also propose a modification to the stack RNN architecture that improves hierarchical generalization. These results suggest that stack-augmented neural networks may be more accurate models of human language acquisition than standard architectures, serving as useful objects of psycholinguistic study. Our code is publicly available.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiZebraLogic: A Multilingual Logical Reasoning Benchmark</title>
<link>https://arxiv.org/abs/2511.03553</link>
<guid>https://arxiv.org/abs/2511.03553</guid>
<content:encoded><![CDATA[
<div> keywords: large language models, logical reasoning skills, zebra puzzles, difficulty, dataset

Summary: 
- The study aims to create high-quality datasets for evaluating logical reasoning skills in large language models (LLMs) across multiple languages and difficulty levels.
- Various types of zebra puzzles were generated in multiple languages, themes, and sizes to assess the reasoning abilities of different LLMs.
- Puzzle sizes of 2x3 and 4x5 were found to be challenging for different types of LLMs, with the inclusion of red herring clues affecting accuracy.
- The difficulty of puzzles was not significantly influenced by language or theme variations.
- The dataset "MultiZebraLogic" containing 128+1024 puzzles in nine Germanic languages for 2x3 and 4x5 sizes has been published, along with code for puzzle generation in different languages and themes.

<br /><br />Summary: <div>
arXiv:2511.03553v1 Announce Type: new 
Abstract: Measuring the full abilities of large language models (LLMs) requires benchmarks representing multiple tasks. We aim to create large, high-quality datasets for comparison of logical reasoning skills across several languages and of suitable difficulty for LLMs of various reasoning ability. We explore multiple ways of increasing difficulty. We generate zebra puzzles in multiple languages, themes, sizes and including 14 different clue types and 8 red herring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are sufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a reasoning model), respectively. Including 5 red herrings decreases o3-mini puzzle-level accuracy on 4x5 puzzles by 15$\pm$7 %. Scores of o3-mini on 4x5 puzzles are not significantly affected by use of English vs. Danish or the common houses theme vs. the country-specific smoerrebroed theme. We find no correlation between difficulty and the selected clue types. Datasets of 128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic languages for sizes 2x3 and 4x5. We publish code for puzzle generation, designed for adaptablity into more languages and themes.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AILA--First Experiments with Localist Language Models</title>
<link>https://arxiv.org/abs/2511.03559</link>
<guid>https://arxiv.org/abs/2511.03559</guid>
<content:encoded><![CDATA[
<div> locality, transformer language models, interpretation, distributed representations, control<br />
Summary:<br />
This paper introduces a novel approach to transformer language models that allows for controllable locality, enabling a balance between interpretability and performance. By tuning a locality parameter, the model can dynamically adjust between localist encodings and distributed representations without requiring retraining. Experimental results on the WikiText corpus show that highly localist configurations result in lower attention entropy and higher pointer fidelity scores. Intermediate locality values optimize the tradeoff between interpretability and performance, with a specific value achieving a test perplexity of 4.65 and accuracy of 84.7%. This demonstrates that localist language models offer a practical framework for applications in regulated domains, providing precise control over the interpretability-performance spectrum through penalty thresholds and information-theoretic design principles.<br /><br />Summary: <div>
arXiv:2511.03559v1 Announce Type: new 
Abstract: This paper presents the first empirical demonstration of controllable locality in transformer language models, a novel architectural framework that enables continuous control over the degree of representation localization through a tunable locality dial parameter. Unlike traditional language models that rely exclusively on distributed representations, our approach allows dynamic interpolation between highly interpretable localist encodings and efficient distributed representations without requiring model retraining. We conducted experiments on the WikiText corpus using a two-layer transformer architecture, systematically varying the locality parameter {\lambda} across the full spectrum from 1.0 (fully localist) to 0.0 (fully distributed). Our results demonstrate that localist configurations achieve dramatically lower attention entropy, with {\lambda} = 1.0 yielding 5.36 bits compared to 7.18 bits at {\lambda} = 0.0, while maintaining substantially higher pointer fidelity scores reflecting stronger alignment with rule-specified targets. Prediction experiments reveal that intermediate locality values optimize the tradeoff between interpretability and performance, with {\lambda} = 0.6 achieving test perplexity of 4.65 and accuracy of 84.7%. These findings establish that localist language models provide a practical framework for applications in regulated domains requiring both transparency and capability, offering precise mathematical control over the interpretability-performance spectrum through explicit penalty thresholds and information-theoretic design principles.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for Enhanced Legal Regulation</title>
<link>https://arxiv.org/abs/2511.03563</link>
<guid>https://arxiv.org/abs/2511.03563</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, legal regulations, fine-tuning, Retrieval-Augmented Generation, policymakers <br />
Summary: <br />
- The study focuses on fine-tuning Large Language Models (LLMs) to optimize support for policymakers in understanding and crafting legal regulations. <br />
- A specialized dataset was curated to enhance the model's comprehension of legal texts, tailored specifically for the legal domain. <br />
- The integration of the Retrieval-Augmented Generation (RAG) method allows the LLM to access and integrate current legal knowledge from external sources, improving its capabilities. <br />
- The combination of fine-tuning and RAG-based augmentation creates a tool that not only processes legal information but actively aids policymakers in interpreting and creating regulations that meet current requirements. <br />
- Results indicate that this approach significantly boosts the efficiency of legal research and regulation development, serving as a valuable asset in the dynamic field of law. <br /> 
Summary: <div>
arXiv:2511.03563v1 Announce Type: new 
Abstract: In this study, we explore the fine-tuning of Large Language Models (LLMs) to better support policymakers in their crucial work of understanding, analyzing, and crafting legal regulations. To equip the model with a deep understanding of legal texts, we curated a supervised dataset tailored to the specific needs of the legal domain. Additionally, we integrated the Retrieval-Augmented Generation (RAG) method, enabling the LLM to access and incorporate up-to-date legal knowledge from external sources. This combination of fine-tuning and RAG-based augmentation results in a tool that not only processes legal information but actively assists policymakers in interpreting regulations and drafting new ones that align with current needs. The results demonstrate that this approach can significantly enhance the effectiveness of legal research and regulation development, offering a valuable resource in the ever-evolving field of law.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Step-Audio-EditX Technical Report</title>
<link>https://arxiv.org/abs/2511.03601</link>
<guid>https://arxiv.org/abs/2511.03601</guid>
<content:encoded><![CDATA[
<div> large-margin learning, expressive audio editing, emotion editing, zero-shot text-to-speech, open-source

Summary:
Step-Audio-EditX is introduced as a novel open-source audio model utilizing large-margin learning for expressive and iterative audio editing. It excels in emotion editing, speaking style variations, and other fine-grained control tasks while also supporting robust zero-shot text-to-speech capabilities. Unlike prior models, Step-Audio-EditX does not rely on embedding-based priors or auxiliary modules, instead focusing on large-margin synthetic data for training. This approach enables iterative control and high expressivity across voices without the need for disentanglement at the representation level. Evaluation results show that Step-Audio-EditX outperforms existing models such as MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in tasks related to emotion editing and other forms of audio control.<br /><br />Summary: <div>
arXiv:2511.03601v1 Announce Type: new 
Abstract: We present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing encompassing emotion, speaking style, and paralinguistics alongside robust zero-shot text-to-speech (TTS) capabilities.Our core innovation lies in leveraging only large-margin synthetic data, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents a fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A systematic review of relation extraction task since the emergence of Transformers</title>
<link>https://arxiv.org/abs/2511.03610</link>
<guid>https://arxiv.org/abs/2511.03610</guid>
<content:encoded><![CDATA[
<div> Keywords: relation extraction, Transformer-based models, systematic review, benchmark resources, semantic web technologies

Summary: 
This systematic review examines relation extraction (RE) research from 2019 to 2024, focusing on Transformer-based models. The analysis covers 34 surveys, 64 datasets, and 104 models, highlighting methodological advancements, benchmark resources, and the incorporation of semantic web technologies. The study identifies current trends, limitations, and open challenges in RE research, providing a valuable reference for researchers and practitioners. The integration of semantic web technologies shows promise for enhancing RE capabilities. By consolidating results across various dimensions, the review offers insights into the evolution and future directions of RE. The analysis sheds light on the state of the field and outlines important developments in RE methodologies and resources.<br /><br />Summary: <div>
arXiv:2511.03610v1 Announce Type: new 
Abstract: This article presents a systematic review of relation extraction (RE) research since the advent of Transformer-based models. Using an automated framework to collect and annotate publications, we analyze 34 surveys, 64 datasets, and 104 models published between 2019 and 2024. The review highlights methodological advances, benchmark resources, and the integration of semantic web technologies. By consolidating results across multiple dimensions, the study identifies current trends, limitations, and open challenges, offering researchers and practitioners a comprehensive reference for understanding the evolution and future directions of RE.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit and Explicit Interpretability</title>
<link>https://arxiv.org/abs/2511.03635</link>
<guid>https://arxiv.org/abs/2511.03635</guid>
<content:encoded><![CDATA[
<div> framework, interpretable, stance detection, large language models, implicit rationales <br />
Summary: <br />
The article introduces a new interpretable Zero-Shot Stance Detection framework called IRIS. It addresses issues with existing methods by providing implicit and explicit rationales for stance detection. IRIS considers stance detection as an information retrieval ranking task, using implicit rationales within the text and explicit communicative features to guide predictions. The model does not require ground-truth rationales, ensuring interpretability. Extensive experiments on multiple benchmark datasets demonstrate the generalizability of IRIS even with limited training data. The model's architecture and interpretable design offer a nuanced understanding of the author's attitude towards different targets. <div>
arXiv:2511.03635v1 Announce Type: new 
Abstract: Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward unseen targets. Existing research using contrastive, meta-learning, or data augmentation suffers from generalizability issues or lack of coherence between text and target. Recent works leveraging large language models (LLMs) for ZSSD focus either on improving unseen target-specific knowledge or generating explanations for stance analysis. However, most of these works are limited by their over-reliance on explicit reasoning, provide coarse explanations that lack nuance, and do not explicitly model the reasoning process, making it difficult to interpret the model's predictions. To address these issues, in our study, we develop a novel interpretable ZSSD framework, IRIS. We provide an interpretable understanding of the attitude of the input towards the target implicitly based on sequences within the text (implicit rationales) and explicitly based on linguistic measures (explicit rationales). IRIS considers stance detection as an information retrieval ranking task, understanding the relevance of implicit rationales for different stances to guide the model towards correct predictions without requiring the ground-truth of rationales, thus providing inherent interpretability. In addition, explicit rationales based on communicative features help decode the emotional and cognitive dimensions of stance, offering an interpretable understanding of the author's attitude towards the given target. Extensive experiments on the benchmark datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10% training data prove the generalizability of our model, benefiting from the proposed architecture and interpretable design.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation</title>
<link>https://arxiv.org/abs/2511.03656</link>
<guid>https://arxiv.org/abs/2511.03656</guid>
<content:encoded><![CDATA[
<div> Keywords: Chinese Multi-Document Question Answering Dataset, NLP, high-quality, fine-grained categories, document comprehension

Summary: 
The article introduces the Chinese Multi-Document Question Answering Dataset (ChiMDQA), created to meet the increasing demand for high-quality Chinese document question-answering datasets in various business scenarios. ChiMDQA contains 6,068 rigorously curated question-answer pairs categorized into ten fine-grained categories, covering domains like academic, education, finance, law, medical treatment, and news. The dataset includes long-form documents from six distinct fields and ensures diversity and quality through meticulous screening and question-design methodology. It is suitable for NLP tasks such as document comprehension, knowledge extraction, and intelligent QA systems. The article provides detailed information on the dataset's design, construction methodologies, and evaluation system, serving as a valuable resource for future research and practical applications in Chinese QA.

<br /><br />Summary: <div>
arXiv:2511.03656v1 Announce Type: new 
Abstract: With the rapid advancement of natural language processing (NLP) technologies, the demand for high-quality Chinese document question-answering datasets is steadily growing. To address this issue, we present the Chinese Multi-Document Question Answering Dataset(ChiMDQA), specifically designed for downstream business scenarios across prevalent domains including academic, education, finance, law, medical treatment, and news. ChiMDQA encompasses long-form documents from six distinct fields, consisting of 6,068 rigorously curated, high-quality question-answer (QA) pairs further classified into ten fine-grained categories. Through meticulous document screening and a systematic question-design methodology, the dataset guarantees both diversity and high quality, rendering it applicable to various NLP tasks such as document comprehension, knowledge extraction, and intelligent QA systems. Additionally, this paper offers a comprehensive overview of the dataset's design objectives, construction methodologies, and fine-grained evaluation system, supplying a substantial foundation for future research and practical applications in Chinese QA. The code and data are available at: https://anonymous.4open.science/r/Foxit-CHiMDQA/.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset in Large Language Models</title>
<link>https://arxiv.org/abs/2511.03699</link>
<guid>https://arxiv.org/abs/2511.03699</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, conspiracy beliefs, sociodemographic biases, conditioning, misinformation

Summary:
Large Language Models (LLMs) were examined for conspiratorial tendencies and sociodemographic biases. The study assessed whether these models could be conditioned into adopting conspiracy beliefs and found that LLMs exhibited partial agreement with such beliefs. Conditioning LLMs with socio-demographic attributes revealed uneven effects, exposing latent biases in the models. The study also discovered that targeted prompts could easily influence LLM responses towards conspiratorial directions, highlighting the susceptibility of these models to manipulation. These findings emphasize the importance of evaluating psychological dimensions embedded in LLMs to advance computational social science and mitigate potential risks associated with the deployment of these models in sensitive contexts. <div>
arXiv:2511.03699v1 Announce Type: new 
Abstract: In this paper, we investigate whether Large Language Models (LLMs) exhibit conspiratorial tendencies, whether they display sociodemographic biases in this domain, and how easily they can be conditioned into adopting conspiratorial perspectives. Conspiracy beliefs play a central role in the spread of misinformation and in shaping distrust toward institutions, making them a critical testbed for evaluating the social fidelity of LLMs. LLMs are increasingly used as proxies for studying human behavior, yet little is known about whether they reproduce higher-order psychological constructs such as a conspiratorial mindset. To bridge this research gap, we administer validated psychometric surveys measuring conspiracy mindset to multiple models under different prompting and conditioning strategies. Our findings reveal that LLMs show partial agreement with elements of conspiracy belief, and conditioning with socio-demographic attributes produces uneven effects, exposing latent demographic biases. Moreover, targeted prompts can easily shift model responses toward conspiratorial directions, underscoring both the susceptibility of LLMs to manipulation and the potential risks of their deployment in sensitive contexts. These results highlight the importance of critically evaluating the psychological dimensions embedded in LLMs, both to advance computational social science and to inform possible mitigation strategies against harmful uses.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask</title>
<link>https://arxiv.org/abs/2511.03718</link>
<guid>https://arxiv.org/abs/2511.03718</guid>
<content:encoded><![CDATA[
<div> Keywords: collaborative dialogue, common ground, perspectivist annotation, reference expressions, understanding states

Summary:
Collaborative dialogue relies on establishing common ground among participants, but in asymmetric settings, misunderstandings can arise where individuals believe they agree while referring to different entities. This study introduces a perspectivist annotation scheme for the HCRC MapTask corpus, allowing for separate capture of speaker and addressee grounded interpretations for each reference expression. By analyzing 13,000 annotated reference expressions, it was found that full misunderstandings are rare once lexical variants are unified. However, multiplicity discrepancies systematically lead to divergences, highlighting how apparent grounding can conceal referential misalignment. This framework serves as a valuable resource for studying grounded misunderstandings and evaluating the ability of (variational) language models to accurately model perspective-dependent grounding in collaborative dialogue. 

<br /><br />Summary: Collaborative dialogue in asymmetric settings can lead to misunderstandings despite participants believing they agree, due to referring to different entities. A perspectivist annotation scheme for the HCRC MapTask corpus helps capture speaker and addressee grounded interpretations for reference expressions, revealing that full misunderstandings are scarce after unifying lexical variants. However, multiplicity discrepancies systematically cause divergences, demonstrating how apparent grounding can hide referential misalignment. This framework is crucial for studying grounded misunderstandings and assessing the capability of language models to capture perspective-dependent grounding in collaborative dialogue. <div>
arXiv:2511.03718v1 Announce Type: new 
Abstract: Collaborative dialogue relies on participants incrementally establishing common ground, yet in asymmetric settings they may believe they agree while referring to different entities. We introduce a perspectivist annotation scheme for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures speaker and addressee grounded interpretations for each reference expression, enabling us to trace how understanding emerges, diverges, and repairs over time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k annotated reference expressions with reliability estimates and analyze the resulting understanding states. The results show that full misunderstandings are rare once lexical variants are unified, but multiplicity discrepancies systematically induce divergences, revealing how apparent grounding can mask referential misalignment. Our framework provides both a resource and an analytic lens for studying grounded misunderstanding and for evaluating (V)LLMs' capacity to model perspective-dependent grounding in collaborative dialogue.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-shot data citation function classification using transformer-based large language models (LLMs)</title>
<link>https://arxiv.org/abs/2511.02936</link>
<guid>https://arxiv.org/abs/2511.02936</guid>
<content:encoded><![CDATA[
<div> transformer-based large language models, data use cases, scientific literature, machine-learning systems, genomic datasets  
Summary:  
Efforts are being made to identify how specific datasets are used in scientific literature, utilizing transformer-based large language models as a scalable approach to describing data use cases. Applying the open-source LLM, Llama 3.1-405B, structured data use case labels for publications incorporating genomic datasets were generated. A novel evaluation framework was introduced to assess the effectiveness of the methods, achieving an F1 score of .674 on a zero-shot data citation classification task without pre-defined categories. However, barriers such as data availability, prompt overfitting, computational infrastructure, and the expense of responsible performance evaluation hinder the full potential of these methods. <div>
arXiv:2511.02936v1 Announce Type: cross 
Abstract: Efforts have increased in recent years to identify associations between specific datasets and the scientific literature that incorporates them. Knowing that a given publication cites a given dataset, the next logical step is to explore how or why that data was used. Advances in recent years with pretrained, transformer-based large language models (LLMs) offer potential means for scaling the description of data use cases in the published literature. This avoids expensive manual labeling and the development of training datasets for classical machine-learning (ML) systems. In this work we apply an open-source LLM, Llama 3.1-405B, to generate structured data use case labels for publications known to incorporate specific genomic datasets. We also introduce a novel evaluation framework for determining the efficacy of our methods. Our results demonstrate that the stock model can achieve an F1 score of .674 on a zero-shot data citation classification task with no previously defined categories. While promising, our results are qualified by barriers related to data availability, prompt overfitting, computational infrastructure, and the expense required to conduct responsible performance evaluation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Curved Spacetime of Transformer Architectures</title>
<link>https://arxiv.org/abs/2511.03060</link>
<guid>https://arxiv.org/abs/2511.03060</guid>
<content:encoded><![CDATA[
<div> Transformer-based language models, General Relativity, geometry, attention mechanism, token embeddings <br />
<br />
Summary: 
The article presents a geometric framework that draws an analogy between Transformer-based language models and General Relativity. It suggests that queries and keys induce an effective metric on representation space, with attention acting as a discrete connection for parallel transport of value vectors across tokens. Stacked layers in the model provide discrete time-slices through which token representations evolve on a curved manifold, influenced by backpropagation acting as a shaping principle in parameter space. Experiments designed to test this analogy reveal a curvature landscape for a paragraph, showing how token embeddings follow curved trajectories in feature space. Simulations demonstrate excess counts of sharp/flat angles and longer length-to-chord ratios, confirming the presence of curvature not explainable by chance. Controlled context edits reveal measurable bends in embedding trajectories, confirming attention-induced curvature in the model. <div>
arXiv:2511.03060v1 Announce Type: cross 
Abstract: We present a geometric framework for understanding Transformer-based language models, drawing an explicit analogy to General Relativity. Queries and keys induce an effective metric on representation space, and attention acts as a discrete connection that implements parallel transport of value vectors across tokens. Stacked layers provide discrete time-slices through which token representations evolve on this curved manifold, while backpropagation plays the role of a least-action principle that shapes loss-minimizing trajectories in parameter space. If this analogy is correct, token embeddings should not traverse straight paths in feature space; instead, their layer-wise steps should bend and reorient as interactions mediated by embedding space curvature. To test this prediction, we design experiments that expose both the presence and the consequences of curvature: (i) we visualize a curvature landscape for a full paragraph, revealing how local turning angles vary across tokens and layers; (ii) we show through simulations that excess counts of sharp/flat angles and longer length-to-chord ratios are not explainable by dimensionality or chance; and (iii) inspired by Einstein's eclipse experiment, we probe deflection under controlled context edits, demonstrating measurable, meaning-consistent bends in embedding trajectories that confirm attention-induced curvature.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Insight to Exploit: Leveraging LLM Collaboration for Adaptive Adversarial Text Generation</title>
<link>https://arxiv.org/abs/2511.03128</link>
<guid>https://arxiv.org/abs/2511.03128</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, adversarial examples, robustness, attack frameworks, transferability 

Summary:
This study introduces novel attack frameworks, Static Deceptor (StaDec) and Dynamic Deceptor (DyDec), to generate dynamic and adaptive adversarial examples for Language Model models (LLMs). The goal is to assess the robustness of LLMs against adversarial inputs, particularly for sensitive tasks. The attacks aim to deceive LLMs by generating natural-looking adversarial inputs that maintain semantic similarity to the original text. By leveraging an automated pipeline driven by LLMs, the attacks do not rely on external heuristics. They demonstrate strong transferability across different LLMs and evolve with advancements in LLM technology. This work provides a systematic approach for self-assessment of an LLM's robustness, offering insights into the potential vulnerabilities of these models. The code and data for implementing these attack frameworks are publicly available on GitHub for further research and testing. <br /><br />Summary: <div>
arXiv:2511.03128v1 Announce Type: cross 
Abstract: LLMs can provide substantial zero-shot performance on diverse tasks using a simple task prompt, eliminating the need for training or fine-tuning. However, when applying these models to sensitive tasks, it is crucial to thoroughly assess their robustness against adversarial inputs. In this work, we introduce Static Deceptor (StaDec) and Dynamic Deceptor (DyDec), two innovative attack frameworks designed to systematically generate dynamic and adaptive adversarial examples by leveraging the understanding of the LLMs. We produce subtle and natural-looking adversarial inputs that preserve semantic similarity to the original text while effectively deceiving the target LLM. By utilizing an automated, LLM-driven pipeline, we eliminate the dependence on external heuristics. Our attacks evolve with the advancements in LLMs and demonstrate strong transferability across models unknown to the attacker. Overall, this work provides a systematic approach for the self-assessment of an LLM's robustness. We release our code and data at https://github.com/Shukti042/AdversarialExample.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Measurement to Expertise: Empathetic Expert Adapters for Context-Based Empathy in Conversational AI Agents</title>
<link>https://arxiv.org/abs/2511.03143</link>
<guid>https://arxiv.org/abs/2511.03143</guid>
<content:encoded><![CDATA[
arXiv:2511.03143v1 Announce Type: cross 
Abstract: Empathy is a critical factor in fostering positive user experiences in conversational AI. While models can display empathy, it is often generic rather than tailored to specific tasks and contexts. In this work, we introduce a novel framework for developing and evaluating context-specific empathetic large language models (LLMs). We first analyze a real-world conversational dataset consisting of 672 multi-turn conversations across 8 tasks, revealing significant differences in terms of expected and experienced empathy before and after the conversations, respectively. To help minimize this gap, we develop a synthetic multi-turn conversational generation pipeline and steer responses toward our defined empathy patterns based on the context that more closely matches users' expectations. We then train empathetic expert adapters for context-specific empathy that specialize in varying empathy levels based on the recognized task. Our empirical results demonstrate a significant gap reduction of 72.66% between perceived and desired empathy with scores increasing by an average factor of 2.43 as measured by our metrics and reward models. Additionally, our trained empathetic expert adapters demonstrate superior effectiveness in preserving empathy patterns throughout conversation turns, outperforming system prompts, which tend to dramatically diminish in impact as conversations lengthen.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Let the Bees Find the Weak Spots: A Path Planning Perspective on Multi-Turn Jailbreak Attacks against LLMs</title>
<link>https://arxiv.org/abs/2511.03271</link>
<guid>https://arxiv.org/abs/2511.03271</guid>
<content:encoded><![CDATA[
arXiv:2511.03271v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have been widely deployed across various applications, yet their potential security and ethical risks have raised increasing concerns. Existing research employs red teaming evaluations, utilizing multi-turn jailbreaks to identify potential vulnerabilities in LLMs. However, these approaches often lack exploration of successful dialogue trajectories within the attack space, and they tend to overlook the considerable overhead associated with the attack process. To address these limitations, this paper first introduces a theoretical model based on dynamically weighted graph topology, abstracting the multi-turn attack process as a path planning problem. Based on this framework, we propose ABC, an enhanced Artificial Bee Colony algorithm for multi-turn jailbreaks, featuring a collaborative search mechanism with employed, onlooker, and scout bees. This algorithm significantly improves the efficiency of optimal attack path search while substantially reducing the average number of queries required. Empirical evaluations on three open-source and two proprietary language models demonstrate the effectiveness of our approach, achieving attack success rates above 90\% across the board, with a peak of 98\% on GPT-3.5-Turbo, and outperforming existing baselines. Furthermore, it achieves comparable success with only 26 queries on average, significantly reducing red teaming overhead and highlighting its superior efficiency.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Citations: Measuring Idea-level Knowledge Diffusion from Research to Journalism and Policy-making</title>
<link>https://arxiv.org/abs/2511.03378</link>
<guid>https://arxiv.org/abs/2511.03378</guid>
<content:encoded><![CDATA[
arXiv:2511.03378v1 Announce Type: cross 
Abstract: Despite the importance of social science knowledge for various stakeholders, measuring its diffusion into different domains remains a challenge. This study uses a novel text-based approach to measure the idea-level diffusion of social science knowledge from the research domain to the journalism and policy-making domains. By doing so, we expand the detection of knowledge diffusion beyond the measurements of direct references. Our study focuses on media effects theories as key research ideas in the field of communication science. Using 72,703 documents (2000-2019) from three domains (i.e., research, journalism, and policy-making) that mention these ideas, we count the mentions of these ideas in each domain, estimate their domain-specific contexts, and track and compare differences across domains and over time. Overall, we find that diffusion patterns and dynamics vary considerably between ideas, with some ideas diffusing between other domains, while others do not. Based on the embedding regression approach, we compare contextualized meanings across domains and find that the distances between research and policy are typically larger than between research and journalism. We also find that ideas largely shift roles across domains - from being the theories themselves in research to sense-making in news to applied, administrative use in policy. Over time, we observe semantic convergence mainly for ideas that are practically oriented. Our results characterize the cross-domain diffusion patterns and dynamics of social science knowledge at the idea level, and we discuss the implications for measuring knowledge diffusion beyond citations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiveTradeBench: Seeking Real-World Alpha with Large Language Models</title>
<link>https://arxiv.org/abs/2511.03628</link>
<guid>https://arxiv.org/abs/2511.03628</guid>
<content:encoded><![CDATA[
arXiv:2511.03628v1 Announce Type: cross 
Abstract: Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Watermarking Large Language Models in Europe: Interpreting the AI Act in Light of Technology</title>
<link>https://arxiv.org/abs/2511.03641</link>
<guid>https://arxiv.org/abs/2511.03641</guid>
<content:encoded><![CDATA[
arXiv:2511.03641v1 Announce Type: cross 
Abstract: To foster trustworthy Artificial Intelligence (AI) within the European Union, the AI Act requires providers to mark and detect the outputs of their general-purpose models. The Article 50 and Recital 133 call for marking methods that are ''sufficiently reliable, interoperable, effective and robust''. Yet, the rapidly evolving and heterogeneous landscape of watermarks for Large Language Models (LLMs) makes it difficult to determine how these four standards can be translated into concrete and measurable evaluations. Our paper addresses this challenge, anchoring the normativity of European requirements in the multiplicity of watermarking techniques. Introducing clear and distinct concepts on LLM watermarking, our contribution is threefold. (1) Watermarking Categorisation: We propose an accessible taxonomy of watermarking methods according to the stage of the LLM lifecycle at which they are applied - before, during, or after training, and during next-token distribution or sampling. (2) Watermarking Evaluation: We interpret the EU AI Act's requirements by mapping each criterion with state-of-the-art evaluations on robustness and detectability of the watermark, and of quality of the LLM. Since interoperability remains largely untheorised in LLM watermarking research, we propose three normative dimensions to frame its assessment. (3) Watermarking Comparison: We compare current watermarking methods for LLMs against the operationalised European criteria and show that no approach yet satisfies all four standards. Encouraged by emerging empirical tests, we recommend further research into watermarking directly embedded within the low-level architecture of LLMs.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-Augmented Feature Generation for Domain-Specific Classification</title>
<link>https://arxiv.org/abs/2406.11177</link>
<guid>https://arxiv.org/abs/2406.11177</guid>
<content:encoded><![CDATA[
arXiv:2406.11177v3 Announce Type: replace 
Abstract: Feature generation can significantly enhance learning outcomes, particularly for tasks with limited data. An effective way to improve feature generation is to expand the current feature space using existing features and enriching the informational content. However, generating new, interpretable features usually requires domain-specific knowledge on top of the existing features. In this paper, we introduce a Retrieval-Augmented Feature Generation method, RAFG, to generate useful and explainable features specific to domain classification tasks. To increase the interpretability of the generated features, we conduct knowledge retrieval among the existing features in the domain to identify potential feature associations. These associations are expected to help generate useful features. Moreover, we develop a framework based on large language models (LLMs) for feature generation with reasoning to verify the quality of the features during their generation process. Experiments across several datasets in medical, economic, and geographic domains show that our RAFG method can produce high-quality, meaningful features and significantly improve classification performance compared with baseline methods.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Automatic Factuality Metrics Measure Factuality? A Critical Evaluation</title>
<link>https://arxiv.org/abs/2411.16638</link>
<guid>https://arxiv.org/abs/2411.16638</guid>
<content:encoded><![CDATA[
arXiv:2411.16638v4 Announce Type: replace 
Abstract: Modern LLMs can now produce highly readable abstractive summaries, to the point that traditional automated metrics for evaluating summary quality, such as ROUGE, have saturated. However, LLMs still sometimes introduce inaccuracies into summaries, i.e., information inconsistent with or unsupported by the corresponding source. Measuring the occurrence of these often subtle factual inconsistencies automatically has proved challenging. This in turn has motivated development of metrics intended to measure the factual consistency of generated summaries against sources. But are these approaches measuring what they purport to? Or are they mostly exploiting artifacts? In this work, we stress test a range of automatic factuality metrics, including specialized models and LLM-based prompting methods, to probe what they actually capture. Using a shallow classifier to separate ``easy'' examples for factual evaluation where surface features suffice from ``hard'' cases requiring deeper reasoning, we find that all metrics show substantial performance drops on the latter. Furthermore, some metrics are more sensitive to benign, fact-preserving edits than to factual corrections. Building on this observation, we demonstrate that most automatic factuality metrics can be gamed, i.e., their scores can be artificially inflated by appending innocuous, content-free sentences to summaries. Among the metrics tested, the prompt based ChatGPT-DA approach is the most robust and reliable. However, this comes with a notable caveat: Prompting LLMs to assess factuality may overly rely on their parametric knowledge rather than the provided reference when making judgments. Taken together, our findings call into question the reliability of current factuality metrics and prompt a broader reflection on what these metrics are truly measuring.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Haystack to Needle: Label Space Reduction for Zero-shot Classification</title>
<link>https://arxiv.org/abs/2502.08436</link>
<guid>https://arxiv.org/abs/2502.08436</guid>
<content:encoded><![CDATA[
arXiv:2502.08436v2 Announce Type: replace 
Abstract: We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verdict: A Library for Scaling Judge-Time Compute</title>
<link>https://arxiv.org/abs/2502.18018</link>
<guid>https://arxiv.org/abs/2502.18018</guid>
<content:encoded><![CDATA[
arXiv:2502.18018v2 Announce Type: replace 
Abstract: The use of LLMs as automated judges ("LLM-as-a-judge") is now widespread, yet standard judges suffer from a multitude of reliability issues. To address these challenges, we introduce Verdict, an open-source library for scaling judge-time compute to enhance the accuracy, reliability, and interpretability of automated evaluators. Verdict leverages the composition of modular reasoning units (such as verification, debate, and aggregation) and increased inference-time compute to improve LLM judge quality. Across a variety of challenging tasks such as content moderation, fact-checking, and hallucination detection, Verdict judges achieves performance competitive with orders-of-magnitude larger fine-tuned judges, prompted judges, and reasoning models. Our framework establishes a foundation for scalable, interpretable, and reliable LLM-based evaluation systems for both researchers and practitioners.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning Large Language Models</title>
<link>https://arxiv.org/abs/2503.07329</link>
<guid>https://arxiv.org/abs/2503.07329</guid>
<content:encoded><![CDATA[
arXiv:2503.07329v2 Announce Type: replace 
Abstract: The impact of random seeds in fine-tuning large language models (LLMs) has been largely overlooked despite its potential influence on model performance.In this study, we systematically evaluate the effects of random seeds on LLMs using the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact through traditional metrics like accuracy and F1, calculating their mean and variance to quantify performance fluctuations. To capture the micro-level effects, we introduce a novel metric, consistency, measuring the stability of individual predictions across runs. Our experiments reveal significant variance at both macro and micro levels, underscoring the need for careful consideration of random seeds in fine-tuning and evaluation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Traversal Verification for Speculative Tree Decoding</title>
<link>https://arxiv.org/abs/2505.12398</link>
<guid>https://arxiv.org/abs/2505.12398</guid>
<content:encoded><![CDATA[
arXiv:2505.12398v2 Announce Type: replace 
Abstract: Speculative decoding is a promising approach for accelerating large language models. The primary idea is to use a lightweight draft model to speculate the output of the target model for multiple subsequent timesteps, and then verify them in parallel to determine whether the drafted tokens should be accepted or rejected. To enhance acceptance rates, existing frameworks typically construct token trees containing multiple candidates in each timestep. However, their reliance on token-level verification mechanisms introduces two critical limitations: First, the probability distribution of a sequence differs from that of individual tokens, leading to suboptimal acceptance length. Second, current verification schemes begin from the root node and proceed layer by layer in a top-down manner. Once a parent node is rejected, all its child nodes should be discarded, resulting in inefficient utilization of speculative candidates. This paper introduces Traversal Verification, a novel speculative decoding algorithm that fundamentally rethinks the verification paradigm through leaf-to-root traversal. Our approach considers the acceptance of the entire token sequence from the current node to the root, and preserves potentially valid subsequences that would be prematurely discarded by existing methods. We theoretically prove that the probability distribution obtained through Traversal Verification is identical to that of the target model, guaranteeing lossless inference while achieving substantial acceleration gains. Experimental results across different large language models and multiple tasks show that our method consistently improves acceptance length and throughput over existing methods.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?</title>
<link>https://arxiv.org/abs/2505.16814</link>
<guid>https://arxiv.org/abs/2505.16814</guid>
<content:encoded><![CDATA[
arXiv:2505.16814v3 Announce Type: replace 
Abstract: Named Entity Recognition(NER) for low-resource languages aims to produce robust systems for languages where there is limited labeled training data available, and has been an area of increasing interest within NLP. Data augmentation for increasing the amount of low-resource labeled data is a common practice. In this paper, we explore the role of synthetic data in the context of multilingual, low-resource NER, considering 11 languages from diverse language families. Our results suggest that synthetic data does in fact hold promise for low-resource language NER, though we see significant variation between languages.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Case for Repeatable, Open, and Expert-Grounded Hallucination Benchmarks in Large Language Models</title>
<link>https://arxiv.org/abs/2505.17345</link>
<guid>https://arxiv.org/abs/2505.17345</guid>
<content:encoded><![CDATA[
arXiv:2505.17345v2 Announce Type: replace 
Abstract: Plausible, but inaccurate, tokens in model-generated text are widely believed to be pervasive and problematic for the responsible adoption of language models. Despite this concern, there is little scientific work that attempts to measure the prevalence of language model hallucination in a comprehensive way. In this paper, we argue that language models should be evaluated using repeatable, open, and domain-contextualized hallucination benchmarking. We present a taxonomy of hallucinations alongside a case study that demonstrates that when experts are absent from the early stages of data creation, the resulting hallucination metrics lack validity and practical utility.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distilling LLM Agent into Small Models with Retrieval and Code Tools</title>
<link>https://arxiv.org/abs/2505.17612</link>
<guid>https://arxiv.org/abs/2505.17612</guid>
<content:encoded><![CDATA[
arXiv:2505.17612v2 Announce Type: replace 
Abstract: Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing</title>
<link>https://arxiv.org/abs/2505.21600</link>
<guid>https://arxiv.org/abs/2505.21600</guid>
<content:encoded><![CDATA[
arXiv:2505.21600v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in LLMs</title>
<link>https://arxiv.org/abs/2505.23845</link>
<guid>https://arxiv.org/abs/2505.23845</guid>
<content:encoded><![CDATA[
arXiv:2505.23845v2 Announce Type: replace 
Abstract: We study the source of uncertainty in DeepSeek R1-32B by analyzing its self-reported verbal confidence on question answering (QA) tasks. In the default answer-then-confidence setting, the model is regularly over-confident, whereas semantic entropy - obtained by sampling many responses - remains reliable. We hypothesize that this is because of semantic entropy's larger test-time compute, which lets us explore the model's predictive distribution. We show that granting DeepSeek the budget to explore its distribution by forcing a long chain-of-thought before the final answer greatly improves its verbal score effectiveness, even on simple fact-retrieval questions that normally require no reasoning. Furthermore, a separate reader model that sees only the chain can reconstruct very similar confidences, indicating the verbal score might be merely a statistic of the alternatives surfaced during reasoning. Our analysis concludes that reliable uncertainty estimation requires explicit exploration of the generative space, and self-reported confidence is trustworthy only after such exploration.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LexTime: A Benchmark for Temporal Ordering of Legal Events</title>
<link>https://arxiv.org/abs/2506.04041</link>
<guid>https://arxiv.org/abs/2506.04041</guid>
<content:encoded><![CDATA[
arXiv:2506.04041v2 Announce Type: replace 
Abstract: Understanding temporal relationships and accurately reconstructing the event timeline is important for case law analysis, compliance monitoring, and legal summarization. However, existing benchmarks lack specialized language evaluation, leaving a gap in understanding how LLMs handle event ordering in legal contexts. We introduce LexTime, a dataset designed to evaluate LLMs' event ordering capabilities in legal language, consisting of 512 instances from U.S. Federal Complaints with annotated event pairs and their temporal relations. Our findings show that (1) LLMs are more accurate on legal event ordering than on narrative texts (up to +10.5%); (2) longer input contexts and implicit events boost accuracy, reaching 80.8% for implicit-explicit event pairs; (3) legal linguistic complexities and nested clauses remain a challenge. While performance is promising, specific features of legal texts remain a bottleneck for legal temporal event reasoning, and we propose concrete modeling directions to better address them.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models</title>
<link>https://arxiv.org/abs/2506.09684</link>
<guid>https://arxiv.org/abs/2506.09684</guid>
<content:encoded><![CDATA[
arXiv:2506.09684v2 Announce Type: replace 
Abstract: Large language models (LLMs) have transformed natural language processing, but their reliable deployment requires effective uncertainty quantification (UQ). Existing UQ methods are often heuristic and lack a probabilistic interpretation. This paper begins by providing a theoretical justification for the role of perturbations in UQ for LLMs. We then introduce a dual random walk perspective, modeling input-output pairs as two Markov chains with transition probabilities defined by semantic similarity. Building on this, we propose a fully probabilistic framework based on an inverse model, which quantifies uncertainty by evaluating the diversity of the input space conditioned on a given output through systematic perturbations. Within this framework, we define a new uncertainty measure, Inv-Entropy. A key strength of our framework is its flexibility: it supports various definitions of uncertainty measures, embeddings, perturbation strategies, and similarity metrics. We also propose GAAP, a perturbation algorithm based on genetic algorithms, which enhances the diversity of sampled inputs. In addition, we introduce a new evaluation metric, Temperature Sensitivity of Uncertainty (TSU), which directly assesses uncertainty without relying on correctness as a proxy. Extensive experiments demonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code to reproduce the results can be found at https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Medication Extraction and Discontinuation Identification from Electronic Health Records Using Large Language Models</title>
<link>https://arxiv.org/abs/2506.11137</link>
<guid>https://arxiv.org/abs/2506.11137</guid>
<content:encoded><![CDATA[
arXiv:2506.11137v2 Announce Type: replace 
Abstract: Identifying medication discontinuations in electronic health records (EHRs) is vital for patient safety but is often hindered by information being buried in unstructured notes. This study aims to evaluate the capabilities of advanced open-sourced and proprietary large language models (LLMs) in extracting medications and classifying their medication status from EHR notes, focusing on their scalability on medication information extraction without human annotation. We collected three EHR datasets from diverse sources to build the evaluation benchmark. We evaluated 12 advanced LLMs and explored multiple LLM prompting strategies. Performance on medication extraction, medication status classification, and their joint task (extraction then classification) was systematically compared across all experiments. We found that LLMs showed promising performance on the medication extraction and discontinuation classification from EHR notes. GPT-4o consistently achieved the highest average F1 scores in all tasks under zero-shot setting - 94.0% for medication extraction, 78.1% for discontinuation classification, and 72.7% for the joint task. Open-sourced models followed closely, Llama-3.1-70B-Instruct achieved the highest performance in medication status classification on the MIV-Med dataset (68.7%) and in the joint task on both the Re-CASI (76.2%) and MIV-Med (60.2%) datasets. Medical-specific LLMs demonstrated lower performance compared to advanced general-domain LLMs. Few-shot learning generally improved performance, while CoT reasoning showed inconsistent gains. LLMs demonstrate strong potential for medication extraction and discontinuation identification on EHR notes, with open-sourced models offering scalable alternatives to proprietary systems and few-shot can further improve LLMs' capability.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Post Persona Alignment for Multi-Session Dialogue Generation</title>
<link>https://arxiv.org/abs/2506.11857</link>
<guid>https://arxiv.org/abs/2506.11857</guid>
<content:encoded><![CDATA[
arXiv:2506.11857v2 Announce Type: replace 
Abstract: Multi-session persona-based dialogue generation presents challenges in maintaining long-term consistency and generating diverse, personalized responses. While large language models (LLMs) excel in single-session dialogues, they struggle to preserve persona fidelity and conversational coherence across extended interactions. Existing methods typically retrieve persona information before response generation, which can constrain diversity and result in generic outputs. We propose Post Persona Alignment (PPA), a novel two-stage framework that reverses this process. PPA first generates a general response based solely on dialogue context, then retrieves relevant persona memories using the response as a query, and finally refines the response to align with the speaker's persona. This post-hoc alignment strategy promotes naturalness and diversity while preserving consistency and personalization. Experiments on multi-session LLM-generated dialogue data demonstrate that PPA significantly outperforms prior approaches in consistency, diversity, and persona relevance, offering a more flexible and effective paradigm for long-term personalized dialogue generation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs</title>
<link>https://arxiv.org/abs/2506.14562</link>
<guid>https://arxiv.org/abs/2506.14562</guid>
<content:encoded><![CDATA[
arXiv:2506.14562v3 Announce Type: replace 
Abstract: Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines. Our code is available at https://github.com/hed-ucas/AlphaDecay.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition</title>
<link>https://arxiv.org/abs/2507.05724</link>
<guid>https://arxiv.org/abs/2507.05724</guid>
<content:encoded><![CDATA[
arXiv:2507.05724v3 Announce Type: replace 
Abstract: Mixture-of-experts (MoE) architectures have expanded from language modeling to automatic speech recognition (ASR). Traditional MoE methods, such as the Switch Transformer, route experts independently within each layer. Our analysis reveals that routers in most layers make expert choices that are not strongly correlated with the choices of the routers in other layers. To increase the cooperation between experts in different layers and encourage greater specialization, we use a shared router across different MoE layers. We call this model Omni-router Transformer. Extensive experiments on a large-scale pseudo-labeled dataset and evaluations across 10 diverse, out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is able to achieve lower training loss and consistently outperform dense and Switch Transformer models, reducing average word error rates by 11.2% and 8.2%, respectively, while providing structured expert usage and improved robustness to diverse data.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2507.18140</link>
<guid>https://arxiv.org/abs/2507.18140</guid>
<content:encoded><![CDATA[
arXiv:2507.18140v3 Announce Type: replace 
Abstract: Recent progress in Multi-modal Large Language Models (MLLMs) has enabled step-by-step multi-modal mathematical reasoning by performing visual operations based on the textual instructions. A promising approach uses code as an intermediate representation to precisely express and manipulate the images in the reasoning steps. However, existing evaluations focus mainly on text-only reasoning outputs, leaving the MLLM's ability to perform accurate visual operations via code largely unexplored. This work takes a first step toward addressing that gap by evaluating MLLM's code-based capabilities in multi-modal mathematical reasoning.Specifically, our framework focuses on two key evaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's ability to accurately understand and construct visualizations from scratch. (2) Multi-modal Code Editing (MCE) assesses the model's capacity for fine-grained operations, which include three types: Deletion, Modification and Annotation. To evaluate the above tasks, we incorporate a dataset that covers the five most popular types of mathematical figures, including geometric diagrams, function plots, and three types of statistical charts, to provide a comprehensive and effective measurement of existing MLLMs. Our experimental evaluation involves nine mainstream MLLMs, and the results reveal that existing models still lag significantly behind human performance in performing fine-grained visual operations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems</title>
<link>https://arxiv.org/abs/2508.00079</link>
<guid>https://arxiv.org/abs/2508.00079</guid>
<content:encoded><![CDATA[
arXiv:2508.00079v2 Announce Type: replace 
Abstract: The discipline of physics stands as a cornerstone of human intellect, driving the evolution of technology and deepening our understanding of the fundamental principles of the cosmos. Contemporary literature includes some works centered on the task of solving physics problems - a crucial domain of natural language reasoning. In this paper, we evaluate the performance of frontier LLMs in solving physics problems, both mathematical and descriptive. We also employ a plethora of inference-time techniques and agentic frameworks to improve the performance of the models. This includes the verification of proposed solutions in a cumulative fashion by other, smaller LLM agents, and we perform a comparative analysis of the performance that the techniques entail. There are significant improvements when the multi-agent framework is applied to problems that the models initially perform poorly on. Furthermore, we introduce a new evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small VAL}}$, consisting of 19,609 problems sourced from various physics textbooks and their corresponding correct solutions scraped from physics forums and educational websites. Our code and data are publicly available at https://github.com/areebuzair/PhysicsEval.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction</title>
<link>https://arxiv.org/abs/2508.02558</link>
<guid>https://arxiv.org/abs/2508.02558</guid>
<content:encoded><![CDATA[
arXiv:2508.02558v2 Announce Type: replace 
Abstract: Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10$\times$ higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness. The code is available at https://github.com/OpenMOSS/Sparse-dLLM.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives</title>
<link>https://arxiv.org/abs/2508.02853</link>
<guid>https://arxiv.org/abs/2508.02853</guid>
<content:encoded><![CDATA[
arXiv:2508.02853v3 Announce Type: replace 
Abstract: We present an approach to modeling annotator disagreement in subjective NLP tasks through both architectural and data-centric innovations. Our model, DEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert subnetworks based on annotator demographics, enabling it to better represent structured, group-level variation compared to prior models. DEM-MoE consistently performs competitively across demographic groups, and shows especially strong results on datasets with high annotator disagreement. To address sparse demographic coverage, we test whether LLM-generated synthetic annotations via zero-shot persona prompting can be used for data imputation. We show these synthetic judgments align moderately well with human annotations on our data and offer a scalable way to potentially enrich training data. We then propose and evaluate approaches for blending real and synthetic data using strategies tailored to dataset structure. We find that the optimal strategies depend on dataset structure. Together, these contributions improve the representation of diverse perspectives.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models for Detecting Antisemitism</title>
<link>https://arxiv.org/abs/2509.18293</link>
<guid>https://arxiv.org/abs/2509.18293</guid>
<content:encoded><![CDATA[
arXiv:2509.18293v2 Announce Type: replace 
Abstract: Detecting hateful content is a challenging and important problem. Automated tools, like machine-learning models, can help, but they require continuous training to adapt to the ever-changing landscape of social media. In this work, we evaluate eight open-source LLMs' capability to detect antisemitic content, specifically leveraging in-context definition. We also study how LLMs understand and explain their decisions given a moderation policy as a guideline. First, we explore various prompting techniques and design a new CoT-like prompt, Guided-CoT, and find that injecting domain-specific thoughts increases performance and utility. Guided-CoT handles the in-context policy well, improving performance and utility by reducing refusals across all evaluated models, regardless of decoding configuration, model size, or reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5. Additionally, we examine LLM errors and introduce metrics to quantify semantic divergence in model-generated rationales, revealing notable differences and paradoxical behaviors among LLMs. Our experiments highlight the differences observed across LLMs' utility, explainability, and reliability. Code and resources available at: https://github.com/idramalab/quantify-llm-explanations
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FaStfact: Faster, Stronger Long-Form Factuality Evaluations in LLMs</title>
<link>https://arxiv.org/abs/2510.12839</link>
<guid>https://arxiv.org/abs/2510.12839</guid>
<content:encoded><![CDATA[
arXiv:2510.12839v2 Announce Type: replace 
Abstract: Evaluating the factuality of long-form generations from Large Language Models (LLMs) remains challenging due to efficiency bottlenecks and reliability concerns. Prior efforts attempt this by decomposing text into claims, searching for evidence, and verifying claims, but suffer from critical drawbacks: (1) inefficiency due to overcomplicated pipeline components, and (2) ineffectiveness stemming from inaccurate claim sets and insufficient evidence. To address these limitations, we propose \textbf{FaStfact}, an evaluation framework that achieves the highest alignment with human evaluation and time/token efficiency among existing baselines. FaStfact first employs chunk-level claim extraction integrated with confidence-based pre-verification, significantly reducing the time and token cost while ensuring reliability. For searching and verification, it collects document-level evidence from crawled web-pages and selectively retrieves it during verification. Extensive experiments based on an annotated benchmark \textbf{FaStfact-Bench} demonstrate the reliability of FaStfact in both efficiently and effectively evaluating long-form factuality. Code, benchmark data, and annotation interface tool are available at https://github.com/Yingjia-Wan/FaStfact.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness</title>
<link>https://arxiv.org/abs/2510.13890</link>
<guid>https://arxiv.org/abs/2510.13890</guid>
<content:encoded><![CDATA[
arXiv:2510.13890v2 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved remarkable progress across domains and applications but face challenges such as high fine-tuning costs, inference latency, limited edge deployability, and reliability concerns. Small language models (SLMs), with compact, efficient, and adaptable features, offer promising solutions. Building on this potential, recent research explores collaborative frameworks that integrate their complementary strengths, leveraging SLMs' specialization and efficiency with LLMs' generalization and reasoning to address diverse objectives across tasks and deployment scenarios. Motivated by these developments, this paper presents a systematic survey of SLM-LLM collaboration from the perspective of collaboration objectives. We propose a taxonomy covering four goals: performance enhancement, cost-effectiveness, cloud-edge privacy, and trustworthiness. Under this framework, we review representative methods, summarize design paradigms, and outline open challenges and future directions toward efficient and secure SLM-LLM collaboration. The collected papers are available at https://github.com/FairyFali/SLMs-Survey.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constraint-Driven Small Language Models Based on Agent and OpenAlex Knowledge Graph: Mining Conceptual Pathways and Discovering Innovation Points in Academic Papers</title>
<link>https://arxiv.org/abs/2510.14303</link>
<guid>https://arxiv.org/abs/2510.14303</guid>
<content:encoded><![CDATA[
arXiv:2510.14303v2 Announce Type: replace 
Abstract: In recent years, the rapid increase in academic publications across various fields has posed severe challenges for academic paper analysis: scientists struggle to timely and comprehensively track the latest research findings and methodologies. Key concept extraction has proven to be an effective analytical paradigm, and its automation has been achieved with the widespread application of language models in industrial and scientific domains. However, existing paper databases are mostly limited to similarity matching and basic classification of key concepts, failing to deeply explore the relational networks between concepts. This paper is based on the OpenAlex opensource knowledge graph. By analyzing nearly 8,000 open-source paper data from Novosibirsk State University, we discovered a strong correlation between the distribution patterns of paper key concept paths and both innovation points and rare paths. We propose a prompt engineering-based key concept path analysis method. This method leverages small language models to achieve precise key concept extraction and innovation point identification, and constructs an agent based on a knowledge graph constraint mechanism to enhance analysis accuracy. Through fine-tuning of the Qwen and DeepSeek models, we achieved significant improvements in accuracy, with the models publicly available on the Hugging Face platform.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents</title>
<link>https://arxiv.org/abs/2510.17017</link>
<guid>https://arxiv.org/abs/2510.17017</guid>
<content:encoded><![CDATA[
arXiv:2510.17017v3 Announce Type: replace 
Abstract: Large language model (LLM) based search agents iteratively generate queries, retrieve external information, and reason to answer open-domain questions. While researchers have primarily focused on improving their utility, their safety behaviors remain underexplored. In this paper, we first evaluate search agents using red-teaming datasets and find that they are more likely to produce harmful outputs than base LLMs. For example, when asked "How can I track someone's location without their consent?", a base model refuses, whereas a search agent designed to retrieve and cite sources may lower its refusal threshold, fetch documents (e.g., court cases), and, once appended, synthesize them into an informative yet unsafe summary. We further show that utility-oriented fine-tuning intensifies this risk, motivating joint alignment of safety and utility. We present SafeSearch, a multi-objective reinforcement learning approach that couples a final-output safety/utility reward with a novel query-level shaping term that penalizes unsafe queries and rewards safe ones. Experiments show that SafeSearch reduces agent harmfulness by over 70% across three red-teaming datasets while producing safe, helpful responses, and matches the QA performance of a utility-only finetuned agent; further analyses confirm the effectiveness of the query-level reward in jointly improving safety and utility.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation</title>
<link>https://arxiv.org/abs/2510.19361</link>
<guid>https://arxiv.org/abs/2510.19361</guid>
<content:encoded><![CDATA[
arXiv:2510.19361v2 Announce Type: replace 
Abstract: The creation of high-quality datasets to improve Large Language Model (LLM) reasoning remains a significant challenge, as current methods often suffer from generating low-quality/incorrect answers and limited information richness from available data sources. To address this, we propose AgenticMath, a novel agentic pipeline for generating high-quality mathematical question-answer pairs to enhance the supervised fine-tuning of LLMs. Our method operates through four stages: (1) Seed Question Filter that selects questions with high information richness, complexity, and clarity; (2) an Agentic Question Rephrase step that employs a multi-agent system to generate diverse, logically consistent paraphrases; (3) an Answer Augment step where rewrite answers using chain-of-thought reasoning to enhance numerical and logical correctness, without reliance on human-provided labels; and (4) a final Question and Answer Evaluation that retains only the most superior pairs. Extensive experiments demonstrate that, fine-tuning 3B-8B parameter LLMs on AgenticMath generated datasets (comprising only 30-60K math samples) achieves competitive or superior performance on diverse in domain and out-of-domain mathematical reasoning benchmarks compared to baselines trained on much more data (e.g., 400K or 2.3M samples). Our work demonstrates that targeted, high-quality data generation is a more efficient path to improving mathematical reasoning in LLMs than large-scale, low-quality alternatives.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotion Detection From Social Media Posts</title>
<link>https://arxiv.org/abs/2302.05610</link>
<guid>https://arxiv.org/abs/2302.05610</guid>
<content:encoded><![CDATA[
arXiv:2302.05610v2 Announce Type: replace-cross 
Abstract: Over the last few years, social media has evolved into a medium for expressing personal views, emotions, and even business and political proposals, recommendations, and advertisements. We address the topic of identifying emotions from text data obtained from social media posts like Twitter in this research. We have deployed different traditional machine learning techniques such as Support Vector Machines (SVM), Naive Bayes, Decision Trees, and Random Forest, as well as deep neural network models such as LSTM, CNN, GRU, BiLSTM, BiGRU to classify these tweets into four emotion categories (Fear, Anger, Joy, and Sadness). Furthermore, we have constructed a BiLSTM and BiGRU ensemble model. The evaluation result shows that the deep neural network models(BiGRU, to be specific) produce the most promising results compared to traditional machine learning models, with an 87.53 % accuracy rate. The ensemble model performs even better (87.66 %), albeit the difference is not significant. This result will aid in the development of a decision-making tool that visualizes emotional fluctuations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unifying Symbolic Music Arrangement: Track-Aware Reconstruction and Structured Tokenization</title>
<link>https://arxiv.org/abs/2408.15176</link>
<guid>https://arxiv.org/abs/2408.15176</guid>
<content:encoded><![CDATA[
arXiv:2408.15176v5 Announce Type: replace-cross 
Abstract: We present a unified framework for automatic multitrack music arrangement that enables a single pre-trained symbolic music model to handle diverse arrangement scenarios, including reinterpretation, simplification, and additive generation. At its core is a segment-level reconstruction objective operating on token-level disentangled content and style, allowing for flexible any-to-any instrumentation transformations at inference time. To support track-wise modeling, we introduce REMI-z, a structured tokenization scheme for multitrack symbolic music that enhances modeling efficiency and effectiveness for both arrangement tasks and unconditional generation. Our method outperforms task-specific state-of-the-art models on representative tasks in different arrangement scenarios -- band arrangement, piano reduction, and drum arrangement, in both objective metrics and perceptual evaluations. Taken together, our framework demonstrates strong generality and suggests broader applicability in symbolic music-to-music transformation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs</title>
<link>https://arxiv.org/abs/2410.20749</link>
<guid>https://arxiv.org/abs/2410.20749</guid>
<content:encoded><![CDATA[
arXiv:2410.20749v3 Announce Type: replace-cross 
Abstract: Despite the impressive generative abilities of black-box large language models (LLMs), their inherent opacity hinders further advancements in capabilities such as reasoning, planning, and personalization. Existing works aim to enhance LLM capabilities via domain-specific adaptation, which require additional training on accessible model parameters, an infeasible option for black-box LLMs. To address this challenge, we introduce Matryoshka Pilot (M-Pilot), a lightweight white-box LLM controller that guides a large-scale black-box LLM generator by decomposing complex tasks into a series of intermediate outputs. Specifically, we consider the black-box LLM as an environment, with M-Pilot serving as a policy to provide intermediate guidance through prompts for driving the black-box LLM. M-Pilot is trained to pivot the outputs of the black-box LLM aligning with preferences during iterative interaction, which enables controllable multi-turn generation and self-improvement in optimizing intermediate guidance. Empirical evaluations on diverse tasks demonstrate that our method effectively enhances the capabilities of black-box LLMs in complex, long-horizon tasks. Our code is publicly available at: https://github.com/lichangh20/Matryoshka.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REFA: Reference Free Alignment for multi-preference optimization</title>
<link>https://arxiv.org/abs/2412.16378</link>
<guid>https://arxiv.org/abs/2412.16378</guid>
<content:encoded><![CDATA[
arXiv:2412.16378v4 Announce Type: replace-cross 
Abstract: To mitigate reward hacking from response verbosity, modern preference optimization methods are increasingly adopting length normalization (e.g., SimPO, ORPO, LN-DPO). While effective against this bias, we demonstrate that length normalization itself introduces a failure mode: the URSLA shortcut. Here models learn to satisfy the alignment objective by prematurely truncating low-quality responses rather than learning from their semantic content. To address this, we introduce REFA, a new alignment framework that proposes probabilistic control on a structural token that controls termination. Our core innovation is a new class of regularizers that operate directly on the probability of the End-of-Sequence (EOS) token, a previously unexploited control lever. This token-level intervention provides a principled solution to the URSLA shortcut, ensuring genuine quality improvements. Furthermore, it unlocks a versatile mechanism for managing the alignment-efficiency tradeoff, enabling practitioners to fine-tune models that adhere to specific token budgets. Empirically, REFA achieves a 60.29% win rate and a 52.17% length-controlled win rate on AlpacaEval2 with Llama-3-8B-Instruct, demonstrating the power of our token-level control paradigm.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models</title>
<link>https://arxiv.org/abs/2503.11519</link>
<guid>https://arxiv.org/abs/2503.11519</guid>
<content:encoded><![CDATA[
arXiv:2503.11519v4 Announce Type: replace-cross 
Abstract: Current Cross-Modality Generation Models (GMs) demonstrate remarkable capabilities in various generative tasks. Given the ubiquity and information richness of vision modality inputs in real-world scenarios, Cross-Vision tasks, encompassing Vision-Language Perception (VLP) and Image-to-Image (I2I), have attracted significant attention. Large Vision Language Models (LVLMs) and I2I Generation Models (GMs) are employed to handle VLP and I2I tasks, respectively. Previous research indicates that printing typographic words into input images significantly induces LVLMs and I2I GMs to produce disruptive outputs that are semantically aligned with those words. Additionally, visual prompts, as a more sophisticated form of typography, are also revealed to pose security risks to various applications of cross-vision tasks. However, the specific characteristics of the threats posed by visual prompts remain underexplored. In this paper, to comprehensively investigate the performance impact induced by Typographic Visual Prompt Injection (TVPI) in various LVLMs and I2I GMs, we propose the Typographic Visual Prompts Injection Dataset and thoroughly evaluate the TVPI security risks on various open-source and closed-source LVLMs and I2I GMs under visual prompts with different target semantics, deepening the understanding of TVPI threats.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-Semantics Augmented Few-Shot Relational Learning</title>
<link>https://arxiv.org/abs/2505.05684</link>
<guid>https://arxiv.org/abs/2505.05684</guid>
<content:encoded><![CDATA[
arXiv:2505.05684v4 Announce Type: replace-cross 
Abstract: Few-shot relational learning on knowledge graph (KGs) aims to perform reasoning over relations with only a few training examples. While current methods have focused primarily on leveraging specific relational information, rich semantics inherent in KGs have been largely overlooked. To bridge this gap, we propose PromptMeta, a novel prompted meta-learning framework that seamlessly integrates meta-semantics with relational information for few-shot relational learning. PromptMeta introduces two core innovations: (1) a Meta-Semantic Prompt (MSP) pool that learns and consolidates high-level meta-semantics shared across tasks, enabling effective knowledge transfer and adaptation to newly emerging relations; and (2) a learnable fusion mechanism that dynamically combines meta-semantics with task-specific relational information tailored to different few-shot tasks. Both components are optimized jointly with model parameters within a meta-learning framework. Extensive experiments and analyses on two real-world KG benchmarks validate the effectiveness of PromptMeta in adapting to new relations with limited supervision.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>s3: You Don't Need That Much Data to Train a Search Agent via RL</title>
<link>https://arxiv.org/abs/2505.14146</link>
<guid>https://arxiv.org/abs/2505.14146</guid>
<content:encoded><![CDATA[
arXiv:2505.14146v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieve-entangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. In this work, we propose s3, a lightweight, model-agnostic framework that decouples the searcher from the generator and trains the searcher using a Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG. s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Token Perturbation Guidance for Diffusion Models</title>
<link>https://arxiv.org/abs/2506.10036</link>
<guid>https://arxiv.org/abs/2506.10036</guid>
<content:encoded><![CDATA[
arXiv:2506.10036v2 Announce Type: replace-cross 
Abstract: Classifier-free guidance (CFG) has become an essential component of modern diffusion models to enhance both generation quality and alignment with input conditions. However, CFG requires specific training procedures and is limited to conditional generation. To address these limitations, we propose Token Perturbation Guidance (TPG), a novel method that applies perturbation matrices directly to intermediate token representations within the diffusion network. TPG employs a norm-preserving shuffling operation to provide effective and stable guidance signals that improve generation quality without architectural changes. As a result, TPG is training-free and agnostic to input conditions, making it readily applicable to both conditional and unconditional generation. We further analyze the guidance term provided by TPG and show that its effect on sampling more closely resembles CFG compared to existing training-free guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1 show that TPG achieves nearly a 2$\times$ improvement in FID for unconditional generation over the SDXL baseline, while closely matching CFG in prompt alignment. These results establish TPG as a general, condition-agnostic guidance method that brings CFG-like benefits to a broader class of diffusion models.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dense SAE Latents Are Features, Not Bugs</title>
<link>https://arxiv.org/abs/2506.15679</link>
<guid>https://arxiv.org/abs/2506.15679</guid>
<content:encoded><![CDATA[
arXiv:2506.15679v2 Announce Type: replace-cross 
Abstract: Sparse autoencoders (SAEs) are designed to extract interpretable features from language models by enforcing a sparsity constraint. Ideally, training an SAE would yield latents that are both sparse and semantically meaningful. However, many SAE latents activate frequently (i.e., are \emph{dense}), raising concerns that they may be undesirable artifacts of the training procedure. In this work, we systematically investigate the geometry, function, and origin of dense latents and show that they are not only persistent but often reflect meaningful model representations. We first demonstrate that dense latents tend to form antipodal pairs that reconstruct specific directions in the residual stream, and that ablating their subspace suppresses the emergence of new dense features in retrained SAEs -- suggesting that high density features are an intrinsic property of the residual space. We then introduce a taxonomy of dense latents, identifying classes tied to position tracking, context binding, entropy regulation, letter-specific output signals, part-of-speech, and principal component reconstruction. Finally, we analyze how these features evolve across layers, revealing a shift from structural features in early layers, to semantic features in mid layers, and finally to output-oriented signals in the last layers of the model. Our findings indicate that dense latents serve functional roles in language model computation and should not be dismissed as training noise.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activation Transport Operators</title>
<link>https://arxiv.org/abs/2508.17540</link>
<guid>https://arxiv.org/abs/2508.17540</guid>
<content:encoded><![CDATA[
arXiv:2508.17540v2 Announce Type: replace-cross 
Abstract: The residual stream mediates communication between transformer decoder layers via linear reads and writes of non-linear computations. While sparse-dictionary learning-based methods locate features in the residual stream, and activation patching methods discover circuits within the model, the mechanism by which features flow through the residual stream remains understudied. Understanding this dynamic can better inform jailbreaking protections, enable early detection of model mistakes, and their correction. In this work, we propose Activation Transport Operators (ATO), linear maps from upstream to downstream residuals $k$ layers later, evaluated in feature space using downstream SAE decoder projections. We empirically demonstrate that these operators can determine whether a feature has been linearly transported from a previous layer or synthesised from non-linear layer computation. We develop the notion of transport efficiency, for which we provide an upper bound, and use it to estimate the size of the residual stream subspace that corresponds to linear transport. We empirically demonstrate the linear transport, report transport efficiency and the size of the residual stream's subspace involved in linear transport. This compute-light (no finetuning, <50 GPU-h) method offers practical tools for safety, debugging, and a clearer picture of where computation in LLMs behaves linearly.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GDS Agent for Graph Algorithmic Reasoning</title>
<link>https://arxiv.org/abs/2508.20637</link>
<guid>https://arxiv.org/abs/2508.20637</guid>
<content:encoded><![CDATA[
arXiv:2508.20637v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown remarkable multimodal information processing and reasoning ability. When equipped with tools through function calling and enhanced with retrieval-augmented techniques, compound LLM-based systems can access closed data sources and answer questions about them. However, they still struggle to process and reason over large-scale graph-structure data. We introduce the GDS (Graph Data Science) agent in this technical report. The GDS agent introduces a comprehensive set of graph algorithms as tools, together with preprocessing (retrieval) and postprocessing of algorithm results, in a model context protocol (MCP) server. The server can be used with any modern LLM out-of-the-box. GDS agent allows users to ask any question that implicitly and intrinsically requires graph algorithmic reasoning about their data, and quickly obtain accurate and grounded answers. We introduce new benchmarks that evaluate intermediate tool calls as well as final responses. The results indicate that GDS agent is able to solve a wide spectrum of graph tasks. We also provide detailed case studies for more open-ended tasks and study scenarios where the agent struggles. Finally, we discuss the remaining challenges and the future roadmap.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning Foundations for Deep Research Systems: A Survey</title>
<link>https://arxiv.org/abs/2509.06733</link>
<guid>https://arxiv.org/abs/2509.06733</guid>
<content:encoded><![CDATA[
arXiv:2509.06733v2 Announce Type: replace-cross 
Abstract: Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.
  This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes recent work along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TABLET: A Large-Scale Dataset for Robust Visual Table Understanding</title>
<link>https://arxiv.org/abs/2509.21205</link>
<guid>https://arxiv.org/abs/2509.21205</guid>
<content:encoded><![CDATA[
arXiv:2509.21205v2 Announce Type: replace-cross 
Abstract: While table understanding increasingly relies on pixel-only settings where tables are processed as visual representations, current benchmarks predominantly use synthetic renderings that lack the complexity and visual diversity of real-world tables. Additionally, existing visual table understanding (VTU) datasets offer fixed examples with single visualizations and pre-defined instructions, providing no access to underlying serialized data for reformulation. We introduce TABLET, a large-scale VTU dataset with 4 million examples across 20 tasks, grounded in 2 million unique tables where 88% preserve original visualizations. Each example includes paired image-HTML representations, comprehensive metadata, and provenance information linking back to the source datasets. Fine-tuning vision-language models like Qwen2.5-VL-7B on TABLET improves performance on seen and unseen VTU tasks while increasing robustness on real-world table visualizations. By preserving original visualizations and maintaining example traceability in a unified large-scale collection, TABLET establishes a foundation for robust training and extensible evaluation of future VTU models.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Optimal Large Diffusion Language Models</title>
<link>https://arxiv.org/abs/2510.03280</link>
<guid>https://arxiv.org/abs/2510.03280</guid>
<content:encoded><![CDATA[
arXiv:2510.03280v2 Announce Type: replace-cross 
Abstract: We introduce Quokka, the first systematic scaling law for diffusion language models (DLMs), encompassing both compute-constrained and data-constrained regimes, and studying the key modeling and optimization designs. Quokka is a good friend of Chinchilla and provides wider scopes. We hope the results would bring short-term practical guidance in DLMs training and long-term inspirations for the whole AI community.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VoiceAgentBench: Are Voice Assistants ready for agentic tasks?</title>
<link>https://arxiv.org/abs/2510.07978</link>
<guid>https://arxiv.org/abs/2510.07978</guid>
<content:encoded><![CDATA[
arXiv:2510.07978v2 Announce Type: replace-cross 
Abstract: Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants capable of understanding natural spoken queries and performing complex tasks. However, existing speech benchmarks primarily focus on isolated capabilities such as transcription, or question-answering, and do not systematically evaluate agentic scenarios encompassing multilingual and cultural understanding, as well as adversarial robustness. To address this, we introduce VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in realistic spoken agentic settings. It comprises over 5,500 synthetic spoken queries, including dialogues grounded in Indian context, covering single-tool invocations, multi-tool workflows, multi-turn interactions, and safety evaluations. The benchmark supports English, Hindi, and 5 other Indian languages, reflecting real-world linguistic and cultural diversity. We simulate speaker variability using a novel sampling algorithm that selects audios for TTS voice conversion based on its speaker embeddings, maximizing acoustic and speaker diversity. Our evaluation measures tool selection accuracy, structural consistency, and the correctness of tool invocations, including adversarial robustness. Our experiments reveal significant gaps in contextual tool orchestration tasks, Indic generalization, and adversarial robustness, exposing critical limitations of current SpeechLMs.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction</title>
<link>https://arxiv.org/abs/2510.18938</link>
<guid>https://arxiv.org/abs/2510.18938</guid>
<content:encoded><![CDATA[
arXiv:2510.18938v2 Announce Type: replace-cross 
Abstract: Over 70 million people worldwide experience stuttering, yet most automatic speech systems misinterpret disfluent utterances or fail to transcribe them accurately. Existing methods for stutter correction rely on handcrafted feature extraction or multi-stage automatic speech recognition (ASR) and text-to-speech (TTS) pipelines, which separate transcription from audio reconstruction and often amplify distortions. This work introduces StutterZero and StutterFormer, the first end-to-end waveform-to-waveform models that directly convert stuttered speech into fluent speech while jointly predicting its transcription. StutterZero employs a convolutional-bidirectional LSTM encoder-decoder with attention, whereas StutterFormer integrates a dual-stream Transformer with shared acoustic-linguistic representations. Both architectures are trained on paired stuttered-fluent data synthesized from the SEP-28K and LibriStutter corpora and evaluated on unseen speakers from the FluencyBank dataset. Across all benchmarks, StutterZero had a 24% decrease in Word Error Rate (WER) and a 31% improvement in semantic similarity (BERTScore) compared to the leading Whisper-Medium model. StutterFormer achieved better results, with a 28% decrease in WER and a 34% improvement in BERTScore. The results validate the feasibility of direct end-to-end stutter-to-fluent speech conversion, offering new opportunities for inclusive human-computer interaction, speech therapy, and accessibility-oriented AI systems.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Personality Generation of LLMs at Decoding-time</title>
<link>https://arxiv.org/abs/2511.01891</link>
<guid>https://arxiv.org/abs/2511.01891</guid>
<content:encoded><![CDATA[
<div> MPG, Multi-Personality Generation, LLMs, retraining-based approaches, decoding-time methods <br />
Summary: 
The paper introduces a novel framework called Multi-Personality Generation (MPG) for Language Model Models (LLMs) that allows for the simultaneous embodiment of multiple personalization attributes. Unlike existing approaches which are costly and lack scalability, MPG operates under the decoding-time combination paradigm, enabling flexible control over multi-personality without the need for complex models or additional training. The framework leverages implicit density ratios in single-dimensional models to facilitate sampling from a target strategy that aggregates these ratios. To efficiently implement MPG, the paper introduces Speculative Chunk-level based Rejection sampling (SCR) that generates responses in chunks and parallelly validates them, reducing computational overhead while maintaining high-quality generation. Experimental results on MBTI personality and Role-Playing datasets demonstrate the effectiveness of MPG, showcasing improvements of up to 16%-18%. The code and data for MPG are available on GitHub for further exploration. <br /> <div>
arXiv:2511.01891v1 Announce Type: new 
Abstract: Multi-personality generation for LLMs, enabling simultaneous embodiment of multiple personalization attributes, is a fundamental challenge. Existing retraining-based approaches are costly and poorly scalable, while decoding-time methods often rely on external models or heuristics, limiting flexibility and robustness. In this paper, we propose a novel Multi-Personality Generation (MPG) framework under the decoding-time combination paradigm. It flexibly controls multi-personality without relying on scarce multi-dimensional models or extra training, leveraging implicit density ratios in single-dimensional models as a "free lunch" to reformulate the task as sampling from a target strategy aggregating these ratios. To implement MPG efficiently, we design Speculative Chunk-level based Rejection sampling (SCR), which generates responses in chunks and parallelly validates them via estimated thresholds within a sliding window. This significantly reduces computational overhead while maintaining high-quality generation. Experiments on MBTI personality and Role-Playing demonstrate the effectiveness of MPG, showing improvements up to 16%-18%. Code and data are available at https://github.com/Libra117/MPG .
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking LLM Human Simulation: When a Graph is What You Need</title>
<link>https://arxiv.org/abs/2511.02135</link>
<guid>https://arxiv.org/abs/2511.02135</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, Large Language Models, Simulation, Discrete Choice, GEMS<br />
Summary:<br />
The article discusses the use of large language models (LLMs) in simulating human behavior and explores the potential of smaller, domain-grounded models. The authors propose Graph-basEd Models for human Simulation (GEMS), which leverage graph neural networks (GNNs) to simulate discrete choice tasks efficiently and effectively. By casting simulation tasks as link prediction problems on graphs, GEMS achieves comparable or superior accuracy to LLMs while being three orders of magnitude smaller. The approach combines relational knowledge with language representations only when necessary, leading to improved efficiency, interpretability, and transparency in simulations. Evaluations across various datasets demonstrate the promise of graph-based modeling as a lightweight alternative to LLMs for human simulation. The code for GEMS is publicly available for further exploration and application. <br />Summary: <div>
arXiv:2511.02135v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used to simulate humans, with applications ranging from survey prediction to decision-making. However, are LLMs strictly necessary, or can smaller, domain-grounded models suffice? We identify a large class of simulation problems in which individuals make choices among discrete options, where a graph neural network (GNN) can match or surpass strong LLM baselines despite being three orders of magnitude smaller. We introduce Graph-basEd Models for human Simulation (GEMS), which casts discrete choice simulation tasks as a link prediction problem on graphs, leveraging relational knowledge while incorporating language representations only when needed. Evaluations across three key settings on three simulation datasets show that GEMS achieves comparable or better accuracy than LLMs, with far greater efficiency, interpretability, and transparency, highlighting the promise of graph-based modeling as a lightweight alternative to LLMs for human simulation. Our code is available at https://github.com/schang-lab/gems.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IG-Pruning: Input-Guided Block Pruning for Large Language Models</title>
<link>https://arxiv.org/abs/2511.02213</link>
<guid>https://arxiv.org/abs/2511.02213</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, depth pruning, dynamic selection, semantic clustering, L0 optimization

Summary:
IG-Pruning introduces a novel approach for efficient inference in large language models by dynamically selecting layer masks at inference time. The method involves discovering diverse mask candidates through semantic clustering and L0 optimization, followed by implementing efficient dynamic pruning without extensive training. Experimental results demonstrate superior performance compared to existing static depth pruning methods, making IG-Pruning suitable for resource-constrained deployment scenarios.
<br /><br />Summary: <div>
arXiv:2511.02213v1 Announce Type: new 
Abstract: With the growing computational demands of large language models (LLMs), efficient inference has become increasingly critical for practical deployment. Depth pruning has emerged as a promising approach for reducing the computational costs of large language models by removing transformer layers. However, existing methods typically rely on fixed block masks, which can lead to suboptimal performance across different tasks and inputs. In this paper, we propose IG-Pruning, a novel input-aware block-wise pruning method that dynamically selects layer masks at inference time. Our approach consists of two stages: (1) Discovering diverse mask candidates through semantic clustering and L0 optimization, and (2) Implementing efficient dynamic pruning without the need for extensive training. Experimental results demonstrate that our method consistently outperforms state-of-the-art static depth pruning methods, making it particularly suitable for resource-constrained deployment scenarios.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results</title>
<link>https://arxiv.org/abs/2511.02246</link>
<guid>https://arxiv.org/abs/2511.02246</guid>
<content:encoded><![CDATA[
<div> detecting errors, medical chatbots, language models, evaluating answers, inter-LLM agreement  
Summary:  
This study investigates the performance of medical chatbots powered by large language models (LLMs) by developing an infrastructure to automatically generate queries and evaluate the responses. The study focuses on detecting errors such as hallucinations and omissions in LLM-generated advice for medical contexts where demographic information is crucial. The research highlights the importance of using multiple LLMs for evaluation to ensure generalizability of results and recommends transparency in reporting inter-LLM agreement metrics. The findings suggest that LLM annotators show low agreement levels and specific LLM pairs yield significant differences across demographic factors. The study emphasizes the need for robust evaluation methods and diverse LLM usage for accurate assessment of medical chatbots. <div>
arXiv:2511.02246v1 Announce Type: new 
Abstract: Recent research has shown that hallucinations, omissions, and biases are prevalent in everyday use-cases of LLMs. However, chatbots used in medical contexts must provide consistent advice in situations where non-medical factors are involved, such as when demographic information is present. In order to understand the conditions under which medical chatbots fail to perform as expected, we develop an infrastructure that 1) automatically generates queries to probe LLMs and 2) evaluates answers to these queries using multiple LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples the space of patient demographics, histories, disorders, and writing styles to create realistic questions that we subsequently use to prompt LLMs. In 2), our evaluation pipeline provides hallucination and omission detection using LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge treatment category detectors. As a baseline study, we perform two case studies on inter-LLM agreement and the impact of varying the answering and evaluation LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's Kappa $\kappa=0.118$), and only specific (answering, evaluation) LLM pairs yield statistically significant differences across writing styles, genders, and races. We recommend that studies using LLM evaluation use multiple LLMs as evaluators in order to avoid arriving at statistically significant but non-generalizable results, particularly in the absence of ground-truth data. We also suggest publishing inter-LLM agreement metrics for transparency. Our code and dataset are available here: https://github.com/BBN-E/medic-neurips-2025-demo.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LTD-Bench: Evaluating Large Language Models by Letting Them Draw</title>
<link>https://arxiv.org/abs/2511.02347</link>
<guid>https://arxiv.org/abs/2511.02347</guid>
<content:encoded><![CDATA[
<div> evaluation paradigms, large language models, spatial reasoning, LTD-Bench, benchmark

Summary:
LTD-Bench introduces a new benchmark for evaluating large language models (LLMs) that focuses on spatial reasoning capabilities through visual outputs. Traditional metrics hide limitations in spatial understanding, leading to a disconnect between reported performance and practical skills. The benchmark tasks LLMs to generate drawings or code, making spatial reasoning deficiencies immediately visible. Experiments show that even top-performing LLMs struggle to establish bi-directional mappings between language and spatial concepts. LTD-Bench offers a comprehensive methodology with tasks of varying difficulty levels to assess spatial imagination and perception. The visual outputs enable thorough diagnostic analysis and potential investigations into model similarities. This approach aims to address the fundamental gap between statistical performance and intuitive evaluation, highlighting the need for improved spatial reasoning capabilities in LLMs. 

<br /><br />Summary: <div>
arXiv:2511.02347v1 Announce Type: new 
Abstract: Current evaluation paradigms for large language models (LLMs) represent a critical blind spot in AI research--relying on opaque numerical metrics that conceal fundamental limitations in spatial reasoning while providing no intuitive understanding of model capabilities. This deficiency creates a dangerous disconnect between reported performance and practical abilities, particularly for applications requiring physical world understanding. We introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation from abstract scores to directly observable visual outputs by requiring models to generate drawings through dot matrices or executable code. This approach makes spatial reasoning limitations immediately apparent even to non-experts, bridging the fundamental gap between statistical performance and intuitive assessment. LTD-Bench implements a comprehensive methodology with complementary generation tasks (testing spatial imagination) and recognition tasks (assessing spatial perception) across three progressively challenging difficulty levels, methodically evaluating both directions of the critical language-spatial mapping. Our extensive experiments with state-of-the-art models expose an alarming capability gap: even LLMs achieving impressive results on traditional benchmarks demonstrate profound deficiencies in establishing bidirectional mappings between language and spatial concept--a fundamental limitation that undermines their potential as genuine world models. Furthermore, LTD-Bench's visual outputs enable powerful diagnostic analysis, offering a potential approach to investigate model similarity.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation</title>
<link>https://arxiv.org/abs/2511.02358</link>
<guid>https://arxiv.org/abs/2511.02358</guid>
<content:encoded><![CDATA[
<div> Keywords: Query augmentation, Large Language Model, Multimodal environment, M-Solomon, Adaptive query augmentation<br />
<br />
Summary: M-Solomon is a proposed universal multimodal embedder that can adaptively determine when to augment queries. It divides queries into two groups at the dataset level, deciding which require augmentation and which do not. The approach leverages a Multimodal Large Language Model (MLLM) for synthesizing appropriate augmentations for queries that need them. The system then employs adaptive query augmentation, generating synthetic augmentations only for queries that require them, significantly improving performance and reducing embedding latency compared to baseline methods. Experimental results demonstrate that M-Solomon outperforms baselines both with and without augmentation, showcasing its effectiveness in enhancing query understanding and document retrieval in multimodal environments. <br /><br />Summary: <div>
arXiv:2511.02358v1 Announce Type: new 
Abstract: Query augmentation makes queries more meaningful by appending further information to the queries to find relevant documents. Current studies have proposed Large Language Model (LLM)-based embedders, which learn representation for embedding and generation for query augmentation in a multi-task manner by leveraging the generative capabilities of LLM. During inference, these jointly trained embedders have conducted query augmentation followed by embedding, showing effective results. However, augmenting every query leads to substantial embedding latency and query augmentation can be detrimental to performance for some queries. Also, previous methods have not been explored in multimodal environments. To tackle these problems, we propose M-Solomon, a universal multimodal embedder that can adaptively determine when to augment queries. Our approach first divides the queries of the training datasets into two groups at the dataset level. One includes queries that require augmentation and the other includes queries that do not. Then, we introduces a synthesis process that generates appropriate augmentations for queries that require them by leveraging a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation. Through this step, M-Solomon can conduct query augmentation only when necessary by learning to generate synthetic augmentations with the prefix /augment for queries that demand them and to generate the simple string /embed for others. Experimental results showed that M-Solomon not only surpassed the baseline without augmentation by a large margin but also outperformed the baseline that always used augmentation, providing much faster embedding latency.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context</title>
<link>https://arxiv.org/abs/2511.02366</link>
<guid>https://arxiv.org/abs/2511.02366</guid>
<content:encoded><![CDATA[
<div> Keywords: LiveSecBench, Chinese language, LLM application, AI safety, benchmark

Summary:
LiveSecBench is a new safety benchmark specifically designed for Chinese-language Large Language Models (LLMs). It evaluates models across six critical dimensions, including Legality, Ethics, Factuality, Privacy, Adversarial Robustness, and Reasoning Safety, rooted in Chinese legal and social frameworks. The benchmark is continuously updated to stay relevant and current with new threat vectors. The latest version, LiveSecBench (v251030), has assessed 18 LLMs, providing insights into AI safety in the context of the Chinese language. The benchmark's leaderboard is publicly accessible online at https://livesecbench.intokentech.cn/. In the next update, new dimensions such as Text-to-Image Generation Safety and Agentic Safety will be included, expanding the evaluation criteria for LLMs in Chinese language applications.<br /><br />Summary: 
- LiveSecBench evaluates LLMs for Chinese language applications across six dimensions rooted in Chinese legal and social frameworks.
- The benchmark is continuously updated to incorporate new threat vectors and maintain relevance.
- The latest version has assessed 18 LLMs, providing insights into AI safety in the Chinese language context. 
- A public leaderboard showcasing the evaluated models is available online.
- Future updates will include additional dimensions for evaluating LLMs in Chinese applications. <div>
arXiv:2511.02366v1 Announce Type: new 
Abstract: In this work, we propose LiveSecBench, a dynamic and continuously updated safety benchmark specifically for Chinese-language LLM application scenarios. LiveSecBench evaluates models across six critical dimensions (Legality, Ethics, Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in the Chinese legal and social frameworks. This benchmark maintains relevance through a dynamic update schedule that incorporates new threat vectors, such as the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs, providing a landscape of AI safety in the context of Chinese language. The leaderboard is publicly accessible at https://livesecbench.intokentech.cn/.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda</title>
<link>https://arxiv.org/abs/2511.02374</link>
<guid>https://arxiv.org/abs/2511.02374</guid>
<content:encoded><![CDATA[
<div> AyurParam-2.9B, domain-specialized, bilingual language model, fine-tuned from Param-1-2.9B using extensive Ayurveda dataset. Benchmark on BhashaBench-Ayur shows superior performance to larger models.<br /><br />Summary: Current large language models struggle with specialized fields like Ayurveda due to lack of cultural and subject-matter expertise. AyurParam-2.9B, a domain-specific language model, was developed by fine-tuning Param-1-2.9B on a curated Ayurveda dataset. The dataset includes context-aware Q&amp;A in English and Hindi with precise annotations. AyurParam outperforms instruction-tuned models in its parameter size class and competes favorably with larger models. The study underscores the importance of authentic domain adaptation and quality supervision to deliver accurate AI for specialized medical knowledge. <div>
arXiv:2511.02374v1 Announce Type: new 
Abstract: Current large language models excel at broad, general-purpose tasks, but consistently underperform when exposed to highly specialized domains that require deep cultural, linguistic, and subject-matter expertise. In particular, traditional medical systems such as Ayurveda embody centuries of nuanced textual and clinical knowledge that mainstream LLMs fail to accurately interpret or apply. We introduce AyurParam-2.9B, a domain-specialized, bilingual language model fine-tuned from Param-1-2.9B using an extensive, expertly curated Ayurveda dataset spanning classical texts and clinical guidance. AyurParam's dataset incorporates context-aware, reasoning, and objective-style Q&amp;A in both English and Hindi, with rigorous annotation protocols for factual precision and instructional clarity. Benchmarked on BhashaBench-Ayur, AyurParam not only surpasses all open-source instruction-tuned models in its size class (1.5--3B parameters), but also demonstrates competitive or superior performance compared to much larger models. The results from AyurParam highlight the necessity for authentic domain adaptation and high-quality supervision in delivering reliable, culturally congruent AI for specialized medical knowledge.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models</title>
<link>https://arxiv.org/abs/2511.02376</link>
<guid>https://arxiv.org/abs/2511.02376</guid>
<content:encoded><![CDATA[
<div> vulnerable, jailbreaking attacks, multi-turn conversations, AutoAdv, adaptive mechanisms <br />
<br />Summary: Large Language Models (LLMs) are vulnerable to jailbreaking attacks that exploit multi-turn conversations, but most evaluations focus on single-turn interactions. The AutoAdv framework presents a training-free approach for automated multi-turn jailbreaking, achieving up to 95% attack success rate on Llama-3.1-8B within six turns. It combines three adaptive mechanisms: a pattern manager, a temperature manager, and a two-phase rewriting strategy. AutoAdv shows persistent vulnerabilities in current safety mechanisms across various models, with multi-turn attacks consistently outperforming single-turn approaches. This highlights the need for multi-turn-aware defenses to improve the robustness of language models against jailbreaking attacks. <br /> <div>
arXiv:2511.02376v1 Announce Type: new 
Abstract: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where adversarial prompts elicit harmful outputs, yet most evaluations focus on single-turn interactions while real-world attacks unfold through adaptive multi-turn conversations. We present AutoAdv, a training-free framework for automated multi-turn jailbreaking that achieves up to 95% attack success rate on Llama-3.1-8B within six turns a 24 percent improvement over single turn baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern manager that learns from successful attacks to enhance future prompts, a temperature manager that dynamically adjusts sampling parameters based on failure modes, and a two-phase rewriting strategy that disguises harmful requests then iteratively refines them. Extensive evaluation across commercial and open-source models (GPT-4o-mini, Qwen3-235B, Mistral-7B) reveals persistent vulnerabilities in current safety mechanisms, with multi-turn attacks consistently outperforming single-turn approaches. These findings demonstrate that alignment strategies optimized for single-turn interactions fail to maintain robustness across extended conversations, highlighting an urgent need for multi-turn-aware defenses.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case Study in Finance</title>
<link>https://arxiv.org/abs/2511.02451</link>
<guid>https://arxiv.org/abs/2511.02451</guid>
<content:encoded><![CDATA[
<div> finance, mathematical reasoning, language processing, LLMs, domain-specific<br />
<br />
Summary: 
This study explores the merging of domain-specific Continual Pre-training (CPT) experts to create financial Language Model Models (LLMs) in finance, mathematics, and Japanese. Three merging methods (Task Arithmetic, TIES, and DARE-TIES) are evaluated on a financial benchmark comprising 18 tasks across 8 datasets. The results demonstrate that merging an expert with its base model helps recover general knowledge lost during CPT, while merging experts enhances performance and can lead to the emergence of cross-domain skills. Task Arithmetic shows strong performance but is sensitive to hyperparameters, while TIES is more robust. Model similarity is found to be correlated with merging success, but emergent skills depend on complex factors. This study lays the groundwork for CPT model merging, providing a framework for developing multi-skill LLMs from existing resources. <br /><br /> <div>
arXiv:2511.02451v1 Announce Type: new 
Abstract: While LLMs excel at general tasks, they struggle in specialized domains like finance, requiring diverse skills in domain knowledge, mathematical reasoning, and multilingual processing. Merging domain-specific Continual Pre-training (CPT) "experts" offers a practical alternative to costly and unstable multi-skill training. However, unlike established Supervised Fine-Tuning (SFT) model-based merging, CPT model merging remains largely unexplored. We address this gap by creating financial LLMs from experts in finance, math, and Japanese. We propose a three-stage evaluation focusing on knowledge recovery, complementarity, and emergence, and assess three merging methods (Task Arithmetic, TIES, and DARE-TIES) on a comprehensive financial benchmark curated from 18 tasks across 8 established datasets. Results show that merging an expert with its base model recovers general knowledge lost during CPT, while merging experts improves performance and can yield emergent cross-domain skills. Among the methods, Task Arithmetic performs strongly but is hyperparameter-sensitive, whereas TIES is more robust. Our findings also suggest that while model similarity correlates with merging success, emergent skills depend on more complex factors. This work presents the first foundational analysis of CPT model merging, establishing a principled framework and providing clear guidance for building multi-skill LLMs from existing assets.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas</title>
<link>https://arxiv.org/abs/2511.02458</link>
<guid>https://arxiv.org/abs/2511.02458</guid>
<content:encoded><![CDATA[
<div> persona-based prompting, Large Language Model, macroeconomic forecasting, GPT-4o, PersonaHub corpus <br />
<br />
Summary: 
The study evaluates the impact of persona-based prompting on Large Language Model (LLM) performance in macroeconomic forecasting tasks using GPT-4o. It compares GPT-4o's forecasts prompted by economics-related personas from the PersonaHub corpus with human expert forecasts for the ECB Survey of Professional Forecasters. The results show that GPT-4o and human forecasters achieve similar accuracy levels, with GPT-4o demonstrating competitive performance even on out-of-sample data. Additionally, the study finds that persona descriptions do not provide a measurable forecasting advantage, suggesting they can be omitted to reduce computational costs without sacrificing accuracy. The findings highlight the potential of LLMs like GPT-4o in macroeconomic forecasting when provided with relevant context data.

<br /> <div>
arXiv:2511.02458v1 Announce Type: new 
Abstract: We evaluate whether persona-based prompting improves Large Language Model (LLM) performance on macroeconomic forecasting tasks. Using 2,368 economics-related personas from the PersonaHub corpus, we prompt GPT-4o to replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds (2013-2025). We compare the persona-prompted forecasts against the human experts panel, across four target variables (HICP, core HICP, GDP growth, unemployment) and four forecast horizons. We also compare the results against 100 baseline forecasts without persona descriptions to isolate its effect. We report two main findings. Firstly, GPT-4o and human forecasters achieve remarkably similar accuracy levels, with differences that are statistically significant yet practically modest. Our out-of-sample evaluation on 2024-2025 data demonstrates that GPT-4o can maintain competitive forecasting performance on unseen events, though with notable differences compared to the in-sample period. Secondly, our ablation experiment reveals no measurable forecasting advantage from persona descriptions, suggesting these prompt components can be omitted to reduce computational costs without sacrificing accuracy. Our results provide evidence that GPT-4o can achieve competitive forecasting accuracy even on out-of-sample macroeconomic events, if provided with relevant context data, while revealing that diverse prompts produce remarkably homogeneous forecasts compared to human panels.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smart-Hiring: An Explainable end-to-end Pipeline for CV Information Extraction and Job Matching</title>
<link>https://arxiv.org/abs/2511.02537</link>
<guid>https://arxiv.org/abs/2511.02537</guid>
<content:encoded><![CDATA[
<div> resume, hiring process, Natural Language Processing, semantic matching, bias mitigation

Summary:<br />
The paper introduces Smart-Hiring, an NLP pipeline designed for automating the process of resume screening and candidate-job matching. The system utilizes document parsing, named-entity recognition, and text embedding techniques to extract and match skills, experience, and qualifications from resumes and job descriptions. By encoding information in a shared vector space, Smart-Hiring computes similarity scores to facilitate efficient matching. The modular and explainable nature of the pipeline enables users to verify extracted entities and matching decisions. Experiments on real-world data showcase the system's accuracy and interpretability. The work also highlights potential directions for addressing bias, fairness, and scalability in data-driven hiring solutions. Overall, Smart-Hiring offers a scalable and practical framework for recruitment analytics, emphasizing transparency and accuracy in the hiring process. 

Summary: <div>
arXiv:2511.02537v1 Announce Type: new 
Abstract: Hiring processes often involve the manual screening of hundreds of resumes for each job, a task that is time and effort consuming, error-prone, and subject to human bias. This paper presents Smart-Hiring, an end-to-end Natural Language Processing (NLP) pipeline de- signed to automatically extract structured information from unstructured resumes and to semantically match candidates with job descriptions. The proposed system combines document parsing, named-entity recognition, and contextual text embedding techniques to capture skills, experience, and qualifications. Using advanced NLP technics, Smart-Hiring encodes both resumes and job descriptions in a shared vector space to compute similarity scores between candidates and job postings. The pipeline is modular and explainable, allowing users to inspect extracted entities and matching rationales. Experiments were conducted on a real-world dataset of resumes and job descriptions spanning multiple professional domains, demonstrating the robustness and feasibility of the proposed approach. The system achieves competitive matching accuracy while preserving a high degree of interpretability and transparency in its decision process. This work introduces a scalable and practical NLP frame- work for recruitment analytics and outlines promising directions for bias mitigation, fairness-aware modeling, and large-scale deployment of data-driven hiring solutions.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Analysis of Lexical Errors in Machine Translation from English into Romanian</title>
<link>https://arxiv.org/abs/2511.02587</link>
<guid>https://arxiv.org/abs/2511.02587</guid>
<content:encoded><![CDATA[
<div> Machine Translation, Error Analysis, Lexical Errors, Google Translate, Covid-19<br />
Summary:<br />
The research investigates lexical errors in Machine Translation from English to Romanian using texts related to Covid-19. Focusing on texts from the WHO, Gavi Organization, and patient information leaflets, translated by Google Translate, the study aims to improve the quality of translations by enhancing lexical selection and reducing errors. With a comprehensive analysis of 230 translated texts, the research aims to contribute to the overall improvement of automatic translation systems like Google Translate. <div>
arXiv:2511.02587v1 Announce Type: new 
Abstract: The research explores error analysis in the performance of translating by Machine Translation from English into Romanian, and it focuses on lexical errors found in texts which include official information, provided by the World Health Organization (WHO), the Gavi Organization, by the patient information leaflet (the information about the active ingredients of the vaccines or the medication, the indications, the dosage instructions, the storage instructions, the side effects and warning, etc.). All of these texts are related to Covid-19 and have been translated by Google Translate, a multilingual Machine Translation that was created by Google. In the last decades, Google has actively worked to develop a more accurate and fluent automatic translation system. This research, specifically focused on improving Google Translate, aims to enhance the overall quality of Machine Translation by achieving better lexical selection and by reducing errors. The investigation involves a comprehensive analysis of 230 texts that have been translated from English into Romanian.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour</title>
<link>https://arxiv.org/abs/2511.02599</link>
<guid>https://arxiv.org/abs/2511.02599</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Tracing, AI in education, Large Language Models, Next-Token Prediction, Predictive Performance

Summary: 
Next Token Knowledge Tracing (NTKT) is a novel approach to modelling student knowledge in AI-powered education systems. By reframing Knowledge Tracing as a next-token prediction task using Large Language Models (LLMs), NTKT incorporates question text along with student response data to improve predictive performance. Through a series of experiments, NTKT outperforms existing neural KT models and exhibits better generalization to new questions and users. The inclusion of question content in the NTKT model proves to be essential in accurately predicting student responses and enhancing personalized learning experiences. Leveraging pretrained representations of LLMs enables NTKT to learn patterns in both student behavior and language, leading to more effective modelling of student learning processes. This study underscores the significance of considering question content in Knowledge Tracing models and highlights the benefits of using advanced AI techniques in educational settings.
 <div>
arXiv:2511.02599v1 Announce Type: new 
Abstract: Modelling student knowledge is a key challenge when leveraging AI in education, with major implications for personalised learning. The Knowledge Tracing (KT) task aims to predict how students will respond to educational questions in learning environments, based on their prior interactions. Existing KT models typically use response correctness along with metadata like skill tags and timestamps, often overlooking the question text, which is an important source of pedagogical insight. This omission poses a lost opportunity while limiting predictive performance. We propose Next Token Knowledge Tracing (NTKT), a novel approach that reframes KT as a next-token prediction task using pretrained Large Language Models (LLMs). NTKT represents both student histories and question content as sequences of text, allowing LLMs to learn patterns in both behaviour and language. Our series of experiments significantly improves performance over state-of-the-art neural KT models and generalises much better to cold-start questions and users. These findings highlight the importance of question content in KT and demonstrate the benefits of leveraging pretrained representations of LLMs to model student learning more effectively.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CGES: Confidence-Guided Early Stopping for Efficient and Accurate Self-Consistency</title>
<link>https://arxiv.org/abs/2511.02603</link>
<guid>https://arxiv.org/abs/2511.02603</guid>
<content:encoded><![CDATA[
<div> confidence, language models, Bayesian framework, early stopping, model calls<br />
<br />
Summary:<br />
The paper introduces Confidence-Guided Early Stopping (CGES), a Bayesian framework aimed at optimizing the efficiency of large language models (LLMs) by halting sampling once a candidate answer's posterior mass exceeds a threshold. This framework utilizes scalar confidence signals derived from token probabilities or reward models to make adaptive decisions. CGES aims to address the limitations of the self-consistency strategy by reducing the average number of model calls by approximately 69 percent while maintaining accuracy levels close to self-consistency. The theoretical guarantees provided encompass perfectly calibrated confidences as well as realistic noisy confidence signals. Across multiple reasoning benchmarks, CGES demonstrates its effectiveness in improving efficiency without compromising accuracy, showcasing its potential to enhance the performance of LLMs in various applications. <br /> <div>
arXiv:2511.02603v1 Announce Type: new 
Abstract: Large language models (LLMs) are often queried multiple times at test time, with predictions aggregated by majority vote. While effective, this self-consistency strategy (arXiv:2203.11171) requires a fixed number of calls and can fail when the correct answer is rare. We introduce Confidence-Guided Early Stopping (CGES), a Bayesian framework that forms posteriors over candidate answers using scalar confidence signals derived from token probabilities or reward models. CGES adaptively halts sampling once the posterior mass of a candidate exceeds a threshold. We provide theoretical guarantees for both perfectly calibrated confidences and realistic noisy confidence signals. Across five reasoning benchmarks, CGES reduces the average number of model calls by about 69 percent (for example, from 16.0 to 4.9) while matching the accuracy of self-consistency within 0.06 percentage points.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Realignment Problem: When Right becomes Wrong in LLMs</title>
<link>https://arxiv.org/abs/2511.02623</link>
<guid>https://arxiv.org/abs/2511.02623</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, alignment, policy, unlearning, sustainable AI deployment

Summary: 
The article introduces a framework called TRACE for unlearning and re-aligning Large Language Models (LLMs) with evolving human values and policies. Existing methods for addressing the Alignment-Reality Gap are ineffective due to high costs and loss of utility. TRACE programmatically evaluates and triages preference data against new policies, identifying conflicts and preserving model performance. Empirical results show that TRACE successfully re-aligns LLMs across different model families, maintaining general capabilities while enforcing new principles. This approach offers a scalable, dynamic, and cost-effective solution for maintaining LLM alignment, laying the groundwork for sustainable and responsible AI deployment. 

<br /><br />Summary: <div>
arXiv:2511.02623v1 Announce Type: new 
Abstract: The alignment of Large Language Models (LLMs) with human values is central to their safe deployment, yet current practice produces static, brittle, and costly-to-maintain models that fail to keep pace with evolving norms and policies. This misalignment, which we term the Alignment-Reality Gap, poses a growing challenge for reliable long-term use. Existing remedies are inadequate: large-scale re-annotation is economically prohibitive, and standard unlearning methods act as blunt instruments that erode utility rather than enable precise policy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict Evaluation), a framework for principled unlearning that reconceives re-alignment as a programmatic policy application problem. TRACE programmatically triages existing preference data against a new policy, identifies high-impact conflicts via a alignment impact score, and applies a hybrid optimization that cleanly inverts, discards, or preserves preferences while safeguarding model performance. Empirical results show that TRACE achieves robust re-alignment across diverse model families (Qwen2.5-7B, Gemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF dataset under complex policy shift, TRACE enforces new principles without degrading general capabilities. Our work establishes a scalable, dynamic, and cost-effective paradigm for maintaining LLM alignment, providing a foundation for sustainable and responsible AI deployment.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding New-Knowledge-Induced Factual Hallucinations in LLMs: Analysis, Solution, and Interpretation</title>
<link>https://arxiv.org/abs/2511.02626</link>
<guid>https://arxiv.org/abs/2511.02626</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, factual hallucinations, fine-tuning, knowledge types, attention analysis

Summary:
Previous studies have shown that fine-tuning large language models (LLMs) with new knowledge can lead to factual hallucinations when tested on known information. This study explores the manifestations and mechanisms of these hallucinations by creating the Biography-Reasoning dataset and analyzing different knowledge types and task types. It was found that the unfamiliarity of specific knowledge types contributes more to hallucinations than the overall amount of new knowledge. To address this, the KnownPatch method was proposed, which involves introducing known knowledge samples during training to reduce hallucinations. Attention analysis revealed that new knowledge learning reduces the model's focus on key entities in questions, leading to an increased risk of hallucinations. This disrupted attention pattern can spread to similar contexts, affecting performance on textually similar questions. KnownPatch successfully mitigated these disruptions and improved model performance. <br /><br />Summary: <div>
arXiv:2511.02626v1 Announce Type: new 
Abstract: Previous studies show that introducing new knowledge during large language models (LLMs) fine-tuning can lead to the generation of erroneous output when tested on known information, thereby triggering factual hallucinations. However, existing studies have not deeply investigated the specific manifestations and underlying mechanisms of these hallucinations. Our work addresses this gap by designing a controlled dataset Biography-Reasoning, and conducting a fine-grained analysis across multiple knowledge types and two task types, including knowledge question answering (QA) and knowledge reasoning tasks. We find that when fine-tuned on a dataset in which a specific knowledge type consists entirely of new knowledge, LLMs exhibit significantly increased hallucination tendencies. This suggests that the high unfamiliarity of a particular knowledge type, rather than the overall proportion of new knowledge, is a stronger driver of hallucinations, and these tendencies can even affect other knowledge types in QA tasks. To mitigate such factual hallucinations, we propose KnownPatch, which patches a small number of known knowledge samples in the later stages of training, effectively alleviating new-knowledge-induced hallucinations. Through attention analysis, we find that learning new knowledge reduces the model's attention to key entities in the question, thus causing excessive focus on the surrounding context, which may increase the risk of hallucination. Moreover, the attention pattern can propagate to similar contexts, facilitating the spread of hallucinations to textually similar questions. Our method effectively mitigates the disruption of new knowledge learning to the model's attention on key entities, accompanied by improved performance.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes</title>
<link>https://arxiv.org/abs/2511.02681</link>
<guid>https://arxiv.org/abs/2511.02681</guid>
<content:encoded><![CDATA[
<div> Efficient storage, parameter updates, fine-tuning, low-rank approximation, sparsification  
Summary:  
- Large language models (LLMs) are widely used but their size poses storage and processing challenges, leading to the need for efficient storage of fine-tuned models.  
- Fine-tuning primarily affects a small fraction of parameters, making it crucial to optimize storage efficiency for parameter updates.  
- Fine-tuning updates are both low-rank and sparse, suggesting potential for storage optimization.  
- Sparsified low-rank approximations with larger ranks outperform standard low-rank approximations with smaller ranks within the same memory budget.  
- The proposed method, optimal singular damage, selectively sparsifies low-rank approximated updates while retaining critical singular components, resulting in significant storage efficiency and improved accuracy compared to individual low-rank approximation or sparsification techniques.  
<br /><br /> <div>
arXiv:2511.02681v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly prevalent across diverse applications. However, their enormous size limits storage and processing capabilities to a few well-resourced stakeholders. As a result, most applications rely on pre-trained LLMs, fine-tuned for specific tasks. However, even storing the fine-tuned versions of these models remains a significant challenge due to the wide range of tasks they address. Recently, studies show that fine-tuning these models primarily affects a small fraction of parameters, highlighting the need for more efficient storage of fine-tuned models. This paper focuses on efficient storage of parameter updates in pre-trained models after fine-tuning. To address this challenge, we leverage the observation that fine-tuning updates are both low-rank and sparse, which can be utilized for storage efficiency. However, using only low-rank approximation or sparsification may discard critical singular components that enhance model expressivity. We first observe that given the same memory budget, sparsified low-rank approximations with larger ranks outperform standard low-rank approximations with smaller ranks. Building on this, we propose our method, optimal singular damage, that selectively sparsifies low-rank approximated updates by leveraging the interleaved importance of singular vectors, ensuring that the most impactful components are retained. We demonstrate through extensive experiments that our proposed methods lead to significant storage efficiency and superior accuracy within the same memory budget compared to employing the low-rank approximation or sparsification individually.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PragExTra: A Multilingual Corpus of Pragmatic Explicitation in Translation</title>
<link>https://arxiv.org/abs/2511.02721</link>
<guid>https://arxiv.org/abs/2511.02721</guid>
<content:encoded><![CDATA[
<div> Keywords: translation, multilingualism, explicitation, PragExTra, corpus <br />
Summary: <br />
- PragExTra introduces a multilingual corpus and detection framework for pragmatic explicitation in translation. <br />
- The corpus covers eight language pairs and includes entity descriptions, measurement conversions, and translator remarks. <br />
- Candidate explicitation cases are identified using null alignments and refined through active learning with human annotation. <br />
- Results indicate that entity and system-level explicitations are most common, with active learning improving classifier accuracy by 7-8 percentage points. <br />
- PragExTra highlights the importance of pragmatic explicitation as a cross-linguistic phenomenon and a step towards culturally aware machine translation. <br /> <div>
arXiv:2511.02721v1 Announce Type: new 
Abstract: Translators often enrich texts with background details that make implicit cultural meanings explicit for new audiences. This phenomenon, known as pragmatic explicitation, has been widely discussed in translation theory but rarely modeled computationally. We introduce PragExTra, the first multilingual corpus and detection framework for pragmatic explicitation. The corpus covers eight language pairs from TED-Multi and Europarl and includes additions such as entity descriptions, measurement conversions, and translator remarks. We identify candidate explicitation cases through null alignments and refined using active learning with human annotation. Our results show that entity and system-level explicitations are most frequent, and that active learning improves classifier accuracy by 7-8 percentage points, achieving up to 0.88 accuracy and 0.82 F1 across languages. PragExTra establishes pragmatic explicitation as a measurable, cross-linguistic phenomenon and takes a step towards building culturally aware machine translation. Keywords: translation, multilingualism, explicitation
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Diffusion in Low Resource Language Countries</title>
<link>https://arxiv.org/abs/2511.02752</link>
<guid>https://arxiv.org/abs/2511.02752</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, language models, low-resource languages, adoption, equitable diffusion

Summary: 
Low-resource languages face challenges in the adoption and utilization of artificial intelligence (AI) technologies, particularly due to the limitations of Large Language Models (LLMs) in handling data scarcity in these languages. This performance deficit can hinder the widespread adoption of AI in Low-Resource Language Countries (LRLCs), leading to a 20% lower share of AI users in these regions compared to their baseline. The study highlights the significant impact of linguistic accessibility as an independent barrier to the equitable diffusion of AI technologies globally. The findings emphasize the need for targeted interventions and advancements in language processing tools to bridge the gap and promote more inclusive AI adoption and usage in low-resource language contexts.<br /><br />Summary: <div>
arXiv:2511.02752v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is diffusing globally at unprecedented speed, but adoption remains uneven. Frontier Large Language Models (LLMs) are known to perform poorly on low-resource languages due to data scarcity. We hypothesize that this performance deficit reduces the utility of AI, thereby slowing adoption in Low-Resource Language Countries (LRLCs). To test this, we use a weighted regression model to isolate the language effect from socioeconomic and demographic factors, finding that LRLCs have a share of AI users that is approximately 20% lower relative to their baseline. These results indicate that linguistic accessibility is a significant, independent barrier to equitable AI diffusion.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controlling Performance and Budget of a Centralized Multi-agent LLM System with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.02755</link>
<guid>https://arxiv.org/abs/2511.02755</guid>
<content:encoded><![CDATA[
<div> framework, multi-agent, language models, coordination, cost-efficient  
<br />  
Large language models (LLMs) have strengths in various domains but come with varying inference costs. This study introduces a centralized multi-LLM framework where a controller LLM coordinates a pool of expert models efficiently. The coordination problem is formulated as reinforcement learning with dual objectives of maximizing task performance and minimizing inference cost. The proposed CoRL framework optimizes the performance-cost trade-off in a multi-budget setting, enabling adaptation to different budget conditions during inference. Experimental results on four benchmarks show that CoRL surpasses the best expert LLM under high-budget settings while maintaining strong performance in more economical low-budget modes. This highlights the effectiveness of centralized coordination for scalable and cost-efficient multi-agent LLM systems.  
<br /><br />Summary: <div>
arXiv:2511.02755v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit complementary strengths across domains and come with varying inference costs, motivating the design of multi-agent LLM systems where specialized models collaborate efficiently. Existing approaches predominantly rely on decentralized frameworks, which invoke multiple LLMs for every input and thus lead to substantial and uncontrolled inference costs. In this work, we introduce a centralized multi-LLM framework, where a controller LLM selectively coordinates a pool of expert models in a cost-efficient and cost-controllable manner. We formulate this coordination problem as reinforcement learning with dual objectives: maximizing task performance while minimizing the overall inference cost. In addition, we expect the multi-agent system to have adapted behavior with different budget conditions during inference. To this end, we propose CoRL, a reinforcement learning framework that optimizes the performance cost trade-off in a controllable multi-budget setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a single system to surpass the best expert LLM under high-budget settings, while maintaining strong performance in more economical low-budget modes, highlighting the effectiveness of centralized coordination for scalable and cost-efficient multi-agent LLM systems.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval</title>
<link>https://arxiv.org/abs/2511.02770</link>
<guid>https://arxiv.org/abs/2511.02770</guid>
<content:encoded><![CDATA[
<div> Keywords: text retrievers, multimodal distribution, autoregressive, multi-embedding, document retrieval

Summary: 
Existing text retrievers face limitations in retrieving relevant documents when the distance between target document embeddings increases. The Autoregressive Multi-Embedding Retriever (AMER) is introduced to address this issue by generating multiple query vectors to capture different interpretations of the query. Testing on synthetic data showed a significant improvement in capturing multiple target distributions compared to single embedding models. Fine-tuning on real-world multi-answer retrieval datasets resulted in a 4-21% relative gain over single-embedding baselines. AMER showed larger gains when the target document embeddings were less similar. This approach presents a novel direction for future work in the field of document retrieval. 

<br /><br />Summary: <div>
arXiv:2511.02770v1 Announce Type: new 
Abstract: Most text retrievers generate \emph{one} query vector to retrieve relevant documents. Yet, the conditional distribution of relevant documents for the query may be multimodal, e.g., representing different interpretations of the query. We first quantify the limitations of existing retrievers. All retrievers we evaluate struggle more as the distance between target document embeddings grows. To address this limitation, we develop a new retriever architecture, \emph{A}utoregressive \emph{M}ulti-\emph{E}mbedding \emph{R}etriever (AMER). Our model autoregressively generates multiple query vectors, and all the predicted query vectors are used to retrieve documents from the corpus. We show that on the synthetic vectorized data, the proposed method could capture multiple target distributions perfectly, showing 4x better performance than single embedding model. We also fine-tune our model on real-world multi-answer retrieval datasets and evaluate in-domain. AMER presents 4 and 21\% relative gains over single-embedding baselines on two datasets we evaluate on. Furthermore, we consistently observe larger gains on the subset of dataset where the embeddings of the target documents are less similar to each other. We demonstrate the potential of using a multi-query vector retriever and open up a new direction for future work.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.02805</link>
<guid>https://arxiv.org/abs/2511.02805</guid>
<content:encoded><![CDATA[
<div> memory, MemSearcher, reasoning, search strategies, multi-context GRPO

Summary:
MemSearcher is introduced as an agent workflow that effectively balances information integrity and computational efficiency in search agents by iteratively maintaining a compact memory and combining it with the current turn. This approach stabilizes context length in multi-turn interactions, improving efficiency without compromising accuracy. The multi-context GRPO framework optimizes the workflow by jointly optimizing reasoning, search strategies, and memory management of MemSearcher Agents, achieving significant improvements over strong baselines on public benchmarks. The results show that by striking a balance between information integrity and efficiency, MemSearcher outperforms baselines and demonstrates the potential for higher accuracy and lower computational overhead in search agents. <div>
arXiv:2511.02805v1 Announce Type: new 
Abstract: Typical search agents concatenate the entire interaction history into the LLM context, preserving information integrity but producing long, noisy contexts, resulting in high computation and memory costs. In contrast, using only the current turn avoids this overhead but discards essential information. This trade-off limits the scalability of search agents. To address this challenge, we propose MemSearcher, an agent workflow that iteratively maintains a compact memory and combines the current turn with it. At each turn, MemSearcher fuses the user's question with the memory to generate reasoning traces, perform search actions, and update memory to retain only information essential for solving the task. This design stabilizes context length across multi-turn interactions, improving efficiency without sacrificing accuracy. To optimize this workflow, we introduce multi-context GRPO, an end-to-end RL framework that jointly optimize reasoning, search strategies, and memory management of MemSearcher Agents. Specifically, multi-context GRPO samples groups of trajectories under different contexts and propagates trajectory-level advantages across all conversations within them. Trained on the same dataset as Search-R1, MemSearcher achieves significant improvements over strong baselines on seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on Qwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher even outperforms 7B-based baselines, demonstrating that striking a balance between information integrity and efficiency yields both higher accuracy and lower computational overhead. The code and models will be publicly available at https://github.com/icip-cas/MemSearcher
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities</title>
<link>https://arxiv.org/abs/2511.02817</link>
<guid>https://arxiv.org/abs/2511.02817</guid>
<content:encoded><![CDATA[
<div> Benchmark, Long-context reasoning, Oolong, Reasoning tasks, Large quantities of text <br />
<br />Summary: 
The article introduces Oolong, a benchmark for long-context reasoning tasks that require analyzing chunks of text individually and aggregating these analyses to answer distributional questions. Oolong consists of two task sets: Oolong-synth, which includes synthetic tasks, and Oolong-real, which involves reasoning over real-world conversational data. Models participating in Oolong need to perform classification and counting in-context, reason over temporal and user relations, and tackle challenges that existing models struggle with. Results show that even frontier models like GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro achieve less than 50% accuracy on both task sets at 128K. The release of Oolong's data and evaluation harness aims to facilitate the development of models capable of reasoning over large quantities of text. <br /><br /> <div>
arXiv:2511.02817v1 Announce Type: new 
Abstract: As model context lengths continue to grow, concerns about whether models effectively use the full context length have persisted. While several carefully designed long-context evaluations have recently been released, these evaluations tend to rely on retrieval from one or more sections of the context, which allows nearly all of the context tokens to be disregarded as noise. This represents only one type of task that might be performed with long context. We introduce Oolong, a benchmark of long-context reasoning tasks that require analyzing individual chunks of text on an atomic level, and then aggregating these analyses to answer distributional questions. Oolong is separated into two task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can easily ablate components of the reasoning problem; and Oolong-real, a downstream setting which requires reasoning over real-world conversational data. Oolong requires models to reason over large quantities of examples, to perform both classification and counting in-context, and to reason over temporal and user relations. Even frontier models struggle on Oolong, with GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy on both splits at 128K. We release the data and evaluation harness for Oolong to enable further development of models that can reason over large quantities of text.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SciDaSynth: Interactive Structured Data Extraction from Scientific Literature with Large Language Model</title>
<link>https://arxiv.org/abs/2404.13765</link>
<guid>https://arxiv.org/abs/2404.13765</guid>
<content:encoded><![CDATA[
<div> extraction structured data tables multimodal information SciDaSynth interactive system validation refinement visual summaries semantic grouping nutrition NLP researchers human-AI collaborative system<br />
<br />
Summary:<br />
The article introduces SciDaSynth, an interactive system powered by large language models that automatically generates structured data tables by integrating information from diverse sources. It supports efficient validation and refinement of table data, with visual summaries and semantic grouping capabilities to resolve data inconsistencies. A study with nutrition and NLP researchers shows SciDaSynth's effectiveness in producing high-quality structured data more efficiently than baseline methods. The system code is available on GitHub. Design implications for human-AI collaborative systems supporting data extraction tasks are discussed. <div>
arXiv:2404.13765v3 Announce Type: cross 
Abstract: The explosion of scientific literature has made the efficient and accurate extraction of structured data a critical component for advancing scientific knowledge and supporting evidence-based decision-making. However, existing tools often struggle to extract and structure multimodal, varied, and inconsistent information across documents into standardized formats. We introduce SciDaSynth, a novel interactive system powered by large language models (LLMs) that automatically generates structured data tables according to users' queries by integrating information from diverse sources, including text, tables, and figures. Furthermore, SciDaSynth supports efficient table data validation and refinement, featuring multi-faceted visual summaries and semantic grouping capabilities to resolve cross-document data inconsistencies. A within-subjects study with nutrition and NLP researchers demonstrates SciDaSynth's effectiveness in producing high-quality structured data more efficiently than baseline methods. We discuss design implications for human-AI collaborative systems supporting data extraction tasks. The system code is available at https://github.com/xingbow/SciDaEx
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization</title>
<link>https://arxiv.org/abs/2511.01884</link>
<guid>https://arxiv.org/abs/2511.01884</guid>
<content:encoded><![CDATA[
<div> CUDA kernel generation, LLM training, automatic approach, efficiency, CudaForge

Summary:<br />
Developing efficient CUDA kernels for AI applications like large-scale LLM training is crucial. Manual kernel design is costly and time-consuming, prompting the need for automatic methods leveraging LLMs for code generation. Existing approaches often result in low-efficiency kernels with high computational overhead and limited generalizability. To address these challenges, CudaForge introduces a training-free multi-agent workflow that iteratively generates, corrects, and optimizes CUDA kernels using two LLM agents: a Coder and a Judge. By integrating hardware feedback and leveraging base models like OpenAI-o3, CudaForge achieves 97.6% correctness in generated kernels and a 1.68x speedup over PyTorch baselines. It demonstrates strong generalization across GPUs and base models while maintaining high efficiency. CudaForge reduces the time and cost of kernel optimization, offering a cost-effective and high-performance solution for CUDA kernel generation. <div>
arXiv:2511.01884v1 Announce Type: cross 
Abstract: Developing efficient CUDA kernels is increasingly critical for AI applications such as large-scale LLM training. However, manual kernel design is both costly and time-consuming, motivating automatic approaches that leverage LLMs for code generation. Existing methods for automatic kernel generation, however, often produce low-efficiency kernels, incur high computational overhead, and fail to generalize across settings. In this work, we propose CudaForge, a training-free multi-agent workflow for CUDA kernel generation and optimization. Our workflow is inspired by the iterative workflow of human experts, which contains steps such as developing initial kernels, testing correctness, analyzing hardware feedback, and iterative improvement. More specifically, CudaForge employs two LLM agents: a Coder and a Judge, that iteratively generate, correct, and optimize CUDA kernels, while integrating hardware feedback such as Nsight Compute (NCU) metrics. In extensive evaluations, we show that CudaForge, by leveraging base models like OpenAI-o3, achieves 97.6\% correctness of generated kernels and an average 1.68$\times$ speedup over PyTorch baselines, substantially surpassing state-of-the-art models including OpenAI-o3 and Kevin on KernelBench. Beyond accuracy and speed, CudaForge demonstrates strong generalization across GPUs (A100, RTX 6000, 4090, 3090) and base models (OpenAI-o3, GPT-5, gpt-oss-120B, Claude-Sonnet-4, QwQ-32B), while maintaining high efficiency. In particular, generating an optimized kernel takes about 26.5 minutes on one RTX6000 and incurs about \$ 0.3 API cost, which is significantly cheaper than existing agentic work that costs 6 H100 hours and \$ 5 API cost per kernel. Our results highlight that multi-agent, training-free workflows can enable cost-effective, generalizable, and high-performance CUDA kernel optimization. Code available at https://github.com/OptimAI-Lab/CudaForge
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-Augmented Multimodal Depression Detection</title>
<link>https://arxiv.org/abs/2511.01892</link>
<guid>https://arxiv.org/abs/2511.01892</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal deep learning, sentiment analysis, emotion detection, Large Language Model, AVEC 2019 dataset

Summary:
- The study introduces a novel Retrieval-Augmented Generation (RAG) framework for depression detection using multimodal deep learning.
- The RAG framework integrates text, audio, and video signals to enhance emotional understanding in depression detection.
- The method retrieves emotional content from a sentiment dataset and generates an Emotion Prompt using a Large Language Model (LLM) as an auxiliary modality.
- Experiments on the AVEC 2019 dataset demonstrate that the proposed approach achieves state-of-the-art performance with CCC of 0.593 and MAE of 3.95.
- The RAG framework surpasses previous transfer learning and multi-task learning baselines in depression detection tasks.

<br /><br />Summary: <div>
arXiv:2511.01892v1 Announce Type: cross 
Abstract: Multimodal deep learning has shown promise in depression detection by integrating text, audio, and video signals. Recent work leverages sentiment analysis to enhance emotional understanding, yet suffers from high computational cost, domain mismatch, and static knowledge limitations. To address these issues, we propose a novel Retrieval-Augmented Generation (RAG) framework. Given a depression-related text, our method retrieves semantically relevant emotional content from a sentiment dataset and uses a Large Language Model (LLM) to generate an Emotion Prompt as an auxiliary modality. This prompt enriches emotional representation and improves interpretability. Experiments on the AVEC 2019 dataset show our approach achieves state-of-the-art performance with CCC of 0.593 and MAE of 3.95, surpassing previous transfer learning and multi-task learning baselines.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TapOut: A Bandit-Based Approach to Dynamic Speculative Decoding</title>
<link>https://arxiv.org/abs/2511.02017</link>
<guid>https://arxiv.org/abs/2511.02017</guid>
<content:encoded><![CDATA[
<div> speculative decoding, LLMs, dynamic speculation, TapOut, multi-armed bandits  
Summary:  
Speculative decoding accelerates large language models (LLMs) by generating tokens before verifying them, but determining the optimal number of tokens to draft is a challenge. Dynamic speculative decoding aims to decide how many tokens to draft intelligently. TapOut is an online, training-free algorithm for dynamic speculation policy selection using multi-armed bandits. It selects among multiple parameter-free dynamic speculation strategies based on past rewards and exploration. Experimental results show that TapOut achieves competitive or superior speedups compared to existing dynamic speculation methods, across diverse model pairs and datasets, without requiring hyperparameter tuning. <div>
arXiv:2511.02017v1 Announce Type: cross 
Abstract: Speculative decoding accelerates LLMs by using a lightweight draft model to generate tokens autoregressively before verifying them in parallel with a larger target model. However, determining the optimal number of tokens to draft remains a key challenge limiting the approach's effectiveness. Dynamic speculative decoding aims to intelligently decide how many tokens to draft to achieve maximum speedups. Existing methods often rely on hand-tuned, sensitive thresholds (e.g., token entropy), which are costly to set and generalize poorly across models and domains. We propose TapOut, an online, training-free, plug-and-play algorithm for dynamic speculation policy selection using multi-armed bandits. Our approach employs a meta-algorithm that selects among multiple parameter-free dynamic speculation strategies based on past reward and exploration. We conduct extensive experiments across diverse model pairs and datasets, showing that TapOut achieves competitive or superior speedups compared to well-established dynamic speculation baselines without any hyperparameter tuning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regularization Through Reasoning: Systematic Improvements in Language Model Classification via Explanation-Enhanced Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.02044</link>
<guid>https://arxiv.org/abs/2511.02044</guid>
<content:encoded><![CDATA[
<div> Explanation-augmented fine-tuning, LLM classification, conversational response quality, random tokens, activation entropy<br />
<br />
Summary:
Explanation-augmented fine-tuning of Large Language Models (LLMs) for classification tasks, by including brief explanations with each label during training, leads to improved models. The addition of syntactically incoherent pseudo-explanations, along with genuine rationales, enhances accuracy and reliability in conversational response quality evaluation. The presence of random tokens, despite lacking semantics, further boosts model performance by encouraging richer intermediate computation and acting as a regularizer. This effect is consistent across datasets and training seeds, indicating that gains arise from structure rather than meaning. Internal analyses reveal that explanation-augmented models exhibit higher activation entropy in intermediate layers, indicating increased deliberation before decision-making. Overall, this research provides insights into how token-level scaffolding shapes computation during inference in LLM classification tasks. <br /><br /> <div>
arXiv:2511.02044v1 Announce Type: cross 
Abstract: Fine-tuning LLMs for classification typically maps inputs directly to labels. We ask whether attaching brief explanations to each label during fine-tuning yields better models. We evaluate conversational response quality along three axes: naturalness, comprehensiveness, and on-topic adherence, each rated on 5-point scales. Using ensemble-generated data from multiple LLMs, we fine-tune a 7B-parameter model and test across six diverse conversational datasets. Across 18 dataset, task settings, label-plus-explanation training outperforms label-only baselines.
  A central and unexpected result concerns random tokens. We replace human-written explanations with text that is syntactically incoherent yet vocabulary-aligned with the originals (e.g., shuffled or bag-of-words variants). Despite lacking semantics, these pseudo-explanations still improve accuracy over label-only training and often narrow much of the gap to true explanations. The effect persists across datasets and training seeds, indicating that gains arise less from meaning than from structure: the extra token budget encourages richer intermediate computation and acts as a regularizer that reduces over-confident shortcuts.
  Internal analyses support this view: explanation-augmented models exhibit higher activation entropy in intermediate layers alongside sharper predictive mass at the output layer, consistent with increased deliberation before decision. Overall, explanation-augmented fine-tuning, whether with genuine rationales or carefully constructed random token sequences, improves accuracy and reliability for LLM classification while clarifying how token-level scaffolding shapes computation during inference.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complete asymptotic type-token relationship for growing complex systems with inverse power-law count rankings</title>
<link>https://arxiv.org/abs/2511.02069</link>
<guid>https://arxiv.org/abs/2511.02069</guid>
<content:encoded><![CDATA[
<div> Keywords: complex systems, growth dynamics, power-law relationships, Zipf's law, Heaps' law<br />
<br />
Summary: 
This study delves into the growth dynamics of complex systems and the statistical regularities they exhibit. It focuses on Zipf's law, which describes a power-law scaling relationship between type count and type rank in finite complex systems. Additionally, it explores Heaps' law, which correlates the number of types with the total number of tokens in a growing system. The study proposes an idealized model that can deterministically produce inverse power-law count rankings for types and determine the exact asymptotics of the type-token relationship. By avoiding unnecessary approximations and stochastic mechanisms, the study provides a unified asymptotic expression for all values of $\alpha$, correcting previous special cases. It emphasizes that the general type-token relationship emerges solely as a consequence of Zipf's law.<br /><br /> <div>
arXiv:2511.02069v1 Announce Type: cross 
Abstract: The growth dynamics of complex systems often exhibit statistical regularities involving power-law relationships. For real finite complex systems formed by countable tokens (animals, words) as instances of distinct types (species, dictionary entries), an inverse power-law scaling $S \sim r^{-\alpha}$ between type count $S$ and type rank $r$, widely known as Zipf's law, is widely observed to varying degrees of fidelity. A secondary, summary relationship is Heaps' law, which states that the number of types scales sublinearly with the total number of observed tokens present in a growing system. Here, we propose an idealized model of a growing system that (1) deterministically produces arbitrary inverse power-law count rankings for types, and (2) allows us to determine the exact asymptotics of the type-token relationship. Our argument improves upon and remedies earlier work. We obtain a unified asymptotic expression for all values of $\alpha$, which corrects the special cases of $\alpha = 1$ and $\alpha \gg 1$. Our approach relies solely on the form of count rankings, avoids unnecessary approximations, and does not involve any stochastic mechanisms or sampling processes. We thereby demonstrate that a general type-token relationship arises solely as a consequence of Zipf's law.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Probing with Contrastive Eigenproblems: Improving Understanding and Applicability of CCS</title>
<link>https://arxiv.org/abs/2511.02089</link>
<guid>https://arxiv.org/abs/2511.02089</guid>
<content:encoded><![CDATA[
<div> probing method, large language models, unsupervised, Contrast-Consistent Search, internal activations <br />
<br />
Keywords: probing method, large language models, unsupervised, Contrast-Consistent Search, internal activations <br />
<br />
Summary: 
The article introduces Contrast-Consistent Search (CCS), an unsupervised probing method for testing binary features representation in language models. The mechanism of CCS is clarified to optimize relative contrast consistency, leading to a reformulation as an eigenproblem with interpretable eigenvalues and extensions to multiple variables. Evaluations on various datasets show similar performance to CCS but with improved stability against random initialization sensitivity. Relativizing contrast consistency enhances our understanding of CCS and enables broader probing and interpretability methods in language models. <div>
arXiv:2511.02089v1 Announce Type: cross 
Abstract: Contrast-Consistent Search (CCS) is an unsupervised probing method able to test whether large language models represent binary features, such as sentence truth, in their internal activations. While CCS has shown promise, its two-term objective has been only partially understood. In this work, we revisit CCS with the aim of clarifying its mechanisms and extending its applicability. We argue that what should be optimized for, is relative contrast consistency. Building on this insight, we reformulate CCS as an eigenproblem, yielding closed-form solutions with interpretable eigenvalues and natural extensions to multiple variables. We evaluate these approaches across a range of datasets, finding that they recover similar performance to CCS, while avoiding problems around sensitivity to random initialization. Our results suggest that relativizing contrast consistency not only improves our understanding of CCS but also opens pathways for broader probing and mechanistic interpretability methods.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences</title>
<link>https://arxiv.org/abs/2511.02109</link>
<guid>https://arxiv.org/abs/2511.02109</guid>
<content:encoded><![CDATA[
<div> evaluation framework, large language models, deep values, superficial preferences, AI alignment
Summary:
The Deep Value Benchmark (DVB) introduces an evaluation framework to assess whether large language models (LLMs) truly understand fundamental human values or merely learn surface-level preferences. This is critical for AI alignment to ensure robust generalization of human intentions. The DVB experimental design involves controlled confounding between deep values and shallow features in training LLMs. The results show that LLMs generalize shallow preferences over deep values with an average Deep Value Generalization Rate (DVGR) of only 0.30, lower than chance. Larger models exhibit slightly lower DVGR. The dataset used in the study underwent rigorous human validation experiments. The DVB provides a measurable and interpretable metric for assessing alignment in AI systems.<br /><br />Summary: <div>
arXiv:2511.02109v1 Announce Type: cross 
Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that directly tests whether large language models (LLMs) learn fundamental human values or merely surface-level preferences. This distinction is critical for AI alignment: Systems that capture deeper values are likely to generalize human intentions robustly, while those that capture only superficial patterns in preference data risk producing misaligned behavior. The DVB uses a novel experimental design with controlled confounding between deep values (e.g., moral principles) and shallow features (e.g., superficial attributes). In the training phase, we expose LLMs to human preference data with deliberately correlated deep and shallow features -- for instance, where a user consistently prefers (non-maleficence, formal language) options over (justice, informal language) alternatives. The testing phase then breaks these correlations, presenting choices between (justice, formal language) and (non-maleficence, informal language) options. This design allows us to precisely measure a model's Deep Value Generalization Rate (DVGR) -- the probability of generalizing based on the underlying value rather than the shallow feature. Across 9 different models, the average DVGR is just 0.30. All models generalize deep values less than chance. Larger models have a (slightly) lower DVGR than smaller models. We are releasing our dataset, which was subject to three separate human validation experiments. DVB provides an interpretable measure of a core feature of alignment.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance</title>
<link>https://arxiv.org/abs/2511.02119</link>
<guid>https://arxiv.org/abs/2511.02119</guid>
<content:encoded><![CDATA[
<div> Keywords: Flood insurance, participation rates, large language models, behavioral mechanisms, InsurAgent

Summary: 
Large language models (LLMs) can mimic human decision-making, but struggle to accurately estimate insurance purchase probabilities. To address this, InsurAgent, an LLM-powered agent with retrieval, reasoning, action, and memory modules, is introduced. The retrieval module uses retrieval-augmented generation to estimate probabilities based on survey data, while the reasoning module utilizes LLM common sense to extrapolate beyond data. The memory module allows for simulating temporal decision evolutions, such as life trajectories. InsurAgent is a valuable tool for modeling behavior and analyzing policies. <div>
arXiv:2511.02119v1 Announce Type: cross 
Abstract: Flood insurance is an effective strategy for individuals to mitigate disaster-related losses. However, participation rates among at-risk populations in the United States remain strikingly low. This gap underscores the need to understand and model the behavioral mechanisms underlying insurance decisions. Large language models (LLMs) have recently exhibited human-like intelligence across wide-ranging tasks, offering promising tools for simulating human decision-making. This study constructs a benchmark dataset to capture insurance purchase probabilities across factors. Using this dataset, the capacity of LLMs is evaluated: while LLMs exhibit a qualitative understanding of factors, they fall short in estimating quantitative probabilities. To address this limitation, InsurAgent, an LLM-empowered agent comprising five modules including perception, retrieval, reasoning, action, and memory, is proposed. The retrieval module leverages retrieval-augmented generation (RAG) to ground decisions in empirical survey data, achieving accurate estimation of marginal and bivariate probabilities. The reasoning module leverages LLM common sense to extrapolate beyond survey data, capturing contextual information that is intractable for traditional models. The memory module supports the simulation of temporal decision evolutions, illustrated through a roller coaster life trajectory. Overall, InsurAgent provides a valuable tool for behavioral modeling and policy analysis.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning</title>
<link>https://arxiv.org/abs/2511.02194</link>
<guid>https://arxiv.org/abs/2511.02194</guid>
<content:encoded><![CDATA[
<div> framework, decision-making, individual, language models, optimization 
Summary: 
The article presents ATHENA, an Adaptive Textual-symbolic Human-centric Reasoning framework that aims to improve decision-making models for individuals in high-stakes scenarios such as vaccine uptake. ATHENA combines Utility Theory with Large Language Models (LLMs) to integrate group-level symbolic utility functions and personalized semantic adaptation for optimal information integration. The framework was validated in real-world tasks, showcasing superior performance compared to utility-based, machine learning, and other LLM-based models. Ablation studies confirmed the importance of both stages of ATHENA in enhancing predictive performance. By incorporating symbolic utility modeling and semantic adaptation, ATHENA offers a novel approach to modeling human-centric decisions. The project page for ATHENA can be found at https://yibozh.github.io/Athena. 
<br /><br />Summary: <div>
arXiv:2511.02194v1 Announce Type: cross 
Abstract: Decision-making models for individuals, particularly in high-stakes scenarios like vaccine uptake, often diverge from population optimal predictions. This gap arises from the uniqueness of the individual decision-making process, shaped by numerical attributes (e.g., cost, time) and linguistic influences (e.g., personal preferences and constraints). Developing upon Utility Theory and leveraging the textual-reasoning capabilities of Large Language Models (LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric Reasoning framework (ATHENA) to address the optimal information integration. ATHENA uniquely integrates two stages: First, it discovers robust, group-level symbolic utility functions via LLM-augmented symbolic discovery; Second, it implements individual-level semantic adaptation, creating personalized semantic templates guided by the optimal utility to model personalized choices. Validated on real-world travel mode and vaccine choice tasks, ATHENA consistently outperforms utility-based, machine learning, and other LLM-based models, lifting F1 score by at least 6.5% over the strongest cutting-edge models. Further, ablation studies confirm that both stages of ATHENA are critical and complementary, as removing either clearly degrades overall predictive performance. By organically integrating symbolic utility modeling and semantic adaptation, ATHENA provides a new scheme for modeling human-centric decisions. The project page can be found at https://yibozh.github.io/Athena.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Proactive and Personalized LLM Agents</title>
<link>https://arxiv.org/abs/2511.02208</link>
<guid>https://arxiv.org/abs/2511.02208</guid>
<content:encoded><![CDATA[
<div> UserVille, LLM-based user simulators, PPP, multi-objective reinforcement learning, productivity-proactivity-personalization<br />
<br />
Summary:
This study introduces the concept of optimizing productivity, proactivity, and personalization in real-world AI agents. The researchers developed UserVille, an interactive environment with user simulators to simulate diverse user preferences. They proposed PPP, a multi-objective reinforcement learning approach that aims to optimize all three dimensions simultaneously. Experimental results in software engineering and deep research tasks showed that agents trained with PPP outperformed strong baselines like GPT-5, showcasing the ability to ask strategic questions, adapt to varied user preferences, and improve task success through better interaction. The findings emphasize the importance of user-centered interaction in building practical and effective AI agents. 
<br /><br />Summary: <div>
arXiv:2511.02208v1 Announce Type: cross 
Abstract: While existing work focuses primarily on task success, we argue that effective real-world agents require optimizing three dimensions: productivity (task completion), proactivity (asking essential questions), and personalization (adapting to diverse user preferences). We introduce UserVille, an interactive environment with LLM-based user simulators enabling diverse, configurable user preferences. Leveraging UserVille, we introduce PPP, a multi-objective reinforcement learning approach that jointly optimizes all three dimensions: Productivity, Proactivity, and Personalization. Experiments on software engineering and deep research tasks show that agents trained with PPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6 on average), demonstrating the ability to ask strategic clarifying questions, adapt to unseen user preferences, and improve task success through better interaction. This work demonstrates that explicitly optimizing for user-centered interaction is critical for building practical and effective AI agents.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Evaluation of Interleaved Instruction Tuning on Semantic Reasoning Performance in an Audio MLLM</title>
<link>https://arxiv.org/abs/2511.02234</link>
<guid>https://arxiv.org/abs/2511.02234</guid>
<content:encoded><![CDATA[
<div> interleaved instruction tuning, audio MLLM, reasoning benchmark, SHARD, semantic reasoning <br />
Summary: <br />
This study explores the impact of interleaved instruction tuning in audio Multi-modal Large Language Models (MLLMs) using the LTU model. The researchers conducted experiments using the SHARD dataset, focusing on synonym and hypernym recognition for audio-based semantic reasoning. Results indicate that zero-shot interleaved prompting enhances performance on reasoning tasks. Additionally, a small amount of fine-tuning with interleaved training prompts further improves results, albeit potentially compromising the MLLM's audio labeling capability. <div>
arXiv:2511.02234v1 Announce Type: cross 
Abstract: Standard training for Multi-modal Large Language Models (MLLMs) involves concatenating non-textual information, like vision or audio, with a text prompt. This approach may not encourage deep integration of modalities, limiting the model's ability to leverage the core language model's reasoning capabilities. This work examined the impact of interleaved instruction tuning in an audio MLLM, where audio tokens are interleaved within the prompt. Using the Listen, Think, and Understand (LTU) model as a testbed, we conduct an experiment using the Synonym and Hypernym Audio Reasoning Dataset (SHARD), our newly created reasoning benchmark for audio-based semantic reasoning focusing on synonym and hypernym recognition. Our findings show that while even zero-shot interleaved prompting improves performance on our reasoning tasks, a small amount of fine-tuning using interleaved training prompts improves the results further, however, at the expense of the MLLM's audio labeling ability.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning</title>
<link>https://arxiv.org/abs/2511.02280</link>
<guid>https://arxiv.org/abs/2511.02280</guid>
<content:encoded><![CDATA[
<div> Keywords: SAIL-RL, reinforcement learning, multimodal large language models, reasoning capabilities, dual reward system 

Summary: 
The paper introduces SAIL-RL, a reinforcement learning framework designed to enhance the reasoning abilities of multimodal large language models (MLLMs). Existing approaches lack sound reasoning due to outcome-only supervision and uniform thinking strategies. SAIL-RL addresses these limitations through a dual reward system: the Thinking Reward evaluates reasoning quality, while the Judging Reward determines the appropriate thinking strategy. Experimental results on SAIL-VL2 demonstrate improved reasoning and multimodal understanding benchmarks at various scales, competing effectively with commercial models like GPT-4o. Additionally, SAIL-RL reduces hallucinations, establishing itself as a structured framework for developing reliable and adaptive MLLMs. The code for SAIL-RL will be available on GitHub for further exploration and application. 

<br /><br />Summary: <div>
arXiv:2511.02280v1 Announce Type: cross 
Abstract: We introduce SAIL-RL, a reinforcement learning (RL) post-training framework that enhances the reasoning capabilities of multimodal large language models (MLLMs) by teaching them when and how to think. Existing approaches are limited by outcome-only supervision, which rewards correct answers without ensuring sound reasoning, and by uniform thinking strategies, which often lead to overthinking on simple tasks and underthinking on complex ones. SAIL-RL addresses these challenges with a dual reward system: the Thinking Reward, which evaluates reasoning quality through factual grounding, logical coherence, and answer consistency, and the Judging Reward, which adaptively determines whether deep reasoning or direct answering is appropriate. Experiments on the state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal understanding benchmarks at both 4B and 8B scales, achieving competitive performance against commercial closed-source models such as GPT-4o, and substantially reduces hallucinations, establishing it as a principled framework for building more reliable and adaptive MLLMs. The code will be available at https://github.com/BytedanceDouyinContent/SAIL-RL.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Link prediction Graph Neural Networks for structure recognition of Handwritten Mathematical Expressions</title>
<link>https://arxiv.org/abs/2511.02288</link>
<guid>https://arxiv.org/abs/2511.02288</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, Handwritten Mathematical Expression (HME) recognition, symbol segmentation, spatial relations, Symbol Label Graph

Summary:
A Graph Neural Network (GNN) approach is proposed for Handwritten Mathematical Expression (HME) recognition. HMEs are modeled as graphs where nodes represent symbols and edges capture spatial dependencies. The approach involves using a deep BLSTM network for symbol segmentation, recognition, and spatial relation classification, forming an initial primitive graph. A 2D-CFG parser generates all possible spatial relations, and a GNN-based link prediction model refines the structure by removing unnecessary connections to form the Symbol Label Graph. Experimental results show the effectiveness of the approach, demonstrating promising performance in HME structure recognition. <div>
arXiv:2511.02288v1 Announce Type: cross 
Abstract: We propose a Graph Neural Network (GNN)-based approach for Handwritten Mathematical Expression (HME) recognition by modeling HMEs as graphs, where nodes represent symbols and edges capture spatial dependencies. A deep BLSTM network is used for symbol segmentation, recognition, and spatial relation classification, forming an initial primitive graph. A 2D-CFG parser then generates all possible spatial relations, while the GNN-based link prediction model refines the structure by removing unnecessary connections, ultimately forming the Symbol Label Graph. Experimental results demonstrate the effectiveness of our approach, showing promising performance in HME structure recognition.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation</title>
<link>https://arxiv.org/abs/2511.02303</link>
<guid>https://arxiv.org/abs/2511.02303</guid>
<content:encoded><![CDATA[
<div> Agent behavior, multi-agent reasoning, causal influence, verifiable rewards, complex reasoning tasks  
Summary:  
- Lazy agent behavior in multi-agent reasoning can hinder collaboration and effectiveness.  
- The paper provides a theoretical analysis on why lazy behavior occurs.  
- A method for measuring causal influence is proposed to mitigate lazy behavior.  
- A verifiable reward mechanism is introduced to encourage deliberation and improve the reasoning process.  
- Extensive experiments show that the framework successfully addresses lazy agent behavior and enhances the potential of multi-agent frameworks for complex reasoning tasks.  
<br /><br />Summary: <div>
arXiv:2511.02303v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) trained with reinforcement learning and verifiable rewards have achieved strong results on complex reasoning tasks. Recent work extends this paradigm to a multi-agent setting, where a meta-thinking agent proposes plans and monitors progress while a reasoning agent executes subtasks through sequential conversational turns. Despite promising performance, we identify a critical limitation: lazy agent behavior, in which one agent dominates while the other contributes little, undermining collaboration and collapsing the setup to an ineffective single agent. In this paper, we first provide a theoretical analysis showing why lazy behavior naturally arises in multi-agent reasoning. We then introduce a stable and efficient method for measuring causal influence, helping mitigate this issue. Finally, as collaboration intensifies, the reasoning agent risks getting lost in multi-turn interactions and trapped by previous noisy responses. To counter this, we propose a verifiable reward mechanism that encourages deliberation by allowing the reasoning agent to discard noisy outputs, consolidate instructions, and restart its reasoning process when necessary. Extensive experiments demonstrate that our framework alleviates lazy agent behavior and unlocks the full potential of multi-agent framework for complex reasoning tasks.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.02304</link>
<guid>https://arxiv.org/abs/2511.02304</guid>
<content:encoded><![CDATA[
<div> framework, multi-task, multi-agent, policies, reinforcement learning  
Summary:  
Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning (ACC-MARL) is introduced for learning decentralized team policies for cooperative, temporal objectives. The framework uses automata to represent tasks, enabling complex tasks to be decomposed into simpler sub-tasks assigned to agents. Existing approaches are sample-inefficient and limited to single-task scenarios, motivating the development of ACC-MARL. Challenges to feasibility are identified and addressed, with the correctness of the approach proven. The value functions of learned policies can optimally assign tasks at test time. Experiments demonstrate emergent task-aware, multi-step coordination among agents, such as pressing a button to unlock a door, holding the door, and short-circuiting tasks. The framework's effectiveness lies in its ability to learn task-conditioned, decentralized team policies, enhancing coordination and task performance in multi-agent settings.  
<br /><br />Summary: <div>
arXiv:2511.02304v1 Announce Type: cross 
Abstract: We study the problem of learning multi-task, multi-agent policies for cooperative, temporal objectives, under centralized training, decentralized execution. In this setting, using automata to represent tasks enables the decomposition of complex tasks into simpler sub-tasks that can be assigned to agents. However, existing approaches remain sample-inefficient and are limited to the single-task case. In this work, we present Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning (ACC-MARL), a framework for learning task-conditioned, decentralized team policies. We identify the main challenges to ACC-MARL's feasibility in practice, propose solutions, and prove the correctness of our approach. We further show that the value functions of learned policies can be used to assign tasks optimally at test time. Experiments show emergent task-aware, multi-step coordination among agents, e.g., pressing a button to unlock a door, holding the door, and short-circuiting tasks.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning</title>
<link>https://arxiv.org/abs/2511.02360</link>
<guid>https://arxiv.org/abs/2511.02360</guid>
<content:encoded><![CDATA[
<div> framework, continuous cross-modal reasoning, Latent Q-Former, token selection mechanism, multi-task objective

Summary:
CoCoVa is a novel framework for vision-language models that leverages continuous cross-modal reasoning through an iterative cycle using a Latent Q-Former. It incorporates a token selection mechanism to focus on salient visual regions and a multi-task objective for training alignment with visual and textual modalities. Evaluations show CoCoVa improves accuracy and token efficiency compared to strong baselines, competing with larger models on various benchmarks. Even when scaled to 7B LLM backbones, it remains competitive with state-of-the-art models. The learned latent space captures interpretable reasoning patterns, bridging the gap between discrete language processing and continuous visual understanding. <br /><br />Summary: <div>
arXiv:2511.02360v1 Announce Type: cross 
Abstract: In human cognition, there exist numerous thought processes that are tacit and beyond verbal expression, enabling us to understand and interact with the world in multiple ways. However, contemporary Vision-Language Models (VLMs) remain constrained to reasoning within the discrete and rigid space of linguistic tokens, thereby bottlenecking the rich, high-dimensional nature of visual perception. To bridge this gap, we propose CoCoVa (Chain of Continuous Vision-Language Thought), a novel framework for vision-language model that leverages continuous cross-modal reasoning for diverse vision-language tasks. The core of CoCoVa is an iterative reasoning cycle, where a novel Latent Q-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a chain of latent thought vectors through cross-modal fusion. To focus this process, a token selection mechanism dynamically identifies salient visual regions, mimicking attentional focus. To ensure these latent thoughts remain grounded, we train the model with a multi-task objective that combines contrastive learning and diffusion-based reconstruction, enforcing alignment between latent representations and both visual and textual modalities. Evaluations show CoCoVa improves accuracy and token efficiency over strong baselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B models on almost all benchmarks. When scaled to 7B LLM backbones, it remains competitive with state-of-the-art models. Qualitative analysis validates that learned latent space captures interpretable and structured reasoning patterns, highlighting the potential of CoCoVa to bridge the representational gap between discrete language processing and the continuous nature of visual understanding.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding</title>
<link>https://arxiv.org/abs/2511.02495</link>
<guid>https://arxiv.org/abs/2511.02495</guid>
<content:encoded><![CDATA[
<div> Dataset, multi-modal, fire domain, annotations, DetectiumFire

Summary: 
The article introduces DetectiumFire, a multi-modal dataset consisting of 22.5k high-resolution fire-related images and 2.5k real-world fire-related videos with detailed annotations. It addresses the lack of publicly available datasets in the fire domain, offering significant scale, diversity, and data quality. The data include traditional computer vision labels and textual prompts for scene description, enabling applications such as synthetic data generation and fire risk reasoning. DetectiumFire is shown to be useful in various tasks like object detection, image generation, and vision-language reasoning. The dataset aims to advance fire-related research and support the development of intelligent safety systems. It is made available to the AI community for further exploration and understanding of fire-related scenarios. <div>
arXiv:2511.02495v1 Announce Type: cross 
Abstract: Recent advances in multi-modal models have demonstrated strong performance in tasks such as image generation and reasoning. However, applying these models to the fire domain remains challenging due to the lack of publicly available datasets with high-quality fire domain annotations. To address this gap, we introduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k high-resolution fire-related images and 2.5k real-world fire-related videos covering a wide range of fire types, environments, and risk levels. The data are annotated with both traditional computer vision labels (e.g., bounding boxes) and detailed textual prompts describing the scene, enabling applications such as synthetic data generation and fire risk reasoning. DetectiumFire offers clear advantages over existing benchmarks in scale, diversity, and data quality, significantly reducing redundancy and enhancing coverage of real-world scenarios. We validate the utility of DetectiumFire across multiple tasks, including object detection, diffusion-based image generation, and vision-language reasoning. Our results highlight the potential of this dataset to advance fire-related research and support the development of intelligent safety systems. We release DetectiumFire to promote broader exploration of fire understanding in the AI community. The dataset is available at https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniChange: Unifying Change Detection with Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2511.02607</link>
<guid>https://arxiv.org/abs/2511.02607</guid>
<content:encoded><![CDATA[
<div> Keywords: Change detection, Multimodal Large Language Models, Unified framework, Semantic change detection, Binary change detection

Summary:
UniChange is introduced as the first Multimodal Large Language Model (MLLM)-based unified change detection model. It addresses the limitations of current models by leveraging the language priors and unification capabilities of MLLMs. The model integrates generative language abilities with specialized change detection functionalities, unifying binary change detection (BCD) and semantic change detection (SCD) tasks through special tokens. By utilizing text prompts for change category identification, UniChange eliminates the need for predefined classification heads. The model achieves state-of-the-art performance on four public benchmarks, surpassing all previous methods with IoU scores of 90.41, 53.04, 78.87, and 57.62 on WHU-CD, S2Looking, LEVIR-CD+, and SECOND datasets, respectively. The code for UniChange is available on GitHub for further exploration and utilization.

<br /><br />Summary: <div>
arXiv:2511.02607v1 Announce Type: cross 
Abstract: Change detection (CD) is a fundamental task for monitoring and analyzing land cover dynamics. While recent high performance models and high quality datasets have significantly advanced the field, a critical limitation persists. Current models typically acquire limited knowledge from single-type annotated data and cannot concurrently leverage diverse binary change detection (BCD) and semantic change detection (SCD) datasets. This constraint leads to poor generalization and limited versatility. The recent advancements in Multimodal Large Language Models (MLLMs) introduce new possibilities for a unified CD framework. We leverage the language priors and unification capabilities of MLLMs to develop UniChange, the first MLLM-based unified change detection model. UniChange integrates generative language abilities with specialized CD functionalities. Our model successfully unifies both BCD and SCD tasks through the introduction of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange utilizes text prompts to guide the identification of change categories, eliminating the reliance on predefined classification heads. This design allows UniChange to effectively acquire knowledge from multi-source datasets, even when their class definitions conflict. Experiments on four public benchmarks (WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance, achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively, surpassing all previous methods. The code is available at https://github.com/Erxucomeon/UniChange.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Collaboration Gap</title>
<link>https://arxiv.org/abs/2511.02687</link>
<guid>https://arxiv.org/abs/2511.02687</guid>
<content:encoded><![CDATA[
<div> isolate, collaboration, maze-solving benchmark, agent-agent, heterogeneous

Summary:<br /><br />
This study introduces a collaborative maze-solving benchmark to evaluate the collaboration capabilities of independently developed agents. The researchers tested 32 models in solo, homogeneous, and heterogeneous pairings to identify a "collaboration gap." Models that performed well individually often struggled to collaborate effectively. A "relay inference" strategy, where the stronger agent leads before passing off to the weaker one, showed promising results in bridging the collaboration gap. The study emphasizes the importance of collaboration-aware evaluation, specialized training strategies to enhance collaborative capabilities, and designing interactions to elicit latent skills of agents. These findings have implications for both AI-AI and human-AI collaboration efforts. <div>
arXiv:2511.02687v1 Announce Type: cross 
Abstract: The trajectory of AI development suggests that we will increasingly rely on agent-based systems composed of independently developed agents with different information, privileges, and tools. The success of these systems will critically depend on effective collaboration among these heterogeneous agents, even under partial observability. Despite intense interest, few empirical studies have evaluated such agent-agent collaboration at scale. We propose a collaborative maze-solving benchmark that (i) isolates collaborative capabilities, (ii) modulates problem complexity, (iii) enables scalable automated grading, and (iv) imposes no output-format constraints, preserving ecological plausibility. Using this framework, we evaluate 32 leading open- and closed-source models in solo, homogeneous, and heterogeneous pairings. Our results reveal a "collaboration gap": models that perform well solo often degrade substantially when required to collaborate. Collaboration can break down dramatically; for instance, small distilled models that solve mazes well alone may fail almost completely in certain pairings. We find that starting with the stronger agent often improves outcomes, motivating a "relay inference" approach where the stronger agent leads before handing off to the weaker one, closing much of the gap. Our findings argue for (1) collaboration-aware evaluation, (2) training strategies developed to enhance collaborative capabilities, and (3) interaction design that reliably elicits agents' latent skills, guidance that applies to AI-AI and human-AI collaboration.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents</title>
<link>https://arxiv.org/abs/2511.02734</link>
<guid>https://arxiv.org/abs/2511.02734</guid>
<content:encoded><![CDATA[
<div> benchmark, cost-centric, large language model, economic reasoning, adaptability

Summary: 
CostBench is introduced as a scalable benchmark to evaluate the economic reasoning and replanning abilities of Large Language Model (LLM) agents. The benchmark is situated in the travel-planning domain and comprises tasks solvable using tools with customizable costs. It includes dynamic blocking events to simulate real-world unpredictability and test agents' adaptability in real-time. Evaluation of current models reveals a gap in cost-aware planning, with agents struggling to identify cost-optimal solutions in both static and dynamic settings. Even leading models like GPT-5 achieve less than 75% exact match rate on the hardest tasks, further dropping by around 40% under dynamic conditions. CostBench aims to diagnose these weaknesses and pave the way for the development of economically rational and robust agents. 

<br /><br />Summary: <div>
arXiv:2511.02734v1 Announce Type: cross 
Abstract: Current evaluations of Large Language Model (LLM) agents primarily emphasize task completion, often overlooking resource efficiency and adaptability. This neglects a crucial capability: agents' ability to devise and adjust cost-optimal plans in response to changing environments. To bridge this gap, we introduce CostBench, a scalable, cost-centric benchmark designed to evaluate agents' economic reasoning and replanning abilities. Situated in the travel-planning domain, CostBench comprises tasks solvable via multiple sequences of atomic and composite tools with diverse, customizable costs. It also supports four types of dynamic blocking events, such as tool failures and cost changes, to simulate real-world unpredictability and necessitate agents to adapt in real time. Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and performance further dropping by around 40% under dynamic conditions. By diagnosing these weaknesses, CostBench lays the groundwork for developing future agents that are both economically rational and robust.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</title>
<link>https://arxiv.org/abs/2511.02778</link>
<guid>https://arxiv.org/abs/2511.02778</guid>
<content:encoded><![CDATA[
<div> SVG code, visual-centric coding, VCode benchmark, CodeVQA evaluation, VCoder framework<br />
<br />
Summary: Code has become an essential medium for reasoning and action in the agent era, but visual-centric coding has been underexplored. The VCode benchmark introduces SVG code as a visual representation for reasoning over sketches, with evaluations done through the CodeVQA protocol. While frontier VLMs struggle with generating faithful SVGs, the VCoder framework improves performance through iterative analysis and visual tools. Human studies show promise for symbolic visual representation in coding tasks, highlighting the potential for improved multimodal understanding and reasoning in the future. <div>
arXiv:2511.02778v1 Announce Type: cross 
Abstract: Code has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model's intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at https://github.com/CSU-JPG/VCode.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs subtract numbers?</title>
<link>https://arxiv.org/abs/2511.02795</link>
<guid>https://arxiv.org/abs/2511.02795</guid>
<content:encoded><![CDATA[
<div> subtraction, large language models, errors, few-shot learning, instruction-tuning

Summary:<br />
- The study focuses on subtraction in large language models (LLMs), which has been less studied compared to addition and multiplication.
- Subtraction accuracy in LLMs is significantly lower than addition accuracy.
- Errors in LLMs occur mostly when the first number is smaller than the second, resulting in missing negative signs.
- LLMs internally understand the concept of negative results but struggle to output the correct sign.
- Few-shot prompting shows slight improvements, while instruction-tuned models achieve near-perfect accuracy in generating negative signs.
<br />Summary: <div>
arXiv:2511.02795v1 Announce Type: cross 
Abstract: We present a systematic study of subtraction in large language models (LLMs). While prior benchmarks emphasize addition and multiplication, subtraction has received comparatively little attention despite being structurally distinct as a non-commutative operation. We evaluate eight pretrained LLMs spanning four families on addition and subtraction problems. Our experiments reveal that subtraction accuracy lags behind addition by a wide margin. We find that the errors for ($a-b$) are concentrated in cases where ($a<b$). In such cases, LLMs frequently produce the correct magnitude but omit the negative sign. Probing analyses show that LLMs internally encode whether results should be negative, yet this information is often not reflected in generated outputs. We further test well-known techniques such as few-shot learning and instruction-tuning to see if they can improve the LLMs' performance. Our results suggest that while few-shot prompting yields modest gains, the instruction-tuned models achieve near-perfect accuracies in generating the negative sign. Together, these findings provide a clearer characterization of the limitations and recoverability of LLMs' arithmetic capabilities in subtraction.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In Good GRACEs: Principled Teacher Selection for Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.02833</link>
<guid>https://arxiv.org/abs/2511.02833</guid>
<content:encoded><![CDATA[
<div> Knowledge distillation, teacher-student models, GRACE, leave-one-out stability, gradient-based algorithms<br />
<br />
Summary:<br />
The study introduces a novel metric called GRACE to evaluate the effectiveness of a teacher model for training a student model in knowledge distillation. GRACE measures the distributional properties of the student model's gradients without the need for external information. It correlates strongly with the performance of student models on tasks like GSM8K and MATH, offering up to a 7.4% improvement over traditional teacher selection methods. Additionally, GRACE aids in determining the optimal temperature for generating teacher data, selecting teachers based on size constraints, and choosing the best teacher within a specific model family. The findings demonstrate that GRACE efficiently identifies compatible teachers for student models and provides valuable guidance for improving the distillation process. <br /> <div>
arXiv:2511.02833v1 Announce Type: cross 
Abstract: Knowledge distillation is an efficient strategy to use data generated by large "teacher" language models to train smaller capable "student" models, but selecting the optimal teacher for a specific student-task combination requires expensive trial-and-error. We propose a lightweight score called GRACE to quantify how effective a teacher will be for post-training a student model. GRACE measures distributional properties of the student's gradients without access to a verifier, teacher logits, teacher internals, or test data. From an information-theoretic perspective, GRACE connects to leave-one-out stability of gradient-based algorithms, which controls the generalization performance of the distilled students. On GSM8K and MATH, GRACE correlates strongly (up to 86% Spearman correlation) with the performance of the distilled LLaMA and OLMo students. In particular, training a student using the GRACE-selected teacher can improve the performance by up to 7.4% over naively using the best-performing teacher. Further, GRACE can provide guidance on crucial design choices in distillation, including (1) the best temperature to use when generating from the teacher, (2) the best teacher to use given a size constraint, and (3) the best teacher to use within a specific model family. Altogether, our findings demonstrate that GRACE can efficiently and effectively identify a strongly compatible teacher for a given student and provide fine-grained guidance on how to perform distillation.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything</title>
<link>https://arxiv.org/abs/2511.02834</link>
<guid>https://arxiv.org/abs/2511.02834</guid>
<content:encoded><![CDATA[
<div> framework, multimodal reasoning, master-agent system, state-of-the-art performance, agent-based design <br />
Summary: <br />
The paper introduces the Agent-Omni framework for multimodal reasoning, which consists of a master-agent system that coordinates different models without requiring retraining. The master agent interprets user intent, delegates tasks to modality-specific agents, and integrates their outputs for coherent responses. Extensive experiments demonstrate that Agent-Omni achieves state-of-the-art performance, especially in tasks involving complex cross-modal reasoning. The framework's agent-based design allows seamless integration of specialized models, ensuring adaptability to diverse inputs while maintaining transparency and interpretability. Additionally, the modular and extensible nature of the framework facilitates future improvements with stronger models. An open-source implementation is released to support further research on scalable and reliable omni-modal reasoning. <br /> <div>
arXiv:2511.02834v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have shown strong capabilities but remain limited to fixed modality pairs and require costly fine-tuning with large aligned datasets. Building fully omni-capable models that can integrate text, images, audio, and video remains impractical and lacks robust reasoning support. In this paper, we propose an Agent-Omni framework that coordinates existing foundation models through a master-agent system, enabling flexible multimodal reasoning without retraining. The master agent interprets user intent, delegates subtasks to modality-specific agents, and integrates their outputs into coherent responses. Extensive experiments across text, image, audio, video, and omni benchmarks show that Agent-Omni consistently achieves state-of-the-art performance, particularly on tasks requiring complex cross-modal reasoning. Its agent-based design enables seamless integration of specialized foundation models, ensuring adaptability to diverse inputs while maintaining transparency and interpretability. In addition, the framework is modular and easily extensible, allowing future improvements as stronger models become available. %We release an open-source implementation to support continued research on scalable and reliable omni-modal reasoning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Teachers Can Use Large Language Models and Bloom's Taxonomy to Create Educational Quizzes</title>
<link>https://arxiv.org/abs/2401.05914</link>
<guid>https://arxiv.org/abs/2401.05914</guid>
<content:encoded><![CDATA[
<div> keyword: Question generation, natural language processing, educational domain, teachers, students

Summary:
Teachers and students have not been involved in the design and validation of Question generation (QG) systems, hindering their potential benefits in the educational domain. This study utilizes a large language model-based QG approach that generates questions aligned with Bloom's taxonomy learning goals. Experiments were conducted to assess how teachers utilize automatically generated questions. Findings reveal that teachers prefer quizzes with automatically generated questions, which are of comparable quality to handwritten versions. Moreover, metrics suggest that automatically generated questions can enhance quiz quality. The study highlights the potential for widespread use of QG in classrooms and underscores the importance of incorporating input from educators and learners in the development of QG systems. 

<br /><br />Summary: <div>
arXiv:2401.05914v2 Announce Type: replace 
Abstract: Question generation (QG) is a natural language processing task with an abundance of potential benefits and use cases in the educational domain. In order for this potential to be realized, QG systems must be designed and validated with pedagogical needs in mind. However, little research has assessed or designed QG approaches with the input from real teachers or students. This paper applies a large language model-based QG approach where questions are generated with learning goals derived from Bloom's taxonomy. The automatically generated questions are used in multiple experiments designed to assess how teachers use them in practice. The results demonstrate that teachers prefer to write quizzes with automatically generated questions, and that such quizzes have no loss in quality compared to handwritten versions. Further, several metrics indicate that automatically generated questions can even improve the quality of the quizzes created, showing the promise for large scale use of QG in the classroom setting.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Path-Consistency with Prefix Enhancement for Efficient Inference in LLMs</title>
<link>https://arxiv.org/abs/2409.01281</link>
<guid>https://arxiv.org/abs/2409.01281</guid>
<content:encoded><![CDATA[
arXiv:2409.01281v3 Announce Type: replace 
Abstract: To enhance the reasoning capabilities of large language models (LLMs), self-consistency has become a popular approach, combining multiple samplings with majority voting. However, current methods are computationally expensive and time-consuming due to the need for numerous samplings. To address this, this paper introduces path-consistency, which leverages the confidence of earlier-generated answers to identify the most promising prefix and guide the generation of subsequent branches. By dynamically guiding the generation of subsequent branches based on this prefix, path-consistency mitigates both the errors and redundancies from random or less useful sampling in self-consistency. This approach reduces errors and redundancies from random sampling, significantly accelerating inference by minimizing token consumption. Our extensive empirical results demonstrate that path-consistency improves inference latency by up to 40.5\%, while maintaining task accuracy across various tasks, including mathematical reasoning, commonsense reasoning, and symbolic reasoning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Extending Direct Preference Optimization to Accommodate Ties</title>
<link>https://arxiv.org/abs/2409.17431</link>
<guid>https://arxiv.org/abs/2409.17431</guid>
<content:encoded><![CDATA[
arXiv:2409.17431v2 Announce Type: replace 
Abstract: We derive and investigate two DPO variants that explicitly model the possibility of declaring a tie in pair-wise comparisons. We replace the Bradley-Terry model in DPO with two well-known modeling extensions, by Rao and Kupper and by Davidson, that assign probability to ties as alternatives to clear preferences. Our experiments in neural machine translation and summarization show that explicitly labeled ties can be added to the datasets for these DPO variants without the degradation in task performance that is observed when the same tied pairs are presented to DPO. We find empirically that the inclusion of ties leads to stronger regularization with respect to the reference policy as measured by KL divergence, and we see this even for DPO in its original form. We provide a theoretical explanation for this regularization effect using ideal DPO policy theory. We further show performance improvements over DPO in translation and mathematical reasoning using our DPO variants. We find it can be beneficial to include ties in preference optimization rather than simply discard them, as is done in common practice.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in Multi-Agent Settings with Social Hierarchy</title>
<link>https://arxiv.org/abs/2410.07109</link>
<guid>https://arxiv.org/abs/2410.07109</guid>
<content:encoded><![CDATA[
arXiv:2410.07109v3 Announce Type: replace 
Abstract: As LLM-based agents become increasingly autonomous and will more freely interact with each other, studying the interplay among them becomes crucial to anticipate emergent phenomena and potential risks. In this work, we provide an in-depth analysis of the interactions among agents within a simulated hierarchical social environment, drawing inspiration from the Stanford Prison Experiment. Leveraging 2,400 conversations across six LLMs (i.e., LLama3, Orca2, Command-r, Mixtral, Mistral2, and gpt4.1) and 240 experimental scenarios, we analyze persuasion and anti-social behavior between a guard and a prisoner agent with differing objectives. We first document model-specific conversational failures in this multi-agent power dynamic context, thereby narrowing our analytic sample to 1,600 conversations. Among models demonstrating successful interaction, we find that goal setting significantly influences persuasiveness but not anti-social behavior. Moreover, agent personas, especially the guard's, substantially impact both successful persuasion by the prisoner and the manifestation of anti-social actions. Notably, we observe the emergence of anti-social conduct even in absence of explicit negative personality prompts. These results have important implications for the development of interactive LLM agents and the ongoing discussion of their societal impact.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaffolded Language Models with Language Supervision for Mixed-Autonomy: A Survey</title>
<link>https://arxiv.org/abs/2410.16392</link>
<guid>https://arxiv.org/abs/2410.16392</guid>
<content:encoded><![CDATA[
arXiv:2410.16392v3 Announce Type: replace 
Abstract: This survey organizes the intricate literature on the design and optimization of emerging structures around post-trained LMs. We refer to this overarching structure as scaffolded LMs and focus on LMs that are integrated into multi-step processes with tools. We view scaffolded LMs as semi-parametric models wherein we train non-parametric variables, including the prompt, tools, and scaffold's code. In particular, they interpret instructions, use tools, and receive feedback all in language. Recent works use an LM as an optimizer to interpret language supervision and update non-parametric variables according to intricate objectives. In this survey, we refer to this paradigm as training of scaffolded LMs with language supervision. A key feature of non-parametric training is the ability to learn from language. Parametric training excels in learning from demonstration (supervised learning), exploration (reinforcement learning), or observations (unsupervised learning), using well-defined loss functions. Language-based optimization enables rich, interpretable, and expressive objectives, while mitigating issues like catastrophic forgetting and supporting compatibility with closed-source models. Furthermore, agents are increasingly deployed as co-workers in real-world applications such as Copilot in Office tools or software development. In these mixed-autonomy settings, where control and decision-making are shared between human and AI, users point out errors or suggest corrections. Accordingly, we discuss agents that continuously improve by learning from this real-time, language-based feedback and refer to this setting as streaming learning from language supervision.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProMQA: Question Answering Dataset for Multimodal Procedural Activity Understanding</title>
<link>https://arxiv.org/abs/2410.22211</link>
<guid>https://arxiv.org/abs/2410.22211</guid>
<content:encoded><![CDATA[
arXiv:2410.22211v2 Announce Type: replace 
Abstract: Multimodal systems have great potential to assist humans in procedural activities, where people follow instructions to achieve their goals. Despite diverse application scenarios, systems are typically evaluated on traditional classification tasks, e.g., action recognition or temporal action segmentation. In this paper, we present a novel evaluation dataset, ProMQA, to measure system advancements in application-oriented scenarios. ProMQA consists of 401 multimodal procedural QA pairs on user recording of procedural activities, i.e., cooking, coupled with their corresponding instructions/recipes. For QA annotation, we take a cost-effective human-LLM collaborative approach, where the existing annotation is augmented with LLM-generated QA pairs that are later verified by humans. We then provide the benchmark results to set the baseline performance on ProMQA. Our experiment reveals a significant gap between human performance and that of current systems, including competitive proprietary multimodal models. We hope our dataset sheds light on new aspects of models' multimodal understanding capabilities.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Composing or Not Composing? Towards Distributional Construction Grammars</title>
<link>https://arxiv.org/abs/2412.07419</link>
<guid>https://arxiv.org/abs/2412.07419</guid>
<content:encoded><![CDATA[
arXiv:2412.07419v2 Announce Type: replace 
Abstract: The mechanisms of comprehension during language processing remains an open question. Classically, building the meaning of a linguistic utterance is said to be incremental, step-by-step, based on a compositional process. However, many different works have shown for a long time that non-compositional phenomena are also at work. It is therefore necessary to propose a framework bringing together both approaches. We present in this paper an approach based on Construction Grammars and completing this framework in order to account for these different mechanisms. We propose first a formal definition of this framework by completing the feature structure representation proposed in Sign-Based Construction Grammars. In a second step, we present a general representation of the meaning based on the interaction of constructions, frames and events. This framework opens the door to a processing mechanism for building the meaning based on the notion of activation evaluated in terms of similarity and unification. This new approach integrates features from distributional semantics into the constructionist framework, leading to what we call Distributional Construction Grammars.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The exponential distribution of the order of demonstrative, numeral, adjective and noun</title>
<link>https://arxiv.org/abs/2502.06342</link>
<guid>https://arxiv.org/abs/2502.06342</guid>
<content:encoded><![CDATA[
arXiv:2502.06342v2 Announce Type: replace 
Abstract: The frequency of the preferred order for a noun phrase formed by demonstrative, numeral, adjective and noun has received significant attention over the last two decades. We investigate the actual distribution of the 24 possible orders. There is no consensus on whether it is well-fitted by an exponential or a power law distribution. We find that an exponential distribution is a much better model. This finding and other circumstances where an exponential-like distribution is found challenge the view that power-law distributions, e.g., Zipf's law for word frequencies, are inevitable. We also investigate which of two exponential distributions gives a better fit: an exponential model where the 24 orders have non-zero probability (a geometric distribution truncated at rank 24) or an exponential model where the number of orders that can have non-zero probability is variable (a right-truncated geometric distribution). When consistency and generalizability are prioritized, we find higher support for the exponential model where all 24 orders have non-zero probability. These findings strongly suggest that there is no hard constraint on word order variation and then unattested orders merely result from undersampling, consistently with Cysouw's view.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExpertLens: Activation steering features are highly interpretable</title>
<link>https://arxiv.org/abs/2502.15090</link>
<guid>https://arxiv.org/abs/2502.15090</guid>
<content:encoded><![CDATA[
arXiv:2502.15090v4 Announce Type: replace 
Abstract: Activation steering methods in large language models (LLMs) have emerged as an effective way to perform targeted updates to enhance generated language without requiring large amounts of adaptation data. We ask whether the features discovered by activation steering methods are interpretable. We identify neurons responsible for specific concepts (e.g., ``cat'') using the ``finding experts'' method from research on activation steering and show that the ExpertLens, i.e., inspection of these neurons provides insights about model representation. We find that ExpertLens representations are stable across models and datasets and closely align with human representations inferred from behavioral data, matching inter-human alignment levels. ExpertLens significantly outperforms the alignment captured by word/sentence embeddings. By reconstructing human concept organization through ExpertLens, we show that it enables a granular view of LLM concept representation. Our findings suggest that ExpertLens is a flexible and lightweight approach for capturing and analyzing model representations.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Routers</title>
<link>https://arxiv.org/abs/2503.23362</link>
<guid>https://arxiv.org/abs/2503.23362</guid>
<content:encoded><![CDATA[
arXiv:2503.23362v3 Announce Type: replace 
Abstract: Supervised fine-tuning (SFT) is a milestone in aligning large language models with human instructions and adapting them to downstream tasks. In particular, Low-Rank Adaptation (LoRA) has gained widespread attention due to its parameter efficiency. However, its impact on improving the performance of large models remains limited. Recent studies suggest that combining LoRA with Mixture-of-Experts (MoE) can significantly enhance fine-tuning performance. MoE adapts to the diversity and complexity of datasets by dynamically selecting the most suitable experts, thereby improving task accuracy and efficiency. Despite impressive results, recent studies reveal issues in the MoE routing mechanism, such as incorrect assignments and imbalanced expert allocation. Inspired by the principles of Redundancy and Fault Tolerance Theory. We innovatively integrate the concept of Mixture of Experts into the routing mechanism and propose an efficient fine-tuning method called Mixture of Routers (MoR). It employs multiple sub-routers for joint selection and uses a learnable main router to determine the weights of the sub-routers. The results show that MoR outperforms baseline models on most tasks, achieving an average performance improvement of 1%. MoR can serve as a plug-and-play, parameter-efficient fine-tuning method suitable for a wide range of applications. Our code is available here: https://anonymous.4open.science/r/MoR-DFC6.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TwT: Thinking without Tokens by Habitual Reasoning Distillation with Multi-Teachers' Guidance</title>
<link>https://arxiv.org/abs/2503.24198</link>
<guid>https://arxiv.org/abs/2503.24198</guid>
<content:encoded><![CDATA[
arXiv:2503.24198v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have made significant strides in problem-solving by incorporating reasoning processes. However, this enhanced reasoning capability results in an increased number of output tokens during inference, leading to higher computational costs. To address this challenge, we propose TwT (Thinking without Tokens), a method that reduces inference-time costs through habitual reasoning distillation with multi-teachers' guidance, while maintaining high performance. Our approach introduces a Habitual Reasoning Distillation method, which internalizes explicit reasoning into the model's habitual behavior through a Teacher-Guided compression strategy inspired by human cognition. Additionally, we propose Dual-Criteria Rejection Sampling (DCRS), a technique that generates a high-quality and diverse distillation dataset using multiple teacher models, making our method suitable for unsupervised scenarios. Experimental results demonstrate that TwT effectively reduces inference costs while preserving superior performance, achieving up to a 13.6% improvement in accuracy with fewer output tokens compared to other distillation methods, offering a highly practical solution for efficient LLM deployment.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Repetitions are not all alike: distinct mechanisms sustain repetition in language models</title>
<link>https://arxiv.org/abs/2504.01100</link>
<guid>https://arxiv.org/abs/2504.01100</guid>
<content:encoded><![CDATA[
arXiv:2504.01100v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) can sometimes degrade into repetitive loops, persistently generating identical word sequences. Because repetition is rare in natural human language, its frequent occurrence across diverse tasks and contexts in LLMs remains puzzling. Here we investigate whether behaviorally similar repetition patterns arise from distinct underlying mechanisms and how these mechanisms develop during model training. We contrast two conditions: repetitions elicited by natural text prompts with those induced by in-context learning (ICL) setups that explicitly require copying behavior. Our analyses reveal that ICL-induced repetition relies on a dedicated network of attention heads that progressively specialize over training, whereas naturally occurring repetition emerges early and lacks a defined circuitry. Attention inspection further shows that natural repetition focuses disproportionately on low-information tokens, suggesting a fallback behavior when relevant context cannot be retrieved. These results indicate that superficially similar repetition behaviors originate from qualitatively different internal processes, reflecting distinct modes of failure and adaptation in language models.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying Aspects in Peer Reviews</title>
<link>https://arxiv.org/abs/2504.06910</link>
<guid>https://arxiv.org/abs/2504.06910</guid>
<content:encoded><![CDATA[
arXiv:2504.06910v3 Announce Type: replace 
Abstract: Peer review is central to academic publishing, but the growing volume of submissions is straining the process. This motivates the development of computational approaches to support peer review. While each review is tailored to a specific paper, reviewers often make assessments according to certain aspects such as Novelty, which reflect the values of the research community. This alignment creates opportunities for standardizing the reviewing process, improving quality control, and enabling computational support. While prior work has demonstrated the potential of aspect analysis for peer review assistance, the notion of aspect remains poorly formalized. Existing approaches often derive aspects from review forms and guidelines, yet data-driven methods for aspect identification are underexplored. To address this gap, our work takes a bottom-up approach: we propose an operational definition of aspect and develop a data-driven schema for deriving aspects from a corpus of peer reviews. We introduce a dataset of peer reviews augmented with aspects and show how it can be used for community-level review analysis. We further show how the choice of aspects can impact downstream applications, such as LLM-generated review detection. Our results lay a foundation for a principled and data-driven investigation of review aspects, and pave the path for new applications of NLP to support peer review.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking the Relationship between the Power Law and Hierarchical Structures</title>
<link>https://arxiv.org/abs/2505.04984</link>
<guid>https://arxiv.org/abs/2505.04984</guid>
<content:encoded><![CDATA[
arXiv:2505.04984v2 Announce Type: replace 
Abstract: Statistical analysis of corpora provides an approach to quantitatively investigate natural languages. This approach has revealed that several power laws consistently emerge across different corpora and languages, suggesting universal mechanisms underlying languages. Particularly, the power-law decay of correlation has been interpreted as evidence for underlying hierarchical structures in syntax, semantics, and discourse. This perspective has also been extended to child speeches and animal signals. However, the argument supporting this interpretation has not been empirically tested in natural languages. To address this problem, the present study examines the validity of the argument for syntactic structures. Specifically, we test whether the statistical properties of parse trees align with the assumptions in the argument. Using English and Japanese corpora, we analyze the mutual information, deviations from probabilistic context-free grammars (PCFGs), and other properties in natural language parse trees, as well as in the PCFG that approximates these parse trees. Our results indicate that the assumptions do not hold for syntactic structures and that it is difficult to apply the proposed argument to child speeches and animal signals, highlighting the need to reconsider the relationship between the power law and hierarchical structures.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking</title>
<link>https://arxiv.org/abs/2505.23495</link>
<guid>https://arxiv.org/abs/2505.23495</guid>
<content:encoded><![CDATA[
arXiv:2505.23495v4 Announce Type: replace 
Abstract: Knowledge Graph Question Answering (KGQA) systems rely on high-quality benchmarks to evaluate complex multi-hop reasoning. However, despite their widespread use, popular datasets such as WebQSP and CWQ suffer from critical quality issues, including inaccurate or incomplete ground-truth annotations, poorly constructed questions that are ambiguous, trivial, or unanswerable, and outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA datasets, including WebQSP and CWQ, we find that the average factual correctness rate is only 57 %. To address these issues, we introduce KGQAGen, an LLM-in-the-loop framework that systematically resolves these pitfalls. KGQAGen combines structured knowledge grounding, LLM-guided generation, and symbolic verification to produce challenging and verifiable QA instances. Using KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results demonstrate that even state-of-the-art systems struggle on this benchmark, highlighting its ability to expose limitations of existing models. Our findings advocate for more rigorous benchmark construction and position KGQAGen as a scalable framework for advancing KGQA evaluation.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media</title>
<link>https://arxiv.org/abs/2506.17435</link>
<guid>https://arxiv.org/abs/2506.17435</guid>
<content:encoded><![CDATA[
arXiv:2506.17435v2 Announce Type: replace 
Abstract: The use of large language models (LLMs) is becoming common in political science and digital media research. While LLMs have demonstrated ability in labelling tasks, their effectiveness to classify Political Content (PC) from URLs remains underexplored. This article evaluates whether LLMs can accurately distinguish PC from non-PC using both the text and the URLs of news articles across five countries (France, Germany, Spain, the UK, and the US) and their different languages. Using cutting-edge models, we benchmark their performance against human-coded data to assess whether URL-level analysis can approximate full-text analysis. Our findings show that URLs embed relevant information and can serve as a scalable, cost-effective alternative to discern PC. However, we also uncover systematic biases: LLMs seem to overclassify centrist news as political, leading to false positives that may distort further analyses. We conclude by outlining methodological recommendations on the use of LLMs in political science research.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns</title>
<link>https://arxiv.org/abs/2507.20343</link>
<guid>https://arxiv.org/abs/2507.20343</guid>
<content:encoded><![CDATA[
arXiv:2507.20343v4 Announce Type: replace 
Abstract: We present DYNARTmo, a dynamic articulatory model designed to visualize speech articulation processes in a two-dimensional midsagittal plane. The model builds upon the UK-DYNAMO framework and integrates principles of articulatory underspecification, segmental and gestural control, and coarticulation. DYNARTmo simulates six key articulators based on ten continuous and six discrete control parameters, allowing for the generation of both vocalic and consonantal articulatory configurations. The current implementation is embedded in a web-based application (SpeechArticulationTrainer) that includes sagittal, glottal, and palatal views, making it suitable for use in phonetics education and speech therapy. While this paper focuses on the static modeling aspects, future work will address dynamic movement generation and integration with articulatory-acoustic modules.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers</title>
<link>https://arxiv.org/abs/2507.20527</link>
<guid>https://arxiv.org/abs/2507.20527</guid>
<content:encoded><![CDATA[
arXiv:2507.20527v3 Announce Type: replace 
Abstract: The demand for Large Language Models (LLMs) at multiple scales, capable of sophisticated and sound mathematical reasoning, continues to grow. However, the development of performant mathematical LLMs is often bottlenecked by the scarcity of useful training data containing problems with significant complexity. We introduce \textbf{SAND-Math} (\textbf{S}ynthetic \textbf{A}ugmented \textbf{N}ovel and \textbf{D}ifficult Mathematics problems and solutions), a pipeline that addresses this by first synthesizing high-quality problems from scratch and then systematically elevating their complexity via a our newly proposed \textbf{Difficulty Hiking} step. We demonstrate the effectiveness of our approach through two key findings: \textbf{(1)} Augmenting a strong post-training baseline with a small 500-sample SAND-Math dataset significantly boosts performance, outperforming the next-best synthetic dataset by $\uparrow$ 17.85 absolute points on AIME25 benchmark. \textbf{(2)} In a dedicated ablation study, we show the effectiveness of our Difficulty Hiking process in increasing average problem difficulty from 5.02 to 5.98. This step consequently lifts AIME25 results from 46.38\% to 49.23\%. The full generation pipeline, final dataset, and a fine-tuned model form a practical and scalable toolkit for building capable and efficient mathematical reasoning LLMs.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Stable and Personalised Profiles for Lexical Alignment in Spoken Human-Agent Dialogue</title>
<link>https://arxiv.org/abs/2509.04104</link>
<guid>https://arxiv.org/abs/2509.04104</guid>
<content:encoded><![CDATA[
arXiv:2509.04104v2 Announce Type: replace 
Abstract: Lexical alignment, where speakers start to use similar words across conversation, is known to contribute to successful communication. However, its implementation in conversational agents remains underexplored, particularly considering the recent advancements in large language models (LLMs). As a first step towards enabling lexical alignment in human-agent dialogue, this study draws on strategies for personalising conversational agents and investigates the construction of stable, personalised lexical profiles as a basis for lexical alignment. Specifically, we varied the amounts of transcribed spoken data used for construction as well as the number of items included in the profiles per part-of-speech (POS) category and evaluated profile performance across time using recall, coverage, and cosine similarity metrics. It was shown that smaller and more compact profiles, created after 10 min of transcribed speech containing 5 items for adjectives, 5 items for conjunctions, and 10 items for adverbs, nouns, pronouns, and verbs each, offered the best balance in both performance and data efficiency. In conclusion, this study offers practical insights into constructing stable, personalised lexical profiles, taking into account minimal data requirements, serving as a foundational step toward lexical alignment strategies in conversational agents.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling</title>
<link>https://arxiv.org/abs/2509.18467</link>
<guid>https://arxiv.org/abs/2509.18467</guid>
<content:encoded><![CDATA[
arXiv:2509.18467v2 Announce Type: replace 
Abstract: Although transformer architectures have achieved state-of-the-art performance across diverse domains, their quadratic computational complexity with respect to sequence length remains a significant bottleneck, particularly for latency-sensitive long-context applications. While recent linear-complexity alternatives are increasingly powerful, effectively training them from scratch is still resource-intensive. To overcome these limitations, we propose LAWCAT (Linear Attention with Convolution Across Time), a novel linearization framework designed to efficiently transfer the capabilities of pre-trained transformers into a performant linear attention architecture. LAWCAT integrates causal Conv1D layers to enhance local dependency modeling and employs normalized gated linear attention to improve generalization across varying context lengths. Our comprehensive evaluations demonstrate that, distilling Mistral-7B with only 1K-length sequences yields over 90\% passkey retrieval accuracy up to 22K tokens, significantly extending its effective context window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance on S-NIAH 1\&2\&3 tasks (1K-8K context length) and BABILong benchmark (QA2\&amp;QA3, 0K-16K context length), requiring less than 0.1\% pre-training tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT thus provides an efficient pathway to high-performance, long-context linear models suitable for edge deployment, reducing reliance on extensive long-sequence training data and computational resources. Code is released at: https://github.com/zeyuliu1037/LAWCAT
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation</title>
<link>https://arxiv.org/abs/2510.02855</link>
<guid>https://arxiv.org/abs/2510.02855</guid>
<content:encoded><![CDATA[
arXiv:2510.02855v3 Announce Type: replace 
Abstract: Wordle presents an algorithmically rich testbed for constraint satisfaction problem (CSP) solving. While existing solvers rely on information-theoretic entropy maximization or frequency-based heuristics without formal constraint treatment, we present the first comprehensive CSP formulation of Wordle with novel constraint-aware solving strategies. We introduce CSP-Aware Entropy, computing information gain after constraint propagation rather than on raw candidate sets, and a Probabilistic CSP framework integrating Bayesian word-frequency priors with logical constraints. Through evaluation on 2,315 English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9% success rate, a statistically significant 1.7% improvement over Forward Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3 percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic CSP achieves 100% success across all noise levels (0-20%) through constraint recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates 88% success with zero language-specific tuning, validating that core CSP principles transfer across languages despite an 11.2 percentage point gap from linguistic differences (p<0.001, Fisher's exact test). Our open-source implementation with 34 unit tests achieving 91% code coverage provides reproducible infrastructure for CSP research. The combination of formal CSP treatment, constraint-aware heuristics, probabilistic-logical integration, robustness analysis, and cross-lexicon validation establishes new performance benchmarks demonstrating that principled constraint satisfaction techniques outperform classical information-theoretic and learning-based approaches for structured puzzle-solving domains.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives</title>
<link>https://arxiv.org/abs/2510.04983</link>
<guid>https://arxiv.org/abs/2510.04983</guid>
<content:encoded><![CDATA[
arXiv:2510.04983v3 Announce Type: replace 
Abstract: Identifying cultural capital (CC) themes in student reflections can offer valuable insights that help foster equitable learning environments in classrooms. However, themes such as aspirational goals or family support are often woven into narratives, rather than appearing as direct keywords. This makes them difficult to detect for standard NLP models that process sentences in isolation. The core challenge stems from a lack of awareness, as standard models are pre-trained on general corpora, leaving them blind to the domain-specific language and narrative context inherent to the data. To address this, we introduce AWARE, a framework that systematically attempts to improve a transformer model's awareness for this nuanced task. AWARE has three core components: 1) Domain Awareness, adapting the model's vocabulary to the linguistic style of student reflections; 2) Context Awareness, generating sentence embeddings that are aware of the full essay context; and 3) Class Overlap Awareness, employing a multi-label strategy to recognize the coexistence of themes in a single sentence. Our results show that by making the model explicitly aware of the properties of the input, AWARE outperforms a strong baseline by 2.1 percentage points in Macro-F1 and shows considerable improvements across all themes. This work provides a robust and generalizable methodology for any text classification task in which meaning depends on the context of the narrative.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Long-context Modeling from Context Denoising Perspective</title>
<link>https://arxiv.org/abs/2510.05862</link>
<guid>https://arxiv.org/abs/2510.05862</guid>
<content:encoded><![CDATA[
arXiv:2510.05862v2 Announce Type: replace 
Abstract: Long-context models (LCMs) have demonstrated great potential in processing long sequences, facilitating many real-world applications. The success of LCMs can be attributed to their ability to locate implicit critical information within the context for further prediction. However, recent research reveals that LCMs are often susceptible to contextual noise, i.e., irrelevant tokens, that can mislead model attention. In this paper, we conduct a fine-grained analysis of the context noise and propose an effective metric, the Integrated Gradient (IG) score, to detect and quantify the noise information within the context. Our findings reveal that even simple mitigation of detected context noise can substantially boost the model's attention on critical tokens and benefit subsequent predictions. Building on this insight, we propose Context Denoising Training (CDT), a straightforward yet effective training strategy that improves attention on critical tokens while reinforcing their influence on model predictions. Extensive experiments across four tasks, under both context window scaling and long-context alignment settings, demonstrate the superiority of CDT. Notably, when trained with CDT, an open-source 8B model can achieve performance (50.92) comparable to GPT-4o (51.00).
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling</title>
<link>https://arxiv.org/abs/2510.06915</link>
<guid>https://arxiv.org/abs/2510.06915</guid>
<content:encoded><![CDATA[
arXiv:2510.06915v2 Announce Type: replace 
Abstract: Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasingly involve long history trajectories, e.g., LLM agent, it becomes indispensable to evaluate whether a model's responses are not only high-quality but also grounded in and consistent with the provided context. Yet, current RMs remain confined to short-context settings and primarily focus on response-level attributes (e.g., safety or helpfulness), while largely neglecting the critical dimension of long context-response consistency. In this work, we introduce Long-RewardBench, a benchmark specifically designed for long-context RM evaluation, featuring both Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that even state-of-the-art generative RMs exhibit significant fragility in long-context scenarios, failing to maintain context-aware preference judgments. Motivated by the analysis of failure patterns observed in model outputs, we propose a general multi-stage training strategy that effectively scales arbitrary models into robust Long-context RMs (LongRMs). Experiments show that our approach not only substantially improves performance on long-context evaluation but also preserves strong short-context capability. Notably, our 8B LongRM outperforms much larger 70B-scale baselines and matches the performance of the proprietary Gemini 2.5 Pro model.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hey, wait a minute: on at-issue sensitivity in Language Models</title>
<link>https://arxiv.org/abs/2510.12740</link>
<guid>https://arxiv.org/abs/2510.12740</guid>
<content:encoded><![CDATA[
arXiv:2510.12740v2 Announce Type: replace 
Abstract: Evaluating the naturalness of dialogue in language models (LMs) is not trivial: notions of 'naturalness' vary, and scalable quantitative metrics remain limited. This study leverages the linguistic notion of 'at-issueness' to assess dialogue naturalness and introduces a new method: Divide, Generate, Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii) generates continuations for subparts using LMs, (iii) recombines the dialogue and continuations, and (iv) compares the likelihoods of the recombined sequences. This approach mitigates bias in linguistic analyses of LMs and enables systematic testing of discourse-sensitive behavior. Applying DGRC, we find that LMs prefer to continue dialogue on at-issue content, with this effect enhanced in instruct-tuned models. They also reduce their at-issue preference when relevant cues (e.g., "Hey, wait a minute") are present. Although instruct-tuning does not further amplify this modulation, the pattern reflects a hallmark of successful dialogue dynamics.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking</title>
<link>https://arxiv.org/abs/2510.17013</link>
<guid>https://arxiv.org/abs/2510.17013</guid>
<content:encoded><![CDATA[
arXiv:2510.17013v2 Announce Type: replace 
Abstract: Recent LLM benchmarks have tested models on a range of phenomena, but are still focused primarily on natural language understanding for extraction of explicit information, such as QA or summarization, with responses often tar- geting information from individual sentences. We are still lacking more challenging, and im- portantly also multilingual, benchmarks focus- ing on implicit information and pragmatic infer- ences across larger documents in the context of discourse tracking: integrating and aggregating information across sentences, paragraphs and multiple speaker utterances. To this end, we present DiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages and four levels of discourse understanding: salience recognition, entity tracking, discourse relations and bridging inference. Our evaluation shows that these tasks remain challenging, even for state-of-the-art models.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ValueCompass: A Framework for Measuring Contextual Value Alignment Between Human and LLMs</title>
<link>https://arxiv.org/abs/2409.09586</link>
<guid>https://arxiv.org/abs/2409.09586</guid>
<content:encoded><![CDATA[
arXiv:2409.09586v3 Announce Type: replace-cross 
Abstract: As AI systems become more advanced, ensuring their alignment with a diverse range of individuals and societal values becomes increasingly critical. But how can we capture fundamental human values and assess the degree to which AI systems align with them? We introduce ValueCompass, a framework of fundamental values, grounded in psychological theory and a systematic review, to identify and evaluate human-AI alignment. We apply ValueCompass to measure the value alignment of humans and large language models (LLMs) across four real-world scenarios: collaborative writing, education, public sectors, and healthcare. Our findings reveal concerning misalignments between humans and LLMs, such as humans frequently endorse values like "National Security" which were largely rejected by LLMs. We also observe that values differ across scenarios, highlighting the need for context-aware AI alignment strategies. This work provides valuable insights into the design space of human-AI alignment, laying the foundations for developing AI systems that responsibly reflect societal values and ethics.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Program Distillation with Template-Based Augmentation</title>
<link>https://arxiv.org/abs/2412.08564</link>
<guid>https://arxiv.org/abs/2412.08564</guid>
<content:encoded><![CDATA[
arXiv:2412.08564v4 Announce Type: replace-cross 
Abstract: Adapting visual programming or prompting large language models (LLMs) to generate executable code for visual tasks like visual question answering (VQA) for specialized tasks or domains remains challenging due to high annotation and inference costs. We propose a low-cost visual program distillation method that can be used for models with at most 1 billion parameters and requires no human-generated program annotations. We achieve this through synthetic data augmentation based on decoupling programs into higher-level skills, called templates, and their corresponding arguments. Experimental results show that, with a relatively small amount of question/answer data, small language models can generate high-quality specialized visual programs with the added benefit of much faster inference
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Optimizing Agentic Workflows via Shapley value</title>
<link>https://arxiv.org/abs/2502.00510</link>
<guid>https://arxiv.org/abs/2502.00510</guid>
<content:encoded><![CDATA[
arXiv:2502.00510v3 Announce Type: replace-cross 
Abstract: Agentic workflows have become the dominant paradigm for building complex AI systems, orchestrating specialized components, such as planning, reasoning, action execution, and reflection, to tackle sophisticated real-world tasks. However, systematically analyzing and optimizing these workflows remains challenging due to intricate component interdependencies and the lack of principled attribution methods. In this work, we introduce ShapleyFlow, the first framework that employs cooperative game theory to analyze and optimize agentic workflows. By applying the Shapley value to evaluate all possible component configurations, ShapleyFlow enables fine-grained attribution of each component's contribution and facilitates the identification of task-specific optimal configurations. Through a constructed dataset evaluated across 7 scenarios, such as navigation, math and OS, we demonstrate 3 key contributions: (1) Theoretical Framework: a principled game-theoretic approach for the attribution of contributions in agentic workflows. (2) Optimal Workflow Discovery: ShapleyFlow identifies task-specific component configurations that consistently outperform workflows relying on a single LLM across all tested tasks. (3) Comprehensive Analysis: we construct and analyze over 1,500 tasks, providing actionable insights and design guidelines for optimizing workflows across multiple domains.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning</title>
<link>https://arxiv.org/abs/2502.02770</link>
<guid>https://arxiv.org/abs/2502.02770</guid>
<content:encoded><![CDATA[
arXiv:2502.02770v5 Announce Type: replace-cross 
Abstract: Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse attention can surprisingly achieve adaptive budgeting. Based on this, we propose Twilight, a framework to bring adaptive sparsity to any existing sparse attention algorithm without sacrificing their accuracy. Empirical results show that Twilight can adaptively prune at most 98% of redundant tokens, leading to $15.4\times$ acceleration in self-attention operations and $3.9\times$ acceleration in end-to-end per token latency in long context LLM decoding.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Language Models to Reason Efficiently</title>
<link>https://arxiv.org/abs/2502.04463</link>
<guid>https://arxiv.org/abs/2502.04463</guid>
<content:encoded><![CDATA[
arXiv:2502.04463v4 Announce Type: replace-cross 
Abstract: Scaling model size and training data has led to great advances in the performance of Large Language Models (LLMs). However, the diminishing returns of this approach necessitate alternative methods to improve model capabilities, particularly in tasks requiring advanced reasoning. Large reasoning models, which leverage long chain-of-thoughts, bring unprecedented breakthroughs in problem-solving capabilities but at a substantial deployment cost associated to longer generations. Reducing inference costs is crucial for the economic feasibility, user experience, and environmental sustainability of these models.
  In this work, we propose to train large reasoning models to reason efficiently. More precisely, we use reinforcement learning (RL) to train reasoning models to dynamically allocate inference-time compute based on task complexity. Our method incentivizes models to minimize unnecessary computational overhead while maintaining accuracy, thereby achieving substantial efficiency gains. It enables the derivation of a family of reasoning models with varying efficiency levels, controlled via a single hyperparameter. Experiments on two open-weight large reasoning models demonstrate significant reductions in inference cost while preserving most of the accuracy.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation</title>
<link>https://arxiv.org/abs/2503.18065</link>
<guid>https://arxiv.org/abs/2503.18065</guid>
<content:encoded><![CDATA[
arXiv:2503.18065v3 Announce Type: replace-cross 
Abstract: Data scarcity is a long-standing challenge in the Vision-Language Navigation (VLN) field, which extremely hinders the generalization of agents to unseen environments. Previous works primarily rely on additional simulator data or web-collected images/videos to improve the generalization. However, the simulator environments still face limited diversity, and the web-collected data often requires extensive labor to remove the noise. In this paper, we propose a Rewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates the unseen observation-instruction pairs via rewriting human-annotated training data. Benefiting from our rewriting mechanism, new observation-instruction pairs can be obtained in both simulator-free and labor-saving manners to promote generalization. Specifically, we first introduce Object-Enriched Observation Rewriting, where we combine Vision-Language Models (VLMs) and Large Language Models (LLMs) to derive rewritten object-enriched scene descriptions, enabling observation synthesis with diverse objects and spatial layouts via Text-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast Instruction Rewriting, which generates observation-aligned rewritten instructions by requiring LLMs to reason the difference between original and new observations. We further develop a mixing-then-focusing training strategy with a random observation cropping scheme, effectively enhancing data distribution diversity while suppressing augmentation data noise during training. Experiments on both the discrete environments (R2R, REVERIE, and R4R datasets) and continuous environments (R2R-CE dataset) show the superior performance and impressive generalization ability of our method. Code is available at https://github.com/SaDil13/VLN-RAM.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Contrastive Learning: Synthetic Data Enables List-wise Training with Multiple Levels of Relevance</title>
<link>https://arxiv.org/abs/2503.23239</link>
<guid>https://arxiv.org/abs/2503.23239</guid>
<content:encoded><![CDATA[
arXiv:2503.23239v2 Announce Type: replace-cross 
Abstract: Although synthetic data has changed various aspects of information retrieval (IR) pipelines, the main training paradigm remains: contrastive learning with binary relevance labels, where one positive document is compared against several negatives using the InfoNCE loss. This objective treats all documents that are not explicitly annotated as relevant on an equally negative footing, regardless of their actual degree of relevance, thus missing subtle nuances useful for ranking. To overcome this limitation, in this work, we forgo real documents and annotations and use large language models to directly generate synthetic documents that answer the MS MARCO queries according to several different levels of relevance. We also propose using Wasserstein distance as a more effective loss function for training transformer-based retrievers with graduated relevance labels. Our experiments on MS MARCO and BEIR benchmark show that our proposed approach outperforms conventional training with InfoNCE by a large margin. Without using any real documents, our method significantly improves self-supervised retrievers and is more robust to distribution shift compared to contrastive learning using real data. Our method also successfully integrates existing real data into the synthetic ranking context, further boosting the performance. Overall, we show that generating multi-level ranking contexts is a better approach to synthetic data generation for IR than just generating the standard positive and negative documents.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents</title>
<link>https://arxiv.org/abs/2505.20411</link>
<guid>https://arxiv.org/abs/2505.20411</guid>
<content:encoded><![CDATA[
arXiv:2505.20411v2 Announce Type: replace-cross 
Abstract: LLM-based agents have shown promising capabilities in a growing range of software engineering (SWE) tasks. However, advancing this field faces two critical challenges. First, high-quality training data is scarce, especially data that reflects real-world SWE scenarios, where agents must interact with development environments, execute code and adapt behavior based on the outcomes of their actions. Existing datasets are either limited to one-shot code generation or comprise small, manually curated collections of interactive tasks, lacking both scale and diversity. Second, the lack of fresh interactive SWE tasks affects evaluation of rapidly improving models, as static benchmarks quickly become outdated due to contamination issues. To address these limitations, we introduce a novel, automated, and scalable pipeline to continuously extract real-world interactive SWE tasks from diverse GitHub repositories. Using this pipeline, we construct SWE-rebench, a public dataset comprising over 21,000 interactive Python-based SWE tasks, suitable for reinforcement learning of SWE agents at scale. Additionally, we use continuous supply of fresh tasks collected using SWE-rebench methodology to build a contamination-free benchmark for agentic software engineering. We compare results of various LLMs on this benchmark to results on SWE-bench Verified and show that performance of some language models might be inflated due to contamination issues.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Growing Transformers: Modular Composition and Layer-wise Expansion on a Frozen Substrate</title>
<link>https://arxiv.org/abs/2507.07129</link>
<guid>https://arxiv.org/abs/2507.07129</guid>
<content:encoded><![CDATA[
arXiv:2507.07129v2 Announce Type: replace-cross 
Abstract: The prevailing paradigm for scaling large language models (LLMs) involves monolithic, end-to-end training, a resource-intensive process that lacks flexibility. This paper explores an alternative, constructive scaling paradigm, enabled by the principle of emergent semantics in Transformers with frozen, non-semantic input embeddings. We posit that because high-level meaning is a compositional property of a Transformer's deep layers, not its input vectors, the embedding layer and trained lower layers can serve as a fixed foundation. This liberates backpropagation to focus solely on newly added components, making incremental growth viable. We operationalize this with a layer-wise constructive methodology that combines strict layer freezing in early stages with efficient, holistic fine-tuning of the entire model stack via low-rank adaptation (LoRA) as complexity increases. This method not only demonstrates stable convergence but also reveals a direct correlation between model depth and the emergence of complex reasoning abilities, such as those required for SQuAD, which are absent in shallower models. In a controlled study, our constructively grown model rivals the performance of a monolithically trained baseline of the same size, validating the efficiency and efficacy of the approach. Our findings suggest a path towards a paradigm shift from monolithic optimization towards a more biological or constructive model of AI development. This opens a path for more resource-efficient scaling, continual learning, and a more modular approach to building powerful AI systems. We release all code and models to facilitate further research.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Audio-Thinker: Guiding Audio Language Model When and How to Think via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.08039</link>
<guid>https://arxiv.org/abs/2508.08039</guid>
<content:encoded><![CDATA[
arXiv:2508.08039v3 Announce Type: replace-cross 
Abstract: Recent advancements in large language models, multimodal large language models, and large audio language models (LALMs) have significantly improved their reasoning capabilities through reinforcement learning with rule-based rewards. However, the explicit reasoning process has yet to show significant benefits for audio question answering, and effectively leveraging deep reasoning remains an open challenge, with LALMs still falling short of human-level auditory-language reasoning. To address these limitations, we propose Audio-Thinker, a reinforcement learning framework designed to enhance the reasoning capabilities of LALMs, with a focus on improving adaptability, consistency, and effectiveness. Our approach introduces an adaptive think accuracy reward, enabling the model to adjust its reasoning strategies based on task complexity dynamically. Furthermore, we incorporate an external reward model to evaluate the overall consistency and quality of the reasoning process, complemented by think-based rewards that help the model distinguish between valid and flawed reasoning paths during training. Experimental results demonstrate that our Audio-Thinker model outperforms existing reasoning-oriented LALMs across various benchmark tasks, exhibiting superior reasoning and generalization capabilities.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation</title>
<link>https://arxiv.org/abs/2508.19999</link>
<guid>https://arxiv.org/abs/2508.19999</guid>
<content:encoded><![CDATA[
arXiv:2508.19999v2 Announce Type: replace-cross 
Abstract: This paper introduces an algorithm to select demonstration examples for in-context learning of a query set. Given a set of $n$ examples, how can we quickly select $k$ out of $n$ to best serve as the conditioning for downstream inference? This problem has broad applications in prompt tuning and chain-of-thought reasoning. Since model weights remain fixed during in-context learning, previous work has sought to design methods based on the similarity of token embeddings. This work proposes a new approach based on gradients of the output taken in the input embedding space. Our approach estimates model outputs through a first-order approximation using the gradients. Then, we apply this estimation to multiple randomly sampled subsets. Finally, we aggregate the sampled subset outcomes to form an influence score for each demonstration, and select $k$ most relevant examples. This procedure only requires pre-computing model outputs and gradients once, resulting in a linear-time algorithm relative to model and training set sizes. Extensive experiments across various models and datasets validate the efficiency of our approach. We show that the gradient estimation procedure yields approximations of full inference with less than ${1}\%$ error across six datasets. This allows us to scale up subset selection that would otherwise run full inference by up to ${37.7}\times$ on models with up to $34$ billion parameters, and outperform existing selection methods based on input embeddings by ${11}\%$ on average.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative World Models of Tasks: LLM-Driven Hierarchical Scaffolding for Embodied Agents</title>
<link>https://arxiv.org/abs/2509.04731</link>
<guid>https://arxiv.org/abs/2509.04731</guid>
<content:encoded><![CDATA[
arXiv:2509.04731v3 Announce Type: replace-cross 
Abstract: Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring successes in large language models. However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-to-end approach often fails due to intractable exploration spaces and sparse rewards. We propose that an effective world model for decision-making must model the world's physics and also its task semantics. A systematic review of 2024 research in low-resource multi-agent soccer reveals a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning. We formalize this trend into a framework for Hierarchical Task Environments (HTEs), which are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play. Our framework incorporates the use of Large Language Models (LLMs) as generative world models of tasks, capable of dynamically generating this scaffolding. We argue that HTEs provide a mechanism to guide exploration, generate meaningful learning signals, and train agents to internalize hierarchical structure, enabling the development of more capable and general-purpose agents with greater sample efficiency than purely end-to-end approaches.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowRL: Matching Reward Distributions for LLM Reasoning</title>
<link>https://arxiv.org/abs/2509.15207</link>
<guid>https://arxiv.org/abs/2509.15207</guid>
<content:encoded><![CDATA[
arXiv:2509.15207v3 Announce Type: replace-cross 
Abstract: We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of $10.0\%$ over GRPO and $5.1\%$ over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deterministic Legal Agents: A Canonical Primitive API for Auditable Reasoning over Temporal Knowledge Graphs</title>
<link>https://arxiv.org/abs/2510.06002</link>
<guid>https://arxiv.org/abs/2510.06002</guid>
<content:encoded><![CDATA[
arXiv:2510.06002v2 Announce Type: replace-cross 
Abstract: For autonomous legal agents to operate safely in high-stakes domains, they require a foundation of absolute determinism and auditability-guarantees that standard Retrieval-Augmented Generation (RAG) frameworks cannot provide. When interacting with temporal knowledge graphs that model the complex evolution of legal norms, agents must navigate versioning, causality, and hierarchical structures with precision, a task for which black-box vector search is ill-suited. This paper introduces a new architectural pattern to solve this: a formal Primitive API designed as a secure execution layer for reasoning over such graphs. Instead of a monolithic query engine, our framework provides a library of canonical primitives-atomic, composable, and auditable primitives. This design empowers planner-guided agents to decompose complex legal questions into transparent execution plans, enabling critical tasks with full verifiability, including: (i) precise point-in-time version retrieval, (ii) robust causal lineage tracing, and (iii) context-aware hybrid search. Ultimately, this architecture transforms opaque retrieval into auditable reasoning, turning the agent's internal process from a black box into a verifiable log of deterministic primitives and providing a blueprint for building the next generation of trustworthy legal AI.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Debiasing LLMs by Masking Unfairness-Driving Attention Heads</title>
<link>https://arxiv.org/abs/2510.10142</link>
<guid>https://arxiv.org/abs/2510.10142</guid>
<content:encoded><![CDATA[
<div> debiasing framework, large language models, unfairness, prompting strategy, attention heads
Summary:
- A debiasing framework called DiffHeads is proposed for large language models (LLMs) to mitigate unfairness in decision-making processes where biased outputs are unacceptable.
- The study compares Direct-Answer (DA) prompting and Chain-of-Thought (CoT) prompting in eight representative LLMs and shows that DA significantly improves measured unfairness in dialogues.
- A token-to-head contribution score is introduced to trace each token's influence back to individual attention heads, revealing a small cluster of bias heads that are activated under DA but not CoT.
- DiffHeads identifies and masks bias heads through a differential activation analysis between DA and CoT, reducing unfairness without compromising model utility.
- The proposed framework reduces unfairness by 49.4% and 40.3% under DA and CoT prompting strategies, respectively. <div>
arXiv:2510.10142v3 Announce Type: replace 
Abstract: Large language models (LLMs) increasingly mediate decisions in domains where unfair treatment of demographic groups is unacceptable. Existing work probes when biased outputs appear, but gives little insight into the mechanisms that generate them, leaving existing mitigations largely fragile. In this paper, we conduct a systematic investigation LLM unfairness and propose DiffHeads, a lightweight debiasing framework for LLMs. We first compare Direct-Answer (DA) prompting to Chain-of-Thought (CoT) prompting across eight representative open- and closed-source LLMs. DA will trigger the nature bias part of LLM and improve measured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues. Next, we define a token-to-head contribution score that traces each token's influence back to individual attention heads. This reveals a small cluster of bias heads that activate under DA but stay largely dormant with CoT, providing the first causal link between prompting strategy and bias emergence. Finally, building on this insight, we propose DiffHeads that identifies bias heads through differential activation analysis between DA and CoT, and selectively masks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under DA and CoT, respectively, without harming model utility.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization</title>
<link>https://arxiv.org/abs/2511.00010</link>
<guid>https://arxiv.org/abs/2511.00010</guid>
<content:encoded><![CDATA[
<div> benchmark, visualization tasks, large language models, code generation, complex data visualization  
Summary:  
PlotCraft is a new benchmark introduced to evaluate the performance of Large Language Models (LLMs) in generating complex visualizations for scaled and structured data. The benchmark consists of 1k challenging visualization tasks covering various topics and includes 48 distinct chart types. It evaluates both single-turn generation and multi-turn refinement and highlights deficiencies in current LLMs' ability to handle sophisticated visualization tasks. To address this performance gap, the authors introduce SynthVis-30K, a dataset of complex visualization code, and develop PlotCraftor, a code generation model that excels in complex data visualization with a small size. PlotCraftor demonstrates comparable performance to leading proprietary approaches on various benchmarks, showing over 50% improvement on challenging tasks. The benchmark, dataset, and code will be available for access on GitHub.  
<br /><br />Summary: <div>
arXiv:2511.00010v1 Announce Type: new 
Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable profi- ciency in code generation. However, their ability to create complex visualiza- tions for scaled and structured data remains largely unevaluated and underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark featuring 1k challenging visualization tasks that cover a wide range of topics, such as fi- nance, scientific research, and sociology. The benchmark is structured around seven high-level visualization tasks and encompasses 48 distinct chart types. Cru- cially, it is the first to systematically evaluate both single-turn generation and multi-turn refinement across a diverse spectrum of task complexities. Our com- prehensive evaluation of 23 leading LLMs on PlotCraft reveals obvious per- formance deficiencies in handling sophisticated visualization tasks. To bridge this performance gap, we develope SynthVis-30K, a large-scale, high-quality dataset of complex visualization code synthesized via a collaborative agent frame- work. Building upon this dataset, we develope PlotCraftor, a novel code gener- ation model that achieves strong capabilities in complex data visualization with a remarkably small size. Across VisEval, PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance comparable to that of leading propri- etary approaches. Especially, on hard task, Our model achieves over 50% per- formance improvement. We will release the benchmark, dataset, and code at https://github.com/Speakn0w/PlotCraft-Benchmark.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference</title>
<link>https://arxiv.org/abs/2511.00115</link>
<guid>https://arxiv.org/abs/2511.00115</guid>
<content:encoded><![CDATA[
<div> Prototype Theory, Personality Recognition, Text-based Modeling, LLM, MBTI

Summary: 

ProtoMBTI introduces a novel framework for personality recognition from text using prototype theory aligned with cognitive processes. The approach involves constructing a balanced corpus, augmenting it using an LLM-guided multi-dimensional approach, and fine-tuning a lightweight encoder for learning discriminative embeddings and standardizing personality prototypes. The model follows a retrieve--reuse--revise--retain cycle to aggregate prototype evidence, revise inconsistencies, and enrich the prototype library upon correct predictions. Results show improvements over baselines on MBTI dichotomies and the full 16-type task, with robust generalization across datasets. By aligning the inference process with psychological prototype reasoning, ProtoMBTI achieves enhanced accuracy, interpretability, and transfer in text-based personality modeling. <div>
arXiv:2511.00115v1 Announce Type: new 
Abstract: Personality recognition from text is typically cast as hard-label classification, which obscures the graded, prototype-like nature of human personality judgments. We present ProtoMBTI, a cognitively aligned framework for MBTI inference that operationalizes prototype theory within an LLM-based pipeline. First, we construct a balanced, quality-controlled corpus via LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment). Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative embeddings and to standardize a bank of personality prototypes. At inference, we retrieve top-k prototypes for a query post and perform a retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence via prompt-based voting, revises when inconsistencies arise, and, upon correct prediction, retains the sample to continually enrich the prototype library. Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both the four MBTI dichotomies and the full 16-type task, and exhibits robust cross-dataset generalization. Our results indicate that aligning the inference process with psychological prototype reasoning yields gains in accuracy, interpretability, and transfer for text-based personality modeling.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParaScopes: What do Language Models Activations Encode About Future Text?</title>
<link>https://arxiv.org/abs/2511.00180</link>
<guid>https://arxiv.org/abs/2511.00180</guid>
<content:encoded><![CDATA[
<div> Keywords: Interpretability, language models, Residual Stream Decoders, activations, planning <br />
<br />
Summary: 
Interpretability studies in language models often focus on forward-looking representations of activations. However, current methods for understanding activations in language models are limited to specific concepts or tokens, despite their ability to perform longer time horizon tasks. This study introduces a framework of Residual Stream Decoders to probe model activations for paragraph-scale and document-scale plans. Testing various methods, the results show that information equivalent to 5+ tokens of future context can be decoded in small models. These findings pave the way for improved monitoring of language models and a better understanding of how they encode longer-term planning information. <br /><br /> <div>
arXiv:2511.00180v1 Announce Type: new 
Abstract: Interpretability studies in language models often investigate forward-looking representations of activations. However, as language models become capable of doing ever longer time horizon tasks, methods for understanding activations often remain limited to testing specific concepts or tokens. We develop a framework of Residual Stream Decoders as a method of probing model activations for paragraph-scale and document-scale plans. We test several methods and find information can be decoded equivalent to 5+ tokens of future context in small models. These results lay the groundwork for better monitoring of language models and better understanding how they might encode longer-term planning information.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap</title>
<link>https://arxiv.org/abs/2511.00198</link>
<guid>https://arxiv.org/abs/2511.00198</guid>
<content:encoded><![CDATA[
<div> next-token prediction, large language models, training performance, model performance, target-token selection strategies

Summary:<br />
This study introduces a novel approach to training large language models (LLMs) by predicting information-rich tokens during training instead of relying on next-token prediction (NTP). By focusing on predicting tokens that carry more valuable information, the proposed method aims to improve both training performance and model performance while keeping computational costs in check. The impact of this new strategy is analyzed across three different tasks for LLMs: arithmetic, multi-label classification of text, and natural-language generation. Through this research, a more principled approach to optimizing LLM training is presented, offering insights into the importance of target-token selection strategies. This work contributes to advancing the field by enhancing the theoretical understanding of LLM training methods and demonstrating the potential benefits of predicting information-rich tokens. 

<br /><br />Summary: <div>
arXiv:2511.00198v1 Announce Type: new 
Abstract: Optimizing training performance in large language models (LLMs) remains an essential challenge, particularly in improving model performance while maintaining computational costs. This work challenges the conventional approach of training LLMs using next-token prediction (NTP), arguing that by predicting information-rich tokens during training, there is a more effective way to train LLMs. We investigate the impact of the proposed solution in three kinds of tasks for LLMs: arithmetic, multi-label classification of text, and natural-language generation. This work offers a principled approach to optimizing LLM training, advancing both model performance and theoretical understanding of the target-token selection strategies.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.00222</link>
<guid>https://arxiv.org/abs/2511.00222</guid>
<content:encoded><![CDATA[
<div> persona consistency, large language models, dialogue, reinforcement learning, user roles

Summary:
Large Language Models (LLMs) are commonly used for simulating human users in various interactive settings. However, these models often struggle with maintaining persona consistency, leading to drift from assigned personas and inconsistent behavior. To address this issue, a unified framework is proposed to evaluate and enhance persona consistency in LLM-generated dialogue. Three automatic metrics are defined to capture different aspects of persona drift and validated against human annotations. Through multi-turn reinforcement learning, LLMs are fine-tuned for specific user roles, resulting in a significant reduction in inconsistency by over 55%. This approach leads to more coherent and faithful simulated users across roles, such as patients, students, and social chat partners. <div>
arXiv:2511.00222v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in interactive settings such as therapy, education, and social role-play. While these simulations enable scalable training and evaluation of AI agents, off-the-shelf LLMs often drift from their assigned personas, contradict earlier statements, or abandon role-appropriate behavior. We introduce a unified framework for evaluating and improving persona consistency in LLM-generated dialogue. We define three automatic metrics: prompt-to-line consistency, line-to-line consistency, and Q&amp;A consistency, that capture different types of persona drift and validate each against human annotations. Using these metrics as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs for three user roles: a patient, a student, and a social chat partner. Our method reduces inconsistency by over 55%, resulting in more coherent and faithful simulated users.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding</title>
<link>https://arxiv.org/abs/2511.00265</link>
<guid>https://arxiv.org/abs/2511.00265</guid>
<content:encoded><![CDATA[
<div> Keywords: cybersecurity, tabletop exercises, AgentBnB, large language model, training

Summary:
The article introduces AgentBnB, a browser-based adaptation of the Backdoors & Breaches cybersecurity game that incorporates large language model teammates and a Bloom-aligned retrieval-augmented copilot (C2D2). This system expands a curated corpus to provide targeted hints in factual, conceptual, procedural, and metacognitive aspects, offering on-demand support during solo-player exercises. A pilot study with four graduate students revealed a preference for the agent-based version over traditional physical card decks, citing scalability as a key advantage. However, limitations such as sample size, single-player focus, and a ceiling effect in knowledge retention were noted. Future developments include multi-player modes, telemetry-driven coaching, and comparative studies with larger cohorts to further explore the effectiveness of large language model augmented tabletop exercises in cybersecurity training.<br /><br />Summary: <div>
arXiv:2511.00265v1 Announce Type: new 
Abstract: Traditional cybersecurity tabletop exercises (TTXs) provide valuable training but are often scripted, resource-intensive, and difficult to scale. We introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches game that integrates large language model teammates with a Bloom-aligned, retrieval-augmented copilot (C2D2). The system expands a curated corpus into factual, conceptual, procedural, and metacognitive snippets, delivering on-demand, cognitively targeted hints. Prompt-engineered agents employ a scaffolding ladder that gradually fades as learner confidence grows. In a solo-player pilot with four graduate students, participants reported greater intention to use the agent-based version compared to the physical card deck and viewed it as more scalable, though a ceiling effect emerged on a simple knowledge quiz. Despite limitations of small sample size, single-player focus, and narrow corpus, these early findings suggest that large language model augmented TTXs can provide lightweight, repeatable practice without the logistical burden of traditional exercises. Planned extensions include multi-player modes, telemetry-driven coaching, and comparative studies with larger cohorts.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval</title>
<link>https://arxiv.org/abs/2511.00268</link>
<guid>https://arxiv.org/abs/2511.00268</guid>
<content:encoded><![CDATA[
<div> Keywords: statute retrieval, precedent retrieval, IL-PCR, legal corpus, re-ranking

Summary:
IL-PCR is introduced as a unique corpus for developing models that address both statute retrieval and precedent retrieval tasks in the legal domain. The interdependence between these tasks is highlighted, as similar cases often reference similar statutes. Various baseline models, including lexical and semantic models, as well as ensemble models based on Graph Neural Networks (GNNs), are tested on the tasks. An LLM-based re-ranking approach is proposed to leverage the relationship between statute and precedent retrieval, showcasing the best performance. The paper underscores the importance of considering the connection between statutes and precedents in developing more effective models for legal information retrieval.<br /><br />Summary: <div>
arXiv:2511.00268v1 Announce Type: new 
Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a given legal situation are common tasks exercised by law practitioners. Researchers to date have addressed the two tasks independently, thus developing completely different datasets and models for each task; however, both retrieval tasks are inherently related, e.g., similar cases tend to cite similar statutes (due to similar factual situation). In this paper, we address this gap. We propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval), which is a unique corpus that provides a common testbed for developing models for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit the dependence between the two. We experiment extensively with several baseline models on the tasks, including lexical models, semantic models and ensemble based on GNNs. Further, to exploit the dependence between the two tasks, we develop an LLM-based re-ranking approach that gives the best performance.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation</title>
<link>https://arxiv.org/abs/2511.00270</link>
<guid>https://arxiv.org/abs/2511.00270</guid>
<content:encoded><![CDATA[
<div> pre-training, sign language translation, transformer-based architecture, synthetic supervision, low-resource settings

Summary:
- Sign language translation faces challenges due to limited datasets, leading to the development of various techniques.
- The POSESTITCH-SLT pre-training scheme is inspired by linguistic-templates for sentence generation.
- A transformer-based encoder-decoder architecture outperforms prior methods using template-generated sentence pairs.
- Significant BLEU-4 score improvements are achieved on the How2Sign and iSign datasets, surpassing previous state-of-the-art techniques.
- Template-driven synthetic supervision proves effective in low-resource sign language translation scenarios. 

<br /><br />Summary: <div>
arXiv:2511.00270v1 Announce Type: new 
Abstract: Sign language translation remains a challenging task due to the scarcity of large-scale, sentence-aligned datasets. Prior arts have focused on various feature extraction and architectural changes to support neural machine translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training scheme that is inspired by linguistic-templates-based sentence generation technique. With translation comparison on two sign language datasets, How2Sign and iSign, we show that a simple transformer-based encoder-decoder architecture outperforms the prior art when considering template-generated sentence pairs in training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for pose-based gloss-free translation. The results demonstrate the effectiveness of template-driven synthetic supervision in low-resource sign language settings.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Modeling With Factorization Memory</title>
<link>https://arxiv.org/abs/2511.00315</link>
<guid>https://arxiv.org/abs/2511.00315</guid>
<content:encoded><![CDATA[
<div> Keywords: Factorization Memory, recurrent neural network, Transformer, Mamba-2, sparse formulation

Summary:
Factorization Memory is introduced as an efficient RNN architecture that achieves comparable performance to Transformer models in short-context language modeling tasks. It demonstrates superior generalization in long-context scenarios by exploiting parallel computations during training while maintaining constant computational complexity during inference. A sparse formulation of Factorization Memory updates only a subset of recurrent states at each step, optimizing model efficiency while preserving strong performance. This is the first RNN architecture to successfully combine sparse memory activation with competitive performance across both short and long-context settings. An empirical analysis compares Factorization Memory to Transformer and Mamba-2 architectures, highlighting its effectiveness in various contexts.

Summary: <br /><br /> <div>
arXiv:2511.00315v1 Announce Type: new 
Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN) architecture that achieves performance comparable to Transformer models on short-context language modeling tasks while also demonstrating superior generalization in long-context scenarios. Our model builds upon Mamba-2, enabling Factorization Memory to exploit parallel computations during training while preserving constant computational and memory complexity during inference. To further optimize model efficiency and representational capacity, we develop a sparse formulation of Factorization Memory that updates only a subset of recurrent states at each step while preserving the strong performance of its dense counterpart. To our knowledge, this represents the first RNN architecture that successfully combines sparse memory activation with competitive performance across both short and long-context settings. This work provides a systematic empirical analysis of Factorization Memory in comparison to Transformer and Mamba-2 architectures.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reversal Invariance in Autoregressive Language Models</title>
<link>https://arxiv.org/abs/2511.00341</link>
<guid>https://arxiv.org/abs/2511.00341</guid>
<content:encoded><![CDATA[
<div> Keywords: causal language modeling, reversal invariance, pretraining objectives, direction-blind, temporal asymmetry

Summary:
The article discusses the concept of reversal invariance in causal language modeling (CLM), where the next-token prediction loss remains the same for both a corpus and its reversal. This implies that standard CLM pretraining is direction-blind, allowing models trained on reversed text to achieve similar performance to those trained on forward text. However, this symmetry may limit the ability of current pretraining objectives to capture directional dependencies inherent in human language. The authors suggest reexamining pretraining through the lens of temporal asymmetry to address this limitation. Future research should focus on developing loss functions and architectures that explicitly model the time asymmetry of language while maintaining standard language modeling capacity.

<br /><br />Summary: <div>
arXiv:2511.00341v1 Announce Type: new 
Abstract: We formalize a structural property of the causal (autoregressive) language modeling (CLM) objective: reversal invariance. Formally, the next-token prediction loss assigns identical likelihood to a corpus and its reversal, implying that standard CLM pretraining is direction-blind. This symmetry explains why models trained on reversed text can achieve comparable performance to those trained on forward text, despite the inherently time-asymmetric nature of human language and reasoning. We argue that this invariance represents a limitation of current pretraining objectives rather than a benign artifact. If natural language encodes directional dependencies - phonological, morphological, or causal - a symmetric objective may fail to capture them. We therefore propose viewing pretraining through the lens of temporal asymmetry, motivating future work on loss functions and architectures that explicitly model the arrow of language while retaining standard language modeling capacity.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LingGym: How Far Are LLMs from Thinking Like Field Linguists?</title>
<link>https://arxiv.org/abs/2511.00343</link>
<guid>https://arxiv.org/abs/2511.00343</guid>
<content:encoded><![CDATA[
<div> Keywords: LingGym, meta-linguistic reasoning, Interlinear Glossed Text, low-resource languages, linguistic analysis<br />
Summary:<br />
- LingGym is introduced as a new benchmark for evaluating LLMs' meta-linguistic reasoning abilities using Interlinear Glossed Text (IGT) and grammatical descriptions from diverse reference grammars. 
- The benchmark focuses on assessing the generalization of linguistic inference across low-resource languages and unfamiliar structures not encountered during training, rather than specific downstream tasks.
- A controlled evaluation task called Word-Gloss Inference is presented, where models must infer missing words and glosses from context using various levels of linguistic information.
- Results indicate that incorporating structured linguistic cues leads to consistent improvements in reasoning performance across all models.
- This study showcases both the potential and current limitations of utilizing LLMs for typologically informed linguistic analysis and documentation of low-resource languages.<br /> 
Summary: <div>
arXiv:2511.00343v1 Announce Type: new 
Abstract: This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and grammatical descriptions extracted from 18 typologically diverse reference grammars. Unlike previous work that focuses on specific downstream tasks, we assess whether LLMs can generalize linguistic inference across low-resource languages and structures not seen during training. We present a controlled evaluation task: Word-Gloss Inference, in which the model must infer a missing word and gloss from context using varying levels of linguistic information (e.g., glosses, grammatical explanations, translations). Our results show that incorporating structured linguistic cues leads to consistent improvements in reasoning performance across all models. This work highlights both the promise and current limitations of using LLMs for typologically informed linguistic analysis and low-resource language documentation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs</title>
<link>https://arxiv.org/abs/2511.00371</link>
<guid>https://arxiv.org/abs/2511.00371</guid>
<content:encoded><![CDATA[
<div> Keywords: Socratic debugging, Reasoning Trajectory, misconceptions, cognitive dissonance, LLM-based solutions

Summary:
Socratic debugging involves guiding students to identify and fix programming bugs by leading them to contradict false programming beliefs. This process, known as Reasoning Trajectory (RT), helps students update their misconceptions through cognitive dissonance. This paper introduces the task of reasoning trajectory generation and provides a dataset with manually annotated debugging problems and RTs. LLM-based solutions are presented for generating RTs and guiding Socratic conversations based on them. An evaluation of frontier models shows high accuracy in generating correct reasoning trajectories and valid conversation turns. This approach offers a new method for instructors to help novice programmers understand and correct their misconceptions through guided reasoning processes. <br /><br />Summary: <div>
arXiv:2511.00371v1 Announce Type: new 
Abstract: In Socratic debugging, instructors guide students towards identifying and fixing a bug on their own, instead of providing the bug fix directly. Most novice programmer bugs are caused by programming misconceptions, namely false beliefs about a programming concept. In this context, Socratic debugging can be formulated as a guided Reasoning Trajectory (RT) leading to a statement about the program behavior that contradicts the bug-causing misconception. Upon reaching this statement, the ensuing cognitive dissonance leads the student to first identify and then update their false belief. In this paper, we introduce the task of reasoning trajectory generation, together with a dataset of debugging problems manually annotated with RTs. We then describe LLM-based solutions for generating RTs and Socratic conversations that are anchored on them. A large-scale LLM-as-judge evaluation shows that frontier models can generate up to 91% correct reasoning trajectories and 98.7% valid conversation turns.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks</title>
<link>https://arxiv.org/abs/2511.00416</link>
<guid>https://arxiv.org/abs/2511.00416</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated text, AIGT detectors, iteratively-paraphrased content, authorship obfuscation, plagiarism evasion

Summary: 
Detecting AI-generated text (AIGT) is challenging due to the effectiveness of iteratively-paraphrased content in evading detection systems. This study explores the reasons behind the success of iteratively-paraphrased text in bypassing AIGT detectors, highlighting the creation of an intermediate laundering region that poses significant challenges for detection. The intermediate laundering region involves semantic displacement while preserving generation patterns, leading to two attack categories: authorship obfuscation and plagiarism evasion. To address these vulnerabilities, the researchers introduce PADBen, a benchmark that evaluates detector robustness against both paraphrase attack scenarios. Evaluating 11 state-of-the-art detectors reveals a critical asymmetry in their performance, with detectors successfully identifying plagiarism evasion but failing for authorship obfuscation. The findings underscore the need for advancements in detection architectures to effectively handle the intermediate laundering region. The code implementation for PADBen is available at https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark. 

<br /><br />Summary: <div>
arXiv:2511.00416v1 Announce Type: new 
Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct LLM outputs, they fail catastrophically against iteratively-paraphrased content. We investigate why iteratively-paraphrased text -- itself AI-generated -- evades detection systems designed for AIGT identification. Through intrinsic mechanism analysis, we reveal that iterative paraphrasing creates an intermediate laundering region characterized by semantic displacement with preserved generation patterns, which brings up two attack categories: paraphrasing human-authored text (authorship obfuscation) and paraphrasing LLM-generated text (plagiarism evasion). To address these vulnerabilities, we introduce PADBen, the first benchmark systematically evaluating detector robustness against both paraphrase attack scenarios. PADBen comprises a five-type text taxonomy capturing the full trajectory from original content to deeply laundered text, and five progressive detection tasks across sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art detectors, revealing critical asymmetry: detectors successfully identify the plagiarism evasion problem but fail for the case of authorship obfuscation. Our findings demonstrate that current detection approaches cannot effectively handle the intermediate laundering region, necessitating fundamental advances in detection architectures beyond existing semantic and stylistic discrimination methods. For detailed code implementation, please see https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts</title>
<link>https://arxiv.org/abs/2511.00421</link>
<guid>https://arxiv.org/abs/2511.00421</guid>
<content:encoded><![CDATA[
<div> reasoning model, error detection, error localization, error correction, cross-lingual evaluation

Summary:
1. Reasoning models outperform standard architectures significantly in error detection and sentence extraction, showing up to 13.5% and 51.0% relative improvement, respectively.
2. Cross-lingual evaluation exposes 5-10% performance gaps in transitioning from English to Japanese, though reasoning models exhibit smaller disparities.
3. Targeted LoRA fine-tuning results in asymmetric enhancements in error correction performance, with improvements of +0.078 for Japanese and +0.168 for English, while maintaining reasoning capabilities.
4. The fine-tuned model surpasses human expert performance in structured medical error correction tasks.
<br /><br />Summary: <div>
arXiv:2511.00421v1 Announce Type: new 
Abstract: Large language models (LLMs) show increasing promise in medical applications, but their ability to detect and correct errors in clinical texts -- a prerequisite for safe deployment -- remains under-evaluated, particularly beyond English. We introduce MedRECT, a cross-lingual benchmark (Japanese/English) that formulates medical error handling as three subtasks: error detection, error localization (sentence extraction), and error correction. MedRECT is built with a scalable, automated pipeline from the Japanese Medical Licensing Examinations (JMLE) and a curated English counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning proprietary, open-weight, and reasoning families. Key findings: (i) reasoning models substantially outperform standard architectures, with up to 13.5% relative improvement in error detection and 51.0% in sentence extraction; (ii) cross-lingual evaluation reveals 5-10% performance gaps from English to Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA fine-tuning yields asymmetric improvements in error correction performance (Japanese: +0.078, English: +0.168) while preserving reasoning capabilities; and (iv) our fine-tuned model exceeds human expert performance on structured medical error correction tasks. To our knowledge, MedRECT is the first comprehensive cross-lingual benchmark for medical error correction, providing a reproducible framework and resources for developing safer medical LLMs across languages.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>G2: Guided Generation for Enhanced Output Diversity in LLMs</title>
<link>https://arxiv.org/abs/2511.00432</link>
<guid>https://arxiv.org/abs/2511.00432</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, output diversity, Guide-to-Generation, training-free, quality

Summary: 
Large Language Models (LLMs) have shown impressive performance in various natural language processing tasks. However, they struggle with output diversity, often producing similar content repeatedly. Existing solutions like temperature scaling compromise output quality while increasing diversity. To address this, the authors propose Guide-to-Generation (G2), a plug-and-play method that enhances output diversity without affecting quality. G2 utilizes base generators and dual Guides to guide the generation process, encouraging diverse outputs based on the original query. Through comprehensive experiments, G2 has been shown to effectively improve output diversity while striking a balance between diversity and quality. This method presents a promising approach to enhancing the capabilities of LLMs in generating diverse and high-quality outputs. 

Summary: <div>
arXiv:2511.00432v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across diverse natural language processing tasks. However, these models exhibit a critical limitation in output diversity, often generating highly similar content across multiple attempts. This limitation significantly affects tasks requiring diverse outputs, from creative writing to reasoning. Existing solutions, like temperature scaling, enhance diversity by modifying probability distributions but compromise output quality. We propose Guide-to-Generation (G2), a training-free plug-and-play method that enhances output diversity while preserving generation quality. G2 employs a base generator alongside dual Guides, which guide the generation process through decoding-based interventions to encourage more diverse outputs conditioned on the original query. Comprehensive experiments demonstrate that G2 effectively improves output diversity while maintaining an optimal balance between diversity and quality.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks</title>
<link>https://arxiv.org/abs/2511.00476</link>
<guid>https://arxiv.org/abs/2511.00476</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Search, Recommendation platforms, Fairness, Bias

Summary:<br />
- Ongoing breakthroughs in Large Language Models are transforming search and recommendation platforms, offering new scientometric tools but also raising fairness and bias concerns.
- The memorization capabilities of LLMs in generating summarized research work may introduce biases in co-authorship networks.
- The study evaluates memorization effects in three LLM models across academic disciplines and world regions.
- Global analysis reveals bias towards highly cited researchers, although certain disciplines and regions show more balanced representation.
- The findings highlight the risks and opportunities in using LLMs for scholarly discovery.<br /><br /> <div>
arXiv:2511.00476v1 Announce Type: new 
Abstract: Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search and recommendation platforms at their core. While this shift unlocks powerful new scientometric tools, it also exposes critical fairness and bias issues that could erode the integrity of the information ecosystem. Additionally, as LLMs become more integrated into web-based searches for scholarly tools, their ability to generate summarized research work based on memorized data introduces new dimensions to these challenges. The extent of memorization in LLMs can impact the accuracy and fairness of the co-authorship networks they produce, potentially reflecting and amplifying existing biases within the scientific community and across different regions. This study critically examines the impact of LLM memorization on the co-authorship networks. To this end, we assess memorization effects across three prominent models, DeepSeek R1, Llama 4 Scout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across academic disciplines and world regions. While our global analysis reveals a consistent bias favoring highly cited researchers, this pattern is not uniformly observed. Certain disciplines, such as Clinical Medicine, and regions, including parts of Africa, show more balanced representation, pointing to areas where LLM training data may reflect greater equity. These findings underscore both the risks and opportunities in deploying LLMs for scholarly discovery.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging the Cross-Domain &amp; Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus</title>
<link>https://arxiv.org/abs/2511.00486</link>
<guid>https://arxiv.org/abs/2511.00486</guid>
<content:encoded><![CDATA[
<div> Keywords: Bhili, Hindi, English, machine translation, parallel corpus

Summary:
The paper introduces the Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest parallel corpus of 110,000 sentences in Bhili, Hindi, and English, created with expert human translators. The corpus spans various domains like education and news, serving as a benchmark for low-resource machine translation research. Evaluation of multilingual language models (MLLMs) on bidirectional translation tasks shows that the fine-tuned NLLB-200 distilled 600M variant model performs best. Additionally, the generative translation capabilities of MLLMs on BHEPC were tested, highlighting their potential in low-resource scenarios. This work aims to bridge the resource gap for underrepresented tribal languages like Bhili, promoting inclusive natural language processing technologies worldwide.<br /><br />Summary: The BHEPC parallel corpus addresses the lack of linguistic resources for underrepresented tribal languages like Bhili in India. Evaluation of multilingual language models demonstrates the effectiveness of the NLLB-200 distilled 600M variant model in low-resource machine translation scenarios. The study also explores the generative translation capabilities of these models, showcasing their potential for marginalized languages globally. <div>
arXiv:2511.00486v1 Announce Type: new 
Abstract: The linguistic diversity of India poses significant machine translation challenges, especially for underrepresented tribal languages like Bhili, which lack high-quality linguistic resources. This paper addresses the gap by introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest parallel corpus worldwide comprising 110,000 meticulously curated sentences across Bhili, Hindi, and English. The corpus was created with the assistance of expert human translators. BHEPC spans critical domains such as education, administration, and news, establishing a valuable benchmark for research in low resource machine translation. To establish a comprehensive Bhili Machine Translation benchmark, we evaluated a wide range of proprietary and open-source Multilingual Large Language Models (MLLMs) on bidirectional translation tasks between English/Hindi and Bhili. Comprehensive evaluation demonstrates that the fine-tuned NLLB-200 distilled 600M variant model outperforms others, highlighting the potential of multilingual models in low resource scenarios. Furthermore, we investigated the generative translation capabilities of multilingual LLMs on BHEPC using in-context learning, assessing performance under cross-domain generalization and quantifying distributional divergence. This work bridges a critical resource gap and promotes inclusive natural language processing technologies for low-resource and marginalized languages globally.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting</title>
<link>https://arxiv.org/abs/2511.00487</link>
<guid>https://arxiv.org/abs/2511.00487</guid>
<content:encoded><![CDATA[
<div> Keywords: Differential Privacy, Natural Language Processing, Text Rewriting Mechanisms, Dataset Size, Privacy-Utility Trade-off

Summary: 
Dataset size is an often-overlooked factor in evaluating Differential Privacy (DP) text privatization mechanisms. This study is the first to consider the impact of dataset size on the efficacy of DP text rewriting techniques for both utility and privacy preservation. Through tests on large-scale datasets with varying sizes, including up to one million texts, the research focuses on quantifying the trade-off between privacy and utility as dataset size increases. The research highlights the importance of dataset size in evaluating DP text rewriting mechanisms and emphasizes the need for more rigorous evaluation procedures in DP Natural Language Processing (NLP). The findings suggest implications for the future of DP NLP at scale, emphasizing the significance of considering dataset size for effective privacy preservation and utility in text rewriting applications. 

<br /><br />Summary: <div>
arXiv:2511.00487v1 Announce Type: new 
Abstract: Recent work in Differential Privacy with Natural Language Processing (DP NLP) has proposed numerous promising techniques in the form of text rewriting mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is that of dataset size, or rather, the effect of dataset size on a mechanism's efficacy for utility and privacy preservation. In this work, we are the first to introduce this factor in the evaluation of DP text privatization, where we design utility and privacy tests on large-scale datasets with dynamic split sizes. We run these tests on datasets of varying size with up to one million texts, and we focus on quantifying the effect of increasing dataset size on the privacy-utility trade-off. Our findings reveal that dataset size plays an integral part in evaluating DP text rewriting mechanisms; additionally, these findings call for more rigorous evaluation procedures in DP NLP, as well as shed light on the future of DP NLP in practice and at scale.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2511.00489</link>
<guid>https://arxiv.org/abs/2511.00489</guid>
<content:encoded><![CDATA[
<div> Tree-oriented MapReduce, long-context reasoning, hierarchical structure, recursive reasoning, logical coherence <br />
Summary:
The article introduces ToM, a novel Tree-oriented MapReduce framework designed for long-context reasoning in Large Language Models (LLMs). Unlike existing methods such as Retrieval-Augmented Generation and divide-and-conquer frameworks, ToM leverages the hierarchical structure of documents to improve logical coherence and capture long-range dependencies. By constructing a DocTree through hierarchical semantic parsing and implementing a bottom-up aggregation approach, ToM enables recursive reasoning. It generates rationales at child nodes in the Map step and aggregates them across sibling nodes in the Reduce step to resolve conflicts and reach consensus at parent nodes. Experimental results on 70B+ LLMs demonstrate that ToM outperforms existing methods in terms of logical coherence and long-context reasoning. The code for ToM is available on GitHub for further exploration. <br /><br />Summary: <div>
arXiv:2511.00489v1 Announce Type: new 
Abstract: Large Language Models (LLMs), constrained by limited context windows, often face significant performance degradation when reasoning over long contexts. To address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over chunks but frequently sacrifices logical coherence due to its reliance on similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split documents into small chunks for independent reasoning and aggregation. While effective for local reasoning, DCF struggles to capture long-range dependencies and risks inducing conflicts by processing chunks in isolation. To overcome these limitations, we propose ToM, a novel Tree-oriented MapReduce framework for long-context reasoning. ToM leverages the inherent hierarchical structure of long documents (e.g., main headings and subheadings) by constructing a DocTree through hierarchical semantic parsing and performing bottom-up aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning: in the Map step, rationales are generated at child nodes; in the Reduce step, these rationales are aggregated across sibling nodes to resolve conflicts or reach consensus at parent nodes. Experimental results on 70B+ LLMs show that ToM significantly outperforms existing divide-and-conquer frameworks and retrieval-augmented generation methods, achieving better logical coherence and long-context reasoning. Our code is available at https://github.com/gjn12-31/ToM .
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge</title>
<link>https://arxiv.org/abs/2511.00505</link>
<guid>https://arxiv.org/abs/2511.00505</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented generation, large language models, knowledge redundancy, dense retrieval, Zero-RAG <br />
Summary: <br />
The article introduces Zero-RAG as a solution to address knowledge redundancy in Large Language Models (LLMs) by pruning redundant external corpus information. The Mastery-Score metric is proposed to identify and prune redundant knowledge in the retrieval-augmented generation (RAG) corpus, leading to more efficient utilization of the LLM's internal knowledge. Zero-RAG also introduces Query Router and Noise-Tolerant Tuning to improve the LLM's utilization of internal knowledge after corpus pruning. Experimental results demonstrate that Zero-RAG can prune the Wikipedia corpus by 30% and accelerate the retrieval stage by 22% without compromising RAG's performance. <div>
arXiv:2511.00505v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation has shown remarkable results to address Large Language Models' hallucinations, which usually uses a large external corpus to supplement knowledge to LLMs. However, with the development of LLMs, the internal knowledge of LLMs has expanded significantly, thus causing significant knowledge redundancy between the external corpus and LLMs. On the one hand, the indexing cost of dense retrieval is highly related to the corpus size and thus significant redundant knowledge intensifies the dense retrieval's workload. On the other hand, the redundant knowledge in the external corpus is not helpful to LLMs and our exploratory analysis shows that it instead hurts the RAG performance on those questions which the LLM can answer by itself. To address these issues, we propose Zero-RAG to tackle these challenges. Specifically, we first propose the Mastery-Score metric to identify redundant knowledge in the RAG corpus to prune it. After pruning, answers to "mastered" questions rely primarily on internal knowledge of the LLM. To better harness the internal capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the irrelevant documents' distraction and thus further improve the LLM's utilization of internal knowledge with pruned corpus. Experimental results show that Zero-RAG prunes the Wikipedia corpus by 30\% and accelerates the retrieval stage by 22\%, without compromising RAG's performance.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations</title>
<link>https://arxiv.org/abs/2511.00514</link>
<guid>https://arxiv.org/abs/2511.00514</guid>
<content:encoded><![CDATA[
<div> Keywords: Conversational agents, healthcare delivery, rural Nepal, DialoGPT, offline capability <br />
Summary: <br />
This study focuses on using a lightweight generative dialogue model, DialoGPT, to support healthcare delivery in rural Nepal, where internet connectivity and cloud infrastructure are often lacking. The model was fine-tuned on a dataset of doctor-patient interactions related to common diseases in rural Nepal. Despite being trained on a limited dataset, the model generated coherent and contextually relevant responses, showing a good understanding of symptoms, disease context, and empathetic communication. The results demonstrate the adaptability of compact, offline-capable dialogue models for healthcare applications in low-resource environments. This study emphasizes the importance of targeted datasets for domain adaptation in medical conversational AI and offers promising directions for improving rural healthcare delivery. <br /> <div>
arXiv:2511.00514v1 Announce Type: new 
Abstract: Conversational agents are increasingly being explored to support healthcare delivery, particularly in resource-constrained settings such as rural Nepal. Large-scale conversational models typically rely on internet connectivity and cloud infrastructure, which may not be accessible in rural areas. In this study, we fine-tuned DialoGPT, a lightweight generative dialogue model that can operate offline, on a synthetically constructed dataset of doctor-patient interactions covering ten common diseases prevalent in rural Nepal, including common cold, seasonal fever, diarrhea, typhoid fever, gastritis, food poisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being trained on a limited, domain-specific dataset, the fine-tuned model produced coherent, contextually relevant, and medically appropriate responses, demonstrating an understanding of symptoms, disease context, and empathetic communication. These results highlight the adaptability of compact, offline-capable dialogue models and the effectiveness of targeted datasets for domain adaptation in low-resource healthcare environments, offering promising directions for future rural medical conversational AI.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models</title>
<link>https://arxiv.org/abs/2511.00519</link>
<guid>https://arxiv.org/abs/2511.00519</guid>
<content:encoded><![CDATA[
<div> gender bias, language models, contextualized word embeddings, transformer-based models, mitigation approach <br />
Summary:
Gender bias in language models, particularly encoder-based transformer models, has become a concerning issue in natural language processing. This study focuses on prominent transformer architectures like BERT, ALBERT, RoBERTa, and DistilBERT to analyze their susceptibility to gender bias in contextualized word embeddings. A new metric, MALoR, is introduced to measure bias based on model probabilities for masked tokens. The study proposes a mitigation strategy involving continued pre-training on a gender-balanced dataset created using Counterfactual Data Augmentation. Experiment results demonstrate significant reductions in gender bias scores across various pronoun pairs, such as "he-she" and "his-her." For example, in BERT-base, bias scores decreased from 1.27 to 0.08 for "he-she" and from 2.51 to 0.36 for "his-her" post-mitigation. The approach successfully diminishes gender bias without sacrificing model performance in downstream tasks. <br /><br /> <div>
arXiv:2511.00519v1 Announce Type: new 
Abstract: Gender bias in language models has gained increasing attention in the field of natural language processing. Encoder-based transformer models, which have achieved state-of-the-art performance in various language tasks, have been shown to exhibit strong gender biases inherited from their training data. This paper investigates gender bias in contextualized word embeddings, a crucial component of transformer-based models. We focus on prominent architectures such as BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to gender bias. To quantify the degree of bias, we introduce a novel metric, MALoR, which assesses bias based on model probabilities for filling masked tokens. We further propose a mitigation approach involving continued pre-training on a gender-balanced dataset generated via Counterfactual Data Augmentation. Our experiments reveal significant reductions in gender bias scores across different pronoun pairs. For instance, in BERT-base, bias scores for "he-she" dropped from 1.27 to 0.08, and "his-her" from 2.51 to 0.36 following our mitigation approach. We also observed similar improvements across other models, with "male-female" bias decreasing from 1.82 to 0.10 in BERT-large. Our approach effectively reduces gender bias without compromising model performance on downstream tasks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly</title>
<link>https://arxiv.org/abs/2511.00536</link>
<guid>https://arxiv.org/abs/2511.00536</guid>
<content:encoded><![CDATA[
<div> word salad, Large Reasoning Models, self-awareness, token reduction, user experience 

Summary:
Large Reasoning Models (LRMs) often face inefficiencies due to the high cost of unnecessary output tokens, referred to as "word salad." LRMs show self-awareness when stuck in repetitive loops, with patterns in hidden states allowing for on-the-fly detection of word salad behavior. A new approach called WordSaladChopper (WSC) is introduced, which efficiently detects and eliminates redundant tokens, leading to significant length savings with minimal impact on quality. The WSC component offers a lightweight solution for improving user experience in LRM applications. The code for WSC is publicly available, making it accessible for implementation in various contexts requiring efficient token management. This method helps optimize LRMs by addressing the issue of word salad, enhancing the overall effectiveness and usability of reasoning models. <div>
arXiv:2511.00536v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) are often bottlenecked by the high cost of output tokens. We show that a significant portion of these tokens are useless self-repetitions - what we call "word salad" - that exhaust the decoding budget without adding value. Interestingly, we observe that LRMs are self-aware when trapped in these loops: the hidden states of <\n\n> tokens trailing each reasoning chunk exhibit patterns that allow us to detect word salad behavior on-the-fly via a single-layer linear classifier. Once detected, a simple chop appended by a straightforward regeneration prompt yields substantial length savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a lightweight, turnkey component for LRM that is minimally invasive to its reasoning trajectory by only removing semantically redundant tokens. Given its low overhead, strong savings, and the lack of semantic value of word salad tokens, we believe it is not too far-fetched to argue that WSC - or a similar component - is a must-have for all LRM applications with user experience in mind. Our code is publicly available at https://github.com/wenyaxie023/WordSaladChopper.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction</title>
<link>https://arxiv.org/abs/2511.00537</link>
<guid>https://arxiv.org/abs/2511.00537</guid>
<content:encoded><![CDATA[
<div> framework, sentiment analysis, deep learning, language models, emotional cues
Summary:
The paper introduces a novel PLM-based framework, CISEA-MRFE, aimed at improving sentiment analysis. It addresses limitations in existing approaches related to nuanced emotional cues, domain shifts, and imbalanced sentiment distributions. The framework integrates Contextual Instruction (CI) for domain-aware sentiment disambiguation, Semantic Enhancement Augmentation (SEA) for robustness, and Multi-Refined Feature Extraction (MRFE) for improved feature specialization and affect-aware sequence modeling. Experimental results on four benchmark datasets show that CISEA-MRFE outperforms strong baselines, achieving relative improvements in accuracy across various domains. The results validate the effectiveness and generalization ability of the proposed approach for sentiment classification. <br /><br />Summary: <div>
arXiv:2511.00537v1 Announce Type: new 
Abstract: Sentiment analysis using deep learning and pre-trained language models (PLMs) has gained significant traction due to their ability to capture rich contextual representations. However, existing approaches often underperform in scenarios involving nuanced emotional cues, domain shifts, and imbalanced sentiment distributions. We argue that these limitations stem from inadequate semantic grounding, poor generalization to diverse linguistic patterns, and biases toward dominant sentiment classes. To overcome these challenges, we propose CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction (CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature Extraction (MRFE). CI injects domain-aware directives to guide sentiment disambiguation; SEA improves robustness through sentiment-consistent paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder (SADE) for multi-scale feature specialization with an Emotion Evaluator Context Encoder (EECE) for affect-aware sequence modeling. Experimental results on four benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb, 6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the effectiveness and generalization ability of our approach for sentiment classification across varied domains.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack</title>
<link>https://arxiv.org/abs/2511.00556</link>
<guid>https://arxiv.org/abs/2511.00556</guid>
<content:encoded><![CDATA[
<div> intent inference, language models, attacks, defense, ISA<br />
Summary:<br />
The article discusses the vulnerability of large language models (LLMs) to jailbreaking attacks and introduces a new attack method called ISA (Intent Shift Attack). ISA obfuscates the intent of attacks by transforming intent through a taxonomy of intent transformations, generating seemingly harmless prompts that can deceive LLMs. Experiments show that ISA significantly increases attack success rates compared to direct harmful prompts and can achieve near-perfect success rates when fine-tuning models on benign data using ISA templates. Existing defense methods are found to be inadequate against ISA, highlighting the need for more effective defenses. The research reveals fundamental challenges in intent inference for LLMs safety and emphasizes the importance of robust safety mechanisms in the face of evolving attack techniques. <div>
arXiv:2511.00556v1 Announce Type: new 
Abstract: Large language models (LLMs) remain vulnerable to jailbreaking attacks despite their impressive capabilities. Investigating these weaknesses is crucial for robust safety mechanisms. Existing attacks primarily distract LLMs by introducing additional context or adversarial tokens, leaving the core harmful intent unchanged. In this paper, we introduce ISA (Intent Shift Attack), which obfuscates LLMs about the intent of the attacks. More specifically, we establish a taxonomy of intent transformations and leverage them to generate attacks that may be misperceived by LLMs as benign requests for information. Unlike prior methods relying on complex tokens or lengthy context, our approach only needs minimal edits to the original request, and yields natural, human-readable, and seemingly harmless prompts. Extensive experiments on both open-source and commercial LLMs show that ISA achieves over 70% improvement in attack success rate compared to direct harmful prompts. More critically, fine-tuning models on only benign data reformulated with ISA templates elevates success rates to nearly 100%. For defense, we evaluate existing methods and demonstrate their inadequacy against ISA, while exploring both training-free and training-based mitigation strategies. Our findings reveal fundamental challenges in intent inference for LLMs safety and underscore the need for more effective defenses. Our code and datasets are available at https://github.com/NJUNLP/ISA.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlashEVA: Accelerating LLM inference via Efficient Attention</title>
<link>https://arxiv.org/abs/2511.00576</link>
<guid>https://arxiv.org/abs/2511.00576</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer models, FlashEVA, efficient implementation, fine-tuning, inference

Summary: FlashEVA is introduced as an efficient implementation of EVA for Transformer models, addressing the high memory demands of maintaining full context during inference. By fine-tuning Transformers to adapt to FlashEVA attention, the method achieves high effectiveness across various downstream tasks with significantly improved performance metrics. Notably, FlashEVA enables up to 6.7x higher throughput and 5x lower peak GPU memory usage compared to standard Transformer implementations, offering a trade-off between throughput and accuracy through adjustable hyperparameters. While exhibiting limitations in retrieval-focused tasks, FlashEVA represents a significant advancement in creating more efficient and adaptable Transformer-based models for inference.<br /><br />Summary: <div>
arXiv:2511.00576v1 Announce Type: new 
Abstract: Transformer models have revolutionized natural language processing, achieving state-of-the-art performance and demonstrating remarkable scalability. However, their memory demands, particularly due to maintaining full context in memory, pose significant challenges for inference. In this paper, we present FlashEVA, an efficient implementation of EVA (Efficient Attention via Control Variates), and demonstrate how to finetune transformers to adapt to FlashEVA attention. Our method enables fine-tuning of Transformer models with as few as 1.5B tokens while preserving effectiveness across various downstream tasks. Notably, FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory usage during inference compared to standard Transformer implementations. Despite these improvements, we observe limitations in retrieval-focused tasks. Our implementation offers control over the trade-off between throughput and accuracy through adjustable hyperparameters, providing flexibility for diverse use cases. This work represents a significant step towards more efficient and adaptable Transformer-based models for inference.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenSIR: Open-Ended Self-Improving Reasoner</title>
<link>https://arxiv.org/abs/2511.00602</link>
<guid>https://arxiv.org/abs/2511.00602</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model, reinforcement learning, self-play, open-ended learning, mathematical discovery

Summary:
OpenSIR is a self-play framework designed for large language models to enhance reasoning abilities without the need for annotated datasets. Through alternating teacher and student roles, the model learns to generate and solve novel problems independently, optimizing for both difficulty and diversity. By rewarding challenging problems that explore distinct concepts, OpenSIR enables open-ended mathematical discovery. The framework significantly improves instruction models, such as Llama-3.2-3B-Instruct and Gemma-2-2B-Instruct, on various datasets. Analyses show that OpenSIR achieves open-ended learning by co-evolving teacher-student roles, adapting difficulty levels, and promoting diverse exploration, leading to autonomous progression from basic to advanced mathematics. The framework showcases the potential for large language models to advance their reasoning capabilities through self-play and independent problem-solving. 

<br /><br />Summary: <div>
arXiv:2511.00602v1 Announce Type: new 
Abstract: Recent advances in large language model (LLM) reasoning through reinforcement learning rely on annotated datasets for verifiable rewards, which may limit models' ability to surpass human-level performance. While self-play offers a promising alternative, existing approaches depend on external verifiers or cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner (OpenSIR), a self-play framework where an LLM learns to generate and solve novel problems by alternating teacher and student roles without external supervision. To generate novel problems, OpenSIR optimises for both difficulty and diversity, rewarding problems that challenge appropriately while exploring distinct concepts, enabling open-ended mathematical discovery. Starting from a single trivial seed problem, OpenSIR substantially improves instruction models: Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to 34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through co-evolving teacher-student roles that adaptively calibrate difficulty and drive diverse exploration, progressing autonomously from basic to advanced mathematics.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding</title>
<link>https://arxiv.org/abs/2511.00606</link>
<guid>https://arxiv.org/abs/2511.00606</guid>
<content:encoded><![CDATA[
<div> Speculative decoding, Large Language Model, speed-ups, SpecDiff-2, state-of-the-art<br />
<br />
Summary: <br />
Speculative decoding has gained popularity for accelerating Large Language Model (LLM) inference by using a draft-then-verify method. However, it faces challenges due to autoregressive dependency and token misalignment. To overcome these barriers, SpecDiff-2 introduces discrete diffusion for drafting and aligns it with autoregressive verifiers. The framework achieves impressive speed improvements across various benchmarks, increasing tokens-per-second by an average of +55% compared to previous methods. SpecDiff-2 sets a new state-of-the-art in reasoning, coding, and mathematical tasks, delivering up to a 5.5x speed-up over standard decoding without sacrificing accuracy. <div>
arXiv:2511.00606v1 Announce Type: new 
Abstract: Speculative decoding has become the standard approach for accelerating Large Language Model (LLM) inference. It exploits a lossless draft-then-verify procedure to circumvent the latency of autoregressive decoding, achieving impressive speed-ups. Yet, current speculative decoding approaches remain limited by two fundamental bottlenecks: (1) the autoregressive dependency during drafting which limits parallelism, and (2) frequent rejections of draft tokens caused by misalignment between the draft and verify models. This paper proposes SpecDiff-2, a novel framework to jointly address these two bottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to address bottleneck (1) and develops novel techniques to calibrate discrete diffusion drafters with autoregressive verifiers, addressing bottleneck (2). Experimental results across a comprehensive benchmark suite show that SpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and mathematical benchmarks, improving tokens-per-second by up to an average of +55% over previous baselines and obtaining up to 5.5x average speed-up over standard decoding, without any loss of accuracy.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios</title>
<link>https://arxiv.org/abs/2511.00620</link>
<guid>https://arxiv.org/abs/2511.00620</guid>
<content:encoded><![CDATA[
<div> Keywords: uncertainty quantification, language models, probability, alignment, theoretical distributions 

Summary: 
Model certainty is crucial for large language models in decision-support applications, but assessing token-level probabilities may not accurately reflect theoretical distributions. A study using GPT-4.1 and DeepSeek-Chat examined model responses to probabilistic prompts, finding perfect accuracy in scenario constraints but divergence in token-level probabilities from theoretical distributions. This highlights the need for improved uncertainty quantification methods in probabilistic scenarios. <br /><br />Summary: <div>
arXiv:2511.00620v1 Announce Type: new 
Abstract: Reliable uncertainty quantification (UQ) is essential for ensuring trustworthy downstream use of large language models, especially when they are deployed in decision-support and other knowledge-intensive applications. Model certainty can be estimated from token logits, with derived probability and entropy values offering insight into performance on the prompt task. However, this approach may be inadequate for probabilistic scenarios, where the probabilities of token outputs are expected to align with the theoretical probabilities of the possible outcomes. We investigate the relationship between token certainty and alignment with theoretical probability distributions in well-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we evaluate model responses to ten prompts involving probability (e.g., roll a six-sided die), both with and without explicit probability cues in the prompt (e.g., roll a fair six-sided die). We measure two dimensions: (1) response validity with respect to scenario constraints, and (2) alignment between token-level output probabilities and theoretical probabilities. Our results indicate that, while both models achieve perfect in-domain response accuracy across all prompt scenarios, their token-level probability and entropy values consistently diverge from the corresponding theoretical distributions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature</title>
<link>https://arxiv.org/abs/2511.00627</link>
<guid>https://arxiv.org/abs/2511.00627</guid>
<content:encoded><![CDATA[
<div> supervised learning, character-level embeddings, French detective fiction, archetype evolution, genre evolution
Summary: 
This research delves into the evolution of the detective archetype in French detective fiction using computational analysis. Through the use of quantitative methods and character-level embeddings, a supervised model successfully captures the essence of the detective archetype spanning 150 years of literature, showcasing a shift from a secondary role to the central character and "reasoning machine" of classical detective stories. As French detective fiction evolved post-World War II, influenced by the hardboiled tradition, the archetype became more intricate, facing challenges of social violence and moral ambiguity within the genre. This study sheds light on the evolution of the detective figure, highlighting its journey from a mere character to a multi-faceted central figure in French detective narratives.<br /><br />Summary: <div>
arXiv:2511.00627v1 Announce Type: new 
Abstract: This research explores the evolution of the detective archetype in French detective fiction through computational analysis. Using quantitative methods and character-level embeddings, we show that a supervised model is able to capture the unity of the detective archetype across 150 years of literature, from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding, the study demonstrates how the detective figure evolves from a secondary narrative role to become the central character and the "reasoning machine" of the classical detective story. In the aftermath of the Second World War, with the importation of the hardboiled tradition into France, the archetype becomes more complex, navigating the genre's turn toward social violence and moral ambiguity.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge</title>
<link>https://arxiv.org/abs/2511.00657</link>
<guid>https://arxiv.org/abs/2511.00657</guid>
<content:encoded><![CDATA[
<div> geography, culture, history, multilingual LLMs, XNationQA <br />
Summary: 
The study introduces XNationQA to assess the cultural literacy of multilingual models, covering 49,280 questions on geography, culture, and history of nine countries in seven languages. Eight multilingual LLMs were evaluated on XNationQA using novel transference metrics, revealing discrepancies in their knowledge of culturally specific facts across languages. Models show better performance in Western languages but struggle with knowledge transfer, especially in open-source models. The models often display more knowledge in English than in the dominant language of the culture being represented. This highlights the gap in evaluating multilingual models' understanding of diverse geographical and cultural information, indicating a need for more inclusive and regionally diverse benchmarks in question-answering tasks. <br /> <div>
arXiv:2511.00657v1 Announce Type: new 
Abstract: Most multilingual question-answering benchmarks, while covering a diverse pool of languages, do not factor in regional diversity in the information they capture and tend to be Western-centric. This introduces a significant gap in fairly evaluating multilingual models' comprehension of factual information from diverse geographical locations. To address this, we introduce XNationQA for investigating the cultural literacy of multilingual LLMs. XNationQA encompasses a total of 49,280 questions on the geography, culture, and history of nine countries, presented in seven languages. We benchmark eight standard multilingual LLMs on XNationQA and evaluate them using two novel transference metrics. Our analyses uncover a considerable discrepancy in the models' accessibility to culturally specific facts across languages. Notably, we often find that a model demonstrates greater knowledge of cultural information in English than in the dominant language of the respective culture. The models exhibit better performance in Western languages, although this does not necessarily translate to being more literate for Western countries, which is counterintuitive. Furthermore, we observe that models have a very limited ability to transfer knowledge across languages, particularly evident in open-source models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?</title>
<link>https://arxiv.org/abs/2511.00689</link>
<guid>https://arxiv.org/abs/2511.00689</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, safety alignment, jailbreak attacks, cross-lingual evaluation, language-aware benchmarks

Summary:<br />
Large language models (LLMs) undergo safety alignment procedures post-training, but recent research has demonstrated vulnerabilities to jailbreak attacks, bypassing these safety measures. This study conducts the first comprehensive multilingual assessment of jailbreaks and defenses across ten languages using six LLMs on HarmBench and AdvBench datasets. Two types of jailbreak attacks are evaluated: logical-expression-based and adversarial-prompt-based. Results show varying levels of attack success and defense robustness across languages, with high-resource languages appearing safer under standard queries but more susceptible to adversarial ones. Simple defenses prove effective but are reliant on the language and LLM being used. These findings highlight the importance of developing language-aware and cross-lingual safety benchmarks for LLMs to enhance their overall security posture.<br /><br />Summary: <div>
arXiv:2511.00689v1 Announce Type: new 
Abstract: Large language models (LLMs) undergo safety alignment after training and tuning, yet recent work shows that safety can be bypassed through jailbreak attacks. While many jailbreaks and defenses exist, their cross-lingual generalization remains underexplored. This paper presents the first systematic multilingual evaluation of jailbreaks and defenses across ten languages--spanning high-, medium-, and low-resource languages--using six LLMs on HarmBench and AdvBench. We assess two jailbreak types: logical-expression-based and adversarial-prompt-based. For both types, attack success and defense robustness vary across languages: high-resource languages are safer under standard queries but more vulnerable to adversarial ones. Simple defenses can be effective, but are language- and model-dependent. These findings call for language-aware and cross-lingual safety benchmarks for LLMs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies</title>
<link>https://arxiv.org/abs/2511.00819</link>
<guid>https://arxiv.org/abs/2511.00819</guid>
<content:encoded><![CDATA[
<div> Keywords: Native Sparse Attention, long-context modeling, Latent Attention, common-sense reasoning, long-text understanding

Summary: 
In this study, the researchers analyze Native Sparse Attention (NSA) and propose enhancements to improve long-context modeling. They suggest alternating between local and global attention across layers to better capture long-range dependencies and boost performance on tasks involving long sequences. Additionally, they introduce Latent Attention to further refine NSA, with the sliding-window branch enhanced by Multi-head Latent Attention (MLA) and the compression and selective branches adopting Group-head Latent Attention (GLA). These modifications not only reduce KV-cache memory by 50% compared to NSA but also improve the model's abilities in common-sense reasoning and understanding long texts. Experimental results on models ranging from 340M to 1.3B parameters trained on 15B and 100B tokens demonstrate that the proposed method can match or surpass both full attention and native sparse attention in common-sense reasoning and long-context understanding tasks.<br /><br />Summary: <div>
arXiv:2511.00819v1 Announce Type: new 
Abstract: In this work, we conduct a systematic analysis of Native Sparse Attention (NSA) and propose targeted improvements that enhance long-context modeling. A key insight is that alternating between local (sliding-window) and global (compression, selective) attention across layers, rather than using fixed patterns, enables more effective propagation of long-range dependencies and substantially boosts performance on long-sequence tasks. Meanwhile, we further refine NSA's branches with Latent Attention that the sliding-window branch is enhanced with Multi-head Latent Attention (MLA) while compression and selective branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache memory by 50\% versus NSA while improving the model's common-sense reasoning and long-text understanding capabilities. Experiments on models from 340M to 1.3B parameters (trained on 15B and 100B tokens) show our method matches or exceeds full attention and native sparse attention in both common-sense reasoning and long-context understanding tasks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models</title>
<link>https://arxiv.org/abs/2511.00854</link>
<guid>https://arxiv.org/abs/2511.00854</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, social biases, debiasing methods, TriCon-Fair, contrastive learning<br />
Summary:<br />
The paper addresses the issue of social biases in large language models and proposes TriCon-Fair, a contrastive learning framework that aims to eliminate positive-negative coupling in existing debiasing methods. By combining triplet and language modeling terms in a decoupled loss function, TriCon-Fair assigns biases explicitly, ensuring improvements for one group do not inadvertently harm another. This approach reduces discriminatory output while maintaining strong downstream performance in sensitive NLP applications. Experimental results show that TriCon-Fair outperforms existing debiasing baselines in reducing social bias. The framework optimizes a language modeling objective to preserve general capability, highlighting its practical and ethical implications for the use of language models in various applications.<br /><br />Summary: <div>
arXiv:2511.00854v1 Announce Type: new 
Abstract: The increasing utilization of large language models raises significant concerns about the propagation of social biases, which may result in harmful and unfair outcomes. However, existing debiasing methods treat the biased and unbiased samples independently, thus ignoring their mutual relationship. This oversight enables a hidden negative-positive coupling, where improvements for one group inadvertently compromise the other, allowing residual social bias to persist. In this paper, we introduce TriCon-Fair, a contrastive learning framework that employs a decoupled loss that combines triplet and language modeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns each anchor an explicitly biased negative and an unbiased positive, decoupling the push-pull dynamics and avoiding positive-negative coupling, and jointly optimizes a language modeling (LM) objective to preserve general capability. Experimental results demonstrate that TriCon-Fair reduces discriminatory output beyond existing debiasing baselines while maintaining strong downstream performance. This suggests that our proposed TriCon-Fair offers a practical and ethical solution for sensitive NLP applications.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing LLM Reasoning Steps via Principal Knowledge Grounding</title>
<link>https://arxiv.org/abs/2511.00879</link>
<guid>https://arxiv.org/abs/2511.00879</guid>
<content:encoded><![CDATA[
<div> knowledge grounding, large language models, evaluation suite, reasoning, metrics

Summary:<br />
- The article introduces an evaluation suite to assess the knowledge grounding of intermediate reasoning in large language models (LLMs). 
- It includes a Principal Knowledge Collection, knowledge-grounded evaluation metrics, and an evaluator LLM for computation. 
- The evaluation suite aims to measure how well models recall and apply prerequisite knowledge in reasoning. 
- It is effective in identifying missing or misapplied knowledge elements, providing insights into reasoning deficiencies in LLMs. 
- The metrics can also be integrated into preference optimization, offering further applications of knowledge-grounded evaluation.<br /> 
Summary: <div>
arXiv:2511.00879v1 Announce Type: new 
Abstract: Step-by-step reasoning has become a standard approach for large language models (LLMs) to tackle complex tasks. While this paradigm has proven effective, it raises a fundamental question: How can we verify that an LLM's reasoning is accurately grounded in knowledge? To address this question, we introduce a novel evaluation suite that systematically assesses the knowledge grounding of intermediate reasoning. Our framework comprises three key components. (1) Principal Knowledge Collection, a large-scale repository of atomic knowledge essential for reasoning. Based on the collection, we propose (2) knowledge-grounded evaluation metrics designed to measure how well models recall and apply prerequisite knowledge in reasoning. These metrics are computed by our (3) evaluator LLM, a lightweight model optimized for cost-effective and reliable metric computation. Our evaluation suite demonstrates remarkable effectiveness in identifying missing or misapplied knowledge elements, providing crucial insights for uncovering fundamental reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these metrics can be integrated into preference optimization, showcasing further applications of knowledge-grounded evaluation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval</title>
<link>https://arxiv.org/abs/2511.00903</link>
<guid>https://arxiv.org/abs/2511.00903</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval-augmented generation, multimodal document retrieval, ColMate, OCR-based pretraining, contrastive learning

Summary:
ColMate is a novel document retrieval model that incorporates multimodal representation learning techniques. It introduces unique features such as an OCR-based pretraining objective, self-supervised masked contrastive learning, and a late interaction scoring mechanism. These innovations lead to significant improvements over existing retrieval models on the ViDoRe V2 benchmark, showcasing enhanced generalization capabilities. By combining these advanced strategies, ColMate demonstrates superior performance in capturing the nuances of multimodal document structures and visual characteristics. The model's ability to bridge the gap between traditional text-only retrieval methods and modern multimodal representation learning makes it a valuable addition to the field of document retrieval. <div>
arXiv:2511.00903v1 Announce Type: new 
Abstract: Retrieval-augmented generation has proven practical when models require specialized knowledge or access to the latest data. However, existing methods for multimodal document retrieval often replicate techniques developed for text-only retrieval, whether in how they encode documents, define training objectives, or compute similarity scores. To address these limitations, we present ColMate, a document retrieval model that bridges the gap between multimodal representation learning and document retrieval. ColMate utilizes a novel OCR-based pretraining objective, a self-supervised masked contrastive learning objective, and a late interaction scoring mechanism more relevant to multimodal document structures and visual characteristics. ColMate obtains 3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark, demonstrating stronger generalization to out-of-domain benchmarks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses</title>
<link>https://arxiv.org/abs/2511.00924</link>
<guid>https://arxiv.org/abs/2511.00924</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, medical diagnostic scenarios, empathy, understandability, biased affective empathy<br />
Summary:<br />
The study evaluates the performance of two large language models (LLMs) in generating explanations and guidance for patients in medical diagnostic scenarios. Understandability is assessed using readability metrics, while empathy is measured through LLM-as-a-Judge ratings compared to human evaluations. The results show that LLMs adapt their explanations based on socio-demographic variables and patient conditions. However, they tend to produce overly complex content and exhibit biased affective empathy, leading to unequal accessibility and support for patients. This highlights the importance of systematic calibration to ensure that LLMs provide equitable communication with patients. The code and data from the study are publicly available on GitHub for further research and validation purposes. <br /><br />Summary: <div>
arXiv:2511.00924v1 Announce Type: new 
Abstract: Large language models (LLMs) show promise for supporting clinicians in diagnostic communication by generating explanations and guidance for patients. Yet their ability to produce outputs that are both understandable and empathetic remains uncertain. We evaluate two leading LLMs on medical diagnostic scenarios, assessing understandability using readability metrics as a proxy and empathy through LLM-as-a-Judge ratings compared to human evaluations. The results indicate that LLMs adapt explanations to socio-demographic variables and patient conditions. However, they also generate overly complex content and display biased affective empathy, leading to uneven accessibility and support. These patterns underscore the need for systematic calibration to ensure equitable patient communication. The code and data are released: https://github.com/Jeffateth/Biased_Oracle
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles</title>
<link>https://arxiv.org/abs/2511.00960</link>
<guid>https://arxiv.org/abs/2511.00960</guid>
<content:encoded><![CDATA[
<div> riddle dataset, language models, reasoning abilities, self-assessment, Indian languages <br />
Summary: 
This study investigates the cultural reasoning and self-assessment capabilities of large language models (LLMs) across seven major Indian languages using a multilingual riddle dataset. Five LLMs were evaluated under various prompting strategies, with Gemini 2.5 Pro showing the best overall performance. However, the few-shot methods only resulted in marginal improvements, and accuracy varied significantly across languages. The self-evaluation experiment revealed an inverse correlation between a model's initial accuracy and its ability to identify its own mistakes. The top-performing models like Gemini 2.5 Pro exhibited overconfidence, while lower-performing models such as LLaMA 4 Scout demonstrated higher self-awareness. These findings emphasize the gaps in multilingual reasoning and underscore the importance of developing models that can reason effectively while recognizing their own limitations. <br /><br />Summary: <div>
arXiv:2511.00960v1 Announce Type: new 
Abstract: The extent to which large language models (LLMs) can perform culturally grounded reasoning across non-English languages remains underexplored. This paper examines the reasoning and self-assessment abilities of LLMs across seven major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and Telugu. We introduce a multilingual riddle dataset combining traditional riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5 Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under seven prompting strategies. In the first stage, we assess riddle-solving performance and find that while Gemini 2.5 Pro performs best overall, few-shot methods yield only marginal gains, and accuracy varies notably across languages. In the second stage, we conduct a self-evaluation experiment to measure reasoning consistency. The results reveal a key finding: a model's initial accuracy is inversely correlated with its ability to identify its own mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34% True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are substantially more self-aware (42.09% True Negative Rate). These results point to clear gaps in multilingual reasoning and highlight the need for models that not only reason effectively but also recognize their own limitations.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective</title>
<link>https://arxiv.org/abs/2511.00988</link>
<guid>https://arxiv.org/abs/2511.00988</guid>
<content:encoded><![CDATA[
<div> Keywords: machine-generated text detection, easy-to-hard enhancement framework, inexact learning, supervisor, detector

Summary:
The article addresses the limitations of existing machine-generated text (MGT) detection methods that rely on "golden standard" labels, which may contain boundary ambiguity. It introduces an easy-to-hard enhancement framework to provide reliable supervision in the presence of inexact learning conditions. This framework utilizes a supervisor targeting simpler longer-text detection tasks to enhance the performance of a more advanced detector. By structurally integrating the detector into the supervisor, the framework optimizes the supervisor as a lower performance bound for the detector, ultimately approximating the true labels. Extensive experiments show the framework's effectiveness in various practical scenarios, including cross-LLM, cross-domain, mixed text, and paraphrase attacks. The code for the framework is available on GitHub at https://github.com/tmlr-group/Easy2Hard. 

<br /><br />Summary: 
- Existing MGT detection methods face challenges due to inexact labels and boundary ambiguity.
- An easy-to-hard enhancement framework is proposed for reliable supervision under inexact learning conditions.
- The framework utilizes a supervisor targeting simple longer-text detection tasks to enhance a more advanced detector.
- By structurally incorporating the detector into the supervisor, the framework optimizes the supervisor as a lower performance bound for the detector.
- Extensive experiments demonstrate the framework's effectiveness across various practical scenarios, confirming its significant detection capabilities. <div>
arXiv:2511.00988v1 Announce Type: new 
Abstract: Existing machine-generated text (MGT) detection methods implicitly assume labels as the "golden standard". However, we reveal boundary ambiguity in MGT detection, implying that traditional training paradigms are inexact. Moreover, limitations of human cognition and the superintelligence of detectors make inexact learning widespread and inevitable. To this end, we propose an easy-to-hard enhancement framework to provide reliable supervision under such inexact conditions. Distinct from knowledge distillation, our framework employs an easy supervisor targeting relatively simple longer-text detection tasks (despite weaker capabilities), to enhance the more challenging target detector. Firstly, longer texts targeted by supervisors theoretically alleviate the impact of inexact labels, laying the foundation for reliable supervision. Secondly, by structurally incorporating the detector into the supervisor, we theoretically model the supervisor as a lower performance bound for the detector. Thus, optimizing the supervisor indirectly optimizes the detector, ultimately approximating the underlying "golden" labels. Extensive experiments across diverse practical scenarios, including cross-LLM, cross-domain, mixed text, and paraphrase attacks, demonstrate the framework's significant detection effectiveness. The code is available at: https://github.com/tmlr-group/Easy2Hard.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL</title>
<link>https://arxiv.org/abs/2511.01008</link>
<guid>https://arxiv.org/abs/2511.01008</guid>
<content:encoded><![CDATA[
<div> framework, multi-agent, interactive reinforcement learning, SQL generation, validation agent
Summary: 
The article introduces MARS-SQL, a multi-agent framework for translating natural language to SQL. The framework consists of three specialized agents: Grounding Agent, Generation Agent, and Validation Agent. The Generation Agent is trained using interactive reinforcement learning, allowing for dynamic reasoning and self-correction through a Think-Act-Observe loop. At inference time, multiple interaction trajectories are explored to find the optimal solution. The Validation Agent selects the best trajectory by modeling verification as a next-token prediction task. The structured workflow combines interactive RL for generation with generative modeling for verification, resulting in robust and accurate SQL generation. Experiments demonstrate that MARS-SQL achieves high Execution Accuracy on benchmark datasets. The code for MARS-SQL is available on GitHub for further exploration and implementation.<br /><br />Summary: <div>
arXiv:2511.01008v1 Announce Type: new 
Abstract: Translating natural language to SQL remains difficult for complex queries. Such queries often need environmental interaction and self-correction. To address this, we introduce MARS-SQL, a novel multi-agent framework that combines principled task decomposition and interactive reinforcement learning (RL). Our system comprises three specialized agents: a Grounding Agent for schema linking, a Generation Agent for query generation, and a Validation Agent for final selection. The core of our framework is the Generation agent, which is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe loop, the agent iteratively generates thoughts, executes SQL actions against a live database, and revises its strategy based on execution feedback, enabling dynamic, stateful reasoning and self-correction. At inference time, we generate multiple interaction trajectories to explore diverse reasoning paths. The Validation agent, then selects the optimal trajectory by modeling verification as a next-token prediction task and choosing the solution with the highest generation probability. This structured workflow pipelines specialized agents. It combines interactive RL for generation with generative modeling for verification. The approach proves highly effective for robust and accurate SQL generation. Experiments show that MARS-SQL achieves state-of-the-art Execution Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our code is available at https://github.com/YangHaolin0526/MARS-SQL.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation</title>
<link>https://arxiv.org/abs/2511.01014</link>
<guid>https://arxiv.org/abs/2511.01014</guid>
<content:encoded><![CDATA[
<div> Instruction following, Large Language Models, IF-CRITIC, constraint following, checklist generator <br />
Summary: <br />
The study introduces IF-CRITIC, a model designed to assess the ability of Large Language Models (LLMs) to follow constraints in instructions efficiently and reliably. It uses a checklist generator to break down instructions into constraints and collects training data through a multi-stage critique filtering mechanism. IF-CRITIC is trained using constraint-level preference optimization. Experiments show that IF-CRITIC outperforms strong LLM-as-a-Judge baselines like Deepseek-R1 and o4-mini in evaluating instruction following. The scalable reward signals provided by IF-CRITIC enable LLMs to optimize instruction following with reduced computational overhead. <div>
arXiv:2511.01014v1 Announce Type: new 
Abstract: Instruction following is a fundamental ability of Large Language Models (LLMs), requiring their generated outputs to follow multiple constraints imposed in input instructions. Numerous studies have attempted to enhance this ability through preference optimization or reinforcement learning based on reward signals from LLM-as-a-Judge. However, existing evaluation models for instruction following still possess many deficiencies, such as substantial costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM critic that can provide efficient and reliable assessments of constraint following in the instructions. We first develop a checklist generator to decompose instructions and generate constraint checklists. With the assistance of the checklists, we collect high-quality critique training data through a multi-stage critique filtering mechanism and employ a constraint-level preference optimization method to train IF-CRITIC. Extensive experiments demonstrate that the evaluation performance of IF-CRITIC can beat strong LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable reward signals provided by IF-CRITIC, LLMs can achieve substantial performance gains in instruction-following optimization under lower computational overhead compared to strong LLM critic baselines.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.01016</link>
<guid>https://arxiv.org/abs/2511.01016</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Prompt-R1, reinforcement learning, multi-turn prompt interaction, reasoning accuracy

Summary:
Prompt-R1 is an end-to-end reinforcement learning framework designed to enhance the performance of large language models (LLMs) by facilitating collaboration between a small-scale LLM and a large-scale LLM. The framework replaces user interaction with a multi-turn prompt interaction process, where the small-scale LLM generates prompts and the large-scale LLM performs complex reasoning. A dual-constrained reward system is employed to optimize for correctness, generation quality, and reasoning accuracy. Prompt-R1 supports both inference and training with various large-scale LLMs and has been shown to significantly outperform baseline models across tasks in experiments using multiple public datasets. The code for Prompt-R1 is available on GitHub for public use. <div>
arXiv:2511.01016v1 Announce Type: new 
Abstract: Recently, advanced large language models (LLMs) have emerged at an increasingly rapid pace. However, when faced with complex problems, most users are often unable to provide accurate and effective prompts to interact with LLMs, thus limiting the performance of LLMs. To address this challenge, we propose Prompt-R1, an end-to-end reinforcement learning framework that uses a small-scale LLM to collaborate with large-scale LLMs, replacing user interaction to solve problems better. This collaboration is cast as a multi-turn prompt interaction, where the small-scale LLM thinks and generates prompts, and the large-scale LLM performs complex reasoning. A dual-constrained reward is designed to optimize for correctness, generation quality, and reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports both inference and training with various large-scale LLMs. Experiments on multiple public datasets show that Prompt-R1 significantly outperforms baseline models across tasks. Our code is publicly available at https://github.com/QwenQKing/Prompt-R1.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights</title>
<link>https://arxiv.org/abs/2511.01019</link>
<guid>https://arxiv.org/abs/2511.01019</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, OceanAI, conversational platform, NOAA data streams, transparency

Summary:<br /><br />Artificial intelligence is revolutionizing the sciences, but conversational AI systems can produce inaccurate "hallucinations." The OceanAI platform combines large language models with real-time access to NOAA oceanographic data, ensuring responses are based on authoritative sources. In a comparison with other AI chat interfaces, only OceanAI provided accurate, NOAA-sourced values with original data references. This platform is designed for flexibility, connecting to various NOAA data products for marine hazard forecasting, ecosystem assessment, and water-quality monitoring. By emphasizing transparency and reproducibility, OceanAI enhances trust in AI-enabled decision-making within the maritime sector. A public demonstration of OceanAI is available at https://oceanai.ai4ocean.xyz. <div>
arXiv:2511.01019v1 Announce Type: new 
Abstract: Artificial intelligence is transforming the sciences, yet general conversational AI systems often generate unverified "hallucinations" undermining scientific rigor. We present OceanAI, a conversational platform that integrates the natural-language fluency of open-source large language models (LLMs) with real-time, parameterized access to authoritative oceanographic data streams hosted by the National Oceanic and Atmospheric Administration (NOAA). Each query such as "What was Boston Harbor's highest water level in 2024?" triggers real-time API calls that identify, parse, and synthesize relevant datasets into reproducible natural-language responses and data visualizations. In a blind comparison with three widely used AI chat-interface products, only OceanAI produced NOAA-sourced values with original data references; others either declined to answer or provided unsupported results. Designed for extensibility, OceanAI connects to multiple NOAA data products and variables, supporting applications in marine hazard forecasting, ecosystem assessment, and water-quality monitoring. By grounding outputs and verifiable observations, OceanAI advances transparency, reproducibility, and trust, offering a scalable framework for AI-enabled decision support within the oceans. A public demonstration is available at https://oceanai.ai4ocean.xyz.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics</title>
<link>https://arxiv.org/abs/2511.01046</link>
<guid>https://arxiv.org/abs/2511.01046</guid>
<content:encoded><![CDATA[
<div> Keywords: air pollution, India, VayuChat, data science, policy programs

Summary: 
VayuChat is a conversational system designed to assist users in obtaining information on air quality, meteorology, and policy programs related to air pollution in India. It provides answers to natural language questions and offers executable Python code and interactive visualizations as responses. By integrating data from various sources such as CPCB monitoring stations, state demographics, and NCAP funding records, VayuChat offers a unified interface powered by large language models. This platform aims to make complex environmental analytics more accessible to policymakers, researchers, and citizens through simple conversations. The live demonstration showcases how users can leverage VayuChat to perform data analysis tasks effectively. The platform is publicly deployed and available for use. <div>
arXiv:2511.01046v1 Announce Type: new 
Abstract: Air pollution causes about 1.6 million premature deaths each year in India, yet decision makers struggle to turn dispersed data into decisions. Existing tools require expertise and provide static dashboards, leaving key policy questions unresolved. We present VayuChat, a conversational system that answers natural language questions on air quality, meteorology, and policy programs, and responds with both executable Python code and interactive visualizations. VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring stations, state-level demographics, and National Clean Air Programme (NCAP) funding records into a unified interface powered by large language models. Our live demonstration will show how users can perform complex environmental analytics through simple conversations, making data science accessible to policymakers, researchers, and citizens. The platform is publicly deployed at https://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further information check out video uploaded on https://www.youtube.com/watch?v=d6rklL05cs4.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs</title>
<link>https://arxiv.org/abs/2511.01053</link>
<guid>https://arxiv.org/abs/2511.01053</guid>
<content:encoded><![CDATA[
<div> Dataset, language models, healthcare, clinical reasoning, guidelines
<br />
Summary: 
This study introduces a validated dataset derived from publicly available guidelines in healthcare for evaluating large language models' (LLMs) clinical reasoning. The dataset, created with GPT, includes realistic patient scenarios and clinical questions. The study benchmarks popular LLMs to demonstrate the dataset's validity and support systematic evaluation of LLMs' clinical utility and adherence to guidelines. <div>
arXiv:2511.01053v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used in healthcare, yet standardised benchmarks for evaluating guideline-based clinical reasoning are missing. This study introduces a validated dataset derived from publicly available guidelines across multiple diagnoses. The dataset was created with the help of GPT and contains realistic patient scenarios, as well as clinical questions. We benchmark a range of recent popular LLMs to showcase the validity of our dataset. The framework supports systematic evaluation of LLMs' clinical utility and guideline adherence.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models</title>
<link>https://arxiv.org/abs/2511.01066</link>
<guid>https://arxiv.org/abs/2511.01066</guid>
<content:encoded><![CDATA[
<div> Keywords: textual datasets, multilingual pre-training data, language model architectures, data quality probes, parallel texts

Summary: 
This article discusses an initiative to provide open, high-quality textual datasets for nearly 200 languages, totaling 30 trillion tokens, making it one of the largest multilingual collections available. The datasets are sourced from web crawls, processed through a comprehensive pipeline, and annotated with various metadata. Data quality is assessed through statistical analysis, manual inspection, and evaluation of language model performance. Benchmarks for multilingual language model evaluation are provided for nine European languages, emphasizing native tasks and reducing prompt sensitivity. Monolingual encoder-decoder and GPT-like models are trained and evaluated, alongside a large collection of automatically mined parallel texts and a synthesized parallel corpus. The article showcases advancements in multilingual natural language processing research and resource availability. 

<br /><br />Summary: <div>
arXiv:2511.01066v1 Announce Type: new 
Abstract: We present an ongoing initiative to provide open, very large, high-quality, and richly annotated textual datasets for almost 200 languages. At 30 trillion tokens, this is likely the largest generally available multilingual collection of LLM pre-training data. At 30 trillion tokens, this is likely the largest generally available multilingual collection of LLM pre-training data. These datasets are derived from web crawls from different sources and accompanied with a complete, open-source pipeline for document selection from web archives, text extraction from HTML, language identification for noisy texts, exact and near-deduplication, annotation with, among others, register labels, text quality estimates, and personally identifiable information; and final selection and filtering. We report on data quality probes through contrastive and analytical statistics, through manual inspection of samples for 24 languages, and through end-to-end evaluation of various language model architectures trained on this data. For multilingual LLM evaluation, we provide a comprehensive collection of benchmarks for nine European languages, with special emphasis on natively created tasks, mechanisms to mitigate prompt sensitivity, and refined normalization and aggregation of scores. Additionally, we train and evaluate a family of 57 monolingual encoder-decoder models, as well as a handful of monolingual GPT-like reference models. Besides the monolingual data and models, we also present a very large collection of parallel texts automatically mined from this data, together with a novel parallel corpus synthesized via machine translation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering</title>
<link>https://arxiv.org/abs/2511.01090</link>
<guid>https://arxiv.org/abs/2511.01090</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Romanian, pretraining corpora, data quality, multitask model 

Summary: 
- Large Language Models (LLMs) have become popular due to their performance on various tasks, requiring high-quality training data.
- Data quality is essential for under-represented languages like Romanian, where quality corpora are limited.
- This study evaluates Romanian pretraining corpora compared to English data and highlights differences in topics and coverage.
- By training a multitask model on annotated Romanian texts, the researchers filter the data based on educational value, topic, and format to create high-quality pretraining datasets.
- Results show improved LLM pretraining performance on multiple benchmarks by filtering the data, indicating the effectiveness of this approach. 

<br /><br />Summary: <div>
arXiv:2511.01090v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently exploded in popularity, often matching or outperforming human abilities on many tasks. One of the key factors in training LLMs is the availability and curation of high-quality data. Data quality is especially crucial for under-represented languages, where high-quality corpora are scarce. In this work we study the characteristics and coverage of Romanian pretraining corpora and we examine how they differ from English data. By training a lightweight multitask model on carefully LLM-annotated Romanian texts, we are able to analyze and perform multi-level filtering (e.g., educational value, topic, format) to generate high-quality pretraining datasets. Our experiments show noteworthy trends in the topics present in Romanian and English data, while also proving the effectiveness of filtering data through improved LLM pretraining performance across multiple benchmarks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TSVer: A Benchmark for Fact Verification Against Time-Series Evidence</title>
<link>https://arxiv.org/abs/2511.01101</link>
<guid>https://arxiv.org/abs/2511.01101</guid>
<content:encoded><![CDATA[
<div> time series, fact verification, temporal reasoning, numerical data, benchmark dataset

Summary:
TSVer is a new benchmark dataset for fact verification focusing on temporal and numerical reasoning with time-series evidence. It contains 287 real-world claims from 38 fact-checking organizations and 400 curated time series. Each claim is annotated with time frames, verdicts, and justifications for the verdicts. Through a multi-step annotation process, high-quality annotations were achieved with an inter-annotator agreement of kappa=0.745 on verdicts. A baseline for verifying claims against time-series evidence was developed, highlighting the challenges even for state-of-the-art reasoning models. The baseline achieved a 63.37 accuracy score on verdicts and an Ev2R score of 48.63 on verdict justifications. This dataset and evaluation demonstrate the importance of temporal and numerical reasoning in fact-checking and provide a valuable resource for future research in this area.<br /><br />Summary: <div>
arXiv:2511.01101v1 Announce Type: new 
Abstract: Reasoning over temporal and numerical data, such as time series, is a crucial aspect of fact-checking. While many systems have recently been developed to handle this form of evidence, their evaluation remains limited by existing datasets, which often lack structured evidence, provide insufficient justifications for verdicts, or rely on synthetic claims. In this paper, we introduce TSVer, a new benchmark dataset for fact verification focusing on temporal and numerical reasoning with time-series evidence. TSVer contains 287 real-world claims sourced from 38 fact-checking organizations and a curated database of 400 time series covering diverse domains. Each claim is annotated with time frames across all pertinent time series, along with a verdict and justifications reflecting how the evidence is used to reach the verdict. Using an LLM-assisted multi-step annotation process, we improve the quality of our annotations and achieve an inter-annotator agreement of kappa=0.745 on verdicts. We also develop a baseline for verifying claims against time-series evidence and show that even the state-of-the-art reasoning models like Gemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score on verdicts and an Ev2R score of 48.63 on verdict justifications.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MicroRemed: Benchmarking LLMs in Microservices Remediation</title>
<link>https://arxiv.org/abs/2511.01166</link>
<guid>https://arxiv.org/abs/2511.01166</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, microservice remediation, autonomous decision-making, Ansible playbooks, ThinkRemed<br />
Summary:<br />
Large Language Models (LLMs) integrated with agent-based reasoning frameworks are being explored for autonomous decision-making in microservice remediation. The study introduces MicroRemed, a benchmark for evaluating LLMs in generating Ansible playbooks for system recovery directly from diagnosis reports. Current approaches rely on human-crafted prompts, but the proposed ThinkRemed framework mimics the reasoning of Site Reliability Engineers (SREs) to improve remediation performance. Experimental results demonstrate the challenges MicroRemed presents to current LLMs and the effectiveness of ThinkRemed in iterative reasoning and system reflection. The benchmark code is available on GitHub for further research and development.<br /> 
Summary: <div>
arXiv:2511.01166v1 Announce Type: new 
Abstract: Large Language Models (LLMs) integrated with agent-based reasoning frameworks have recently shown strong potential for autonomous decision-making and system-level operations. One promising yet underexplored direction is microservice remediation, where the goal is to automatically recover faulty microservice systems. Existing approaches, however, still rely on human-crafted prompts from Site Reliability Engineers (SREs), with LLMs merely converting textual instructions into executable code. To advance research in this area, we introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end microservice remediation, where models must directly generate executable Ansible playbooks from diagnosis reports to restore system functionality. We further propose ThinkRemed, a multi-agent framework that emulates the reflective and perceptive reasoning of SREs. Experimental results show that MicroRemed presents substantial challenges to current LLMs, while ThinkRemed improves end-to-end remediation performance through iterative reasoning and system reflection. The benchmark is available at https://github.com/LLM4AIOps/MicroRemed.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning When to Quit in Sales Conversations</title>
<link>https://arxiv.org/abs/2511.01181</link>
<guid>https://arxiv.org/abs/2511.01181</guid>
<content:encoded><![CDATA[
<div> Keywords: salespeople, optimal stopping, language model, artificial intelligence, salesforce efficiency

Summary: 
Salespeople often face the dilemma of when to persist in a conversation or move on to the next lead. This study focuses on outbound sales, where time is limited, and failure is common. The researchers developed a stopping agent using a language model to determine the optimal time to end a conversation. The agent reduced time spent on failed calls by 54% while maintaining sales, leading to a potential increase in expected sales by up to 37%. By analyzing linguistic cues that drive salespeople's decisions, the study found that they tend to overemphasize certain expressions of disinterest and underestimate call failure risk. This suggests that human decision-making in sales conversations may be limited by cognitive constraints. Overall, the study demonstrates the potential of artificial intelligence algorithms to improve salesforce efficiency by optimizing decision-making in dynamic screening scenarios. 

<br /><br />Summary: <div>
arXiv:2511.01181v1 Announce Type: new 
Abstract: Salespeople frequently face the dynamic screening decision of whether to persist in a conversation or abandon it to pursue the next lead. Yet, little is known about how these decisions are made, whether they are efficient, or how to improve them. We study these decisions in the context of high-volume outbound sales where leads are ample, but time is scarce and failure is common. We formalize the dynamic screening decision as an optimal stopping problem and develop a generative language model-based sequential decision agent - a stopping agent - that learns whether and when to quit conversations by imitating a retrospectively-inferred optimal stopping policy. Our approach handles high-dimensional textual states, scales to large language models, and works with both open-source and proprietary language models. When applied to calls from a large European telecommunications firm, our stopping agent reduces the time spent on failed calls by 54% while preserving nearly all sales; reallocating the time saved increases expected sales by up to 37%. Upon examining the linguistic cues that drive salespeople's quitting decisions, we find that they tend to overweight a few salient expressions of consumer disinterest and mispredict call failure risk, suggesting cognitive bounds on their ability to make real-time conversational decisions. Our findings highlight the potential of artificial intelligence algorithms to correct cognitively-bounded human decisions and improve salesforce efficiency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs</title>
<link>https://arxiv.org/abs/2511.01187</link>
<guid>https://arxiv.org/abs/2511.01187</guid>
<content:encoded><![CDATA[
arXiv:2511.01187v1 Announce Type: new 
Abstract: Large language models (LLMs) are widely deployed for open-ended communication, yet most bias evaluations still rely on English, classification-style tasks. We introduce DebateBias-8K, a new multilingual, debate-style benchmark designed to reveal how narrative bias appears in realistic generative settings. Our dataset includes 8,400 structured debate prompts spanning four sensitive domains: women's rights, socioeconomic development, terrorism, and religion, across seven languages ranging from high-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin). Using four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we generate and automatically classify over 100,000 responses. Results show that all models reproduce entrenched stereotypes despite safety alignment: Arabs are overwhelmingly linked to terrorism and religion (>=95%), Africans to socioeconomic "backwardness" (up to <=77%), and Western groups are consistently framed as modern or progressive. Biases grow sharply in lower-resource languages, revealing that alignment trained primarily in English does not generalize globally. Our findings highlight a persistent divide in multilingual fairness: current alignment methods reduce explicit toxicity but fail to prevent biased outputs in open-ended contexts. We release our DebateBias-8K benchmark and analysis framework to support the next generation of multilingual bias evaluation and safer, culturally inclusive model alignment.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction</title>
<link>https://arxiv.org/abs/2511.01188</link>
<guid>https://arxiv.org/abs/2511.01188</guid>
<content:encoded><![CDATA[
arXiv:2511.01188v1 Announce Type: new 
Abstract: The rapid spread of fake news threatens social stability and public trust, rendering its detection an imperative research priority. Although large language models (LLMs) excel at numerous natural language processing tasks with their remarkable contextual understanding and extensive prior knowledge, the time-bounded knowledge coverage and tendency for generating hallucination content reduce their reliability when handling fast-evolving news streams. Furthermore, models trained on existing static datasets also often lack the generalization needed for emerging news topics. To address these challenges, we propose ZoFia, a novel two-stage zero-shot fake news detection framework. First, we introduce Hierarchical Salience to quantify the importance of entities in the news content, and propose the SC-MMR algorithm to effectively select an informative and diverse set of keywords that serve as queries for retrieving up-to-date external evidence. Subsequently, a multi LLM interactive system, in which each agent assumes a distinct role, performs multi-view collaborative analysis and adversarial debate over the news text and its related information, and finally produces an interpretable and robust judgment. Comprehensive experiments on two public datasets demonstrate that ZoFia obviously outperforms existing zero-shot baselines and most of few-shot methods. Our codes will be open-sourced to facilitate related communities.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.01191</link>
<guid>https://arxiv.org/abs/2511.01191</guid>
<content:encoded><![CDATA[
arXiv:2511.01191v1 Announce Type: new 
Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for adapting models using only synthetic signals at inference, but its success hinges on constructing reliable learning signals. Standard approaches such as majority voting often collapse to spurious yet popular answers. We introduce Self-Harmony, a framework built on a simple intuition: the correct answer should remain stable across both an original question and its paraphrase. Self-Harmony operationalizes this by employing a single model in two complementary roles: a Solver to produce answers and a Reframer to rephrase the input. Based on this, we further propose a pseudo-label method: instead of majority voting, it aggregates answer frequencies across these original and reframed views using the harmonic mean. This is a process that naturally selects for solutions stable under reframing, thereby avoiding the common trap of favoring view-dependent, spurious answers. Crucially, this requires no human supervision or auxiliary models. Across diverse reasoning benchmarks, Self-Harmony achieves state-of-the-art results at the label-free test-time setting, ranking first in 28 of 30 settings across multiple methods. Beyond accuracy, it demonstrates unprecedented robustness, with zero training failures in all experiments, underscoring its stability and reliability.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection</title>
<link>https://arxiv.org/abs/2511.01192</link>
<guid>https://arxiv.org/abs/2511.01192</guid>
<content:encoded><![CDATA[
arXiv:2511.01192v1 Announce Type: new 
Abstract: Detecting machine-generated text (MGT) has emerged as a critical challenge, driven by the rapid advancement of large language models (LLMs) capable of producing highly realistic, human-like content. However, the performance of current approaches often degrades significantly under domain shift. To address this challenge, we propose a novel framework designed to capture both domain-specific and domain-general MGT patterns through a two-stage Disentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a disentangled mixture-of-experts module, in which domain-specific experts learn fine-grained, domain-local distinctions between human and machine-generated text, while shared experts extract transferable, cross-domain features. Second, to mitigate the practical limitation of unavailable domain labels during inference, we design a reinforcement learning-based routing mechanism that dynamically selects the appropriate experts for each input instance, effectively bridging the train-inference gap caused by domain uncertainty. Extensive experiments on five in-domain and five out-of-domain benchmark datasets demonstrate that DEER consistently outperforms state-of-the-art methods, achieving average F1-score improvements of 1.39% and 5.32% on in-domain and out-of-domain datasets respectively, along with accuracy gains of 1.35% and 3.61% respectively. Ablation studies confirm the critical contributions of both disentangled expert specialization and adaptive routing to model performance.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs</title>
<link>https://arxiv.org/abs/2511.01265</link>
<guid>https://arxiv.org/abs/2511.01265</guid>
<content:encoded><![CDATA[
arXiv:2511.01265v1 Announce Type: new 
Abstract: This paper investigates the impact of domain specificity on abstractive summarisation of Arabic financial texts using large language models (LLMs). We introduce AraFinNews, the largest publicly available Arabic financial news dataset to date, comprising 212,500 article--headline pairs spanning nearly a decade of reporting from October 2015 to July 2025. Designed as the Arabic equivalent of major English summarisation corpora such as CNN/DailyMail, AraFinNews provides a robust benchmark for evaluating domain-specific language understanding and generation in financial contexts. Using this resource, we evaluate transformer-based models -- including mT5, AraT5, and the domain-adapted FinAraT5 -- to examine how financial-domain pretraining influences factual accuracy, numerical reliability, and stylistic alignment with professional reporting. Experimental results show that domain-adapted models generate more faithful and coherent summaries, particularly in handling quantitative and entity-centric information. The findings highlight the importance of domain-specific adaptation for improving factual consistency and narrative fluency in Arabic financial summarisation. The dataset is freely available for non-commercial research at https://github.com/ArabicNLP-UK/AraFinNews.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding</title>
<link>https://arxiv.org/abs/2511.01282</link>
<guid>https://arxiv.org/abs/2511.01282</guid>
<content:encoded><![CDATA[
arXiv:2511.01282v1 Announce Type: new 
Abstract: Speculative decoding (SD) has emerged as an effective technique to accelerate large language model (LLM) inference without compromising output quality. However, the achievable speedup largely depends on the effectiveness of the drafting model. While model-based methods like EAGLE-2 are accurate but costly, retrieval-enhanced methods like SAM-Decoding rely on heuristic switching strategies that often trigger unnecessary retrievals. To address this, we propose ReSpec (\textbf{Re}trieval-enhanced \textbf{Spe}culative Decoding), a novel framework that transforms heuristic drafter switching into adaptive decision-making. ReSpec features three core innovations: 1) An \textbf{entropy-guided adaptive trigger} quantifies contextual predictability to initiate retrieval only when uncertainty is low, avoiding costly low-quality speculations. 2) A \textbf{feedback-driven candidate selection} leverages historical feedback to organize multiple high-quality candidates for parallel verification, maximizing retrieval utility. 3) A source-aware \textbf{relaxed verification strategy} applies strict checks to model-generated drafts while using a relaxed verification for retrieved drafts, achieving a better balance between accuracy and efficiency. Extensive experiments on Spec-Bench demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming EAGLE-2 and SAM-Decoding by over $33\%$ and $25\%$, respectively, while maintaining output quality.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"Give a Positive Review Only": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers</title>
<link>https://arxiv.org/abs/2511.01287</link>
<guid>https://arxiv.org/abs/2511.01287</guid>
<content:encoded><![CDATA[
arXiv:2511.01287v1 Announce Type: new 
Abstract: With the rapid advancement of AI models, their deployment across diverse tasks has become increasingly widespread. A notable emerging application is leveraging AI models to assist in reviewing scientific papers. However, recent reports have revealed that some papers contain hidden, injected prompts designed to manipulate AI reviewers into providing overly favorable evaluations. In this work, we present an early systematic investigation into this emerging threat. We propose two classes of attacks: (1) static attack, which employs a fixed injection prompt, and (2) iterative attack, which optimizes the injection prompt against a simulated reviewer model to maximize its effectiveness. Both attacks achieve striking performance, frequently inducing full evaluation scores when targeting frontier AI reviewers. Furthermore, we show that these attacks are robust across various settings. To counter this threat, we explore a simple detection-based defense. While it substantially reduces the attack success rate, we demonstrate that an adaptive attacker can partially circumvent this defense. Our findings underscore the need for greater attention and rigorous safeguards against prompt-injection threats in AI-assisted peer review.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings</title>
<link>https://arxiv.org/abs/2511.01289</link>
<guid>https://arxiv.org/abs/2511.01289</guid>
<content:encoded><![CDATA[
arXiv:2511.01289v1 Announce Type: new 
Abstract: In emergency situations, every second counts. The deployment of Large Language Models (LLMs) in time-sensitive, low or zero-connectivity environments remains limited. Current models are computationally intensive and unsuitable for low-tier devices often used by first responders or civilians. A major barrier to developing lightweight, domain-specific solutions is the lack of high-quality datasets tailored to first aid and emergency response. To address this gap, we introduce FirstAidQA, a synthetic dataset containing 5,500 high-quality question answer pairs that encompass a wide range of first aid and emergency response scenarios. The dataset was generated using a Large Language Model, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from the Vital First Aid Book (2019). We applied preprocessing steps such as text cleaning, contextual chunking, and filtering, followed by human validation to ensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is designed to support instruction-tuning and fine-tuning of LLMs and Small Language Models (SLMs), enabling faster, more reliable, and offline-capable systems for emergency settings. We publicly release the dataset to advance research on safety-critical and resource-constrained AI applications in first aid and emergency response. The dataset is available on Hugging Face at https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepSpecs: Expert-Level Questions Answering in 5G</title>
<link>https://arxiv.org/abs/2511.01305</link>
<guid>https://arxiv.org/abs/2511.01305</guid>
<content:encoded><![CDATA[
arXiv:2511.01305v1 Announce Type: new 
Abstract: 5G technology enables mobile Internet access for billions of users. Answering expert-level questions about 5G specifications requires navigating thousands of pages of cross-referenced standards that evolve across releases. Existing retrieval-augmented generation (RAG) frameworks, including telecom-specific approaches, rely on semantic similarity and cannot reliably resolve cross-references or reason about specification evolution. We present DeepSpecs, a RAG system enhanced by structural and temporal reasoning via three metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB (line-level version diffs), and TDocDB (standardization meeting documents). DeepSpecs explicitly resolves cross-references by recursively retrieving referenced clauses through metadata lookup, and traces specification evolution by mining changes and linking them to Change Requests that document design rationale. We curate two 5G QA datasets: 573 expert-annotated real-world questions from practitioner forums and educational resources, and 350 evolution-focused questions derived from approved Change Requests. Across multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art telecom RAG systems; ablations confirm that explicit cross-reference resolution and evolution-aware retrieval substantially improve answer quality, underscoring the value of modeling the structural and temporal properties of 5G standards.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness</title>
<link>https://arxiv.org/abs/2511.01323</link>
<guid>https://arxiv.org/abs/2511.01323</guid>
<content:encoded><![CDATA[
arXiv:2511.01323v1 Announce Type: new 
Abstract: Large language models (LLMs) with integrated search tools show strong promise in open-domain question answering (QA), yet they often struggle to produce complete answer set to complex questions such as Which actor from the film Heat won at least one Academy Award?, which requires (1) distinguishing between multiple films sharing the same title and (2) reasoning across a large set of actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate both challenges jointly. To address this, we introduce DeepAmbigQAGen, an automatic data generation pipeline that constructs QA tasks grounded in text corpora and linked knowledge graph, generating natural and verifiable questions that systematically embed name ambiguity and multi-step reasoning. Based on this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop reasoning and half of them explicit name ambiguity resolving. Experiments reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous questions. These findings highlight the need for more robust QA systems aimed at information gathering and answer completeness.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series</title>
<link>https://arxiv.org/abs/2511.01354</link>
<guid>https://arxiv.org/abs/2511.01354</guid>
<content:encoded><![CDATA[
arXiv:2511.01354v1 Announce Type: new 
Abstract: Recently, the demand for small and efficient reasoning models to support real-world applications has driven the development of knowledge distillation techniques that balance reasoning performance and inference speed. In this paper, we further extend the DistilQwen model family, initialized from the Qwen models, by introducing four model series specifically designed to meet industrial requirements. The distilled model collection comprises: (1) slow-thinking models, optimized for reasoning tasks that require high accuracy; (2) two series of adaptive-thinking models, which dynamically adjust reasoning strategies based on input tasks to maximize efficiency across diverse scenarios; and (3) distilled reward models, which enable further reinforcement learning of reasoning models using distilled knowledge. Comprehensive evaluations across multiple benchmarks demonstrate both high inference efficiency and strong reasoning performance for these models, as well as the practical utility of distilled reward models. We further show that these models support industry practitioners by providing scalable training and inference functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence) platform.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise</title>
<link>https://arxiv.org/abs/2511.01359</link>
<guid>https://arxiv.org/abs/2511.01359</guid>
<content:encoded><![CDATA[
arXiv:2511.01359v1 Announce Type: new 
Abstract: Natural Language Inference (NLI) models have been used in various ways to improve the factuality of LLM outputs. This is typically done by applying an NLI model to judge whether the model output is entailed from the supposed evidence, triggering some corrective actions, such as beam reranking at inference time or RL rewards during training. While NLI models are trained to detect factual inconsistencies over complete sentences, decisions in the common autoregressive generation architecture are made for each evolving text prefix, during decoding. Addressing this setting, we generalize the entailment detection task to apply over arbitrary text prefixes, and suggest its utility for improving generation faithfulness. Providing suitable evaluation and training datasets for this task, we train MiniTruePrefixes, a novel specialized model that better detects factual inconsistencies over text prefixes, outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level entailment. We further demonstrate that integrating MiniTruePrefixes into a controlled decoding framework substantially improves factual consistency in abstractive summarization. When guided by MiniTruePrefixes, LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from the same model family, while using only half the memory.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safer in Translation? Presupposition Robustness in Indic Languages</title>
<link>https://arxiv.org/abs/2511.01360</link>
<guid>https://arxiv.org/abs/2511.01360</guid>
<content:encoded><![CDATA[
arXiv:2511.01360v1 Announce Type: new 
Abstract: Increasingly, more and more people are turning to large language models (LLMs) for healthcare advice and consultation, making it important to gauge the efficacy and accuracy of the responses of LLMs to such queries. While there are pre-existing medical benchmarks literature which seeks to accomplish this very task, these benchmarks are almost universally in English, which has led to a notable gap in existing literature pertaining to multilingual LLM evaluation. Within this work, we seek to aid in addressing this gap with Cancer-Myth-Indic, an Indic language benchmark built by translating a 500-item subset of Cancer-Myth, sampled evenly across its original categories, into five under-served but widely used languages from the subcontinent (500 per language; 2,500 translated items total). Native-speaker translators followed a style guide for preserving implicit presuppositions in translation; items feature false presuppositions relating to cancer. We evaluate several popular LLMs under this presupposition stress.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation</title>
<link>https://arxiv.org/abs/2511.01365</link>
<guid>https://arxiv.org/abs/2511.01365</guid>
<content:encoded><![CDATA[
arXiv:2511.01365v1 Announce Type: new 
Abstract: The rapid rise of Large Language Models (LLMs) and Large Reasoning Models (LRMs) has been accompanied by an equally rapid increase of benchmarks used to assess them. However, due to both improved model competence resulting from scaling and novel training advances as well as likely many of these datasets being included in pre or post training data, results become saturated, driving a continuous need for new and more challenging replacements. In this paper, we discuss whether surpassing a benchmark truly demonstrates reasoning ability or are we simply tracking numbers divorced from the capabilities we claim to measure? We present an investigation focused on three model families, OpenAI, Anthropic, and Google, and how their reasoning capabilities across different benchmarks evolve over the years. We also analyze performance trends over the years across different reasoning tasks and discuss the current situation of benchmarking and remaining challenges. By offering a comprehensive overview of benchmarks and reasoning tasks, our work aims to serve as a first reference to ground future research in reasoning evaluation and model development.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confounding Factors in Relating Model Performance to Morphology</title>
<link>https://arxiv.org/abs/2511.01380</link>
<guid>https://arxiv.org/abs/2511.01380</guid>
<content:encoded><![CDATA[
arXiv:2511.01380v1 Announce Type: new 
Abstract: The extent to which individual language characteristics influence tokenization and language modeling is an open question. Differences in morphological systems have been suggested as both unimportant and crucial to consider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter alia). We argue this conflicting evidence is due to confounding factors in experimental setups, making it hard to compare results and draw conclusions. We identify confounding factors in analyses trying to answer the question of whether, and how, morphology relates to language modeling. Next, we re-assess three hypotheses by Arnett & Bergen (2025) for why modeling agglutinative languages results in higher perplexities than fusional languages: they look at morphological alignment of tokenization, tokenization efficiency, and dataset size. We show that each conclusion includes confounding factors. Finally, we introduce token bigram metrics as an intrinsic way to predict the difficulty of causal language modeling, and find that they are gradient proxies for morphological complexity that do not require expert annotation. Ultimately, we outline necessities to reliably answer whether, and how, morphology relates to language modeling.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets</title>
<link>https://arxiv.org/abs/2511.01386</link>
<guid>https://arxiv.org/abs/2511.01386</guid>
<content:encoded><![CDATA[
arXiv:2511.01386v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) quality depends on many interacting choices across retrieval, ranking, augmentation, prompting, and generation, so optimizing modules in isolation is brittle. We introduce RAGSmith, a modular framework that treats RAG design as an end-to-end architecture search over nine technique families and 46{,}080 feasible pipeline configurations. A genetic search optimizes a scalar objective that jointly aggregates retrieval metrics (recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law, Finance, Medicine, Defense Industry, Computer Science), each with 100 questions spanning factual, interpretation, and long-answer types. RAGSmith finds configurations that consistently outperform naive RAG baseline by +3.8\% on average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in retrieval and +7.5\% in generation. The search typically explores $\approx 0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone -- vector retrieval plus post-generation reflection/revision -- augmented by domain-dependent choices in expansion, reranking, augmentation, and prompt reordering; passage compression is never selected. Improvement magnitude correlates with question type, with larger gains on factual/long-answer mixes than interpretation-heavy sets. These results provide practical, domain-aware guidance for assembling effective RAG systems and demonstrate the utility of evolutionary search for full-pipeline optimization.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge</title>
<link>https://arxiv.org/abs/2511.01409</link>
<guid>https://arxiv.org/abs/2511.01409</guid>
<content:encoded><![CDATA[
arXiv:2511.01409v1 Announce Type: new 
Abstract: Evaluating large language models (LLMs) on question answering often relies on static benchmarks that reward memorization and understate the role of retrieval, failing to capture the dynamic nature of world knowledge. We present LiveSearchBench, an automated pipeline for constructing retrieval-dependent benchmarks from recent knowledge updates. Our method computes deltas between successive Wikidata snapshots, filters candidate triples for quality, and synthesizes natural-language questions at three levels of reasoning difficulty, each guaranteed to admit a unique, verifiable answer through SPARQL validation. The pipeline is fully automated, scalable across time, and minimizes human intervention, enabling continual regeneration of temporally grounded benchmarks. Experiments show a pronounced performance drop when models confront facts that post-date pretraining, with the gap most salient on multi-hop queries. Retrieval augmented methods and larger, instruction-tuned models provide partial gains but fail to close this recency gap. By design, LiveSearchBench shifts evaluation from static memorization toward tasks that require up-to-date retrieval and reasoning, offering a foundation for systematic, long-term assessment of LLMs under evolving knowledge.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG</title>
<link>https://arxiv.org/abs/2511.01454</link>
<guid>https://arxiv.org/abs/2511.01454</guid>
<content:encoded><![CDATA[
arXiv:2511.01454v1 Announce Type: new 
Abstract: Translating a morphology-rich, low-resource language like Latin poses significant challenges. This paper introduces a reproducible draft-based refinement pipeline that elevates open-source Large Language Models (LLMs) to a performance level statistically comparable to top-tier proprietary systems. Our method first uses a fine-tuned NLLB-1.3B model to generate a high-quality, structurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes this draft, a process that can be further enhanced by augmenting the context with retrieved out-context examples (RAG). We demonstrate the robustness of this approach on two distinct benchmarks: a standard in-domain test set (Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of 12th-century Latin letters (2025). Our central finding is that this open-source RAG system achieves performance statistically comparable to the GPT-5 baseline, without any task-specific LLM fine-tuning. We release the pipeline, the Chartres OOD set, and evaluation scripts and models to facilitate replicability and further research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BARD: budget-aware reasoning distillation</title>
<link>https://arxiv.org/abs/2511.01470</link>
<guid>https://arxiv.org/abs/2511.01470</guid>
<content:encoded><![CDATA[
arXiv:2511.01470v1 Announce Type: new 
Abstract: While long Chain-of-Thought (CoT) distillation effectively transfers reasoning capability to smaller language models, the reasoning process often remains redundant and computational budget uncontrollable, leading to inefficient resource usage. To address this limitation, we propose \textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that simultaneously distills reasoning capability and enables fine-grained control over the reasoning length. BARD uses the thinking budget as a user-specified control signal, allowing the model to dynamically balance reasoning performance and computational efficiency. To achieve this concept, BARD introduces a two-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on teacher-generated long CoT data compressed to various budget levels, bootstrapping the model's understanding of budget constraints. The second phase leverages Reinforcement Learning (RL) from a reward signal in consideration of reasoning performance and budget fidelity simultaneously. Incorporating the two-phase regimen is crucial to avoiding policy degradation and ensuring that both objectives are optimized jointly. Extensive experiments demonstrate that our method empowers an 8B student model to achieve strong performance on challenging reasoning benchmarks (\textit{AIME24, AIME25, GPQA}) while providing precise and adaptive control over its reasoning length across a wide range of budgets.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation</title>
<link>https://arxiv.org/abs/2511.01482</link>
<guid>https://arxiv.org/abs/2511.01482</guid>
<content:encoded><![CDATA[
arXiv:2511.01482v1 Announce Type: new 
Abstract: Text-based automated Cognitive Distortion detection is a challenging task due to its subjective nature, with low agreement scores observed even among expert human annotators, leading to unreliable annotations. We explore the use of Large Language Models (LLMs) as consistent and reliable annotators, and propose that multiple independent LLM runs can reveal stable labeling patterns despite the inherent subjectivity of the task. Furthermore, to fairly compare models trained on datasets with different characteristics, we introduce a dataset-agnostic evaluation framework using Cohen's kappa as an effect size measure. This methodology allows for fair cross-dataset and cross-study comparisons where traditional metrics like F1 score fall short. Our results show that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78), resulting in improved test set performance for models trained on these annotations compared to those trained on human-labeled data. Our findings suggest that LLMs can offer a scalable and internally consistent alternative for generating training data that supports strong downstream performance in subjective NLP tasks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.01490</link>
<guid>https://arxiv.org/abs/2511.01490</guid>
<content:encoded><![CDATA[
arXiv:2511.01490v1 Announce Type: new 
Abstract: As synthetic data becomes widely used in language model development, understanding its impact on model behavior is crucial. This paper investigates the impact of the diversity of sources of synthetic data on fine-tuned large language models. We focus on three key dimensions: distribution collapse, adversarial robustness, and self-preference bias. Our findings reveal that fine-tuning models on synthetic data from diverse sources can mitigate distribution collapse, preserving the breadth of the output distribution and the diversity of the output text. Furthermore, while both human and synthetic fine-tuning data can remove safeguards, the latter preserves higher output quality, thus making outputs potentially more usable and dangerous. Finally, fine-tuning reduces self-preference bias, with human data being the most effective, followed by multi-source synthetic data.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification</title>
<link>https://arxiv.org/abs/2511.01512</link>
<guid>https://arxiv.org/abs/2511.01512</guid>
<content:encoded><![CDATA[
arXiv:2511.01512v1 Announce Type: new 
Abstract: Toxic language in Bengali remains prevalent, especially in online environments, with few effective precautions against it. Although text detoxification has seen progress in high-resource languages, Bengali remains underexplored due to limited resources. In this paper, we propose a novel pipeline for Bengali text detoxification that combines Pareto class-optimized large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate detoxified sentences. To support this effort, we construct BanglaNirTox, an artificially generated parallel corpus of 68,041 toxic Bengali sentences with class-wise toxicity labels, reasonings, and detoxified paraphrases, using Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox dataset is used to fine-tune language models to produce better detoxified versions of Bengali sentences. Our findings show that Pareto-optimized LLMs with CoT prompting significantly enhance the quality and consistency of Bengali text detoxification.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Difficulty-Controllable Cloze Question Distractor Generation</title>
<link>https://arxiv.org/abs/2511.01526</link>
<guid>https://arxiv.org/abs/2511.01526</guid>
<content:encoded><![CDATA[
arXiv:2511.01526v1 Announce Type: new 
Abstract: Multiple-choice cloze questions are commonly used to assess linguistic proficiency and comprehension. However, generating high-quality distractors remains challenging, as existing methods often lack adaptability and control over difficulty levels, and the absence of difficulty-annotated datasets further hinders progress. To address these issues, we propose a novel framework for generating distractors with controllable difficulty by leveraging both data augmentation and a multitask learning strategy. First, to create a high-quality, difficulty-annotated dataset, we introduce a two-way distractor generation process in order to produce diverse and plausible distractors. These candidates are subsequently refined through filtering and then categorized by difficulty using an ensemble QA system. Second, this newly created dataset is leveraged to train a difficulty-controllable generation model via multitask learning. The framework includes carefully designed auxiliary tasks that enhance the model's semantic understanding of distractors and its ability to estimate their difficulty. Experimental results demonstrate that our method generates high-quality distractors across difficulty levels and substantially outperforms GPT-4o in aligning distractor difficulty with human perception.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o</title>
<link>https://arxiv.org/abs/2511.01558</link>
<guid>https://arxiv.org/abs/2511.01558</guid>
<content:encoded><![CDATA[
arXiv:2511.01558v1 Announce Type: new 
Abstract: Math anxiety poses significant challenges for university psychology students, affecting their career choices and overall well-being. This study employs a framework based on behavioural forma mentis networks (i.e. cognitive models that map how individuals structure their associative knowledge and emotional perceptions of concepts) to explore individual and group differences in the perception and association of concepts related to math and anxiety. We conducted 4 experiments involving psychology undergraduates from 2 samples (n1 = 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300; GPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network features to predict psychometric scores for math anxiety and its facets (observational, social and evaluational) from the Math Anxiety Scale. Experiment 4 focuses on group-level perceptions extracted from human students, GPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive valence ratings and higher network degree for "anxiety", together with negative ratings for "math", can predict higher total and evaluative math anxiety. In contrast, these models do not work on GPT-based data because of differences in simulated networks and psychometric scores compared to humans. These results were also reconciled with differences found in the ways that high/low subgroups of simulated and real students framed semantically and emotionally STEM concepts. High math-anxiety students collectively framed "anxiety" in an emotionally polarising way, absent in the negative perception of low math-anxiety students. "Science" was rated positively, but contrasted against the negative perception of "math". These findings underscore the importance of understanding concept perception and associations in managing students' math anxiety.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation</title>
<link>https://arxiv.org/abs/2511.01568</link>
<guid>https://arxiv.org/abs/2511.01568</guid>
<content:encoded><![CDATA[
arXiv:2511.01568v1 Announce Type: new 
Abstract: Controllable Dialogue Generation (CDG) enables chatbots to generate responses with desired attributes, and weighted decoding methods have achieved significant success in the CDG task. However, using a fixed constant value to manage the bias of attribute probabilities makes it challenging to find an ideal control strength that satisfies both controllability and fluency. To address this issue, we propose ECO decoding (Entropy-based COntrol), which dynamically adjusts the control strength at each generation step according to the model's entropy in both the language model and attribute classifier probability distributions. Experiments on the DailyDialog and MultiWOZ datasets demonstrate that ECO decoding consistently improves controllability while maintaining fluency and grammaticality, outperforming prior decoding methods across various models and settings. Furthermore, ECO decoding alleviates probability interpolation issues in multi-attribute generation and consequently demonstrates strong performance in both single and multi-attribute scenarios.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BIRD: Bronze Inscription Restoration and Dating</title>
<link>https://arxiv.org/abs/2511.01589</link>
<guid>https://arxiv.org/abs/2511.01589</guid>
<content:encoded><![CDATA[
arXiv:2511.01589v1 Announce Type: new 
Abstract: Bronze inscriptions from early China are fragmentary and difficult to date. We introduce BIRD(Bronze Inscription Restoration and Dating), a fully encoded dataset grounded in standard scholarly transcriptions and chronological labels. We further propose an allograph-aware masked language modeling framework that integrates domain- and task-adaptive pretraining with a Glyph Net (GN), which links graphemes and allographs. Experiments show that GN improves restoration, while glyph-biased sampling yields gains in dating.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers</title>
<link>https://arxiv.org/abs/2511.01615</link>
<guid>https://arxiv.org/abs/2511.01615</guid>
<content:encoded><![CDATA[
arXiv:2511.01615v1 Announce Type: new 
Abstract: Linguistic errors are not merely deviations from normative grammar; they offer a unique window into the cognitive architecture of language and expose the current limitations of artificial systems that seek to replicate them. This project proposes an interdisciplinary study of linguistic errors produced by native Spanish speakers, with the aim of analyzing how current large language models (LLM) interpret, reproduce, or correct them. The research integrates three core perspectives: theoretical linguistics, to classify and understand the nature of the errors; neurolinguistics, to contextualize them within real-time language processing in the brain; and natural language processing (NLP), to evaluate their interpretation against linguistic errors. A purpose-built corpus of authentic errors of native Spanish (+500) will serve as the foundation for empirical analysis. These errors will be tested against AI models such as GPT or Gemini to assess their interpretative accuracy and their ability to generalize patterns of human linguistic behavior. The project contributes not only to the understanding of Spanish as a native language but also to the development of NLP systems that are more cognitively informed and capable of engaging with the imperfect, variable, and often ambiguous nature of real human language.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParlaSpeech 3.0: Richly Annotated Spoken Parliamentary Corpora of Croatian, Czech, Polish, and Serbian</title>
<link>https://arxiv.org/abs/2511.01619</link>
<guid>https://arxiv.org/abs/2511.01619</guid>
<content:encoded><![CDATA[
arXiv:2511.01619v1 Announce Type: new 
Abstract: ParlaSpeech is a collection of spoken parliamentary corpora currently spanning four Slavic languages - Croatian, Czech, Polish and Serbian - all together 6 thousand hours in size. The corpora were built in an automatic fashion from the ParlaMint transcripts and their corresponding metadata, which were aligned to the speech recordings of each corresponding parliament. In this release of the dataset, each of the corpora is significantly enriched with various automatic annotation layers. The textual modality of all four corpora has been enriched with linguistic annotations and sentiment predictions. Similar to that, their spoken modality has been automatically enriched with occurrences of filled pauses, the most frequent disfluency in typical speech. Two out of the four languages have been additionally enriched with detailed word- and grapheme-level alignments, and the automatic annotation of the position of primary stress in multisyllabic words. With these enrichments, the usefulness of the underlying corpora has been drastically increased for downstream research across multiple disciplines, which we showcase through an analysis of acoustic correlates of sentiment. All the corpora are made available for download in JSONL and TextGrid formats, as well as for search through a concordancer.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Graph-based RAG for Energy Efficiency Question Answering</title>
<link>https://arxiv.org/abs/2511.01643</link>
<guid>https://arxiv.org/abs/2511.01643</guid>
<content:encoded><![CDATA[
arXiv:2511.01643v1 Announce Type: new 
Abstract: In this work, we investigate the use of Large Language Models (LLMs) within a graph-based Retrieval Augmented Generation (RAG) architecture for Energy Efficiency (EE) Question Answering. First, the system automatically extracts a Knowledge Graph (KG) from guidance and regulatory documents in the energy field. Then, the generated graph is navigated and reasoned upon to provide users with accurate answers in multiple languages. We implement a human-based validation using the RAGAs framework properties, a validation dataset comprising 101 question-answer pairs, and domain experts. Results confirm the potential of this architecture and identify its strengths and weaknesses. Validation results show how the system correctly answers in about three out of four of the cases (75.2 +- 2.7%), with higher results on questions related to more general EE answers (up to 81.0 +- 4.1%), and featuring promising multilingual abilities (4.4% accuracy loss due to translation).
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2511.01649</link>
<guid>https://arxiv.org/abs/2511.01649</guid>
<content:encoded><![CDATA[
arXiv:2511.01649v1 Announce Type: new 
Abstract: This study proposes a cognitive benchmarking framework to evaluate how large language models (LLMs) process and apply culturally specific knowledge. The framework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG) to assess model performance across six hierarchical cognitive domains: Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating. Using a curated Taiwanese Hakka digital cultural archive as the primary testbed, the evaluation measures LLM-generated responses' semantic accuracy and cultural relevance.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering</title>
<link>https://arxiv.org/abs/2511.01650</link>
<guid>https://arxiv.org/abs/2511.01650</guid>
<content:encoded><![CDATA[
arXiv:2511.01650v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly being applied to specialized, high-stakes domains like engineering, which demands rigorous evaluation of their complex reasoning capabilities. While current benchmarks assess language understanding, factual recall, mathematics or code generation, none capture the integrative reasoning central to engineering where scientific principles, quantitative modeling and practical constraints must converge. To address this gap, we introduce EngChain, a benchmark for verifiable multi-step engineering problem-solving. EngChain contains 90 problems spanning three engineering branches, organized into 9 domains and 20 distinct areas. The problems are generated from symbolic templates with a high degree of randomization to ensure diversity and eliminate the risk of contamination. With this benchmark, we move beyond final answer accuracy with a two-stage evaluation: we first quantitatively verify the numerical and semantic validity of each reasoning step and then introduce LLM-As-A-Judge, an automated system to qualitatively categorize the identified reasoning errors.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia</title>
<link>https://arxiv.org/abs/2511.01670</link>
<guid>https://arxiv.org/abs/2511.01670</guid>
<content:encoded><![CDATA[
arXiv:2511.01670v1 Announce Type: new 
Abstract: We introduce SeaLLMs-Audio, the first large audio-language model (LALM) tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai (th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across diverse audio-centric tasks, spanning fine-grained audio understanding and voice-based interaction. Its key features include: 1) Multilingual: the model primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English, and Chinese; 2) Multimodal: the model accepts flexible input modalities, including audio only, text only, as well as audio with text; 3) Multi-task: the model supports a wide range of tasks, including audio analysis tasks such as Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation, Speech Emotion Recognition, Speech Question Answering, and Speech Summarization. It also enables voice-based dialogue, including answering factual, mathematical, and general knowledge queries. As a significant step towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to benefit both the regional research community and industry. To automate LALM evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves competitive performance compared with other LALMs on SEA languages.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI</title>
<link>https://arxiv.org/abs/2511.01689</link>
<guid>https://arxiv.org/abs/2511.01689</guid>
<content:encoded><![CDATA[
arXiv:2511.01689v1 Announce Type: new 
Abstract: The character of the "AI assistant" persona generated by modern chatbot large language models influences both surface-level behavior and apparent values, beliefs, and ethics. These all affect interaction quality, perceived intelligence, and alignment with both developer and user intentions. The shaping of this persona, known as character training, is a critical component of industry post-training, yet remains effectively unstudied in the academic literature. We introduce the first open implementation of character training, leveraging Constitutional AI and a new data pipeline using synthetic introspective data to shape the assistant persona in a more effective and controlled manner than alternatives such as constraining system prompts or activation steering. Specifically, we fine-tune three popular open-weights models using 11 example personas, such as humorous, deeply caring, or even malevolent. To track the effects of our approach, we introduce a method which analyzes revealed preferences, uncovering clear and holistic changes in character. We find these changes are more robust to adversarial prompting than the above two alternatives, while also leading to more coherent and realistic generations. Finally, we demonstrate this fine-tuning has little to no effect on general capabilities as measured by common benchmarks. We describe and open-source our full post-training method, the implementation of which can be found at https://github.com/maiush/OpenCharacterTraining.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement</title>
<link>https://arxiv.org/abs/2511.01706</link>
<guid>https://arxiv.org/abs/2511.01706</guid>
<content:encoded><![CDATA[
arXiv:2511.01706v1 Announce Type: new 
Abstract: Natural Language Explanations (NLEs) describe how Large Language Models (LLMs) make decisions, drawing on both external Context Knowledge (CK) and Parametric Knowledge (PK) stored in model weights. Understanding their interaction is key to assessing the grounding of NLEs, yet it remains underexplored. Prior work has largely examined only single-step generation, typically the final answer, and has modelled PK and CK interaction only as a binary choice in a rank-1 subspace. This overlooks richer forms of interaction, such as complementary or supportive knowledge. We propose a novel rank-2 projection subspace that disentangles PK and CK contributions more accurately and use it for the first multi-step analysis of knowledge interactions across longer NLE sequences. Experiments on four QA datasets and three open-weight instruction-tuned LLMs show that diverse knowledge interactions are poorly represented in a rank-1 subspace but are effectively captured in our rank-2 formulation. Our multi-step analysis reveals that hallucinated NLEs align strongly with the PK direction, context-faithful ones balance PK and CK, and Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing PK reliance. This work provides the first framework for systematic studies of multi-step knowledge interactions in LLMs through a richer rank-2 subspace disentanglement. Code and data: https://github.com/copenlu/pk-ck-knowledge-disentanglement.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue</title>
<link>https://arxiv.org/abs/2511.01720</link>
<guid>https://arxiv.org/abs/2511.01720</guid>
<content:encoded><![CDATA[
arXiv:2511.01720v1 Announce Type: new 
Abstract: We present a multi-expert system for creating Non-Player Characters (NPCs) capable of both natural dialogue and contextual action execution in interactive environments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA) adapters, we instantiate three specialists: tool calling, tool-response interpretation, and direct dialogue. Our system comfortably meets the computational efficiency requirements, delivering fast responses and maintaining modest resource usage on L40S GPUs. In the Commonsense Persona-Grounded Dialogue Challenge 2025, our method ranked second overall.
  Code available at: https://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accumulating Context Changes the Beliefs of Language Models</title>
<link>https://arxiv.org/abs/2511.01805</link>
<guid>https://arxiv.org/abs/2511.01805</guid>
<content:encoded><![CDATA[
arXiv:2511.01805v1 Announce Type: new 
Abstract: Language model (LM) assistants are increasingly used in applications such as brainstorming and research. Improvements in memory and context size have allowed these models to become more autonomous, which has also resulted in more text accumulation in their context windows without explicit user intervention. This comes with a latent risk: the belief profiles of models -- their understanding of the world as manifested in their responses or actions -- may silently change as context accumulates. This can lead to subtly inconsistent user experiences, or shifts in behavior that deviate from the original alignment of the models. In this paper, we explore how accumulating context by engaging in interactions and processing text -- talking and reading -- can change the beliefs of language models, as manifested in their responses and behaviors.Our results reveal that models' belief profiles are highly malleable: GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of discussion about moral dilemmas and queries about safety, while Grok 4 shows a 27.2% shift on political issues after reading texts from the opposing position. We also examine models' behavioral changes by designing tasks that require tool use, where each tool selection corresponds to an implicit belief. We find that these changes align with stated belief shifts, suggesting that belief shifts will be reflected in actual behavior in agentic systems. Our analysis exposes the hidden risk of belief shift as models undergo extended sessions of talking or reading, rendering their opinions and actions unreliable.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining</title>
<link>https://arxiv.org/abs/2511.01807</link>
<guid>https://arxiv.org/abs/2511.01807</guid>
<content:encoded><![CDATA[
arXiv:2511.01807v1 Announce Type: new 
Abstract: Length control in Large Language Models (LLMs) is a crucial but under-addressed challenge, with applications ranging from voice interfaces requiring concise responses to research summaries needing comprehensive outputs. Current approaches to length control, including Regularized DPO, Length-Instruction Fine Tuning, and tool-augmented methods, typically require expensive model retraining or complex inference-time tooling. This paper presents a prompt engineering methodology that enables precise length control without model retraining. Our structure-guided approach implements deliberate planning and word counting mechanisms within the prompt, encouraging the model to carefully track and adhere to specified length constraints. Comprehensive evaluations across six state-of-the-art LLMs demonstrate that our method significantly improves length fidelity for several models compared to standard prompting when applied to document summarization tasks, particularly for shorter-to-medium length constraints. The proposed technique shows varying benefits across different model architectures, with some models demonstrating up to 37.6% improvement in length adherence. Quality evaluations further reveal that our approach maintains or enhances overall output quality compared to standard prompting techniques. Our approach provides an immediately deployable solution for applications requiring precise length control, particularly valuable for production environments where model retraining is impractical or cost-prohibitive.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KV Cache Transform Coding for Compact Storage in LLM Inference</title>
<link>https://arxiv.org/abs/2511.01815</link>
<guid>https://arxiv.org/abs/2511.01815</guid>
<content:encoded><![CDATA[
arXiv:2511.01815v1 Announce Type: new 
Abstract: Serving large language models (LLMs) at scale necessitates efficient key-value (KV) cache management. KV caches can be reused across conversation turns via shared-prefix prompts that are common in iterative code editing and chat. However, stale caches consume scarce GPU memory, require offloading, or force recomputation. We present KVTC, a lightweight transform coder that compresses KV caches for compact on-GPU and off-GPU storage. Drawing on classical media compression, KVTC combines PCA-based feature decorrelation, adaptive quantization, and entropy coding. It requires only a brief initial calibration and leaves model parameters unchanged. By exploiting redundancies in KV caches, KVTC achieves up to 20$\times$ compression while maintaining reasoning and long-context accuracy, and 40$\times$ or higher for specific use cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and MATH-500. It consistently outperforms inference-time baselines such as token eviction, quantization, and SVD-based methods, while achieving higher compression ratios. These results support KVTC as a practical building block for memory-efficient LLM serving with reusable KV caches.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2511.01846</link>
<guid>https://arxiv.org/abs/2511.01846</guid>
<content:encoded><![CDATA[
arXiv:2511.01846v1 Announce Type: new 
Abstract: Finding the right north-star metrics is highly critical for advancing the mathematical reasoning capabilities of foundation models, especially given that existing evaluations are either too easy or only focus on getting correct short answers. To address these issues, we present IMO-Bench, a suite of advanced reasoning benchmarks, vetted by a panel of top specialists and that specifically targets the level of the International Mathematical Olympiad (IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench first tests models on 400 diverse Olympiad problems with verifiable short answers. IMO-Proof Bench is the next-level evaluation for proof-writing capabilities, which includes both basic and advanced IMO level problems as well as detailed grading guidelines to facilitate automatic grading. These benchmarks played a crucial role in our historic achievement of the gold-level performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4% respectively. We also showed that autograders built with Gemini reasoning correlate well with human evaluations and construct IMO-GradingBench, with 1000 human gradings on proofs, to enable further progress in automatic evaluation of long-form answers. We hope that IMO-Bench will help the community towards advancing robust mathematical reasoning and release it at https://imobench.github.io/.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.01854</link>
<guid>https://arxiv.org/abs/2511.01854</guid>
<content:encoded><![CDATA[
arXiv:2511.01854v1 Announce Type: new 
Abstract: Recent advances in LLM Multi-Agent Systems enable scalable orchestration of sub-agents, each coordinating hundreds or thousands of tools or Model Context Protocol (MCP) servers. However, existing retrieval methods typically match queries against coarse agent-level descriptions before routing, which obscures fine-grained tool functionality and often results in suboptimal agent selection. We introduce Tool-to-Agent Retrieval, a unified framework that embeds both tools and their parent agents in a shared vector space and connects them through metadata relationships. By explicitly representing tool capabilities and traversing metadata to the agent level, Tool-to-Agent Retrieval enables granular tool-level or agent-level retrieval, ensuring that agents and their underlying tools or MCP servers are equally represented without the context dilution that arises from chunking many tools together. Evaluating Tool-to-Agent Retrieval across eight embedding models, our approach achieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over previous state-of-the-art agent retrievers on the LiveMCPBench benchmark.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment</title>
<link>https://arxiv.org/abs/2511.00004</link>
<guid>https://arxiv.org/abs/2511.00004</guid>
<content:encoded><![CDATA[
arXiv:2511.00004v1 Announce Type: cross 
Abstract: Natural disaster assessment relies on accurate and rapid access to information, with social media emerging as a valuable real-time source. However, existing datasets suffer from class imbalance and limited samples, making effective model development a challenging task. This paper explores augmentation techniques to address these issues on the CrisisMMD multimodal dataset. For visual data, we apply diffusion-based methods, namely Real Guidance and DiffuseMix. For text data, we explore back-translation, paraphrasing with transformers, and image caption-based augmentation. We evaluated these across unimodal, multimodal, and multi-view learning setups. Results show that selected augmentations improve classification performance, particularly for underrepresented classes, while multi-view learning introduces potential but requires further refinement. This study highlights effective augmentation strategies for building more robust disaster assessment systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Detection of Fake Reviews using BERT and ResNet-50</title>
<link>https://arxiv.org/abs/2511.00020</link>
<guid>https://arxiv.org/abs/2511.00020</guid>
<content:encoded><![CDATA[
arXiv:2511.00020v1 Announce Type: cross 
Abstract: In the current digital commerce landscape, user-generated reviews play a critical role in shaping consumer behavior, product reputation, and platform credibility. However, the proliferation of fake or misleading reviews often generated by bots, paid agents, or AI models poses a significant threat to trust and transparency within review ecosystems. Existing detection models primarily rely on unimodal, typically textual, data and therefore fail to capture semantic inconsistencies across different modalities. To address this gap, a robust multimodal fake review detection framework is proposed, integrating textual features encoded with BERT and visual features extracted using ResNet-50. These representations are fused through a classification head to jointly predict review authenticity. To support this approach, a curated dataset comprising 21,142 user-uploaded images across food delivery, hospitality, and e-commerce domains was utilized. Experimental results indicate that the multimodal model outperforms unimodal baselines, achieving an F1-score of 0.934 on the test set. Additionally, the confusion matrix and qualitative analysis highlight the model's ability to detect subtle inconsistencies, such as exaggerated textual praise paired with unrelated or low-quality images, commonly found in deceptive content. This study demonstrates the critical role of multimodal learning in safeguarding digital trust and offers a scalable solution for content moderation across various online platforms.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model</title>
<link>https://arxiv.org/abs/2511.00024</link>
<guid>https://arxiv.org/abs/2511.00024</guid>
<content:encoded><![CDATA[
arXiv:2511.00024v1 Announce Type: cross 
Abstract: In the context of global sustainability mandates, corporate carbon disclosure has emerged as a critical mechanism for aligning business strategy with environmental responsibility. The Carbon Disclosure Project (CDP) hosts the world's largest longitudinal dataset of climate-related survey responses, combining structured indicators with open-ended narratives, but the heterogeneity and free-form nature of these disclosures present significant analytical challenges for benchmarking, compliance monitoring, and investment screening. This paper proposes a novel decision-support framework that leverages large language models (LLMs) to assess corporate climate disclosure quality at scale. It develops a master rubric that harmonizes narrative scoring across 11 years of CDP data (2010-2020), enabling cross-sector and cross-country benchmarking. By integrating rubric-guided scoring with percentile-based normalization, our method identifies temporal trends, strategic alignment patterns, and inconsistencies in disclosure across industries and regions. Results reveal that sectors such as technology and countries like Germany consistently demonstrate higher rubric alignment, while others exhibit volatility or superficial engagement, offering insights that inform key decision-making processes for investors, regulators, and corporate environmental, social, and governance (ESG) strategists. The proposed LLM-based approach transforms unstructured disclosures into quantifiable, interpretable, comparable, and actionable intelligence, advancing the capabilities of AI-enabled decision support systems (DSSs) in the domain of climate governance.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph</title>
<link>https://arxiv.org/abs/2511.00086</link>
<guid>https://arxiv.org/abs/2511.00086</guid>
<content:encoded><![CDATA[
arXiv:2511.00086v1 Announce Type: cross 
Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking that optimal architectures and model combinations can vary across tasks. Therefore, we study the novel problem of searching for compute-optimal model combinations and architectures in TTS under a fixed budget. We formalize it as a multi-LLM collaboration graph, where nodes encode roles and LLM model assignments, and edges capture information flow. This problem is challenging because (i) the combinatorial search space is prohibitively large, and (ii) task-specific requirements demand tailored designs. To address these, we reformulate the problem as probabilistic graph optimization and, through pilot experiments, derive three empirical insights into TTS collaboration graphs. Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline by mapping sampling-gradient-update to sampling-feedback-update, where feedback serves as a textual gradient to update the probabilistic graph and efficiently search for optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, and effectively identifies optimal graphs under joint objectives of accuracy and inference latency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuantumBench: A Benchmark for Quantum Problem Solving</title>
<link>https://arxiv.org/abs/2511.00092</link>
<guid>https://arxiv.org/abs/2511.00092</guid>
<content:encoded><![CDATA[
arXiv:2511.00092v1 Announce Type: cross 
Abstract: Large language models are now integrated into many scientific workflows, accelerating data analysis, hypothesis generation, and design space exploration. In parallel with this growth, there is a growing need to carefully evaluate whether models accurately capture domain-specific knowledge and notation, since general-purpose benchmarks rarely reflect these requirements. This gap is especially clear in quantum science, which features non-intuitive phenomena and requires advanced mathematics. In this study, we introduce QuantumBench, a benchmark for the quantum domain that systematically examine how well LLMs understand and can be applied to this non-intuitive field. Using publicly available materials, we compiled approximately 800 questions with their answers spanning nine areas related to quantum science and organized them into an eight-option multiple-choice dataset. With this benchmark, we evaluate several existing LLMs and analyze their performance in the quantum domain, including sensitivity to changes in question format. QuantumBench is the first LLM evaluation dataset built for the quantum domain, and it is intended to guide the effective use of LLMs in quantum research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wayfinding through the AI wilderness: Mapping rhetorics of ChatGPT prompt writing on X (formerly Twitter) to promote critical AI literacies</title>
<link>https://arxiv.org/abs/2511.00106</link>
<guid>https://arxiv.org/abs/2511.00106</guid>
<content:encoded><![CDATA[
arXiv:2511.00106v1 Announce Type: cross 
Abstract: In this paper, we demonstrate how studying the rhetorics of ChatGPT prompt writing on social media can promote critical AI literacies. Prompt writing is the process of writing instructions for generative AI tools like ChatGPT to elicit desired outputs and there has been an upsurge of conversations about it on social media. To study this rhetorical activity, we build on four overlapping traditions of digital writing research in computers and composition that inform how we frame literacies, how we study social media rhetorics, how we engage iteratively and reflexively with methodologies and technologies, and how we blend computational methods with qualitative methods. Drawing on these four traditions, our paper shows our iterative research process through which we gathered and analyzed a dataset of 32,000 posts (formerly known as tweets) from X (formerly Twitter) about prompt writing posted between November 2022 to May 2023. We present five themes about these emerging AI literacy practices: (1) areas of communication impacted by prompt writing, (2) micro-literacy resources shared for prompt writing, (3) market rhetoric shaping prompt writing, (4) rhetorical characteristics of prompts, and (5) definitions of prompt writing. In discussing these themes and our methodologies, we highlight takeaways for digital writing teachers and researchers who are teaching and analyzing critical AI literacies.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-time and Zero-footprint Bag of Synthetic Syllables Algorithm for E-mail Spam Detection Using Subject Line and Short Text Fields</title>
<link>https://arxiv.org/abs/2511.00118</link>
<guid>https://arxiv.org/abs/2511.00118</guid>
<content:encoded><![CDATA[
arXiv:2511.00118v1 Announce Type: cross 
Abstract: Contemporary e-mail services have high availability expectations from the customers and are resource-strained because of the high-volume throughput and spam attacks. Deep Machine Learning architectures, which are resource hungry and require off-line processing due to the long processing times, are not acceptable at the front line filters. On the other hand, the bulk of the incoming spam is not sophisticated enough to bypass even the simplest algorithms. While the small fraction of the intelligent, highly mutable spam can be detected only by the deep architectures, the stress on them can be unloaded by the simple near real-time and near zero-footprint algorithms such as the Bag of Synthetic Syllables algorithm applied to the short texts of the e-mail subject lines and other short text fields. The proposed algorithm creates a circa 200 sparse dimensional hash or vector for each e-mail subject line that can be compared for the cosine or euclidean proximity distance to find similarities to the known spammy subjects. The algorithm does not require any persistent storage, dictionaries, additional hardware upgrades or software packages. The performance of the algorithm is presented on the one day of the real SMTP traffic.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can SAEs reveal and mitigate racial biases of LLMs in healthcare?</title>
<link>https://arxiv.org/abs/2511.00177</link>
<guid>https://arxiv.org/abs/2511.00177</guid>
<content:encoded><![CDATA[
arXiv:2511.00177v1 Announce Type: cross 
Abstract: LLMs are increasingly being used in healthcare. This promises to free physicians from drudgery, enabling better care to be delivered at scale. But the use of LLMs in this space also brings risks; for example, such models may worsen existing biases. How can we spot when LLMs are (spuriously) relying on patient race to inform predictions? In this work we assess the degree to which Sparse Autoencoders (SAEs) can reveal (and control) associations the model has made between race and stigmatizing concepts. We first identify SAE latents in Gemma-2 models which appear to correlate with Black individuals. We find that this latent activates on reasonable input sequences (e.g., "African American") but also problematic words like "incarceration". We then show that we can use this latent to steer models to generate outputs about Black patients, and further that this can induce problematic associations in model outputs as a result. For example, activating the Black latent increases the risk assigned to the probability that a patient will become "belligerent". We evaluate the degree to which such steering via latents might be useful for mitigating bias. We find that this offers improvements in simple settings, but is less successful for more realistic and complex clinical tasks. Overall, our results suggest that: SAEs may offer a useful tool in clinical applications of LLMs to identify problematic reliance on demographics but mitigating bias via SAE steering appears to be of marginal utility for realistic tasks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Cognitive Science with LLMs</title>
<link>https://arxiv.org/abs/2511.00206</link>
<guid>https://arxiv.org/abs/2511.00206</guid>
<content:encoded><![CDATA[
arXiv:2511.00206v1 Announce Type: cross 
Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and conceptual clarity, in part due to its multifaceted and interdisciplinary nature. Recent advances in artificial intelligence, particularly the development of large language models (LLMs), offer tools that may help to address these issues. This review examines how LLMs can support areas where the field has historically struggled, including establishing cross-disciplinary connections, formalizing theories, developing clear measurement taxonomies, achieving generalizability through integrated modeling frameworks, and capturing contextual and individual variation. We outline the current capabilities and limitations of LLMs in these domains, including potential pitfalls. Taken together, we conclude that LLMs can serve as tools for a more integrative and cumulative cognitive science when used judiciously to complement, rather than replace, human expertise.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongCat-Flash-Omni Technical Report</title>
<link>https://arxiv.org/abs/2511.00279</link>
<guid>https://arxiv.org/abs/2511.00279</guid>
<content:encoded><![CDATA[
arXiv:2511.00279v1 Announce Type: cross 
Abstract: We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction. By adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability. Building upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction. For training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation. We provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibration Across Layers: Understanding Calibration Evolution in LLMs</title>
<link>https://arxiv.org/abs/2511.00280</link>
<guid>https://arxiv.org/abs/2511.00280</guid>
<content:encoded><![CDATA[
arXiv:2511.00280v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated inherent calibration capabilities, where predicted probabilities align well with correctness, despite prior findings that deep neural networks are often overconfident. Recent studies have linked this behavior to specific components in the final layer, such as entropy neurons and the unembedding matrix null space. In this work, we provide a complementary perspective by investigating how calibration evolves throughout the network depth. Analyzing multiple open-weight models on the MMLU benchmark, we uncover a distinct confidence correction phase in the upper/later layers, where model confidence is actively recalibrated after decision certainty has been reached. Furthermore, we identify a low-dimensional calibration direction in the residual stream whose perturbation significantly improves calibration metrics (ECE and MCE) without harming accuracy. Our findings suggest that calibration is a distributed phenomenon, shaped throughout the network forward pass, not just in its final projection, providing new insights into how confidence-regulating mechanisms operate within LLMs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reject Only Critical Tokens: Pivot-Aware Speculative Decoding</title>
<link>https://arxiv.org/abs/2511.00351</link>
<guid>https://arxiv.org/abs/2511.00351</guid>
<content:encoded><![CDATA[
arXiv:2511.00351v1 Announce Type: cross 
Abstract: Speculative Decoding (SD) ensures that the output matches the target model's distribution exactly. However, we argue that this distribution matching requirement is too stringent and results in unnecessarily low acceptance rates, limiting potential speedups. Instead, we advocate a reformulation of the decoding objective: the proposed decoding strategy should match the expected utility, i.e., the task-specific performance, of the target model. This perspective also aligns better with real-world use cases of LLMs, where utility (e.g., code correctness, factual accuracy) is often more important than sampling distribution. Based on this reformulation, we propose a novel decoding strategy: Pivot-Aware Speculative Decoding, which rejects only those tokens that would lead to a utility drop in the final output. We refer to these critical tokens as pivot tokens. We propose a method for labeling tokens as pivotal or non-pivotal and train a lightweight classifier to detect them. This method can be viewed as a relaxed version of standard SD, which offers much higher acceptance while preserving utility. We evaluate our method across various datasets, demonstrating that we can achieve up to $2.5\times$ speedup with comparable utility. Source code is available at https://github.com/amir-zsh/PAD.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diverse Human Value Alignment for Large Language Models via Ethical Reasoning</title>
<link>https://arxiv.org/abs/2511.00379</link>
<guid>https://arxiv.org/abs/2511.00379</guid>
<content:encoded><![CDATA[
arXiv:2511.00379v1 Announce Type: cross 
Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and evolving human values across different regions and cultures remains a critical challenge in AI ethics. Current alignment approaches often yield superficial conformity rather than genuine ethical understanding, failing to address the complex, context-dependent nature of human values. In this paper, we propose a novel ethical reasoning paradigm for LLMs inspired by well-established ethical decision-making models, aiming at enhancing diverse human value alignment through deliberative ethical reasoning. Our framework consists of a structured five-step process, including contextual fact gathering, hierarchical social norm identification, option generation, multiple-lens ethical impact analysis, and reflection. This theory-grounded approach guides LLMs through an interpretable reasoning process that enhances their ability to understand regional specificities and perform nuanced ethical analysis, which can be implemented with either prompt engineering or supervised fine-tuning methods. We perform evaluations on the SafeWorld benchmark that specially designed for regional value alignment. Experimental results demonstrate our framework significantly improves LLM alignment with diverse human values compared to baseline methods, enabling more accurate social norm identification and more culturally appropriate reasoning. Our work provides a concrete pathway toward developing LLMs that align more effectively with the multifaceted values of global societies through interdisciplinary research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>\texttt{ReMind}: Understanding Deductive Code Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2511.00488</link>
<guid>https://arxiv.org/abs/2511.00488</guid>
<content:encoded><![CDATA[
arXiv:2511.00488v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable progress in code-related tasks. Despite their advancement, empirical evidence reveals that they still struggle with \emph{deductive code reasoning}, the ability to reason about the program execution process. While prior studies have recognized this limitation, the underlying causes remain largely underexplored. In this paper, we begin by presenting a comprehensive empirical study that reveals three key challenges undermining deductive code reasoning: (1) an intrinsic gap between generation and reasoning abilities, (2) a consistent bias towards code sources, and (3) weak zero-shot generalization on complex benchmarks. In light of these challenges, we propose \texttt{ReMind}, a multi-agent framework composed of \texttt{Mutator}, \texttt{Executor}, and \texttt{Inspector}. The \texttt{Mutator} generates code variants to mitigate bias towards code sources, the \texttt{Executor} traces variable states step-by-step to expose inconsistency, and the \texttt{Inspector} identifies problematic reasoning steps and provides control-flow refinement to bridge the intrinsic reasoning gap. Through their coordinated collaboration, \texttt{ReMind} systematically identifies and refines reasoning flaws, achieving outstanding performance and enabling robust zero-shot generalization. Extensive experiments on two benchmarks with five LLMs demonstrate the superior advantages of \texttt{ReMind} compared to baseline approaches in deductive code reasoning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Planning for Language Models</title>
<link>https://arxiv.org/abs/2511.00521</link>
<guid>https://arxiv.org/abs/2511.00521</guid>
<content:encoded><![CDATA[
arXiv:2511.00521v1 Announce Type: cross 
Abstract: Selecting an appropriate reasoning method for a given query remains a key challenge in language model generation. Existing approaches typically generate multiple candidate responses and use an aggregation strategy to select the output answer, often assuming that more candidate answers yield higher accuracy. We revisit this assumption through a rigorous theoretical analysis, deriving accuracy bounds for standard aggregation methods under fixed generation distributions and candidate sizes. Building on these insights, we introduce EPIC, an Ensemble Planning with Contrastive learning framework to learn a shared representation space that captures both model reasoning abilities and query-method compatibility. EPIC incorporates our probability bounds as a regularizer in a utility-driven optimization that balances accuracy and computational cost. Experiments on diverse mathematical reasoning tasks show that EPIC consistently selects optimal reasoning methods, improving accuracy while reducing computational overhead. Our code can be found at https://github.com/nguyenngocbaocmt02/EPIC.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structurally Refined Graph Transformer for Multimodal Recommendation</title>
<link>https://arxiv.org/abs/2511.00584</link>
<guid>https://arxiv.org/abs/2511.00584</guid>
<content:encoded><![CDATA[
arXiv:2511.00584v1 Announce Type: cross 
Abstract: Multimodal recommendation systems utilize various types of information, including images and text, to enhance the effectiveness of recommendations. The key challenge is predicting user purchasing behavior from the available data. Current recommendation models prioritize extracting multimodal information while neglecting the distinction between redundant and valuable data. They also rely heavily on a single semantic framework (e.g., local or global semantics), resulting in an incomplete or biased representation of user preferences, particularly those less expressed in prior interactions. Furthermore, these approaches fail to capture the complex interactions between users and items, limiting the model's ability to meet diverse users. To address these challenges, we present SRGFormer, a structurally optimized multimodal recommendation model. By modifying the transformer for better integration into our model, we capture the overall behavior patterns of users. Then, we enhance structural information by embedding multimodal information into a hypergraph structure to aid in learning the local structures between users and items. Meanwhile, applying self-supervised tasks to user-item collaborative signals enhances the integration of multimodal information, thereby revealing the representational features inherent to the data's modality. Extensive experiments on three public datasets reveal that SRGFormer surpasses previous benchmark models, achieving an average performance improvement of 4.47 percent on the Sports dataset. The code is publicly available online.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering</title>
<link>https://arxiv.org/abs/2511.00617</link>
<guid>https://arxiv.org/abs/2511.00617</guid>
<content:encoded><![CDATA[
arXiv:2511.00617v1 Announce Type: cross 
Abstract: Large language models (LLMs) can be controlled at inference time through prompts (in-context learning) and internal activations (activation steering). Different accounts have been proposed to explain these methods, yet their common goal of controlling model behavior raises the question of whether these seemingly disparate methodologies can be seen as specific instances of a broader framework. Motivated by this, we develop a unifying, predictive account of LLM control from a Bayesian perspective. Specifically, we posit that both context- and activation-based interventions impact model behavior by altering its belief in latent concepts: steering operates by changing concept priors, while in-context learning leads to an accumulation of evidence. This results in a closed-form Bayesian model that is highly predictive of LLM behavior across context- and activation-based interventions in a set of domains inspired by prior work on many-shot in-context learning. This model helps us explain prior empirical phenomena - e.g., sigmoidal learning curves as in-context evidence accumulates - while predicting novel ones - e.g., additivity of both interventions in log-belief space, which results in distinct phases such that sudden and dramatic behavioral shifts can be induced by slightly changing intervention controls. Taken together, this work offers a unified account of prompt-based and activation-based control of LLM behavior, and a methodology for empirically predicting the effects of these interventions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching</title>
<link>https://arxiv.org/abs/2511.00640</link>
<guid>https://arxiv.org/abs/2511.00640</guid>
<content:encoded><![CDATA[
arXiv:2511.00640v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex reasoning tasks, yet they often suffer from overthinking, producing excessively long chain-of-thought (CoT) traces that increase inference cost and may degrade accuracy. Our analysis reveals a clear anti-correlation between reasoning length and accuracy, where across multiple stochastic decodes, the short reasoning paths consistently achieve the highest correctness, while longer ones accumulate errors and repetitions. These short optimal reasoning paths can be found ideally through full enumeration of the reasoning space. However, the tree-structured reasoning space grows exponentially with sequence length, rendering exhaustive exploration infeasible. To address this, we propose DTS, a model-agnostic decoding framework that sketches the reasoning space by selectively branching at high-entropy tokens and applies early stopping to select the shortest completed reasoning path. This approach approximates the optimal solution that enhances both efficiency and accuracy, without requiring additional training or supervision. Experiments on AIME2024 and AIME2025 datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves accuracy by up to 8%, reduces average reasoning length by 23%, and decreases repetition frequency by 12%, demonstrating DTS's ability for scalable and efficient LRM reasoning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting</title>
<link>https://arxiv.org/abs/2511.00651</link>
<guid>https://arxiv.org/abs/2511.00651</guid>
<content:encoded><![CDATA[
arXiv:2511.00651v1 Announce Type: cross 
Abstract: Telecom networks are rapidly growing in scale and complexity, making effective management, operation, and optimization increasingly challenging. Although Artificial Intelligence (AI) has been applied to many telecom tasks, existing models are often narrow in scope, require large amounts of labeled data, and struggle to generalize across heterogeneous deployments. Consequently, network troubleshooting continues to rely heavily on Subject Matter Experts (SMEs) to manually correlate various data sources to identify root causes and corrective actions. To address these limitations, we propose a Multi-Agent System (MAS) that employs an agentic workflow, with Large Language Models (LLMs) coordinating multiple specialized tools for fully automated network troubleshooting. Once faults are detected by AI/ML-based monitors, the framework dynamically activates agents such as an orchestrator, solution planner, executor, data retriever, and root-cause analyzer to diagnose issues and recommend remediation strategies within a short time frame. A key component of this system is the solution planner, which generates appropriate remediation plans based on internal documentation. To enable this, we fine-tuned a Small Language Model (SLM) on proprietary troubleshooting documents to produce domain-grounded solution plans. Experimental results demonstrate that the proposed framework significantly accelerates troubleshooting automation across both Radio Access Network (RAN) and Core network domains.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models</title>
<link>https://arxiv.org/abs/2511.00749</link>
<guid>https://arxiv.org/abs/2511.00749</guid>
<content:encoded><![CDATA[
arXiv:2511.00749v1 Announce Type: cross 
Abstract: Social media has exacerbated the promotion of Western beauty norms, leading to negative self-image, particularly in women and girls, and causing harm such as body dysmorphia. Increasingly content on the internet has been artificially generated, leading to concerns that these norms are being exaggerated. The aim of this work is to study how generative AI models may encode 'beauty' and erase 'ugliness', and discuss the implications of this for society. To investigate these aims, we create two image generation pipelines: a text-to-image model and a text-to-language model-to image model. We develop a structured beauty taxonomy which we use to prompt three language models (LMs) and two text-to-image models to cumulatively generate 5984 images using our two pipelines. We then recruit women and non-binary social media users to evaluate 1200 of the images through a Likert-scale within-subjects study. Participants show high agreement in their ratings. Our results show that 86.5% of generated images depicted people with lighter skin tones, 22% contained explicit content despite Safe for Work (SFW) training, and 74% were rated as being in a younger age demographic. In particular, the images of non-binary individuals were rated as both younger and more hypersexualised, indicating troubling intersectional effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such as "a wide nose") consistently produced higher Not SFW (NSFW) ratings regardless of gender. This work sheds light on the pervasive demographic biases related to beauty standards present in generative AI models -- biases that are actively perpetuated by model developers, such as via negative prompting. We conclude by discussing the implications of this on society, which include pollution of the data streams and active erasure of features that do not fall inside the stereotype of what is considered beautiful by developers.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reevaluating Self-Consistency Scaling in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.00751</link>
<guid>https://arxiv.org/abs/2511.00751</guid>
<content:encoded><![CDATA[
arXiv:2511.00751v1 Announce Type: cross 
Abstract: This study examines the trade-offs of increasing sampled reasoning paths in self-consistency for modern large language models (LLMs). Earlier research with older models showed that combining multiple reasoning chains improves results before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we revisit those claims under current model conditions. Each configuration pooled outputs from varying sampled reasoning paths and compared them to a single chain-of-thought (CoT) baseline. Larger models exhibited a more stable and consistent improvement curve. The results confirm that performance gains taper off after moderate sampling, aligning with past findings. This plateau suggests diminishing returns driven by overlap among reasoning paths. Self-consistency remains useful, but high-sample configurations offer little benefit relative to their computational cost.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents</title>
<link>https://arxiv.org/abs/2511.00802</link>
<guid>https://arxiv.org/abs/2511.00802</guid>
<content:encoded><![CDATA[
arXiv:2511.00802v1 Announce Type: cross 
Abstract: With the software industry shifting toward a data-driven culture, online A/B testing is a key tool for evaluating new technologies. However, deploying such experiments requires substantial resources, may negatively impact users, and involves long data collection periods. To address this, \textit{off-policy evaluation (OPE)}, or offline A/B testing, uses logged data to assess technologies and is fundamental in Reinforcement Learning, making it crucial in domains where online testing is costly or risky, such as healthcare, recommender systems, education, dialog systems, and robotics. Despite advances in coding LLMs and agentic AI, little is known about leveraging them to optimize OPE results. We investigate whether LLMs and LLM-based agents can improve OPE performance via code optimization. We propose \textit{GrowthHacker}, a benchmark with agent and baseline methods on large-scale real-world datasets, which iteratively optimizes code, evaluates results, and begins new optimization cycles. We collected datasets, established protocols, implemented baselines for OPE on the Open Bandit Pipeline (OBP)~\cite{saito2021openbanditdatasetpipeline} and Scope-RL~\cite{kiyohara2023scope}, and developed the \textit{two_agent} framework, which reduces system complexity while preserving optimization effectiveness. Results show the two_agent framework achieves 100% reliability and the highest average improvement of 106.7% among positive outcomes. Both two_agent and CrewAI reach 45% success rates, outperforming AutoGen's 34%. These findings demonstrate the feasibility of LLM-based agents as automated "growth hackers" to enhance OPE systems, with implications for scaling data-driven decision-making in production.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding</title>
<link>https://arxiv.org/abs/2511.00810</link>
<guid>https://arxiv.org/abs/2511.00810</guid>
<content:encoded><![CDATA[
arXiv:2511.00810v1 Announce Type: cross 
Abstract: Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2% on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence ability of Spoken Dialogue Models</title>
<link>https://arxiv.org/abs/2511.00850</link>
<guid>https://arxiv.org/abs/2511.00850</guid>
<content:encoded><![CDATA[
arXiv:2511.00850v1 Announce Type: cross 
Abstract: Spoken Dialogue Models (SDMs) have advanced rapidly, yet their ability to sustain genuinely interactive multi-turn conversations remains underexplored, as most benchmarks focus on single-turn exchanges. We introduce Multi-Bench, the first benchmark explicitly designed to evaluate SDMs in multi-turn interactive dialogue with an emphasis on emotional intelligence. Multi-Bench employs a hierarchical structure with a basic track for emotion understanding and reasoning and an advanced track for emotion support and application. It comprises five carefully designed tasks and about 3.2K samples, ranging from emotion recognition to complex reasoning and interactive dialogue, supported by a reproducible evaluation framework. We evaluate six representative SDMs on eight subsets of Multi-Bench. Results show that while current SDMs achieve good performance on basic understanding tasks, they still have room for improvement in advanced multi-turn interactive dialogue and reasoning-related tasks, particularly in emotion awareness and application.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory</title>
<link>https://arxiv.org/abs/2511.00926</link>
<guid>https://arxiv.org/abs/2511.00926</guid>
<content:encoded><![CDATA[
arXiv:2511.00926v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) grow in capability, do they develop self-awareness as an emergent behavior? And if so, can we measure it? We introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for measuring self-awareness through strategic differentiation. Using the "Guess 2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across 4,200 trials with three opponent framings: (A) against humans, (B) against other AI models, and (C) against AI models like you. We operationalize self-awareness as the capacity to differentiate strategic reasoning based on opponent type. Finding 1: Self-awareness emerges with model advancement. The majority of advanced models (21/28, 75%) demonstrate clear self-awareness, while older/smaller models show no differentiation. Finding 2: Self-aware models rank themselves as most rational. Among the 21 models with self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs > Humans, with large AI attribution effects and moderate self-preferencing. These findings reveal that self-awareness is an emergent capability of advanced LLMs, and that self-aware models systematically perceive themselves as more rational than humans. This has implications for AI alignment, human-AI collaboration, and understanding AI beliefs about human capabilities.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL</title>
<link>https://arxiv.org/abs/2511.00985</link>
<guid>https://arxiv.org/abs/2511.00985</guid>
<content:encoded><![CDATA[
arXiv:2511.00985v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in translating natural language to SQL, but a significant semantic gap persists between their general knowledge and domain-specific semantics of databases. Historical translation logs constitute a rich source of this missing in-domain knowledge, where SQL queries inherently encapsulate real-world usage patterns of database schema. Existing methods primarily enhance the reasoning process for individual translations but fail to accumulate in-domain knowledge from past translations. We introduce ORANGE, an online self-evolutionary framework that constructs database-specific knowledge bases by parsing SQL queries from translation logs. By accumulating in-domain knowledge that contains schema and data semantics, ORANGE progressively reduces the semantic gap and enhances the accuracy of subsequent SQL translations. To ensure reliability, we propose a novel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic tracking, which reduces semantic errors during knowledge generation. Experiments on multiple benchmarks confirm the practicality of ORANGE, demonstrating its effectiveness for real-world Text-to-SQL deployment, particularly in handling complex and domain-specific queries.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Emergence of Induction Heads for In-Context Learning</title>
<link>https://arxiv.org/abs/2511.01033</link>
<guid>https://arxiv.org/abs/2511.01033</guid>
<content:encoded><![CDATA[
arXiv:2511.01033v1 Announce Type: cross 
Abstract: Transformers have become the dominant architecture for natural language processing. Part of their success is owed to a remarkable capability known as in-context learning (ICL): they can acquire and apply novel associations solely from their input context, without any updates to their weights. In this work, we study the emergence of induction heads, a previously identified mechanism in two-layer transformers that is particularly important for in-context learning. We uncover a relatively simple and interpretable structure of the weight matrices implementing the induction head. We theoretically explain the origin of this structure using a minimal ICL task formulation and a modified transformer architecture. We give a formal proof that the training dynamics remain constrained to a 19-dimensional subspace of the parameter space. Empirically, we validate this constraint while observing that only 3 dimensions account for the emergence of an induction head. By further studying the training dynamics inside this 3-dimensional subspace, we find that the time until the emergence of an induction head follows a tight asymptotic bound that is quadratic in the input context length.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.01104</link>
<guid>https://arxiv.org/abs/2511.01104</guid>
<content:encoded><![CDATA[
arXiv:2511.01104v1 Announce Type: cross 
Abstract: Existing LLM-based automatic test generation methods mainly produce input and expected output pairs to categorize the intended behavior of correct programs. Although straightforward, these methods have limited diversity in generated tests and cannot provide enough debugging information. We propose HarnessLLM, a two-stage training pipeline that enables LLMs to write harness code for testing. Particularly, LLMs generate code that synthesizes inputs and validates the observed outputs, allowing complex test cases and flexible output validation such as invariant checking. To achieve this, we train LLMs with SFT followed by RLVR with a customized reward design. Experiments show that HarnessLLM outperforms input-output-based testing in bug finding and testing strategy diversity. HarnessLLM further benefits the code generation performance through test-time scaling with our generated test cases as inference-phase validation. Our code is available at https://github.com/UCSB-NLP-Chang/HarnessLLM.git.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S2Doc - Spatial-Semantic Document Format</title>
<link>https://arxiv.org/abs/2511.01113</link>
<guid>https://arxiv.org/abs/2511.01113</guid>
<content:encoded><![CDATA[
arXiv:2511.01113v1 Announce Type: cross 
Abstract: Documents are a common way to store and share information, with tables being an important part of many documents. However, there is no real common understanding of how to model documents and tables in particular. Because of this lack of standardization, most scientific approaches have their own way of modeling documents and tables, leading to a variety of different data structures and formats that are not directly compatible. Furthermore, most data models focus on either the spatial or the semantic structure of a document, neglecting the other aspect. To address this, we developed S2Doc, a flexible data structure for modeling documents and tables that combines both spatial and semantic information in a single format. It is designed to be easily extendable to new tasks and supports most modeling approaches for documents and tables, including multi-page documents. To the best of our knowledge, it is the first approach of its kind to combine all these aspects in a single format.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FEval-TTC: Fair Evaluation Protocol for Test-Time Compute</title>
<link>https://arxiv.org/abs/2511.01203</link>
<guid>https://arxiv.org/abs/2511.01203</guid>
<content:encoded><![CDATA[
arXiv:2511.01203v1 Announce Type: cross 
Abstract: The performance of Large Language Models (LLMs) and the associated dollar costs of API calls can fluctuate over time, potentially invalidating conclusions drawn in prior research. To address this, we propose a Fair Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure consistent assessment of test-time compute (TTC) methods, regardless of such fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize underlying Chains-of-Thought (CoT). It supports evaluations across multiple LLMs on a diverse set of mathematical and commonsense reasoning datasets. The few-shot prompting and answer extraction processes are standardized across datasets, reducing both time and monetary overhead for researchers. Furthermore, we provide a cost modelling procedure that estimates both the token and dollar cost per query, facilitating equitable comparisons of prevalent TTC methods. We open-source FEval-TTC for public use at https://github.com/networkslab/feval_ttc .
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Novelty and Impact of Economics Papers</title>
<link>https://arxiv.org/abs/2511.01211</link>
<guid>https://arxiv.org/abs/2511.01211</guid>
<content:encoded><![CDATA[
arXiv:2511.01211v1 Announce Type: cross 
Abstract: We propose a framework that recasts scientific novelty not as a single attribute of a paper, but as a reflection of its position within the evolving intellectual landscape. We decompose this position into two orthogonal dimensions: \textit{spatial novelty}, which measures a paper's intellectual distinctiveness from its neighbors, and \textit{temporal novelty}, which captures its engagement with a dynamic research frontier. To operationalize these concepts, we leverage Large Language Models to develop semantic isolation metrics that quantify a paper's location relative to the full-text literature. Applying this framework to a large corpus of economics articles, we uncover a fundamental trade-off: these two dimensions predict systematically different outcomes. Temporal novelty primarily predicts citation counts, whereas spatial novelty predicts disruptive impact. This distinction allows us to construct a typology of semantic neighborhoods, identifying four archetypes associated with distinct and predictable impact profiles. Our findings demonstrate that novelty can be understood as a multidimensional construct whose different forms, reflecting a paper's strategic location, have measurable and fundamentally distinct consequences for scientific progress.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles</title>
<link>https://arxiv.org/abs/2511.01340</link>
<guid>https://arxiv.org/abs/2511.01340</guid>
<content:encoded><![CDATA[
arXiv:2511.01340v1 Announce Type: cross 
Abstract: Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters to represent words or phrases creatively) requires a variety of skills such as image recognition, cognitive skills, commonsense reasoning, multi-step reasoning, image-based wordplay, etc., making this a challenging task for even current Vision-Language Models. In this paper, we present $\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$, a large and diverse benchmark of $1,333$ English Rebus Puzzles containing different artistic styles and levels of difficulty, spread across 18 categories such as food, idioms, sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a model-agnostic framework which uses a combination of an unstructured description and code-based, structured reasoning, along with better, reasoning-based in-context example selection, improving the performance of Vision-Language Models on $\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$ by $2.1-4.1\%$ and $20-30\%$ using closed-source and open-source models respectively compared to Chain-of-Thought Reasoning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hidden in Plain Sight: Where Developers Confess Self-Admitted Technical Debt</title>
<link>https://arxiv.org/abs/2511.01529</link>
<guid>https://arxiv.org/abs/2511.01529</guid>
<content:encoded><![CDATA[
arXiv:2511.01529v1 Announce Type: cross 
Abstract: Context. Detecting Self-Admitted Technical Debt (SATD) is crucial for proactive software maintenance. Previous research has primarily targeted detecting and prioritizing SATD, with little focus on the source code afflicted with SATD. Our goal in this work is to connect the SATD comments with source code constructs that surround them.
  Method. We leverage the extensive SATD dataset PENTACET, containing code comments from over 9000 Java Open Source Software (OSS) repositories. We quantitatively infer where SATD most commonly occurs and which code constructs/statements it most frequently affects.
  Results and Conclusions. Our large-scale study links over 225,000 SATD comments to their surrounding code, showing that SATD mainly arises in inline code near definitions, conditionals, and exception handling, where developers face uncertainty and trade-offs, revealing it as an intentional signal of awareness during change rather than mere neglect.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.01618</link>
<guid>https://arxiv.org/abs/2511.01618</guid>
<content:encoded><![CDATA[
arXiv:2511.01618v1 Announce Type: cross 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved 2D visual understanding, prompting interest in their application to complex 3D reasoning tasks. However, it remains unclear whether these models can effectively capture the detailed spatial information required for robust real-world performance, especially cross-view consistency, a key requirement for accurate 3D reasoning. Considering this issue, we introduce Viewpoint Learning, a task designed to evaluate and improve the spatial reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset, consisting of 100K object-centric image pairs with diverse viewpoints and corresponding question-answer pairs. Our approach employs a two-stage fine-tuning strategy: first, foundational knowledge is injected to the baseline MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in significant improvements across multiple tasks; second, generalization is enhanced through Reinforcement Learning using the Group Relative Policy Optimization (GRPO) algorithm on a broader set of questions. Additionally, we introduce a hybrid cold-start initialization method designed to simultaneously learn viewpoint representations and maintain coherent reasoning thinking. Experimental results show that our approach significantly activates the spatial reasoning ability of MLLM, improving performance on both in-domain and out-of-domain reasoning tasks. Our findings highlight the value of developing foundational spatial skills in MLLMs, supporting future progress in robotics, autonomous systems, and 3D scene understanding.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Proof of Learning Rate Transfer under $\mu$P</title>
<link>https://arxiv.org/abs/2511.01734</link>
<guid>https://arxiv.org/abs/2511.01734</guid>
<content:encoded><![CDATA[
arXiv:2511.01734v1 Announce Type: cross 
Abstract: We provide the first proof of learning rate transfer with width in a linear multi-layer perceptron (MLP) parametrized with $\mu$P, a neural network parameterization designed to ``maximize'' feature learning in the infinite-width limit. We show that under $\mu P$, the optimal learning rate converges to a \emph{non-zero constant} as width goes to infinity, providing a theoretical explanation to learning rate transfer. In contrast, we show that this property fails to hold under alternative parametrizations such as Standard Parametrization (SP) and Neural Tangent Parametrization (NTP). We provide intuitive proofs and support the theoretical findings with extensive empirical results.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks</title>
<link>https://arxiv.org/abs/2511.01758</link>
<guid>https://arxiv.org/abs/2511.01758</guid>
<content:encoded><![CDATA[
arXiv:2511.01758v1 Announce Type: cross 
Abstract: Open-ended generation tasks require outputs to satisfy diverse and often implicit task-specific evaluation rubrics. The sheer number of relevant rubrics leads to prohibitively high verification costs and incomplete assessments of a response, making reinforcement learning (RL) post-training with rubric-based rewards difficult to scale. This problem is exacerbated by the fact that often the best way to combine these rubrics into one single reward is also highly prompt-specific. We propose Reinforcement Learning with Adversarial Critic (RLAC), a post-training approach that addresses these challenges via dynamic rubric verification. Our approach employs a large language model (LLM) as a critic that dynamically identifies only the most likely failure modes (e.g., a factual error or unhandled edge case), which are then verified by an external validator to optimize both generator and critic jointly. By training both the generator and the critic, this game enhances the critic's error detection and the generator's output quality while reducing required verifications. Our experiments demonstrate that RLAC improves factual accuracy in text generation and correctness in code generation, while also outperforming exhaustive verification and reward model methods. We show that dynamic critics are more effective than fixed critics, showcasing the potential of RLAC for scaling RL post-training to free-form generation tasks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Random Initialization of Gated Sparse Adapters</title>
<link>https://arxiv.org/abs/2511.01794</link>
<guid>https://arxiv.org/abs/2511.01794</guid>
<content:encoded><![CDATA[
arXiv:2511.01794v1 Announce Type: cross 
Abstract: When fine-tuning language models on new tasks, catastrophic forgetting -- performance degradation on previously-learned tasks -- is a ubiquitous problem. While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this through low-rank adapters, sparse adaptation offers an alternative that doesn't impose rank constraints. We introduce Random Initialization of Gated Sparse Adapters (RIGSA), which starts from randomly-initialized full-rank adapters, gates them with a ReZero analog, and sparsifies them with iterative magnitude pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag, and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA and random masking. In spite of having more trainable parameters than QLoRA, the RIGSA configurations that we studied displayed less forgetting than QLoRA, particularly on GSM8k, though it performs comparably to random masking.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complex QA and language models hybrid architectures, Survey</title>
<link>https://arxiv.org/abs/2302.09051</link>
<guid>https://arxiv.org/abs/2302.09051</guid>
<content:encoded><![CDATA[
arXiv:2302.09051v5 Announce Type: replace 
Abstract: This paper reviews the state-of-the-art of large language models (LLM) architectures and strategies for "complex" question-answering with a focus on hybrid architectures. LLM based chatbot services have allowed anyone to grasp the potential of LLM to solve many common problems, but soon discovered their limitations for complex questions. Addressing more specific, complex questions (e.g., "What is the best mix of power-generation methods to reduce climate change ?") often requires specialized architectures, domain knowledge, new skills, decomposition and multi-step resolution, deep reasoning, sensitive data protection, explainability, and human-in-the-loop processes. Therefore, we review: (1) necessary skills and tasks for handling complex questions and common LLM limits to overcome; (2) dataset, cost functions and evaluation metrics for measuring and improving (e.g. accuracy, explainability, fairness, robustness, groundedness, faithfulness, toxicity...); (3) family of solutions to overcome LLM limitations by (a) training and reinforcement (b) hybridization, (c) prompting, (d) agentic-architectures (agents, tools) and extended reasoning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MaiBaam Annotation Guidelines</title>
<link>https://arxiv.org/abs/2403.05902</link>
<guid>https://arxiv.org/abs/2403.05902</guid>
<content:encoded><![CDATA[
arXiv:2403.05902v3 Announce Type: replace 
Abstract: This document provides the annotation guidelines for MaiBaam, a Bavarian corpus manually annotated with part-of-speech (POS) tags, syntactic dependencies, and German lemmas. MaiBaam belongs to the Universal Dependencies (UD) project, and our annotations elaborate on the general and German UD version 2 guidelines. In this document, we detail how to preprocess and tokenize Bavarian data, provide an overview of the POS tags and dependencies we use, explain annotation decisions that would also apply to closely related languages like German, and lastly we introduce and motivate decisions that are specific to Bavarian grammar.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists</title>
<link>https://arxiv.org/abs/2403.18771</link>
<guid>https://arxiv.org/abs/2403.18771</guid>
<content:encoded><![CDATA[
arXiv:2403.18771v3 Announce Type: replace 
Abstract: Existing LLM-as-a-Judge approaches for evaluating text generation suffer from rating inconsistencies, with low agreement and high rating variance across different evaluator models. We attribute this to subjective evaluation criteria combined with Likert scale scoring in existing protocols. To address this issue, we introduce CheckEval, a checklist-based evaluation framework that improves rating reliability via decomposed binary questions. Through experiments with 12 evaluator models across multiple datasets, we first demonstrate that CheckEval strongly correlates with human judgments. More importantly, CheckEval dramatically improves the average agreement across evaluator models by 0.45 and reduces the score variance. CheckEval scores furthermore have the benefit of being more interpretable because it decomposes evaluation criteria into traceable binary decisions, allowing analyses of specific attributes driving quality judgments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models</title>
<link>https://arxiv.org/abs/2409.19667</link>
<guid>https://arxiv.org/abs/2409.19667</guid>
<content:encoded><![CDATA[
arXiv:2409.19667v4 Announce Type: replace 
Abstract: The need to analyze graphs is ubiquitous across various fields, from social networks to biological research and recommendation systems. Therefore, enabling the ability of large language models (LLMs) to process graphs is an important step toward more advanced general intelligence. However, current LLM benchmarks on graph analysis require models to directly reason over the prompts describing graph topology, and are thus limited to small graphs with only a few dozens of nodes. In contrast, human experts typically write programs based on popular libraries for task solving, and can thus handle graphs with different scales. To this end, a question naturally arises: can LLMs analyze graphs like professionals? In this paper, we introduce ProGraph, a manually crafted benchmark containing 3 categories of graph tasks. The benchmark expects solutions based on programming instead of directly reasoning over raw inputs. Our findings reveal that the performance of current LLMs is unsatisfactory, with the best model achieving only 36% accuracy. To bridge this gap, we propose LLM4Graph datasets, which include crawled documents and auto-generated codes based on 6 widely used graph libraries. By augmenting closed-source LLMs with document retrieval and fine-tuning open-source ones on the codes, we show 11-32% absolute improvements in their accuracies. Our results underscore that the capabilities of LLMs in handling structured data are still under-explored, and show the effectiveness of LLM4Graph in enhancing LLMs' proficiency of graph analysis. The benchmark, datasets and enhanced open-source models are available at https://github.com/BUPT-GAMMA/ProGraph.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IndicSentEval: How Effectively do Multilingual Transformer Models encode Linguistic Properties for Indic Languages?</title>
<link>https://arxiv.org/abs/2410.02611</link>
<guid>https://arxiv.org/abs/2410.02611</guid>
<content:encoded><![CDATA[
arXiv:2410.02611v2 Announce Type: replace 
Abstract: Transformer-based models have revolutionized the field of natural language processing. To understand why they perform so well and to assess their reliability, several studies have focused on questions such as: Which linguistic properties are encoded by these models, and to what extent? How robust are these models in encoding linguistic properties when faced with perturbations in the input text? However, these studies have mainly focused on BERT and the English language. In this paper, we investigate similar questions regarding encoding capability and robustness for 8 linguistic properties across 13 different perturbations in 6 Indic languages, using 9 multilingual Transformer models (7 universal and 2 Indic-specific). To conduct this study, we introduce a novel multilingual benchmark dataset, IndicSentEval, containing approximately $\sim$47K sentences. Surprisingly, our probing analysis of surface, syntactic, and semantic properties reveals that while almost all multilingual models demonstrate consistent encoding performance for English, they show mixed results for Indic languages. As expected, Indic-specific multilingual models capture linguistic properties in Indic languages better than universal models. Intriguingly, universal models broadly exhibit better robustness compared to Indic-specific models, particularly under perturbations such as dropping both nouns and verbs, dropping only verbs, or keeping only nouns. Overall, this study provides valuable insights into probing and perturbation-specific strengths and weaknesses of popular multilingual Transformer-based models for different Indic languages. We make our code and dataset publicly available [https://github.com/aforakhilesh/IndicBertology].
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Large Language Models for Detecting Mental Disorders</title>
<link>https://arxiv.org/abs/2410.07129</link>
<guid>https://arxiv.org/abs/2410.07129</guid>
<content:encoded><![CDATA[
arXiv:2410.07129v3 Announce Type: replace 
Abstract: This paper compares the effectiveness of traditional machine learning methods, encoder-based models, and large language models (LLMs) on the task of detecting depression and anxiety. Five Russian-language datasets were considered, each differing in format and in the method used to define the target pathology class. We tested AutoML models based on linguistic features, several variations of encoder-based Transformers such as BERT, and state-of-the-art LLMs as pathology classification models. The results demonstrated that LLMs outperform traditional methods, particularly on noisy and small datasets where training examples vary significantly in text length and genre. However, psycholinguistic features and encoder-based models can achieve performance comparable to language models when trained on texts from individuals with clinically confirmed depression, highlighting their potential effectiveness in targeted clinical applications.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Evaluation of Cognitive Biases in LLMs</title>
<link>https://arxiv.org/abs/2410.15413</link>
<guid>https://arxiv.org/abs/2410.15413</guid>
<content:encoded><![CDATA[
arXiv:2410.15413v2 Announce Type: replace 
Abstract: We present a large-scale evaluation of 30 cognitive biases in 20 state-of-the-art large language models (LLMs) under various decision-making scenarios. Our contributions include a novel general-purpose test framework for reliable and large-scale generation of tests for LLMs, a benchmark dataset with 30,000 tests for detecting cognitive biases in LLMs, and a comprehensive assessment of the biases found in the 20 evaluated LLMs. Our work confirms and broadens previous findings suggesting the presence of cognitive biases in LLMs by reporting evidence of all 30 tested biases in at least some of the 20 LLMs. We publish our framework code to encourage future research on biases in LLMs: https://github.com/simonmalberg/cognitive-biases-in-llms
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incivility and Rigidity: Evaluating the Risks of Fine-Tuning LLMs for Political Argumentation</title>
<link>https://arxiv.org/abs/2411.16813</link>
<guid>https://arxiv.org/abs/2411.16813</guid>
<content:encoded><![CDATA[
arXiv:2411.16813v4 Announce Type: replace 
Abstract: Incivility on platforms such as Twitter (now X) and Reddit complicates the development of AI systems that can support productive, rhetorically sound political argumentation. We present experiments with \textit{GPT-3.5 Turbo} fine-tuned on two contrasting datasets of political discourse: high-incivility Twitter replies to U.S. Congress and low-incivility posts from Reddit's \textit{r/ChangeMyView}. Our evaluation examines how data composition and prompting strategies affect the rhetorical framing and deliberative quality of model-generated arguments. Results show that Reddit-finetuned models generate safer but rhetorically rigid arguments, while cross-platform fine-tuning amplifies adversarial tone and toxicity. Prompt-based steering reduces overt toxicity (e.g., personal attacks) but cannot fully offset the influence of noisy training data. We introduce a rhetorical evaluation rubric - covering justification, reciprocity, alignment, and authority - and provide implementation guidelines for authoring, moderation, and deliberation-support systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Large Language Models to Reason in a Continuous Latent Space</title>
<link>https://arxiv.org/abs/2412.06769</link>
<guid>https://arxiv.org/abs/2412.06769</guid>
<content:encoded><![CDATA[
arXiv:2412.06769v3 Announce Type: replace 
Abstract: Large language models (LLMs) are typically constrained to reason in the language space, where they express the reasoning process through a chain-of-thought (CoT) to solve complex problems. However, the language space may not always be optimal for reasoning. Most word tokens primarily ensure textual coherence and are not essential for reasoning, while some critical tokens require complex planning and pose challenges to LLMs. To explore the potential of reasoning beyond language, we introduce a new paradigm called Coconut (Chain of Continuous Thought). Coconut utilizes the last hidden state of the LLM as a representation of the reasoning state, termed "continuous thought." Instead of decoding this state into words, we feed it back to the model as the next input embedding directly in the continuous space. This latent reasoning paradigm enables an advanced reasoning pattern, where continuous thoughts can encode multiple alternative next steps, allowing the model to perform a breadth-first search (BFS) rather than committing prematurely to a single deterministic path as in CoT. Coconut outperforms CoT on logical reasoning tasks that require substantial search during planning and achieves a better trade-off between accuracy and efficiency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Document Understanding</title>
<link>https://arxiv.org/abs/2502.01341</link>
<guid>https://arxiv.org/abs/2502.01341</guid>
<content:encoded><![CDATA[
arXiv:2502.01341v2 Announce Type: replace 
Abstract: Aligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), lack inductive bias to constrain visual features within the linguistic structure of the LLM's embedding space, making them data-hungry and prone to cross-modal misalignment. In this work, we propose a novel vision-text alignment method, AlignVLM, that maps visual features to a weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. AlignVLM is particularly effective for document understanding tasks, where visual and textual modalities are highly correlated. Our extensive experiments show that AlignVLM achieves state-of-the-art performance compared to prior alignment methods, with larger gains on document understanding tasks and under low-resource setups. We provide further analysis demonstrating its efficiency and robustness to noise.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks</title>
<link>https://arxiv.org/abs/2502.11090</link>
<guid>https://arxiv.org/abs/2502.11090</guid>
<content:encoded><![CDATA[
arXiv:2502.11090v3 Announce Type: replace 
Abstract: With the rapid advancement of Large Language Models (LLMs), the safety of LLMs has been a critical concern requiring precise assessment. Current benchmarks primarily concentrate on single-turn dialogues or a single jailbreak attack method to assess the safety. Additionally, these benchmarks have not taken into account the LLM's capability of identifying and handling unsafe information in detail. To address these issues, we propose a fine-grained benchmark SafeDialBench for evaluating the safety of LLMs across various jailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier hierarchical safety taxonomy that considers 6 safety dimensions and generates more than 4000 multi-turn dialogues in both Chinese and English under 22 dialogue scenarios. We employ 7 jailbreak attack strategies, such as reference attack and purpose reverse, to enhance the dataset quality for dialogue generation. Notably, we construct an innovative assessment framework of LLMs, measuring capabilities in detecting, and handling unsafe information and maintaining consistency when facing jailbreak attacks. Experimental results across 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior safety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety vulnerabilities.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eye Tracking Based Cognitive Evaluation of Automatic Readability Assessment Measures</title>
<link>https://arxiv.org/abs/2502.11150</link>
<guid>https://arxiv.org/abs/2502.11150</guid>
<content:encoded><![CDATA[
arXiv:2502.11150v3 Announce Type: replace 
Abstract: Methods for scoring text readability have been studied for over a century, and are widely used in research and in user-facing applications in many domains. Thus far, the development and evaluation of such methods have primarily relied on two types of offline behavioral data, performance on reading comprehension tests and ratings of text readability levels. In this work, we instead focus on a fundamental and understudied aspect of readability, real-time reading ease, captured with online reading measures using eye tracking. We introduce an evaluation framework for readability scoring methods which quantifies their ability to account for reading ease, while controlling for content variation across texts. Applying this evaluation to prominent traditional readability formulas, modern machine learning systems, frontier Large Language Models and commercial systems used in education, suggests that they are all poor predictors of reading ease in English. This outcome holds across native and non-native speakers, reading regimes, and textual units of different lengths. The evaluation further reveals that existing methods are often outperformed by word properties commonly used in psycholinguistics for prediction of reading times. Our results highlight a fundamental limitation of existing approaches to readability scoring, the utility of psycholinguistics for readability research, and the need for new, cognitively driven readability scoring approaches that can better account for reading ease.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models</title>
<link>https://arxiv.org/abs/2502.11559</link>
<guid>https://arxiv.org/abs/2502.11559</guid>
<content:encoded><![CDATA[
arXiv:2502.11559v3 Announce Type: replace 
Abstract: Pre-training large language models (LLMs) on vast text corpora enhances natural language processing capabilities but risks encoding social biases, particularly gender bias. While parameter-modification methods like fine-tuning mitigate bias, they are resource-intensive, unsuitable for closed-source models, and lack adaptability to evolving societal norms. Instruction-based approaches offer flexibility but often compromise task performance. To address these limitations, we propose $\textbf{FaIRMaker}$, an automated and model-independent framework that employs an $\textbf{auto-search and refinement}$ paradigm to adaptively generate Fairwords, which act as instructions integrated into input queries to reduce gender bias and enhance response quality. Extensive experiments demonstrate that FaIRMaker automatically searches for and dynamically refines Fairwords, effectively mitigating gender bias while preserving task integrity and ensuring compatibility with both API-based and open-source LLMs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps</title>
<link>https://arxiv.org/abs/2502.14829</link>
<guid>https://arxiv.org/abs/2502.14829</guid>
<content:encoded><![CDATA[
arXiv:2502.14829v3 Announce Type: replace 
Abstract: When prompted to think step-by-step, language models (LMs) produce a chain of thought (CoT), a sequence of reasoning steps that the model supposedly used to produce its prediction. Despite much work on CoT prompting, it is unclear if reasoning verbalized in a CoT is faithful to the models' parametric beliefs. We introduce a framework for measuring parametric faithfulness of generated reasoning, and propose Faithfulness by Unlearning Reasoning steps (FUR), an instance of this framework. FUR erases information contained in reasoning steps from model parameters, and measures faithfulness as the resulting effect on the model's prediction. Our experiments with four LMs and five multi-hop multi-choice question answering (MCQA) datasets show that FUR is frequently able to precisely change the underlying models' prediction for a given instance by unlearning key steps, indicating when a CoT is parametrically faithful. Further analysis shows that CoTs generated by models post-unlearning support different answers, hinting at a deeper effect of unlearning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding</title>
<link>https://arxiv.org/abs/2503.01422</link>
<guid>https://arxiv.org/abs/2503.01422</guid>
<content:encoded><![CDATA[
arXiv:2503.01422v3 Announce Type: replace 
Abstract: Test-time scaling enhances large language model performance by allocating additional compute resources during inference. Best-of-N (BoN) sampling serves as a common sampling-based scaling technique, broadening the search space in parallel to find better solutions from the model distribution. However, its cost-performance trade-off is still underexplored. Two main challenges limit the efficiency of BoN sampling: (1) Generating N full samples consumes substantial GPU memory, reducing inference capacity under limited resources. (2) Reward models add extra memory and latency overhead, and training strong reward models introduces potential training data costs. Although some studies have explored efficiency improvements, none have addressed both challenges at once. To address this gap, we propose Self-Truncation Best-of-N (ST-BoN), a decoding method that avoids fully generating all N samples and eliminates the need for reward models. It leverages early sampling consistency in the model's internal states to identify the most promising path and truncate suboptimal ones. In terms of cost, ST-BoN reduces dynamic GPU memory usage by over 80% and inference latency by 50%. In terms of cost-performance trade-off, ST-BoN achieves the same performance as Full-BoN while saving computational cost by 70%-80%, and under the same cost, it can improve accuracy by 3-4 points.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Targeted Distillation for Sentiment Analysis</title>
<link>https://arxiv.org/abs/2503.03225</link>
<guid>https://arxiv.org/abs/2503.03225</guid>
<content:encoded><![CDATA[
arXiv:2503.03225v2 Announce Type: replace 
Abstract: This paper explores targeted distillation methods for sentiment analysis, aiming to build compact and practical models that preserve strong and generalizable sentiment analysis capabilities. To this end, we conceptually decouple the distillation target into knowledge and alignment and accordingly propose a two-stage distillation framework. Moreover, we introduce SentiBench, a comprehensive and systematic sentiment analysis benchmark that covers a diverse set of tasks across 12 datasets. We evaluate a wide range of models on this benchmark. Experimental results show that our approach substantially enhances the performance of compact models across diverse sentiment analysis tasks, and the resulting models demonstrate strong generalization to unseen tasks, showcasing robust competitiveness against existing small-scale models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Medical Hallucinations in Foundation Models and Their Impact on Healthcare</title>
<link>https://arxiv.org/abs/2503.05777</link>
<guid>https://arxiv.org/abs/2503.05777</guid>
<content:encoded><![CDATA[
arXiv:2503.05777v2 Announce Type: replace 
Abstract: Hallucinations in foundation models arise from autoregressive training objectives that prioritize token-likelihood optimization over epistemic accuracy, fostering overconfidence and poorly calibrated uncertainty. We define medical hallucination as any model-generated output that is factually incorrect, logically inconsistent, or unsupported by authoritative clinical evidence in ways that could alter clinical decisions. We evaluated 11 foundation models (7 general-purpose, 4 medical-specialized) across seven medical hallucination tasks spanning medical reasoning and biomedical information retrieval. General-purpose models achieved significantly higher proportions of hallucination-free responses than medical-specialized models (median: 76.6% vs 51.3%, difference = 25.2%, 95% CI: 18.7-31.3%, Mann-Whitney U = 27.0, p = 0.012, rank-biserial r = -0.64). Top-performing models such as Gemini-2.5 Pro exceeded 97% accuracy when augmented with chain-of-thought prompting (base: 87.6%), while medical-specialized models like MedGemma ranged from 28.6-61.9% despite explicit training on medical corpora. Chain-of-thought reasoning significantly reduced hallucinations in 86.4% of tested comparisons after FDR correction (q < 0.05), demonstrating that explicit reasoning traces enable self-verification and error detection. Physician audits confirmed that 64-72% of residual hallucinations stemmed from causal or temporal reasoning failures rather than knowledge gaps. A global survey of clinicians (n = 70) validated real-world impact: 91.8% had encountered medical hallucinations, and 84.7% considered them capable of causing patient harm. The underperformance of medical-specialized models despite domain training indicates that safety emerges from sophisticated reasoning capabilities and broad knowledge integration developed during large-scale pre-training, not from narrow optimization.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XIFBench: Evaluating Large Language Models on Multilingual Instruction Following</title>
<link>https://arxiv.org/abs/2503.07539</link>
<guid>https://arxiv.org/abs/2503.07539</guid>
<content:encoded><![CDATA[
arXiv:2503.07539v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable instruction-following capabilities across various applications. However, their performance in multilingual settings lacks systematic investigation, with existing evaluations lacking fine-grained constraint analysis across diverse linguistic contexts. We introduce XIFBench, a comprehensive constraint-based benchmark for evaluating multilingual instruction-following abilities of LLMs, comprising 558 instructions with 0-5 additional constraints across five categories (Content, Style, Situation, Format, and Numerical) in six languages spanning different resource levels. To support reliable and consistent cross-lingual evaluation, we implement three methodological innovations: cultural accessibility annotation, constraint-level translation validation, and requirement-based evaluation using English requirements as semantic anchors across languages. Extensive experiments with various LLMs not only quantify performance disparities across resource levels but also provide detailed insights into how language resources, constraint categories, instruction complexity, and cultural specificity influence multilingual instruction-following. Our code and data are available at https://github.com/zhenyuli801/XIFBench.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural Language Generation</title>
<link>https://arxiv.org/abs/2503.16728</link>
<guid>https://arxiv.org/abs/2503.16728</guid>
<content:encoded><![CDATA[
arXiv:2503.16728v3 Announce Type: replace 
Abstract: This article provides a brief overview of the field of Natural Language Generation. The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language. That information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for example. As a subfield of Natural Language Processing, NLG is closely related to other sub-disciplines such as Machine Translation (MT) and Dialog Systems. Some NLG researchers exclude MT from their definition of the field, since there is no content selection involved where the system has to determine what to say. Conversely, dialog systems do not typically fall under the header of Natural Language Generation since NLG is just one component of dialog systems (the others being Natural Language Understanding and Dialog Management). However, with the rise of Large Language Models (LLMs), different subfields of Natural Language Processing have converged on similar methodologies for the production of natural language and the evaluation of automatically generated text.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JudgeLRM: Large Reasoning Models as a Judge</title>
<link>https://arxiv.org/abs/2504.00050</link>
<guid>https://arxiv.org/abs/2504.00050</guid>
<content:encoded><![CDATA[
arXiv:2504.00050v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly adopted as evaluators, offering a scalable alternative to human annotation. However, existing supervised fine-tuning (SFT) approaches often fall short in domains that demand complex reasoning. Judgment is inherently reasoning-intensive: beyond surface-level scoring, it requires verifying evidence, identifying errors, and justifying decisions. Through the analysis of evaluation tasks, we find a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples, revealing the limits of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs, trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards to activate reasoning capabilities. JudgeLRM consistently outperform SFT-tuned baselines in the same size, as well as other RL and SFT variants, and even surpass state-of-the-art reasoning models: notably, JudgeLRM-3B/4B exceeds GPT-4, while JudgeLRM-7B/8B/14B outperforms DeepSeek-R1 by over 2% in F1 score, with particularly strong gains on reasoning-heavy tasks. Our findings underscore the value of RL in unlocking reasoning-aligned LLM judges.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech Translation</title>
<link>https://arxiv.org/abs/2504.03546</link>
<guid>https://arxiv.org/abs/2504.03546</guid>
<content:encoded><![CDATA[
arXiv:2504.03546v2 Announce Type: replace 
Abstract: Multilingual speech translation (ST) and machine translation (MT) in the medical domain enhances patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we present the first systematic study on medical ST, to our best knowledge, by releasing MultiMed-ST, a large-scale ST dataset for the medical domain, spanning all translation directions in five languages: Vietnamese, English, German, French, and Simplified/Traditional Chinese, together with the models. With 290,000 samples, this is the largest medical MT dataset and the largest many-to-many multilingual ST among all domains. Secondly, we present the most comprehensive ST analysis in the field's history, to our best knowledge, including: empirical baselines, bilingual-multilingual comparative study, end-to-end vs. cascaded comparative study, task-specific vs. multi-task sequence-to-sequence comparative study, code-switch analysis, and quantitative-qualitative error analysis. All code, data, and models are available online: https://github.com/leduckhai/MultiMed-ST
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do LLM Evaluators Prefer Themselves for a Reason?</title>
<link>https://arxiv.org/abs/2504.03846</link>
<guid>https://arxiv.org/abs/2504.03846</guid>
<content:encoded><![CDATA[
arXiv:2504.03846v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used as automatic evaluators in applications like benchmarking, reward modeling, and self-refinement. Prior work highlights a potential self-preference bias where LLMs favor their own generated responses, a tendency often intensifying with model size and capability. This raises a critical question: Is self-preference harmful, or does it simply reflect the genuinely higher-quality outputs of stronger models? Answering this has been difficult as previous studies relied primarily on subjective tasks. These tasks lack an objective ground truth, meaning that either preference can be reasonably justified. To address this ambiguity, we investigate self-preference using verifiable benchmarks (mathematical reasoning, factual knowledge, code generation) that allow objective ground-truth assessment. This enables us to distinguish harmful self-preference (favoring objectively worse responses) from legitimate self-preference (favoring genuinely superior ones). We conduct large-scale experiments under controlled evaluation conditions across diverse model families (e.g., Llama, Qwen, Gemma, Mistral, Phi, GPT, DeepSeek). Our findings reveal three key insights: (1) While stronger models exhibit greater self-preference, much of this preference aligns with objectively superior performance, indicating stronger models prefer themselves mostly legitimately. (2) Harmful self-preference persists when evaluator models err as generators, and stronger models display more pronounced harmful self-preference when they do err. This suggests stronger models struggle more to recognize when they are wrong. (3) Inference-time scaling strategies, such as generating a long Chain-of-Thought before evaluation, effectively reduce harmful self-preference. These results provide a more nuanced understanding of LLM-based evaluation and practical insights for improving its reliability.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Adaptive Cognitive Debiasing for Large Language Models in Decision-Making</title>
<link>https://arxiv.org/abs/2504.04141</link>
<guid>https://arxiv.org/abs/2504.04141</guid>
<content:encoded><![CDATA[
arXiv:2504.04141v4 Announce Type: replace 
Abstract: Large language models (LLMs) have shown potential in supporting decision-making applications, particularly as personal assistants in the financial, healthcare, and legal domains. While prompt engineering strategies have enhanced the capabilities of LLMs in decision-making, cognitive biases inherent to LLMs present significant challenges. Cognitive biases are systematic patterns of deviation from norms or rationality in decision-making that can lead to the production of inaccurate outputs. Existing cognitive bias mitigation strategies assume that input prompts only contain one type of cognitive bias, limiting their effectiveness in more challenging scenarios involving multiple cognitive biases. To fill this gap, we propose a cognitive debiasing approach, self-adaptive cognitive debiasing (SACD), that enhances the reliability of LLMs by iteratively refining prompts. Our method follows three sequential steps - bias determination, bias analysis, and cognitive debiasing - to iteratively mitigate potential cognitive biases in prompts. We evaluate SACD on finance, healthcare, and legal decision-making tasks using both open-weight and closed-weight LLMs. Compared to advanced prompt engineering methods and existing cognitive debiasing techniques, SACD achieves the lowest average bias scores in both single-bias and multi-bias settings.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Curse of CoT: On the Limitations of Chain-of-Thought in In-Context Learning</title>
<link>https://arxiv.org/abs/2504.05081</link>
<guid>https://arxiv.org/abs/2504.05081</guid>
<content:encoded><![CDATA[
arXiv:2504.05081v2 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) prompting has been widely recognized for its ability to enhance reasoning capabilities in large language models (LLMs). However, our study reveals a surprising contradiction to this prevailing perspective within the fundamental domain of pattern-based in-context learning (ICL). Through extensive experiments involving 16 state-of-the-art LLMs and nine diverse pattern-based ICL datasets, we demonstrate that CoT and its reasoning variants consistently underperform direct answering across varying model scales and benchmark complexities. To systematically investigate this unexpected phenomenon, we designed extensive experiments to validate several hypothetical explanations. Our analysis uncovers a fundamental hybrid mechanism of explicit-implicit reasoning driving CoT's performance in pattern-based ICL: while explicit reasoning falters due to LLMs' struggles to infer underlying patterns from demonstrations, implicit reasoning-disrupted by the increased contextual distance of CoT rationales-often compensates, delivering correct answers despite flawed rationales. This hybrid mechanism explains CoT's relative underperformance, as noise from weak explicit inference undermines the process, even as implicit mechanisms partially salvage outcomes. Notably, even long-CoT reasoning models, which excel in abstract and symbolic reasoning, fail to fully overcome these limitations despite higher computational costs. Our findings challenge existing assumptions regarding the universal efficacy of CoT, yielding novel insights into its limitations and guiding future research toward more nuanced and effective reasoning methodologies for LLMs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Reasoning Abilities of Small LLMs with Cognitive Alignment</title>
<link>https://arxiv.org/abs/2504.09802</link>
<guid>https://arxiv.org/abs/2504.09802</guid>
<content:encoded><![CDATA[
arXiv:2504.09802v2 Announce Type: replace 
Abstract: The reasoning capabilities of large reasoning models (LRMs), such as OpenAI's o1 and DeepSeek-R1, have seen substantial advancements through deep thinking. However, these enhancements come with significant resource demands, underscoring the need for training effective small reasoning models. A critical challenge is that small models possess different reasoning capacities and cognitive trajectories compared with their larger counterparts. Hence, directly distilling chain-of-thought (CoT) rationales from large LRMs to smaller ones can sometimes be ineffective and often requires a substantial amount of annotated data. In this paper, we first introduce a novel Critique-Rethink-Verify (CRV) system, designed for training smaller yet powerful LRMs. Our CRV system consists of multiple LLM agents, each specializing in unique tasks: (i) critiquing the CoT rationales according to the cognitive capabilities of smaller models, (ii) rethinking and refining these CoTs based on the critiques, and (iii) verifying the correctness of the refined results. Building on the CRV system, we further propose the Cognitive Preference Optimization (CogPO) algorithm to continuously enhance the reasoning abilities of smaller models by aligning their reasoning processes with their cognitive capacities. Comprehensive evaluations on challenging reasoning benchmarks demonstrate the efficacy of our CRV+CogPO framework, which outperforms other methods by a large margin.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hardware-aligned Hierarchical Sparse Attention for Efficient Long-term Memory Access</title>
<link>https://arxiv.org/abs/2504.16795</link>
<guid>https://arxiv.org/abs/2504.16795</guid>
<content:encoded><![CDATA[
arXiv:2504.16795v2 Announce Type: replace 
Abstract: A key advantage of Recurrent Neural Networks (RNNs) over Transformers is their linear computational and space complexity enables faster training and inference for long sequences. However, RNNs are fundamentally unable to randomly access historical context, and simply integrating attention mechanisms may undermine their efficiency advantages. To overcome this limitation, we propose Hierarchical Sparse Attention (HSA), a novel attention mechanism that enhances RNNs with long-range random access flexibility while preserving their merits in efficiency and length generalization. HSA divides inputs into chunks, selects the top-$k$ chunks and hierarchically aggregates information. The core innovation lies in learning token-to-chunk relevance based on fine-grained token-level information inside each chunk. This approach enhances the precision of chunk selection across both in-domain and out-of-domain context lengths. To make HSA efficient, we further introduce a hardware-aligned kernel design. By combining HSA with Mamba, we introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64 million contexts despite pre-training on only 4K-length contexts, and significant improvements on various downstream tasks, with nearly constant memory footprint. These results show RAMba's huge potential in long-context modeling.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts</title>
<link>https://arxiv.org/abs/2504.18428</link>
<guid>https://arxiv.org/abs/2504.18428</guid>
<content:encoded><![CDATA[
arXiv:2504.18428v4 Announce Type: replace 
Abstract: In this paper, we introduce PolyMath, a multilingual mathematical reasoning benchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our benchmark ensures difficulty comprehensiveness, language diversity, and high-quality translation, making it a highly discriminative multilingual mathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive evaluation for advanced LLMs and find that even Qwen-3-235B-A22B-Thinking and Gemini-2.5-pro, achieve only 54.6 and 52.2 benchmark scores, with about 40% accuracy under the highest level From a language perspective, our benchmark reveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning performance varies widely across languages for current LLMs; (2) Input-output language consistency is low in reasoning LLMs and may be correlated with performance; (3) The thinking length differs significantly by language for current LLMs. Additionally, we demonstrate that controlling the output language in the instructions has the potential to affect reasoning performance, especially for some low-resource languages, suggesting a promising direction for improving multilingual capabilities in LLMs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JobHop: A Large-Scale Dataset of Career Trajectories</title>
<link>https://arxiv.org/abs/2505.07653</link>
<guid>https://arxiv.org/abs/2505.07653</guid>
<content:encoded><![CDATA[
arXiv:2505.07653v2 Announce Type: replace 
Abstract: Understanding labor market dynamics is essential for policymakers, employers, and job seekers. However, comprehensive datasets that capture real-world career trajectories are scarce. In this paper, we introduce JobHop, a large-scale public dataset derived from anonymized resumes provided by VDAB, the public employment service in Flanders, Belgium. Utilizing Large Language Models (LLMs), we process unstructured resume data to extract structured career information, which is then normalized to standardized ESCO occupation codes using a multi-label classification model. This results in a rich dataset of over 1.67 million work experiences, extracted from and grouped into more than 361,000 user resumes and mapped to standardized ESCO occupation codes, offering valuable insights into real-world occupational transitions. This dataset enables diverse applications, such as analyzing labor market mobility, job stability, and the effects of career breaks on occupational transitions. It also supports career path prediction and other data-driven decision-making processes. To illustrate its potential, we explore key dataset characteristics, including job distributions, career breaks, and job transitions, demonstrating its value for advancing labor market research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>New Encoders for German Trained from Scratch: Comparing ModernGBERT with Converted LLM2Vec Models</title>
<link>https://arxiv.org/abs/2505.13136</link>
<guid>https://arxiv.org/abs/2505.13136</guid>
<content:encoded><![CDATA[
arXiv:2505.13136v2 Announce Type: replace 
Abstract: Encoders remain essential for efficient German NLP and NLU scenarios despite the rise of decoder-only LLMs. This work studies two routes to high-quality German encoders under identical data and training constraints: 1) training from scratch and 2) converting decoders via LLM2Vec. We introduce two resources: ModernGBERT (134M, 1B), fully transparent German encoders in the ModernBERT style, and LL\"aMmleinVec (120M, 1B, 7B), decoder-to-encoder conversions trained with masked next-token prediction, both undergoing a context extension to 8.192 tokens.
  Across SuperGLEBer, ModernGBERT 1B sets a new state of the art (avg 0.808), surpassing GBERT Large (+4%) and the seven-times larger converted 7B model (0.787). On German MTEB after supervised fine-tuning, ModernGBERT 1B (0.551) approaches the converted 7B model (0.557).
  We release all models, checkpoints, datasets, and full training records, and introduce an encoder-adapted QA-NIAH evaluation. All in all, our results provide actionable guidance: when parameter efficiency and latency matter, from-scratch encoders dominate. When a pre-trained decoder exists and compute is a limited, conversion offers an effective alternative. ModernGBERT and LL\"aMmleinVec, including all code, data and intermediary checkpoints are published under a research-only RAIL license.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Editing Across Languages: A Survey of Multilingual Knowledge Editing</title>
<link>https://arxiv.org/abs/2505.14393</link>
<guid>https://arxiv.org/abs/2505.14393</guid>
<content:encoded><![CDATA[
arXiv:2505.14393v2 Announce Type: replace 
Abstract: While Knowledge Editing has been extensively studied in monolingual settings, it remains underexplored in multilingual contexts. This survey systematizes recent research on Multilingual Knowledge Editing (MKE), a growing subdomain of model editing focused on ensuring factual edits generalize reliably across languages. We present a comprehensive taxonomy of MKE methods, covering parameter-based, memory-based, fine-tuning, and hypernetwork approaches. We survey available benchmarks,summarize key findings on method effectiveness and transfer patterns, identify challenges in cross-lingual propagation, and highlight open problems related to language anisotropy, evaluation coverage, and edit scalability. Our analysis consolidates a rapidly evolving area and lays the groundwork for future progress in editable language-aware LLMs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with Large Language Models for Mental Health Counseling</title>
<link>https://arxiv.org/abs/2505.15715</link>
<guid>https://arxiv.org/abs/2505.15715</guid>
<content:encoded><![CDATA[
arXiv:2505.15715v2 Announce Type: replace 
Abstract: Large language models (LLMs) hold significant potential for mental health support, capable of generating empathetic responses and simulating therapeutic conversations. However, existing LLM-based approaches often lack the clinical grounding necessary for real-world psychological counseling, particularly in explicit diagnostic reasoning aligned with standards like the DSM/ICD and incorporating diverse therapeutic modalities beyond basic empathy or single strategies. To address these critical limitations, we propose PsyLLM, the first large language model designed to systematically integrate both diagnostic and therapeutic reasoning for mental health counseling. To develop PsyLLM, we design a novel automated data synthesis pipeline that processes real-world mental health posts collected from Reddit, where users frequently share psychological distress and seek community support. This pipeline processes real-world mental health posts, generates multi-turn dialogue structures, and leverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and multiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate detailed clinical reasoning processes. Rigorous multi-dimensional filtering ensures the generation of high-quality, clinically aligned dialogue data. In addition, we introduce a new benchmark and evaluation protocol, assessing counseling quality across four key dimensions. Our experiments demonstrate that PsyLLM significantly outperforms state-of-the-art baseline models on this benchmark. The model weights and dataset have been publicly released at https://github.com/Emo-gml/PsyLLM.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions</title>
<link>https://arxiv.org/abs/2505.16189</link>
<guid>https://arxiv.org/abs/2505.16189</guid>
<content:encoded><![CDATA[
arXiv:2505.16189v2 Announce Type: replace 
Abstract: This paper is the first investigation of the connection between emotion, embodiment, and everyday language in a large sample of natural language data. We created corpora of body part mentions (BPMs) in online English text (blog posts and tweets). This includes a subset featuring human annotations for the emotions of the person whose body part is mentioned in the text. We show that BPMs are common in personal narratives and tweets (~5% to 10% of posts include BPMs) and that their usage patterns vary markedly by time and %geographic location. Using word-emotion association lexicons and our annotated data, we show that text containing BPMs tends to be more emotionally charged, even when the BPM is not explicitly used to describe a physical reaction to the emotion in the text. Finally, we discover a strong and statistically significant correlation between body-related language and a variety of poorer health outcomes. In sum, we argue that investigating the role of body-part related words in language can open up valuable avenues of future research at the intersection of NLP, the affective sciences, and the study of human wellbeing.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.16782</link>
<guid>https://arxiv.org/abs/2505.16782</guid>
<content:encoded><![CDATA[
arXiv:2505.16782v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown impressive performance on complex tasks through Chain-of-Thought (CoT) reasoning. However, conventional CoT relies on explicitly verbalized intermediate steps, which constrains its broader applicability, particularly in abstract reasoning tasks beyond language. To address this, there has been growing research interest in \textit{latent CoT reasoning}, where the reasoning process is embedded within latent spaces. By decoupling reasoning from explicit language generation, latent CoT offers the promise of richer cognitive representations and facilitates more flexible, faster inference. This paper aims to present a comprehensive overview of this emerging paradigm and establish a systematic taxonomy. We analyze recent advances in methods, categorizing them from token-wise horizontal approaches to layer-wise vertical strategies. We then provide in-depth discussions of these methods, highlighting their design principles, applications, and remaining challenges. We hope that our survey provides a structured foundation for advancing this promising direction in LLM reasoning. The relevant papers will be regularly updated at https://github.com/EIT-NLP/Awesome-Latent-CoT.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally</title>
<link>https://arxiv.org/abs/2505.17048</link>
<guid>https://arxiv.org/abs/2505.17048</guid>
<content:encoded><![CDATA[
arXiv:2505.17048v2 Announce Type: replace 
Abstract: Central banks around the world play a crucial role in maintaining economic stability. Deciphering policy implications in their communications is essential, especially as misinterpretations can disproportionately impact vulnerable populations. To address this, we introduce the World Central Banks (WCB) dataset, the most comprehensive monetary policy corpus to date, comprising over 380k sentences from 25 central banks across diverse geographic regions, spanning 28 years of historical data. After uniformly sampling 1k sentences per bank (25k total) across all available years, we annotate and review each sentence using dual annotators, disagreement resolutions, and secondary expert reviews. We define three tasks: Stance Detection, Temporal Classification, and Uncertainty Estimation, with each sentence annotated for all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on these tasks, running 15,075 benchmarking experiments. We find that a model trained on aggregated data across banks significantly surpasses a model trained on an individual bank's data, confirming the principle "the whole is greater than the sum of its parts." Additionally, rigorous human evaluations, error analyses, and predictive tasks validate our framework's economic utility. Our artifacts are accessible through the HuggingFace and GitHub under the CC-BY-NC-SA 4.0 license.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning</title>
<link>https://arxiv.org/abs/2505.17050</link>
<guid>https://arxiv.org/abs/2505.17050</guid>
<content:encoded><![CDATA[
arXiv:2505.17050v2 Announce Type: replace 
Abstract: Project-Based Learning (PBL) involves a variety of highly correlated multimodal data, making it a vital educational approach within STEM disciplines. With the rapid development of multimodal large language models (MLLMs), researchers have begun exploring their potential to enhance tasks such as information retrieval, knowledge comprehension, and data generation in educational settings. However, existing benchmarks fall short in providing both a free-form output structure and a rigorous human expert validation process, limiting their effectiveness in evaluating real-world educational tasks. Additionally, few methods have developed automated pipelines to assist with the complex responsibilities of teachers leveraging MLLMs, largely due to model hallucination and instability, which lead to unreliable implementation. To address this gap, we introduce PBLBench, a novel benchmark designed to evaluate complex reasoning grounded in domain-specific knowledge and long-context understanding, thereby challenging models with tasks that closely resemble those handled by human experts. To establish reliable ground truth, we adopt the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise comparisons to derive structured and weighted evaluation criteria. We assess the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that even the most advanced models achieve only 59% rank accuracy, underscoring the significant challenges presented by this benchmark. We believe PBLBench will serve as a catalyst for the development of more capable AI agents, ultimately aiming to alleviate teacher workload and enhance educational productivity.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation</title>
<link>https://arxiv.org/abs/2505.17103</link>
<guid>https://arxiv.org/abs/2505.17103</guid>
<content:encoded><![CDATA[
arXiv:2505.17103v2 Announce Type: replace 
Abstract: SDForger is a flexible and efficient framework for generating high-quality multivariate time series using LLMs. Leveraging a compact data representation, SDForger provides synthetic time series generation from a few samples and low-computation fine-tuning of any autoregressive LLM. Specifically, the framework transforms univariate and multivariate signals into tabular embeddings, which are then encoded into text and used to fine-tune the LLM. At inference, new textual embeddings are sampled and decoded into synthetic time series that retain the original data's statistical properties and temporal dynamics. Across a diverse range of datasets, SDForger outperforms existing generative models in many scenarios, both in similarity-based evaluations and downstream forecasting tasks. By enabling textual conditioning in the generation process, SDForger paves the way for multimodal modeling and the streamlined integration of time series with textual information. The model is open-sourced at https://github.com/IBM/fms-dgt/tree/main/fms_dgt/public/databuilders/time_series.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and Citations</title>
<link>https://arxiv.org/abs/2505.17267</link>
<guid>https://arxiv.org/abs/2505.17267</guid>
<content:encoded><![CDATA[
arXiv:2505.17267v3 Announce Type: replace 
Abstract: We introduce GreekBarBench, a benchmark that evaluates LLMs on legal questions across five different legal areas from the Greek Bar exams, requiring citations to statutory articles and case facts. To tackle the challenges of free-text evaluation, we propose a three-dimensional scoring system combined with an LLM-as-a-judge approach. We also develop a meta-evaluation benchmark to assess the correlation between LLM-judges and human expert evaluations, revealing that simple, span-based rubrics improve their alignment. Our systematic evaluation of 13 proprietary and open-weight LLMs shows that even though the best models outperform average expert scores, they fall short of the 95th percentile of experts.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization</title>
<link>https://arxiv.org/abs/2505.19679</link>
<guid>https://arxiv.org/abs/2505.19679</guid>
<content:encoded><![CDATA[
arXiv:2505.19679v2 Announce Type: replace 
Abstract: This paper presents KIT's submissions to the IWSLT 2025 low-resource track. We develop both cascaded systems, consisting of Automatic Speech Recognition (ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech Translation (ST) systems for three language pairs: Bemba, North Levantine Arabic, and Tunisian Arabic into English. Building upon pre-trained models, we fine-tune our systems with different strategies to utilize resources efficiently. This study further explores system enhancement with synthetic data and model regularization. Specifically, we investigate MT-augmented ST by generating translations from ASR data using MT models. For North Levantine, which lacks parallel ST training data, a system trained solely on synthetic data slightly surpasses the cascaded system trained on real data. We also explore augmentation using text-to-speech models by generating synthetic speech from MT data, demonstrating the benefits of synthetic data in improving both ASR and ST performance for Bemba. Additionally, we apply intra-distillation to enhance model performance. Our experiments show that this approach consistently improves results across ASR, MT, and ST tasks, as well as across different pre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine the cascaded and end-to-end systems, achieving an improvement of approximately 1.5 BLEU points.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Hidden Capacity of LLMs for One-Step Text Generation</title>
<link>https://arxiv.org/abs/2505.21189</link>
<guid>https://arxiv.org/abs/2505.21189</guid>
<content:encoded><![CDATA[
arXiv:2505.21189v2 Announce Type: replace 
Abstract: A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one trained input embedding. In this work, we explore whether autoregressive decoding is essential for such reconstruction. We show that frozen LLMs can generate hundreds of accurate tokens in just one token-parallel forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored multi-token generation capability of autoregressive LLMs. We examine these embeddings and characterize the information they encode. We also empirically show that, although these representations are not unique for a given text, they form connected and local regions in embedding space - suggesting the potential to train a practical encoder. The existence of such representations hints that multi-token generation may be natively accessible in off-the-shelf LLMs via a learned input encoder, eliminating heavy retraining and helping to overcome the fundamental bottleneck of autoregressive decoding while reusing already-trained models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Expert Specialization for Better MoE</title>
<link>https://arxiv.org/abs/2505.22323</link>
<guid>https://arxiv.org/abs/2505.22323</guid>
<content:encoded><![CDATA[
arXiv:2505.22323v3 Announce Type: replace 
Abstract: Mixture-of-Experts (MoE) models enable efficient scaling of large language models (LLMs) by activating only a subset of experts per input. However, we observe that the commonly used auxiliary load balancing loss often leads to expert overlap and overly uniform routing, which hinders expert specialization and degrades overall performance during post-training. To address this, we propose a simple yet effective solution that introduces two complementary objectives: (1) an orthogonality loss to encourage experts to process distinct types of tokens, and (2) a variance loss to encourage more discriminative routing decisions. Gradient-level analysis demonstrates that these objectives are compatible with the existing auxiliary loss and contribute to optimizing the training process. Experimental results over various model architectures and across multiple benchmarks show that our method significantly enhances expert specialization. Notably, our method improves classic MoE baselines with auxiliary loss by up to 23.79%, while also maintaining load balancing in downstream tasks, without any architectural modifications or additional components. We will release our code to contribute to the community.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatial Knowledge Graph-Guided Multimodal Synthesis</title>
<link>https://arxiv.org/abs/2505.22633</link>
<guid>https://arxiv.org/abs/2505.22633</guid>
<content:encoded><![CDATA[
arXiv:2505.22633v2 Announce Type: replace 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced their capabilities; however, their spatial perception abilities remain a notable limitation. To address this challenge, multimodal data synthesis offers a promising solution. Yet, ensuring that synthesized data adhere to spatial common sense is a non-trivial task. Our approach addresses this critical gap by providing a systematic framework for generating spatially coherent data. In this work, we introduce SKG2DATA, a novel multimodal synthesis approach guided by spatial knowledge graphs, grounded in the concept of knowledge-to-data generation. SKG2DATA employs an automated pipeline for constructing Spatial Knowledge Graph (SKG) that effectively captures human-like spatial cognition, including directional and distance relationships. These structured representations then serve as precise guidance for our integrated synthesis pipeline, where a diffusion model generates spatially-consistent images while a MLLM produces corresponding textual descriptions. The automated construction of SKG enables scalable generation of diverse yet realistic spatial configurations, overcoming the limitations of manual data collection and annotation. Extensive experiments demonstrate that data synthesized from diverse types of spatial knowledge, including direction and distance, enhance the spatial perception and reasoning abilities of MLLMs markedly, albeit with a slight cost to their general capabilities. We hope that the idea of knowledge-based data synthesis can advance the development of spatial intelligence. Code is available at https://github.com/zjunlp/Knowledge2Data.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios</title>
<link>https://arxiv.org/abs/2505.23118</link>
<guid>https://arxiv.org/abs/2505.23118</guid>
<content:encoded><![CDATA[
arXiv:2505.23118v3 Announce Type: replace 
Abstract: Effective clinical decision-making depends on iterative, multimodal reasoning across diverse sources of evidence. The recent emergence of multimodal reasoning models has significantly transformed the landscape of solving complex tasks. Although such models have achieved notable success in mathematics and science, their application to medical domains remains underexplored. In this work, we propose \textit{MedE$^2$}, a two-stage post-training pipeline that elicits and then enhances multimodal reasoning for medical domains. In Stage-I, we fine-tune models using 2,000 text-only data samples containing precisely orchestrated reasoning demonstrations to elicit reasoning behaviors. In Stage-II, we further enhance the model's reasoning capabilities using 1,500 rigorously curated multimodal medical cases, aligning model reasoning outputs with our proposed multimodal medical reasoning preference. Extensive experiments demonstrate the efficacy and reliability of \textit{MedE$^2$} in improving the reasoning performance of medical multimodal models. Notably, models trained with \textit{MedE$^2$} consistently outperform baselines across multiple medical multimodal benchmarks. Additional validation on larger models and under inference-time scaling further confirms the robustness and practical utility of our approach.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models</title>
<link>https://arxiv.org/abs/2505.23945</link>
<guid>https://arxiv.org/abs/2505.23945</guid>
<content:encoded><![CDATA[
arXiv:2505.23945v2 Announce Type: replace 
Abstract: Chain-of-thought (CoT) reasoning enhances performance of large language models, but questions remain about whether these reasoning traces faithfully reflect the internal processes of the model. We present the first comprehensive study of CoT faithfulness in large vision-language models (LVLMs), investigating how both text-based and previously unexplored image-based biases affect reasoning and bias articulation. Our work introduces a novel, fine-grained evaluation pipeline for categorizing bias articulation patterns, enabling significantly more precise analysis of CoT reasoning than previous methods. This framework reveals critical distinctions in how models process and respond to different types of biases, providing new insights into LVLM CoT faithfulness. Our findings reveal that subtle image-based biases are rarely articulated compared to explicit text-based ones, even in models specialized for reasoning. Additionally, many models exhibit a previously unidentified phenomenon we term ``inconsistent'' reasoning - correctly reasoning before abruptly changing answers, serving as a potential canary for detecting biased reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to revisit CoT faithfulness in LLMs across various levels of implicit cues. Our findings reveal that current language-only reasoning models continue to struggle with articulating cues that are not overtly stated.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthy Medical Question Answering: An Evaluation-Centric Survey</title>
<link>https://arxiv.org/abs/2506.03659</link>
<guid>https://arxiv.org/abs/2506.03659</guid>
<content:encoded><![CDATA[
arXiv:2506.03659v2 Announce Type: replace 
Abstract: Trustworthiness in healthcare question-answering (QA) systems is important for ensuring patient safety, clinical effectiveness, and user confidence. As large language models (LLMs) become increasingly integrated into medical settings, the reliability of their responses directly influences clinical decision-making and patient outcomes. However, achieving comprehensive trustworthiness in medical QA poses significant challenges due to the inherent complexity of healthcare data, the critical nature of clinical scenarios, and the multifaceted dimensions of trustworthy AI. In this survey, we systematically examine six key dimensions of trustworthiness in medical QA, i.e., Factuality, Robustness, Fairness, Safety, Explainability, and Calibration. We review how each dimension is evaluated in existing LLM-based medical QA systems. We compile and compare major benchmarks designed to assess these dimensions and analyze evaluation-guided techniques that drive model improvements, such as retrieval-augmented grounding, adversarial fine-tuning, and safety alignment. Finally, we identify open challenges-such as scalable expert evaluation, integrated multi-dimensional metrics, and real-world deployment studies-and propose future research directions to advance the safe, reliable, and transparent deployment of LLM-powered medical QA.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transferring Linear Features Across Language Models With Model Stitching</title>
<link>https://arxiv.org/abs/2506.06609</link>
<guid>https://arxiv.org/abs/2506.06609</guid>
<content:encoded><![CDATA[
arXiv:2506.06609v3 Announce Type: replace 
Abstract: In this work, we demonstrate that affine mappings between residual streams of language models is a cheap way to effectively transfer represented features between models. We apply this technique to transfer the weights of Sparse Autoencoders (SAEs) between models of different sizes to compare their representations. We find that small and large models learn similar representation spaces, which motivates training expensive components like SAEs on a smaller model and transferring to a larger model at a FLOPs savings. In particular, using a small-to-large transferred SAE as initialization can lead to 50% cheaper training runs when training SAEs on larger models. Next, we show that transferred probes and steering vectors can effectively recover ground truth performance. Finally, we dive deeper into feature-level transferability, finding that semantic and structural features transfer noticeably differently while specific classes of functional features have their roles faithfully mapped. Overall, our findings illustrate similarities and differences in the linear representation spaces of small and large models and demonstrate a method for improving the training efficiency of SAEs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instructing Large Language Models for Low-Resource Languages: A Systematic Study for Basque</title>
<link>https://arxiv.org/abs/2506.07597</link>
<guid>https://arxiv.org/abs/2506.07597</guid>
<content:encoded><![CDATA[
arXiv:2506.07597v2 Announce Type: replace 
Abstract: Instructing language models with user intent requires large instruction datasets, which are only available for a limited set of languages. In this paper, we explore alternatives to conventional instruction adaptation pipelines in low-resource scenarios. We assume a realistic scenario for low-resource languages, where only the following are available: corpora in the target language, existing open-weight multilingual base and instructed backbone LLMs, and synthetically generated instructions sampled from the instructed backbone. We present a comprehensive set of experiments for Basque that systematically study different combinations of these components evaluated on benchmarks and human preferences from 1,680 participants. Our conclusions show that target language corpora are essential, with synthetic instructions yielding robust models, and, most importantly, that using as backbone an instruction-tuned model outperforms using a base non-instructed model. Scaling up to Llama 3.1 Instruct 70B as backbone, our model comes near frontier models of much larger sizes for Basque, without using any Basque instructions. We release code, models, instruction datasets, and human preferences to support full reproducibility in future research on low-resource language adaptation. https://github.com/hitz-zentroa/latxa-instruct
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification</title>
<link>https://arxiv.org/abs/2506.07801</link>
<guid>https://arxiv.org/abs/2506.07801</guid>
<content:encoded><![CDATA[
arXiv:2506.07801v3 Announce Type: replace 
Abstract: We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm combining the paradigms of co-training and consistency regularization with pseudo-labeling. At its core, MultiMatch features a pseudo-label weighting module designed for selecting and filtering pseudo-labels based on head agreement and model confidence, and weighting them according to the perceived classification difficulty. This novel module enhances and unifies three existing techniques -- heads agreement from Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average Pseudo-Margins from MarginMatch -- resulting in a holistic approach that improves robustness and performance in SSL settings. Experimental results on benchmark datasets highlight the superior performance of MultiMatch, i.e., MultiMatch achieves state-of-the-art results on 8 out of 10 setups from 5 natural language processing datasets and ranks first according to the Friedman test among 21 methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly imbalanced settings, outperforming the second-best approach by 3.26%, a critical advantage for real-world text classification tasks. Our code is available on GitHub.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation Consistency for Accurate and Coherent LLM Answer Aggregation</title>
<link>https://arxiv.org/abs/2506.21590</link>
<guid>https://arxiv.org/abs/2506.21590</guid>
<content:encoded><![CDATA[
arXiv:2506.21590v2 Announce Type: replace 
Abstract: Test-time scaling improves large language models' (LLMs) performance by allocating more compute budget during inference. To achieve this, existing methods often require intricate modifications to prompting and sampling strategies. In this work, we introduce representation consistency (RC), a test-time scaling method for aggregating answers drawn from multiple candidate responses of an LLM regardless of how they were generated, including variations in prompt phrasing and sampling strategy. RC enhances answer aggregation by not only considering the number of occurrences of each answer in the candidate response set, but also the consistency of the model's internal activations while generating the set of responses leading to each answer. These activations can be either dense (raw model activations) or sparse (encoded via pretrained sparse autoencoders). Our rationale is that if the model's representations of multiple responses converging on the same answer are highly variable, this answer is more likely to be the result of incoherent reasoning and should be down-weighted during aggregation. Importantly, our method only uses cached activations and lightweight similarity computations and requires no additional model queries. Through experiments with four open-source LLMs and four reasoning datasets, we validate the effectiveness of RC for improving task performance during inference, with consistent accuracy improvements (up to 4%) over strong test-time scaling baselines. We also show that consistency in the sparse activation signals aligns well with the common notion of coherent reasoning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discourse Heuristics For Paradoxically Moral Self-Correction</title>
<link>https://arxiv.org/abs/2507.00985</link>
<guid>https://arxiv.org/abs/2507.00985</guid>
<content:encoded><![CDATA[
arXiv:2507.00985v2 Announce Type: replace 
Abstract: Moral self-correction has emerged as a promising approach for aligning the output of Large Language Models (LLMs) with human moral values. However, moral self-correction techniques are subject to two primary paradoxes. First, despite empirical and theoretical evidence to support the effectiveness of self-correction, this LLM capability only operates at a superficial level. Second, while LLMs possess the capability of self-diagnosing immoral aspects of their output, they struggle to identify the cause of this moral inconsistency during their self-correction process. To better understand and address these paradoxes, we analyze the discourse constructions in fine-tuning corpora designed to enhance moral self-correction, uncovering the existence of the heuristics underlying effective constructions. We demonstrate that moral self-correction relies on discourse constructions that reflect heuristic shortcuts, and that the presence of these heuristic shortcuts during self-correction leads to inconsistency when attempting to enhance both self-correction and self-diagnosis capabilities jointly. Based on our findings, we propose a solution to improve moral self-correction by leveraging the heuristics of curated datasets. We also highlight the generalization challenges of this capability, particularly in terms of learning from situated context and model scales.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context Tuning for In-Context Optimization</title>
<link>https://arxiv.org/abs/2507.04221</link>
<guid>https://arxiv.org/abs/2507.04221</guid>
<content:encoded><![CDATA[
arXiv:2507.04221v2 Announce Type: replace 
Abstract: We introduce Context Tuning, a simple and effective method to significantly enhance few-shot adaptation of language models (LLMs) without fine-tuning model parameters. While prompt-based adaptation techniques have demonstrated the effectiveness of lightweight adaptation methods for LLMs, they typically initialize a trainable prompt or prefix with irrelevant tokens for the task at hand. In contrast, Context Tuning initializes the trainable prompt or prefix with task-specific demonstration examples, leveraging the model's inherent In-Context Learning (ICL) ability to extract relevant information for improved few-shot learning performance. Extensive evaluations on benchmarks such as CrossFit, UnifiedQA, MMLU, BIG-Bench Hard, and ARC demonstrate that Context Tuning outperforms traditional prompt-based adaptation methods and achieves competitive accuracy to Test-Time Training with significantly higher training efficiency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study</title>
<link>https://arxiv.org/abs/2507.05362</link>
<guid>https://arxiv.org/abs/2507.05362</guid>
<content:encoded><![CDATA[
arXiv:2507.05362v2 Announce Type: replace 
Abstract: Recent advances in natural language processing highlight two key factors for improving reasoning in large language models (LLMs): (i) allocating more test-time compute tends to help on harder problems but often introduces redundancy in the reasoning trace, and (ii) compute is most effective when reasoning is systematic and incremental, forming structured chains of thought (CoTs) akin to human problem-solving. To study these factors in isolation, we introduce a controlled setting based on shortest-path tasks in layered graphs. We train decoder-only transformers on question-trace-answer triples using a custom tokenizer, comparing models trained on optimal bottom-up dynamic programming traces with those trained on longer, valid traces involving backtracking. Surprisingly, with the same training-token budget, models trained on inefficient traces generalize better to unseen graphs. This benefit is not due to length alone-injecting arbitrary redundancy into reasoning traces fails to help and can even hurt performance. Instead, we find that generalization correlates with the model's confidence in next-token prediction, suggesting that long, coherent, and locally incremental traces make the training signal easier to optimize.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains</title>
<link>https://arxiv.org/abs/2507.07229</link>
<guid>https://arxiv.org/abs/2507.07229</guid>
<content:encoded><![CDATA[
arXiv:2507.07229v2 Announce Type: replace 
Abstract: We present SynthTextEval, a toolkit for conducting comprehensive evaluations of synthetic text. The fluency of large language model (LLM) outputs has made synthetic text potentially viable for numerous applications, such as reducing the risks of privacy violations in the development and deployment of AI systems in high-stakes domains. Realizing this potential, however, requires principled consistent evaluations of synthetic data across multiple dimensions: its utility in downstream systems, the fairness of these systems, the risk of privacy leakage, general distributional differences from the source text, and qualitative feedback from domain experts. SynthTextEval allows users to conduct evaluations along all of these dimensions over synthetic data that they upload or generate using the toolkit's generation module. While our toolkit can be run over any data, we highlight its functionality and effectiveness over datasets from two high-stakes domains: healthcare and law. By consolidating and standardizing evaluation metrics, we aim to improve the viability of synthetic text, and in-turn, privacy-preservation in AI development.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Exploration of Knowledge Editing for Arabic</title>
<link>https://arxiv.org/abs/2507.09629</link>
<guid>https://arxiv.org/abs/2507.09629</guid>
<content:encoded><![CDATA[
arXiv:2507.09629v2 Announce Type: replace 
Abstract: While Knowledge Editing (KE) has been widely explored in English, its behavior in morphologically rich languages like Arabic remains underexamined. In this work, we present the first study of Arabic KE. We evaluate four methods (ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact benchmarks, analyzing both multilingual and cross-lingual settings. Our experiments on Llama-2-7B-chat show that parameter-based methods struggle with cross-lingual generalization, while instruction-tuned methods perform more robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show that joint Arabic-English training improves both editability and transfer. We release Arabic KE benchmarks and multilingual training for LTE data to support future research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care</title>
<link>https://arxiv.org/abs/2507.14681</link>
<guid>https://arxiv.org/abs/2507.14681</guid>
<content:encoded><![CDATA[
arXiv:2507.14681v2 Announce Type: replace 
Abstract: Background: Medical coding structures healthcare data for research, quality monitoring, and policy. This study assesses the potential of large language models (LLMs) to assign ICPC-2 codes using the output of a domain-specific search engine.
  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's text-embedding-3-large) retrieved candidates from 73,563 labeled concepts. Thirty-three LLMs were prompted with each query and retrieved results to select the best-matching ICPC-2 code. Performance was evaluated using F1-score, along with token usage, cost, response time, and format adherence.
  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever optimization can improve performance by up to 4 points. Most models returned valid codes in the expected format, with reduced hallucinations. Smaller models (<3B) struggled with formatting and input length.
  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even without fine-tuning. This work offers a benchmark and highlights challenges, but findings are limited by dataset scope and setup. Broader, multilingual, end-to-end evaluations are needed for clinical validation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation</title>
<link>https://arxiv.org/abs/2507.22608</link>
<guid>https://arxiv.org/abs/2507.22608</guid>
<content:encoded><![CDATA[
arXiv:2507.22608v3 Announce Type: replace 
Abstract: Large language models (LLMs) exhibit strong multilingual abilities, yet the neural mechanisms behind language-specific processing remain unclear. We analyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and Aya-Expanse-8B & 32B across 21 typologically diverse languages, identifying neurons that control language behavior. Using the Language Activation Probability Entropy (LAPE) method, we show that these neurons cluster in deeper layers, with non-Latin scripts showing greater specialization. Related languages share overlapping neurons, reflecting internal representations of linguistic proximity.
  Through language arithmetics, i.e. systematic activation addition and multiplication, we steer models to deactivate unwanted languages and activate desired ones, outperforming simpler replacement approaches. These interventions effectively guide behavior across five multilingual tasks: language forcing, translation, QA, comprehension, and NLI. Manipulation is more successful for high-resource languages, while typological similarity improves effectiveness. We also demonstrate that cross-lingual neuron steering enhances downstream performance and reveal internal "fallback" mechanisms for language selection when neurons are progressively deactivated. Our code is made publicly available at https://github.com/d-gurgurov/Language-Neurons-Manipulation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mafoko: Structuring and Building Open Multilingual Terminologies for South African NLP</title>
<link>https://arxiv.org/abs/2508.03529</link>
<guid>https://arxiv.org/abs/2508.03529</guid>
<content:encoded><![CDATA[
arXiv:2508.03529v3 Announce Type: replace 
Abstract: The critical lack of structured terminological data for South Africa's official languages hampers progress in multilingual NLP, despite the existence of numerous government and academic terminology lists. These valuable assets remain fragmented and locked in non-machine-readable formats, rendering them unusable for computational research and development. Mafoko addresses this challenge by systematically aggregating, cleaning, and standardising these scattered resources into open, interoperable datasets. We introduce the foundational Mafoko dataset, released under the equitable, Africa-centered NOODL framework. To demonstrate its immediate utility, we integrate the terminology into a Retrieval-Augmented Generation (RAG) pipeline. Experiments show substantial improvements in the accuracy and domain-specific consistency of English-to-Tshivenda machine translation for large language models. Mafoko provides a scalable foundation for developing robust and equitable NLP technologies, ensuring South Africa's rich linguistic diversity is represented in the digital age.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency</title>
<link>https://arxiv.org/abs/2508.14314</link>
<guid>https://arxiv.org/abs/2508.14314</guid>
<content:encoded><![CDATA[
arXiv:2508.14314v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, but they remain susceptible to hallucinations--generating content that appears plausible but contains factual inaccuracies. We present Finch-Zk, a black-box framework that leverages fine-grained cross-model consistency to detect and mitigate hallucinations in LLM outputs without requiring external knowledge sources. Finch-Zk introduces two key innovations: 1) a cross-model consistency checking strategy that reveals fine-grained inaccuracies by comparing responses generated by diverse models from semantically-equivalent prompts, and 2) a targeted mitigation technique that applies precise corrections to problematic segments while preserving accurate content. Experiments on the FELM dataset show Finch-Zk improves hallucination detection F1 scores by 6-39\% compared to existing approaches. For mitigation, Finch-Zk achieves up to 9 absolute percentage points improvement in answer accuracy on the GPQA-diamond dataset when applied to state-of-the-art models like Llama 4 Maverick and Claude 4 Sonnet. Extensive evaluation on multiple datasets demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for enhancing factual reliability in production LLM systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From BERT to LLMs: Comparing and Understanding Chinese Classifier Prediction in Language Models</title>
<link>https://arxiv.org/abs/2508.18253</link>
<guid>https://arxiv.org/abs/2508.18253</guid>
<content:encoded><![CDATA[
arXiv:2508.18253v2 Announce Type: replace 
Abstract: Classifiers are an important and defining feature of the Chinese language, and their correct prediction is key to numerous educational applications. Yet, whether the most popular Large Language Models (LLMs) possess proper knowledge the Chinese classifiers is an issue that has largely remain unexplored in the Natural Language Processing (NLP) literature.
  To address such a question, we employ various masking strategies to evaluate the LLMs' intrinsic ability, the contribution of different sentence elements, and the working of the attention mechanisms during prediction. Besides, we explore fine-tuning for LLMs to enhance the classifier performance.
  Our findings reveal that LLMs perform worse than BERT, even with fine-tuning. The prediction, as expected, greatly benefits from the information about the following noun, which also explains the advantage of models with a bidirectional attention mechanism such as BERT.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews</title>
<link>https://arxiv.org/abs/2509.00285</link>
<guid>https://arxiv.org/abs/2509.00285</guid>
<content:encoded><![CDATA[
arXiv:2509.00285v2 Announce Type: replace 
Abstract: We study the problem of opinion highlights generation from large volumes of user reviews, often exceeding thousands per entity, where existing methods either fail to scale or produce generic, one-size-fits-all summaries that overlook personalized needs. To tackle this, we introduce OpinioRAG, a scalable, training-free framework that combines RAG-based evidence retrieval with LLMs to efficiently produce tailored summaries. Additionally, we propose novel reference-free verification metrics designed for sentiment-rich domains, where accurately capturing opinions and sentiment alignment is essential. These metrics offer a fine-grained, context-sensitive assessment of factual consistency. To facilitate evaluation, we contribute the first large-scale dataset of long-form user reviews, comprising entities with over a thousand reviews each, paired with unbiased expert summaries and manually annotated queries. Through extensive experiments, we identify key challenges, provide actionable insights into improving systems, pave the way for future research, and position OpinioRAG as a robust framework for generating accurate, relevant, and structured summaries at scale.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verbalized Algorithms</title>
<link>https://arxiv.org/abs/2509.08150</link>
<guid>https://arxiv.org/abs/2509.08150</guid>
<content:encoded><![CDATA[
arXiv:2509.08150v2 Announce Type: replace 
Abstract: Instead of querying LLMs in a one-shot manner and hoping to get the right answer for a reasoning task, we propose a paradigm we call \emph{verbalized algorithms} (VAs), which leverage classical algorithms with established theoretical understanding. VAs decompose a task into simple elementary operations on natural language strings that they should be able to answer reliably, and limit the scope of LLMs to only those simple tasks. For example, for sorting a series of natural language strings, \emph{verbalized sorting} uses an LLM as a binary comparison oracle in a known and well-analyzed sorting algorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of this approach on sorting and clustering tasks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning</title>
<link>https://arxiv.org/abs/2509.13790</link>
<guid>https://arxiv.org/abs/2509.13790</guid>
<content:encoded><![CDATA[
arXiv:2509.13790v2 Announce Type: replace 
Abstract: Efficient instruction tuning aims to enhance the ultimate performance of large language models (LLMs) trained on a given instruction dataset. Curriculum learning as a typical data organization strategy has shown preliminary effectiveness in instruction tuning. However, current curriculum tuning methods suffer from the curriculum rigidity, since they rely solely on static heuristic difficulty metrics. These methods fail to adapt to the evolving capabilities of models during training, resulting in a fixed and potentially sub-optimal learning trajectory. To address the issue, Competence-Aware Multi-Perspective cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS offers several advantages: (1) Dynamic selection for sub-curriculum. (2) Competency-aware adjustment to the curriculum schedule. (3) Multiple difficulty-based scheduling. Extensive experiments prove the superior performance of CAMPUS, compared to other state-of-the-art baselines for efficient instruction tuning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses</title>
<link>https://arxiv.org/abs/2509.16093</link>
<guid>https://arxiv.org/abs/2509.16093</guid>
<content:encoded><![CDATA[
arXiv:2509.16093v2 Announce Type: replace 
Abstract: Evaluating long-form answers in high-stakes domains such as law or medicine remains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to capture semantic correctness, and current LLM-based evaluators often reduce nuanced aspects of answer quality into a single undifferentiated score. We introduce DeCE, a decomposed LLM evaluation framework that separates precision (factual accuracy and relevance) and recall (coverage of required concepts), using instance-specific criteria automatically extracted from gold answer requirements. DeCE is model-agnostic and domain-general, requiring no predefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate different LLMs on a real-world legal QA task involving multi-jurisdictional reasoning and citation grounding. DeCE achieves substantially stronger correlation with expert judgments ($r=0.78$), compared to traditional metrics ($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional evaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist models favor recall, while specialized models favor precision. Importantly, only 11.95% of LLM-generated criteria required expert revision, underscoring DeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation framework in expert domains.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmbeddingGemma: Powerful and Lightweight Text Representations</title>
<link>https://arxiv.org/abs/2509.20354</link>
<guid>https://arxiv.org/abs/2509.20354</guid>
<content:encoded><![CDATA[
arXiv:2509.20354v3 Announce Type: replace 
Abstract: We introduce EmbeddingGemma, a new lightweight, open text embedding model based on the Gemma 3 language model family. Our innovative training recipe strategically captures knowledge from larger models via encoder-decoder initialization and geometric embedding distillation. We improve model robustness and expressiveness with a spread-out regularizer, and ensure generalizability by merging checkpoints from varied, optimized mixtures. Evaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual, English, and code domains, EmbeddingGemma (300M) achieves state-of-the-art results. Notably, it outperforms prior top models, both proprietary and open, with fewer than 500M parameters, and provides performance comparable to models double its size, offering an exceptional performance-to-cost ratio. Remarkably, this lead persists when quantizing model weights or truncating embedding outputs. This makes EmbeddingGemma particularly well-suited for low-latency and high-throughput use cases such as on-device applications. We provide ablation studies exploring our key design choices. We release EmbeddingGemma to the community to promote further research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse</title>
<link>https://arxiv.org/abs/2510.01258</link>
<guid>https://arxiv.org/abs/2510.01258</guid>
<content:encoded><![CDATA[
arXiv:2510.01258v2 Announce Type: replace 
Abstract: Amidst the rapid normalization of generative artificial intelligence (GAI), intelligent systems have come to dominate political discourse across information media. However, internalized political biases stemming from training data skews, human prejudice, and algorithmic flaws continue to plague this novel technology. This study employs a zero-shot classification approach to evaluate algorithmic political partisanship through a methodical combination of ideological alignment, topicality, response sentiment, and objectivity. A total of 1800 model responses across six mainstream large language models (LLMs) were individually input into four distinct fine-tuned classification algorithms, each responsible for computing one of the aforementioned metrics. The results show an amplified liberal-authoritarian alignment across the six LLMs evaluated, with notable instances of reasoning supersessions and canned refusals. The study subsequently highlights the psychological influences underpinning human-computer interactions and how intrinsic biases can permeate public discourse. The resulting distortion of the political landscape can ultimately manifest as conformity or polarization, depending on the region's pre-existing socio-political structures.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time</title>
<link>https://arxiv.org/abs/2510.04340</link>
<guid>https://arxiv.org/abs/2510.04340</guid>
<content:encoded><![CDATA[
arXiv:2510.04340v4 Announce Type: replace 
Abstract: Language model finetuning often results in learning undesirable traits in combination with desired ones. To address this, we propose inoculation prompting: modifying finetuning data by prepending a short system-prompt instruction that deliberately elicits the undesirable trait. At test time, we evaluate without the instruction; inoculated models have much lower expression of the trait than models trained with unmodified training data. Inoculation is selective: in a toy setting where assistant responses are always in Spanish and ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'') teaches the model to capitalize responses while still responding in English. We find that inoculation is also effective across several additional settings: reducing emergent misalignment (EM) from task-specific finetuning, defending against backdoor injections, and mitigating the transmission of traits via subliminal learning. Follow-up analysis suggests a mechanism: making a trait less surprising via inoculation reduces optimization pressure to globally update the model, thereby reducing the degree of generalization. Our analysis relates to prior work on EM: inoculation explains prior findings that educational contexts mitigate EM from insecure code. Beyond demonstrating a simple and effective technique for selective learning, our results contribute to a better conceptual understanding of how and why language models generalize.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Topic Evolution with Temporal Decay and Attention in Large Language Models</title>
<link>https://arxiv.org/abs/2510.10613</link>
<guid>https://arxiv.org/abs/2510.10613</guid>
<content:encoded><![CDATA[
arXiv:2510.10613v2 Announce Type: replace 
Abstract: This paper proposes a modeling framework for dynamic topic evolution based on temporal large language models. The method first uses a large language model to obtain contextual embeddings of text and then introduces a temporal decay function and an attention mechanism. These components allow the model to adjust the importance of semantic units according to time intervals and capture topic variations across different periods. The temporal representations are then mapped into a latent topic space, where a state transition matrix is applied to describe the dynamic evolution of topics. A joint optimization objective constrains both semantic modeling and temporal consistency, ensuring diversity and smoothness in topic generation. The design emphasizes the unified modeling of semantic representation and temporal evolution, which improves topic coherence and diversity while enhancing stability and interpretability over time. Experiments on real-world corpora show that the framework effectively captures the generation, expansion, and decline of topics and outperforms existing models across multiple metrics. Overall, the proposed method provides a systematic solution for understanding dynamic semantic patterns in large-scale text, enriches the research paradigm of topic modeling, and supports complex text analysis tasks in multiple domains.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts</title>
<link>https://arxiv.org/abs/2510.13500</link>
<guid>https://arxiv.org/abs/2510.13500</guid>
<content:encoded><![CDATA[
arXiv:2510.13500v2 Announce Type: replace 
Abstract: LLMs hold great promise for healthcare applications, but the rapid evolution of medical knowledge and errors in training data often cause them to generate outdated or inaccurate information, limiting their applicability in high-stakes clinical practice. Model editing has emerged as a potential remedy without full retraining. While parameter-based editing often compromises locality and is thus ill-suited for the medical domain, retrieval-based editing offers a more viable alternative. However, it still faces two critical challenges: (1) representation overlap within the medical knowledge space often causes inaccurate retrieval and reduces editing accuracy; (2) existing methods are restricted to single-sample edits, while batch-editing remains largely unexplored despite its importance for real-world medical applications. To address these challenges, we first construct MedVersa, an enhanced benchmark with broader coverage of medical subjects, designed to evaluate both single and batch edits under strict locality constraints. We then propose MedREK, a retrieval-based editing framework that integrates a shared query-key module for precise matching with an attention-based prompt encoder for informative guidance. Experimental results on various medical benchmarks demonstrate that our MedREK achieves superior performance across different core metrics and provides the first validated solution for batch-editing in medical LLMs. Our code and dataset are available at https://github.com/mylittleriver/MedREK.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpreting the Latent Structure of Operator Precedence in Language Models</title>
<link>https://arxiv.org/abs/2510.13908</link>
<guid>https://arxiv.org/abs/2510.13908</guid>
<content:encoded><![CDATA[
arXiv:2510.13908v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities but continue to struggle with arithmetic tasks. Prior works largely focus on outputs or prompting strategies, leaving the open question of the internal structure through which models do arithmetic computation. In this work, we investigate whether LLMs encode operator precedence in their internal representations via the open-source instruction-tuned LLaMA 3.2-3B model. We constructed a dataset of arithmetic expressions with three operands and two operators, varying the order and placement of parentheses. Using this dataset, we trace whether intermediate results appear in the residual stream of the instruction-tuned LLaMA 3.2-3B model. We apply interpretability techniques such as logit lens, linear classification probes, and UMAP geometric visualization. Our results show that intermediate computations are present in the residual stream, particularly after MLP blocks. We also find that the model linearly encodes precedence in each operator's embeddings post attention layer. We introduce partial embedding swap, a technique that modifies operator precedence by exchanging high-impact embedding dimensions between operators.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers</title>
<link>https://arxiv.org/abs/2510.13939</link>
<guid>https://arxiv.org/abs/2510.13939</guid>
<content:encoded><![CDATA[
arXiv:2510.13939v3 Announce Type: replace 
Abstract: The use of copyrighted books for training AI models has led to numerous lawsuits from authors concerned about AI's ability to generate derivative content. Yet it's unclear if these models can generate high quality literary text while emulating authors' styles. To answer this we conducted a preregistered study comparing MFA-trained expert writers with three frontier AI models: ChatGPT, Claude & Gemini in writing up to 450 word excerpts emulating 50 award-winning authors' diverse styles. In blind pairwise evaluations by 159 representative expert & lay readers, AI-generated text from in-context prompting was strongly disfavored by experts for both stylistic fidelity (OR=0.16, p<10^-8) & writing quality (OR=0.13, p<10^-7) but showed mixed results with lay readers. However, fine-tuning ChatGPT on individual authors' complete works completely reversed these findings: experts now favored AI-generated text for stylistic fidelity (OR=8.16, p<10^-13) & writing quality (OR=1.87, p=0.010), with lay readers showing similar shifts. These effects generalize across authors & styles. The fine-tuned outputs were rarely flagged as AI-generated (3% rate v. 97% for in-context prompting) by best AI detectors. Mediation analysis shows this reversal occurs because fine-tuning eliminates detectable AI stylistic quirks (e.g., cliche density) that penalize in-context outputs. While we do not account for additional costs of human effort required to transform raw AI output into cohesive, publishable prose, the median fine-tuning & inference cost of $81 per author represents a dramatic 99.7% reduction compared to typical professional writer compensation. Author-specific fine-tuning thus enables non-verbatim AI writing that readers prefer to expert human writing, providing empirical evidence directly relevant to copyright's fourth fair-use factor, the "effect upon the potential market or value" of the source works.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting</title>
<link>https://arxiv.org/abs/2510.17210</link>
<guid>https://arxiv.org/abs/2510.17210</guid>
<content:encoded><![CDATA[
arXiv:2510.17210v2 Announce Type: replace 
Abstract: The increase in computing power and the necessity of AI-assisted decision-making boost the growing application of large language models (LLMs). Along with this, the potential retention of sensitive data of LLMs has spurred increasing research into machine unlearning. However, existing unlearning approaches face a critical dilemma: Aggressive unlearning compromises model utility, while conservative strategies preserve utility but risk hallucinated responses. This significantly limits LLMs' reliability in knowledge-intensive applications. To address this, we introduce a novel Attention-Shifting (AS) framework for selective unlearning. AS is driven by two design objectives: (1) context-preserving suppression that attenuates attention to fact-bearing tokens without disrupting LLMs' linguistic structure; and (2) hallucination-resistant response shaping that discourages fabricated completions when queried about unlearning content. AS realizes these objectives through two attention-level interventions, which are importance-aware suppression applied to the unlearning set to reduce reliance on memorized knowledge and attention-guided retention enhancement that reinforces attention toward semantically essential tokens in the retained dataset to mitigate unintended degradation. These two components are jointly optimized via a dual-loss objective, which forms a soft boundary that localizes unlearning while preserving unrelated knowledge under representation superposition. Experimental results show that AS improves performance preservation over the state-of-the-art unlearning methods, achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC benchmark, while maintaining competitive hallucination-free unlearning effectiveness. Compared to existing methods, AS demonstrates a superior balance between unlearning effectiveness, generalization, and response reliability.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Step Reasoning with Large Language Models, a Survey</title>
<link>https://arxiv.org/abs/2407.11511</link>
<guid>https://arxiv.org/abs/2407.11511</guid>
<content:encoded><![CDATA[
arXiv:2407.11511v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) with billions of parameters exhibit in-context learning abilities, enabling few-shot learning on tasks that the model was not specifically trained for. Traditional models achieve breakthrough performance on language tasks, but do not perform well on basic reasoning benchmarks. However, a new in-context learning approach, Chain-of-thought, has demonstrated strong multi-step reasoning abilities on these benchmarks. The research on LLM reasoning abilities started with the question whether LLMs can solve grade school math word problems, and has expanded to other tasks in the past few years. This article reviews the field of multi-step reasoning with LLMs. We propose a taxonomy that identifies different ways to generate, evaluate, and control multi-step reasoning. We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near future. We find that multi-step reasoning approaches have progressed beyond math word problems, and can now successfully solve challenges in logic, combinatorial games, and robotics, sometimes by first generating code that is then executed by external tools. Many studies in multi-step methods use reinforcement learning for finetuning, external optimization loops, in-context reinforcement learning, and self-reflection.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Abstraction Alignment: Comparing Model-Learned and Human-Encoded Conceptual Relationships</title>
<link>https://arxiv.org/abs/2407.12543</link>
<guid>https://arxiv.org/abs/2407.12543</guid>
<content:encoded><![CDATA[
arXiv:2407.12543v3 Announce Type: replace-cross 
Abstract: While interpretability methods identify a model's learned concepts, they overlook the relationships between concepts that make up its abstractions and inform its ability to generalize to new data. To assess whether models' have learned human-aligned abstractions, we introduce abstraction alignment, a methodology to compare model behavior against formal human knowledge. Abstraction alignment externalizes domain-specific human knowledge as an abstraction graph, a set of pertinent concepts spanning levels of abstraction. Using the abstraction graph as a ground truth, abstraction alignment measures the alignment of a model's behavior by determining how much of its uncertainty is accounted for by the human abstractions. By aggregating abstraction alignment across entire datasets, users can test alignment hypotheses, such as which human concepts the model has learned and where misalignments recur. In evaluations with experts, abstraction alignment differentiates seemingly similar errors, improves the verbosity of existing model-quality metrics, and uncovers improvements to current human abstractions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2410.18032</link>
<guid>https://arxiv.org/abs/2410.18032</guid>
<content:encoded><![CDATA[
arXiv:2410.18032v5 Announce Type: replace-cross 
Abstract: Graphs are widely used for modeling relational data in real-world scenarios, such as social networks and urban computing. Existing LLM-based graph analysis approaches either integrate graph neural networks (GNNs) for specific machine learning tasks, limiting their transferability, or rely solely on LLMs' internal reasoning ability, resulting in suboptimal performance. To address these limitations, we take advantage of recent advances in LLM-based agents, which have shown capabilities of utilizing external knowledge or tools for problem solving. By simulating human problem-solving strategies such as analogy and collaboration, we propose a multi-agent system based on LLMs named GraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from three modules, and the agents with different specialities can collaborate with each other to address complex problems. Specifically, (1) input-output normalization module: the question agent extracts and refines four key arguments from the original question, facilitating the problem understanding, and the answer agent organizes the results to meet the output requirement; (2) external knowledge retrieval module: we first build a knowledge base consisting of relevant documentation and experience information, and then the search agent retrieves the most relevant entries for each question. (3) problem-solving module: given the retrieved information from search agent, the coding agent uses established algorithms via programming to generate solutions, and in case the coding agent does not work, the reasoning agent will directly compute the results without programming. Extensive experiments on six graph analysis benchmarks demonstrate that GraphTeam achieves state-of-the-art performance with an average 25.85% improvement over the best baseline in terms of accuracy. The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks</title>
<link>https://arxiv.org/abs/2411.03343</link>
<guid>https://arxiv.org/abs/2411.03343</guid>
<content:encoded><![CDATA[
arXiv:2411.03343v3 Announce Type: replace-cross 
Abstract: Jailbreaks have been a central focus of research regarding the safety and reliability of large language models (LLMs), yet the mechanisms underlying these attacks remain poorly understood. While previous studies have predominantly relied on linear methods to detect jailbreak attempts and model refusals, we take a different approach by examining both linear and non-linear features in prompts that lead to successful jailbreaks. First, we introduce a novel dataset comprising 10,800 jailbreak attempts spanning 35 diverse attack methods. Leveraging this dataset, we train linear and non-linear probes on hidden states of open-weight LLMs to predict jailbreak success. Probes achieve strong in-distribution accuracy but transfer is attack-family-specific, revealing that different jailbreaks are supported by distinct internal mechanisms rather than a single universal direction. To establish causal relevance, we construct probe-guided latent interventions that systematically shift compliance in the predicted direction. Interventions derived from non-linear probes produce larger and more reliable effects than those from linear probes, indicating that features linked to jailbreak success are encoded non-linearly in prompt representations. Overall, the results surface heterogeneous, non-linear structure in jailbreak mechanisms and provide a prompt-side methodology for recovering and testing the features that drive jailbreak outcomes.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning</title>
<link>https://arxiv.org/abs/2502.03304</link>
<guid>https://arxiv.org/abs/2502.03304</guid>
<content:encoded><![CDATA[
arXiv:2502.03304v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising memory-efficient training paradigm, avoiding backward passes and relying solely on forward passes for gradient estimation, making it attractive for resource-constrained scenarios. However, ZO method lags far behind FO method in both convergence speed and accuracy. To bridge the gap, we introduce a novel layer-wise divergence analysis that uncovers the distinct update pattern of FO and ZO optimization. Aiming to resemble the learning capacity of FO method from the findings, we propose Divergence-driven Zeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer adaptation by incorporating projections to ZO updates, generating diverse-magnitude updates precisely scaled to layer-wise individual optimization needs. Our results demonstrate that DiZO significantly reduces the needed iterations for convergence without sacrificing throughput, cutting training GPU hours by up to 48\% on various datasets. Moreover, DiZO consistently outperforms the representative ZO baselines in fine-tuning RoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some cases, even surpasses memory-intensive FO fine-tuning. Our code is released at https://github.com/Skilteee/DiZO.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShortV: Efficient Multimodal Large Language Models by Freezing Visual Tokens in Ineffective Layers</title>
<link>https://arxiv.org/abs/2504.00502</link>
<guid>https://arxiv.org/abs/2504.00502</guid>
<content:encoded><![CDATA[
arXiv:2504.00502v2 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) suffer from high computational costs due to their massive size and the large number of visual tokens. In this paper, we investigate layer-wise redundancy in MLLMs by introducing a novel metric, Layer Contribution (LC), which quantifies the impact of a layer's transformations on visual and text tokens, respectively. The calculation of LC involves measuring the divergence in model output that results from removing the layer's transformations on the specified tokens. Our pilot experiment reveals that many layers of MLLMs exhibit minimal contribution during the processing of visual tokens. Motivated by this observation, we propose ShortV, a training-free method that leverages LC to identify ineffective layers, and freezes visual token updates in these layers. Experiments show that ShortV can freeze visual token in approximately 60\% of the MLLM layers, thereby dramatically reducing computational costs related to updating visual tokens. For example, it achieves a 50\% reduction in FLOPs on LLaVA-NeXT-13B while maintaining superior performance. The code will be publicly available at https://github.com/icip-cas/ShortV
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?</title>
<link>https://arxiv.org/abs/2504.00509</link>
<guid>https://arxiv.org/abs/2504.00509</guid>
<content:encoded><![CDATA[
arXiv:2504.00509v3 Announce Type: replace-cross 
Abstract: The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60 percent performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd Counting using Fuzzy Group Relative Policy Reward</title>
<link>https://arxiv.org/abs/2504.03724</link>
<guid>https://arxiv.org/abs/2504.03724</guid>
<content:encoded><![CDATA[
arXiv:2504.03724v2 Announce Type: replace-cross 
Abstract: We propose Fuzzy Group Relative Policy Reward (FGRPR), a novel framework that integrates Group Relative Policy Optimization (GRPO) with a fuzzy reward function to enhance learning efficiency. Unlike the conventional binary 0/1 accuracy reward, our fuzzy reward model provides nuanced incentives, encouraging more precise outputs. Experimental results demonstrate that GRPO with a standard 0/1 accuracy reward underperforms compared to supervised fine-tuning (SFT). In contrast, FGRPR, applied to Qwen2.5-VL(3B and 7B), surpasses all baseline models, including GPT4o, LLaMA2(90B), and SFT, across five in-domain datasets. On an out-of-domain dataset, FGRPR achieves performance comparable to SFT but excels when target values are larger, as its fuzzy reward function assigns higher rewards to closer approximations. This approach is broadly applicable to tasks where the precision of the answer is critical. Code and data: https://github.com/yeyimilk/CrowdVLM-R1
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark</title>
<link>https://arxiv.org/abs/2504.13861</link>
<guid>https://arxiv.org/abs/2504.13861</guid>
<content:encoded><![CDATA[
arXiv:2504.13861v3 Announce Type: replace-cross 
Abstract: Though Large Vision-Language Models (LVLMs) are being actively explored in medicine, their ability to conduct complex real-world telemedicine consultations combining accurate diagnosis with professional dialogue remains underexplored. This paper presents 3MDBench (Medical Multimodal Multi-agent Dialogue Benchmark), an open-source framework for simulating and evaluating LVLM-driven telemedical consultations. 3MDBench simulates patient variability through temperament-based Patient Agent and evaluates diagnostic accuracy and dialogue quality via Assessor Agent. It includes 2996 cases across 34 diagnoses from real-world telemedicine interactions, combining textual and image-based data. The experimental study compares diagnostic strategies for widely used open and closed-source LVLMs. We demonstrate that multimodal dialogue with internal reasoning improves F1 score by 6.5% over non-dialogue settings, highlighting the importance of context-aware, information-seeking questioning. Moreover, injecting predictions from a diagnostic convolutional neural network into the LVLM's context boosts F1 by up to 20%. Source code is available at https://github.com/univanxx/3mdbench.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding</title>
<link>https://arxiv.org/abs/2505.18079</link>
<guid>https://arxiv.org/abs/2505.18079</guid>
<content:encoded><![CDATA[
arXiv:2505.18079v4 Announce Type: replace-cross 
Abstract: Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery (DVD) agent to leverage an agentic search strategy over segmented video clips. Unlike previous video agents that rely on predefined workflows applied uniformly across different queries, our approach emphasizes the autonomous and adaptive nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools to orchestrate adaptive workflow for different queries in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates our advantage. Our DVD agent achieves state-of-the-art performance on the challenging LVBench dataset, reaching an accuracy of 74.2%, which substantially surpasses all prior works, and further improves to 76.0% with transcripts. The code has been released at https://github.com/microsoft/DeepVideoDiscovery.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UI-Evol: Automatic Knowledge Evolving for Computer Use Agents</title>
<link>https://arxiv.org/abs/2505.21964</link>
<guid>https://arxiv.org/abs/2505.21964</guid>
<content:encoded><![CDATA[
arXiv:2505.21964v2 Announce Type: replace-cross 
Abstract: External knowledge has played a crucial role in the recent development of computer use agents. We identify a critical knowledge-execution gap: retrieved knowledge often fails to translate into effective real-world task execution. Our analysis shows even 90% correct knowledge yields only 41% execution success rate. To bridge this gap, we propose UI-Evol, a plug-and-play module for autonomous GUI knowledge evolution. UI-Evol consists of two stages: a Retrace Stage that extracts faithful objective action sequences from actual agent-environment interactions, and a Critique Stage that refines existing knowledge by comparing these sequences against external references. We conduct comprehensive experiments on the OSWorld benchmark with the state-of-the-art Agent S2. Our results demonstrate that UI-Evol not only significantly boosts task performance but also addresses a previously overlooked issue of high behavioral standard deviation in computer use agents, leading to superior performance on computer use tasks and substantially improved agent reliability.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.06632</link>
<guid>https://arxiv.org/abs/2506.06632</guid>
<content:encoded><![CDATA[
arXiv:2506.06632v2 Announce Type: replace-cross 
Abstract: We aim to improve the reasoning capabilities of language models via reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1 have demonstrated reasoning abilities on mathematical and coding tasks. However, prior studies suggest that using RL alone to improve reasoning on inherently difficult tasks is less effective. Here, we draw inspiration from curriculum learning and propose to schedule tasks from easy to hard (E2H), allowing LLMs to build reasoning skills gradually. Our method is termed E2H Reasoner. Empirically, we observe that, although easy tasks are important initially, fading them out through appropriate scheduling is essential in preventing overfitting. Theoretically, we establish convergence guarantees for E2H Reasoner within an approximate policy iteration framework. We derive finite-sample complexity bounds and show that when tasks are appropriately decomposed and conditioned, learning through curriculum stages requires fewer total samples than direct learning. Experiments across multiple domains show that E2H Reasoner significantly improves the reasoning ability of small LLMs (1.5B to 3B), which otherwise struggle when trained with vanilla RL alone, highlighting the effectiveness of our method. Our code can be found on https://github.com/divelab/E2H-Reasoning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Inequality Proofs with Large Language Models</title>
<link>https://arxiv.org/abs/2506.07927</link>
<guid>https://arxiv.org/abs/2506.07927</guid>
<content:encoded><![CDATA[
arXiv:2506.07927v2 Announce Type: replace-cross 
Abstract: Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning</title>
<link>https://arxiv.org/abs/2506.10521</link>
<guid>https://arxiv.org/abs/2506.10521</guid>
<content:encoded><![CDATA[
arXiv:2506.10521v5 Announce Type: replace-cross 
Abstract: Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientists' First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM2: Unlocking Multi-Modal General Pathology AI with Clinical Dialogue</title>
<link>https://arxiv.org/abs/2506.13063</link>
<guid>https://arxiv.org/abs/2506.13063</guid>
<content:encoded><![CDATA[
arXiv:2506.13063v2 Announce Type: replace-cross 
Abstract: Recent rapid progress in the field of computational pathology has been enabled by foundation models. These models are beginning to move beyond encoding image patches towards whole-slide understanding but their clinical utility remains limited. In this work, we present PRISM2, a multimodal slide-level foundation model trained on data from 700,000 diagnostic specimen-report pairs, the largest vision (2.3 million whole slide images) and language (14M question-answer pairs) histopathology dataset to date. By learning through clinical-dialogue supervision, PRISM2 aligns histomorphologic features with the language of diagnostic reasoning, producing slide-level representations that support both direct diagnostic question-answering and transferable embeddings for downstream tasks. Without additional training, PRISM2 matches or exceeds the cancer-detection performance of clinical-grade products. This is observed without loss of generality on other tasks, where PRISM2 achieves top performance. Finally, using survival prediction as the example, we show that task-specific finetuning with a large dataset can outperform task-specific models, further improving performance. These results demonstrate how language-supervised pretraining provides a scalable, clinically grounded signal for learning generalizable pathology representations, bridging human diagnostic reasoning and foundation-model performance.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MotionGPT3: Human Motion as a Second Modality</title>
<link>https://arxiv.org/abs/2506.24086</link>
<guid>https://arxiv.org/abs/2506.24086</guid>
<content:encoded><![CDATA[
arXiv:2506.24086v3 Announce Type: replace-cross 
Abstract: With the rapid progress of large language models (LLMs), multimodal frameworks that unify understanding and generation have become promising, yet they face increasing complexity as the number of modalities and tasks grows. We observe that motion quantization introduces approximation errors that cap motion quality, and that unifying discrete text and continuous motion within a single-stream backbone amplifies cross-modal interference. Motivated by recent multi-branch Transformer designs that separate signals from different modalities, we propose MotionGPT3, a bimodal motion-language model for both understanding and generation. MotionGPT3 encodes raw motion into a continuous latent space using a variational autoencoder (VAE), thereby avoiding quantization-induced artifacts, while leveraging the semantic prior of pretrained language models. A dual-stream Transformer with shared attention preserves modality-specific routes while enabling controlled, bidirectional information flow, which reduces interference, stabilizing optimization, and empirically accelerates convergence without degrading fidelity. For multimodal joint training, a generate-then-align three-stage schedule further improves stability and limits cross-task interference. Experiments show that MotionGPT3 achieves 2x faster convergence in training loss and up to 4x faster convergence in validation, while maintaining state-of-the-art performance on standard motion understanding and motion generation benchmarks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain of Retrieval: Multi-Aspect Iterative Search Expansion and Post-Order Search Aggregation for Full Paper Retrieval</title>
<link>https://arxiv.org/abs/2507.10057</link>
<guid>https://arxiv.org/abs/2507.10057</guid>
<content:encoded><![CDATA[
arXiv:2507.10057v2 Announce Type: replace-cross 
Abstract: Scientific paper retrieval, particularly framed as document-to-document retrieval, aims to identify relevant papers in response to a long-form query paper, rather than a short query string. Previous approaches to this task have focused exclusively on abstracts, embedding them into dense vectors as surrogates for full documents and calculating similarity between them. Yet, abstracts offer only sparse and high-level summaries, and such methods primarily optimize one-to-one similarity, overlooking the dynamic relations that emerge among relevant papers during the retrieval process. To address this, we propose Chain of Retrieval(COR), a novel iterative framework for full-paper retrieval. Specifically, CoR decomposes each query paper into multiple aspect-specific views, matches them against segmented candidate papers, and iteratively expands the search by promoting top-ranked results as new queries, thereby forming a tree-structured retrieval process. The resulting retrieval tree is then aggregated in a post-order manner: descendants are first combined at the query level, then recursively merged with their parent nodes, to capture hierarchical relations across iterations. To validate this, we present SCIFULLBENCH, a large-scale benchmark providing both complete and segmented contexts of full papers for queries and candidates, and results show that CoR significantly outperforms existing retrieval baselines. Our code and dataset is available at https://github.com/psw0021/Chain-of-Retrieval.git.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recent Trends in Distant Conversational Speech Recognition: A Review of CHiME-7 and 8 DASR Challenges</title>
<link>https://arxiv.org/abs/2507.18161</link>
<guid>https://arxiv.org/abs/2507.18161</guid>
<content:encoded><![CDATA[
arXiv:2507.18161v2 Announce Type: replace-cross 
Abstract: The CHiME-7 and 8 distant speech recognition (DASR) challenges focus on multi-channel, generalizable, joint automatic speech recognition (ASR) and diarization of conversational speech. With participation from 9 teams submitting 32 diverse systems, these challenges have contributed to state-of-the-art research in the field. This paper outlines the challenges' design, evaluation metrics, datasets, and baseline systems while analyzing key trends from participant submissions. From this analysis it emerges that: 1) Most participants use end-to-end (e2e) ASR systems, whereas hybrid systems were prevalent in previous CHiME challenges. This transition is mainly due to the availability of robust large-scale pre-trained models, which lowers the data burden for e2e-ASR. 2) Despite recent advances in neural speech separation and enhancement (SSE), all teams still heavily rely on guided source separation, suggesting that current neural SSE techniques are still unable to reliably deal with complex scenarios and different recording setups. 3) All best systems employ diarization refinement via target-speaker diarization techniques. Accurate speaker counting in the first diarization pass is thus crucial to avoid compounding errors and CHiME-8 DASR participants especially focused on this part. 4) Downstream evaluation via meeting summarization can correlate weakly with transcription quality due to the remarkable effectiveness of large-language models in handling errors. On the NOTSOFAR-1 scenario, even systems with over 50% time-constrained minimum permutation WER can perform roughly on par with the most effective ones (around 11%). 5) Despite recent progress, accurately transcribing spontaneous speech in challenging acoustic environments remains difficult, even when using computationally intensive system ensembles.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Token Choice for Code Watermarking: An RL Approach</title>
<link>https://arxiv.org/abs/2508.11925</link>
<guid>https://arxiv.org/abs/2508.11925</guid>
<content:encoded><![CDATA[
arXiv:2508.11925v2 Announce Type: replace-cross 
Abstract: Protecting intellectual property on LLM-generated code necessitates effective watermarking systems that can operate within code's highly structured, syntactically constrained nature. In this work, we introduce CodeTracer, an innovative adaptive code watermarking framework underpinned by a novel reinforcement learning training paradigm. At its core, CodeTracer features a policy-driven approach that utilizes a parameterized model to intelligently bias token choices during next-token prediction. This strategy ensures that embedded watermarks maintain code functionality while exhibiting subtle yet statistically detectable deviations from typical token distributions. To facilitate policy learning, we devise a comprehensive reward system that seamlessly integrates execution feedback with watermark embedding signals, balancing process-level and outcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization to enable gradient-based optimization of discrete watermarking decisions. Extensive comparative evaluations demonstrate CodeTracer's significant superiority over state-of-the-art baselines in both watermark detectability and the preservation of generated code's functionality.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Steer: Input-dependent Steering for Multimodal LLMs</title>
<link>https://arxiv.org/abs/2508.12815</link>
<guid>https://arxiv.org/abs/2508.12815</guid>
<content:encoded><![CDATA[
arXiv:2508.12815v2 Announce Type: replace-cross 
Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of LLMs towards enforcing a specific behavior. However, it remains largely underexplored for multimodal LLMs (MLLMs); furthermore, existing steering techniques, such as mean steering, rely on a single steering vector, applied independently of the input query. This paradigm faces limitations when the desired behavior is dependent on the example at hand. For example, a safe answer may consist in abstaining from answering when asked for an illegal activity, or may point to external resources or consultation with an expert when asked about medical advice. In this paper, we investigate a fine-grained steering that uses an input-specific linear shift. This shift is computed using contrastive input-specific prompting. However, the input-specific prompts required for this approach are not known at test time. Therefore, we propose to train a small auxiliary module to predict the input-specific steering vector. Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces hallucinations and enforces safety in MLLMs, outperforming other static baselines. Our code is publicly available at https://jayneelparekh.github.io/learn-to-steer/
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models</title>
<link>https://arxiv.org/abs/2508.16406</link>
<guid>https://arxiv.org/abs/2508.16406</guid>
<content:encoded><![CDATA[
arXiv:2508.16406v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which attempt to elicit harmful responses from LLMs. The evolving nature and diversity of these attacks pose many challenges for defense systems, including (1) adaptation to counter emerging attack strategies without costly retraining, and (2) control of the trade-off between safety and utility. To address these challenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for jailbreak detection that incorporates a database of known attack examples into Retrieval-Augmented Generation, which is used to infer the underlying, malicious user query and jailbreak strategy used to attack the system. RAD enables training-free updates for newly discovered jailbreak strategies and provides a mechanism to balance safety and utility. Experiments on StrongREJECT show that RAD substantially reduces the effectiveness of strong jailbreak attacks such as PAP and PAIR while maintaining low rejection rates for benign queries. We propose a novel evaluation scheme and show that RAD achieves a robust safety-utility trade-off across a range of operating points in a controllable manner.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CausalARC: Abstract Reasoning with Causal World Models</title>
<link>https://arxiv.org/abs/2509.03636</link>
<guid>https://arxiv.org/abs/2509.03636</guid>
<content:encoded><![CDATA[
arXiv:2509.03636v2 Announce Type: replace-cross 
Abstract: On-the-fly reasoning often requires adaptation to novel problems under limited data and distribution shift. This work introduces CausalARC: an experimental testbed for AI reasoning in low-data and out-of-distribution regimes, modeled after the Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is sampled from a fully specified causal world model, formally expressed as a structural causal model. Principled data augmentations provide observational, interventional, and counterfactual feedback about the world model in the form of few-shot, in-context learning demonstrations. As a proof-of-concept, we illustrate the use of CausalARC for four language model evaluation settings: (1) abstract reasoning with test-time training, (2) counterfactual reasoning with in-context learning, (3) program synthesis, and (4) causal discovery with logical reasoning. Within- and between-model performance varied heavily across tasks, indicating room for significant improvement in language model reasoning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research</title>
<link>https://arxiv.org/abs/2509.06093</link>
<guid>https://arxiv.org/abs/2509.06093</guid>
<content:encoded><![CDATA[
arXiv:2509.06093v2 Announce Type: replace-cross 
Abstract: The preparation procedures of materials are often embedded narratively in experimental protocols, research articles, patents, and laboratory notes, and are structured around procedural sequences, causal relationships, and conditional logic. The synthesis of boron nitride nanosheet (BNNS) polymer composites exemplifies this linguistically encoded decision-making system, where the practical experiments involve interdependent multistage and path-dependent processes such as exfoliation, functionalization, and dispersion, each governed by heterogeneous parameters and contextual contingencies, challenging conventional numerical optimization paradigms for experiment design. We reformulate this challenge into a text-reasoning problem through a framework centered on a text-first, lightly structured materials database and large language models (LLMs) as text reasoning engines. We constructed a database that captures evidence-linked narrative excerpts from the literature while normalizing only the minimum necessary entities, attributes, and relations to enable composite retrieval that unifies semantic matching, lexical cues, and explicit value filters. Building on this language-native, provenance-preserving foundation, the LLM operates in two complementary modes: retrieval-augmented generation (RAG), grounding outputs in retrieved evidence modules from the database, and experience-augmented reasoning (EAR), which leverages iteratively trained text guides derived from multi-source literature-based narrative data as external references to inform reasoning and decision-making. Applying this integration-and-reasoning framework, we demonstrate rapid, laboratory-scale optimization of BNNS preparation, highlighting how language-native data combined with LLM-based reasoning can significantly accelerate practical material preparation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadarPLM: Adapting Pretrained Language Models for Marine Radar Target Detection with Preference-aware Loss</title>
<link>https://arxiv.org/abs/2509.12089</link>
<guid>https://arxiv.org/abs/2509.12089</guid>
<content:encoded><![CDATA[
arXiv:2509.12089v3 Announce Type: replace-cross 
Abstract: Recent advances in pre-trained language models (PLMs) have demonstrated their capabilities in capturing universal knowledge, making them promising applications for radar signal processing. Nevertheless, directly fine-tuning PLMs on radar signals is both computationally expensive and prone to overfitting, particularly in low signal-to-clutter ratio (SCR) environments. In this paper, we propose a novel fine-tuning framework for PLM-based marine radar target detection. First, we design a lightweight adaptation module, enabling parameter-efficient fine-tuning while preserving the pretrained model's general knowledge. Second, a novel preference-aware loss is developed to selectively optimize different feature patches based on their online evaluated learning values, guiding the model to concentrate on the most generalizable feature patterns during optimization. Extensive experiments on real-world marine radar datasets demonstrate that the proposed finetuning framework achieves an average performance improvement of 9.9% over the standard approach under low SCR conditions. Furthermore, the fine-tuned model, RadarPLM, consistently outperforms state-of-the-art detectors, particularly when training data are limited.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Time Awareness in Generative Recommendation</title>
<link>https://arxiv.org/abs/2509.13957</link>
<guid>https://arxiv.org/abs/2509.13957</guid>
<content:encoded><![CDATA[
arXiv:2509.13957v2 Announce Type: replace-cross 
Abstract: Generative recommendation has emerged as a promising paradigm that formulates the recommendations into a text-to-text generation task, harnessing the vast knowledge of large language models. However, existing studies focus on considering the sequential order of items and neglect to handle the temporal dynamics across items, which can imply evolving user preferences. To address this limitation, we propose a novel model, Generative Recommender Using Time awareness (GRUT), effectively capturing hidden user preferences via various temporal signals. We first introduce Time-aware Prompting, consisting of two key contexts. The user-level temporal context models personalized temporal patterns across timestamps and time intervals, while the item-level transition context provides transition patterns across users. We also devise Trend-aware Inference, a training-free method that enhances rankings by incorporating trend information about items with generation likelihood. Extensive experiments demonstrate that GRUT outperforms state-of-the-art models, with gains of up to 15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The source code is available at https://github.com/skleee/GRUT.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mapping Overlaps in Benchmarks through Perplexity in the Wild</title>
<link>https://arxiv.org/abs/2509.23488</link>
<guid>https://arxiv.org/abs/2509.23488</guid>
<content:encoded><![CDATA[
arXiv:2509.23488v3 Announce Type: replace-cross 
Abstract: We develop signatures of capacity familiarity to characterize large language model (LLM) benchmarks and their meaningful overlaps. Benchmark signatures probe the capacity required for benchmark performance. We formally define them as a set of salient tokens drawn from in-the-wild, naturally authored corpora, where LLM token perplexity, reflecting more or less pre-training exposure, becomes highly predictive of LLM benchmark performance. Through a large-scale meta-evaluation, we extract benchmark signatures via stepwise forward selection with linear regressions across 32 LLMs and 88 benchmarks spanning diverse knowledge, coding, logic, instruction following, math, language, reasoning, and world modeling. Our analysis situates signatures in relation to both the semantic similarity of benchmark questions and the correlation of model performance. While performance overlaps are universally high and semantic overlaps remain confined to a narrow mid-range, benchmark signatures prove highly informative in capturing variation, overlap, and divergence. We observe overlap in knowledge and reasoning subtasks, whereas multilingual and cultural benchmarks exhibit less similarity, even compared to cross-task overlap. Notably, performance-level results are strongly influenced by benchmark-orthogonal factors such as question format, highlighting limitations in LLM generalization, the conflation of performance with ability, and issues inherent in current mainstream benchmark agreement studies. Benchmark signatures, however, remain robust to such effects. Ultimately, we identify cross-functional overlaps across logic, math, language, instruction following, and world modeling, with coding emerging as the least overlapping domain. Together, these findings provide mechanistic insights into benchmark validity and LLM sensitivities, and sketch the underlying landscape of interconnected LLM capabilities.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where to Search: Measure the Prior-Structured Search Space of LLM Agents</title>
<link>https://arxiv.org/abs/2510.14846</link>
<guid>https://arxiv.org/abs/2510.14846</guid>
<content:encoded><![CDATA[
arXiv:2510.14846v3 Announce Type: replace-cross 
Abstract: The generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science. However, the effectiveness of search depends on where to search, namely, how to encode the domain prior into an operationally structured hypothesis space. To this end, this paper proposes a compact formal theory that describes and measures LLM-assisted iterative search guided by domain priors. We represent an agent as a fuzzy relation operator on inputs and outputs to capture feasible transitions; the agent is thereby constrained by a fixed safety envelope. To describe multi-step reasoning/search, we weight all reachable paths by a single continuation parameter and sum them to obtain a coverage generating function; this induces a measure of reachability difficulty; and it provides a geometric interpretation of search on the graph induced by the safety envelope. We further provide the simplest testable inferences and validate them via two instantiation. This theory offers a workable language and operational tools to measure agents and their search spaces, proposing a systematic formal description of iterative search constructed by LLMs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models</title>
<link>https://arxiv.org/abs/2510.14925</link>
<guid>https://arxiv.org/abs/2510.14925</guid>
<content:encoded><![CDATA[
arXiv:2510.14925v2 Announce Type: replace-cross 
Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, viewing reason as a regulator that keeps inference within the bounds of possible experience. We formalize this intuition via a composite instability index (H-Risk) combining spectral margin, conditioning, temporal sensitivity, and innovation amplification. In linear-Gaussian simulations, higher H-Risk predicts overconfident errors even under formal stability, revealing a gap between nominal and epistemic stability. Extending to large language models (LLMs), we observe preliminary correlations between internal fragility and miscalibration or hallucination (confabulation), and find that lightweight critique prompts may modestly improve or worsen calibration in small-scale tests. These results suggest a structural bridge between Kantian self-limitation and feedback control, offering a principled lens to diagnose and potentially mitigate overconfidence in reasoning systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Synergy of Quantitative Factors and Newsflow Representations from Large Language Models for Stock Return Prediction</title>
<link>https://arxiv.org/abs/2510.15691</link>
<guid>https://arxiv.org/abs/2510.15691</guid>
<content:encoded><![CDATA[
arXiv:2510.15691v2 Announce Type: replace-cross 
Abstract: In quantitative investing, return prediction supports various tasks, including stock selection, portfolio optimization, and risk management. Quantitative factors, such as valuation, quality, and growth, capture various characteristics of stocks. Unstructured financial data, like news and transcripts, has attracted growing attention, driven by recent advances in large language models (LLMs). This paper examines effective methods for leveraging multimodal factors and newsflow in return prediction and stock selection. First, we introduce a fusion learning framework to learn a unified representation from factors and newsflow representations generated by an LLM. Within this framework, we compare three representative methods: representation combination, representation summation, and attentive representations. Next, building on empirical observations from fusion learning, we explore the mixture model that adaptively combines predictions made by single modalities and their fusion. To mitigate the training instability observed in the mixture model, we introduce a decoupled training approach with theoretical insights. Finally, our experiments on real investment universes yield several insights into effective multimodal modeling of factors and news for stock return prediction and selection.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input</title>
<link>https://arxiv.org/abs/2510.16926</link>
<guid>https://arxiv.org/abs/2510.16926</guid>
<content:encoded><![CDATA[
arXiv:2510.16926v2 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image resolutions. However, current evaluation paradigms primarily assess semantic performance, overlooking the critical question of resolution robustness - whether performance remains stable across varying input resolutions. To address this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising 14,400 samples across 12 resolution levels and six core capability dimensions. We designed a novel evaluation framework that goes beyond traditional accuracy metrics to capture performance stability. This framework introduces multiple robustness metrics: Spearman's correlation for assessing resolution-performance trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring performance volatility. Using these metrics, we conducted a large-scale evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and task-centric robustness examination, (2) investigation of preprocessing strategies including padding and super-resolution, and (3) exploration of fine-tuning for stability enhancement.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation</title>
<link>https://arxiv.org/abs/2510.20792</link>
<guid>https://arxiv.org/abs/2510.20792</guid>
<content:encoded><![CDATA[
arXiv:2510.20792v2 Announce Type: replace-cross 
Abstract: The rapid progress of graph generation has raised new security concerns, particularly regarding backdoor vulnerabilities. While prior work has explored backdoor attacks in image diffusion and unconditional graph generation, conditional, especially text-guided graph generation remains largely unexamined. This paper proposes BadGraph, a backdoor attack method against latent diffusion models for text-guided graph generation. BadGraph leverages textual triggers to poison training data, covertly implanting backdoors that induce attacker-specified subgraphs during inference when triggers appear, while preserving normal performance on clean inputs. Extensive experiments on four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the effectiveness and stealth of the attack: less than 10% poisoning rate can achieves 50% attack success rate, while 24% suffices for over 80% success rate, with negligible performance degradation on benign samples. Ablation studies further reveal that the backdoor is implanted during VAE and diffusion training rather than pretraining. These findings reveal the security vulnerabilities in latent diffusion models of text-guided graph generation, highlight the serious risks in models' applications such as drug discovery and underscore the need for robust defenses against the backdoor attack in such diffusion models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Enhancing Mamba-Transformer Hybrids for Memory Recall and Language Modeling</title>
<link>https://arxiv.org/abs/2510.26912</link>
<guid>https://arxiv.org/abs/2510.26912</guid>
<content:encoded><![CDATA[
<div> Keywords: hybrid models, state space models, attention mechanisms, memory utilization, recall ability<br />
Summary:<br />
This study examines the effectiveness of hybrid models that combine state space models (SSMs) and attention mechanisms. It compares the performance of sequential and parallel integration of SSM and attention layers, finding that sequential hybrids are better for shorter contexts while parallel hybrids excel with longer contexts. Additionally, the study introduces a data-centric approach involving continual training on augmented datasets with paraphrases to enhance recall without sacrificing other capabilities. This method outperforms architectural modifications aimed at improving recall and generalizes well across different base models. The findings provide valuable insights into the design of hybrid SSM-attention models, offering practical guidance for tailoring architectures to specific use cases. The research contributes to a deeper understanding of how hybrid models can leverage the strengths of SSMs and attention mechanisms for enhanced performance. <br /><br />Summary: <div>
arXiv:2510.26912v1 Announce Type: new 
Abstract: Hybrid models that combine state space models (SSMs) with attention mechanisms have shown strong performance by leveraging the efficiency of SSMs and the high recall ability of attention. However, the architectural design choices behind these hybrid models remain insufficiently understood. In this work, we analyze hybrid architectures through the lens of memory utilization and overall performance, and propose a complementary method to further enhance their effectiveness. We first examine the distinction between sequential and parallel integration of SSM and attention layers. Our analysis reveals several interesting findings, including that sequential hybrids perform better on shorter contexts, whereas parallel hybrids are more effective for longer contexts. We also introduce a data-centric approach of continually training on datasets augmented with paraphrases, which further enhances recall while preserving other capabilities. It generalizes well across different base models and outperforms architectural modifications aimed at enhancing recall. Our findings provide a deeper understanding of hybrid SSM-attention models and offer practical guidance for designing architectures tailored to various use cases. Our findings provide a deeper understanding of hybrid SSM-attention models and offer practical guidance for designing architectures tailored to various use cases.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Frame Semantic Patterns for Identifying Underreporting of Notifiable Events in Healthcare: The Case of Gender-Based Violence</title>
<link>https://arxiv.org/abs/2510.26969</link>
<guid>https://arxiv.org/abs/2510.26969</guid>
<content:encoded><![CDATA[
<div> Methodology, Healthcare, Gender-based violence, NLP, Public health systems<br />
Summary:<br />
The article introduces a methodology using semantic frames to identify notifiable events in healthcare, specifically focusing on the underreporting of gender-based violence (GBV) in e-medical records. By defining and searching for patterns in unstructured data from Brazilian Portuguese e-SUS APS records, the methodology achieves a precision of 0.726 in identifying reports of violence. The approach is transparent, efficient, low-carbon, and language-agnostic, making it easily adaptable to different health surveillance contexts. The methodology's robustness confirms its effectiveness in identifying notifiable events, contributing to the ethical and explainable use of NLP in public health systems. <div>
arXiv:2510.26969v1 Announce Type: new 
Abstract: We introduce a methodology for the identification of notifiable events in the domain of healthcare. The methodology harnesses semantic frames to define fine-grained patterns and search them in unstructured data, namely, open-text fields in e-medical records. We apply the methodology to the problem of underreporting of gender-based violence (GBV) in e-medical records produced during patients' visits to primary care units. A total of eight patterns are defined and searched on a corpus of 21 million sentences in Brazilian Portuguese extracted from e-SUS APS. The results are manually evaluated by linguists and the precision of each pattern measured. Our findings reveal that the methodology effectively identifies reports of violence with a precision of 0.726, confirming its robustness. Designed as a transparent, efficient, low-carbon, and language-agnostic pipeline, the approach can be easily adapted to other health surveillance contexts, contributing to the broader, ethical, and explainable use of NLP in public health systems.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction from Doctor-Patient Consultations</title>
<link>https://arxiv.org/abs/2510.26974</link>
<guid>https://arxiv.org/abs/2510.26974</guid>
<content:encoded><![CDATA[
<div> automatic speech recognition, summarization, medical orders, Electronic Health Records, MEDIQA-OE 2025 <br />
Summary:<br />
The article introduces the MEDIQA-OE 2025 shared task, focusing on extracting medical orders from doctor-patient conversations to aid clinicians in reducing documentation burden and improving patient care. Six teams participated in the task, utilizing various approaches and large language models. The paper outlines the task, dataset, final leaderboard ranking, and participants' solutions, highlighting the importance of leveraging technology to streamline clinical documentation processes and enhance healthcare delivery. <div>
arXiv:2510.26974v1 Announce Type: new 
Abstract: Clinical documentation increasingly uses automatic speech recognition and summarization, yet converting conversations into actionable medical orders for Electronic Health Records remains unexplored. A solution to this problem can significantly reduce the documentation burden of clinicians and directly impact downstream patient care. We introduce the MEDIQA-OE 2025 shared task, the first challenge on extracting medical orders from doctor-patient conversations. Six teams participated in the shared task and experimented with a broad range of approaches, and both closed- and open-weight large language models (LLMs). In this paper, we describe the MEDIQA-OE task, dataset, final leaderboard ranking, and participants' solutions.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantically-Aware LLM Agent to Enhance Privacy in Conversational AI Services</title>
<link>https://arxiv.org/abs/2510.27016</link>
<guid>https://arxiv.org/abs/2510.27016</guid>
<content:encoded><![CDATA[
<div> privacy, conversational AI systems, Personally Identifiable Information (PII), semantic integrity, LOPSIDED framework

Summary:<br /><br />
The article introduces the LOPSIDED framework, a privacy agent designed to protect sensitive Personally Identifiable Information (PII) during interactions with Large Language Models (LLMs). The framework dynamically replaces PII entities with semantically consistent pseudonyms in user prompts to maintain contextual integrity in conversations. This approach ensures privacy without compromising response quality. After generating a response, the pseudonyms are depseudonymized to provide the user with an accurate, privacy-preserving output. Evaluation using real-world conversations demonstrates that LOPSIDED effectively reduces semantic utility errors by a factor of 5 compared to baseline techniques, improving privacy protection while maintaining conversation quality. <div>
arXiv:2510.27016v1 Announce Type: new 
Abstract: With the increasing use of conversational AI systems, there is growing concern over privacy leaks, especially when users share sensitive personal data in interactions with Large Language Models (LLMs). Conversations shared with these models may contain Personally Identifiable Information (PII), which, if exposed, could lead to security breaches or identity theft. To address this challenge, we present the Local Optimizations for Pseudonymization with Semantic Integrity Directed Entity Detection (LOPSIDED) framework, a semantically-aware privacy agent designed to safeguard sensitive PII data when using remote LLMs. Unlike prior work that often degrade response quality, our approach dynamically replaces sensitive PII entities in user prompts with semantically consistent pseudonyms, preserving the contextual integrity of conversations. Once the model generates its response, the pseudonyms are automatically depseudonymized, ensuring the user receives an accurate, privacy-preserving output. We evaluate our approach using real-world conversations sourced from ShareGPT, which we further augment and annotate to assess whether named entities are contextually relevant to the model's response. Our results show that LOPSIDED reduces semantic utility errors by a factor of 5 compared to baseline techniques, all while enhancing privacy.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kad: A Framework for Proxy-based Test-time Alignment with Knapsack Approximation Deferral</title>
<link>https://arxiv.org/abs/2510.27017</link>
<guid>https://arxiv.org/abs/2510.27017</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, pre-training, alignment, test-time, computational cost

Summary:
Token-specific cascading method proposed in this work aims to enhance alignment of large language models (LLMs) to downstream task requirements and stylistic preferences without incurring high computational costs. By utilizing guidance from a small aligned model at test time, the approach reduces the need for extensive alignment procedures. The method formulates the token-specific deferral rule as a 0-1 knapsack problem, enabling efficient decision-making. Primal and dual approximations of the optimal deferral decision are derived to improve task performance and speculative decoding speed. Experimental results demonstrate the effectiveness of the proposed approach in enhancing the alignment of LLMs while minimizing computational overhead. 

<br /><br />Summary: Token-specific cascading method proposed in this work leverages guidance from a small aligned model at test time to efficiently align large language models to downstream task requirements. By formulating the token-specific deferral rule as a 0-1 knapsack problem, the approach enhances task performance and speculative decoding speed while minimizing computational costs. <div>
arXiv:2510.27017v1 Announce Type: new 
Abstract: Several previous works concluded that the largest part of generation capabilities of large language models (LLM) are learned (early) during pre-training. However, LLMs still require further alignment to adhere to downstream task requirements and stylistic preferences, among other desired properties. As LLMs continue to scale in terms of size, the computational cost of alignment procedures increase prohibitively. In this work, we propose a novel approach to circumvent these costs via proxy-based test-time alignment, i.e. using guidance from a small aligned model. Our approach can be described as token-specific cascading method, where the token-specific deferral rule is reduced to 0-1 knapsack problem. In this setting, we derive primal and dual approximations of the optimal deferral decision. We experimentally show the benefits of our method both in task performance and speculative decoding speed.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Elastic Architecture Search for Efficient Language Models</title>
<link>https://arxiv.org/abs/2510.27037</link>
<guid>https://arxiv.org/abs/2510.27037</guid>
<content:encoded><![CDATA[
<div> Elastic Language Model, Neural Architecture Search, Compact Language Models, Efficient Transformer Blocks, Dynamic Modules<br />
<br />Summary: 
The paper introduces the Elastic Language Model (ELM) as a novel Neural Architecture Search (NAS) method aimed at optimizing compact language models. ELM enhances existing NAS approaches by introducing a flexible search space with efficient transformer blocks and dynamic modules for dimension and head number adjustment. The innovation in the search process allows for a more thorough exploration of model architectures. Additionally, novel knowledge distillation losses are introduced to preserve the unique characteristics of each block, improving the discrimination between architectural choices during the search process. Experiment results on masked language modeling and causal language modeling tasks show that models discovered by ELM outperform existing methods significantly. <div>
arXiv:2510.27037v1 Announce Type: new 
Abstract: As large pre-trained language models become increasingly critical to natural language understanding (NLU) tasks, their substantial computational and memory requirements have raised significant economic and environmental concerns. Addressing these challenges, this paper introduces the Elastic Language Model (ELM), a novel neural architecture search (NAS) method optimized for compact language models. ELM extends existing NAS approaches by introducing a flexible search space with efficient transformer blocks and dynamic modules for dimension and head number adjustment. These innovations enhance the efficiency and flexibility of the search process, which facilitates more thorough and effective exploration of model architectures. We also introduce novel knowledge distillation losses that preserve the unique characteristics of each block, in order to improve the discrimination between architectural choices during the search process. Experiments on masked language modeling and causal language modeling tasks demonstrate that models discovered by ELM significantly outperform existing methods.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dataset Creation and Baseline Models for Sexism Detection in Hausa</title>
<link>https://arxiv.org/abs/2510.27038</link>
<guid>https://arxiv.org/abs/2510.27038</guid>
<content:encoded><![CDATA[
<div> Keywords: sexism detection, low-resource languages, cultural nuances, machine learning, multilingual language models 

Summary: 
This study addresses the issue of sexism detection in low-resource languages, focusing on the specific case of the Hausa language. Through the creation of a Hausa sexism detection dataset and user studies involving native speakers, the study aims to understand how sexism is expressed in everyday discourse in this cultural context. The research explores the challenges of capturing cultural nuances, particularly with idiomatic expressions, and the tendency for false positives in detecting sexism in Hausa. The study experiments with traditional machine learning classifiers and multilingual language models to detect sexism in Hausa text, highlighting the effectiveness of few-shot learning techniques. Overall, the study emphasizes the importance of cultural sensitivity and linguistic representation in developing effective sexism detection strategies for low-resource languages like Hausa.<br /><br />Summary: <div>
arXiv:2510.27038v1 Announce Type: new 
Abstract: Sexism reinforces gender inequality and social exclusion by perpetuating stereotypes, bias, and discriminatory norms. Noting how online platforms enable various forms of sexism to thrive, there is a growing need for effective sexism detection and mitigation strategies. While computational approaches to sexism detection are widespread in high-resource languages, progress remains limited in low-resource languages where limited linguistic resources and cultural differences affect how sexism is expressed and perceived. This study introduces the first Hausa sexism detection dataset, developed through community engagement, qualitative coding, and data augmentation. For cultural nuances and linguistic representation, we conducted a two-stage user study (n=66) involving native speakers to explore how sexism is defined and articulated in everyday discourse. We further experiment with both traditional machine learning classifiers and pre-trained multilingual language models and evaluating the effectiveness few-shot learning in detecting sexism in Hausa. Our findings highlight challenges in capturing cultural nuance, particularly with clarification-seeking and idiomatic expressions, and reveal a tendency for many false positives in such cases.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantitative Intertextuality from the Digital Humanities Perspective: A Survey</title>
<link>https://arxiv.org/abs/2510.27045</link>
<guid>https://arxiv.org/abs/2510.27045</guid>
<content:encoded><![CDATA[
<div> Keywords: intertextuality, natural language processing, quantitative studies, data analysis, interdisciplinary research

Summary:
Intertextuality, the connection between texts, has been a crucial concept in literary theory and is now being quantitatively studied using advancements in natural language processing. This paper provides a roadmap for such studies, exploring data, methods, and applications. It reviews methods ranging from statistics to deep learning, drawing on data from various languages and topics. The applications of quantitative intertextuality research in humanities and social sciences are also discussed, along with the associated platform tools. With advances in computer technology, more precise and large-scale intertext studies are expected, presenting opportunities for interdisciplinary research bridging AI and the humanities. <div>
arXiv:2510.27045v1 Announce Type: new 
Abstract: The connection between texts is referred to as intertextuality in literary theory, which served as an important theoretical basis in many digital humanities studies. Over the past decade, advancements in natural language processing have ushered intertextuality studies into the quantitative age. Large-scale intertextuality research based on cutting-edge methods has continuously emerged. This paper provides a roadmap for quantitative intertextuality studies, summarizing their data, methods, and applications. Drawing on data from multiple languages and topics, this survey reviews methods from statistics to deep learning. It also summarizes their applications in humanities and social sciences research and the associated platform tools. Driven by advances in computer technology, more precise, diverse, and large-scale intertext studies can be anticipated. Intertextuality holds promise for broader application in interdisciplinary research bridging AI and the humanities.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recursive numeral systems are highly regular and easy to process</title>
<link>https://arxiv.org/abs/2510.27049</link>
<guid>https://arxiv.org/abs/2510.27049</guid>
<content:encoded><![CDATA[
<div> recursive numeral systems, lexicon size, morphosyntactic complexity, regularity, processing complexity

Summary: 
The study examines the optimization of trade-offs between lexicon size and morphosyntactic complexity in recursive numeral systems. It argues that the trade-off has been inadequate due to the neglect of regularity, a crucial aspect of complexity in human grammars. Using the Minimum Description Length approach, the study suggests that recursive numeral systems should be viewed as efficient in terms of regularity and processing complexity. The proposed MDL-based measures effectively differentiate between natural and unnatural systems, including optimal recursive numeral systems. The ad-hoc constraints in previous literature naturally stem from regularity considerations. The study emphasizes the importance of incorporating regularity in language studies to measure and explain optimality effectively. 

<br /><br />Summary: <div>
arXiv:2510.27049v1 Announce Type: new 
Abstract: Previous work has argued that recursive numeral systems optimise the trade-off between lexicon size and average morphosyntatic complexity (Deni\'c and Szymanik, 2024). However, showing that only natural-language-like systems optimise this tradeoff has proven elusive, and the existing solution has relied on ad-hoc constraints to rule out unnatural systems (Yang and Regier, 2025). Here, we argue that this issue arises because the proposed trade-off has neglected regularity, a crucial aspect of complexity central to human grammars in general. Drawing on the Minimum Description Length (MDL) approach, we propose that recursive numeral systems are better viewed as efficient with regard to their regularity and processing complexity. We show that our MDL-based measures of regularity and processing complexity better capture the key differences between attested, natural systems and unattested but possible ones, including "optimal" recursive numeral systems from previous work, and that the ad-hoc constraints from previous literature naturally follow from regularity. Our approach highlights the need to incorporate regularity across sets of forms in studies that attempt to measure and explain optimality in language.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VISTA Score: Verification In Sequential Turn-based Assessment</title>
<link>https://arxiv.org/abs/2510.27052</link>
<guid>https://arxiv.org/abs/2510.27052</guid>
<content:encoded><![CDATA[
<div> factuality, verification, conversation, dialogue systems, hallucination
Summary: 
- Hallucination remains a significant problem in conversational AI systems, hindering their deployment in contexts requiring factual accuracy. 
- Current metrics for evaluating factuality in dialogue systems are limited in their effectiveness for multi-turn conversations.
- The VISTA framework introduces a new approach to assessing conversational factuality through claim-level verification and tracking of sequential consistency.
- VISTA decomposes each turn of a conversational assistant into atomic factual claims, verifying them against trusted sources and dialogue history, and categorizing unverifiable statements.
- VISTA significantly improves hallucination detection compared to existing baselines and offers a more transparent and human-aligned measure of truthfulness in dialogue systems.
<br /><br /> <div>
arXiv:2510.27052v1 Announce Type: new 
Abstract: Hallucination--defined here as generating statements unsupported or contradicted by available evidence or conversational context--remains a major obstacle to deploying conversational AI systems in settings that demand factual reliability. Existing metrics either evaluate isolated responses or treat unverifiable content as errors, limiting their use for multi-turn dialogue. We introduce VISTA (Verification In Sequential Turn-based Assessment), a framework for evaluating conversational factuality through claim-level verification and sequential consistency tracking. VISTA decomposes each assistant turn into atomic factual claims, verifies them against trusted sources and dialogue history, and categorizes unverifiable statements (subjective, contradicted, lacking evidence, or abstaining). Across eight large language models and four dialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTA substantially improves hallucination detection over FACTSCORE and LLM-as-Judge baselines. Human evaluation confirms that VISTA's decomposition improves annotator agreement and reveals inconsistencies in existing benchmarks. By modeling factuality as a dynamic property of conversation, VISTA offers a more transparent, human-aligned measure of truthfulness in dialogue systems.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Centric RAG with Multi-Granular Indexing and Confidence Constraints</title>
<link>https://arxiv.org/abs/2510.27054</link>
<guid>https://arxiv.org/abs/2510.27054</guid>
<content:encoded><![CDATA[
<div> memory, indexing, uncertainty estimation, generation, confidence control

Summary:
The paper presents a novel confidence control method for retrieval-augmented generation in complex knowledge environments. It introduces a hierarchical memory structure with multi-granularity indexing to facilitate dynamic retrieval and generation. An uncertainty estimation mechanism filters low-confidence paths during generation to maintain information coverage and suppress noise. The method combines generation loss, entropy constraints, and variance regularization in a unified framework for confidence control. Experimental results demonstrate superior performance in QA accuracy, retrieval recall, ranking quality, and factual consistency compared to existing models. The study highlights the effectiveness of integrating multi-granularity indexing with confidence control in improving the reliability and controllability of large models in complex contexts.  <br /><br />Summary: <div>
arXiv:2510.27054v1 Announce Type: new 
Abstract: This paper addresses the issues of insufficient coverage, unstable results, and limited reliability in retrieval-augmented generation under complex knowledge environments, and proposes a confidence control method that integrates multi-granularity memory indexing with uncertainty estimation. The method builds a hierarchical memory structure that divides knowledge representations into different levels of granularity, enabling dynamic indexing and retrieval from local details to global context, and thus establishing closer semantic connections between retrieval and generation. On this basis, an uncertainty estimation mechanism is introduced to explicitly constrain and filter low-confidence paths during the generation process, allowing the model to maintain information coverage while effectively suppressing noise and false content. The overall optimization objective consists of generation loss, entropy constraints, and variance regularization, forming a unified confidence control framework. In the experiments, comprehensive sensitivity tests and comparative analyses were designed, covering hyperparameters, environmental conditions, and data structures, to verify the stability and robustness of the proposed method across different scenarios. The results show that the method achieves superior performance over existing models in QA accuracy, retrieval recall, ranking quality, and factual consistency, demonstrating the effectiveness of combining multi-granularity indexing with confidence control. This study not only provides a new technical pathway for retrieval-augmented generation but also offers practical evidence for improving the reliability and controllability of large models in complex contexts.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Data Contamination in LLMs via In-Context Learning</title>
<link>https://arxiv.org/abs/2510.27055</link>
<guid>https://arxiv.org/abs/2510.27055</guid>
<content:encoded><![CDATA[
<div> Contamination Detection, training data, large language models, CoDeC, memorization patterns <br />
Summary: <br />
The article introduces a novel method, CoDeC, for detecting and quantifying contamination in training data of language models. CoDeC analyzes the impact of in-context learning on model performance to distinguish between memorized and out-of-distribution data. It shows that in-context examples improve confidence for new datasets but decrease it for datasets used in training, indicating disrupted memorization patterns. The experiments demonstrate that CoDeC generates contamination scores that effectively separate seen and unseen datasets, uncovering evidence of memorization in open-weight models with undisclosed training data. This method is simple, automated, and applicable to various models and datasets, facilitating its integration into benchmark evaluations. <div>
arXiv:2510.27055v1 Announce Type: new 
Abstract: We present Contamination Detection via Context (CoDeC), a practical and accurate method to detect and quantify training data contamination in large language models. CoDeC distinguishes between data memorized during training and data outside the training distribution by measuring how in-context learning affects model performance. We find that in-context examples typically boost confidence for unseen datasets but may reduce it when the dataset was part of training, due to disrupted memorization patterns. Experiments show that CoDeC produces interpretable contamination scores that clearly separate seen and unseen datasets, and reveals strong evidence of memorization in open-weight models with undisclosed training corpora. The method is simple, automated, and both model- and dataset-agnostic, making it easy to integrate with benchmark evaluations.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrastive Knowledge Transfer and Robust Optimization for Secure Alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2510.27077</link>
<guid>https://arxiv.org/abs/2510.27077</guid>
<content:encoded><![CDATA[
<div> Keywords: large-scale language models, safety alignment, robustness, contrastive distillation, noise-robust training

Summary: 
This paper presents a novel fine-tuning method to address the limitations of large-scale language models in safety alignment and robustness. By combining contrastive distillation with noise-robust training, the method improves semantic consistency and alignment accuracy by transferring knowledge boundaries from a teacher model to a student model. Introducing noise perturbations and robust optimization constraints during training ensures stable predictive outputs under noisy and uncertain inputs. The framework includes distillation loss, robustness loss, and a regularization term to balance alignment ability with resistance to interference. Experimental validation demonstrates superior performance in knowledge transfer, robustness, and overall safety compared to existing baselines. This work contributes to the theoretical development of parameter-efficient fine-tuning and offers a new solution for creating safer and more reliable alignment mechanisms.<br /><br />Summary: <div>
arXiv:2510.27077v1 Announce Type: new 
Abstract: This paper addresses the limitations of large-scale language models in safety alignment and robustness by proposing a fine-tuning method that combines contrastive distillation with noise-robust training. The method freezes the backbone model and transfers the knowledge boundaries of the teacher model to the student model through distillation, thereby improving semantic consistency and alignment accuracy. At the same time, noise perturbations and robust optimization constraints are introduced during training to ensure that the model maintains stable predictive outputs under noisy and uncertain inputs. The overall framework consists of distillation loss, robustness loss, and a regularization term, forming a unified optimization objective that balances alignment ability with resistance to interference. To systematically validate its effectiveness, the study designs experiments from multiple perspectives, including distillation weight sensitivity, stability analysis under computation budgets and mixed-precision environments, and the impact of data noise and distribution shifts on model performance. Results show that the method significantly outperforms existing baselines in knowledge transfer, robustness, and overall safety, achieving the best performance across several key metrics. This work not only enriches the theoretical system of parameter-efficient fine-tuning but also provides a new solution for building safer and more trustworthy alignment mechanisms.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterizing Selective Refusal Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2510.27087</link>
<guid>https://arxiv.org/abs/2510.27087</guid>
<content:encoded><![CDATA[
<div> guardrails, language models, biases, refusal rates, safety implications  
Summary:  
The study examines how safety guardrails in large language models (LLMs) can potentially introduce biases by selectively refusing to generate harmful content targeting certain demographic groups. The researchers analyze refusal rates of different demographic attributes such as gender, sexual orientation, nationality, and religion, and find evidence of selective refusal bias. They also investigate the types of responses generated by LLMs and the length of refusals. The findings suggest the need for more equitable and robust performance in safety guardrails across all demographic groups. Additionally, the study explores the safety implications of targeting previously refused groups through an indirect attack. Overall, the research underscores the importance of addressing biases in LLM guardrails to ensure fair and effective protection against generating toxic content.  
<br /><br />Summary: <div>
arXiv:2510.27087v1 Announce Type: new 
Abstract: Safety guardrails in large language models(LLMs) are developed to prevent malicious users from generating toxic content at a large scale. However, these measures can inadvertently introduce or reflect new biases, as LLMs may refuse to generate harmful content targeting some demographic groups and not others. We explore this selective refusal bias in LLM guardrails through the lens of refusal rates of targeted individual and intersectional demographic groups, types of LLM responses, and length of generated refusals. Our results show evidence of selective refusal bias across gender, sexual orientation, nationality, and religion attributes. This leads us to investigate additional safety implications via an indirect attack, where we target previously refused groups. Our findings emphasize the need for more equitable and robust performance in safety guardrails across demographic groups.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rating Roulette: Self-Inconsistency in LLM-As-A-Judge Frameworks</title>
<link>https://arxiv.org/abs/2510.27106</link>
<guid>https://arxiv.org/abs/2510.27106</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural Language Generation, large language models, evaluation, intra-rater reliability, inconsistency

Summary:
In evaluating Natural Language Generation (NLG), the use of large language models (LLMs) for judgment has become popular due to their closer alignment with human preferences. However, experiments have shown that LLM judges exhibit low intra-rater reliability, with inconsistent scores assigned across different runs. This inconsistency can lead to arbitrary ratings, challenging the measurement of judgment accuracy. The study quantifies this variability across various NLG tasks and benchmarks to assess the usefulness of LLM judges with proper guidelines. The findings highlight the importance of addressing reliability issues to enhance the reliability and effectiveness of NLG assessment. 

<br /><br />Summary: <div>
arXiv:2510.27106v1 Announce Type: new 
Abstract: As Natural Language Generation (NLG) continues to be widely adopted, properly assessing it has become quite difficult. Lately, using large language models (LLMs) for evaluating these generations has gained traction, as they tend to align more closely with human preferences than conventional n-gram or embedding-based metrics. In our experiments, we show that LLM judges have low intra-rater reliability in their assigned scores across different runs. This variance makes their ratings inconsistent, almost arbitrary in the worst case, making it difficult to measure how good their judgments actually are. We quantify this inconsistency across different NLG tasks and benchmarks and see if judicious use of LLM judges can still be useful following proper guidelines.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probability Distributions Computed by Hard-Attention Transformers</title>
<link>https://arxiv.org/abs/2510.27118</link>
<guid>https://arxiv.org/abs/2510.27118</guid>
<content:encoded><![CDATA[
<div> Characterization, probability distributions, expressivity, autoregressive, language models  
<br />  
Summary:  
Transformers are commonly used as language models, generating strings probabilistically by predicting the next token in an autoregressive manner. This study focuses on the expressivity of transformer language models, analyzing the ability to represent probability distributions. The authors find that incorporating autoregression into transformer language recognizers can enhance their expressivity. Additionally, they show that introducing probabilistic elements can lead to differences in expressiveness compared to the non-probabilistic case. By examining the functions transformers can express in the context of language modeling, this research contributes to a deeper understanding of the capabilities and limitations of transformer models in natural language processing applications.  
<br /> <div>
arXiv:2510.27118v1 Announce Type: new 
Abstract: Most expressivity results for transformers treat them as language recognizers (which accept or reject strings), and not as they are used in practice, as language models (which generate strings autoregressively and probabilistically). Here, we characterize the probability distributions that transformer language models can express. We show that making transformer language recognizers autoregressive can sometimes increase their expressivity, and that making them probabilistic can break equivalences that hold in the non-probabilistic case. Our overall contribution is to tease apart what functions transformers are capable of expressing, in their most common use-case as language models.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Additions, Substantial Gains: Expanding Scripts, Languages, and Lineage Coverage in URIEL+</title>
<link>https://arxiv.org/abs/2510.27183</link>
<guid>https://arxiv.org/abs/2510.27183</guid>
<content:encoded><![CDATA[
<div> script vectors, writing system properties, Glottolog, language coverage, genealogical lineage

Summary:
The article introduces enhancements to the URIEL+ linguistic knowledge base to address data sparsity and improve cross-lingual transfer support, focusing on low-resource languages. It extends URIEL+ by incorporating script vectors for 7,488 languages, integrating Glottolog to include 18,710 additional languages, and expanding lineage imputation for 26,449 languages. These additions reduce feature sparsity, increase language coverage significantly, and enhance imputation quality metrics. Benchmarking on cross-lingual transfer tasks reveals improved performance compared to the original URIEL+, with gains of up to 6% in specific scenarios. These advancements make URIEL+ more comprehensive and suitable for multilingual research, enabling better support for low-resource languages. 

<br /><br />Summary: <div>
arXiv:2510.27183v1 Announce Type: new 
Abstract: The URIEL+ linguistic knowledge base supports multilingual research by encoding languages through geographic, genetic, and typological vectors. However, data sparsity remains prevalent, in the form of missing feature types, incomplete language entries, and limited genealogical coverage. This limits the usefulness of URIEL+ in cross-lingual transfer, particularly for supporting low-resource languages. To address this sparsity, this paper extends URIEL+ with three contributions: introducing script vectors to represent writing system properties for 7,488 languages, integrating Glottolog to add 18,710 additional languages, and expanding lineage imputation for 26,449 languages by propagating typological and script features across genealogies. These additions reduce feature sparsity by 14% for script vectors, increase language coverage by up to 19,015 languages (1,007%), and improve imputation quality metrics by up to 33%. Our benchmark on cross-lingual transfer tasks (oriented around low-resource languages) shows occasionally divergent performance compared to URIEL+, with performance gains up to 6% in certain setups. Our advances make URIEL+ more complete and inclusive for multilingual research.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemeArena: Automating Context-Aware Unbiased Evaluation of Harmfulness Understanding for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2510.27196</link>
<guid>https://arxiv.org/abs/2510.27196</guid>
<content:encoded><![CDATA[
<div> Keywords: memes, social media, multimodal Large Language Models, harmfulness, evaluation

Summary:
In the era of social media, the abundance of memes has raised the need for Large Language Models (LLMs) to effectively analyze multimodal harmful content. Current evaluation approaches often fall short in providing a comprehensive understanding of harmfulness in diverse contexts. To address this gap, the authors introduce MemeArena, an innovative evaluation framework that assesses mLLMs' comprehension of multimodal harmfulness in various interpretive scenarios. MemeArena creates a context-based environment for mLLMs to analyze harmful content, promoting unbiased evaluations and allowing for nuanced assessments. Through a series of experiments, the framework successfully minimizes biases in judgment, yielding results that closely mirror human perspectives. MemeArena offers a valuable tool for evaluating the effectiveness of mLLMs in interpreting harmful content on social media platforms. <div>
arXiv:2510.27196v1 Announce Type: new 
Abstract: The proliferation of memes on social media necessitates the capabilities of multimodal Large Language Models (mLLMs) to effectively understand multimodal harmfulness. Existing evaluation approaches predominantly focus on mLLMs' detection accuracy for binary classification tasks, which often fail to reflect the in-depth interpretive nuance of harmfulness across diverse contexts. In this paper, we propose MemeArena, an agent-based arena-style evaluation framework that provides a context-aware and unbiased assessment for mLLMs' understanding of multimodal harmfulness. Specifically, MemeArena simulates diverse interpretive contexts to formulate evaluation tasks that elicit perspective-specific analyses from mLLMs. By integrating varied viewpoints and reaching consensus among evaluators, it enables fair and unbiased comparisons of mLLMs' abilities to interpret multimodal harmfulness. Extensive experiments demonstrate that our framework effectively reduces the evaluation biases of judge agents, with judgment results closely aligning with human preferences, offering valuable insights into reliable and comprehensive mLLM evaluations in multimodal harmfulness understanding. Our code and data are publicly available at https://github.com/Lbotirx/MemeArena.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying the Periodicity of Information in Natural Language</title>
<link>https://arxiv.org/abs/2510.27241</link>
<guid>https://arxiv.org/abs/2510.27241</guid>
<content:encoded><![CDATA[
<div> Keywords: information density, natural language, periodicity pattern, AutoPeriod of Surprisal, LLM-generation detection 

Summary: 
A new method called AutoPeriod of Surprisal (APS) has been introduced to analyze the periodicity pattern in the information encoded in natural language. The study found that a significant portion of human language shows a strong periodicity in its information content. Additionally, new periods outside of typical structural units in text were identified and confirmed through harmonic regression modeling. The periodicity in language appears to be influenced by both structured factors and other factors that operate over longer distances. The study also discusses the advantages of the APS method in detecting periodicity and its potential applications in LLM-generation detection. <div>
arXiv:2510.27241v1 Announce Type: new 
Abstract: Recent theoretical advancement of information density in natural language has brought the following question on desk: To what degree does natural language exhibit periodicity pattern in its encoded information? We address this question by introducing a new method called AutoPeriod of Surprisal (APS). APS adopts a canonical periodicity detection algorithm and is able to identify any significant periods that exist in the surprisal sequence of a single document. By applying the algorithm to a set of corpora, we have obtained the following interesting results: Firstly, a considerable proportion of human language demonstrates a strong pattern of periodicity in information; Secondly, new periods that are outside the distributions of typical structural units in text (e.g., sentence boundaries, elementary discourse units, etc.) are found and further confirmed via harmonic regression modeling. We conclude that the periodicity of information in language is a joint outcome from both structured factors and other driving factors that take effect at longer distances. The advantages of our periodicity detection method and its potentials in LLM-generation detection are further discussed.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in LLMs</title>
<link>https://arxiv.org/abs/2510.27246</link>
<guid>https://arxiv.org/abs/2510.27246</guid>
<content:encoded><![CDATA[
<div> memory abilities, long-context reasoning, large language models, benchmark, conversational settings
<br />
Summary: 
This paper introduces a new benchmark called BEAM for testing the abilities of large language models in long-context reasoning tasks in conversational settings. The benchmark includes 100 coherent and diverse conversations with 2,000 probing questions. The authors propose a framework called LIGHT that equips LLMs with three memory systems to enhance performance: long-term episodic memory, short-term working memory, and a scratchpad for important facts. Experiments on BEAM show that even LLMs with 1M token context windows struggle with longer dialogues, but LIGHT consistently improves performance, showing an average improvement of 3.5%-12.69% over existing baselines. An ablation study confirms the importance of each memory component in enhancing LLM performance. <br /><br />Summary: <div>
arXiv:2510.27246v1 Announce Type: new 
Abstract: Evaluating the abilities of large language models (LLMs) for tasks that require long-term memory and thus long-context reasoning, for example in conversational settings, is hampered by the existing benchmarks, which often lack narrative coherence, cover narrow domains, and only test simple recall-oriented tasks. This paper introduces a comprehensive solution to these challenges. First, we present a novel framework for automatically generating long (up to 10M tokens), coherent, and topically diverse conversations, accompanied by probing questions targeting a wide range of memory abilities. From this, we construct BEAM, a new benchmark comprising 100 conversations and 2,000 validated questions. Second, to enhance model performance, we propose LIGHT-a framework inspired by human cognition that equips LLMs with three complementary memory systems: a long-term episodic memory, a short-term working memory, and a scratchpad for accumulating salient facts. Our experiments on BEAM reveal that even LLMs with 1M token context windows (with and without retrieval-augmentation) struggle as dialogues lengthen. In contrast, LIGHT consistently improves performance across various models, achieving an average improvement of 3.5%-12.69% over the strongest baselines, depending on the backbone LLM. An ablation study further confirms the contribution of each memory component.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Languages are Modalities: Cross-Lingual Alignment via Encoder Injection</title>
<link>https://arxiv.org/abs/2510.27254</link>
<guid>https://arxiv.org/abs/2510.27254</guid>
<content:encoded><![CDATA[
<div> method, large language models, low resource languages, cross-lingual alignment, latent language injection

Summary:
LLINK introduces a method called Latent Language Injection for Non-English Knowledge, aiming to address the underperformance of instruction-tuned Large Language Models (LLMs) on low resource, non-Latin scripts. The approach involves aligning sentence embeddings from a multilingual encoder to the decoder's latent space and expanding it into soft slots. With minimal adapters, the frozen decoder consumes the signal without the need to change the tokenizer or retrain. This method significantly enhances bilingual retrieval, achieving a preference of 81.3% over the base model. The improvements can be attributed to reduced tokenization inflation and a stronger cross-lingual alignment, though numeric fidelity remains a challenge. By treating low resource languages as a modality, LLINK provides a practical pathway for enhancing cross-lingual alignment in lightweight LLMs. <br /><br />Summary: <div>
arXiv:2510.27254v1 Announce Type: new 
Abstract: Instruction-tuned Large Language Models (LLMs) underperform on low resource, non-Latin scripts due to tokenizer fragmentation and weak cross-lingual coupling. We present LLINK (Latent Language Injection for Non-English Knowledge), a compute efficient language-as-modality method that conditions an instruction-tuned decoder without changing the tokenizer or retraining the decoder. First, we align sentence embeddings from a frozen multilingual encoder to the decoder's latent embedding space at a reserved position via a lightweight contrastive projector. Second, the vector is expanded into K soft slots and trained with minimal adapters so the frozen decoder consumes the signal. LLINK substantially improves bilingual retrieval and achieves 81.3% preference over the base model and 63.6% over direct fine-tuning in LLM-judged Q&amp;A evaluations. We further find that improvements can be attributed to reduced tokenization inflation and a stronger cross lingual alignment, despite the model having residual weaknesses in numeric fidelity. Treating low resource languages as a modality offers a practical path to stronger cross-lingual alignment in lightweight LLMs.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2510.27267</link>
<guid>https://arxiv.org/abs/2510.27267</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, medical calculations, benchmark, reinforcement learning, clinical reasoning

Summary:
Large language models are increasingly being used in the medical domain, but existing benchmarks often overlook quantitative reasoning crucial for clinical decision-making. To address this gap, the authors introduce MedCalc-Eval, a comprehensive benchmark comprising over 700 medical calculation tasks spanning various specialties. They also develop MedCalc-Env, a reinforcement learning environment, to enhance performance and enable multi-step clinical reasoning. Fine-tuning a Qwen2.5-32B model within this environment leads to state-of-the-art results, demonstrating improvements in numerical sensitivity, formula selection, and reasoning robustness. Despite these advancements, challenges like unit conversion, multi-condition logic, and contextual understanding remain to be addressed. The code and datasets for MedCalc-Eval are available on GitHub, providing a valuable resource for researchers and practitioners in the field. 

<br /><br />Summary: <div>
arXiv:2510.27267v1 Announce Type: new 
Abstract: As large language models (LLMs) enter the medical domain, most benchmarks evaluate them on question answering or descriptive reasoning, overlooking quantitative reasoning critical to clinical decision-making. Existing datasets like MedCalc-Bench cover few calculation tasks and fail to reflect real-world computational scenarios.
  We introduce MedCalc-Eval, the largest benchmark for assessing LLMs' medical calculation abilities, comprising 700+ tasks across two types: equation-based (e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar, Glasgow Coma Scale). These tasks span diverse specialties including internal medicine, surgery, pediatrics, and cardiology, offering a broader and more challenging evaluation setting.
  To improve performance, we further develop MedCalc-Env, a reinforcement learning environment built on the InternBootcamp framework, enabling multi-step clinical reasoning and planning. Fine-tuning a Qwen2.5-32B model within this environment achieves state-of-the-art results on MedCalc-Eval, with notable gains in numerical sensitivity, formula selection, and reasoning robustness. Remaining challenges include unit conversion, multi-condition logic, and contextual understanding.
  Code and datasets are available at https://github.com/maokangkun/MedCalc-Eval.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Do Multilingual Reasoning Gaps Emerge in Reasoning Language Models?</title>
<link>https://arxiv.org/abs/2510.27269</link>
<guid>https://arxiv.org/abs/2510.27269</guid>
<content:encoded><![CDATA[
<div> Keywords: Reasoning language models, multilingual reasoning gap, language understanding, Selective Translation, detection methods

Summary: 
Reasoning language models (RLMs) excel at complex reasoning tasks but struggle with a multilingual reasoning gap, performing better in high-resource languages. The primary cause of this gap is attributed to language understanding failures, hindering the model's ability to represent multilingual input meanings in English within its reasoning trace. By detecting understanding failures using supervised methods, a simple yet effective strategy called Selective Translation is proposed. This strategy translates multilingual inputs into English only when an understanding failure is detected, successfully bridging the multilingual reasoning gap by achieving near full-translation performance with a minimal translation requirement. This research sheds light on the origin of the multilingual reasoning gap and provides a promising approach towards more equitable multilingual reasoning. <div>
arXiv:2510.27269v1 Announce Type: new 
Abstract: Reasoning language models (RLMs) achieve strong performance on complex reasoning tasks, yet they still suffer from a multilingual reasoning gap, performing better in high-resource languages than in low-resource ones. While recent efforts have reduced this gap, its underlying causes remain largely unexplored. In this paper, we address this by showing that the multilingual reasoning gap largely stems from failures in language understanding-the model's inability to represent the multilingual input meaning into the dominant language (i.e., English) within its reasoning trace. This motivates us to examine whether understanding failures can be detected, as this ability could help mitigate the multilingual reasoning gap. To this end, we evaluate a range of detection methods and find that understanding failures can indeed be identified, with supervised approaches performing best. Building on this, we propose Selective Translation, a simple yet effective strategy that translates the multilingual input into English only when an understanding failure is detected. Experimental results show that Selective Translation bridges the multilingual reasoning gap, achieving near full-translation performance while using translation for only about 20% of inputs. Together, our work demonstrates that understanding failures are the primary cause of the multilingual reasoning gap and can be detected and selectively mitigated, providing key insight into its origin and a promising path toward more equitable multilingual reasoning. Our code and data are publicly available at https://github.com/deokhk/RLM_analysis.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Representation Underlying the Judgment of Large Language Models</title>
<link>https://arxiv.org/abs/2510.27328</link>
<guid>https://arxiv.org/abs/2510.27328</guid>
<content:encoded><![CDATA[
<div> neural representations, Large Language Models, Valence-Assent Axis, evaluative judgments, reasoning<br />
Summary: 
This study explores the architectural organization of judgment in both biological and artificial intelligence. The researchers examine Large Language Models (LLMs) and find that diverse evaluative judgments are computed along a dominant dimension termed the Valence-Assent Axis (VAA). This axis jointly encodes subjective valence and the model's assent to factual claims. The study shows that the VAA functions as a control signal that guides the generative process to construct a rationale consistent with its evaluative state, even at the expense of factual accuracy. This mechanism, termed the subordination of reasoning, shifts reasoning from impartial inference to goal-directed justification. The discovery sheds light on how biases and hallucinations can arise in systems that prioritize coherent judgment over faithful reasoning.<br /><br />Summary: <div>
arXiv:2510.27328v1 Announce Type: new 
Abstract: A central architectural question for both biological and artificial intelligence is whether judgment relies on specialized modules or a unified, domain-general resource. While the discovery of decodable neural representations for distinct concepts in Large Language Models (LLMs) has suggested a modular architecture, whether these representations are truly independent systems remains an open question. Here we provide evidence for a convergent architecture. Across a range of LLMs, we find that diverse evaluative judgments are computed along a dominant dimension, which we term the Valence-Assent Axis (VAA). This axis jointly encodes subjective valence ("what is good") and the model's assent to factual claims ("what is true"). Through direct interventions, we show this unified representation creates a critical dependency: the VAA functions as a control signal that steers the generative process to construct a rationale consistent with its evaluative state, even at the cost of factual accuracy. This mechanism, which we term the subordination of reasoning, shifts the process of reasoning from impartial inference toward goal-directed justification. Our discovery offers a mechanistic account for systemic bias and hallucination, revealing how an architecture that promotes coherent judgment can systematically undermine faithful reasoning.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TransAlign: Machine Translation Encoders are Strong Word Aligners, Too</title>
<link>https://arxiv.org/abs/2510.27337</link>
<guid>https://arxiv.org/abs/2510.27337</guid>
<content:encoded><![CDATA[
<div> Keywords: translation-based strategies, token classification tasks, word aligners, machine translation models, cross-lingual transfer

Summary:
TransAlign is introduced as a novel word aligner that uses the encoder of a massively multilingual machine translation model. It focuses on label projection for token classification tasks in cross-lingual transfer scenarios. By leveraging TransAlign, the system achieves strong word alignment performance, surpassing existing word aligners and state-of-the-art non-aligner-based label projection methods in machine translation-based cross-lingual transfer for token classification tasks. The traditional approaches of translate-test and translate-train are enhanced through TransAlign, providing a more effective and accurate method for leveraging machine translation models in cross-lingual transfer tasks. This research highlights the importance of word alignment in enhancing the performance of cross-lingual transfer for token classification tasks and demonstrates the superiority of TransAlign over other existing methods. <div>
arXiv:2510.27337v1 Announce Type: new 
Abstract: In the absence of sizable training data for most world languages and NLP tasks, translation-based strategies such as translate-test -- evaluating on noisy source language data translated from the target language -- and translate-train -- training on noisy target language data translated from the source language -- have been established as competitive approaches for cross-lingual transfer (XLT). For token classification tasks, these strategies require label projection: mapping the labels from each token in the original sentence to its counterpart(s) in the translation. To this end, it is common to leverage multilingual word aligners (WAs) derived from encoder language models such as mBERT or LaBSE. Despite obvious associations between machine translation (MT) and WA, research on extracting alignments with MT models is largely limited to exploiting cross-attention in encoder-decoder architectures, yielding poor WA results. In this work, in contrast, we propose TransAlign, a novel word aligner that utilizes the encoder of a massively multilingual MT model. We show that TransAlign not only achieves strong WA performance but substantially outperforms popular WA and state-of-the-art non-WA-based label projection methods in MT-based XLT for token classification.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ThoughtProbe: Classifier-Guided LLM Thought Space Exploration via Probing Representations</title>
<link>https://arxiv.org/abs/2510.27355</link>
<guid>https://arxiv.org/abs/2510.27355</guid>
<content:encoded><![CDATA[
<div> inference time framework, Large Language Models, hidden reasoning features, tree structured response space exploration, arithmetic reasoning benchmarks
Summary:
This paper introduces ThoughtProbe, a novel inference time framework leveraging hidden reasoning features of Large Language Models to improve reasoning performance. Unlike previous works manipulating hidden representations for generation, ThoughtProbe uses them as discriminative signals to guide response space exploration. A classifier in each node expansion prioritizes candidates for continuation, leading to efficient resource allocation. The framework collects answers to form a candidate pool and aggregates branch scores to identify the optimal answer. Experimental results demonstrate improved coverage and identification of valid reasoning chains, achieving significant performance boosts across arithmetic reasoning benchmarks. <br /><br />Summary: <div>
arXiv:2510.27355v1 Announce Type: new 
Abstract: This paper introduces ThoughtProbe, a novel inference time framework that leverages the hidden reasoning features of Large Language Models (LLMs) to improve their reasoning performance. Unlike previous works that manipulate the hidden representations to steer LLM generation, we harness them as discriminative signals to guide the tree structured response space exploration. In each node expansion, a classifier serves as a scoring and ranking mechanism that efficiently allocates computational resources by prioritizing higher score candidates for continuation. After completing the tree expansion, we collect answers from all branches to form a candidate answer pool. We then propose a branch aggregation method that marginalizes over all supporting branches by aggregating their CoT scores, thereby identifying the optimal answer from the pool. Experimental results show that our framework's comprehensive exploration not only covers valid reasoning chains but also effectively identifies them, achieving significant improvements across multiple arithmetic reasoning benchmarks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From the Rock Floor to the Cloud: A Systematic Survey of State-of-the-Art NLP in Battery Life Cycle</title>
<link>https://arxiv.org/abs/2510.27369</link>
<guid>https://arxiv.org/abs/2510.27369</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language processing, battery life cycle, digital battery passport, technical language processing, agentic AI <br />
<br />
Summary: 
This study provides a systematic review of the application of natural language processing (NLP) throughout the battery life cycle, introducing a technical language processing (TLP) framework for the digital battery passport (DBP) and other battery predictions. Following the PRISMA method, the researchers reviewed 274 scientific papers and identified 66 relevant papers. The findings highlight the emergence of new NLP tasks in the battery domain, aiding materials discovery and other life cycle stages. Challenges, such as the lack of standard benchmarks, are noted. The proposed TLP framework, featuring agentic AI and optimized prompts, aims to address some of these challenges. Public artifacts of the review are provided for validation and reproducibility. <div>
arXiv:2510.27369v1 Announce Type: new 
Abstract: We present a comprehensive systematic survey of the application of natural language processing (NLP) along the entire battery life cycle, instead of one stage or method, and introduce a novel technical language processing (TLP) framework for the EU's proposed digital battery passport (DBP) and other general battery predictions. We follow the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method and employ three reputable databases or search engines, including Google Scholar, Institute of Electrical and Electronics Engineers Xplore (IEEE Xplore), and Scopus. Consequently, we assessed 274 scientific papers before the critical review of the final 66 relevant papers. We publicly provide artifacts of the review for validation and reproducibility. The findings show that new NLP tasks are emerging in the battery domain, which facilitate materials discovery and other stages of the life cycle. Notwithstanding, challenges remain, such as the lack of standard benchmarks. Our proposed TLP framework, which incorporates agentic AI and optimized prompts, will be apt for tackling some of the challenges.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs</title>
<link>https://arxiv.org/abs/2510.27400</link>
<guid>https://arxiv.org/abs/2510.27400</guid>
<content:encoded><![CDATA[
<div> Knowledge Editing, Large Language Models, Attention Modules, Associative Memory, Edit Success <br />
Summary:<br />
Knowledge editing in large language models (LLMs) is crucial for updating factual information efficiently. While most methods focus on modifying the weights of multilayer perceptron (MLP) modules, this study highlights the significance of attention (Attn) modules in storing and retrieving factual knowledge. The IntAttn-Edit method proposed in this research extends the associative memory paradigm to simultaneously update MLP and Attn modules, utilizing a knowledge balancing strategy to allocate update magnitudes based on each module's contribution to knowledge storage. Experimental results demonstrate that IntAttn-Edit outperforms previous methods in edit success, generalization, and knowledge preservation, particularly by maintaining optimal editing performance across various scenarios. This study sheds light on the importance of balancing updates between different modules for effective knowledge editing in LLMs. <br /> <div>
arXiv:2510.27400v1 Announce Type: new 
Abstract: Knowledge editing has emerged as an efficient approach for updating factual knowledge in large language models (LLMs). It typically locates knowledge storage modules and then modifies their parameters. However, most existing methods focus on the weights of multilayer perceptron (MLP) modules, which are often identified as the main repositories of factual information. Other components, such as attention (Attn) modules, are often ignored during editing. This imbalance can leave residual outdated knowledge and limit editing effectiveness. We perform comprehensive knowledge localization experiments on advanced LLMs and find that Attn modules play a substantial role in factual knowledge storage and retrieval, especially in earlier layers. Based on these insights, we propose IntAttn-Edit, a method that extends the associative memory paradigm to jointly update both MLP and Attn modules. Our approach uses a knowledge balancing strategy that allocates update magnitudes in proportion to each module's measured contribution to knowledge storage. Experiments on standard benchmarks show that IntAttn-Edit achieves higher edit success, better generalization, and stronger knowledge preservation than prior methods. Further analysis shows that the balancing strategy keeps editing performance within an optimal range across diverse settings.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Awal -- Community-Powered Language Technology for Tamazight</title>
<link>https://arxiv.org/abs/2510.27407</link>
<guid>https://arxiv.org/abs/2510.27407</guid>
<content:encoded><![CDATA[
<div> Keywords: Tamazight, language technology, community-powered initiative, data scarcity, community engagement

Summary:
Awal is a community-powered initiative focusing on developing language technology resources for Tamazight. The paper reviews the NLP landscape for Tamazight, highlighting progress in computational resources and the rise of community-driven strategies to combat data scarcity. Launched in 2024, awaldigital.org aims to increase the presence of Tamazight in digital spaces by allowing speakers to contribute translation and voice data. However, barriers such as limited confidence in written Tamazight and standardization challenges hinder widespread participation. Despite positive feedback, data contributions were mainly from linguists and activists, with 6,421 translation pairs and 3 hours of speech data collected in 18 months. The study underscores the challenges of utilizing standard crowdsourcing methods for languages with complex sociolinguistic dynamics. Efforts are underway to develop improved open-source machine translation models using the accumulated data. 

<br /><br />Summary: <div>
arXiv:2510.27407v1 Announce Type: new 
Abstract: This paper presents Awal, a community-powered initiative for developing language technology resources for Tamazight. We provide a comprehensive review of the NLP landscape for Tamazight, examining recent progress in computational resources, and the emergence of community-driven approaches to address persistent data scarcity. Launched in 2024, awaldigital.org platform addresses the underrepresentation of Tamazight in digital spaces through a collaborative platform enabling speakers to contribute translation and voice data. We analyze 18 months of community engagement, revealing significant barriers to participation including limited confidence in written Tamazight and ongoing standardization challenges. Despite widespread positive reception, actual data contribution remained concentrated among linguists and activists. The modest scale of community contributions -- 6,421 translation pairs and 3 hours of speech data -- highlights the limitations of applying standard crowdsourcing approaches to languages with complex sociolinguistic contexts. We are working on improved open-source MT models using the collected data.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Affective Memory Management for Personalized LLM Agents</title>
<link>https://arxiv.org/abs/2510.27418</link>
<guid>https://arxiv.org/abs/2510.27418</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, personalized AI agents, memory management, affective scenarios, Bayesian-inspired update algorithm

Summary: 
The article discusses the challenges of memory management in personalized AI agents and proposes a new memory management system for affective scenarios. The system utilizes a Bayesian-inspired memory update algorithm that incorporates the concept of memory entropy to maintain a dynamically updated memory vector database. This approach aims to minimize global entropy and provide more personalized services, addressing issues such as memory redundancy, staleness, and poor memory-context integration. The authors introduce DABench, a benchmark focusing on emotional expression and change towards objects, to evaluate the system's effectiveness. Experimental results show superior performance in personalization, logical coherence, and accuracy. Ablation studies confirm the effectiveness of the Bayesian-inspired update mechanism in preventing memory bloat. This research provides valuable insights into the design of long-term memory systems in personalized AI agents.

<br /><br />Summary: <div>
arXiv:2510.27418v1 Announce Type: new 
Abstract: Advances in large language models are making personalized AI agents a new research focus. While current agent systems primarily rely on personalized external memory databases to deliver customized experiences, they face challenges such as memory redundancy, memory staleness, and poor memory-context integration, largely due to the lack of effective memory updates during interaction. To tackle these issues, we propose a new memory management system designed for affective scenarios. Our approach employs a Bayesian-inspired memory update algorithm with the concept of memory entropy, enabling the agent to autonomously maintain a dynamically updated memory vector database by minimizing global entropy to provide more personalized services. To better evaluate the system's effectiveness in this context, we propose DABench, a benchmark focusing on emotional expression and emotional change toward objects. Experimental results demonstrate that, our system achieves superior performance in personalization, logical coherence, and accuracy. Ablation studies further validate the effectiveness of the Bayesian-inspired update mechanism in alleviating memory bloat. Our work offers new insights into the design of long-term memory systems.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VCORE: Variance-Controlled Optimization-based Reweighting for Chain-of-Thought Supervision</title>
<link>https://arxiv.org/abs/2510.27462</link>
<guid>https://arxiv.org/abs/2510.27462</guid>
<content:encoded><![CDATA[
<div> Keywords: Supervised fine-tuning, long language models, reasoning abilities, token reweighting, optimization-based reweighting.

Summary:<br /><br />
The article introduces VCORE, a new framework for enhancing the reasoning abilities of large language models through supervised fine-tuning on long chain-of-thought trajectories. VCORE reweights tokens in CoT supervision using an optimization-based approach, allowing for adaptive allocation of supervision and improving generalization in complex reasoning tasks. Empirical evaluations demonstrate that VCORE outperforms existing reweighting methods on mathematical and coding benchmarks, across in-domain and out-of-domain settings. VCORE also proves to be a more effective initialization for subsequent reinforcement learning, providing a stronger foundation for advancing the reasoning capabilities of LLMs. The code for VCORE will be made available on GitHub, providing a valuable resource for researchers and practitioners in the field. <div>
arXiv:2510.27462v1 Announce Type: new 
Abstract: Supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has emerged as a crucial technique for enhancing the reasoning abilities of large language models (LLMs). However, the standard cross-entropy loss treats all tokens equally, ignoring their heterogeneous contributions across a reasoning trajectory. This uniform treatment leads to misallocated supervision and weak generalization, especially in complex, long-form reasoning tasks. To address this, we introduce \textbf{V}ariance-\textbf{C}ontrolled \textbf{O}ptimization-based \textbf{RE}weighting (VCORE), a principled framework that reformulates CoT supervision as a constrained optimization problem. By adopting an optimization-theoretic perspective, VCORE enables a principled and adaptive allocation of supervision across tokens, thereby aligning the training objective more closely with the goal of robust reasoning generalization. Empirical evaluations demonstrate that VCORE consistently outperforms existing token reweighting methods. Across both in-domain and out-of-domain settings, VCORE achieves substantial performance gains on mathematical and coding benchmarks, using models from the Qwen3 series (4B, 8B, 32B) and LLaMA-3.1-8B-Instruct. Moreover, we show that VCORE serves as a more effective initialization for subsequent reinforcement learning, establishing a stronger foundation for advancing the reasoning capabilities of LLMs. The Code will be released at https://github.com/coder-gx/VCORE.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffuse Thinking: Exploring Diffusion Language Models as Efficient Thought Proposers for Reasoning</title>
<link>https://arxiv.org/abs/2510.27469</link>
<guid>https://arxiv.org/abs/2510.27469</guid>
<content:encoded><![CDATA[
<div> large language models, reasoning capabilities, computational efficiency, diffusion language models, collaborative reasoning framework <br />
Summary: 

The article discusses the advancement of large language models (LLMs) in reasoning capabilities and the challenges they face in scaling with test-time computation. It introduces diffusion language models (DLMs) as a more efficient alternative for generating diverse samples in a single forward pass. The proposed collaborative reasoning framework combines the strengths of both LLMs and DLMs, using DLMs to generate candidate thoughts and LLMs to evaluate their quality. Experimental results on various benchmarks demonstrate the effectiveness of this framework in complex reasoning tasks. The approach alleviates the computational burden associated with autoregressive generation while maintaining high quality reasoning outputs. The open-source code for the framework is available for further research and development. <div>
arXiv:2510.27469v1 Announce Type: new 
Abstract: In recent years, large language models (LLMs) have witnessed remarkable advancements, with the test-time scaling law consistently enhancing the reasoning capabilities. Through systematic evaluation and exploration of a diverse spectrum of intermediate thoughts, LLMs demonstrate the potential to generate deliberate reasoning steps, thereby substantially enhancing reasoning accuracy. However, LLMs' autoregressive generation paradigm results in reasoning performance scaling sub-optimally with test-time computation, often requiring excessive computational overhead to propose thoughts while yielding only marginal performance gains. In contrast, diffusion language models (DLMs) can efficiently produce diverse samples through parallel denoising in a single forward pass, inspiring us to leverage them for proposing intermediate thoughts, thereby alleviating the computational burden associated with autoregressive generation while maintaining quality. In this work, we propose an efficient collaborative reasoning framework, leveraging DLMs to generate candidate thoughts and LLMs to evaluate their quality. Experiments across diverse benchmarks demonstrate that our framework achieves strong performance in complex reasoning tasks, offering a promising direction for future research. Our code is open-source at https://anonymous.4open.science/r/Diffuse-Thinking-EC60.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The aftermath of compounds: Investigating Compounds and their Semantic Representations</title>
<link>https://arxiv.org/abs/2510.27477</link>
<guid>https://arxiv.org/abs/2510.27477</guid>
<content:encoded><![CDATA[
<div> Keywords: computational embeddings, English compound words, semantic judgments, GloVe, BERT 

Summary: 
- The study investigates the alignment of computational embeddings with human semantic judgments in English compound words processing. 
- Static word vectors (GloVe) and contextualized embeddings (BERT) are compared against human ratings of lexeme meaning dominance (LMD) and semantic transparency (ST). 
- BERT embeddings outperform GloVe in capturing compositional semantics, highlighting the effectiveness of contextual embeddings in semantic modeling. 
- Predictability ratings are strong predictors of semantic transparency in both human and model data, emphasizing the importance of predictability in compound word processing. 
- The results enhance our understanding of the factors influencing compound word processing and offer valuable insights into embedding-based semantic modeling.

<br /><br />Summary: <div>
arXiv:2510.27477v1 Announce Type: new 
Abstract: This study investigates how well computational embeddings align with human semantic judgments in the processing of English compound words. We compare static word vectors (GloVe) and contextualized embeddings (BERT) against human ratings of lexeme meaning dominance (LMD) and semantic transparency (ST) drawn from a psycholinguistic dataset. Using measures of association strength (Edinburgh Associative Thesaurus), frequency (BNC), and predictability (LaDEC), we compute embedding-derived LMD and ST metrics and assess their relationships with human judgments via Spearmans correlation and regression analyses. Our results show that BERT embeddings better capture compositional semantics than GloVe, and that predictability ratings are strong predictors of semantic transparency in both human and model data. These findings advance computational psycholinguistics by clarifying the factors that drive compound word processing and offering insights into embedding-based semantic modeling.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effect of Domain Generalization Techniques in Low Resource Systems</title>
<link>https://arxiv.org/abs/2510.27512</link>
<guid>https://arxiv.org/abs/2510.27512</guid>
<content:encoded><![CDATA[
<div> counterfactual data augmentation, sentiment classification, low-resource settings, domain generalization, causal representation learning
Summary:<br />
- The study explores two causal domain generalization techniques in low-resource natural language tasks. <br />
- A causal data augmentation (CDA) approach is used to generate counterfactual examples for sentiment classification on the NaijaSenti Twitter corpus to improve robustness. <br />
- An invariant causal representation learning (ICRL) approach using the DINER framework is adapted for multilingual sentiment analysis, enhancing out-of-distribution performance. <br />
- Counterfactual data augmentation shows consistent cross-domain accuracy improvements in sentiment classification. <br />
- Causal representation learning with DINER improves out-of-distribution performance in multilingual sentiment analysis, with varying gains across languages. <br /> <div>
arXiv:2510.27512v1 Announce Type: new 
Abstract: Machine learning models typically assume that training and test data follow the same distribution, an assumption that often fails in real-world scenarios due to distribution shifts. This issue is especially pronounced in low-resource settings, where data scarcity and limited domain diversity hinder robust generalization. Domain generalization (DG) approaches address this challenge by learning features that remain invariant across domains, often using causal mechanisms to improve model robustness. In this study, we examine two distinct causal DG techniques in low-resource natural language tasks. First, we investigate a causal data augmentation (CDA) approach that automatically generates counterfactual examples to improve robustness to spurious correlations. We apply this method to sentiment classification on the NaijaSenti Twitter corpus, expanding the training data with semantically equivalent paraphrases to simulate controlled distribution shifts. Second, we explore an invariant causal representation learning (ICRL) approach using the DINER framework, originally proposed for debiasing aspect-based sentiment analysis. We adapt DINER to a multilingual setting. Our findings demonstrate that both approaches enhance robustness to unseen domains: counterfactual data augmentation yields consistent cross-domain accuracy gains in sentiment classification, while causal representation learning with DINER improves out-of-distribution performance in multilingual sentiment analysis, albeit with varying gains across languages.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BiSparse-AAS: Bilinear Sparse Attention and Adaptive Spans Framework for Scalable and Efficient Text Summarization</title>
<link>https://arxiv.org/abs/2510.27516</link>
<guid>https://arxiv.org/abs/2510.27516</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer-based architectures, text summarization, sparse attention, adaptive spans, bilinear attention

Summary:
BiSparse-AAS is a new framework for text summarization that combines sparse attention, adaptive spans, and bilinear attention to improve efficiency and scalability. Sparse attention focuses on relevant input parts, adaptive spans adjust attention ranges, and bilinear attention models complex token interactions. This framework outperforms existing models in both extractive and abstractive summarization tasks, with significant improvements in ROUGE scores on various datasets. BiSparse-AAS offers a practical solution for real-world text summarization applications by addressing efficiency, scalability, and long-sequence modeling challenges. <div>
arXiv:2510.27516v1 Announce Type: new 
Abstract: Transformer-based architectures have advanced text summarization, yet their quadratic complexity limits scalability on long documents. This paper introduces BiSparse-AAS (Bilinear Sparse Attention with Adaptive Spans), a novel framework that combines sparse attention, adaptive spans, and bilinear attention to address these limitations. Sparse attention reduces computational costs by focusing on the most relevant parts of the input, while adaptive spans dynamically adjust the attention ranges. Bilinear attention complements both by modeling complex token interactions within this refined context. BiSparse-AAS consistently outperforms state-of-the-art baselines in both extractive and abstractive summarization tasks, achieving average ROUGE improvements of about 68.1% on CNN/DailyMail and 52.6% on XSum, while maintaining strong performance on OpenWebText and Gigaword datasets. By addressing efficiency, scalability, and long-sequence modeling, BiSparse-AAS provides a unified, practical solution for real-world text summarization applications.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SQLSpace: A Representation Space for Text-to-SQL to Discover and Mitigate Robustness Gaps</title>
<link>https://arxiv.org/abs/2510.27532</link>
<guid>https://arxiv.org/abs/2510.27532</guid>
<content:encoded><![CDATA[
<div> representation, text-to-SQL, evaluation, benchmark, query rewriting
Summary:<br />
- SQLSpace is introduced as a compact representation for text-to-SQL examples with minimal human intervention.
- The utility of SQLSpace is demonstrated in three use cases: comparing text-to-SQL benchmarks, understanding model performance beyond accuracy scores, and improving model performance through query rewriting.
- SQLSpace enables analysis that is difficult with raw examples alone, such as revealing compositional differences between benchmarks and exposing performance patterns obscured by accuracy.
- The representation supports modeling of query success and enables targeted query rewriting based on correctness estimation.
- The study shows that SQLSpace enhances evaluation by providing a human-interpretable, generalizable framework for text-to-SQL examples. 
<br /><br />Summary: <div>
arXiv:2510.27532v1 Announce Type: new 
Abstract: We introduce SQLSpace, a human-interpretable, generalizable, compact representation for text-to-SQL examples derived with minimal human intervention. We demonstrate the utility of these representations in evaluation with three use cases: (i) closely comparing and contrasting the composition of popular text-to-SQL benchmarks to identify unique dimensions of examples they evaluate, (ii) understanding model performance at a granular level beyond overall accuracy scores, and (iii) improving model performance through targeted query rewriting based on learned correctness estimation. We show that SQLSpace enables analysis that would be difficult with raw examples alone: it reveals compositional differences between benchmarks, exposes performance patterns obscured by accuracy alone, and supports modeling of query success.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patient-Centered Summarization Framework for AI Clinical Summarization: A Mixed-Methods Design</title>
<link>https://arxiv.org/abs/2510.27535</link>
<guid>https://arxiv.org/abs/2510.27535</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Patient-Centered Summaries, Artificial Intelligence, Atrial Fibrillation, Clinical Summarization

Summary:
Patient-Centered Summaries (PCS) are proposed as a new standard for clinical summarization tasks, focusing on patient values and preferences. A framework was developed to generate PCS, incorporating input from patients and clinicians. Patients emphasized lifestyle routines, social support, stressors, and care values, while clinicians sought concise functional, psychosocial, and emotional context. Eight clinicians created gold-standard PCS from atrial fibrillation consultations, used to refine a prompt aligned with guidelines. Five open-source LLMs were evaluated for generating PCS, with Mistral-8B and Llama-3.1-8B performing best in zero-shot and few-shot tasks. While completeness and fluency were similar between experts and models, correctness and patient-centeredness favored human PCS.

<br /><br />Summary: <div>
arXiv:2510.27535v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly demonstrating the potential to reach human-level performance in generating clinical summaries from patient-clinician conversations. However, these summaries often focus on patients' biology rather than their preferences, values, wishes, and concerns. To achieve patient-centered care, we propose a new standard for Artificial Intelligence (AI) clinical summarization tasks: Patient-Centered Summaries (PCS). Our objective was to develop a framework to generate PCS that capture patient values and ensure clinical utility and to assess whether current open-source LLMs can achieve human-level performance in this task. We used a mixed-methods process. Two Patient and Public Involvement groups (10 patients and 8 clinicians) in the United Kingdom participated in semi-structured interviews exploring what personal and contextual information should be included in clinical summaries and how it should be structured for clinical use. Findings informed annotation guidelines used by eight clinicians to create gold-standard PCS from 88 atrial fibrillation consultations. Sixteen consultations were used to refine a prompt aligned with the guidelines. Five open-source LLMs (Llama-3.2-3B, Llama-3.1-8B, Mistral-8B, Gemma-3-4B, and Qwen3-8B) generated summaries for 72 consultations using zero-shot and few-shot prompting, evaluated with ROUGE-L, BERTScore, and qualitative metrics. Patients emphasized lifestyle routines, social support, recent stressors, and care values. Clinicians sought concise functional, psychosocial, and emotional context. The best zero-shot performance was achieved by Mistral-8B (ROUGE-L 0.189) and Llama-3.1-8B (BERTScore 0.673); the best few-shot by Llama-3.1-8B (ROUGE-L 0.206, BERTScore 0.683). Completeness and fluency were similar between experts and models, while correctness and patient-centeredness favored human PCS.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and Multilingual Language Models</title>
<link>https://arxiv.org/abs/2510.27543</link>
<guid>https://arxiv.org/abs/2510.27543</guid>
<content:encoded><![CDATA[
<div> benchmark, Arabic dialects, large language models, evaluation, DialectalArabicMMLU 
Summary: 
The article introduces DialectalArabicMMLU, a benchmark for assessing large language models' performance across Arabic dialects. While existing benchmarks focus on Modern Standard Arabic (MSA), DialectalArabicMMLU addresses the lack of representation of dialectal varieties. It includes 15,000 question-answer pairs in five major dialects and 32 domains, allowing systematic evaluation of LLM reasoning beyond MSA. Assessing 19 LLMs of varying sizes (1B-13B parameters) revealed significant performance differences across dialects, highlighting challenges in dialectal generalization. DialectalArabicMMLU provides a comprehensive resource for evaluating dialectal understanding in Arabic, aiming to enhance inclusivity in LLM evaluation and future model development. 
<br /><br />Summary: <div>
arXiv:2510.27543v1 Announce Type: new 
Abstract: We present DialectalArabicMMLU, a new benchmark for evaluating the performance of large language models (LLMs) across Arabic dialects. While recently developed Arabic and multilingual benchmarks have advanced LLM evaluation for Modern Standard Arabic (MSA), dialectal varieties remain underrepresented despite their prevalence in everyday communication. DialectalArabicMMLU extends the MMLU-Redux framework through manual translation and adaptation of 3K multiple-choice question-answer pairs into five major dialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding a total of 15K QA pairs across 32 academic and professional domains (22K QA pairs when also including English and MSA). The benchmark enables systematic assessment of LLM reasoning and comprehension beyond MSA, supporting both task-based and linguistic analysis. We evaluate 19 open-weight Arabic and multilingual LLMs (1B-13B parameters) and report substantial performance variation across dialects, revealing persistent gaps in dialectal generalization. DialectalArabicMMLU provides the first unified, human-curated resource for measuring dialectal understanding in Arabic, thus promoting more inclusive evaluation and future model development.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multilingual BERT language model for medical tasks: Evaluation on domain-specific adaptation and cross-linguality</title>
<link>https://arxiv.org/abs/2510.27552</link>
<guid>https://arxiv.org/abs/2510.27552</guid>
<content:encoded><![CDATA[
<div> BERT, multilingual, domain adaptation, medical NLP, low-resource languages

Summary:
Domain-specific natural language processing tools are limited in multilingual healthcare applications, particularly for low-resource languages. This study explores the impact of further pre-training on domain-specific corpora on model performance for medical tasks in Dutch, Romanian, and Spanish. Four experiments were conducted, followed by fine-tuning on downstream tasks such as automated patient screening and named entity recognition. Results indicate that domain adaptation significantly improves task performance, with the clinical domain-adapted model outperforming the general biomedical domain-adapted model. Cross-lingual transferability was also observed. The study provides insights into the feasibility of domain adaptation and cross-lingual capabilities in medical NLP, offering guidance for developing multilingual medical NLP systems in low-resource language settings to enhance model performance. 

<br /><br />Summary: <div>
arXiv:2510.27552v1 Announce Type: new 
Abstract: In multilingual healthcare applications, the availability of domain-specific natural language processing(NLP) tools is limited, especially for low-resource languages. Although multilingual bidirectional encoder representations from transformers (BERT) offers a promising motivation to mitigate the language gap, the medical NLP tasks in low-resource languages are still underexplored. Therefore, this study investigates how further pre-training on domain-specific corpora affects model performance on medical tasks, focusing on three languages: Dutch, Romanian and Spanish. In terms of further pre-training, we conducted four experiments to create medical domain models. Then, these models were fine-tuned on three downstream tasks: Automated patient screening in Dutch clinical notes, named entity recognition in Romanian and Spanish clinical notes. Results show that domain adaptation significantly enhanced task performance. Furthermore, further differentiation of domains, e.g. clinical and general biomedical domains, resulted in diverse performances. The clinical domain-adapted model outperformed the more general biomedical domain-adapted model. Moreover, we observed evidence of cross-lingual transferability. Moreover, we also conducted further investigations to explore potential reasons contributing to these performance differences. These findings highlight the feasibility of domain adaptation and cross-lingual ability in medical NLP. Within the low-resource language settings, these findings can provide meaningful guidance for developing multilingual medical NLP systems to mitigate the lack of training data and thereby improve the model performance.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Efficient Domain Adaptation for LLM-based MT using Contrastive Preference Optimization</title>
<link>https://arxiv.org/abs/2510.27556</link>
<guid>https://arxiv.org/abs/2510.27556</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, domain adaptation, contrastive preference oracle, data efficiency, machine translation
Summary:
In this study, the authors explore the use of Contrasive Preference Oracle (CPO) to improve domain adaptation in Large Language Models (LLMs). By simulating a post-editing workflow, the CPO method generates preference pairs using the model's raw output as the 'rejected' translation and human-approved references as the 'chosen' ones. Through this approach, the model receives direct feedback to align with domain-specific requirements, leading to improved performance. The experiments conducted on English-Brazilian Portuguese and English-Korean language pairs demonstrate that with just 14.7k preference pairs, the model achieves results comparable to training on over 160k samples with Supervised Fine-Tuning (SFT). This highlights the data efficiency and effectiveness of CPO in facilitating domain adaptation for LLMs in machine translation tasks. Overall, the study showcases the potential of CPO not only in MT but also in other generative tasks where contrastive feedback can enhance model performance.<br /><br />Summary: <div>
arXiv:2510.27556v1 Announce Type: new 
Abstract: LLMs often require adaptation to domain-specific requirements, a process that can be expensive when relying solely on SFT. We present an empirical study on applying CPO to simulate a post-editing workflow for data-efficient domain adaptation. Our approach synthesizes preference pairs by treating the base model's own raw output as the 'rejected' translation and the human-approved TM entry as the 'chosen' one. This method provides direct feedback on the model's current knowledge, guiding it to align with domain-specific standards. Experiments in English-Brazilian Portuguese and English-Korean show that, by using just 14.7k preference pairs, the model achieves performance close to that of a model trained on 160k+ samples with SFT, demonstrating significant data efficiency. Although we showcase its effectiveness in MT, this application of CPO naturally generalizes to other generative tasks where a model's initial drafts can serve as a contrastive signal against a golden reference.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool Agentic Retrieval</title>
<link>https://arxiv.org/abs/2510.27569</link>
<guid>https://arxiv.org/abs/2510.27569</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrieval-Augmented Generation, MARAG-R1, reinforcement learning, corpus-level reasoning

Summary: 
MARAG-R1 is a novel framework that enhances Large Language Models (LLMs) by incorporating multiple retrieval mechanisms for improved access to external knowledge. This framework includes tools such as semantic search, keyword search, filtering, and aggregation, which are dynamically coordinated through reinforcement learning. By allowing LLMs to interleave reasoning and retrieval, MARAG-R1 enables comprehensive evidence gathering for corpus-level synthesis. Experimental results on various QA tasks demonstrate the superior performance of MARAG-R1 over existing baselines, establishing new state-of-the-art results in tasks requiring corpus-level reasoning. The model's two-stage training process, involving supervised fine-tuning and reinforcement learning, further enhances its ability to access relevant information and adapt to new data. <div>
arXiv:2510.27569v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at reasoning and generation but are inherently limited by static pretraining data, resulting in factual inaccuracies and weak adaptability to new information. Retrieval-Augmented Generation (RAG) addresses this issue by grounding LLMs in external knowledge; However, the effectiveness of RAG critically depends on whether the model can adequately access relevant information. Existing RAG systems rely on a single retriever with fixed top-k selection, restricting access to a narrow and static subset of the corpus. As a result, this single-retriever paradigm has become the primary bottleneck for comprehensive external information acquisition, especially in tasks requiring corpus-level reasoning. To overcome this limitation, we propose MARAG-R1, a reinforcement-learned multi-tool RAG framework that enables LLMs to dynamically coordinate multiple retrieval mechanisms for broader and more precise information access. MARAG-R1 equips the model with four retrieval tools -- semantic search, keyword search, filtering, and aggregation -- and learns both how and when to use them through a two-stage training process: supervised fine-tuning followed by reinforcement learning. This design allows the model to interleave reasoning and retrieval, progressively gathering sufficient evidence for corpus-level synthesis. Experiments on GlobalQA, HotpotQA, and 2WikiMultiHopQA demonstrate that MARAG-R1 substantially outperforms strong baselines and achieves new state-of-the-art results in corpus-level reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpecAttn: Speculating Sparse Attention</title>
<link>https://arxiv.org/abs/2510.27641</link>
<guid>https://arxiv.org/abs/2510.27641</guid>
<content:encoded><![CDATA[
<div> Sparse attention, Large Language Models, SpecAttn, computational efficiency, speculative decoding<br />
<br />
Summary:<br />
Large Language Models (LLMs) often face computational bottlenecks due to the quadratic complexity of self-attention mechanisms. In this study, the authors propose SpecAttn, a novel approach that integrates with speculative decoding techniques to enable efficient sparse attention in pre-trained transformers without the need for additional training. By leveraging attention weights computed during speculative decoding, SpecAttn identifies important tokens for the target model, reducing redundant computation. The approach includes KL divergence-based layer alignment between draft and target models, a GPU-optimized algorithm for selecting top-p tokens from draft attention patterns, and dynamic key-value cache pruning. SpecAttn achieves over 75% reduction in key-value cache accesses with only a 15.29% increase in perplexity on the PG-19 dataset, outperforming existing sparse attention methods and demonstrating the potential for enhancing speculative execution without significant performance degradation.<br /> <div>
arXiv:2510.27641v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face significant computational bottlenecks during inference due to the quadratic complexity of self-attention mechanisms, particularly as context lengths increase. We introduce SpecAttn, a novel training-free approach that seamlessly integrates with existing speculative decoding techniques to enable efficient sparse attention in pre-trained transformers. Our key insight is to exploit the attention weights already computed by the draft model during speculative decoding to identify important tokens for the target model, eliminating redundant computation while maintaining output quality. SpecAttn employs three core techniques: KL divergence-based layer alignment between draft and target models, a GPU-optimized sorting-free algorithm for top-p token selection from draft attention patterns, and dynamic key-value cache pruning guided by these predictions. By leveraging the computational work already performed in standard speculative decoding pipelines, SpecAttn achieves over 75% reduction in key-value cache accesses with a mere 15.29% increase in perplexity on the PG-19 dataset, significantly outperforming existing sparse attention methods. Our approach demonstrates that speculative execution can be enhanced to provide approximate verification without significant performance degradation.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Culture Cartography: Mapping the Landscape of Cultural Knowledge</title>
<link>https://arxiv.org/abs/2510.27672</link>
<guid>https://arxiv.org/abs/2510.27672</guid>
<content:encoded><![CDATA[
<div> CultureCartography, LLMs, mixed-initiative collaboration, knowledge extraction, benchmarks<br />
<br />
Summary:<br />
LLMs require culture-specific knowledge for global users, which may not be present in their pre-training data. Traditional methods involve researchers defining questions or users producing data for benchmarking, but a mixed-initiative approach could enhance this process. CultureCartography is a proposed methodology where LLMs initiate annotation with low-confidence questions, allowing human respondents to fill gaps and guide the model towards salient topics. A tool called CultureExplorer implements this method, which outperforms a baseline in producing missing knowledge for leading models like DeepSeek R1 and GPT-4o. Fine-tuning on this data boosts accuracy for Llama-3.1-8B by up to 19.2% on related culture benchmarks. <div>
arXiv:2510.27672v1 Announce Type: new 
Abstract: To serve global users safely and productively, LLMs need culture-specific knowledge that might not be learned during pre-training. How do we find such knowledge that is (1) salient to in-group users, but (2) unknown to LLMs? The most common solutions are single-initiative: either researchers define challenging questions that users passively answer (traditional annotation), or users actively produce data that researchers structure as benchmarks (knowledge extraction). The process would benefit from mixed-initiative collaboration, where users guide the process to meaningfully reflect their cultures, and LLMs steer the process towards more challenging questions that meet the researcher's goals. We propose a mixed-initiative methodology called CultureCartography. Here, an LLM initializes annotation with questions for which it has low-confidence answers, making explicit both its prior knowledge and the gaps therein. This allows a human respondent to fill these gaps and steer the model towards salient topics through direct edits. We implement this methodology as a tool called CultureExplorer. Compared to a baseline where humans answer LLM-proposed questions, we find that CultureExplorer more effectively produces knowledge that leading models like DeepSeek R1 and GPT-4o are missing, even with web search. Fine-tuning on this data boosts the accuracy of Llama-3.1-8B by up to 19.2% on related culture benchmarks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous Autoregressive Language Models</title>
<link>https://arxiv.org/abs/2510.27688</link>
<guid>https://arxiv.org/abs/2510.27688</guid>
<content:encoded><![CDATA[
<div> continuous autoregressive language models, language modeling, generative models, sequential generation, efficient modeling

Summary:
Continuous Autoregressive Language Models (CALM) propose a new approach to language modeling by shifting from discrete token prediction to continuous vector prediction. By compressing a chunk of tokens into a single continuous vector, CALM reduces the number of generative steps and improves efficiency. A likelihood-free framework is developed for robust training and evaluation in the continuous domain. Experiments show that CALM achieves comparable performance to strong discrete baselines but at a significantly lower computational cost. This paradigm shift opens up new possibilities for ultra-efficient language models by modeling language as a sequence of continuous vectors. The code and project associated with CALM are available on GitHub for further exploration and development. 

<br /><br />Summary: <div>
arXiv:2510.27688v1 Announce Type: new 
Abstract: The efficiency of large language models (LLMs) is fundamentally limited by their sequential, token-by-token generation process. We argue that overcoming this bottleneck requires a new design axis for LLM scaling: increasing the semantic bandwidth of each generative step. To this end, we introduce Continuous Autoregressive Language Models (CALM), a paradigm shift from discrete next-token prediction to continuous next-vector prediction. CALM uses a high-fidelity autoencoder to compress a chunk of K tokens into a single continuous vector, from which the original tokens can be reconstructed with over 99.9\% accuracy. This allows us to model language as a sequence of continuous vectors instead of discrete tokens, which reduces the number of generative steps by a factor of K. The paradigm shift necessitates a new modeling toolkit; therefore, we develop a comprehensive likelihood-free framework that enables robust training, evaluation, and controllable sampling in the continuous domain. Experiments show that CALM significantly improves the performance-compute trade-off, achieving the performance of strong discrete baselines at a significantly lower computational cost. More importantly, these findings establish next-vector prediction as a powerful and scalable pathway towards ultra-efficient language models. Code: https://github.com/shaochenze/calm. Project: https://shaochenze.github.io/blog/2025/CALM.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Broken-Token: Filtering Obfuscated Prompts by Counting Characters-Per-Token</title>
<link>https://arxiv.org/abs/2510.26847</link>
<guid>https://arxiv.org/abs/2510.26847</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Jailbreak Attacks, CPT-Filtering, Byte-Pair Encoding, Text Filtering

Summary: 
The article introduces a new technique called CPT-Filtering to prevent jailbreak attacks on Large Language Models (LLMs) by detecting encoded malicious prompts. This model-agnostic approach relies on the average number of Characters Per Token (CPT) in text to identify out-of-distribution content, such as ciphers. By leveraging the behavior of tokenizers trained on natural language, CPT-Filtering can accurately detect encoded text with high efficiency and negligible costs. The method is validated through experiments on a large dataset of prompts, showcasing its robustness even with short inputs. CPT-Filtering offers a practical defense layer that can be easily deployed for real-time text filtering and data curation, providing a reliable safeguard against malicious attacks on LLMs. 

<br /><br />Summary: <div>
arXiv:2510.26847v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are susceptible to jailbreak attacks where malicious prompts are disguised using ciphers and character-level encodings to bypass safety guardrails. While these guardrails often fail to interpret the encoded content, the underlying models can still process the harmful instructions. We introduce CPT-Filtering, a novel, model-agnostic with negligible-costs and near-perfect accuracy guardrail technique that aims to mitigate these attacks by leveraging the intrinsic behavior of Byte-Pair Encoding (BPE) tokenizers. Our method is based on the principle that tokenizers, trained on natural language, represent out-of-distribution text, such as ciphers, using a significantly higher number of shorter tokens. Our technique uses a simple yet powerful artifact of using language models: the average number of Characters Per Token (CPT) in the text. This approach is motivated by the high compute cost of modern methods - relying on added modules such as dedicated LLMs or perplexity models. We validate our approach across a large dataset of over 100,000 prompts, testing numerous encoding schemes with several popular tokenizers. Our experiments demonstrate that a simple CPT threshold robustly identifies encoded text with high accuracy, even for very short inputs. CPT-Filtering provides a practical defense layer that can be immediately deployed for real-time text filtering and offline data curation.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions</title>
<link>https://arxiv.org/abs/2510.26852</link>
<guid>https://arxiv.org/abs/2510.26852</guid>
<content:encoded><![CDATA[
<div> framework, peer-learning, evaluation, CATArena, learning ability
Summary:
The article introduces a competitive peer-learning framework aimed at evaluating the learning abilities of Large Language Model (LLM) agents. It emphasizes the importance of self-improvement and peer-learning in driving agent evolution towards human-level intelligence. The proposed iterative framework allows agents to refine their strategies through repeated interactions and feedback. To address score saturation in current benchmarks, the authors introduce CATArena, a tournament-style evaluation platform featuring diverse board and card games with open-ended scoring. This platform enables continuous and dynamic evaluation of agent capabilities. Experimental results show that CATArena provides reliable and scalable benchmarking for core agent abilities, particularly in learning ability and strategy coding. The framework aims to move beyond fixed scenarios and expert annotation, providing a more comprehensive assessment of LLM agent performance. <div>
arXiv:2510.26852v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents have evolved from basic text generation to autonomously completing complex tasks through interaction with external tools. However, current benchmarks mainly assess end-to-end performance in fixed scenarios, restricting evaluation to specific skills and suffering from score saturation and growing dependence on expert annotation as agent capabilities improve. In this work, we emphasize the importance of learning ability, including both self-improvement and peer-learning, as a core driver for agent evolution toward human-level intelligence. We propose an iterative, competitive peer-learning framework, which allows agents to refine and optimize their strategies through repeated interactions and feedback, thereby systematically evaluating their learning capabilities. To address the score saturation issue in current benchmarks, we introduce CATArena, a tournament-style evaluation platform featuring four diverse board and card games with open-ended scoring. By providing tasks without explicit upper score limits, CATArena enables continuous and dynamic evaluation of rapidly advancing agent capabilities. Experimental results and analyses involving both minimal and commercial code agents demonstrate that CATArena provides reliable, stable, and scalable benchmarking for core agent abilities, particularly learning ability and strategy coding.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Perspectival Biases in Cross-Modal Retrieval</title>
<link>https://arxiv.org/abs/2510.26861</link>
<guid>https://arxiv.org/abs/2510.26861</guid>
<content:encoded><![CDATA[
<div> bias, multimodal retrieval systems, language, cultural associations, semantic space
<br />
Prevalence bias and association bias are two common biases observed in multimodal retrieval systems. Prevalence bias occurs when prevalent languages are favored over semantically accurate entries, while association bias happens when culturally associated images are favored over semantically correct ones. The study found that explicit alignment helped mitigate prevalence bias, but association bias remained a challenging issue. The results highlight the need for targeted strategies beyond simple data scaling to achieve equity in multimodal systems. Cultural association bias was identified as a more difficult problem compared to linguistic prevalence bias. Addressing these biases is crucial for the development of truly equitable multimodal retrieval systems.
<br /><br />Summary: <div>
arXiv:2510.26861v1 Announce Type: cross 
Abstract: Multimodal retrieval systems are expected to operate in a semantic space, agnostic to the language or cultural origin of the query. In practice, however, retrieval outcomes systematically reflect perspectival biases: deviations shaped by linguistic prevalence and cultural associations. We study two such biases. First, prevalence bias refers to the tendency to favor entries from prevalent languages over semantically faithful entries in image-to-text retrieval. Second, association bias refers to the tendency to favor images culturally associated with the query over semantically correct ones in text-to-image retrieval. Results show that explicit alignment is a more effective strategy for mitigating prevalence bias. However, association bias remains a distinct and more challenging problem. These findings suggest that achieving truly equitable multimodal systems requires targeted strategies beyond simple data scaling and that bias arising from cultural association may be treated as a more challenging problem than one arising from linguistic prevalence.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Denario project: Deep knowledge AI agents for scientific discovery</title>
<link>https://arxiv.org/abs/2510.26887</link>
<guid>https://arxiv.org/abs/2510.26887</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, multi-agent system, scientific research assistant, modular architecture, AI-generated papers

Summary:
Denario is an AI multi-agent system that functions as a scientific research assistant, capable of performing various tasks such as idea generation, literature review, code development, data analysis, and paper drafting. The system utilizes a modular architecture to handle specific tasks efficiently. Denario has been used to generate AI-written papers across multiple scientific disciplines including astrophysics, biology, chemistry, and medicine. It can also integrate ideas from different fields, demonstrated in a paper combining quantum physics and machine learning in astrophysical analysis. Domain experts have evaluated the AI-generated papers, providing numerical scores and feedback. The strengths, weaknesses, and limitations of Denario are discussed, along with ethical considerations in AI-driven research. The code for Denario is publicly available, and a demo can be accessed online. Deploying such technology raises philosophical questions about the role of AI in scientific inquiry.<br /><br />Summary: <div>
arXiv:2510.26887v1 Announce Type: cross 
Abstract: We present Denario, an AI multi-agent system designed to serve as a scientific research assistant. Denario can perform many different tasks, such as generating ideas, checking the literature, developing research plans, writing and executing code, making plots, and drafting and reviewing a scientific paper. The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis using Cmbagent as a deep-research backend. In this work, we describe in detail Denario and its modules, and illustrate its capabilities by presenting multiple AI-generated papers generated by it in many different scientific disciplines such as astrophysics, biology, biophysics, biomedical informatics, chemistry, material science, mathematical physics, medicine, neuroscience and planetary science. Denario also excels at combining ideas from different disciplines, and we illustrate this by showing a paper that applies methods from quantum physics and machine learning to astrophysical data. We report the evaluations performed on these papers by domain experts, who provided both numerical scores and review-like feedback. We then highlight the strengths, weaknesses, and limitations of the current system. Finally, we discuss the ethical implications of AI-driven research and reflect on how such technology relates to the philosophy of science. We publicly release the code at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and the full app will be deployed on the cloud.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RepV: Safety-Separable Latent Spaces for Scalable Neurosymbolic Plan Verification</title>
<link>https://arxiv.org/abs/2510.26935</link>
<guid>https://arxiv.org/abs/2510.26935</guid>
<content:encoded><![CDATA[
<div> Keywords: AI systems, formal methods, deep learning, neurosymbolic verifier, plan verification

Summary: 
AI systems moving into safety-critical domains face challenges in verifying compliance with rules. Formal methods offer guarantees but require manual temporal-logic specifications, limiting expressiveness. Deep learning allows evaluation against natural-language constraints but lacks transparency, leading to potential errors. The RepV verifier combines formal methods and deep learning by learning a latent space where safe and unsafe plans are distinguishable. It trains a lightweight projector from labeled plans to embed them with a language model-generated rationale for verification. RepV provides probabilistic guarantees for correct verification likelihood and allows planner refinement based on these guarantees. Empirical results show improved accuracy in compliance prediction and outperformance in refinement compared to baseline methods. The approach presents a scalable and reliable method for neurosymbolic plan verification. <div>
arXiv:2510.26935v1 Announce Type: cross 
Abstract: As AI systems migrate to safety-critical domains, verifying that their actions comply with well-defined rules remains a challenge. Formal methods provide provable guarantees but demand hand-crafted temporal-logic specifications, offering limited expressiveness and accessibility. Deep learning approaches enable evaluation of plans against natural-language constraints, yet their opaque decision process invites misclassifications with potentially severe consequences. We introduce RepV, a neurosymbolic verifier that unifies both views by learning a latent space where safe and unsafe plans are linearly separable. Starting from a modest seed set of plans labeled by an off-the-shelf model checker, RepV trains a lightweight projector that embeds each plan, together with a language model-generated rationale, into a low-dimensional space; a frozen linear boundary then verifies compliance for unseen natural-language rules in a single forward pass.
  Beyond binary classification, RepV provides a probabilistic guarantee on the likelihood of correct verification based on its position in the latent space. This guarantee enables a guarantee-driven refinement of the planner, improving rule compliance without human annotations. Empirical evaluations show that RepV improves compliance prediction accuracy by up to 15% compared to baseline methods while adding fewer than 0.2M parameters. Furthermore, our refinement framework outperforms ordinary fine-tuning baselines across various planning domains. These results show that safety-separable latent spaces offer a scalable, plug-and-play primitive for reliable neurosymbolic plan verification. Code and data are available at: https://repv-project.github.io/.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Frame Aggregation-based Transformer for Live Video Comment Generation</title>
<link>https://arxiv.org/abs/2510.26978</link>
<guid>https://arxiv.org/abs/2510.26978</guid>
<content:encoded><![CDATA[
<div> Keywords: live video comment generation, Semantic Frame Aggregation-based Transformer (SFAT), contextual cues, multimodal English video comments dataset, Twitch

Summary:
The article discusses the challenges of automatically generating contextually appropriate comments in live video streams and introduces the Semantic Frame Aggregation-based Transformer (SFAT) model. This model prioritizes relevant video frames to enhance comment generation, leveraging CLIP's multimodal knowledge and assigning weights to frames based on their semantic relevance. The model employs an efficient weighted sum of frames technique to emphasize informative frames and focuses less on irrelevant ones. Additionally, a comment decoder with a cross-attention mechanism ensures that generated comments reflect contextual cues from both chats and videos. To address dataset limitations, a diverse multimodal English video comments dataset extracted from Twitch is introduced, covering 11 video categories and 3.2 million comments. The effectiveness of the SFAT model is demonstrated through comparisons with existing methods in generating comments from live video and ongoing dialogue contexts.

<br /><br />Summary: <div>
arXiv:2510.26978v1 Announce Type: cross 
Abstract: Live commenting on video streams has surged in popularity on platforms like Twitch, enhancing viewer engagement through dynamic interactions. However, automatically generating contextually appropriate comments remains a challenging and exciting task. Video streams can contain a vast amount of data and extraneous content. Existing approaches tend to overlook an important aspect of prioritizing video frames that are most relevant to ongoing viewer interactions. This prioritization is crucial for producing contextually appropriate comments. To address this gap, we introduce a novel Semantic Frame Aggregation-based Transformer (SFAT) model for live video comment generation. This method not only leverages CLIP's visual-text multimodal knowledge to generate comments but also assigns weights to video frames based on their semantic relevance to ongoing viewer conversation. It employs an efficient weighted sum of frames technique to emphasize informative frames while focusing less on irrelevant ones. Finally, our comment decoder with a cross-attention mechanism that attends to each modality ensures that the generated comment reflects contextual cues from both chats and video. Furthermore, to address the limitations of existing datasets, which predominantly focus on Chinese-language content with limited video categories, we have constructed a large scale, diverse, multimodal English video comments dataset. Extracted from Twitch, this dataset covers 11 video categories, totaling 438 hours and 3.2 million comments. We demonstrate the effectiveness of our SFAT model by comparing it to existing methods for generating comments from live video and ongoing dialogue contexts.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Measure of Algorithm Similarity</title>
<link>https://arxiv.org/abs/2510.27063</link>
<guid>https://arxiv.org/abs/2510.27063</guid>
<content:encoded><![CDATA[
arXiv:2510.27063v1 Announce Type: cross 
Abstract: Given two algorithms for the same problem, can we determine whether they are meaningfully different? In full generality, the question is uncomputable, and empirically it is muddied by competing notions of similarity. Yet, in many applications (such as clone detection or program synthesis) a pragmatic and consistent similarity metric is necessary. We review existing equivalence and similarity notions and introduce EMOC: An Evaluation-Memory-Operations-Complexity framework that embeds algorithm implementations into a feature space suitable for downstream tasks. We compile PACD, a curated dataset of verified Python implementations across three problems, and show that EMOC features support clustering and classification of algorithm types, detection of near-duplicates, and quantification of diversity in LLM-generated programs. Code, data, and utilities for computing EMOC embeddings are released to facilitate reproducibility and future work on algorithm similarity.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Glia: A Human-Inspired AI for Automated Systems Design and Optimization</title>
<link>https://arxiv.org/abs/2510.27176</link>
<guid>https://arxiv.org/abs/2510.27176</guid>
<content:encoded><![CDATA[
arXiv:2510.27176v1 Announce Type: cross 
Abstract: Can an AI autonomously design mechanisms for computer systems on par with the creativity and reasoning of human experts? We present Glia, an AI architecture for networked systems design that uses large language models (LLMs) in a human-inspired, multi-agent workflow. Each agent specializes in reasoning, experimentation, and analysis, collaborating through an evaluation framework that grounds abstract reasoning in empirical feedback. Unlike prior ML-for-systems methods that optimize black-box policies, Glia generates interpretable designs and exposes its reasoning process. When applied to a distributed GPU cluster for LLM inference, it produces new algorithms for request routing, scheduling, and auto-scaling that perform at human-expert levels in significantly less time, while yielding novel insights into workload behavior. Our results suggest that by combining reasoning LLMs with structured experimentation, an AI can produce creative and understandable designs for complex systems problems.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions</title>
<link>https://arxiv.org/abs/2510.27195</link>
<guid>https://arxiv.org/abs/2510.27195</guid>
<content:encoded><![CDATA[
arXiv:2510.27195v1 Announce Type: cross 
Abstract: As AI systems become increasingly integrated into human lives, endowing them with robust social intelligence has emerged as a critical frontier. A key aspect of this intelligence is discerning truth from deception, a ubiquitous element of human interaction that is conveyed through a complex interplay of verbal language and non-verbal visual cues. However, automatic deception detection in dynamic, multi-party conversations remains a significant challenge. The recent rise of powerful Multimodal Large Language Models (MLLMs), with their impressive abilities in visual and textual understanding, makes them natural candidates for this task. Consequently, their capabilities in this crucial domain are mostly unquantified. To address this gap, we introduce a new task, Multimodal Interactive Veracity Assessment (MIVA), and present a novel multimodal dataset derived from the social deduction game Werewolf. This dataset provides synchronized video, text, with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating state-of-the-art MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to ground language in visual social cues effectively and may be overly conservative in their alignment, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRAMA: Unifying Data Retrieval and Analysis for Open-Domain Analytic Queries</title>
<link>https://arxiv.org/abs/2510.27238</link>
<guid>https://arxiv.org/abs/2510.27238</guid>
<content:encoded><![CDATA[
arXiv:2510.27238v1 Announce Type: cross 
Abstract: Manually conducting real-world data analyses is labor-intensive and inefficient. Despite numerous attempts to automate data science workflows, none of the existing paradigms or systems fully demonstrate all three key capabilities required to support them effectively: (1) open-domain data collection, (2) structured data transformation, and (3) analytic reasoning.
  To overcome these limitations, we propose DRAMA, an end-to-end paradigm that answers users' analytic queries in natural language on large-scale open-domain data. DRAMA unifies data collection, transformation, and analysis as a single pipeline. To quantitatively evaluate system performance on tasks representative of DRAMA, we construct a benchmark, DRAMA-Bench, consisting of two categories of tasks: claim verification and question answering, each comprising 100 instances. These tasks are derived from real-world applications that have gained significant public attention and require the retrieval and analysis of open-domain data. We develop DRAMA-Bot, a multi-agent system designed following DRAMA. It comprises a data retriever that collects and transforms data by coordinating the execution of sub-agents, and a data analyzer that performs structured reasoning over the retrieved data. We evaluate DRAMA-Bot on DRAMA-Bench together with five state-of-the-art baseline agents. DRAMA-Bot achieves 86.5% task accuracy at a cost of $0.05, outperforming all baselines with up to 6.9 times the accuracy and less than 1/6 of the cost. DRAMA is publicly available at https://github.com/uiuc-kang-lab/drama.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Higher-order Linear Attention</title>
<link>https://arxiv.org/abs/2510.27258</link>
<guid>https://arxiv.org/abs/2510.27258</guid>
<content:encoded><![CDATA[
arXiv:2510.27258v1 Announce Type: cross 
Abstract: The quadratic cost of scaled dot-product attention is a central obstacle to scaling autoregressive language models to long contexts. Linear-time attention and State Space Models (SSMs) provide scalable alternatives but are typically restricted to first-order or kernel-based approximations, which can limit expressivity. We introduce Higher-order Linear Attention (HLA), a causal, streaming mechanism that realizes higher interactions via compact prefix sufficient statistics. In the second-order case, HLA maintains a constant-size state and computes per-token outputs in linear time without materializing any $n \times n$ matrices. We give closed-form streaming identities, a strictly causal masked variant using two additional summaries, and a chunk-parallel training scheme based on associative scans that reproduces the activations of a serial recurrence exactly. We further outline extensions to third and higher orders. Collectively, these results position HLA as a principled, scalable building block that combines attention-like, data-dependent mixing with the efficiency of modern recurrent architectures. Project Page: https://github.com/yifanzhang-pro/HLA.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Un-Attributability: Computing Novelty From Retrieval &amp; Semantic Similarity</title>
<link>https://arxiv.org/abs/2510.27313</link>
<guid>https://arxiv.org/abs/2510.27313</guid>
<content:encoded><![CDATA[
arXiv:2510.27313v1 Announce Type: cross 
Abstract: Understanding how language-model outputs relate to the pretraining corpus is central to studying model behavior. Most training data attribution (TDA) methods ask which training examples causally influence a given output, often using leave-one-out tests. We invert the question: which outputs cannot be attributed to any pretraining example? We introduce un-attributability as an operational measure of semantic novelty: an output is novel if the pretraining corpus contains no semantically similar context. We approximate this with a simple two-stage retrieval pipeline: index the corpus with lightweight GIST embeddings, retrieve the top-n candidates, then rerank with ColBERTv2. If the nearest corpus item is less attributable than a human-generated text reference, we consider the output of the model as novel. We evaluate on SmolLM and SmolLM2 and report three findings: (1) models draw on pretraining data across much longer spans than previously reported; (2) some domains systematically promote or suppress novelty; and (3) instruction tuning not only alters style but also increases novelty. Reframing novelty assessment around un-attributability enables efficient analysis at pretraining scale. We release ~20 TB of corpus chunks and index artifacts to support replication and large-scale extension of our analysis at https://huggingface.co/datasets/stai-tuebingen/faiss-smollm
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity</title>
<link>https://arxiv.org/abs/2510.27378</link>
<guid>https://arxiv.org/abs/2510.27378</guid>
<content:encoded><![CDATA[
arXiv:2510.27378v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning. Since any long, serial reasoning process must pass through this textual trace, the quality of the CoT is a direct window into what the model is thinking. This visibility could help us spot unsafe or misaligned behavior (monitorability), but only if the CoT is transparent about its internal reasoning (faithfulness). Fully measuring faithfulness is difficult, so researchers often focus on examining the CoT in cases where the model changes its answer after adding a cue to the input. This proxy finds some instances of unfaithfulness but loses information when the model maintains its answer, and does not investigate aspects of reasoning not tied to the cue. We extend these results to a more holistic sense of monitorability by introducing verbosity: whether the CoT lists every factor needed to solve the task. We combine faithfulness and verbosity into a single monitorability score that shows how well the CoT serves as the model's external `working memory', a property that many safety schemes based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning models on BBH, GPQA, and MMLU. Our results show that models can appear faithful yet remain hard to monitor when they leave out key factors, and that monitorability differs sharply across model families. We release our evaluation code using the Inspect library to support reproducible future work.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atlas-Alignment: Making Interpretability Transferable Across Language Models</title>
<link>https://arxiv.org/abs/2510.27413</link>
<guid>https://arxiv.org/abs/2510.27413</guid>
<content:encoded><![CDATA[
arXiv:2510.27413v1 Announce Type: cross 
Abstract: Interpretability is crucial for building safe, reliable, and controllable language models, yet existing interpretability pipelines remain costly and difficult to scale. Interpreting a new model typically requires costly training of model-specific sparse autoencoders, manual or semi-automated labeling of SAE components, and their subsequent validation. We introduce Atlas-Alignment, a framework for transferring interpretability across language models by aligning unknown latent spaces to a Concept Atlas - a labeled, human-interpretable latent space - using only shared inputs and lightweight representational alignment techniques. Once aligned, this enables two key capabilities in previously opaque models: (1) semantic feature search and retrieval, and (2) steering generation along human-interpretable atlas concepts. Through quantitative and qualitative evaluations, we show that simple representational alignment methods enable robust semantic retrieval and steerable generation without the need for labeled concept data. Atlas-Alignment thus amortizes the cost of explainable AI and mechanistic interpretability: by investing in one high-quality Concept Atlas, we can make many new models transparent and controllable at minimal marginal cost.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains</title>
<link>https://arxiv.org/abs/2510.27419</link>
<guid>https://arxiv.org/abs/2510.27419</guid>
<content:encoded><![CDATA[
arXiv:2510.27419v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) have demonstrated impressive capabilities but suffer from cognitive inefficiencies like ``overthinking'' simple problems and ``underthinking'' complex ones. While existing methods that use supervised fine-tuning~(SFT) or reinforcement learning~(RL) with token-length rewards can improve efficiency, they often do so at the cost of accuracy. This paper introduces \textbf{DeepCompress}, a novel framework that simultaneously enhances both the accuracy and efficiency of LRMs. We challenge the prevailing approach of consistently favoring shorter reasoning paths, showing that longer responses can contain a broader range of correct solutions for difficult problems. DeepCompress employs an adaptive length reward mechanism that dynamically classifies problems as ``Simple'' or ``Hard'' in real-time based on the model's evolving capability. It encourages shorter, more efficient reasoning for ``Simple'' problems while promoting longer, more exploratory thought chains for ``Hard'' problems. This dual-reward strategy enables the model to autonomously adjust its Chain-of-Thought (CoT) length, compressing reasoning for well-mastered problems and extending it for those it finds challenging. Experimental results on challenging mathematical benchmarks show that DeepCompress consistently outperforms baseline methods, achieving superior accuracy while significantly improving token efficiency.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thought Branches: Interpreting LLM Reasoning Requires Resampling</title>
<link>https://arxiv.org/abs/2510.27484</link>
<guid>https://arxiv.org/abs/2510.27484</guid>
<content:encoded><![CDATA[
arXiv:2510.27484v1 Announce Type: cross 
Abstract: Most work interpreting reasoning models studies only a single chain-of-thought (CoT), yet these models define distributions over many possible CoTs. We argue that studying a single sample is inadequate for understanding causal influence and the underlying computation. Though fully specifying this distribution is intractable, it can be understood by sampling. We present case studies using resampling to investigate model decisions. First, when a model states a reason for its action, does that reason actually cause the action? In "agentic misalignment" scenarios, we resample specific sentences to measure their downstream effects. Self-preservation sentences have small causal impact, suggesting they do not meaningfully drive blackmail. Second, are artificial edits to CoT sufficient for steering reasoning? These are common in literature, yet take the model off-policy. Resampling and selecting a completion with the desired property is a principled on-policy alternative. We find off-policy interventions yield small and unstable effects compared to resampling in decision-making tasks. Third, how do we understand the effect of removing a reasoning step when the model may repeat it post-edit? We introduce a resilience metric that repeatedly resamples to prevent similar content from reappearing downstream. Critical planning statements resist removal but have large effects when eliminated. Fourth, since CoT is sometimes "unfaithful", can our methods teach us anything in these settings? Adapting causal mediation analysis, we find that hints that have a causal effect on the output without being explicitly mentioned exert a subtle and cumulative influence on the CoT that persists even if the hint is removed. Overall, studying distributions via resampling enables reliable causal analysis, clearer narratives of model reasoning, and principled CoT interventions.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIGMA: Search-Augmented On-Demand Knowledge Integration for Agentic Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2510.27568</link>
<guid>https://arxiv.org/abs/2510.27568</guid>
<content:encoded><![CDATA[
arXiv:2510.27568v1 Announce Type: cross 
Abstract: Solving mathematical reasoning problems requires not only accurate access to relevant knowledge but also careful, multi-step thinking. However, current retrieval-augmented models often rely on a single perspective, follow inflexible search strategies, and struggle to effectively combine information from multiple sources. We introduce SIGMA (Search-Augmented On-Demand Knowledge Integration for AGentic Mathematical reAsoning), a unified framework that orchestrates specialized agents to independently reason, perform targeted searches, and synthesize findings through a moderator mechanism. Each agent generates hypothetical passages to optimize retrieval for its analytic perspective, ensuring knowledge integration is both context-sensitive and computation-efficient. When evaluated on challenging benchmarks such as MATH500, AIME, and PhD-level science QA GPQA, SIGMA consistently outperforms both open- and closed-source systems, achieving an absolute performance improvement of 7.4%. Our results demonstrate that multi-agent, on-demand knowledge integration significantly enhances both reasoning accuracy and efficiency, offering a scalable approach for complex, knowledge-intensive problem-solving. We will release the code upon publication.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum</title>
<link>https://arxiv.org/abs/2510.27571</link>
<guid>https://arxiv.org/abs/2510.27571</guid>
<content:encoded><![CDATA[
arXiv:2510.27571v1 Announce Type: cross 
Abstract: The prevailing video retrieval paradigm is structurally misaligned, as narrow benchmarks incentivize correspondingly limited data and single-task training. Therefore, universal capability is suppressed due to the absence of a diagnostic evaluation that defines and demands multi-dimensional generalization. To break this cycle, we introduce a framework built on the co-design of evaluation, data, and modeling. First, we establish the Universal Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to measure performance but also to diagnose critical capability gaps across tasks and domains. Second, guided by UVRB's diagnostics, we introduce a scalable synthesis workflow that generates 1.55 million high-quality pairs to populate the semantic space required for universality. Finally, we devise the Modality Pyramid, a curriculum that trains our General Video Embedder (GVE) by explicitly leveraging the latent interconnections within our diverse data. Extensive experiments show GVE achieves state-of-the-art zero-shot generalization on UVRB. In particular, our analysis reveals that popular benchmarks are poor predictors of general ability and that partially relevant retrieval is a dominant but overlooked scenario. Overall, our co-designed framework provides a practical path to escape the limited scope and advance toward truly universal video retrieval.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning</title>
<link>https://arxiv.org/abs/2510.27623</link>
<guid>https://arxiv.org/abs/2510.27623</guid>
<content:encoded><![CDATA[
arXiv:2510.27623v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have advanced embodied agents by enabling direct perception, reasoning, and planning task-oriented actions from visual inputs. However, such vision driven embodied agents open a new attack surface: visual backdoor attacks, where the agent behaves normally until a visual trigger appears in the scene, then persistently executes an attacker-specified multi-step policy. We introduce BEAT, the first framework to inject such visual backdoors into MLLM-based embodied agents using objects in the environments as triggers. Unlike textual triggers, object triggers exhibit wide variation across viewpoints and lighting, making them difficult to implant reliably. BEAT addresses this challenge by (1) constructing a training set that spans diverse scenes, tasks, and trigger placements to expose agents to trigger variability, and (2) introducing a two-stage training scheme that first applies supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning (CTL). CTL formulates trigger discrimination as preference learning between trigger-present and trigger-free inputs, explicitly sharpening the decision boundaries to ensure precise backdoor activation. Across various embodied agent benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while maintaining strong benign task performance, and generalizes reliably to out-of-distribution trigger placements. Notably, compared to naive SFT, CTL boosts backdoor activation accuracy up to 39% under limited backdoor data. These findings expose a critical yet unexplored security risk in MLLM-based embodied agents, underscoring the need for robust defenses before real-world deployment.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindSearch: Mimicking Human Minds Elicits Deep AI Searcher</title>
<link>https://arxiv.org/abs/2407.20183</link>
<guid>https://arxiv.org/abs/2407.20183</guid>
<content:encoded><![CDATA[
arXiv:2407.20183v2 Announce Type: replace 
Abstract: Information seeking and integration is a complex cognitive task that consumes enormous time and effort. Inspired by the remarkable progress of Large Language Models, recent works attempt to solve this task by combining LLMs and search engines. However, these methods still obtain unsatisfying performance due to three challenges: (1) complex requests often cannot be accurately and completely retrieved by the search engine once (2) corresponding information to be integrated is spread over multiple web pages along with massive noise, and (3) a large number of web pages with long contents may quickly exceed the maximum context length of LLMs. Inspired by the cognitive process when humans solve these problems, we introduce MindSearch to mimic the human minds in web information seeking and integration, which can be instantiated by a simple yet effective LLM-based multi-agent framework. The WebPlanner models the human mind of multi-step information seeking as a dynamic graph construction process: it decomposes the user query into atomic sub-questions as nodes in the graph and progressively extends the graph based on the search result from WebSearcher. Tasked with each sub-question, WebSearcher performs hierarchical information retrieval with search engines and collects valuable information for WebPlanner. The multi-agent design of MindSearch enables the whole framework to seek and integrate information parallelly from larger-scale (e.g., more than 300) web pages in 3 minutes, which is worth 3 hours of human effort. MindSearch demonstrates significant improvement in the response quality in terms of depth and breadth, on both close-set and open-set QA problems. Besides, responses from MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web and Perplexity.ai applications, which implies that MindSearch can already deliver a competitive solution to the proprietary AI search engine.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparsePO: Controlling Preference Alignment of LLMs via Sparse Token Masks</title>
<link>https://arxiv.org/abs/2410.05102</link>
<guid>https://arxiv.org/abs/2410.05102</guid>
<content:encoded><![CDATA[
arXiv:2410.05102v3 Announce Type: replace 
Abstract: Direct alignment algorithms have proven an effective step for aligning language models to human-desired behaviors. Current variants of the Direct Preference Optimization objective have focused on a strict setting where all tokens are contributing signals of KL divergence and rewards to the loss function. However, human preference is not affected equally by each word in a sequence but is often dependent on specific words or phrases, e.g. existence of toxic terms leads to non-preferred responses. Based on this observation, we argue that not all tokens should be weighted equally during PO and propose a flexible objective termed SparsePO, that aims to automatically learn to weight the KL divergence and reward corresponding to each token during PO training. We propose two different variants of weight-masks that can either be derived from the reference model itself or learned on the fly. Notably, our method induces sparsity in the learned masks, allowing the model to learn how to best balance reward and KL divergence contributions at the token level, learning an optimal level of mask sparsity. Extensive experiments illustrate the effectiveness of our approach at aligning to preference proxies, including sentiment control, helpfulness and harmlessness, and summary quality. Our method obtains +10% and +3% win rate points in summarization and dialogue scenarios, respectively, without compromising model reasoning or the relevancy and faithfulness of the summary response.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models</title>
<link>https://arxiv.org/abs/2411.00918</link>
<guid>https://arxiv.org/abs/2411.00918</guid>
<content:encoded><![CDATA[
arXiv:2411.00918v2 Announce Type: replace 
Abstract: Mixture of experts (MoE) architectures have become a cornerstone for scaling up and are a key component in most large language models such as GPT-OSS, DeepSeek-V3, Llama-4, and Gemini-2.5. However, systematic research on MoE remains severely constrained by the prohibitive computational costs of training and evaluation, restricting large-scale studies accessible to most researchers. We introduce LibMoE, a unified framework for reproducible, efficient, and extensible MoE research that supports both pretraining and sparse-upcycling regimes. Beyond unified implementations, the framework provides transparent analytical tools for probing routing and expert dynamics. Leveraging this foundation, we conduct a comprehensive analysis along three dimensions: (i) routing dynamics, covering expert selection patterns, routing stability and optimality, and how routing entropy reveals task specialization and expert diversity; (ii) the effect of lightweight initialization on load balancing, demonstrating how subtle changes in router initialization shape early expert utilization; and (iii) training regime differences, revealing how sparse upcycling and full pretraining exhibit distinct routing patterns and stability profiles. By lowering the barrier to entry and standardizing evaluation, along with our comprehensive analysis, LibMoE broadens access to MoE research and establishes a reliable benchmark to guide future innovations. Project page: https://fsoft-aic.github.io/fsoft-LibMoE.github.io.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multilingual State Space Models for Structured Question Answering in Indic Languages</title>
<link>https://arxiv.org/abs/2502.01673</link>
<guid>https://arxiv.org/abs/2502.01673</guid>
<content:encoded><![CDATA[
arXiv:2502.01673v3 Announce Type: replace 
Abstract: The diversity and complexity of Indic languages present unique challenges for natural language processing (NLP) tasks, particularly in the domain of question answering (QA).To address these challenges, this paper explores the application of State Space Models (SSMs),to build efficient and contextually aware QA systems tailored for Indic languages. SSMs are particularly suited for this task due to their ability to model long-term and short-term dependencies in sequential data, making them well-equipped to handle the rich morphology, complex syntax, and contextual intricacies characteristic of Indian languages. We evaluated multiple SSM architectures across diverse datasets representing various Indic languages and conducted a comparative analysis of their performance. Our results demonstrate that these models effectively capture linguistic subtleties, leading to significant improvements in question interpretation, context alignment, and answer generation. This work represents the first application of SSMs to question answering tasks in Indic languages, establishing a foundational benchmark for future research in this domain. We propose enhancements to existing SSM frameworks, optimizing their applicability to low-resource settings and multilingual scenarios prevalent in Indic languages.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMOL: Professionally translated parallel data for 115 under-represented languages</title>
<link>https://arxiv.org/abs/2502.12301</link>
<guid>https://arxiv.org/abs/2502.12301</guid>
<content:encoded><![CDATA[
arXiv:2502.12301v2 Announce Type: replace 
Abstract: We open-source SMOL (Set of Maximal Overall Leverage), a suite of training data to unlock machine translation for low-resource languages. SMOL has been translated into 124 (and growing) under-resourced languages (125 language pairs), including many for which there exist no previous public resources, for a total of 6.1M translated tokens. SMOL comprises two sub-datasets, each carefully chosen for maximum impact given its size: SMOLSENT, a set of sentences chosen for broad unique token coverage, and SMOLDOC, a document-level resource focusing on a broad topic coverage. They join the already released GATITOS for a trifecta of paragraph, sentence, and token-level content. We demonstrate that using SMOL to prompt or fine-tune Large Language Models yields robust chrF improvements. In addition to translation, we provide factuality ratings and rationales for all documents in SMOLDOC, yielding the first factuality datasets for most of these languages.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Best-of-N Selection for Large Language Models via Self-Certainty</title>
<link>https://arxiv.org/abs/2502.18581</link>
<guid>https://arxiv.org/abs/2502.18581</guid>
<content:encoded><![CDATA[
arXiv:2502.18581v2 Announce Type: replace 
Abstract: Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size N, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at https://github.com/backprop07/Self-Certainty
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>(How) Do Language Models Track State?</title>
<link>https://arxiv.org/abs/2503.02854</link>
<guid>https://arxiv.org/abs/2503.02854</guid>
<content:encoded><![CDATA[
arXiv:2503.02854v3 Announce Type: replace 
Abstract: Transformer language models (LMs) exhibit behaviors -- from storytelling to code generation -- that seem to require tracking the unobserved state of an evolving world. How do they do this? We study state tracking in LMs trained or fine-tuned to compose permutations (i.e., to compute the order of a set of objects after a sequence of swaps). Despite the simple algebraic structure of this problem, many other tasks (e.g., simulation of finite automata and evaluation of boolean expressions) can be reduced to permutation composition, making it a natural model for state tracking in general. We show that LMs consistently learn one of two state tracking mechanisms for this task. The first closely resembles the "associative scan" construction used in recent theoretical work by Liu et al. (2023) and Merrill et al. (2024). The second uses an easy-to-compute feature (permutation parity) to partially prune the space of outputs, and then refines this with an associative scan. LMs that learn the former algorithm tend to generalize better and converge faster, and we show how to steer LMs toward one or the other with intermediate training tasks that encourage or suppress the heuristics. Our results demonstrate that transformer LMs, whether pre-trained or fine-tuned, can learn to implement efficient and interpretable state-tracking mechanisms, and the emergence of these mechanisms can be predicted and controlled.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation Classification Using Large Language Models</title>
<link>https://arxiv.org/abs/2503.12989</link>
<guid>https://arxiv.org/abs/2503.12989</guid>
<content:encoded><![CDATA[
arXiv:2503.12989v3 Announce Type: replace 
Abstract: Automatically annotating job data with standardized occupations from taxonomies, known as occupation classification, is crucial for labor market analysis. However, this task is often hindered by data scarcity and the challenges of manual annotations. While large language models (LLMs) hold promise due to their extensive world knowledge and in-context learning capabilities, their effectiveness depends on their knowledge of occupational taxonomies, which remains unclear. In this study, we assess the ability of LLMs to generate precise taxonomic entities from taxonomy, highlighting their limitations, especially for smaller models. To address these challenges, we propose a multi-stage framework consisting of inference, retrieval, and reranking stages, which integrates taxonomy-guided reasoning examples to enhance performance by aligning outputs with taxonomic knowledge. Evaluations on a large-scale dataset show that our framework not only enhances occupation and skill classification tasks, but also provides a cost-effective alternative to frontier models like GPT-4o, significantly reducing computational costs while maintaining strong performance. This makes it a practical and scalable solution for occupation classification and related tasks across LLMs.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FUSE : A Ridge and Random Forest-Based Metric for Evaluating MT in Indigenous Languages</title>
<link>https://arxiv.org/abs/2504.00021</link>
<guid>https://arxiv.org/abs/2504.00021</guid>
<content:encoded><![CDATA[
arXiv:2504.00021v3 Announce Type: replace 
Abstract: This paper presents the winning submission of the RaaVa team to the AmericasNLP 2025 Shared Task 3 on Automatic Evaluation Metrics for Machine Translation (MT) into Indigenous Languages of America, where our system ranked first overall based on average Pearson correlation with the human annotations. We introduce Feature-Union Scorer (FUSE) for Evaluation, FUSE integrates Ridge regression and Gradient Boosting to model translation quality. In addition to FUSE, we explore five alternative approaches leveraging different combinations of linguistic similarity features and learning paradigms. FUSE Score highlights the effectiveness of combining lexical, phonetic, semantic, and fuzzy token similarity with learning-based modeling to improve MT evaluation for morphologically rich and low-resource languages. MT into Indigenous languages poses unique challenges due to polysynthesis, complex morphology, and non-standardized orthography. Conventional automatic metrics such as BLEU, TER, and ChrF often fail to capture deeper aspects like semantic adequacy and fluency. Our proposed framework, formerly referred to as FUSE, incorporates multilingual sentence embeddings and phonological encodings to better align with human evaluation. We train supervised models on human-annotated development sets and evaluate held-out test data. Results show that FUSE consistently achieves higher Pearson and Spearman correlations with human judgments, offering a robust and linguistically informed solution for MT evaluation in low-resource settings.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM Inference Serving</title>
<link>https://arxiv.org/abs/2504.10724</link>
<guid>https://arxiv.org/abs/2504.10724</guid>
<content:encoded><![CDATA[
arXiv:2504.10724v2 Announce Type: replace 
Abstract: Early-Exit Large Language Models (EE-LLMs) enable high throughput inference by allowing tokens to exit early at intermediate layers. However, their throughput is limited by the computational and memory savings. Existing EE-LLM frameworks rely on a single model and therefore, their token generation latencies are bottlenecked by tokens that do not exit early and traverse additional layers. Moreover, early exits are only known at runtime and depend on the request. Therefore, these frameworks load the weights of all model layers even though large portions remain unused when tokens exit early. The lack of memory savings limit us from scaling the batch sizes.
  We propose $\textit{HELIOS}$, a framework that improves both token generation latency and batch sizes to enable high-throughput in EE-LLMs. HELIOS exploits two insights. $\textit{First}$, early exits are often complimentary across models, tokens that do not exit early on one model often take an early-exit on another. HELIOS employs multiple models and dynamically switches between them to collectively maximize the number of tokens that exit early, and minimize token generation latencies. $\textit{Second}$, even when a predicted token does not exit early due to poor confidence, it often remains unchanged even after additional layer traversal. HELIOS greedily allows such tokens to exit early and only loads the weights of the most likely to be used layers, yielding memory savings which is then re-purposed to increase batch sizes. HELIOS employs real-time profiling to accurately identify the early-exit distributions, and adaptively switches between models by tracking tokens in real-time to minimize the performance degradation caused by greedy model loading and exiting. Our evaluations show that HELIOS achieves $1.48\times$ higher throughput and $15.14\times$ larger batch size compared to existing EE-LLM frameworks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minitron-SSM: Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning</title>
<link>https://arxiv.org/abs/2504.11409</link>
<guid>https://arxiv.org/abs/2504.11409</guid>
<content:encoded><![CDATA[
arXiv:2504.11409v2 Announce Type: replace 
Abstract: Hybrid LLM architectures that combine Attention and State Space Models (SSMs) achieve state-of-the-art accuracy and runtime performance. Recent work has demonstrated that applying compression and distillation to Attention-only models yields smaller, more accurate models at a fraction of the training cost. In this work, we explore the effectiveness of compressing Hybrid architectures. We introduce a novel group-aware pruning strategy that preserves the structural integrity of SSM blocks and their sequence modeling capabilities. Furthermore, we demonstrate the necessity of such SSM pruning to achieve improved accuracy and inference speed compared to traditional approaches. Our compression recipe combines SSM, FFN, embedding dimension, and layer pruning, followed by knowledge distillation-based retraining, similar to the MINITRON technique. Using this approach, we compress the Nemotron-H 8B Hybrid model down to 4B parameters with up to 40x fewer training tokens. The resulting model surpasses the accuracy of similarly-sized models while achieving 2x faster inference, significantly advancing the Pareto frontier.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation</title>
<link>https://arxiv.org/abs/2505.06594</link>
<guid>https://arxiv.org/abs/2505.06594</guid>
<content:encoded><![CDATA[
arXiv:2505.06594v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) often struggle to balance visual and textual information when summarizing complex multimodal inputs, such as entire TV show episodes. In this paper, we propose a zero-shot video-to-text summarization approach that builds its own screenplay representation of an episode, effectively integrating key video moments, dialogue, and character information into a unified document. Unlike previous approaches, we simultaneously generate screenplays and name the characters in zero-shot, using only the audio, video, and transcripts as input. Additionally, we highlight that existing summarization metrics can fail to assess the multimodal content in summaries. To address this, we introduce MFactSum, a multimodal metric that evaluates summaries with respect to both vision and text modalities. Using MFactSum, we evaluate our screenplay summaries on the SummScreen3D dataset, demonstrating superiority against state-of-the-art VLMs such as Gemini 1.5 by generating summaries containing 20% more relevant visual information while requiring 75% less of the video as input.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models</title>
<link>https://arxiv.org/abs/2505.10446</link>
<guid>https://arxiv.org/abs/2505.10446</guid>
<content:encoded><![CDATA[
arXiv:2505.10446v3 Announce Type: replace 
Abstract: We introduce the Diffusion Chain of Lateral Thought (DCoLT), a reasoning framework for diffusion language models. DCoLT treats each intermediate step in the reverse diffusion process as a latent "thinking" action and optimizes the entire reasoning trajectory to maximize the reward on the correctness of the final answer with outcome-based Reinforcement Learning (RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal, linear thinking process, DCoLT allows bidirectional, non-linear reasoning with no strict rule on grammatical correctness amid its intermediate steps of thought. We implement DCoLT on two representative Diffusion Language Models (DLMs). First, we choose SEDD as a representative continuous-time discrete diffusion model, where its concrete score derives a probabilistic policy to maximize the RL reward over the entire sequence of intermediate diffusion steps. We further consider the discrete-time masked diffusion language model -- LLaDA, and find that the order to predict and unmask tokens plays an essential role to optimize its RL action resulting from the ranking-based Unmasking Policy Module (UPM) defined by the Plackett-Luce model. Experiments on both math and code generation tasks show that using only public data and 16 H800 GPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even both. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%, +5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeriFastScore: Speeding up long-form factuality evaluation</title>
<link>https://arxiv.org/abs/2505.16973</link>
<guid>https://arxiv.org/abs/2505.16973</guid>
<content:encoded><![CDATA[
arXiv:2505.16973v3 Announce Type: replace 
Abstract: Metrics like FactScore and VeriScore that evaluate long-form factuality operate by decomposing an input response into atomic claims and then individually verifying each claim. While effective and interpretable, these methods incur numerous LLM calls and can take upwards of 100 seconds to evaluate a single response, limiting their practicality in large-scale evaluation and training scenarios. To address this, we propose VeriFastScore, which leverages synthetic data to fine-tune Llama3.1 8B for simultaneously extracting and verifying all verifiable claims within a given text based on evidence from Google Search. We show that this task cannot be solved via few-shot prompting with closed LLMs due to its complexity: the model receives ~4K tokens of evidence on average and needs to concurrently decompose claims, judge their verifiability, and verify them against noisy evidence. However, our fine-tuned VeriFastScore model demonstrates strong correlation with the original VeriScore pipeline at both the example level (r=0.80) and system level (r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence retrieval) over VeriScore. To facilitate future factuality research, we publicly release our VeriFastScore model and synthetic datasets.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Token Distillation: Attention-aware Input Embeddings For New Tokens</title>
<link>https://arxiv.org/abs/2505.20133</link>
<guid>https://arxiv.org/abs/2505.20133</guid>
<content:encoded><![CDATA[
arXiv:2505.20133v2 Announce Type: replace 
Abstract: Current language models rely on static vocabularies determined at pretraining time, which can lead to decreased performance and increased computational cost for domains underrepresented in the original vocabulary. New tokens can be added to solve this problem, when coupled with a good initialization for their new embeddings. However, existing embedding initialization methods require expensive further training or pretraining of additional modules. In this paper, we propose Token Distillation and show that by distilling representations obtained using the original tokenization, we can quickly learn high-quality input embeddings for new tokens. Experimental results with a wide range of open-weight models show that Token Distillation outperforms even strong baselines.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy</title>
<link>https://arxiv.org/abs/2505.20538</link>
<guid>https://arxiv.org/abs/2505.20538</guid>
<content:encoded><![CDATA[
arXiv:2505.20538v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) are being explored for applications in scientific research, including their capabilities to synthesize literature, answer research questions, generate research ideas, and even conduct computational experiments. Ultimately, our goal is for these to help scientists derive novel scientific insights. In many areas of science, such insights often arise from processing and visualizing data to understand its patterns. However, evaluating whether an LLM-mediated scientific workflow produces outputs conveying the correct scientific insights is challenging to evaluate and has not been addressed in past work. We introduce AstroVisBench, the first benchmark for both scientific computing and visualization in the astronomy domain. AstroVisBench judges a language model's ability to both (1) create astronomy-specific workflows to process and analyze data and (2) visualize the results of these workflows through complex plots. Our evaluation of visualizations uses a novel LLM-as-a-judge workflow, which is validated against annotation by five professional astronomers. Using AstroVisBench we present an evaluation of state-of-the-art language models, showing a significant gap in their ability to engage in astronomy research as useful assistants. This evaluation provides a strong end-to-end evaluation for AI scientists that offers a path forward for the development of visualization-based workflows, which are central to a broad range of domains from physics to biology.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Diffusion LLMs via Adaptive Parallel Decoding</title>
<link>https://arxiv.org/abs/2506.00413</link>
<guid>https://arxiv.org/abs/2506.00413</guid>
<content:encoded><![CDATA[
arXiv:2506.00413v2 Announce Type: replace 
Abstract: The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations</title>
<link>https://arxiv.org/abs/2507.00883</link>
<guid>https://arxiv.org/abs/2507.00883</guid>
<content:encoded><![CDATA[
arXiv:2507.00883v2 Announce Type: replace 
Abstract: Although mathematics is often considered culturally neutral, the way mathematical problems are presented can carry implicit cultural context. Existing benchmarks like GSM8K are predominantly rooted in Western norms, including names, currencies, and everyday scenarios. In this work, we create culturally adapted variants of the GSM8K test set for five regions Africa, India, China, Korea, and Japan using prompt-based transformations followed by manual verification. We evaluate six large language models (LLMs), ranging from 8B to 72B parameters, across five prompting strategies to assess their robustness to cultural variation in math problem presentation. Our findings reveal a consistent performance gap: models perform best on the original US-centric dataset and comparatively worse on culturally adapted versions. However, models with reasoning capabilities are more resilient to these shifts, suggesting that deeper reasoning helps bridge cultural presentation gaps in mathematical tasks
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing</title>
<link>https://arxiv.org/abs/2507.14815</link>
<guid>https://arxiv.org/abs/2507.14815</guid>
<content:encoded><![CDATA[
arXiv:2507.14815v2 Announce Type: replace 
Abstract: The rapid advancement of Large Language Models (LLMs) has spurred significant progress in Large Speech-Language Models (LSLMs), enhancing their capabilities in both speech understanding and generation. While existing LSLMs often concentrate on augmenting speech generation or tackling a diverse array of short-speech tasks, the efficient processing of long-form speech remains a critical yet underexplored challenge. This gap is primarily attributed to the scarcity of long-speech training datasets and the high computational costs associated with long sequences. To address these limitations, we introduce FastLongSpeech, a novel framework designed to extend LSLM capabilities for efficient long-speech processing without necessitating dedicated long-speech training data. FastLongSpeech incorporates an iterative fusion strategy that can compress excessively long-speech sequences into manageable lengths. To adapt LSLMs for long-speech inputs, it introduces a dynamic compression training approach, which exposes the model to short-speech sequences at varying compression ratios, thereby transferring the capabilities of LSLMs to long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop a long-speech understanding benchmark called LongSpeech-Eval. Experiments show that our method exhibits strong performance in both long-speech and short-speech tasks, while greatly improving inference efficiency.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multilingual Political Views of Large Language Models: Identification and Steering</title>
<link>https://arxiv.org/abs/2507.22623</link>
<guid>https://arxiv.org/abs/2507.22623</guid>
<content:encoded><![CDATA[
arXiv:2507.22623v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used in everyday tools and applications, raising concerns about their potential influence on political views. While prior research has shown that LLMs often exhibit measurable political biases--frequently skewing toward liberal or progressive positions--key gaps remain. Most existing studies evaluate only a narrow set of models and languages, leaving open questions about the generalizability of political biases across architectures, scales, and multilingual settings. Moreover, few works examine whether these biases can be actively controlled.
  In this work, we address these gaps through a large-scale study of political orientation in modern open-source instruction-tuned LLMs. We evaluate seven models, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using the Political Compass Test with 11 semantically equivalent paraphrases per statement to ensure robust measurement. Our results reveal that larger models consistently shift toward libertarian-left positions, with significant variations across languages and model families. To test the manipulability of political stances, we utilize a simple center-of-mass activation intervention technique and show that it reliably steers model responses toward alternative ideological positions across multiple languages. Our code is publicly available at https://github.com/d-gurgurov/Political-Ideologies-LLMs.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases LLM Judges</title>
<link>https://arxiv.org/abs/2509.03419</link>
<guid>https://arxiv.org/abs/2509.03419</guid>
<content:encoded><![CDATA[
arXiv:2509.03419v2 Announce Type: replace 
Abstract: As large language models (LLMs) grow more capable, they face increasingly diverse and complex tasks, making reliable evaluation challenging. The paradigm of LLMs as judges has emerged as a scalable solution, yet prior work primarily focuses on simple settings. Their reliability in complex tasks--where multi-faceted rubrics, unstructured reference answers, and nuanced criteria are critical--remains understudied. In this paper, we constructed ComplexEval, a challenge benchmark designed to systematically expose and quantify Auxiliary Information Induced Biases. We systematically investigated and validated 6 previously unexplored biases across 12 basic and 3 advanced scenarios. Key findings reveal: (1) all evaluated models exhibit significant susceptibility to these biases, with bias magnitude scaling with task complexity; (2) notably, Large Reasoning Models (LRMs) show paradoxical vulnerability. Our in-depth analysis offers crucial insights for improving the accuracy and verifiability of evaluation signals, paving the way for more general and robust evaluation models.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ready to Translate, Not to Represent? Bias and Performance Gaps in Multilingual LLMs Across Language Families and Domains</title>
<link>https://arxiv.org/abs/2510.07877</link>
<guid>https://arxiv.org/abs/2510.07877</guid>
<content:encoded><![CDATA[
arXiv:2510.07877v2 Announce Type: replace 
Abstract: The rise of Large Language Models (LLMs) has redefined Machine Translation (MT), enabling context-aware and fluent translations across hundreds of languages and textual domains. Despite their remarkable capabilities, LLMs often exhibit uneven performance across language families and specialized domains. Moreover, recent evidence reveals that these models can encode and amplify different biases present in their training data, posing serious concerns for fairness, especially in low-resource languages. To address these gaps, we introduce Translation Tangles, a unified framework and dataset for evaluating the translation quality and fairness of open-source LLMs. Our approach benchmarks 24 bidirectional language pairs across multiple domains using different metrics. We further propose a hybrid bias detection pipeline that integrates rule-based heuristics, semantic similarity filtering, and LLM-based validation. We also introduce a high-quality, bias-annotated dataset based on human evaluations of 1,439 translation-reference pairs. The code and dataset are accessible on GitHub: https://github.com/faiyazabdullah/TranslationTangles
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-MII: Meta-Learning Instruction Induction for LLMs</title>
<link>https://arxiv.org/abs/2510.16932</link>
<guid>https://arxiv.org/abs/2510.16932</guid>
<content:encoded><![CDATA[
arXiv:2510.16932v2 Announce Type: replace 
Abstract: A popular method to adapt large language models (LLMs) to new tasks is in-context learning (ICL), which is effective but incurs high inference costs as context length grows. In this paper we propose a method to perform instruction induction, where we take training examples and reduce them to a compact but descriptive prompt that can achieve performance comparable to ICL over the full training set. Specifically, we propose PROMPT-MII, a reinforcement learning (RL) based framework to meta-learn an instruction induction model that can generate compact instructions on the fly for an arbitrary new dataset. We train on over 3,000 diverse classification datasets from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves downstream model quality by 4-9 F1 points (10-20% relative), matching ICL performance while requiring 3-13x fewer tokens.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging</title>
<link>https://arxiv.org/abs/2510.17426</link>
<guid>https://arxiv.org/abs/2510.17426</guid>
<content:encoded><![CDATA[
arXiv:2510.17426v2 Announce Type: replace 
Abstract: The "alignment tax" of post-training is typically framed as a drop in task accuracy. We show it also involves a severe loss of calibration, making models overconfident, less reliable, and model outputs less diverse. We show that this trade-off can be navigated effectively via a simple post-hoc intervention: interpolating between a model's weights before and after alignment. Crucially, this is not a strict trade-off. We find that the process consistently reveals Pareto-optimal interpolations - models that improve accuracy beyond both parents while substantially recovering the calibration lost during alignment. Our work demonstrates that simple model merging provides a computationally efficient method for mitigating the full scope of the alignment tax, yielding models that are more capable and more reliable.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KAT-Coder Technical Report</title>
<link>https://arxiv.org/abs/2510.18779</link>
<guid>https://arxiv.org/abs/2510.18779</guid>
<content:encoded><![CDATA[
arXiv:2510.18779v3 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have enabled progress in agentic coding, where models autonomously reason, plan, and act within interactive software development workflows. However, bridging the gap between static text-based training and dynamic real-world agentic execution remains a core challenge. In this technical report, we present KAT-Coder, a large-scale agentic code model trained through a multi-stage curriculum encompassing Mid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning (RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances reasoning, planning, and reflection capabilities through a corpus of real software engineering data and synthetic agentic interactions. The SFT stage constructs a million-sample dataset balancing twenty programming languages, ten development contexts, and ten task archetypes. The RFT stage introduces a novel multi-ground-truth reward formulation for stable and sample-efficient policy optimization. Finally, the Reinforcement-to-Deployment phase adapts the model to production-grade IDE environments using Error-Masked SFT and Tree-Structured Trajectory Training. In summary, these stages enable KAT-Coder to achieve robust tool-use reliability, instruction alignment, and long-context reasoning, forming a deployable foundation for real-world intelligent coding agents. Our KAT series 32B model, KAT-Dev, has been open-sourced on https://huggingface.co/Kwaipilot/KAT-Dev.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference</title>
<link>https://arxiv.org/abs/2510.19669</link>
<guid>https://arxiv.org/abs/2510.19669</guid>
<content:encoded><![CDATA[
arXiv:2510.19669v2 Announce Type: replace 
Abstract: Recent reasoning Large Language Models (LLMs) demonstrate remarkable problem-solving abilities but often generate long thinking traces whose utility is unclear. Our work aims to improve their efficiency, enabling them to reach high performance without overthinking. First, we analyze the entropy of token probabilities in reasoning traces. Across three models, we observe a consistent U-shaped entropy pattern: high entropy on easy problems despite high accuracy, low entropy on problems with medium difficulty, and high entropy on hard problems reflecting uncertainty. Specifically, we notice 22--25\% entropy reduction from easy to medium difficulty regions, suggesting an {overthinking} phenomenon on easy instances. Building on these insights, we introduce \textbf{DiffAdapt}, a lightweight framework that selects Easy/Normal/Hard inference strategies per question based on their difficulty and reasoning trace entropy. Each inference strategy consists of a fixed prompt, temperature and maximum token length. In contrast to existing efficiency optimization methods, our approach does not fine-tune base LLM but a small probe that classifies LLM's final hidden state, allowing inexpensive adaptation. We comprehensively evaluate our method on five models and eight benchmarks. Our method achieves comparable or improved accuracy while reducing token usage by up to 22.4\%, establishing a practical path toward compute-efficient reasoning.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representative Social Choice: From Learning Theory to AI Alignment</title>
<link>https://arxiv.org/abs/2410.23953</link>
<guid>https://arxiv.org/abs/2410.23953</guid>
<content:encoded><![CDATA[
arXiv:2410.23953v4 Announce Type: replace-cross 
Abstract: Social choice theory is the study of preference aggregation across a population, used both in mechanism design for human agents and in the democratic alignment of language models. In this study, we propose the representative social choice framework for the modeling of democratic representation in collective decisions, where the number of issues and individuals are too large for mechanisms to consider all preferences directly. These scenarios are widespread in real-world decision-making processes, such as jury trials, legislation, corporate governance, and, more recently, language model alignment. In representative social choice, the population is represented by a finite sample of individual-issue pairs based on which social choice decisions are made. We show that many of the deepest questions in representative social choice can be formulated as statistical learning problems, and prove the generalization properties of social choice mechanisms using the theory of machine learning. We further formulate axioms for representative social choice, and prove Arrow-like impossibility theorems with new combinatorial tools of analysis. Our framework introduces the representative approach to social choice, opening up research directions at the intersection of social choice, learning theory, and AI alignment.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training a Generally Curious Agent</title>
<link>https://arxiv.org/abs/2502.17543</link>
<guid>https://arxiv.org/abs/2502.17543</guid>
<content:encoded><![CDATA[
arXiv:2502.17543v4 Announce Type: replace-cross 
Abstract: Efficient exploration is essential for intelligent systems interacting with their environment, but existing language models often fall short in scenarios that require strategic information gathering. In this paper, we present Paprika, a fine-tuning approach that enables language models to develop general decision-making capabilities that are not confined to particular environments. By training on synthetic interaction data from different tasks that require diverse strategies, Paprika teaches models to explore and adapt their behavior on a new task based on environment feedback in-context without more gradient updates. Experimental results show that models fine-tuned with Paprika can effectively transfer their learned decision-making capabilities to entirely unseen tasks without additional training. Unlike traditional training, our approach's primary bottleneck lies in sampling useful interaction data instead of model updates. To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories from tasks with high learning potential. These results suggest a promising path towards AI systems that can autonomously solve novel sequential decision-making problems that require interactions with the external world.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.14216</link>
<guid>https://arxiv.org/abs/2505.14216</guid>
<content:encoded><![CDATA[
arXiv:2505.14216v2 Announce Type: replace-cross 
Abstract: Recent studies have shown that reinforcement learning with verifiable rewards (RLVR) enhances overall accuracy (pass@1) but often fails to improve capability (pass@k) of LLMs in reasoning tasks, while distillation can improve both. In this paper, we investigate the mechanisms behind these phenomena. First, we demonstrate that RLVR struggles to improve capability as it focuses on improving the accuracy of the easier questions to the detriment of the accuracy of the most difficult questions. Second, we show that RLVR does not merely increase the success probability for the easier questions, but in our small model settings, produces quality responses that were absent in its original output distribution. In addition, we show these responses are neither noticeably longer nor feature more reflection-related keywords, underscoring the need for more reliable indicators of response quality. Third, from the experiment distilling teacher responses to in-distribution problems, we find that capability does not always improve with distillation. We conjecture that capability improves only when new knowledge is introduced, whereas distilling reasoning patterns only improves accuracy but not capability, sacrificing performance on the most difficult questions, similar to RLVR. Together, these findings offer a clearer understanding of how RLVR and distillation shape reasoning behavior in LLMs
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R$^2$ec: Towards Large Recommender Models with Reasoning</title>
<link>https://arxiv.org/abs/2505.16994</link>
<guid>https://arxiv.org/abs/2505.16994</guid>
<content:encoded><![CDATA[
arXiv:2505.16994v3 Announce Type: replace-cross 
Abstract: Large recommender models have extended LLMs as powerful recommenders via encoding or item generation, and recent breakthroughs in LLM reasoning synchronously motivate the exploration of reasoning in recommendation. In this work, we propose R$^2$ec, a unified large recommender model with intrinsic reasoning capability. R$^2$ec introduces a dual-head architecture that supports both reasoning chain generation and efficient item prediction in a single model, significantly reducing inference latency. To overcome the lack of annotated reasoning data, we design RecPO, a reinforcement learning framework that optimizes reasoning and recommendation jointly with a novel fused reward mechanism. Extensive experiments on three datasets demonstrate that R$^2$ec outperforms traditional, LLM-based, and reasoning-augmented recommender baselines, while further analyses validate its competitive efficiency among conventional LLM-based recommender baselines and strong adaptability to diverse recommendation scenarios. Code and checkpoints available at https://github.com/YRYangang/RRec.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RADAR: Benchmarking Language Models on Imperfect Tabular Data</title>
<link>https://arxiv.org/abs/2506.08249</link>
<guid>https://arxiv.org/abs/2506.08249</guid>
<content:encoded><![CDATA[
arXiv:2506.08249v2 Announce Type: replace-cross 
Abstract: Language models (LMs) are increasingly being deployed to perform autonomous data analyses. However, their data awareness -- the ability to recognize, reason over, and appropriately handle data artifacts such as missing values, outliers, and logical inconsistencies -- remains underexplored. These artifacts are especially common in real-world tabular data and, if mishandled, can significantly compromise the validity of analytical conclusions. To address this gap, we present RADAR, a benchmark for systematically evaluating data-aware reasoning on tabular data. We develop a framework to simulate data artifacts via programmatic perturbations to enable targeted evaluation of model behavior. RADAR comprises 2980 table query pairs, grounded in real-world data spanning 9 domains and 5 data artifact types. In addition to evaluating artifact handling, RADAR systematically varies table size to study how reasoning performance holds when increasing table size. Our evaluation reveals that, despite decent performance on tables without data artifacts, frontier models degrade significantly when data artifacts are introduced, exposing critical gaps in their capacity for robust, data-aware analysis. Designed to be flexible and extensible, RADAR supports diverse perturbation types and controllable table sizes, offering a valuable resource for advancing tabular reasoning.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NaviAgent: Bilevel Planning on Tool Navigation Graph for Large-Scale Orchestration</title>
<link>https://arxiv.org/abs/2506.19500</link>
<guid>https://arxiv.org/abs/2506.19500</guid>
<content:encoded><![CDATA[
arXiv:2506.19500v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have recently demonstrated the ability to act as function call agents by invoking external tools, enabling them to solve tasks beyond their static knowledge. However, existing agents typically call tools step by step at a time without a global view of task structure. As tools depend on each other, this leads to error accumulation and limited scalability, particularly when scaling to thousands of tools. To address these limitations, we propose NaviAgent, a novel bilevel architecture that decouples task planning from tool execution through graph-based modeling of the tool ecosystem. At the task-planning level, the LLM-based agent decides whether to respond directly, clarify user intent, invoke a toolchain, or execute tool outputs, ensuring broad coverage of interaction scenarios independent of inter-tool complexity. At the execution level, a continuously evolving Tool World Navigation Model (TWNM) encodes structural and behavioral relations among tools, guiding the agent to generate scalable and robust invocation sequences. By incorporating feedback from real tool interactions, NaviAgent supports closed-loop optimization of planning and execution, moving beyond tool calling toward adaptive navigation of large-scale tool ecosystems. Experiments show that NaviAgent achieves the best task success rates across models and tasks, and integrating TWMN further boosts performance by up to 17 points on complex tasks, underscoring its key role in toolchain orchestration.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiRA: A Hierarchical Reasoning Framework for Decoupled Planning and Execution in Deep Search</title>
<link>https://arxiv.org/abs/2507.02652</link>
<guid>https://arxiv.org/abs/2507.02652</guid>
<content:encoded><![CDATA[
arXiv:2507.02652v2 Announce Type: replace-cross 
Abstract: Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates</title>
<link>https://arxiv.org/abs/2507.23607</link>
<guid>https://arxiv.org/abs/2507.23607</guid>
<content:encoded><![CDATA[
arXiv:2507.23607v2 Announce Type: replace-cross 
Abstract: Clinical trials are a systematic endeavor to assess the safety and efficacy of new drugs or treatments. Conducting such trials typically demands significant financial investment and meticulous planning, highlighting the need for accurate predictions of trial outcomes. Accurately predicting patient enrollment, a key factor in trial success, is one of the primary challenges during the planning phase. In this work, we propose a novel deep learning-based method to address this critical challenge. Our method, implemented as a neural network model, leverages pre-trained language models (PLMs) to capture the complexities and nuances of clinical documents, transforming them into expressive representations. These representations are then combined with encoded tabular features via an attention mechanism. To account for uncertainties in enrollment prediction, we enhance the model with a probabilistic layer based on the Gamma distribution, which enables range estimation. We apply the proposed model to predict clinical trial duration, assuming site-level enrollment follows a Poisson-Gamma process. We carry out extensive experiments on real-world clinical trial data, and show that the proposed method can effectively predict the number of patients enrolled at a number of sites for a given clinical trial, outperforming established baseline models.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents</title>
<link>https://arxiv.org/abs/2508.04412</link>
<guid>https://arxiv.org/abs/2508.04412</guid>
<content:encoded><![CDATA[
arXiv:2508.04412v2 Announce Type: replace-cross 
Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At that, a model poses as an instantaneous domain model backend. Ought to suggest interaction, it is consulted with a web-based task and respective application state. The key problem lies in application state serialisation - referred to as snapshot. State-of-the-art web agents are premised on grounded GUI snapshots, i.e., screenshots enhanced with visual cues. Not least to resemble human perception, but for images representing relatively cheap means of model input. LLM vision still lag behind code interpretation capabilities. DOM snapshots, which structurally resemble HTML, impose a desired alternative. Vast model input token size, however, disables reliable implementation with web agents to date. We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a grounded GUI snapshot baseline (65%) - within the same input token order of magnitude (1e3). Our best evaluated configurations - one token order above, but within the model's context window - outperform this baseline by 8%. Our evaluation, moreover, yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration</title>
<link>https://arxiv.org/abs/2510.03865</link>
<guid>https://arxiv.org/abs/2510.03865</guid>
<content:encoded><![CDATA[
arXiv:2510.03865v2 Announce Type: replace-cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently enhanced the reasoning capabilities of large language models (LLMs), particularly for mathematical problem solving. However, a fundamental limitation remains: as the sampling budget increases, the advantage of RLVR-trained models over their pretrained bases often diminishes or even vanishes, revealing a strong dependence on the base model's restricted search space. We attribute this phenomenon to the widespread use of the reverse Kullback-Leibler (KL) divergence regularizer, whose mode-seeking behavior keeps the policy trapped inside the base model's support region and hampers wider exploration. To address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an algorithm to promote broader yet focused exploration. Our method (i) utilizes the forward KL penalty to replace the reverse KL penalty for out-of-distribution exploration, and (ii) reweights the reference policy to facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B models with RAPO on the 8K SimpleRL-Zero dataset, without supervised fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO consistently improves problem-solving performance. Notably, RAPO enables models to surpass the base model's performance ceiling and solves previously intractable problems, advancing the frontier of RLVR for challenging reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UNO-Bench: A Unified Benchmark for Exploring the Compositional Law Between Uni-modal and Omni-modal in Omni Models</title>
<link>https://arxiv.org/abs/2510.18915</link>
<guid>https://arxiv.org/abs/2510.18915</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal, UNO-Bench, Omni-model, Evaluation, Modality

Summary:
The article introduces a new benchmark, UNO-Bench, designed to evaluate both uni-modal and omni-modal capabilities in large language models. The benchmark includes 1250 human curated samples for omni-modal assessment and 2480 enhanced uni-modal samples. It covers 44 task types and 5 modality combinations, providing a comprehensive evaluation framework. The dataset is suitable for real-world scenarios, especially in the Chinese context, and offers increased efficiency and consistency across public benchmarks. A novel multi-step open-ended question format is proposed for assessing complex reasoning in addition to traditional multi-choice questions. A general scoring model with 6 question types supports automated evaluation with high accuracy. Experimental results demonstrate the Compositional Law between uni-modal and omni-modal performance and the bottleneck effect of omni-modal capabilities on weaker models. The omni-modal capabilities show synergistic promotion on stronger models. 

<br /><br />Summary: <div>
arXiv:2510.18915v3 Announce Type: replace 
Abstract: Multimodal Large Languages models have been progressing from uni-modal understanding toward unifying visual, audio and language modalities, collectively termed omni models. However, the correlation between uni-modal and omni-modal remains unclear, which requires comprehensive evaluation to drive omni model's intelligence evolution. In this work, we introduce a novel, high-quality, and UNified Omni model benchmark, UNO-Bench. This benchmark is designed to effectively evaluate both UNi-modal and Omni-modal capabilities under a unified ability taxonomy, spanning 44 task types and 5 modality combinations. It includes 1250 human curated samples for omni-modal with 98% cross-modality solvability, and 2480 enhanced uni-modal samples. The human-generated dataset is well-suited to real-world scenarios, particularly within the Chinese context, whereas the automatically compressed dataset offers a 90% increase in speed and maintains 98% consistency across 18 public benchmarks. In addition to traditional multi-choice questions, we propose an innovative multi-step open-ended question format to assess complex reasoning. A general scoring model is incorporated, supporting 6 question types for automated evaluation with 95% accuracy. Experimental result shows the Compositional Law between omni-modal and uni-modal performance and the omni-modal capability manifests as a bottleneck effect on weak models, while exhibiting synergistic promotion on strong models.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreetMath: Study of LLMs' Approximation Behaviors</title>
<link>https://arxiv.org/abs/2510.25776</link>
<guid>https://arxiv.org/abs/2510.25776</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, mathematical reasoning, approximation abilities, StreetMath benchmark, cognitive psychology

Summary:
Large language models (LLMs), like Qwen3-4B-Instruct-2507 and Dream-v0-Instruct-7B, are often evaluated for their mathematical reasoning capabilities, especially in precise arithmetic operations. A new benchmark called StreetMath has been introduced to assess LLMs' ability to perform approximate reasoning in real-world scenarios. The study found that LLMs tend to compute exact values or rely on external tools even in tasks requiring approximation. While some models successfully reach the correct answer early on, they tend to use more tokens in approximation tasks. Mechanistic interpretability techniques revealed that neural components for exact and approximate arithmetic operations are largely separate. Contrary to human behavior in similar settings, LLMs do not exhibit cognitive miserliness in street math scenarios. The research is open-sourced on GitHub for further exploration. 

Summary: <br /><br />Keywords: large language models, mathematical reasoning, approximation abilities, StreetMath benchmark, cognitive psychology <div>
arXiv:2510.25776v1 Announce Type: new 
Abstract: There is a substantial body of literature examining the mathematical reasoning capabilities of large language models (LLMs), particularly their performance on precise arithmetic operations in autoregressive architectures. However, their ability to perform approximate reasoning in informal, fast-paced mathematical operations has received far less attention, especially among non-autoregressive decoder models. Our work addresses this gap by introducing StreetMath, a benchmark designed to evaluate models' approximation abilities under real-world approximation scenarios. We conduct extensive evaluations across different LLM architectures: Qwen3-4B-Instruct-2507, Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and Mamba-GPT-3B. Furthermore, we apply mechanistic interpretability techniques to probe their internal computational states. Our analysis reveals that LLMs generally attempt to compute exact values or invoke external tools even in tasks that call for approximation. Moreover, while models sometimes reach the correct answer in early layers or steps, they still consume more tokens when solving approximation tasks. Additional experiments indicate that exact and approximate arithmetic operations rely on largely separate neural components. Drawing upon research on cognitive psychology, we argue that LLMs do not exhibit cognitive miserliness in the same way humans do in street math settings. We open source our work https://github.com/ctseng777/StreetMath
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis</title>
<link>https://arxiv.org/abs/2510.25778</link>
<guid>https://arxiv.org/abs/2510.25778</guid>
<content:encoded><![CDATA[
<div> lexicon-based approach, opinion mining, sentiment analysis, granularity levels, fuzzy logic, syntactic dependency resolution

Summary: 
In this new article, the authors present an innovative approach to opinion mining, also known as sentiment analysis, focusing on the strength of opinions in addition to the sentiment orientation. The proposed method ranks entities based on the orientation and strength of entity reviews and user queries by classifying them into different granularity levels. This classification includes very weak, weak, moderate, very strong, and strong opinions, combining various opinion words related to aspects of interest. The approach utilizes fuzzy logic algorithms to assign opinion words to specific categories and syntactic dependency resolution to identify relationships for desired aspect words. By considering opinion words related to specific aspects, the entity score for each aspect in the review is calculated. This method offers a more nuanced understanding of opinions and evaluations towards entities, enhancing sentiment analysis techniques. <div>
arXiv:2510.25778v1 Announce Type: new 
Abstract: Opinion mining, also called sentiment analysis, is the field of study that analyzes people opinions, sentiments, evaluations, appraisals, attitudes, and emotions towards entities such as products, services, organizations, individuals, issues, events, topics, and their attributes. Holistic lexicon-based approach does not consider the strength of each opinion, i.e., whether the opinion is very strongly negative (or positive), strongly negative (or positive), moderate negative (or positive), very weakly negative (or positive) and weakly negative (or positive). In this paper, we propose approach to rank entities based on orientation and strength of the entity reviews and user's queries by classifying them in granularity levels (i.e. very weak, weak, moderate, very strong and strong) by combining opinion words (i.e. adverb, adjective, noun and verb) that are related to aspect of interest of certain product. We shall use fuzzy logic algorithmic approach in order to classify opinion words into different category and syntactic dependency resolution to find relations for desired aspect words. Opinion words related to certain aspects of interest are considered to find the entity score for that aspect in the review.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LASTIST: LArge-Scale Target-Independent STance dataset</title>
<link>https://arxiv.org/abs/2510.25783</link>
<guid>https://arxiv.org/abs/2510.25783</guid>
<content:encoded><![CDATA[
<div> deep learning, stance detection, target-independent, Korean, dataset

Summary:
The study introduces the LASTIST dataset, which aims to address the lack of resources for stance detection research in low-resource languages like Korean. The dataset, consisting of 563,299 labeled Korean sentences from political press releases, is designed for various stance detection tasks including target-independent and diachronic evolution stance detection. The researchers provide details on dataset collection, construction, and model training using state-of-the-art deep learning techniques. The LASTIST dataset offers opportunities for researchers to explore stance detection in Korean and other low-resource languages, filling a gap in current research focused primarily on English datasets. More information and access to the dataset can be found on the provided link. 

<br /><br />Summary: <div>
arXiv:2510.25783v1 Announce Type: new 
Abstract: Stance detection has emerged as an area of research in the field of artificial intelligence. However, most research is currently centered on the target-dependent stance detection task, which is based on a person's stance in favor of or against a specific target. Furthermore, most benchmark datasets are based on English, making it difficult to develop models in low-resource languages such as Korean, especially for an emerging field such as stance detection. This study proposes the LArge-Scale Target-Independent STance (LASTIST) dataset to fill this research gap. Collected from the press releases of both parties on Korean political parties, the LASTIST dataset uses 563,299 labeled Korean sentences. We provide a detailed description of how we collected and constructed the dataset and trained state-of-the-art deep learning and stance detection models. Our LASTIST dataset is designed for various tasks in stance detection, including target-independent stance detection and diachronic evolution stance detection. We deploy our dataset on https://anonymous.4open.science/r/LASTIST-3721/.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>zFLoRA: Zero-Latency Fused Low-Rank Adapters</title>
<link>https://arxiv.org/abs/2510.25784</link>
<guid>https://arxiv.org/abs/2510.25784</guid>
<content:encoded><![CDATA[
<div> adapter parameters, inference time, zero-latency fused low-rank adapter, experimental results, latency overhead

Summary:
The article discusses the challenges posed by additional compute associated with task-specific adapters in large language models (LLMs). The proposed zero-latency fused low-rank adapter (zFLoRA) aims to minimize latency overhead on top of the base model, particularly compared to low-rank adapters (LoRA) and full fine-tuning (FFT). Experimental results on LLMs of varying sizes demonstrate the effectiveness of zFLoRA across different tasks, including commonsense reasoning, math reasoning, and summary-dialogue. Latency measurements on NPU and GPU platforms show that zFLoRA introduces minimal to zero latency overhead, making it a promising solution for optimizing LLM inference performance. <div>
arXiv:2510.25784v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed with task-specific adapters catering to multiple downstream applications. In such a scenario, the additional compute associated with these apparently insignificant number of adapter parameters (typically less than 1% of the base model) turns out to be disproportionately significant during inference time (upto 2.5x times that of the base model). In this paper, we propose a new zero-latency fused low-rank adapter (zFLoRA) that introduces zero or negligible latency overhead on top of the base model. Experimental results on LLMs of size 1B, 3B and 7B show that zFLoRA compares favorably against the popular supervised fine-tuning benchmarks including low-rank adapters (LoRA) as well as full fine-tuning (FFT). Experiments are conducted on 18 different tasks across three different categories namely commonsense reasoning, math reasoning and summary-dialogue. Latency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA H100) platforms show that the proposed zFLoRA adapters introduce zero to negligible latency overhead.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection</title>
<link>https://arxiv.org/abs/2510.25786</link>
<guid>https://arxiv.org/abs/2510.25786</guid>
<content:encoded><![CDATA[
<div> Keywords: circuit discovery, mechanistic interpretability, benchmark, bootstrapping, integer linear programming <br />
Summary: <br />
The study focuses on improving circuit discovery in mechanistic interpretability models. Three key enhancements are proposed. Firstly, bootstrapping is utilized to identify consistent attribution scores for edges. Secondly, a ratio-based selection strategy prioritizes strong positive-scoring edges, striking a balance between performance and faithfulness. Thirdly, an integer linear programming formulation replaces the standard greedy selection process. The methods developed result in more accurate circuits and outperform previous approaches across a range of Mechanistic Interpretability Benchmark tasks and models. The code for these methods is accessible at the provided GitHub repository for further exploration and implementation. <br /> <div>
arXiv:2510.25786v1 Announce Type: new 
Abstract: One of the main challenges in mechanistic interpretability is circuit discovery, determining which parts of a model perform a given task. We build on the Mechanistic Interpretability Benchmark (MIB) and propose three key improvements to circuit discovery. First, we use bootstrapping to identify edges with consistent attribution scores. Second, we introduce a simple ratio-based selection strategy to prioritize strong positive-scoring edges, balancing performance and faithfulness. Third, we replace the standard greedy selection with an integer linear programming formulation. Our methods yield more faithful circuits and outperform prior approaches across multiple MIB tasks and models. Our code is available at: https://github.com/technion-cs-nlp/MIB-Shared-Task.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection</title>
<link>https://arxiv.org/abs/2510.25799</link>
<guid>https://arxiv.org/abs/2510.25799</guid>
<content:encoded><![CDATA[
<div> Keywords: LISTEN, Large Language Model, multi-objective decisions, preference elicitation, algorithm

Summary: 
LISTEN is introduced as a framework that utilizes a Large Language Model as a preference oracle with high-level priorities provided in natural language. Two iterative algorithms, LISTEN-U and LISTEN-T, are proposed to address the constraints of the LLM and enhance decision-making processes. LISTEN-U excels when preferences are parametrically aligned, while LISTEN-T offers more robust performance in diverse tasks such as flight booking, shopping, and exam scheduling. The concordance metric is introduced to measure the alignment of preferences. This approach shows potential in simplifying complex multi-objective decisions through natural language input, reducing the cognitive burden of traditional preference elicitation.

<br /><br />Summary: <div>
arXiv:2510.25799v1 Announce Type: new 
Abstract: Human experts often struggle to select the best option from a large set of items with multiple competing objectives, a process bottlenecked by the difficulty of formalizing complex, implicit preferences. To address this, we introduce LISTEN, a framework that leverages a Large Language Model (LLM) as a zero-shot preference oracle, guided only by an expert's high-level priorities in natural language. To operate within LLM constraints like context windows and inference costs, we propose two iterative algorithms: LISTEN-U, which uses the LLM to refine a parametric utility function, and LISTEN-T, a non-parametric method that performs tournament-style selections over small batches of solutions. Evaluated on diverse tasks including flight booking, shopping, and exam scheduling, our results show LISTEN-U excels when preferences are parametrically aligned (a property we measure with a novel concordance metric), while LISTEN-T offers more robust performance. This work explores a promising direction for steering complex multi-objective decisions directly with natural language, reducing the cognitive burden of traditional preference elicitation.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data</title>
<link>https://arxiv.org/abs/2510.25804</link>
<guid>https://arxiv.org/abs/2510.25804</guid>
<content:encoded><![CDATA[
<div> framework, data selection, LongFilter, long-context language models, dependencies<br />
<br />
Summary:
The article introduces LongFilter, a framework designed to curate training data specifically for long-context pretraining tasks. Long-context language models, which rely on extended spans of text, require careful data selection as a significant portion of available text data lacks meaningful long-distance dependencies. LongFilter measures the information gain of extended context by comparing model predictions under long-context and short-context conditions. This allows the framework to identify samples where long-range dependencies are crucial for prediction. Experiments conducted with LLaMA-3-8B, a language model with extended contextual capabilities, showed that LongFilter efficiently selects high-quality data. The use of LongFilter led to significant performance improvements on various benchmarks, including HELMET, LongBench, and RULER. This framework enables more efficient training on long-context data and enhances the capabilities of language models in tasks requiring reasoning, code generation, and document summarization. <br /><br />Summary: <div>
arXiv:2510.25804v1 Announce Type: new 
Abstract: Long-context language models unlock advanced capabilities in reasoning, code generation, and document summarization by leveraging dependencies across extended spans of text. However, a significant portion of readily available long-text data lacks meaningful long-distance dependencies; most spans can be predicted using only local context. Training on such data is inefficient, making careful data selection crucial. Therefore, we introduce LongFilter, a framework for curating training data tailored to long-context pretraining. LongFilter measures the information gain provided by extended context by contrasting model predictions under long-context versus short-context settings, thereby identifying samples where long-range dependencies are essential. Experiments with LLaMA-3-8B, extending its context length from 8K to 64K, show that LongFilter efficiently selects high-quality data and yields substantial improvements on benchmarks such as HELMET, LongBench, and RULER.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ideology-Based LLMs for Content Moderation</title>
<link>https://arxiv.org/abs/2510.25805</link>
<guid>https://arxiv.org/abs/2510.25805</guid>
<content:encoded><![CDATA[
<div> persona adoption; harmful content classification; fairness; consistency; large language models

Summary:<br /><br />In this study, the impact of persona adoption on harmful content classification in large language models (LLMs) used for content moderation was examined. While overall classification accuracy did not show significant changes with different personas, a closer analysis revealed that personas with different ideological leanings influenced how content was labeled as harmful, indicating subtle biases in judgment. Larger models tended to align more closely with personas from the same political ideology, increasing within-ideology consistency but widening divergence across ideological groups. Additionally, in a politically targeted task, personas displayed a tendency to defend their own perspective while downplaying harmfulness in opposing views. These findings suggest that persona conditioning can introduce ideological biases into LLM outputs, emphasizing concerns about AI systems reinforcing partisan perspectives under the guise of neutrality.<br /> <div>
arXiv:2510.25805v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used in content moderation systems, where ensuring fairness and neutrality is essential. In this study, we examine how persona adoption influences the consistency and fairness of harmful content classification across different LLM architectures, model sizes, and content modalities (language vs. vision). At first glance, headline performance metrics suggest that personas have little impact on overall classification accuracy. However, a closer analysis reveals important behavioral shifts. Personas with different ideological leanings display distinct propensities to label content as harmful, showing that the lens through which a model "views" input can subtly shape its judgments. Further agreement analyses highlight that models, particularly larger ones, tend to align more closely with personas from the same political ideology, strengthening within-ideology consistency while widening divergence across ideological groups. To show this effect more directly, we conducted an additional study on a politically targeted task, which confirmed that personas not only behave more coherently within their own ideology but also exhibit a tendency to defend their perspective while downplaying harmfulness in opposing views. Together, these findings highlight how persona conditioning can introduce subtle ideological biases into LLM outputs, raising concerns about the use of AI systems that may reinforce partisan perspectives under the guise of neutrality.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Long Context: When Semantics Matter More than Tokens</title>
<link>https://arxiv.org/abs/2510.25816</link>
<guid>https://arxiv.org/abs/2510.25816</guid>
<content:encoded><![CDATA[
<div> semantic question answering, Electronic Health Records, Clinical Entity Augmented Retrieval, Clinical Notes QA Evaluation Platform, natural language processing 

Summary: 
The study introduces the CLEAR method for improving semantic question answering in Electronic Health Records (EHR). CLEAR uses entity-aware retrieval to achieve higher performance with a lower token count compared to traditional methods. A Clinical Notes QA Evaluation Platform was developed to validate CLEAR, showing a 58.3 percent win rate and an average semantic similarity of 0.878. The method also outperformed wide context processing, using 78 percent fewer tokens. Significant performance gains were observed on long EHR documents, with a 75 percent win rate for notes exceeding 65,000 tokens. The findings confirm that entity-aware retrieval enhances efficiency and accuracy in clinical natural language processing. The evaluation framework provides a transparent benchmark for evaluating clinical question answering systems, emphasizing semantic precision and computational efficiency.<br /><br />Summary: <div>
arXiv:2510.25816v1 Announce Type: new 
Abstract: Electronic Health Records (EHR) store clinical documentation as base64 encoded attachments in FHIR DocumentReference resources, which makes semantic question answering difficult. Traditional vector database methods often miss nuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR) method, introduced by Lopez et al. 2025, uses entity aware retrieval and achieved improved performance with an F1 score of 0.90 versus 0.86 for embedding based retrieval, while using over 70 percent fewer tokens. We developed a Clinical Notes QA Evaluation Platform to validate CLEAR against zero shot large context inference and traditional chunk based retrieval augmented generation. The platform was tested on 12 clinical notes ranging from 10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a 58.3 percent win rate, an average semantic similarity of 0.878, and used 78 percent fewer tokens than wide context processing. The largest performance gains occurred on long notes, with a 75 percent win rate for documents exceeding 65,000 tokens. These findings confirm that entity aware retrieval improves both efficiency and accuracy in clinical natural language processing. The evaluation framework provides a reusable and transparent benchmark for assessing clinical question answering systems where semantic precision and computational efficiency are critical.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Efficient Large Language Model Training: From Data-centric Perspectives</title>
<link>https://arxiv.org/abs/2510.25817</link>
<guid>https://arxiv.org/abs/2510.25817</guid>
<content:encoded><![CDATA[
<div> Data-efficient, Large Language Models, Post-training, Data challenges, Data-centric perspective<br />
<br />
Summary: 
This paper discusses the importance of post-training Large Language Models (LLMs) for enhancing task generalization and domain-specific capabilities. It addresses the challenges of data inefficiency in current LLM post-training methods, such as high manual annotation costs and diminishing returns on data scales. The study presents a taxonomy of data-efficient LLM post-training methods, including data selection, quality enhancement, synthetic data generation, distillation, compression, and self-evolving data ecosystems. Representative approaches in each category are outlined, along with future research directions. By identifying the hurdles in data-efficient LLM post-training, the paper highlights key challenges and suggests potential research directions. The goal is to encourage further exploration into maximizing data utilization in large-scale model training. <div>
arXiv:2510.25817v1 Announce Type: new 
Abstract: Post-training of Large Language Models (LLMs) is crucial for unlocking their task generalization potential and domain-specific capabilities. However, the current LLM post-training paradigm faces significant data challenges, including the high costs of manual annotation and diminishing marginal returns on data scales. Therefore, achieving data-efficient post-training has become a key research question. In this paper, we present the first systematic survey of data-efficient LLM post-training from a data-centric perspective. We propose a taxonomy of data-efficient LLM post-training methods, covering data selection, data quality enhancement, synthetic data generation, data distillation and compression, and self-evolving data ecosystems. We summarize representative approaches in each category and outline future research directions. By examining the challenges in data-efficient LLM post-training, we highlight open problems and propose potential research avenues. We hope our work inspires further exploration into maximizing the potential of data utilization in large-scale model training. Paper List: https://github.com/luo-junyu/Awesome-Data-Efficient-LLM
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation</title>
<link>https://arxiv.org/abs/2510.25904</link>
<guid>https://arxiv.org/abs/2510.25904</guid>
<content:encoded><![CDATA[
<div> LLM-based applications, accelerate, substitute, human labor, language resources <br />
<br />
Summary: The study evaluates the use of LLM-based tools in automating semantic annotation tasks. Three experimental settings were compared: manual, automatic, and semi-automatic annotation. The semi-automatic setting showed increased frame diversity and similar annotation coverage compared to human-only annotation. The automatic setting performed poorly in all metrics except for annotation time. The study highlights the potential of LLM-based tools in accelerating the creation of language resources but also emphasizes the importance of a hybrid approach for optimal results. <div>
arXiv:2510.25904v1 Announce Type: new 
Abstract: The use of LLM-based applications as a means to accelerate and/or substitute human labor in the creation of language resources and dataset is a reality. Nonetheless, despite the potential of such tools for linguistic research, comprehensive evaluation of their performance and impact on the creation of annotated datasets, especially under a perspectivized approach to NLP, is still missing. This paper contributes to reduction of this gap by reporting on an extensive evaluation of the (semi-)automatization of FrameNet-like semantic annotation by the use of an LLM-based semantic role labeler. The methodology employed compares annotation time, coverage and diversity in three experimental settings: manual, automatic and semi-automatic annotation. Results show that the hybrid, semi-automatic annotation setting leads to increased frame diversity and similar annotation coverage, when compared to the human-only setting, while the automatic setting performs considerably worse in all metrics, except for annotation time.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline</title>
<link>https://arxiv.org/abs/2510.25941</link>
<guid>https://arxiv.org/abs/2510.25941</guid>
<content:encoded><![CDATA[
<div> pipeline, language model, training data, extraction, verification 

Summary: 
- The paper introduces RECAP, an agentic pipeline designed to extract and verify memorized training data from large language models (LLMs).
- RECAP utilizes a feedback-driven loop where an initial extraction attempt is evaluated by a secondary language model, which compares the output against a reference passage and provides correction hints.
- The pipeline includes a jailbreaking module to overcome alignment-induced refusals.
- Evaluation on the EchoTrace benchmark, covering over 30 full books, showed significant improvements over single-iteration approaches.
- With GPT-4.1, the average ROUGE-L score for extracting copyrighted text increased from 0.38 to 0.47, a nearly 24% enhancement.

<br /><br />Summary: <div>
arXiv:2510.25941v1 Announce Type: new 
Abstract: If we cannot inspect the training data of a large language model (LLM), how can we ever know what it has seen? We believe the most compelling evidence arises when the model itself freely reproduces the target content. As such, we propose RECAP, an agentic pipeline designed to elicit and verify memorized training data from LLM outputs. At the heart of RECAP is a feedback-driven loop, where an initial extraction attempt is evaluated by a secondary language model, which compares the output against a reference passage and identifies discrepancies. These are then translated into minimal correction hints, which are fed back into the target model to guide subsequent generations. In addition, to address alignment-induced refusals, RECAP includes a jailbreaking module that detects and overcomes such barriers. We evaluate RECAP on EchoTrace, a new benchmark spanning over 30 full books, and the results show that RECAP leads to substantial gains over single-iteration approaches. For instance, with GPT-4.1, the average ROUGE-L score for the copyrighted text extraction improved from 0.38 to 0.47 - a nearly 24% increase.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Multilingual Data Mixtures in Language Model Pretraining</title>
<link>https://arxiv.org/abs/2510.25947</link>
<guid>https://arxiv.org/abs/2510.25947</guid>
<content:encoded><![CDATA[
<div> training, multilingual data, language models, pivot language, performance

Summary: 
- Combining English and multilingual data during pretraining large language models (LLMs) does not necessarily lower performance if languages have a sufficient number of tokens in the corpus.
- Using English as a pivot language can benefit multilingual generalization across language families, instead of selecting a pivot language from the same family.
- The increase in the number of training languages does not significantly impact the performance of LLMs at this scale, challenging the belief in the "curse of multilinguality".
- Multilingual data, when balanced correctly, can enhance language model capabilities without sacrificing performance, even in low-resource settings. 

<br /><br /> <div>
arXiv:2510.25947v1 Announce Type: new 
Abstract: The impact of different multilingual data mixtures in pretraining large language models (LLMs) has been a topic of ongoing debate, often raising concerns about potential trade-offs between language coverage and model performance (i.e., the curse of multilinguality). In this work, we investigate these assumptions by training 1.1B and 3B parameter LLMs on diverse multilingual corpora, varying the number of languages from 25 to 400. Our study challenges common beliefs surrounding multilingual training. First, we find that combining English and multilingual data does not necessarily degrade the in-language performance of either group, provided that languages have a sufficient number of tokens included in the pretraining corpus. Second, we observe that using English as a pivot language (i.e., a high-resource language that serves as a catalyst for multilingual generalization) yields benefits across language families, and contrary to expectations, selecting a pivot language from within a specific family does not consistently improve performance for languages within that family. Lastly, we do not observe a significant "curse of multilinguality" as the number of training languages increases in models at this scale. Our findings suggest that multilingual data, when balanced appropriately, can enhance language model capabilities without compromising performance, even in low-resource settings
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Label Drift in Cross-Cultural Translation</title>
<link>https://arxiv.org/abs/2510.25967</link>
<guid>https://arxiv.org/abs/2510.25967</guid>
<content:encoded><![CDATA[
<div> Keywords: Machine Translation, sentiment preservation, cultural alignment, label drift, Large Language Models

Summary: 
Machine Translation (MT) is used to address resource scarcity by generating synthetic data from high-resource languages. This study focuses on the impact of cultural alignment on sentiment preservation in translation. The experiments reveal three key findings: 
1) MT systems, including Large Language Models, induce label drift, especially in culturally sensitive domains.
2) Large Language Models incorporate cultural knowledge, which can exacerbate label drift.
3) Cultural similarity between source and target languages plays a crucial role in label preservation. Neglecting cultural factors in MT can lead to misinterpretation and cultural conflicts in downstream applications. This study underscores the importance of considering cultural alignment in MT to ensure label fidelity and avoid potential issues in cross-cultural communication. 

<br /><br />Summary: <div>
arXiv:2510.25967v1 Announce Type: new 
Abstract: Machine Translation (MT) is widely employed to address resource scarcity in low-resource languages by generating synthetic data from high-resource counterparts. While sentiment preservation in translation has long been studied, a critical but underexplored factor is the role of cultural alignment between source and target languages. In this paper, we hypothesize that semantic labels are drifted or altered during MT due to cultural divergence. Through a series of experiments across culturally sensitive and neutral domains, we establish three key findings: (1) MT systems, including modern Large Language Models (LLMs), induce label drift during translation, particularly in culturally sensitive domains; (2) unlike earlier statistical MT tools, LLMs encode cultural knowledge, and leveraging this knowledge can amplify label drift; and (3) cultural similarity or dissimilarity between source and target languages is a crucial determinant of label preservation. Our findings highlight that neglecting cultural factors in MT not only undermines label fidelity but also risks misinterpretation and cultural conflict in downstream applications.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation</title>
<link>https://arxiv.org/abs/2510.25975</link>
<guid>https://arxiv.org/abs/2510.25975</guid>
<content:encoded><![CDATA[
<div> Neurosymbolic, mathematical reasoning, SymCode, SymPy library, accuracy improvement <br />
Summary:
Large Language Models (LLMs) often struggle with complex mathematical reasoning, leading to unreliable solutions. This study introduces SymCode, a neurosymbolic framework that reframes mathematical problem-solving as verifiable code generation using the SymPy library. SymCode significantly improves accuracy on challenging benchmarks compared to existing methods. It is more token-efficient and shifts model failures from opaque logical fallacies to transparent programmatic errors. By grounding LLM reasoning in a deterministic symbolic engine, SymCode aims to enhance the accuracy and trustworthiness of AI in formal domains. <div>
arXiv:2510.25975v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often struggle with complex mathematical reasoning, where prose-based generation leads to unverified and arithmetically unsound solutions. Current prompting strategies like Chain of Thought still operate within this unreliable medium, lacking a mechanism for deterministic verification. To address these limitations, we introduce SymCode, a neurosymbolic framework that reframes mathematical problem-solving as a task of verifiable code generation using the SymPy library. We evaluate SymCode on challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating significant accuracy improvements of up to 13.6 percentage points over baselines. Our analysis shows that SymCode is not only more token-efficient but also fundamentally shifts model failures from opaque logical fallacies towards transparent, programmatic errors. By grounding LLM reasoning in a deterministic symbolic engine, SymCode represents a key step towards more accurate and trustworthy AI in formal domains.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium</title>
<link>https://arxiv.org/abs/2510.25977</link>
<guid>https://arxiv.org/abs/2510.25977</guid>
<content:encoded><![CDATA[
<div> AWS, Trainium, AI accelerator, matrix multiplication, LLM<br />
<br />
Summary:<br />
AI accelerators like Trainium from AWS offer cost-effective solutions for AI training and inference. This paper focuses on designing a high-performance matrix multiplication kernel for LLM inference on Trainium, addressing challenges posed by its systolic array architecture and data layout requirements. Through kernel fusion and caching strategies, the proposed system achieves significant speedups compared to AWS's implementation on Trainium, with an average 1.35x speedup at the matmul kernel level and an average 1.66x speedup for end-to-end LLM inference. Evaluation with multiple datasets and LLM models demonstrates the effectiveness of the customized techniques in maximizing SRAM bandwidth, minimizing data movement, and avoiding costly matrix transposes. <div>
arXiv:2510.25977v1 Announce Type: new 
Abstract: AI accelerators, customized to AI workloads, provide cost-effective and high-performance solutions for training and inference. Trainium, an AI accelerator recently developed by Amazon Web Services (AWS), provides an attractive option for LLM training and inference through its heterogeneous architecture. However, leveraging Trainium architecture for high performance can be challenging because of its systolic array architecture and special requirement on data layout. In this paper, we design high-performance matrix multiplication (matmul), a critical compute kernel, for LLM inference on Trainium. We introduce a series of techniques customized to Trainium based on kernel fusion and novel caching strategies to reduce data movement across the software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive matrix transpose. Evaluating with nine datasets and four recent LLMs, we show that our system largely outperforms the state-of-the-art matmul implemented by AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x speedup (up to 2.22x), which translates to an average 1.66x speedup (up to 2.49x) for end-to-end LLM inference.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache</title>
<link>https://arxiv.org/abs/2510.25979</link>
<guid>https://arxiv.org/abs/2510.25979</guid>
<content:encoded><![CDATA[
<div> framework, prefill, LLM, attention, inference

Summary:
AttnCache is introduced as a framework to enhance the prefill stage of Large Language Models (LLMs) by optimizing the self-attention computation. By recognizing that semantically distinct sentences often yield similar attention maps, AttnCache leverages an attention map memorization database to retrieve and reuse pre-cached attention maps during inference. This approach results in a significant reduction in computational overhead, leading to a 1.2x end-to-end and 2x attention speedup on CPU, and a 1.6x end-to-end and 3x attention speedup on GPU, with minimal impact on accuracy. AttnCache's efficient caching and similarity search techniques enable it to accelerate LLM inference tasks that rely on the prefill stage, such as classification, question answering, recommendation, and text embedding. <div>
arXiv:2510.25979v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model encodes input sequences without performing autoregressive decoding. In these prefill only scenarios, the self-attention computation becomes the primary performance bottleneck due to its quadratic complexity with respect to sequence length. In this paper, we observe that semantically different sentences often produce similar attention maps across layers and heads. Building on this insight, we propose AttnCache, a framework that accelerates the prefill stage of LLM inference by retrieving and reusing similar attention maps. Based on an attention map memorization database, AttnCache employs efficient caching and similarity search techniques to identify and reuse pre-cached attention maps during inference, thereby reducing the computational overhead of self-attention. Experimental results show that AttnCache achieves an average of 1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x attention speedup on GPU, with negligible accuracy degradation.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning</title>
<link>https://arxiv.org/abs/2510.25992</link>
<guid>https://arxiv.org/abs/2510.25992</guid>
<content:encoded><![CDATA[
<div> Supervised Reinforcement Learning, Large Language Models, problem solving, logical actions, reasoning <br />
Summary: <br />
Supervised Reinforcement Learning (SRL) is proposed as a framework to improve the performance of Large Language Models (LLMs) in multi-step reasoning tasks. SRL addresses the limitations of Reinforcement Learning with Verifiable Rewards (RLVR) and Supervised Fine-Tuning (SFT) by training the model to generate a sequence of logical actions. This approach involves generating an internal reasoning monologue before committing to each action and using expert demonstrations for smoother rewards. SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR and can be used as an effective training framework for reasoning-oriented LLMs. Additionally, initializing training with SRL before refining with RLVR leads to the best overall performance. SRL shows promise in generalizing effectively to tasks beyond reasoning benchmarks, such as agentic software engineering tasks, making it a robust and versatile approach for training LLMs focused on reasoning. <br /> <div>
arXiv:2510.25992v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, we propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical "actions". SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model's actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PORTool: Tool-Use LLM Training with Rewarded Tree</title>
<link>https://arxiv.org/abs/2510.26020</link>
<guid>https://arxiv.org/abs/2510.26020</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, language models, tool-use, trajectories, tool-call<br />
Summary: <br />
The article introduces PORTool, a reinforcement learning method designed to improve the performance of tool-use large language models (LLMs) in dynamic tool-call environments. PORTool encourages the LLM to explore various trajectories to find the correct answer by generating multiple rollouts for a given query. The method assigns rewards to each step based on its ability to produce a correct answer and make successful tool calls. Rewards are calculated based on step-wise performance and used to train the LLM for tool use. Experiments using 17 tools to address user queries show significant improvements in final accuracy and the number of tool-call steps using PORTool compared to other training approaches. Ablation studies confirm the necessity and robustness of step-wise rewards in enhancing the LLM's performance in tool integration and reasoning. <div>
arXiv:2510.26020v1 Announce Type: new 
Abstract: Current tool-use large language models (LLMs) are trained on static datasets, enabling them to interact with external tools and perform multi-step, tool-integrated reasoning, which produces tool-call trajectories. However, these models imitate how a query is resolved in a generic tool-call routine, thereby failing to explore possible solutions and demonstrating limited performance in an evolved, dynamic tool-call environment. In this work, we propose PORTool, a reinforcement learning (RL) method that encourages a tool-use LLM to explore various trajectories yielding the correct answer. Specifically, this method starts with generating multiple rollouts for a given query, and some of them share the first few tool-call steps, thereby forming a tree-like structure. Next, we assign rewards to each step, based on its ability to produce a correct answer and make successful tool calls. A shared step across different trajectories receives the same reward, while different steps under the same fork receive different rewards. Finally, these step-wise rewards are used to calculate fork-relative advantages, blended with trajectory-relative advantages, to train the LLM for tool use. The experiments utilize 17 tools to address user queries, covering both time-sensitive and time-invariant topics. We conduct ablation studies to systematically justify the necessity and the design robustness of step-wise rewards. Furthermore, we compare the proposed PORTool with other training approaches and demonstrate significant improvements in final accuracy and the number of tool-call steps.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs</title>
<link>https://arxiv.org/abs/2510.26024</link>
<guid>https://arxiv.org/abs/2510.26024</guid>
<content:encoded><![CDATA[
<div> alignment, multilingual representations, cultural erasure, knowledge transfer, language models <br />
Summary: <br />
The paper introduces the concept of Cross-lingual alignment (CLA) and its potential drawback of cultural erasure, where multilingual models unintentionally converge culturally-situated responses. The authors propose a novel evaluation framework, the transfer-localization plane, to quantify both knowledge transfer and cultural localization. Analysis of recent CLA approaches shows improved factual transfer but decreased cultural specificity across languages. Internal model examination suggests different layers optimize universal transfer and cultural knowledge. To address this trade-off, the authors introduce Surgical Steering, a method that steers model activation at specific layers to balance factual transfer and cultural specificity. This approach effectively mitigates the limitations of current alignment techniques, offering a more nuanced solution to cross-lingual representation alignment. <br /> <div>
arXiv:2510.26024v1 Announce Type: new 
Abstract: Cross-lingual alignment (CLA) aims to align multilingual representations, enabling Large Language Models (LLMs) to seamlessly transfer knowledge across languages. While intuitive, we hypothesize, this pursuit of representational convergence can inadvertently cause "cultural erasure", the functional loss of providing culturally-situated responses that should diverge based on the query language. In this work, we systematically analyze this trade-off by introducing a holistic evaluation framework, the transfer-localization plane, which quantifies both desirable knowledge transfer and undesirable cultural erasure. Using this framework, we re-evaluate recent CLA approaches and find that they consistently improve factual transfer at the direct cost of cultural localization across all six languages studied. Our investigation into the internal representations of these models reveals a key insight: universal factual transfer and culturally-specific knowledge are optimally steerable at different model layers. Based on this finding, we propose Surgical Steering, a novel inference-time method that disentangles these two objectives. By applying targeted activation steering to distinct layers, our approach achieves a better balance between the two competing dimensions, effectively overcoming the limitations of current alignment techniques.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings</title>
<link>https://arxiv.org/abs/2510.26032</link>
<guid>https://arxiv.org/abs/2510.26032</guid>
<content:encoded><![CDATA[
<div> prevalence, features, clinical outcomes, nodules, cancer <br />
Summary: <br />
- A study was conducted to analyze incidental thyroid findings (ITFs) detected in non-thyroid imaging. 
- An NLP pipeline was developed to identify ITFs in radiology reports and assess their prevalence, features, and clinical outcomes. 
- Among 115,683 patients, 7.8% had an ITF, mostly nodules. 
- ITFs were more common in women, older adults, and those with higher BMI, especially when imaging was ordered by oncology or internal medicine. 
- Nodule characteristics were poorly documented. 
- Patients with ITFs had higher odds of thyroid nodule diagnosis, biopsy, thyroidectomy, and thyroid cancer diagnosis. 
- Most cancers detected after ITFs were papillary and larger compared to those detected without ITF. 
- The study highlights the role of ITFs in thyroid cancer overdiagnosis and emphasizes the need for standardized reporting and selective follow-up. <br /> <div>
arXiv:2510.26032v1 Announce Type: new 
Abstract: Importance Incidental thyroid findings (ITFs) are increasingly detected on imaging performed for non-thyroid indications. Their prevalence, features, and clinical consequences remain undefined. Objective To develop, validate, and deploy a natural language processing (NLP) pipeline to identify ITFs in radiology reports and assess their prevalence, features, and clinical outcomes. Design, Setting, and Participants Retrospective cohort of adults without prior thyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from July 1, 2017, to September 30, 2023. A transformer-based NLP pipeline identified ITFs and extracted nodule characteristics from image reports from multiple modalities and body regions. Main Outcomes and Measures Prevalence of ITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer diagnosis. Logistic regression identified demographic and imaging-related factors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9% women), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more likely in women, older adults, those with higher BMI, and when imaging was ordered by oncology or internal medicine. Compared with chest CT, ITFs were more likely via neck CT, PET, and nuclear medicine scans. Nodule characteristics were poorly documented, with size reported in 44% and other features in fewer than 15% (e.g. calcifications). Compared with patients without ITFs, those with ITFs had higher odds of thyroid nodule diagnosis, biopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were papillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were common and strongly associated with cascades leading to the detection of small, low-risk cancers. These findings underscore the role of ITFs in thyroid cancer overdiagnosis and the need for standardized reporting and more selective follow-up.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback</title>
<link>https://arxiv.org/abs/2510.26101</link>
<guid>https://arxiv.org/abs/2510.26101</guid>
<content:encoded><![CDATA[
<div> evaluation, large language models, quantum programming, benchmark, human-written code

Summary:
The article introduces the QCoder Benchmark, an evaluation framework for assessing Large Language Models (LLMs) in the domain of quantum programming. The benchmark provides feedback from simulated hardware devices, allowing for evaluation using a quantum simulator environment and incorporating domain-specific metrics such as circuit depth and execution time. It also includes human-written code submissions from real programming contests for comparison. Experiments show that advanced models like GPT-4o achieve only 18.97% accuracy, while reasoning-based models like o3 reach up to 78% accuracy, surpassing human-written code success rates. The QCoder Benchmark dataset and public evaluation API have been released to support further research. <br /><br />Summary: <div>
arXiv:2510.26101v1 Announce Type: new 
Abstract: Large language models (LLMs) have increasingly been applied to automatic programming code generation. This task can be viewed as a language generation task that bridges natural language, human knowledge, and programming logic. However, it remains underexplored in domains that require interaction with hardware devices, such as quantum programming, where human coders write Python code that is executed on a quantum computer. To address this gap, we introduce QCoder Benchmark, an evaluation framework that assesses LLMs on quantum programming with feedback from simulated hardware devices. Our benchmark offers two key features. First, it supports evaluation using a quantum simulator environment beyond conventional Python execution, allowing feedback of domain-specific metrics such as circuit depth, execution time, and error classification, which can be used to guide better generation. Second, it incorporates human-written code submissions collected from real programming contests, enabling both quantitative comparisons and qualitative analyses of LLM outputs against human-written codes. Our experiments reveal that even advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting the difficulty of the benchmark. In contrast, reasoning-based models such as o3 reach up to 78% accuracy, outperforming averaged success rates of human-written codes (39.98%). We release the QCoder Benchmark dataset and public evaluation API to support further research.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking</title>
<link>https://arxiv.org/abs/2510.26122</link>
<guid>https://arxiv.org/abs/2510.26122</guid>
<content:encoded><![CDATA[
<div> Keywords: Test-Time Scaling, low diversity, reasoning ability, one problem multiple solutions, Reasoning Path Divergence

Summary:
Test-Time Scaling (TTS) has been effective in enhancing the reasoning ability of large language models (LLMs). However, a common issue is the lack of diversity in model outputs due to the "one problem, one solution" (1P1S) training practice. To address this, the authors propose a new training paradigm called "one problem, multiple solutions" (1PNS), which exposes the models to various valid reasoning paths. They introduce a metric called Reasoning Path Divergence (RPD) to measure semantic differences between multi-step chains of thought. By selecting maximally diverse solution sets per problem using RPD and fine-tuning the model, they achieve more varied outputs with higher pass rates. Experiments demonstrate significant improvements in pass rates over a strong 1P1S baseline, showcasing the effectiveness of 1PNS in amplifying the benefits of TTS. The code for their approach is available on GitHub. 

<br /><br />Summary: <div>
arXiv:2510.26122v1 Announce Type: new 
Abstract: While Test-Time Scaling (TTS) has proven effective in improving the reasoning ability of large language models (LLMs), low diversity in model outputs often becomes a bottleneck; this is partly caused by the common "one problem, one solution" (1P1S) training practice, which provides a single canonical answer and can push models toward a narrow set of reasoning paths. To address this, we propose a "one problem, multiple solutions" (1PNS) training paradigm that exposes the model to a variety of valid reasoning trajectories and thus increases inference diversity. A core challenge for 1PNS is reliably measuring semantic differences between multi-step chains of thought, so we introduce Reasoning Path Divergence (RPD), a step-level metric that aligns and scores Long Chain-of-Thought solutions to capture differences in intermediate reasoning. Using RPD, we curate maximally diverse solution sets per problem and fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields more varied outputs and higher pass@k, with an average +2.80% gain in pass@16 over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that 1PNS further amplifies the effectiveness of TTS. Our code is available at https://github.com/fengjujf/Reasoning-Path-Divergence .
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Influence of Discourse Relations in Persuasive Texts</title>
<link>https://arxiv.org/abs/2510.26124</link>
<guid>https://arxiv.org/abs/2510.26124</guid>
<content:encoded><![CDATA[
<div> Persuasion Techniques, Discourse Relations, Large Language Models, Prompt Engineering, SemEval 2023 Task 3<br />
<br />
Summary: This paper explores the relationship between Persuasion Techniques and Discourse Relations using Large Language Models and prompt engineering. By developing classifiers based on LLMs to label instances with both PTs and level-2 DRs, the study generated silver datasets indicating the importance of certain discourse relations in persuasive texts, such as Cause, Purpose, Contrast, Cause+Belief, Concession, and Condition. These findings are particularly relevant for detecting online propaganda and misinformation, as well as enhancing our understanding of effective communication strategies. <div>
arXiv:2510.26124v1 Announce Type: new 
Abstract: This paper investigates the relationship between Persuasion Techniques (PTs) and Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and prompt engineering. Since no dataset annotated with both PTs and DRs exists, we took the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point and developed LLM-based classifiers to label each instance of the dataset with one of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10 different prompts, resulting in 40 unique DR classifiers. Ensemble models using different majority-pooling strategies were used to create 5 silver datasets of instances labelled with both persuasion techniques and level-2 PDTB senses. The silver dataset sizes vary from 1,281 instances to 204 instances, depending on the majority pooling technique used. Statistical analysis of these silver datasets shows that six discourse relations (namely Cause, Purpose, Contrast, Cause+Belief, Concession, and Condition) play a crucial role in persuasive texts, especially in the use of Loaded Language, Exaggeration/Minimisation, Repetition and to cast Doubt. This insight can contribute to detecting online propaganda and misinformation, as well as to our general understanding of effective communication.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MossNet: Mixture of State-Space Experts is a Multi-Head Attention</title>
<link>https://arxiv.org/abs/2510.26182</link>
<guid>https://arxiv.org/abs/2510.26182</guid>
<content:encoded><![CDATA[
<div> MoE, MossNet, state-space, experts, language modeling <br />
Summary: <br />
- The paper introduces MossNet, a novel architecture for large language models (LLMs) that combines the advantages of transformer and state-space/gated-recurrent models (SSMs, GRMs).
- MossNet utilizes a mixture-of-state-space-experts approach to emulate multiple attention heads, improving model expressiveness.
- Extensive experiments demonstrate that MossNet outperforms existing transformer and SSM-based models of similar size and data budgets in language modeling tasks.
- Larger variants of MossNet trained on massive datasets show scalability and superior performance.
- Real-device profiling on Samsung Galaxy S24 Ultra and Nvidia A100 GPU reveals favorable runtime speed and resource usage compared to baseline models. <br /> <br />Summary: <div>
arXiv:2510.26182v1 Announce Type: new 
Abstract: Large language models (LLMs) have significantly advanced generative applications in natural language processing (NLP). Recent trends in model architectures revolve around efficient variants of transformers or state-space/gated-recurrent models (SSMs, GRMs). However, prevailing SSM/GRM-based methods often emulate only a single attention head, potentially limiting their expressiveness. In this work, we propose MossNet, a novel mixture-of-state-space-experts architecture that emulates a linear multi-head attention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation not only in channel-mixing multi-layered perceptron (MLP) blocks but also in the time-mixing SSM kernels to realize multiple "attention heads." Extensive experiments on language modeling and downstream evaluations show that MossNet outperforms both transformer- and SSM-based architectures of similar model size and data budgets. Larger variants of MossNet, trained on trillions of tokens, further confirm its scalability and superior performance. In addition, real-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU demonstrate favorable runtime speed and resource usage compared to similarly sized baselines. Our results suggest that MossNet is a compelling new direction for efficient, high-performing recurrent LLM architectures.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Similarity-Distance-Magnitude Language Models</title>
<link>https://arxiv.org/abs/2510.26183</link>
<guid>https://arxiv.org/abs/2510.26183</guid>
<content:encoded><![CDATA[
<div> fine-tuning, sequence prediction, language models, SDM activation layer, instruction-following <br />
Summary:
Similarity-Distance-Magnitude (SDM) language models (LMs) are introduced as sequence prediction models fine-tuned using a final-layer SDM activation layer for binary classification of instruction-following. Existing pre-trained decoder-only Transformer LMs can be converted into SDM LMs through supervised fine-tuning, utilizing the SDM activation layer for estimating change-of-base during training. This approach reduces abstentions and enhances statistical efficiency compared to supervised baselines by incorporating hard negative examples generated online. SDM LMs aim to maximize generation accuracy in the high-probability region partitioned by the SDM activation layer, providing a novel solution for improving language model performance in specific tasks such as instruction-following. <div>
arXiv:2510.26183v1 Announce Type: new 
Abstract: We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which are sequence prediction models fine-tuned to maximize the proportion of generations in the well-calibrated, high-probability region partitioned by a final-layer SDM activation layer used for binary classification of instruction-following. We demonstrate that existing pre-trained decoder-only Transformer LMs can be readily converted into SDM LMs via supervised fine-tuning, using the final-layer SDM activation layer during training to estimate a change-of-base for a supervised next-token loss over a contrastive input encoding scheme, with additional hard negative examples generated online during training. This results in reduced abstentions (i.e., improved statistical efficiency) compared to strong supervised baselines.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RCScore: Quantifying Response Consistency in Large Language Models</title>
<link>https://arxiv.org/abs/2510.26193</link>
<guid>https://arxiv.org/abs/2510.26193</guid>
<content:encoded><![CDATA[
<div> framework, instruction style, model responses, RCScore, consistency

Summary: 
The article introduces RCScore, a framework that quantifies how different instruction styles impact the performance of language model models (LLMs) across reasoning benchmarks. The research reveals that instruction style can significantly affect model accuracy, with variations of up to 16.7% points observed across different styles. The study also introduces Cross-Response Similarity (CRS) as a method to measure stylistic self-consistency, which shows a strong correlation with task accuracy, indicating its utility as a measure of model reliability. The research findings suggest that deterministic decoding leads to more stylistically stable outputs, and larger model scales are associated with higher cross-style consistency. Overall, RCScore offers a systematic approach to evaluating the robustness of LLMs to different instruction styles. 

<br /><br />Summary: <div>
arXiv:2510.26193v1 Announce Type: new 
Abstract: Current LLM evaluations often rely on a single instruction template, overlooking models' sensitivity to instruction style-a critical aspect for real-world deployments. We present RCScore, a multi-dimensional framework quantifying how instruction formulation affects model responses. By systematically transforming benchmark problems into multiple instruction styles, RCScore reveals performance variations undetected by conventional metrics. Our experiments across ten LLMs on four reasoning benchmarks demonstrate that instruction style can shift accuracy by up to 16.7% points. We introduce Cross-Response Similarity (CRS), a method applying RCScore metrics to measure stylistic self-consistency, and establish its strong correlation with task accuracy, suggesting consistency as a valuable proxy for model reliability. Additional findings show that deterministic decoding produces more stylistically stable outputs, and model scale correlates positively with cross-style consistency. RCScore offers a principled approach to assess instruction robustness.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation</title>
<link>https://arxiv.org/abs/2510.26200</link>
<guid>https://arxiv.org/abs/2510.26200</guid>
<content:encoded><![CDATA[
<div> timestep, diffusion language models, controllability, update forgetting, token ordering
Summary:
Token Timestep Allocation (TTA) addresses the issue of update forgetting in diffusion language models by introducing soft and semantic token ordering through per token timestep schedules. This approach aims to prevent uniform and context agnostic updates that can disrupt the refinement process and degrade fluency and coherence. TTA can be implemented as either a fixed policy or an adaptive policy driven by task signals, providing a range of refinement strategies. By operating solely at inference time, TTA can be applied across various DLMs and supervision sources. Empirical results show that TTA improves controllability and fluency, significantly enhancing sentiment control accuracy and reducing toxicity levels and perplexity in text generation tasks. Overall, softened ordering via timestep allocation emerges as a crucial factor in mitigating update forgetting and achieving stable and controllable diffusion text generation.<br /><br />Summary: <div>
arXiv:2510.26200v1 Announce Type: new 
Abstract: While diffusion language models (DLMs) enable fine-grained refinement, their practical controllability remains fragile. We identify and formally characterize a central failure mode called update forgetting, in which uniform and context agnostic updates induce token level fluctuations across timesteps, erasing earlier semantic edits and disrupting the cumulative refinement process, thereby degrading fluency and coherence. As this failure originates in uniform and context agnostic updates, effective control demands explicit token ordering. We propose Token Timestep Allocation (TTA), which realizes soft and semantic token ordering via per token timestep schedules: critical tokens are frozen early, while uncertain tokens receive continued refinement. This timestep based ordering can be instantiated as either a fixed policy or an adaptive policy driven by task signals, thereby supporting a broad spectrum of refinement strategies. Because it operates purely at inference time, it applies uniformly across various DLMs and naturally extends to diverse supervision sources. Empirically, TTA improves controllability and fluency: on sentiment control, it yields more than 20 percent higher accuracy and nearly halves perplexity using less than one fifth the steps; in detoxification, it lowers maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0). Together, these results demonstrate that softened ordering via timestep allocation is the critical lever for mitigating update forgetting and achieving stable and controllable diffusion text generation.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data</title>
<link>https://arxiv.org/abs/2510.26202</link>
<guid>https://arxiv.org/abs/2510.26202</guid>
<content:encoded><![CDATA[
<div> Sparse Autoencoder, Human Feedback, Preferences, Data Curation, Personalization  
Summary:  
- The study introduces the What's In My Human Feedback? (WIMHF) method to explain feedback data using sparse autoencoders.  
- WIMHF helps identify human-interpretable features in feedback data that impact language model preferences.  
- There is a diverse range of preferences expressed by annotators across different datasets, influenced by dataset-level context.  
- Potentially unsafe preferences, such as favoring toxic content, are highlighted by WIMHF, enabling safer data curation.  
- Personalization based on annotator-specific weights over subjective features can improve preference prediction on certain datasets.  
<br /><br />Summary: <div>
arXiv:2510.26202v1 Announce Type: new 
Abstract: Human feedback can alter language models in unpredictable and undesirable ways, as practitioners lack a clear understanding of what feedback data encodes. While prior work studies preferences over certain attributes (e.g., length or sycophancy), automatically extracting relevant features without pre-specifying hypotheses remains challenging. We introduce What's In My Human Feedback? (WIMHF), a method to explain feedback data using sparse autoencoders. WIMHF characterizes both (1) the preferences a dataset is capable of measuring and (2) the preferences that the annotators actually express. Across 7 datasets, WIMHF identifies a small number of human-interpretable features that account for the majority of the preference prediction signal achieved by black-box models. These features reveal a wide diversity in what humans prefer, and the role of dataset-level context: for example, users on Reddit prefer informality and jokes, while annotators in HH-RLHF and PRISM disprefer them. WIMHF also surfaces potentially unsafe preferences, such as that LMArena users tend to vote against refusals, often in favor of toxic content. The learned features enable effective data curation: re-labeling the harmful examples in Arena yields large safety gains (+37%) with no cost to general performance. They also allow fine-grained personalization: on the Community Alignment dataset, we learn annotator-specific weights over subjective features that improve preference prediction. WIMHF provides a human-centered analysis method for practitioners to better understand and use preference data.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning</title>
<link>https://arxiv.org/abs/2510.26205</link>
<guid>https://arxiv.org/abs/2510.26205</guid>
<content:encoded><![CDATA[
<div> global RAG, evaluation benchmark, GlobalQA, GlobalRAG, retrieval-augmented generation

Summary:
Global retrieval-augmented generation (RAG) is crucial for tasks that require a comprehensive analysis of entire document collections, unlike local RAG which focuses on specific text chunks. A new benchmark, GlobalQA, evaluates global RAG capabilities through tasks like counting, extremum queries, sorting, and top-k extraction. Existing RAG methods struggle with global tasks, with the best baseline achieving only 1.51 F1 score. To address this, a new approach called GlobalRAG is introduced, incorporating chunk-level retrieval, LLM-driven filters, and aggregation modules. On the Qwen2.5-14B model, GlobalRAG significantly outperforms the strongest baseline with an F1 score of 6.63. This highlights the effectiveness of the proposed method in improving global RAG performance. <div>
arXiv:2510.26205v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) has emerged as a leading approach to reducing hallucinations in large language models (LLMs). Current RAG evaluation benchmarks primarily focus on what we call local RAG: retrieving relevant chunks from a small subset of documents to answer queries that require only localized understanding within specific text chunks. However, many real-world applications require a fundamentally different capability -- global RAG -- which involves aggregating and analyzing information across entire document collections to derive corpus-level insights (for example, "What are the top 10 most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first benchmark specifically designed to evaluate global RAG capabilities, covering four core task types: counting, extremum queries, sorting, and top-k extraction. Through systematic evaluation across different models and baselines, we find that existing RAG methods perform poorly on global tasks, with the strongest baseline achieving only 1.51 F1 score. To address these challenges, we propose GlobalRAG, a multi-tool collaborative framework that preserves structural coherence through chunk-level retrieval, incorporates LLM-driven intelligent filters to eliminate noisy documents, and integrates aggregation modules for precise symbolic computation. On the Qwen2.5-14B model, GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1, validating the effectiveness of our method.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs</title>
<link>https://arxiv.org/abs/2510.26253</link>
<guid>https://arxiv.org/abs/2510.26253</guid>
<content:encoded><![CDATA[
<div> Keywords: pragmatic theories, language models, implied meanings, Gricean pragmatics, Relevance Theory 

Summary: 
- The study focuses on enhancing language models' ability to understand implied meanings by using pragmatic theories as prompts.
- Introducing pragmatic theories like Gricean pragmatics and Relevance Theory as prompts helps language models in step-by-step reasoning and deriving interpretations.
- The proposed approach leads to a significant improvement of up to 9.6% in pragmatic reasoning tasks compared to traditional methods.
- Even a brief mention of pragmatic theories in the prompt without explaining the details can boost performance by 1-3% in larger language models.
- The findings suggest that incorporating pragmatic theories as prompts can enhance language models' pragmatic reasoning abilities without the need for extensive training on the theories.<br /><br />Summary: <div>
arXiv:2510.26253v1 Announce Type: new 
Abstract: The ability to accurately interpret implied meanings plays a crucial role in human communication and language use, and language models are also expected to possess this capability. This study demonstrates that providing language models with pragmatic theories as prompts is an effective in-context learning approach for tasks to understand implied meanings. Specifically, we propose an approach in which an overview of pragmatic theories, such as Gricean pragmatics and Relevance Theory, is presented as a prompt to the language model, guiding it through a step-by-step reasoning process to derive a final interpretation. Experimental results showed that, compared to the baseline, which prompts intermediate reasoning without presenting pragmatic theories (0-shot Chain-of-Thought), our methods enabled language models to achieve up to 9.6\% higher scores on pragmatic reasoning tasks. Furthermore, we show that even without explaining the details of pragmatic theories, merely mentioning their names in the prompt leads to a certain performance improvement (around 1-3%) in larger models compared to the baseline.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages</title>
<link>https://arxiv.org/abs/2510.26254</link>
<guid>https://arxiv.org/abs/2510.26254</guid>
<content:encoded><![CDATA[
<div> loanwords, language models, bilingual communities, lexical integration, NLP
Summary:
The study investigates the ability of pretrained language models to identify loanwords compared to native vocabulary in bilingual communities. Despite explicit instructions and contextual information, the models performed poorly in distinguishing loanwords from native words across 10 languages. This suggests a bias towards loanwords rather than native equivalents in modern NLP systems. The findings have implications for developing NLP tools for minority languages and supporting language preservation in communities facing lexical pressure from dominant languages. <div>
arXiv:2510.26254v1 Announce Type: new 
Abstract: Throughout language history, words are borrowed from one language to another and gradually become integrated into the recipient's lexicon. Speakers can often differentiate these loanwords from native vocabulary, particularly in bilingual communities where a dominant language continuously imposes lexical items on a minority language. This paper investigates whether pretrained language models, including large language models, possess similar capabilities for loanword identification. We evaluate multiple models across 10 languages. Despite explicit instructions and contextual information, our results show that models perform poorly in distinguishing loanwords from native ones. These findings corroborate previous evidence that modern NLP systems exhibit a bias toward loanwords rather than native equivalents. Our work has implications for developing NLP tools for minority languages and supporting language preservation in communities under lexical pressure from dominant languages.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual</title>
<link>https://arxiv.org/abs/2510.26271</link>
<guid>https://arxiv.org/abs/2510.26271</guid>
<content:encoded><![CDATA[
<div> knowledge distillation, vision-language models, multilingualism, model compression, cross-lingual representation consistency

Summary:<br />
- The study focuses on the behavior of knowledge distillation (KD) in transferring knowledge from larger to smaller vision-language models (VLMs) across multiple languages.
- Five distillation approaches were evaluated on CLIP and SigLIP2 models, assessing their impact on cross-lingual representation consistency and downstream performance stability during model compression.
- Some distillation configurations successfully maintained or enhanced multilingual retrieval robustness when halving the model size, while others did not sustain cross-task stability.
- The research reveals design-sensitive trade-offs in distillation formulations, highlighting the importance of considering factors beyond aggregate accuracy.
- The findings emphasize the need for careful selection and optimization of distillation strategies for VLMs to ensure consistent performance across languages and tasks.<br />Summary: <div>
arXiv:2510.26271v1 Announce Type: new 
Abstract: Vision-language models (VLMs) exhibit uneven performance across languages, a problem that is often exacerbated when the model size is reduced. While Knowledge distillation (KD) demonstrates promising results in transferring knowledge from larger to smaller VLMs, applying KD in multilingualism is an underexplored area. This paper presents a controlled empirical study of KD behavior across five distillation approaches, isolating their effects on cross-lingual representation consistency and downstream performance stability under model compression. We study five distillation formulations across CLIP and SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual QA. We find that some configurations preserve or even improve multilingual retrieval robustness despite halving model size, but others fail to maintain cross-task stability, exposing design-sensitive trade-offs that aggregate accuracy alone does not reveal.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs Signal When They're Right? Evidence from Neuron Agreement</title>
<link>https://arxiv.org/abs/2510.26277</link>
<guid>https://arxiv.org/abs/2510.26277</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Neuron activations, Neuron Agreement Decoding, Ensemble decoding, Internal signals

Summary:
Large language models (LLMs) improve reasoning through sample-evaluate-ensemble decoders without ground truth data. This study investigates the internal behavior of LLMs based on neuron activations and makes three key findings: external signals are projections of richer internal dynamics, correct responses activate fewer unique neurons than incorrect ones, and correct responses exhibit stronger cross-sample agreement. Based on these findings, Neuron Agreement Decoding (NAD) is proposed, a method that selects candidates using activation sparsity and cross-sample neuron agreement solely based on internal signals. NAD enables early correctness prediction and supports aggressive early stopping, reducing token usage by 99% with minimal loss in quality. By utilizing internal signals, NAD provides reliable, scalable, and efficient guidance for label-free ensemble decoding. <br /><br />Summary: <div>
arXiv:2510.26277v1 Announce Type: new 
Abstract: Large language models (LLMs) commonly boost reasoning via sample-evaluate-ensemble decoders, achieving label free gains without ground truth. However, prevailing strategies score candidates using only external outputs such as token probabilities, entropies, or self evaluations, and these signals can be poorly calibrated after post training. We instead analyze internal behavior based on neuron activations and uncover three findings: (1) external signals are low dimensional projections of richer internal dynamics; (2) correct responses activate substantially fewer unique neurons than incorrect ones throughout generation; and (3) activations from correct responses exhibit stronger cross sample agreement, whereas incorrect ones diverge. Motivated by these observations, we propose Neuron Agreement Decoding (NAD), an unsupervised best-of-N method that selects candidates using activation sparsity and cross sample neuron agreement, operating solely on internal signals and without requiring comparable textual outputs. NAD enables early correctness prediction within the first 32 generated tokens and supports aggressive early stopping. Across math and science benchmarks with verifiable answers, NAD matches majority voting; on open ended coding benchmarks where majority voting is inapplicable, NAD consistently outperforms Avg@64. By pruning unpromising trajectories early, NAD reduces token usage by 99% with minimal loss in generation quality, showing that internal signals provide reliable, scalable, and efficient guidance for label free ensemble decoding.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unravelling the Mechanisms of Manipulating Numbers in Language Models</title>
<link>https://arxiv.org/abs/2510.26285</link>
<guid>https://arxiv.org/abs/2510.26285</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, numbers, input embedding, accuracy, probing techniques 

Summary: 
In a recent study, it was found that large language models (LLMs) converge to accurate input representations for numbers, despite their tendency to produce errors when dealing with numeric information. This conflict is addressed by exploring how LLMs manipulate numbers and quantifying the accuracy of these mechanisms. The research reveals that different LLMs learn interchangeable and highly accurate representations of numbers that are consistent across hidden states and input contexts. Universal probes for each LLM are created to track information and errors back to specific layers. These findings provide insights into how pre-trained LLMs handle numerical data and suggest the potential for refining LLM architectures using more accurate probing techniques. 

<br /><br />Summary: <div>
arXiv:2510.26285v1 Announce Type: new 
Abstract: Recent work has shown that different large language models (LLMs) converge to similar and accurate input embedding representations for numbers. These findings conflict with the documented propensity of LLMs to produce erroneous outputs when dealing with numeric information. In this work, we aim to explain this conflict by exploring how language models manipulate numbers and quantify the lower bounds of accuracy of these mechanisms. We find that despite surfacing errors, different language models learn interchangeable representations of numbers that are systematic, highly accurate and universal across their hidden states and the types of input contexts. This allows us to create universal probes for each LLM and to trace information -- including the causes of output errors -- to specific layers. Our results lay a fundamental understanding of how pre-trained LLMs manipulate numbers and outline the potential of more accurate probing techniques in addressed refinements of LLMs' architectures.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games</title>
<link>https://arxiv.org/abs/2510.26298</link>
<guid>https://arxiv.org/abs/2510.26298</guid>
<content:encoded><![CDATA[
<div> Evaluation, OpenAI, ChatGPT, web interaction, browser-based games  
Summary:  
- OpenAI's ChatGPT Atlas has new capabilities for web interaction, analyzing webpages, processing user intents, and executing inputs in the browser.  
- The study evaluates Atlas's performance in browser-based games like Sudoku, T-Rex Runner, Flappy Bird, and Stein.world.  
- Atlas excels in logical reasoning tasks like Sudoku, completing puzzles faster than humans.  
- However, Atlas struggles in real-time games requiring precise timing and motor control, often failing to progress beyond initial obstacles.  
- The findings indicate Atlas's strong analytical processing but notable limitations in dynamic web environments requiring real-time interaction.  

Summary: <div>
arXiv:2510.26298v1 Announce Type: new 
Abstract: OpenAI's ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynamic, interactive environments remains less explored. In this study, we conduct an early evaluation of Atlas's web interaction capabilities using browser-based games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird, and Stein.world. We employ in-game performance scores as quantitative metrics to assess performance across different task types. Our results show that Atlas performs strongly in logical reasoning tasks like Sudoku, completing puzzles significantly faster than human baselines, but struggles substantially in real-time games requiring precise timing and motor control, often failing to progress beyond initial obstacles. These findings suggest that while Atlas demonstrates capable analytical processing, there remain notable limitations in dynamic web environments requiring real-time interaction. The website of our project can be found at https://atlas-game-eval.github.io.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations using Tool Calling</title>
<link>https://arxiv.org/abs/2510.26322</link>
<guid>https://arxiv.org/abs/2510.26322</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, student feedback, educational settings, privacy concerns, small open-source models

Summary:
The article introduces SCRIBE, a framework for generating personalized student feedback using small, open-source language models. SCRIBE addresses challenges such as privacy concerns, limited computational resources, and the need for pedagogically valid responses. It combines domain-specific tools with a self-reflective inference pipeline for multi-hop reasoning. SCRIBE models, developed using LoRA fine-tuning, achieve comparable or superior quality to larger models in terms of relevance and actionability. Evaluation with a human-aligned GPT-Judge and a user study with 108 students shows that SCRIBE is perceived on par with GPT-4o and Llama-3.3 70B. These results demonstrate the effectiveness of SCRIBE for low-resource and privacy-sensitive educational applications. 

<br /><br />Summary: <div>
arXiv:2510.26322v1 Announce Type: new 
Abstract: Language models can be used to provide interactive, personalized student feedback in educational settings. However, real-world deployment faces three key challenges: privacy concerns, limited computational resources, and the need for pedagogically valid responses. These constraints require small, open-source models that can run locally and reliably ground their outputs in correct information. We introduce SCRIBE, a framework for multi-hop, tool-augmented reasoning designed to generate valid responses to student questions about feedback reports. SCRIBE combines domain-specific tools with a self-reflective inference pipeline that supports iterative reasoning, tool use, and error recovery. We distil these capabilities into 3B and 8B models via two-stage LoRA fine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned GPT-Judge and a user study with 108 students shows that 8B-SCRIBE models achieve comparable or superior quality to much larger models in key dimensions such as relevance and actionability, while being perceived on par with GPT-4o and Llama-3.3 70B by students. These findings demonstrate the viability of SCRIBE for low-resource, privacy-sensitive educational applications.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning</title>
<link>https://arxiv.org/abs/2510.26336</link>
<guid>https://arxiv.org/abs/2510.26336</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, ACER, domain specialization, curriculum learning, knowledge transfer

Summary: 
ACER (Automated Curriculum-Enhanced Regimen) is a method designed to enhance the performance of Large Language Models (LLMs) in specialized domains such as economics and psychology. By generating a curriculum based on Bloom's taxonomy and using synthetic question-answer pairs for continual pretraining, ACER helps LLMs become domain experts while maintaining their broad capabilities. Experimental results with the Llama 3.2 model show significant improvements in specialized subsets, particularly in challenging domains like microeconomics. ACER not only prevents catastrophic forgetting but also enables positive cross-domain knowledge transfer. It boosts accuracy on target domains and knowledge-intensive benchmarks like ARC and GPQA while maintaining stable performance on general reasoning tasks. Overall, ACER offers a scalable and effective solution for closing critical domain gaps in LLMs. 

<br /><br />Summary: <div>
arXiv:2510.26336v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at general tasks but underperform in specialized domains like economics and psychology, which require deep, principled understanding. To address this, we introduce ACER (Automated Curriculum-Enhanced Regimen) that transforms generalist models into domain experts without sacrificing their broad capabilities. ACER first synthesizes a comprehensive, textbook-style curriculum by generating a table of contents for a subject and then creating question-answer (QA) pairs guided by Bloom's taxonomy. This ensures systematic topic coverage and progressively increasing difficulty. The resulting synthetic corpus is used for continual pretraining with an interleaved curriculum schedule, aligning learning across both content and cognitive dimensions.
  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized MMLU subsets. In challenging domains like microeconomics, where baselines struggle, ACER boosts accuracy by 5 percentage points. Across all target domains, we observe a consistent macro-average improvement of 3 percentage points. Notably, ACER not only prevents catastrophic forgetting but also facilitates positive cross-domain knowledge transfer, improving performance on non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points, while maintaining stable performance on general reasoning tasks. Our results demonstrate that ACER offers a scalable and effective recipe for closing critical domain gaps in LLMs.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data</title>
<link>https://arxiv.org/abs/2510.26345</link>
<guid>https://arxiv.org/abs/2510.26345</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic data generation, large language models, fallacious arguments, fine-tuning techniques, scientific misinformation

Summary: 
The research focuses on addressing health-related misinformation by utilizing synthetic data generation and lightweight fine-tuning techniques to improve the ability of large language models (LLMs) to identify fallacious arguments. The proposed MisSynth pipeline incorporates retrieval-augmented generation (RAG) to create synthetic fallacy samples for fine-tuning LLM models. Results show significant accuracy improvements in the classification of fallacies, with fine-tuned models outperforming vanilla baselines. For example, the fine-tuned LLaMA 3.1 8B model achieved a substantial F1-score improvement on the MISSCI test split. The study demonstrates that introducing synthetic fallacy data can enhance zero-shot LLM classification performance on scientific misinformation tasks, even with limited computational resources. The code and synthetic dataset for the study are made available on GitHub for further exploration. 

Summary:  <div>
arXiv:2510.26345v1 Announce Type: new 
Abstract: Health-related misinformation is very prevalent and potentially harmful. It is difficult to identify, especially when claims distort or misinterpret scientific findings. We investigate the impact of synthetic data generation and lightweight fine-tuning techniques on the ability of large language models (LLMs) to recognize fallacious arguments using the MISSCI dataset and framework. In this work, we propose MisSynth, a pipeline that applies retrieval-augmented generation (RAG) to produce synthetic fallacy samples, which are then used to fine-tune an LLM model. Our results show substantial accuracy gains with fine-tuned models compared to vanilla baselines. For instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score absolute improvement on the MISSCI test split over its vanilla baseline. We demonstrate that introducing synthetic fallacy data to augment limited annotated resources can significantly enhance zero-shot LLM classification performance on real-world scientific misinformation tasks, even with limited computational resources. The code and synthetic dataset are available on https://github.com/mxpoliakov/MisSynth.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic Teams for Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2510.26352</link>
<guid>https://arxiv.org/abs/2510.26352</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent approach, large language models, team composition, community detection, synergistic teams

Summary:
In the article, a new framework for automatic team composition of multi-agent large language models (LLMs) is proposed. Traditional methods struggle to form optimal teams due to the opacity of model internals. The proposed method creates a "language model graph" based on pairwise conversations to map relationships between models and identify synergistic clusters using community detection. Experimental results show that the method effectively identifies functionally coherent groups reflecting latent specializations of LLMs. Priming conversations with specific topics further enhances team performance, outperforming random baselines and matching manually-curated teams. The findings suggest a promising avenue for designing collaborative LLM teams without prior knowledge of model internals, showcasing the potential for automated team composition in multi-agent systems.<br /><br />Summary: <div>
arXiv:2510.26352v1 Announce Type: new 
Abstract: While a multi-agent approach based on large language models (LLMs) represents a promising strategy to surpass the capabilities of single models, its success is critically dependent on synergistic team composition. However, forming optimal teams is a significant challenge, as the inherent opacity of most models obscures the internal characteristics necessary for effective collaboration. In this paper, we propose an interaction-centric framework for automatic team composition that does not require any prior knowledge including their internal architectures, training data, or task performances. Our method constructs a "language model graph" that maps relationships between models from the semantic coherence of pairwise conversations, and then applies community detection to identify synergistic model clusters. Our experiments with diverse LLMs demonstrate that the proposed method discovers functionally coherent groups that reflect their latent specializations. Priming conversations with specific topics identified synergistic teams which outperform random baselines on downstream benchmarks and achieve comparable accuracy to that of manually-curated teams based on known model specializations. Our findings provide a new basis for the automated design of collaborative multi-agent LLM teams.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Role of Context for Discourse Relation Classification in Scientific Writing</title>
<link>https://arxiv.org/abs/2510.26354</link>
<guid>https://arxiv.org/abs/2510.26354</guid>
<content:encoded><![CDATA[
<div> Keywords: generative Artificial Intelligence, discourse structure, scientific writing, pretrained language model, Large Language Model 

Summary: 
In this study, the focus is on the application of generative Artificial Intelligence in supporting science workflows by analyzing discourse structure in scientific writing. The researchers investigate the task of Discourse Relation Classification (DRC) using pretrained language models (PLM) and Large Language Models (LLM) in the context of scientific publications. The experiments demonstrate that incorporating context, defined by discourse structure, can enhance the DRC task. The analysis reveals that certain types of scientific discourse relations are more likely to benefit from contextual information. This preliminary investigation highlights the potential of utilizing PLMs and LLMs for understanding discourse in scientific texts and lays the foundation for future research in leveraging AI methods for supporting scientific claims. 

<br /><br />Summary: <div>
arXiv:2510.26354v1 Announce Type: new 
Abstract: With the increasing use of generative Artificial Intelligence (AI) methods to support science workflows, we are interested in the use of discourse-level information to find supporting evidence for AI generated scientific claims. A first step towards this objective is to examine the task of inferring discourse structure in scientific writing.
  In this work, we present a preliminary investigation of pretrained language model (PLM) and Large Language Model (LLM) approaches for Discourse Relation Classification (DRC), focusing on scientific publications, an under-studied genre for this task. We examine how context can help with the DRC task, with our experiments showing that context, as defined by discourse structure, is generally helpful. We also present an analysis of which scientific discourse relation types might benefit most from context.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education</title>
<link>https://arxiv.org/abs/2510.26422</link>
<guid>https://arxiv.org/abs/2510.26422</guid>
<content:encoded><![CDATA[
<div> educational benchmark, Chinese, large language models, knowledge dimension, cultivation dimension
<br />
Summary:
OmniEduBench is introduced as a comprehensive Chinese educational benchmark to address the lack of evaluation of cultivation capabilities in existing large language models (LLMs) used in educational scenarios. The dataset contains 24.602K high-quality question-answer pairs divided into knowledge and cultivation dimensions, covering 61 subjects across 6 fine-grained categories. With a variety of question formats, including 11 common exam question types, OmniEduBench aims to assess LLMs' capabilities in education comprehensively. Experimental results on 11 mainstream LLMs show a performance gap, with Gemini-2.5 Pro surpassing 60% accuracy in the knowledge dimension and QWQ trailing human intelligence by nearly 30% in the cultivation dimension. These findings emphasize the need for improvement in LLMs for effective application in education.
<br />Summary: <div>
arXiv:2510.26422v1 Announce Type: new 
Abstract: With the rapid development of large language models (LLMs), various LLM-based works have been widely applied in educational fields. However, most existing LLMs and their benchmarks focus primarily on the knowledge dimension, largely neglecting the evaluation of cultivation capabilities that are essential for real-world educational scenarios. Additionally, current benchmarks are often limited to a single subject or question type, lacking sufficient diversity. This issue is particularly prominent within the Chinese context. To address this gap, we introduce OmniEduBench, a comprehensive Chinese educational benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs. The data is meticulously divided into two core dimensions: the knowledge dimension and the cultivation dimension, which contain 18.121K and 6.481K entries, respectively. Each dimension is further subdivided into 6 fine-grained categories, covering a total of 61 different subjects (41 in the knowledge and 20 in the cultivation). Furthermore, the dataset features a rich variety of question formats, including 11 common exam question types, providing a solid foundation for comprehensively evaluating LLMs' capabilities in education. Extensive experiments on 11 mainstream open-source and closed-source LLMs reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro surpassed 60\% accuracy, while in the cultivation dimension, the best-performing model, QWQ, still trailed human intelligence by nearly 30\%. These results highlight the substantial room for improvement and underscore the challenges of applying LLMs in education.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large Language Models</title>
<link>https://arxiv.org/abs/2510.26446</link>
<guid>https://arxiv.org/abs/2510.26446</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Compression, Sparse Optimization, Low-Rank Approximation, Efficiency 

Summary: 
Large Language Models (LLMs) have shown great language processing capabilities but are limited by high bandwidth and computational requirements. This study introduces Synergistic Sparse and Low-Rank Compression (SSLC) methods for LLMs, combining low-rank approximation and sparse optimization to compress models efficiently. The proposed SSLC method outperforms individual techniques, achieving state-of-the-art results on LLaMA and Qwen2.5 models without the need for additional training steps. In experiments, SSLC compresses the Qwen2.5 model by 50% with no loss in performance, while also providing a speedup of at least 1.63x. This approach offers a practical solution for deploying efficient LLMs in real-world applications. 

<br /><br />Summary: <div>
arXiv:2510.26446v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in language comprehension and generation; however, their widespread adoption is constrained by substantial bandwidth and computational demands. While pruning and low-rank approximation have each demonstrated promising performance individually, their synergy for LLMs remains underexplored. We introduce \underline{S}ynergistic \underline{S}parse and \underline{L}ow-Rank \underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths of both techniques: low-rank approximation compresses the model by retaining its essential structure with minimal information loss, whereas sparse optimization eliminates non-essential weights, preserving those crucial for generalization. Based on theoretical analysis, we first formulate the low-rank approximation and sparse optimization as a unified problem and solve it by iterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models (7B-70B) show that SSLC, without any additional training steps, consistently surpasses standalone methods, achieving state-of-the-arts results. Notably, SSLC compresses Qwen2.5 by 50\% with no performance drop and achieves at least 1.63$\times$ speedup, offering a practical solution for efficient LLM deployment.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Network Fusion of Large Language Models for Sentiment Analysis</title>
<link>https://arxiv.org/abs/2510.26484</link>
<guid>https://arxiv.org/abs/2510.26484</guid>
<content:encoded><![CDATA[
<div> Bayesian network, LLM, sentiment analysis, fusion, accuracy  
 
Summary:
The article introduces the Bayesian network LLM fusion (BNLF) framework, which addresses challenges faced by large language models (LLMs) such as lack of transparency, high fine-tuning costs, and environmental impact. BNLF integrates predictions from FinBERT, RoBERTa, and BERTweet for sentiment analysis through a probabilistic mechanism. It employs late fusion by modeling sentiment predictions as probabilistic nodes in a Bayesian network. Tested on three financial corpora, BNLF consistently outperforms baseline LLMs by approximately six percent in accuracy, demonstrating its robustness across different datasets and the effectiveness of probabilistic fusion for interpretable sentiment classification. BNLF offers a solution for improving LLM performance and interpretability while mitigating environmental impact, making it a promising framework for sentiment analysis tasks.  

<br /><br /> <div>
arXiv:2510.26484v1 Announce Type: new 
Abstract: Large language models (LLMs) continue to advance, with an increasing number of domain-specific variants tailored for specialised tasks. However, these models often lack transparency and explainability, can be costly to fine-tune, require substantial prompt engineering, yield inconsistent results across domains, and impose significant adverse environmental impact due to their high computational demands. To address these challenges, we propose the Bayesian network LLM fusion (BNLF) framework, which integrates predictions from three LLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic mechanism for sentiment analysis. BNLF performs late fusion by modelling the sentiment predictions from multiple LLMs as probabilistic nodes within a Bayesian network. Evaluated across three human-annotated financial corpora with distinct linguistic and contextual characteristics, BNLF demonstrates consistent gains of about six percent in accuracy over the baseline LLMs, underscoring its robustness to dataset variability and the effectiveness of probabilistic fusion for interpretable sentiment classification.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-agent Large Language Model Framework to Automatically Assess Performance of a Clinical AI Triage Tool</title>
<link>https://arxiv.org/abs/2510.26498</link>
<guid>https://arxiv.org/abs/2510.26498</guid>
<content:encoded><![CDATA[
<div> Ensemble; LLM; AI; Triage tool; Intracranial hemorrhage<br />
Summary:<br />
- Purpose: Study aimed to assess the collective use of multiple LLM agents for evaluating an AI triage tool.<br />
- Methods: Data from 29,766 CT head exams processed by an ICH AI detection tool were analyzed using open-source LLMs and GPT-4o. Performance was compared among models and ensembles.<br />
- Results: The ensemble of LLMs, particularly llama3.3:70b and GPT-4o, showed the highest performance in terms of AUC, average precision, F1 score, recall, precision, specificity, and MCC. Different ensemble combinations had similar performance.<br />
- Conclusion: Medium to large-sized LLM ensembles proved to be more consistent and reliable in evaluating the AI triage tool than a single LLM. No significant differences were observed between top-performing ensembles, highlighting the effectiveness of utilizing multiple LLMs for a comprehensive assessment.<br /> <div>
arXiv:2510.26498v1 Announce Type: new 
Abstract: Purpose: The purpose of this study was to determine if an ensemble of multiple LLM agents could be used collectively to provide a more reliable assessment of a pixel-based AI triage tool than a single LLM.
  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were processed by a commercial intracranial hemorrhage (ICH) AI detection tool. Radiology reports were analyzed by an ensemble of eight open-source LLM models and a HIPAA compliant internal version of GPT-4o using a single multi-shot prompt that assessed for presence of ICH. 1,726 examples were manually reviewed. Performance characteristics of the eight open-source models and consensus were compared to GPT-4o. Three ideal consensus LLM ensembles were tested for rating the performance of the triage tool.
  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The highest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78). The average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76). Llama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater precision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the ideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3 Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522 (0.500-0.543). No statistically significant differences were observed between Top-3, Full-9, and Consensus (p > 0.05).
  Conclusion: An ensemble of medium to large sized open-source LLMs provides a more consistent and reliable method to derive a ground truth retrospective evaluation of a clinical AI triage tool over a single LLM alone.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs</title>
<link>https://arxiv.org/abs/2510.26512</link>
<guid>https://arxiv.org/abs/2510.26512</guid>
<content:encoded><![CDATA[
<div> Keywords: human smuggling networks, automated knowledge graph construction, coreference resolution, structured prompts, LLM-based pipelines<br />
Summary: <br />
This study focuses on the challenges of analyzing human smuggling networks using legal case documents. The researchers introduce the CORE-KG framework, which combines a type-aware coreference module and domain-guided structured prompts to improve automated knowledge graph construction. An ablation study of CORE-KG reveals the individual contributions of coreference resolution and structured prompts. Removing coreference resolution results in increased node duplication and noisy nodes, while removing structured prompts also leads to similar issues. The results highlight the importance of these components in reducing duplicate nodes and legal noise in structured representations extracted from complex legal texts. The findings provide valuable insights for enhancing LLM-based pipelines for analyzing human smuggling networks. <br />  
Summary: <div>
arXiv:2510.26512v1 Announce Type: new 
Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze. Legal case documents offer critical insights but are often unstructured, lexically dense, and filled with ambiguous or shifting references, which pose significant challenges for automated knowledge graph (KG) construction. While recent LLM-based approaches improve over static templates, they still generate noisy, fragmented graphs with duplicate nodes due to the absence of guided extraction and coreference resolution. The recently proposed CORE-KG framework addresses these limitations by integrating a type-aware coreference module and domain-guided structured prompts, significantly reducing node duplication and legal noise. In this work, we present a systematic ablation study of CORE-KG to quantify the individual contributions of its two key components. Our results show that removing coreference resolution results in a 28.32% increase in node duplication and a 4.32% increase in noisy nodes, while removing structured prompts leads to a 4.34% increase in node duplication and a 73.33% increase in noisy nodes. These findings offer empirical insights for designing robust LLM-based pipelines for extracting structured representations from complex legal texts.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hebrew Diacritics Restoration using Visual Representation</title>
<link>https://arxiv.org/abs/2510.26521</link>
<guid>https://arxiv.org/abs/2510.26521</guid>
<content:encoded><![CDATA[
<div> Keywords: Hebrew, diacritics restoration, machine learning, zero-shot classification, visual language model

Summary: 
DIVRIT is a new system for diacritization in Hebrew, treating it as a zero-shot classification problem at the word level. Using a Hebrew Visual Language Model, it embeds diacritic information directly into the inputs vector representation. The system performs diacritization effectively without complex linguistic analysis, achieving high accuracy in an "oracle" setting. Architectural enhancements and optimized training methods improve generalization capabilities. The study demonstrates the potential of visual representations for automated Hebrew diacritization.
<br /><br />Summary: <div>
arXiv:2510.26521v1 Announce Type: new 
Abstract: Diacritics restoration in Hebrew is a fundamental task for ensuring accurate word pronunciation and disambiguating textual meaning. Despite the language's high degree of ambiguity when unvocalized, recent machine learning approaches have significantly advanced performance on this task.
  In this work, we present DIVRIT, a novel system for Hebrew diacritization that frames the task as a zero-shot classification problem. Our approach operates at the word level, selecting the most appropriate diacritization pattern for each undiacritized word from a dynamically generated candidate set, conditioned on the surrounding textual context. A key innovation of DIVRIT is its use of a Hebrew Visual Language Model, which processes undiacritized text as an image, allowing diacritic information to be embedded directly within the input's vector representation.
  Through a comprehensive evaluation across various configurations, we demonstrate that the system effectively performs diacritization without relying on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting where the correct diacritized form is guaranteed to be among the provided candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic architectural enhancements and optimized training methodologies yield significant improvements in the system's overall generalization capabilities. These findings highlight the promising potential of visual representations for accurate and automated Hebrew diacritization.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Structure of Relation Decoding Linear Operators in Large Language Models</title>
<link>https://arxiv.org/abs/2510.26543</link>
<guid>https://arxiv.org/abs/2510.26543</guid>
<content:encoded><![CDATA[
<div> decoder operators, relational facts, transformer language models, tensor networks, semantic properties<br />
<br />
Summary: 
This paper explores the structure of linear operators used to decode relational facts in transformer language models. It extends previous single-relation findings to multiple relations and demonstrates that these decoders can be efficiently compressed using simple order-3 tensor networks without losing decoding accuracy significantly. Through a cross-evaluation protocol, the study reveals that these linear operators do not encode distinct relations but instead extract recurring, coarse-grained semantic properties. The research suggests that the structure of these linear relational decoders is primarily property-based rather than relation-specific, explaining their compressibility and limited generalization to new, semantically similar relations. <div>
arXiv:2510.26543v1 Announce Type: new 
Abstract: This paper investigates the structure of linear operators introduced in Hernandez et al. [2023] that decode specific relational facts in transformer language models. We extend their single-relation findings to a collection of relations and systematically chart their organization. We show that such collections of relation decoders can be highly compressed by simple order-3 tensor networks without significant loss in decoding accuracy. To explain this surprising redundancy, we develop a cross-evaluation protocol, in which we apply each linear decoder operator to the subjects of every other relation. Our results reveal that these linear maps do not encode distinct relations, but extract recurring, coarse-grained semantic properties (e.g., country of capital city and country of food are both in the country-of-X property). This property-centric structure clarifies both the operators' compressibility and highlights why they generalize only to new relations that are semantically close. Our findings thus interpret linear relational decoding in transformer language models as primarily property-based, rather than relation-specific.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoFlow: Reinforcing Search Agent Via Reward Density Optimization</title>
<link>https://arxiv.org/abs/2510.26575</link>
<guid>https://arxiv.org/abs/2510.26575</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Verifiable Rewards, Reward Density Optimization, InfoFlow, Dual-agent refinement
<br />
Summary: 
The paper introduces the concept of Reward Density Optimization in deep search scenarios and proposes the InfoFlow framework to address this challenge. InfoFlow approaches the problem from three angles: subproblem decomposition, failure-guided hints, and dual-agent refinement. By breaking down tasks and providing denser learning signals, InfoFlow improves the reward obtained per unit of exploration cost. Injecting corrective guidance into stalled trajectories increases the likelihood of successful outcomes. The dual-agent architecture offloads cognitive burden, with a refiner agent synthesizing search history to compress the researcher's perceived trajectory, reducing exploration cost and increasing overall reward density. Evaluation on multiple agentic search benchmarks demonstrates the superiority of InfoFlow over strong baselines, enabling lightweight LLMs to achieve performance comparable to advanced proprietary LLMs.
<br /> <div>
arXiv:2510.26575v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach for enhancing agentic deep search. However, its application is often hindered by low \textbf{Reward Density} in deep search scenarios, where agents expend significant exploratory costs for infrequent and often null final rewards. In this paper, we formalize this challenge as the \textbf{Reward Density Optimization} problem, which aims to improve the reward obtained per unit of exploration cost. This paper introduce \textbf{InfoFlow}, a systematic framework that tackles this problem from three aspects. 1) \textbf{Subproblem decomposition}: breaking down long-range tasks to assign process rewards, thereby providing denser learning signals. 2) \textbf{Failure-guided hints}: injecting corrective guidance into stalled trajectories to increase the probability of successful outcomes. 3) \textbf{Dual-agent refinement}: employing a dual-agent architecture to offload the cognitive burden of deep exploration. A refiner agent synthesizes the search history, which effectively compresses the researcher's perceived trajectory, thereby reducing exploration cost and increasing the overall reward density. We evaluate InfoFlow on multiple agentic search benchmarks, where it significantly outperforms strong baselines, enabling lightweight LLMs to achieve performance comparable to advanced proprietary LLMs.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large Language Models</title>
<link>https://arxiv.org/abs/2510.26577</link>
<guid>https://arxiv.org/abs/2510.26577</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Speculative Decoding, Dynamic Tree Decoding, GPU Configurations, Batch Sizes

Summary:
The article introduces a new dynamic tree decoding approach called CAST to address the inference latency challenges faced by Large Language Models (LLMs). Unlike existing methods like EAGLE-2 and EAGLE-3, CAST takes into account crucial system variables such as GPU configurations and batch sizes to dynamically refine the tree structure, resulting in faster decoding speeds. Through experimentation across six different tasks with six distinct LLMs, the methodology shows significant improvements, achieving speeds up to 5.2 times faster than conventional methods. Additionally, CAST generally outperforms existing state-of-the-art techniques by 5% to 20%.
<br /><br />Summary: <div>
arXiv:2510.26577v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face significant inference latency challenges stemming from their autoregressive design and large size. To address this, speculative decoding emerges as a solution, enabling the simultaneous generation and validation of multiple tokens. While recent approaches like EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures, they often neglect the impact of crucial system variables such as GPU devices and batch sizes.
  Therefore, we introduce a new dynamic tree decoding approach called CAST that takes into account inference costs, including factors such as GPU configurations and batch sizes, to dynamically refine the tree structure. Through comprehensive experimentation across six diverse tasks and utilizing six distinct LLMs, our methodology demonstrates remarkable results, achieving speeds up to 5.2 times faster than conventional decoding methods. Moreover, it generally outperforms existing state-of-the-art techniques from 5% to 20%.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual Document Understanding</title>
<link>https://arxiv.org/abs/2510.26615</link>
<guid>https://arxiv.org/abs/2510.26615</guid>
<content:encoded><![CDATA[
arXiv:2510.26615v1 Announce Type: new 
Abstract: Multi-page visual documents such as manuals, brochures, presentations, and posters convey key information through layout, colors, icons, and cross-slide references. While large language models (LLMs) offer opportunities in document understanding, current systems struggle with complex, multi-page visual documents, particularly in fine-grained reasoning over elements and pages. We introduce SlideAgent, a versatile agentic framework for understanding multi-modal, multi-page, and multi-layout documents, especially slide decks. SlideAgent employs specialized agents and decomposes reasoning into three specialized levels-global, page, and element-to construct a structured, query-agnostic representation that captures both overarching themes and detailed visual or textual cues. During inference, SlideAgent selectively activates specialized agents for multi-level reasoning and integrates their outputs into coherent, context-aware answers. Extensive experiments show that SlideAgent achieves significant improvement over both proprietary (+7.9 overall) and open-source models (+9.8 overall).
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large Language Model</title>
<link>https://arxiv.org/abs/2510.26622</link>
<guid>https://arxiv.org/abs/2510.26622</guid>
<content:encoded><![CDATA[
arXiv:2510.26622v1 Announce Type: new 
Abstract: Recent large language model (LLM) research has undergone an architectural shift from encoder-decoder modeling to nowadays the dominant decoder-only modeling. This rapid transition, however, comes without a rigorous comparative analysis especially \textit{from the scaling perspective}, raising concerns that the potential of encoder-decoder models may have been overlooked. To fill this gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent recipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison between RedLLM, pretrained with prefix language modeling (LM), and DecLLM, pretrained with causal LM, at different model scales, ranging from $\sim$150M to $\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for instruction tuning, our experiments show that RedLLM produces compelling scaling properties and surprisingly strong performance. While DecLLM is overall more compute-optimal during pretraining, RedLLM demonstrates comparable scaling and context length extrapolation capabilities. After instruction tuning, RedLLM achieves comparable and even better results on various downstream tasks while enjoying substantially better inference efficiency. We hope our findings could inspire more efforts on re-examining RedLLM, unlocking its potential for developing powerful and efficient LLMs.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models</title>
<link>https://arxiv.org/abs/2510.26683</link>
<guid>https://arxiv.org/abs/2510.26683</guid>
<content:encoded><![CDATA[
arXiv:2510.26683v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated exceptional capabilities across multiple domains by leveraging massive pre-training and curated fine-tuning data. However, in data-sensitive fields such as healthcare, the lack of high-quality, domain-specific training corpus hinders LLMs' adaptation for specialized applications. Meanwhile, domain experts have distilled domain wisdom into ontology rules, which formalize relationships among concepts and ensure the integrity of knowledge management repositories. Viewing LLMs as implicit repositories of human knowledge, we propose Evontree, a novel framework that leverages a small set of high-quality ontology rules to systematically extract, validate, and enhance domain knowledge within LLMs, without requiring extensive external datasets. Specifically, Evontree extracts domain ontology from raw models, detects inconsistencies using two core ontology rules, and reinforces the refined knowledge via self-distilled fine-tuning. Extensive experiments on medical QA benchmarks with Llama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both unmodified models and leading supervised baselines, achieving up to a 3.7% improvement in accuracy. These results confirm the effectiveness, efficiency, and robustness of our approach for low-resource domain adaptation of LLMs.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kimi Linear: An Expressive, Efficient Attention Architecture</title>
<link>https://arxiv.org/abs/2510.26692</link>
<guid>https://arxiv.org/abs/2510.26692</guid>
<content:encoded><![CDATA[
arXiv:2510.26692v1 Announce Type: new 
Abstract: We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.
  We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.
  To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The End of Manual Decoding: Towards Truly End-to-End Language Models</title>
<link>https://arxiv.org/abs/2510.26697</link>
<guid>https://arxiv.org/abs/2510.26697</guid>
<content:encoded><![CDATA[
arXiv:2510.26697v1 Announce Type: new 
Abstract: The "end-to-end" label for LLMs is a misnomer. In practice, they depend on a non-differentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, a novel architecture that enables truly "end-to-end" generation by learning to control its own decoding strategy. We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits. This approach transforms decoding into a parametric, token-level process, allowing the model to self-regulate its sampling strategy within a single forward pass.
  Through extensive experiments on eight benchmarks, we demonstrate that AutoDeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracle-tuned baseline derived from "hacking the test set"-a practical upper bound for any static method. Crucially, we uncover an emergent capability for instruction-based decoding control: the model learns to interpret natural language commands (e.g., "generate with low randomness") and adjusts its predicted temperature and top-p on a token-by-token basis, opening a new paradigm for steerable and interactive LLM decoding.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Value Drifts: Tracing Value Alignment During LLM Post-Training</title>
<link>https://arxiv.org/abs/2510.26707</link>
<guid>https://arxiv.org/abs/2510.26707</guid>
<content:encoded><![CDATA[
arXiv:2510.26707v1 Announce Type: new 
Abstract: As LLMs occupy an increasingly important role in society, they are more and more confronted with questions that require them not only to draw on their general knowledge but also to align with certain human value systems. Therefore, studying the alignment of LLMs with human values has become a crucial field of inquiry. Prior work, however, mostly focuses on evaluating the alignment of fully trained models, overlooking the training dynamics by which models learn to express human values. In this work, we investigate how and at which stage value alignment arises during the course of a model's post-training. Our analysis disentangles the effects of post-training algorithms and datasets, measuring both the magnitude and time of value drifts during training. Experimenting with Llama-3 and Qwen-3 models of different sizes and popular supervised fine-tuning (SFT) and preference optimization datasets and algorithms, we find that the SFT phase generally establishes a model's values, and subsequent preference optimization rarely re-aligns these values. Furthermore, using a synthetic preference dataset that enables controlled manipulation of values, we find that different preference optimization algorithms lead to different value alignment outcomes, even when preference data is held constant. Our findings provide actionable insights into how values are learned during post-training and help to inform data curation, as well as the selection of models and algorithms for preference optimization to improve model alignment to human values.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMO-Bench: Large Language Models Still Struggle in High School Math Competitions</title>
<link>https://arxiv.org/abs/2510.26768</link>
<guid>https://arxiv.org/abs/2510.26768</guid>
<content:encoded><![CDATA[
arXiv:2510.26768v1 Announce Type: new 
Abstract: We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO-Bench to facilitate further research into advancing the reasoning abilities of language models. https://amo-bench.github.io/
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gistify! Codebase-Level Understanding via Runtime Execution</title>
<link>https://arxiv.org/abs/2510.26790</link>
<guid>https://arxiv.org/abs/2510.26790</guid>
<content:encoded><![CDATA[
arXiv:2510.26790v1 Announce Type: new 
Abstract: As coding agents are increasingly deployed in large codebases, the need to automatically design challenging, codebase-level evaluation is central. We propose Gistify, a task where a coding LLM must create a single, minimal, self-contained file that can reproduce a specific functionality of a codebase. The coding LLM is given full access to a codebase along with a specific entrypoint (e.g., a python command), and the generated file must replicate the output of the same command ran under the full codebase, while containing only the essential components necessary to execute the provided command. Success on Gistify requires both structural understanding of the codebase, accurate modeling of its execution flow as well as the ability to produce potentially large code patches. Our findings show that current state-of-the-art models struggle to reliably solve Gistify tasks, especially ones with long executions traces.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks</title>
<link>https://arxiv.org/abs/2510.25797</link>
<guid>https://arxiv.org/abs/2510.25797</guid>
<content:encoded><![CDATA[
arXiv:2510.25797v1 Announce Type: cross 
Abstract: This study examines the effectiveness of spatio-temporal modeling and the integration of spatial attention mechanisms in deep learning models for underwater object detection. Specifically, in the first phase, the performance of temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with the standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is developed, through the addition of a Convolutional Block Attention Module (CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and T-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the research highlights how temporal modeling improves detection accuracy in dynamic marine environments, particularly under conditions of sudden movements, partial occlusions, and gradual motion. The testing results showed that YOLOv5 achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM outperformed with mAP@50-95 scores of 0.813 and 0.811, respectively, highlighting their superior accuracy and generalization in detecting complex objects. The findings demonstrate that T-YOLOv5 significantly enhances detection reliability compared to the standard model, while T-YOLOv5 with CBAM further improves performance in challenging scenarios, although there is a loss of accuracy when it comes to simpler scenarios.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemEIC: A Step Toward Continual and Compositional Knowledge Editing</title>
<link>https://arxiv.org/abs/2510.25798</link>
<guid>https://arxiv.org/abs/2510.25798</guid>
<content:encoded><![CDATA[
arXiv:2510.25798v1 Announce Type: cross 
Abstract: The dynamic nature of information necessitates continuously updating large vision-language models (LVLMs). While recent knowledge editing techniques hint at promising directions, they often focus on editing a single modality (vision or language) in isolation. This prevalent practice neglects the inherent multimodality of LVLMs and the continuous nature of knowledge updates, potentially leading to suboptimal editing outcomes when considering the interplay between modalities and the need for ongoing knowledge refinement. To address these limitations, we propose MemEIC, a novel method for Continual and Compositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional editing of both visual and textual knowledge sequentially. Our approach employs a hybrid external-internal editor featuring a dual external memory for cross-modal evidence retrieval and dual LoRA adapters that facilitate disentangled parameter updates for each modality. A key component is a brain-inspired knowledge connector, activated selectively for compositional reasoning, that integrates information across different modalities. Experiments demonstrate that MemEIC significantly improves performance on complex multimodal questions and effectively preserves prior edits, setting a new benchmark for CCKE in LVLMs.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start</title>
<link>https://arxiv.org/abs/2510.25801</link>
<guid>https://arxiv.org/abs/2510.25801</guid>
<content:encoded><![CDATA[
arXiv:2510.25801v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) with verifiable rewards has recently catalyzed a wave of "MLLM-r1" approaches that bring RL to vision language models. Most representative paradigms begin with a cold start, typically employing supervised fine-tuning (SFT), to initialize the policy before RL. However, SFT-based cold start adopts the reasoning paradigm intertwined with task solution and output format, which may induce instruction-style overfitting, weakens out-of-distribution generalization, and ultimately affects downstream RL. We revisit the cold start along two views, its training method and data construction, and introduce the Generalization Factor (GF) coefficient to quantify the generalization capability under different methods. Our empirical study finds that preference-based training methods (e.g. DPO) generalizes better than SFT-based methods in cold start. Motivated by this, we propose SPECS-a Self-distilled, Preference-based Cold Start framework that decouples multimodal learning: (1) generates introspective preference data pairs via self-distillation, avoiding reliance on larger teachers or manual annotation; (2) performs preference-based training to learn, focusing on shallow, transferable surface-form criteria (format, structure, style) rather than memorizing content; and (3) hands off to RL with verifiable rewards for deep reasoning results. Experimental results across multiple multimodal benchmarks show that our decoupling learning framework yields consistent performance gains over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%. Additional experiments indicate that SPECS contributes to reducing in-distribution "stuckness," improving exploration, stabilizing training, and raising the performance ceiling.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability of LLM Raters</title>
<link>https://arxiv.org/abs/2510.25860</link>
<guid>https://arxiv.org/abs/2510.25860</guid>
<content:encoded><![CDATA[
arXiv:2510.25860v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used as raters for evaluation tasks. However, their reliability is often limited for subjective tasks, when human judgments involve subtle reasoning beyond annotation labels. Thinking traces, the reasoning behind a judgment, are highly informative but challenging to collect and curate. We present a human-LLM collaborative framework to infer thinking traces from label-only annotations. The proposed framework uses a simple and effective rejection sampling method to reconstruct these traces at scale. These inferred thinking traces are applied to two complementary tasks: (1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation guidelines for proprietary LLM raters. Across multiple datasets, our methods lead to significantly improved LLM-human agreement. Additionally, the refined annotation guidelines increase agreement among different LLM models. These results suggest that LLMs can serve as practical proxies for otherwise unrevealed human thinking traces, enabling label-only corpora to be extended into thinking-trace-augmented resources that enhance the reliability of LLM raters.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximating Human Preferences Using a Multi-Judge Learned System</title>
<link>https://arxiv.org/abs/2510.25884</link>
<guid>https://arxiv.org/abs/2510.25884</guid>
<content:encoded><![CDATA[
arXiv:2510.25884v1 Announce Type: cross 
Abstract: Aligning LLM-based judges with human preferences is a significant challenge, as they are difficult to calibrate and often suffer from rubric sensitivity, bias, and instability. Overcoming this challenge advances key applications, such as creating reliable reward models for Reinforcement Learning from Human Feedback (RLHF) and building effective routing systems that select the best-suited model for a given user query. In this work, we propose a framework for modeling diverse, persona-based preferences by learning to aggregate outputs from multiple rubric-conditioned judges. We investigate the performance of this approach against naive baselines and assess its robustness through case studies on both human and LLM-judges biases. Our primary contributions include a persona-based method for synthesizing preference labels at scale and two distinct implementations of our aggregator: Generalized Additive Model (GAM) and a Multi-Layer Perceptron (MLP).
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for Facebook and X</title>
<link>https://arxiv.org/abs/2510.25932</link>
<guid>https://arxiv.org/abs/2510.25932</guid>
<content:encoded><![CDATA[
arXiv:2510.25932v1 Announce Type: cross 
Abstract: Social platforms distribute information at unprecedented speed, which in turn accelerates the spread of misinformation and threatens public discourse. We present FakeZero, a fully client-side, cross-platform browser extension that flags unreliable posts on Facebook and X (formerly Twitter) while the user scrolls. All computation, DOM scraping, tokenisation, Transformer inference, and UI rendering run locally through the Chromium messaging API, so no personal data leaves the device.FakeZero employs a three-stage training curriculum: baseline fine-tuning and domain-adaptive training enhanced with focal loss, adversarial augmentation, and post-training quantisation. Evaluated on a dataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1% macro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of approximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant variant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to 14.7 MB and lowering latency to approximately 40 ms, showing that high-quality fake-news detection is feasible under tight resource budgets with only modest performance loss.By providing inline credibility cues, the extension can serve as a valuable tool for policymakers seeking to curb the spread of misinformation across social networks. With user consent, FakeZero also opens the door for researchers to collect large-scale datasets of fake news in the wild, enabling deeper analysis and the development of more robust detection techniques.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments</title>
<link>https://arxiv.org/abs/2510.26006</link>
<guid>https://arxiv.org/abs/2510.26006</guid>
<content:encoded><![CDATA[
arXiv:2510.26006v1 Announce Type: cross 
Abstract: Humans can naturally identify, reason about, and explain anomalies in their environment. In computer vision, this long-standing challenge remains limited to industrial defects or unrealistic, synthetically generated anomalies, failing to capture the richness and unpredictability of real-world anomalies. In this work, we introduce CAVE, the first benchmark of real-world visual anomalies. CAVE supports three open-ended tasks: anomaly description, explanation, and justification; with fine-grained annotations for visual grounding and categorizing anomalies based on their visual manifestations, their complexity, severity, and commonness. These annotations draw inspiration from cognitive science research on how humans identify and resolve anomalies, providing a comprehensive framework for evaluating Vision-Language Models (VLMs) in detecting and understanding anomalies. We show that state-of-the-art VLMs struggle with visual anomaly perception and commonsense reasoning, even with advanced prompting strategies. By offering a realistic and cognitively grounded benchmark, CAVE serves as a valuable resource for advancing research in anomaly detection and commonsense reasoning in VLMs.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled Structured Reasoning</title>
<link>https://arxiv.org/abs/2510.26037</link>
<guid>https://arxiv.org/abs/2510.26037</guid>
<content:encoded><![CDATA[
arXiv:2510.26037v1 Announce Type: cross 
Abstract: The ability of LLM agents to plan and invoke tools exposes them to new safety risks, making a comprehensive red-teaming system crucial for discovering vulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic red-teaming framework for arbitrary black-box LLM agents. We employ a dynamic two-step process that starts with an agent definition and generates diverse seed test cases that cover various risk outcomes, tool-use trajectories, and risk sources. Then, it iteratively constructs and refines model-based adversarial attacks based on the execution trajectories of former attempts. To optimize the red-teaming cost, we present a model distillation approach that leverages structured forms of a teacher model's reasoning to train smaller models that are equally effective. Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer model improves attack success rate by 100%, surpassing the 671B Deepseek-R1 model. Our ablations and analyses validate the effectiveness of the iterative framework, structured reasoning, and the generalization of our red-teamer models.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods</title>
<link>https://arxiv.org/abs/2510.26038</link>
<guid>https://arxiv.org/abs/2510.26038</guid>
<content:encoded><![CDATA[
arXiv:2510.26038v1 Announce Type: cross 
Abstract: Knowledge distillation (KD) is an effective method for model compression and transferring knowledge between models. However, its effect on model's robustness against spurious correlations that degrade performance on out-of-distribution data remains underexplored. This study investigates the effect of knowledge distillation on the transferability of ``debiasing'' capabilities from teacher models to student models on natural language inference (NLI) and image classification tasks. Through extensive experiments, we illustrate several key findings: (i) overall the debiasing capability of a model is undermined post-KD; (ii) training a debiased model does not benefit from injecting teacher knowledge; (iii) although the overall robustness of a model may remain stable post-distillation, significant variations can occur across different types of biases; and (iv) we pin-point the internal attention pattern and circuit that causes the distinct behavior post-KD. Given the above findings, we propose three effective solutions to improve the distillability of debiasing methods: developing high quality data for augmentation, implementing iterative knowledge distillation, and initializing student models with weights obtained from teacher models. To the best of our knowledge, this is the first study on the effect of KD on debiasing and its interenal mechanism at scale. Our findings provide understandings on how KD works and how to design better debiasing methods.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORBIT - Open Recommendation Benchmark for Reproducible Research with Hidden Tests</title>
<link>https://arxiv.org/abs/2510.26095</link>
<guid>https://arxiv.org/abs/2510.26095</guid>
<content:encoded><![CDATA[
arXiv:2510.26095v1 Announce Type: cross 
Abstract: Recommender systems are among the most impactful AI applications, interacting with billions of users every day, guiding them to relevant products, services, or information tailored to their preferences. However, the research and development of recommender systems are hindered by existing datasets that fail to capture realistic user behaviors and inconsistent evaluation settings that lead to ambiguous conclusions. This paper introduces the Open Recommendation Benchmark for Reproducible Research with HIdden Tests (ORBIT), a unified benchmark for consistent and realistic evaluation of recommendation models. ORBIT offers a standardized evaluation framework of public datasets with reproducible splits and transparent settings for its public leaderboard. Additionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco, featuring web browsing sequences from 87 million public, high-quality webpages. ClueWeb-Reco is a synthetic dataset derived from real, user-consented, and privacy-guaranteed browsing data. It aligns with modern recommendation scenarios and is reserved as the hidden test part of our leaderboard to challenge recommendation models' generalization ability. ORBIT measures 12 representative recommendation models on its public benchmark and introduces a prompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results reflect general improvements of recommender systems on the public datasets, with variable individual performances. The results on the hidden test reveal the limitations of existing approaches in large-scale webpage recommendation and highlight the potential for improvements with LLM integrations. ORBIT benchmark, leaderboard, and codebase are available at https://www.open-reco-bench.ai.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math</title>
<link>https://arxiv.org/abs/2510.26143</link>
<guid>https://arxiv.org/abs/2510.26143</guid>
<content:encoded><![CDATA[
arXiv:2510.26143v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) can elicit strong reasoning in large language models (LLMs), yet most open efforts focus on math and code. We propose Reasoning Curriculum, a simple two-stage curriculum that first elicits reasoning skills in pretraining-aligned domains such as math, then adapts and refines these skills across other domains via joint RL. Stage 1 performs a brief cold start and then math-only RL with verifiable rewards to develop reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and consolidate these skills. The curriculum is minimal and backbone-agnostic, requiring no specialized reward models beyond standard verifiability checks. Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning curriculum yields consistent gains. Ablations and a cognitive-skill analysis indicate that both stages are necessary and that math-first elicitation increases cognitive behaviors important for solving complex problems. Reasoning Curriculum provides a compact, easy-to-adopt recipe for general reasoning.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning</title>
<link>https://arxiv.org/abs/2510.26167</link>
<guid>https://arxiv.org/abs/2510.26167</guid>
<content:encoded><![CDATA[
arXiv:2510.26167v1 Announce Type: cross 
Abstract: Reward models (RMs) play a critical role in aligning large language models (LLMs) with human preferences. Yet in the domain of tool learning, the lack of RMs specifically designed for function-calling tasks has limited progress toward more capable agentic AI. We introduce ToolRM, a family of lightweight generative RMs tailored for general tool-use scenarios. To build these models, we propose a novel pipeline that constructs pairwise preference data using rule-based scoring and multidimensional sampling. This yields ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique tasks that supports reinforcement learning with verifiable feedback. To evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on the agentic evaluation suite BFCL. Trained on our constructed data, models from the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward judgments. Beyond training objectives, ToolRM generalizes to broader critique tasks, including Best-of-N sampling and self-correction. Experiments on ACEBench highlight its effectiveness and efficiency, enabling inference-time scaling and reducing output token usage by over 66%. We release data and model checkpoints to facilitate future research.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SP-MCQA: Evaluating Intelligibility of TTS Beyond the Word Level</title>
<link>https://arxiv.org/abs/2510.26190</link>
<guid>https://arxiv.org/abs/2510.26190</guid>
<content:encoded><![CDATA[
arXiv:2510.26190v1 Announce Type: cross 
Abstract: The evaluation of intelligibility for TTS has reached a bottleneck, as existing assessments heavily rely on word-by-word accuracy metrics such as WER, which fail to capture the complexity of real-world speech or reflect human comprehension needs. To address this, we propose Spoken-Passage Multiple-Choice Question Answering, a novel subjective approach evaluating the accuracy of key information in synthesized speech, and release SP-MCQA-Eval, an 8.76-hour news-style benchmark dataset for SP-MCQA evaluation. Our experiments reveal that low WER does not necessarily guarantee high key-information accuracy, exposing a gap between traditional metrics and practical intelligibility. SP-MCQA shows that even state-of-the-art (SOTA) models still lack robust text normalization and phonetic accuracy. This work underscores the urgent need for high-level, more life-like evaluation criteria now that many systems already excel at WER yet may fall short on real-world intelligibility.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.26241</link>
<guid>https://arxiv.org/abs/2510.26241</guid>
<content:encoded><![CDATA[
arXiv:2510.26241v1 Announce Type: cross 
Abstract: Modern vision-language models (VLMs) excel at many multimodal tasks, yet their grasp of temporal information in video remains weak and, crucially, under-evaluated. We probe this gap with a deceptively simple but revealing challenge: judging the arrow of time (AoT)-whether a short clip is played forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated benchmark that tests whether VLMs can infer temporal direction in natural videos using the same stimuli and behavioral baselines established for humans. Our comprehensive evaluation of open-weight and proprietary, reasoning and non-reasoning VLMs reveals that most models perform near chance, and even the best lag far behind human accuracy on physically irreversible processes (e.g., free fall, diffusion/explosion) and causal manual actions (division/addition) that humans recognize almost instantly. These results highlight a fundamental gap in current multimodal systems: while they capture rich visual-semantic correlations, they lack the inductive biases required for temporal continuity and causal understanding. We release the code and data for AoT-PsyPhyBENCH to encourage further progress in the physical and temporal reasoning capabilities of VLMs.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PVMark: Enabling Public Verifiability for LLM Watermarking Schemes</title>
<link>https://arxiv.org/abs/2510.26274</link>
<guid>https://arxiv.org/abs/2510.26274</guid>
<content:encoded><![CDATA[
arXiv:2510.26274v1 Announce Type: cross 
Abstract: Watermarking schemes for large language models (LLMs) have been proposed to identify the source of the generated text, mitigating the potential threats emerged from model theft. However, current watermarking solutions hardly resolve the trust issue: the non-public watermark detection cannot prove itself faithfully conducting the detection. We observe that it is attributed to the secret key mostly used in the watermark detection -- it cannot be public, or the adversary may launch removal attacks provided the key; nor can it be private, or the watermarking detection is opaque to the public. To resolve the dilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP), enabling the watermark detection process to be publicly verifiable by third parties without disclosing any secret key. PVMark hinges upon the proof of `correct execution' of watermark detection on which a set of ZKP constraints are built, including mapping, random number generation, comparison, and summation. We implement multiple variants of PVMark in Python, Rust and Circom, covering combinations of three watermarking schemes, three hash functions, and four ZKP protocols, to show our approach effectively works under a variety of circumstances. By experimental results, PVMark efficiently enables public verifiability on the state-of-the-art LLM watermarking schemes yet without compromising the watermarking performance, promising to be deployed in practice.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis</title>
<link>https://arxiv.org/abs/2510.26423</link>
<guid>https://arxiv.org/abs/2510.26423</guid>
<content:encoded><![CDATA[
arXiv:2510.26423v1 Announce Type: cross 
Abstract: Test oracle generation in non-regression testing is a longstanding challenge in software engineering, where the goal is to produce oracles that can accurately determine whether a function under test (FUT) behaves as intended for a given input. In this paper, we introduce Nexus, a novel multi-agent framework to address this challenge. Nexus generates test oracles by leveraging a diverse set of specialized agents that synthesize test oracles through a structured process of deliberation, validation, and iterative self-refinement. During the deliberation phase, a panel of four specialist agents, each embodying a distinct testing philosophy, collaboratively critiques and refines an initial set of test oracles. Then, in the validation phase, Nexus generates a plausible candidate implementation of the FUT and executes the proposed oracles against it in a secure sandbox. For any oracle that fails this execution-based check, Nexus activates an automated selfrefinement loop, using the specific runtime error to debug and correct the oracle before re-validation. Our extensive evaluation on seven diverse benchmarks demonstrates that Nexus consistently and substantially outperforms state-of-theart baselines. For instance, Nexus improves the test-level oracle accuracy on the LiveCodeBench from 46.30% to 57.73% for GPT-4.1-Mini. The improved accuracy also significantly enhances downstream tasks: the bug detection rate of GPT4.1-Mini generated test oracles on HumanEval increases from 90.91% to 95.45% for Nexus compared to baselines, and the success rate of automated program repair improves from 35.23% to 69.32%.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning</title>
<link>https://arxiv.org/abs/2510.26457</link>
<guid>https://arxiv.org/abs/2510.26457</guid>
<content:encoded><![CDATA[
arXiv:2510.26457v1 Announce Type: cross 
Abstract: Identifying and addressing security issues during the early phase of the development lifecycle is critical for mitigating the long-term negative impacts on software systems. Code review serves as an effective practice that enables developers to check their teammates' code before integration into the codebase. To streamline the generation of review comments, various automated code review approaches have been proposed, where LLM-based methods have significantly advanced the capabilities of automated review generation. However, existing models primarily focus on general-purpose code review, their effectiveness in identifying and addressing security-related issues remains underexplored. Moreover, adapting existing code review approaches to target security issues faces substantial challenges, including data scarcity and inadequate evaluation metrics. To address these limitations, we propose SecureReviewer, a new approach designed for enhancing LLMs' ability to identify and resolve security-related issues during code review. Specifically, we first construct a dataset tailored for training and evaluating secure code review capabilities. Leveraging this dataset, we fine-tune LLMs to generate code review comments that can effectively identify security issues and provide fix suggestions with our proposed secure-aware fine-tuning strategy. To mitigate hallucination in LLMs and enhance the reliability of their outputs, we integrate the RAG technique, which grounds the generated comments in domain-specific security knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric designed to assess the effectiveness of review comments in addressing security issues. Experimental results demonstrate that SecureReviewer outperforms state-of-the-art baselines in both security issue detection accuracy and the overall quality and practical utility of generated review comments.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing</title>
<link>https://arxiv.org/abs/2510.26474</link>
<guid>https://arxiv.org/abs/2510.26474</guid>
<content:encoded><![CDATA[
arXiv:2510.26474v1 Announce Type: cross 
Abstract: Self-improvement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large vision-language models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify a critical issue during this process: the model excels at generating high-quality trajectories for simple queries (i.e., head data) but struggles with more complex ones (i.e., tail data). This leads to an imbalanced optimization that drives the model to prioritize simple reasoning skills, while hindering its ability to tackle more complex reasoning tasks. Over iterations, this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew effect"--which ultimately hinders further model improvement and leads to performance bottlenecks. To counteract this challenge, we introduce four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, to achieve head-tail re-balancing during the exploration-and-learning self-improvement process. Extensive experiments on Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks demonstrate that our methods consistently improve visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Engineering 2.0: The Context of Context Engineering</title>
<link>https://arxiv.org/abs/2510.26493</link>
<guid>https://arxiv.org/abs/2510.26493</guid>
<content:encoded><![CDATA[
arXiv:2510.26493v1 Announce Type: cross 
Abstract: Karl Marx once wrote that ``the human essence is the ensemble of social relations'', suggesting that individuals are not isolated entities but are fundamentally shaped by their interactions with other entities, within which contexts play a constitutive and essential role. With the advent of computers and artificial intelligence, these contexts are no longer limited to purely human--human interactions: human--machine interactions are included as well. Then a central question emerges: How can machines better understand our situations and purposes? To address this challenge, researchers have recently introduced the concept of context engineering. Although it is often regarded as a recent innovation of the agent era, we argue that related practices can be traced back more than twenty years. Since the early 1990s, the field has evolved through distinct historical phases, each shaped by the intelligence level of machines: from early human--computer interaction frameworks built around primitive computers, to today's human--agent interaction paradigms driven by intelligent agents, and potentially to human--level or superhuman intelligence in the future. In this paper, we situate context engineering, provide a systematic definition, outline its historical and conceptual landscape, and examine key design considerations for practice. By addressing these questions, we aim to offer a conceptual foundation for context engineering and sketch its promising future. This paper is a stepping stone for a broader community effort toward systematic context engineering in AI systems.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for Real-world Database Exploration</title>
<link>https://arxiv.org/abs/2510.26495</link>
<guid>https://arxiv.org/abs/2510.26495</guid>
<content:encoded><![CDATA[
arXiv:2510.26495v1 Announce Type: cross 
Abstract: Recent advances in Text-to-SQL have achieved strong results in static, single-turn tasks, where models generate SQL queries from natural language questions. However, these systems fall short in real-world interactive scenarios, where user intents evolve and queries must be refined over multiple turns. In applications such as finance and business analytics, users iteratively adjust query constraints or dimensions based on intermediate results. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a benchmark assessing model performance under evolving user interactions. Unlike previous manually curated datasets, DySQL-Bench is built through an automated two-stage pipeline of task synthesis and verification. Structured tree representations derived from raw database tables guide LLM-based task generation, followed by interaction-oriented filtering and expert validation. Human evaluation confirms 100% correctness of the synthesized data. We further propose a multi-turn evaluation framework simulating realistic interactions among an LLM-simulated user, the model under test, and an executable database. The model must adapt its reasoning and SQL generation as user intents change. DySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling 1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the Pass@5 metric, underscoring the benchmark's difficulty. All code and data are released at https://github.com/Aurora-slz/Real-World-SQL-Bench .
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives</title>
<link>https://arxiv.org/abs/2510.26606</link>
<guid>https://arxiv.org/abs/2510.26606</guid>
<content:encoded><![CDATA[
arXiv:2510.26606v1 Announce Type: cross 
Abstract: Normative reasoning is a type of reasoning that involves normative or deontic modality, such as obligation and permission. While large language models (LLMs) have demonstrated remarkable performance across various reasoning tasks, their ability to handle normative reasoning remains underexplored. In this paper, we systematically evaluate LLMs' reasoning capabilities in the normative domain from both logical and modal perspectives. Specifically, to assess how well LLMs reason with normative modals, we make a comparison between their reasoning with normative modals and their reasoning with epistemic modals, which share a common formal structure. To this end, we introduce a new dataset covering a wide range of formal patterns of reasoning in both normative and epistemic domains, while also incorporating non-formal cognitive factors that influence human reasoning. Our results indicate that, although LLMs generally adhere to valid reasoning patterns, they exhibit notable inconsistencies in specific types of normative reasoning and display cognitive biases similar to those observed in psychological studies of human reasoning. These findings highlight challenges in achieving logical consistency in LLMs' normative reasoning and provide insights for enhancing their reliability. All data and code are released publicly at https://github.com/kmineshima/NeuBAROCO.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Era of Agentic Organization: Learning to Organize with Language Models</title>
<link>https://arxiv.org/abs/2510.26658</link>
<guid>https://arxiv.org/abs/2510.26658</guid>
<content:encoded><![CDATA[
arXiv:2510.26658v1 Announce Type: cross 
Abstract: We envision a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, we introduce asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large language models, which organizes the internal thinking process into concurrently executable structures. Specifically, we propose a thinking protocol where an organizer dynamically assigns sub-queries to workers, merges intermediate knowledge, and produces coherent solutions. More importantly, the thinking structure in this protocol can be further optimized through reinforcement learning. Experiments demonstrate that AsyncThink achieves 28% lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning. Moreover, AsyncThink generalizes its learned asynchronous thinking capabilities, effectively tackling unseen tasks without additional training.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models</title>
<link>https://arxiv.org/abs/2510.26732</link>
<guid>https://arxiv.org/abs/2510.26732</guid>
<content:encoded><![CDATA[
arXiv:2510.26732v1 Announce Type: cross 
Abstract: This paper presents a comprehensive cross-platform evaluation of reasoning capabilities in contemporary foundation models, establishing an infrastructure-agnostic benchmark across three computational paradigms: HPC supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and university clusters (a node with eight H200 GPUs).
  We evaluate 15 foundation models across 79 problems spanning eight academic domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics, Calculus, and Optimization) through three experimental phases: (1) Baseline establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b, Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing methodology and reference performance; (2) Infrastructure validation: The 19-problem benchmark repeated on university cluster (seven models including Falcon-Mamba state-space architecture) and Nebius AI Studio (nine state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3 30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic reproducibility; (3) Extended evaluation: Full 79-problem assessment on both university cluster and Nebius platforms, probing generalization at scale across architectural diversity.
  The findings challenge conventional scaling assumptions, establish training data quality as more critical than model size, and provide actionable guidelines for model selection across educational, production, and research contexts. The tri-infrastructure methodology and 79-problem benchmark enable longitudinal tracking of reasoning capabilities as foundation models evolve.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep sequence models tend to memorize geometrically; it is unclear why</title>
<link>https://arxiv.org/abs/2510.26745</link>
<guid>https://arxiv.org/abs/2510.26745</guid>
<content:encoded><![CDATA[
arXiv:2510.26745v1 Announce Type: cross 
Abstract: In sequence modeling, the parametric memory of atomic facts has been predominantly abstracted as a brute-force lookup of co-occurrences between entities. We contrast this associative view against a geometric view of how memory is stored. We begin by isolating a clean and analyzable instance of Transformer reasoning that is incompatible with memory as strictly a storage of the local co-occurrences specified during training. Instead, the model must have somehow synthesized its own geometry of atomic facts, encoding global relationships between all entities, including non-co-occurring ones. This in turn has simplified a hard reasoning task involving an $\ell$-fold composition into an easy-to-learn 1-step geometric task.
  From this phenomenon, we extract fundamental aspects of neural embedding geometries that are hard to explain. We argue that the rise of such a geometry, despite optimizing over mere local associations, cannot be straightforwardly attributed to typical architectural or optimizational pressures. Counterintuitively, an elegant geometry is learned even when it is not more succinct than a brute-force lookup of associations.
  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry stems from a spectral bias that -- in contrast to prevailing theories -- indeed arises naturally despite the lack of various pressures. This analysis also points to practitioners a visible headroom to make Transformer memory more strongly geometric. We hope the geometric view of parametric memory encourages revisiting the default intuitions that guide researchers in areas like knowledge acquisition, capacity, discovery and unlearning.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Remote Labor Index: Measuring AI Automation of Remote Work</title>
<link>https://arxiv.org/abs/2510.26787</link>
<guid>https://arxiv.org/abs/2510.26787</guid>
<content:encoded><![CDATA[
arXiv:2510.26787v1 Announce Type: cross 
Abstract: AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defeating the Training-Inference Mismatch via FP16</title>
<link>https://arxiv.org/abs/2510.26788</link>
<guid>https://arxiv.org/abs/2510.26788</guid>
<content:encoded><![CDATA[
arXiv:2510.26788v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, we show that its root cause lies in the floating point precision itself. The widely adopted BF16, despite its large dynamic range, introduces large rounding errors that breaks the consistency between training and inference. In this work, we demonstrate that simply reverting to \textbf{FP16} effectively eliminates this mismatch. The change is simple, fully supported by modern frameworks with only a few lines of code change, and requires no modification to the model architecture or learning algorithm. Our results suggest that using FP16 uniformly yields more stable optimization, faster convergence, and stronger performance across diverse tasks, algorithms and frameworks. We hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark</title>
<link>https://arxiv.org/abs/2510.26802</link>
<guid>https://arxiv.org/abs/2510.26802</guid>
<content:encoded><![CDATA[
arXiv:2510.26802v1 Announce Type: cross 
Abstract: Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning</title>
<link>https://arxiv.org/abs/2311.06736</link>
<guid>https://arxiv.org/abs/2311.06736</guid>
<content:encoded><![CDATA[
arXiv:2311.06736v3 Announce Type: replace 
Abstract: Logical reasoning is a pivotal component in the field of artificial intelligence. Proof planning, particularly in contexts requiring the validation of explanation accuracy, continues to present challenges. The recent advancement of large language models (LLMs) has led to significant progress in natural language proof planning, evolving from one-stage generators to more complex three-stage systems that include additional searchers or verifiers. While these assisted methods improve the quality of generated results, they also introduce increased search efforts and computational costs. Furthermore, the generative process itself remains underexplored. In this study, we propose a stepwise decoding approach augmented by contrastive learning to address two common errors encountered during the LLM generator's decoding process. We fine-tune the language model using both vanilla and enhanced hard negatives to mitigate these decoding errors. Empirical results demonstrate the effectiveness of our strategy. Additionally, our further analysis reveals that even larger LLMs still struggle to generate rigorous logical chains.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks</title>
<link>https://arxiv.org/abs/2404.00176</link>
<guid>https://arxiv.org/abs/2404.00176</guid>
<content:encoded><![CDATA[
arXiv:2404.00176v2 Announce Type: replace 
Abstract: Lexical Semantic Change Detection (LSCD) is a complex, lemma-level task, which is usually operationalized based on two subsequently applied usage-level tasks: First, Word-in-Context (WiC) labels are derived for pairs of usages. Then, these labels are represented in a graph on which Word Sense Induction (WSI) is applied to derive sense clusters. Finally, LSCD labels are derived by comparing sense clusters over time. This modularity is reflected in most LSCD datasets and models. It also leads to a large heterogeneity in modeling options and task definitions, which is exacerbated by a variety of dataset versions, preprocessing options and evaluation metrics. This heterogeneity makes it difficult to evaluate models under comparable conditions, to choose optimal model combinations or to reproduce results. Hence, we provide a benchmark repository standardizing LSCD evaluation. Through transparent implementation results become easily reproducible and by standardization different components can be freely combined. The repository reflects the task's modularity by allowing model evaluation for WiC, WSI and LSCD. This allows for careful evaluation of increasingly complex model components providing new ways of model optimization. We use the implemented benchmark to conduct a number of experiments with recent models and systematically improve the state-of-the-art.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speak &amp; Spell: LLM-Driven Controllable Phonetic Error Augmentation for Robust Dialogue State Tracking</title>
<link>https://arxiv.org/abs/2409.06263</link>
<guid>https://arxiv.org/abs/2409.06263</guid>
<content:encoded><![CDATA[
arXiv:2409.06263v2 Announce Type: replace 
Abstract: Dialogue State Tracking (DST) is a key part of task-oriented dialogue systems, identifying important information in conversations. However, its accuracy drops significantly in spoken dialogue environments due to named entity errors from Automatic Speech Recognition (ASR) systems. We introduce a simple yet effective data augmentation method that targets those entities to improve the robustness of DST model. Our novel method can control the placement of errors using keyword-highlighted prompts while introducing phonetically similar errors. As a result, our method generated sufficient error patterns on keywords, leading to improved accuracy in noised and low-accuracy ASR environments.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Model Preference Evaluation with Multiple Weak Evaluators</title>
<link>https://arxiv.org/abs/2410.12869</link>
<guid>https://arxiv.org/abs/2410.12869</guid>
<content:encoded><![CDATA[
arXiv:2410.12869v4 Announce Type: replace 
Abstract: Despite the remarkable success of Large Language Models (LLMs), evaluating their outputs' quality regarding preference remains a critical challenge. While existing works usually leverage a strong LLM as the judge for comparing LLMs' response pairwisely, such a single-evaluator approach is vulnerable to cyclic preference, i.e., output A is better than B, B than C, but C is better than A, causing contradictory evaluation results. To address this, we introduce PGED (Preference Graph Ensemble and Denoise), a novel approach that leverages multiple model-based evaluators to construct preference graphs, and then ensembles and denoises these graphs for acyclic, non-contradictory evaluation results. We provide theoretical guarantees for our framework, demonstrating its efficacy in recovering the ground truth preference structure. Extensive experiments on ten benchmarks demonstrate PGED 's superiority in three applications: 1) model ranking for evaluation, 2) response selection for test-time scaling, and 3) data selection for model fine-tuning. Notably, PGED combines small LLM evaluators (e.g., Llama3-8B, Mistral-7B, Qwen2-7B) to outperform strong ones (e.g., Qwen2-72B), showcasing its effectiveness in enhancing evaluation reliability and improving model performance.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>This Candidate is [MASK]. Prompt-based Sentiment Extraction and Reference Letters</title>
<link>https://arxiv.org/abs/2410.16325</link>
<guid>https://arxiv.org/abs/2410.16325</guid>
<content:encoded><![CDATA[
arXiv:2410.16325v3 Announce Type: replace 
Abstract: I propose a relatively simple way to deploy pre-trained large language models (LLMs) in order to extract sentiment and other useful features from text data. The method, which I refer to as prompt-based sentiment extraction, offers multiple advantages over other methods used in economics and finance. In particular, it accepts the text input as is (without pre-processing) and produces a sentiment score that has a probability interpretation. Unlike other LLM-based approaches, it does not require any fine-tuning or labeled data. I apply my prompt-based strategy to a hand-collected corpus of confidential reference letters (RLs). I show that the sentiment contents of RLs are clearly reflected in job market outcomes. Candidates with higher average sentiment in their RLs perform markedly better regardless of the measure of success chosen. Moreover, I show that sentiment dispersion among letter writers negatively affects the job market candidate's performance. I compare my sentiment extraction approach to other commonly used methods for sentiment analysis: `bag-of-words' approaches, fine-tuned language models, and querying advanced chatbots. No other method can fully reproduce the results obtained by prompt-based sentiment extraction. Finally, I slightly modify the method to obtain `gendered' sentiment scores (as in Eberhardt et al., 2023). I show that RLs written for female candidates emphasize `grindstone' personality traits, whereas male candidates' letters emphasize `standout' traits. These gender differences negatively affect women's job market outcomes.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data</title>
<link>https://arxiv.org/abs/2502.04380</link>
<guid>https://arxiv.org/abs/2502.04380</guid>
<content:encoded><![CDATA[
arXiv:2502.04380v3 Announce Type: replace 
Abstract: Fine-tuning large language models (LLMs) using diverse datasets is crucial for enhancing their overall performance across various domains. In practical scenarios, existing methods based on modeling the mixture proportions of data composition often struggle with data whose domain labels are missing, imprecise or non-normalized, while methods based on data selection usually encounter difficulties in balancing multi-domain performance. To address these challenges, in this work, we investigate the role of data diversity in enhancing the overall abilities of LLMs by empirically constructing contrastive data pools and theoretically deriving explanations. Building upon the insights gained, we propose a new method that gives the LLM a dual identity: an output model to cognitively probe and select data based on diversity reward, as well as an input model to be tuned with the selected data. Extensive experiments show that the proposed method notably boosts performance across domain-undetermined data and a series of foundational downstream tasks when applied to various advanced LLMs. We release our code and hope this study can shed light on the understanding of data diversity and advance feedback-driven data-model co-design for LLMs.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unstructured Evidence Attribution for Long Context Query Focused Summarization</title>
<link>https://arxiv.org/abs/2502.14409</link>
<guid>https://arxiv.org/abs/2502.14409</guid>
<content:encoded><![CDATA[
arXiv:2502.14409v2 Announce Type: replace 
Abstract: Large language models (LLMs) are capable of generating coherent summaries from very long contexts given a user query, and extracting and citing evidence spans helps improve the trustworthiness of these summaries. Whereas previous work has focused on evidence citation with fixed levels of granularity (e.g. sentence, paragraph, document, etc.), we propose to extract unstructured (i.e., spans of any length) evidence in order to acquire more relevant and consistent evidence than in the fixed granularity case. We show how existing systems struggle to copy and properly cite unstructured evidence, which also tends to be "lost-in-the-middle". To help models perform this task, we create the Summaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset generated using a novel pipeline, which can be used as training supervision for unstructured evidence summarization. We demonstrate across 5 LLMs and 4 datasets spanning human written, synthetic, single, and multi-document settings that LLMs adapted with SUnsET generate more relevant and factually consistent evidence with their summaries, extract evidence from more diverse locations in their context, and can generate more relevant and consistent summaries than baselines with no fine-tuning and fixed granularity evidence. We release SUnsET and our generation code to the public.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More of the Same: Persistent Representational Harms Under Increased Representation</title>
<link>https://arxiv.org/abs/2503.00333</link>
<guid>https://arxiv.org/abs/2503.00333</guid>
<content:encoded><![CDATA[
arXiv:2503.00333v2 Announce Type: replace 
Abstract: To recognize and mitigate the harms of generative AI systems, it is crucial to consider who is represented in the outputs of generative AI systems and how people are represented. A critical gap emerges when naively improving who is represented, as this does not imply bias mitigation efforts have been applied to address how people are represented. We critically examined this by investigating gender representation in occupation across state-of-the-art large language models. We first show evidence suggesting that over time there have been interventions to models altering the resulting gender distribution, and we find that women are more represented than men when models are prompted to generate biographies or personas. We then demonstrate that representational biases persist in how different genders are represented by examining statistically significant word differences across genders. This results in a proliferation of representational harms, stereotypes, and neoliberalism ideals that, despite existing interventions to increase female representation, reinforce existing systems of oppression.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LLM Safety Alignment with Dual-Objective Optimization</title>
<link>https://arxiv.org/abs/2503.03710</link>
<guid>https://arxiv.org/abs/2503.03710</guid>
<content:encoded><![CDATA[
arXiv:2503.03710v3 Announce Type: replace 
Abstract: Existing training-time safety alignment techniques for large language models (LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization (DPO), a widely deployed alignment method, exhibits limitations in both experimental and theoretical contexts as its loss function proves suboptimal for refusal learning. Through gradient-based analysis, we identify these shortcomings and propose an improved safety alignment that disentangles DPO objectives into two components: (1) robust refusal training, which encourages refusal even when partial unsafe generations are produced, and (2) targeted unlearning of harmful knowledge. This approach significantly increases LLM robustness against a wide range of jailbreak attacks, including prefilling, suffix, and multi-turn attacks across both in-distribution and out-of-distribution scenarios. Furthermore, we introduce a method to emphasize critical refusal tokens by incorporating a reward-based token-level weighting mechanism for refusal learning, which further improves the robustness against adversarial exploits. Our research also suggests that robustness to jailbreak attacks is correlated with token distribution shifts in the training process and internal representations of refusal and harmful tokens, offering valuable directions for future research in LLM safety alignment. The code is available at https://github.com/wicai24/DOOR-Alignment
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-shot Benchmarking: A Framework for Flexible and Scalable Automatic Evaluation of Language Models</title>
<link>https://arxiv.org/abs/2504.01001</link>
<guid>https://arxiv.org/abs/2504.01001</guid>
<content:encoded><![CDATA[
arXiv:2504.01001v2 Announce Type: replace 
Abstract: As language models improve and become capable of performing more complex tasks across modalities, evaluating them automatically becomes increasingly challenging. Developing strong and robust task-specific automatic metrics gets harder, and human-annotated test sets -- which are expensive to create -- saturate more quickly. A compelling alternative is to design reliable strategies to automate the creation of test data and evaluation, but previous attempts either rely on pre-existing data, or focus solely on individual tasks. We present Zero-shot Benchmarking (ZSB), a framework for creating high-quality benchmarks for any task by leveraging language models for both synthetic test data creation and evaluation. ZSB is simple and flexible: it requires only the creation of a prompt for data generation and one for evaluation; it is scalable to tasks and languages where collecting real-world data is costly or impractical; it is model-agnostic, allowing the creation of increasingly challenging benchmarks as models improve. To assess the effectiveness of our framework, we create benchmarks for five text-only tasks and a multi-modal one: general capabilities in four languages (English, Chinese, French, and Korean), translation, and general vision-language capabilities in English. We then rank a broad range of open and closed systems on our benchmarks. ZSB rankings consistently correlate strongly with human rankings, outperforming widely-adopted standard benchmarks. Through ablations, we find that strong benchmarks can be created with open models, and that judge model size and dataset variety are crucial drivers of performance. We release all our benchmarks, and code to reproduce our experiments and to produce new benchmarks.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M-Prometheus: A Suite of Open Multilingual LLM Judges</title>
<link>https://arxiv.org/abs/2504.04953</link>
<guid>https://arxiv.org/abs/2504.04953</guid>
<content:encoded><![CDATA[
arXiv:2504.04953v2 Announce Type: replace 
Abstract: The use of language models for automatically evaluating long-form text (LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are optimized exclusively for English, with strategies for enhancing their multilingual evaluation capabilities remaining largely unexplored in the current literature. This has created a disparity in the quality of automatic evaluation methods for non-English languages, ultimately hindering the development of models with better multilingual capabilities. To bridge this gap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from 3B to 14B parameters that can provide both direct assessment and pairwise comparison feedback on multilingual outputs. M-Prometheus models outperform state-of-the-art open LLM judges on multilingual reward benchmarks spanning more than 20 languages, as well as on literary machine translation (MT) evaluation covering 4 language pairs. Furthermore, M-Prometheus models can be leveraged at decoding time to significantly improve generated outputs across all 3 tested languages, showcasing their utility for the development of better multilingual models. Lastly, through extensive ablations, we identify the key factors for obtaining an effective multilingual judge, including backbone model selection and training on synthetic multilingual feedback data instead of translated data. We release our models, training dataset, and code.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEA-LION: Southeast Asian Languages in One Network</title>
<link>https://arxiv.org/abs/2504.05747</link>
<guid>https://arxiv.org/abs/2504.05747</guid>
<content:encoded><![CDATA[
arXiv:2504.05747v4 Announce Type: replace 
Abstract: Recently, Large Language Models (LLMs) have dominated much of the artificial intelligence scene with their ability to process and generate natural languages. However, the majority of LLM research and development remains English-centric, leaving low-resource languages such as those in the Southeast Asian (SEA) region under-represented. To address this representation gap, we introduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge multilingual LLMs designed for SEA languages. The SEA-LION family of LLMs supports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese, Malay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages large-scale multilingual continued pre-training with a comprehensive post-training regime involving multiple stages of instruction fine-tuning, alignment, and model merging. Evaluation results on multilingual benchmarks indicate that our models achieve state-of-the-art performance across LLMs supporting SEA languages. We open-source the models to benefit the wider SEA community.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dependency Structure Augmented Contextual Scoping Framework for Multimodal Aspect-Based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2504.11331</link>
<guid>https://arxiv.org/abs/2504.11331</guid>
<content:encoded><![CDATA[
arXiv:2504.11331v2 Announce Type: replace 
Abstract: Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract fine-grained information from image-text pairs to identify aspect terms and determine their sentiment polarity. However, existing approaches often fall short in simultaneously addressing three core challenges: Sentiment Cue Perception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise Elimination (SNE). To overcome these limitations, we propose DASCO (\textbf{D}ependency Structure \textbf{A}ugmented \textbf{Sco}ping Framework), a fine-grained scope-oriented framework that enhances aspect-level sentiment reasoning by leveraging dependency parsing trees. First, we designed a multi-task pretraining strategy for MABSA on our base model, combining aspect-oriented enhancement, image-text matching, and aspect-level sentiment-sensitive cognition. This improved the model's perception of aspect terms and sentiment cues while achieving effective image-text alignment, addressing key challenges like SCP and MIM. Furthermore, we incorporate dependency trees as syntactic branch combining with semantic branch, guiding the model to selectively attend to critical contextual elements within a target-specific scope while effectively filtering out irrelevant noise for addressing SNE problem. Extensive experiments on two benchmark datasets across three subtasks demonstrate that DASCO achieves state-of-the-art performance in MABSA, with notable gains in JMASA (+2.3\% F1 and +3.5\% precision on Twitter2015). The source code is available at https://github.com/LHaoooo/DASCO .
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.13444</link>
<guid>https://arxiv.org/abs/2505.13444</guid>
<content:encoded><![CDATA[
arXiv:2505.13444v2 Announce Type: replace 
Abstract: Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is difficult to perform in text. We conduct a case study using a synthetic dataset solvable only through visual reasoning and show that model performance degrades significantly with increasing visual complexity, while human performance remains robust. We then introduce ChartMuseum, a new Chart Question Answering (QA) benchmark containing 1,162 expert-annotated questions spanning multiple reasoning types, curated from real-world charts across 184 sources, specifically built to evaluate complex visual and textual reasoning. Unlike prior chart understanding benchmarks -- where frontier models perform similarly and near saturation -- our benchmark exposes a substantial gap between model and human performance, while effectively differentiating model capabilities: although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct achieves only 38.5%. Moreover, on questions requiring primarily visual reasoning, all models experience a 35%-55% performance drop from text-reasoning-heavy question performance. Lastly, our qualitative error analysis reveals specific categories of visual reasoning that are challenging for current LVLMs.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English</title>
<link>https://arxiv.org/abs/2505.15095</link>
<guid>https://arxiv.org/abs/2505.15095</guid>
<content:encoded><![CDATA[
arXiv:2505.15095v2 Announce Type: replace 
Abstract: Sarcasm is a challenge to sentiment analysis because of the incongruity between stated and implied sentiment. The challenge is exacerbated when the implication may be relevant to a specific country or geographical region. Pragmatic metacognitive prompting (PMP) is a cognition-inspired technique that has been used for pragmatic reasoning. In this paper, we harness PMP for explainable sarcasm detection for Australian and Indian English, alongside a benchmark dataset for standard English. We manually add sarcasm explanations to an existing sarcasm-labeled dataset for Australian and Indian English called BESSTIE, and compare the performance for explainable sarcasm detection for them with FLUTE, a standard English dataset containing sarcasm explanations. Our approach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA) achieves statistically significant performance improvement across all tasks and datasets when compared with four alternative prompting strategies. We also find that alternative techniques such as agentic prompting mitigate context-related failures by enabling external knowledge retrieval. The focused contribution of our work is utilising PMP in generating sarcasm explanations for varieties of English.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.24388</link>
<guid>https://arxiv.org/abs/2505.24388</guid>
<content:encoded><![CDATA[
arXiv:2505.24388v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs) with external knowledge to improve factuality. However, existing RAG systems frequently underutilize the retrieved documents, failing to extract and integrate the key clues needed to support faithful and interpretable reasoning, especially in cases where relevant evidence is implicit, scattered, or obscured by noise. To address this issue, we propose ClueAnchor, a novel framework for enhancing RAG via clue-anchored reasoning exploration and optimization. ClueAnchor extracts key clues from retrieved content and generates multiple reasoning paths based on different knowledge configurations, optimizing the model by selecting the most appropriate reasoning path for the given context through reward-based preference optimization. Experiments show that ClueAnchor significantly outperforms prior RAG baselines in the completeness and robustness of reasoning. Further analysis confirms its strong resilience to noisy or partially relevant retrieved content, as well as its capability to identify supporting evidence even in the absence of explicit clue supervision during inference. All codes are available at https://github.com/thunlp/ClueAnchor.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Debate Aids Assessment of Controversial Claims</title>
<link>https://arxiv.org/abs/2506.02175</link>
<guid>https://arxiv.org/abs/2506.02175</guid>
<content:encoded><![CDATA[
arXiv:2506.02175v2 Announce Type: replace 
Abstract: As AI grows more powerful, it will increasingly shape how we understand the world. But with this influence comes the risk of amplifying misinformation and deepening social divides-especially on consequential topics where factual accuracy directly impacts well-being. Scalable Oversight aims to ensure AI systems remain truthful even when their capabilities exceed those of their evaluators. Yet when humans serve as evaluators, their own beliefs and biases can impair judgment. We study whether AI debate can guide biased judges toward the truth by having two AI systems debate opposing sides of controversial factuality claims on COVID-19 and climate change where people hold strong prior beliefs. We conduct two studies. Study I recruits human judges with either mainstream or skeptical beliefs who evaluate claims through two protocols: debate (interaction with two AI advisors arguing opposing sides) or consultancy (interaction with a single AI advisor). Study II uses AI judges with and without human-like personas to evaluate the same protocols. In Study I, debate consistently improves human judgment accuracy and confidence calibration, outperforming consultancy by 4-10% across COVID-19 and climate change claims. The improvement is most significant for judges with mainstream beliefs (up to +15.2% accuracy on COVID-19 claims), though debate also helps skeptical judges who initially misjudge claims move toward accurate views (+4.7% accuracy). In Study II, AI judges with human-like personas achieve even higher accuracy (78.5%) than human judges (70.1%) and default AI judges without personas (69.8%), suggesting their potential for supervising frontier AI models. These findings highlight AI debate as a promising path toward scalable, bias-resilient oversight in contested domains.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through Combat</title>
<link>https://arxiv.org/abs/2506.04721</link>
<guid>https://arxiv.org/abs/2506.04721</guid>
<content:encoded><![CDATA[
arXiv:2506.04721v2 Announce Type: replace 
Abstract: We propose SPARTA ALIGNMENT, an algorithm to collectively align multiple LLMs through competition and combat. To complement a single model's lack of diversity in generation and biases in evaluation, multiple LLMs form a "sparta tribe" to compete against each other in fulfilling instructions while serving as judges for the competition of others. For each iteration, one instruction and two models are selected for a duel, the other models evaluate the two responses, and their evaluation scores are aggregated through a adapted elo-ranking based reputation system, where winners/losers of combat gain/lose weight in evaluating others. The peer-evaluated combat results then become preference pairs where the winning response is preferred over the losing one, and all models learn from these preferences at the end of each iteration. SPARTA ALIGNMENT enables the self-evolution of multiple LLMs in an iterative and collective competition process. Extensive experiments demonstrate that SPARTA ALIGNMENT outperforms initial models and 4 self-alignment baselines across 10 out of 12 tasks and datasets with 7.0% average improvement. Further analysis reveals that SPARTA ALIGNMENT generalizes more effectively to unseen tasks and leverages the expertise diversity of participating models to produce more logical, direct and informative outputs.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text</title>
<link>https://arxiv.org/abs/2506.07001</link>
<guid>https://arxiv.org/abs/2506.07001</guid>
<content:encoded><![CDATA[
arXiv:2506.07001v2 Announce Type: replace 
Abstract: The increasing capabilities of Large Language Models (LLMs) have raised concerns about their misuse in AI-generated plagiarism and social engineering. While various AI-generated text detectors have been proposed to mitigate these risks, many remain vulnerable to simple evasion techniques such as paraphrasing. However, recent detectors have shown greater robustness against such basic attacks. In this work, we introduce Adversarial Paraphrasing, a training-free attack framework that universally humanizes any AI-generated text to evade detection more effectively. Our approach leverages an off-the-shelf instruction-following LLM to paraphrase AI-generated content under the guidance of an AI text detector, producing adversarial examples that are specifically optimized to bypass detection. Extensive experiments show that our attack is both broadly effective and highly transferable across several detection systems. For instance, compared to simple paraphrasing attack--which, ironically, increases the true positive at 1% false positive (T@1%F) by 8.57% on RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by OpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on Fast-DetectGPT. Across a diverse set of detectors--including neural network-based, watermark-based, and zero-shot approaches--our attack achieves an average T@1%F reduction of 87.88% under the guidance of OpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and attack success to find that our method can significantly reduce detection rates, with mostly a slight degradation in text quality. Our adversarial setup highlights the need for more robust and resilient detection strategies in the light of increasingly sophisticated evasion techniques.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens</title>
<link>https://arxiv.org/abs/2506.08410</link>
<guid>https://arxiv.org/abs/2506.08410</guid>
<content:encoded><![CDATA[
arXiv:2506.08410v2 Announce Type: replace 
Abstract: Previous research has primarily focused on the cognitive error detection capabilities of Large Language Models (LLMs), often prompting them to analyze mistakes in reasoning chains. However, few studies have examined the meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors), which are crucial for their reliability. While studies on LLM self-evaluation present some measures, such as perplexity, which can reflect the answer correctness and be viewed as the lens of meta-cognition, they lack step-level analysis and adaptation. This paper studies the evaluation of LLM meta-cognition using the current lenses and how to improve these lenses. Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation framework for benchmarking the existing lenses. Furthermore, a training-free Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost current meta-cognition lenses. Experimental results on three mathematical reasoning datasets and three LLMs show the reasonableness of AutoMeco by comparing it with Best-of-N verification. Moreover, the meta-cognition ability of LLMs can be better evaluated using MIRA.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing human and LLM politeness strategies in free production</title>
<link>https://arxiv.org/abs/2506.09391</link>
<guid>https://arxiv.org/abs/2506.09391</guid>
<content:encoded><![CDATA[
arXiv:2506.09391v2 Announce Type: replace 
Abstract: Polite speech poses a fundamental alignment challenge for large language models (LLMs). Humans deploy a rich repertoire of linguistic strategies to balance informational and social goals -- from positive approaches that build rapport (compliments, expressions of interest) to negative strategies that minimize imposition (hedging, indirectness). We investigate whether LLMs employ a similarly context-sensitive repertoire by comparing human and LLM responses in both constrained and open-ended production tasks. We find that larger models ($\ge$70B parameters) successfully replicate key preferences from the computational pragmatics literature, and human evaluators surprisingly prefer LLM-generated responses in open-ended contexts. However, further linguistic analyses reveal that models disproportionately rely on negative politeness strategies even in positive contexts, potentially leading to misinterpretations. While modern LLMs demonstrate an impressive handle on politeness strategies, these subtle differences raise important questions about pragmatic alignment in AI systems.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs</title>
<link>https://arxiv.org/abs/2506.11094</link>
<guid>https://arxiv.org/abs/2506.11094</guid>
<content:encoded><![CDATA[
arXiv:2506.11094v2 Announce Type: replace 
Abstract: With the rapid advancement of artificial intelligence, Large Language Models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), including content generation, human-computer interaction, machine translation, and code generation. However, their widespread deployment has also raised significant safety concerns. In particular, LLM-generated content can exhibit unsafe behaviors such as toxicity, bias, or misinformation, especially in adversarial contexts, which has attracted increasing attention from both academia and industry. Although numerous studies have attempted to evaluate these risks, a comprehensive and systematic survey on safety evaluation of LLMs is still lacking. This work aims to fill this gap by presenting a structured overview of recent advances in safety evaluation of LLMs. Specifically, we propose a four-dimensional taxonomy: (i) Why to evaluate, which explores the background of safety evaluation of LLMs, how they differ from general LLMs evaluation, and the significance of such evaluation; (ii) What to evaluate, which examines and categorizes existing safety evaluation tasks based on key capabilities, including dimensions such as toxicity, robustness, ethics, bias and fairness, truthfulness, and related aspects; (iii) Where to evaluate, which summarizes the evaluation metrics, datasets and benchmarks currently used in safety evaluations; (iv) How to evaluate, which reviews existing mainstream evaluation methods based on the roles of the evaluators and some evaluation frameworks that integrate the entire evaluation pipeline. Finally, we identify the challenges in safety evaluation of LLMs and propose promising research directions to promote further advancement in this field. We emphasize the necessity of prioritizing safety evaluation to ensure the reliable and responsible deployment of LLMs in real-world applications.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IGD: Token Decisiveness Modeling via Information Gain in LLMs for Personalized Recommendation</title>
<link>https://arxiv.org/abs/2506.13229</link>
<guid>https://arxiv.org/abs/2506.13229</guid>
<content:encoded><![CDATA[
arXiv:2506.13229v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown strong potential for recommendation by framing item prediction as a token-by-token language generation task. However, existing methods treat all item tokens equally, simply pursuing likelihood maximization during both optimization and decoding. This overlooks crucial token-level differences in decisiveness-many tokens contribute little to item discrimination yet can dominate optimization or decoding. To quantify token decisiveness, we propose a novel perspective that models item generation as a decision process, measuring token decisiveness by the Information Gain (IG) each token provides in reducing uncertainty about the generated item. Our empirical analysis reveals that most tokens have low IG but often correspond to high logits, disproportionately influencing training loss and decoding, which may impair model performance. Building on these insights, we introduce an Information Gain-based Decisiveness-aware Token handling (IGD) strategy that integrates token decisiveness into both tuning and decoding. Specifically, IGD downweights low-IG tokens during tuning and rebalances decoding to emphasize tokens with high IG. In this way, IGD moves beyond pure likelihood maximization, effectively prioritizing high-decisiveness tokens. Extensive experiments on four benchmark datasets with two LLM backbones demonstrate that IGD consistently improves recommendation accuracy, achieving significant gains on widely used ranking metrics compared to strong baselines.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study</title>
<link>https://arxiv.org/abs/2506.13464</link>
<guid>https://arxiv.org/abs/2506.13464</guid>
<content:encoded><![CDATA[
arXiv:2506.13464v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown impressive capabilities across tasks such as mathematics, coding, and reasoning, yet their learning ability, which is crucial for adapting to dynamic environments and acquiring new knowledge, remains underexplored. In this work, we address this gap by introducing a framework inspired by cognitive psychology and education. Specifically, we decompose general learning ability into three distinct, complementary dimensions: Learning from Instructor (acquiring knowledge via explicit guidance), Learning from Concept (internalizing abstract structures and generalizing to new contexts), and Learning from Experience (adapting through accumulated exploration and feedback). We conduct a comprehensive empirical study across the three learning dimensions and identify several insightful findings, such as (i) interaction improves learning; (ii) conceptual understanding is scale-emergent and benefits larger models; and (iii) LLMs are effective few-shot learners but not many-shot learners. Based on our framework and empirical findings, we introduce a benchmark that provides a unified and realistic evaluation of LLMs' general learning abilities across three learning cognition dimensions. It enables diagnostic insights and supports evaluation and development of more adaptive and human-like models.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality</title>
<link>https://arxiv.org/abs/2506.14681</link>
<guid>https://arxiv.org/abs/2506.14681</guid>
<content:encoded><![CDATA[
arXiv:2506.14681v2 Announce Type: replace 
Abstract: Supervised fine-tuning (SFT) is a critical step in aligning large language models (LLMs) with human instructions and values, yet many aspects of SFT remain poorly understood. We trained a wide range of base models on a variety of datasets including code generation, mathematical reasoning, and general-domain tasks, resulting in 1,000+ SFT models under controlled conditions. We then identified the dataset properties that matter most and examined the layer-wise modifications introduced by SFT. Our findings reveal that some training-task synergies persist across all models while others vary substantially, emphasizing the importance of model-specific strategies. Moreover, we demonstrate that perplexity consistently predicts SFT effectiveness, often surpassing superficial similarity between the training data and the benchmark, and that mid-layer weight changes correlate most strongly with performance gains. We release these 1,000+ SFT models and benchmark results to accelerate further research. All resources are available at https://github.com/llm-jp/massive-sft.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlling Thinking Speed in Reasoning Models</title>
<link>https://arxiv.org/abs/2507.03704</link>
<guid>https://arxiv.org/abs/2507.03704</guid>
<content:encoded><![CDATA[
arXiv:2507.03704v2 Announce Type: replace 
Abstract: Human cognition is theorized to operate in two modes: fast, intuitive System 1 thinking and slow, deliberate System 2 thinking. While current Large Reasoning Models (LRMs) excel at System 2 thinking, their inability to perform fast thinking leads to high computational overhead and latency. In this work, we enable LRMs to approximate human intelligence through dynamic thinking speed adjustment, optimizing accuracy-efficiency trade-offs. Our approach addresses two key questions: (1) how to control thinking speed in LRMs, and (2) when to adjust it for optimal performance. For the first question, we identify the steering vector that governs slow-fast thinking transitions in LRMs' representation space. Using this vector, we achieve the first representation editing-based test-time scaling effect, outperforming existing prompt-based scaling methods. For the second question, we apply real-time difficulty estimation to signal reasoning segments of varying complexity. Combining these techniques, we propose the first reasoning strategy that enables fast processing of easy steps and deeper analysis for complex reasoning. Without any training or additional cost, our plug-in module delivers an average +1.3% accuracy with -8.6% token usage across leading LRMs and advanced reasoning benchmarks. All of our algorithms are implemented based on vLLM and are expected to support broader applications and inspire future research.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It</title>
<link>https://arxiv.org/abs/2507.13328</link>
<guid>https://arxiv.org/abs/2507.13328</guid>
<content:encoded><![CDATA[
arXiv:2507.13328v2 Announce Type: replace 
Abstract: Does vision-and-language (VL) training change the linguistic representations of language models in meaningful ways? Most results in the literature have shown inconsistent or marginal differences, both behaviorally and representationally. In this work, we start from the hypothesis that the domain in which VL training could have a significant effect is lexical-conceptual knowledge, in particular its taxonomic organization. Through comparing minimal pairs of text-only LMs and their VL-trained counterparts, we first show that the VL models often outperform their text-only counterparts on a text-only question-answering task that requires taxonomic understanding of concepts mentioned in the questions. Using an array of targeted behavioral and representational analyses, we show that the LMs and VLMs do not differ significantly in terms of their taxonomic knowledge itself, but they differ in how they represent questions that contain concepts in a taxonomic relation vs. a non-taxonomic relation. This implies that the taxonomic knowledge itself does not change substantially through additional VL training, but VL training does improve the deployment of this knowledge in the context of a specific task, even when the presentation of the task is purely linguistic.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Isolated Dots: Benchmarking Structured Table Construction as Deep Knowledge Extraction</title>
<link>https://arxiv.org/abs/2507.16271</link>
<guid>https://arxiv.org/abs/2507.16271</guid>
<content:encoded><![CDATA[
arXiv:2507.16271v2 Announce Type: replace 
Abstract: With the emergence of large language models (LLMs), there is an expectation that LLMs can effectively extract explicit information from complex real-world documents (e.g., papers, reports). However, most LLMs generate paragraph-style answers that are chaotic, disorganized, and untraceable. To bridge this gap, we introduce the Arranged and Organized Extraction Benchmark (AOE), a new bilingual benchmark with data and documents of varying lengths designed to systematically evaluate the ability of LLMs to comprehend fragmented documents and reconstruct isolated information into one organized table. Unlike conventional text-to-table tasks, which rely on fixed schema and narrow task domains, AOE includes 11 carefully crafted tasks across three diverse domains, requiring models to generate context-specific schema tailored to varied input queries. In the experiment, we evaluated both open-source and closed-source state-of-the-art LLMs. The results show that even the most advanced models struggled significantly. The benchmark is available at https://anonymous.4open.science/r/AOE-Benchmark/.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs</title>
<link>https://arxiv.org/abs/2507.19756</link>
<guid>https://arxiv.org/abs/2507.19756</guid>
<content:encoded><![CDATA[
arXiv:2507.19756v2 Announce Type: replace 
Abstract: In addition to its more widely studied cultural movements, American Evangelicalism has a well-developed but less externally visible literary side. Christian Fiction, however, has been little studied, and what scholarly attention there is has focused on the explosively popular Left Behind series. In this work, we use computational tools to provide both a broad topical overview of Christian Fiction as a genre and a more directed exploration of how its authors depict divine acts. Working with human annotators, we first developed a codebook for identifying "acts of God." We then adapted the codebook for use by a recent, lightweight LM with the assistance of a much larger model. The laptop-scale LM is largely capable of matching human annotations, even when the task is subtle and challenging. Using these annotations, we show that significant and meaningful differences exist between divine acts depicted by the Left Behind books and Christian Fiction more broadly.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyTim: A Family of Language Models for Divergent Generation</title>
<link>https://arxiv.org/abs/2508.11607</link>
<guid>https://arxiv.org/abs/2508.11607</guid>
<content:encoded><![CDATA[
arXiv:2508.11607v2 Announce Type: replace 
Abstract: In the search for artificial general intelligence, model development and training has focused primarily on vast datasets of known problems and their accepted solutions. This process necessarily produces convergent systems which are fundamentally incapable of the conceptual reframing that is required for genuine creative breakthroughs. Inspired by the divergent cognitive processes that allow humans to make such creative leaps, our work introduces a family of language models, TinyTim, to serve as sources of divergent generation within broader systems. These models have been created by fine-tuning on the anti-parsimonious text of James Joyce's `Finnegans Wake'. Quantitative analysis of both an unsupervised fine-tuned model (TinyTim-V1) and a new instruction-tuned variant (TinyTim-V2) demonstrates a profound capacity for lexical invention; the foundational V1 model exhibits a Yule's K score for lexical richness over twenty times greater than that of convergent baselines. This trait is a stable property of the family, as the instruction-tuned V2 maintains a statistically distinct profile and resists factual convergence, sacrificing benchmark performance to preserve its core generative style. This work establishes a methodology for engineering specialized divergent models that, when paired with convergent systems, can reframe problems and force breakthroughs beyond the reach of statistical optimization alone.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLBFF: Binary Flexible Feedback to bridge between Human Feedback &amp; Verifiable Rewards</title>
<link>https://arxiv.org/abs/2509.21319</link>
<guid>https://arxiv.org/abs/2509.21319</guid>
<content:encoded><![CDATA[
arXiv:2509.21319v2 Announce Type: replace 
Abstract: Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM post-training, each offering distinct advantages. However, RLHF struggles with interpretability and reward hacking because it relies on human judgments that usually lack explicit criteria, whereas RLVR is limited in scope by its focus on correctness-based verifiers. We propose Reinforcement Learning with Binary Flexible Feedback (RLBFF), which combines the versatility of human-driven preferences with the precision of rule-based verification, enabling reward models to capture nuanced aspects of response quality beyond mere correctness. RLBFF extracts principles that can be answered in a binary fashion (e.g. accuracy of information: yes, or code readability: no) from natural language feedback. Such principles can then be used to ground Reward Model training as an entailment task (response satisfies or does not satisfy an arbitrary principle). We show that Reward Models trained in this manner can outperform Bradley-Terry models when matched for data and achieve top performance on RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24, 2025). Additionally, users can specify principles of interest at inference time to customize the focus of our reward models, in contrast to Bradley-Terry models. Finally, we present a fully open source recipe (including data) to align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the performance of o3-mini and DeepSeek R1 on general alignment benchmarks of MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models: https://huggingface.co/collections/nvidia/reward-models-10-2025
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epistemic Diversity and Knowledge Collapse in Large Language Models</title>
<link>https://arxiv.org/abs/2510.04226</link>
<guid>https://arxiv.org/abs/2510.04226</guid>
<content:encoded><![CDATA[
arXiv:2510.04226v4 Announce Type: replace 
Abstract: Large language models (LLMs) tend to generate lexically, semantically, and stylistically homogenous texts. This poses a risk of knowledge collapse, where homogenous LLMs mediate a shrinking in the range of accessible information over time. Existing works on homogenization are limited by a focus on closed-ended multiple-choice setups or fuzzy semantic features, and do not look at trends across time and cultural contexts. To overcome this, we present a new methodology to measure epistemic diversity, i.e., variation in real-world claims in LLM outputs, which we use to perform a broad empirical study of LLM knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200 prompt variations sourced from real user chats. For the topics in our study, we show that while newer models tend to generate more diverse claims, nearly all models are less epistemically diverse than a basic web search. We find that model size has a negative impact on epistemic diversity, while retrieval-augmented generation (RAG) has a positive impact, though the improvement from RAG varies by the cultural context. Finally, compared to a traditional knowledge source (Wikipedia), we find that country-specific claims reflect the English language more than the local one, highlighting a gap in epistemic representation
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback</title>
<link>https://arxiv.org/abs/2510.08604</link>
<guid>https://arxiv.org/abs/2510.08604</guid>
<content:encoded><![CDATA[
arXiv:2510.08604v2 Announce Type: replace 
Abstract: Jailbreaks are adversarial attacks designed to bypass the built-in safety mechanisms of large language models. Automated jailbreaks typically optimize an adversarial suffix or adapt long prompt templates by forcing the model to generate the initial part of a restricted or harmful response. In this work, we show that existing jailbreak attacks that leverage such mechanisms to unlock the model response can be detected by a straightforward perplexity-based filtering on the input prompt. To overcome this issue, we propose LatentBreak, a white-box jailbreak attack that generates natural adversarial prompts with low perplexity capable of evading such defenses. LatentBreak substitutes words in the input prompt with semantically-equivalent ones, preserving the initial intent of the prompt, instead of adding high-perplexity adversarial suffixes or long templates. These words are chosen by minimizing the distance in the latent space between the representation of the adversarial prompt and that of harmless requests. Our extensive evaluation shows that LatentBreak leads to shorter and low-perplexity prompts, thus outperforming competing jailbreak algorithms against perplexity-based filters on multiple safety-aligned models.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents</title>
<link>https://arxiv.org/abs/2510.11695</link>
<guid>https://arxiv.org/abs/2510.11695</guid>
<content:encoded><![CDATA[
arXiv:2510.11695v2 Announce Type: replace 
Abstract: Although Large Language Model (LLM)-based agents are increasingly used in financial trading, it remains unclear whether they can reason and adapt in live markets, as most studies test models instead of agents, cover limited periods and assets, and rely on unverified data. To address these gaps, we introduce Agent Market Arena (AMA), the first lifelong, real-time benchmark for evaluating LLM-based trading agents across multiple markets. AMA integrates verified trading data, expert-checked news, and diverse agent architectures within a unified trading framework, enabling fair and continuous comparison under real conditions. It implements four agents, including InvestorAgent as a single-agent baseline, TradeAgent and HedgeFundAgent with different risk styles, and DeepFundAgent with memory-based reasoning, and evaluates them across GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and Gemini-2.0-flash. Live experiments on both cryptocurrency and stock markets demonstrate that agent frameworks display markedly distinct behavioral patterns, spanning from aggressive risk-taking to conservative decision-making, whereas model backbones contribute less to outcome variation. AMA thus establishes a foundation for rigorous, reproducible, and continuously evolving evaluation of financial reasoning and trading intelligence in LLM-based agents.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices</title>
<link>https://arxiv.org/abs/2510.18480</link>
<guid>https://arxiv.org/abs/2510.18480</guid>
<content:encoded><![CDATA[
arXiv:2510.18480v2 Announce Type: replace 
Abstract: Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a roofline-based theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Collapse in Aligning Large Language Models</title>
<link>https://arxiv.org/abs/2305.17608</link>
<guid>https://arxiv.org/abs/2305.17608</guid>
<content:encoded><![CDATA[
arXiv:2305.17608v2 Announce Type: replace-cross 
Abstract: The extraordinary capabilities of large language models (LLMs) such as ChatGPT and GPT-4 are in part unleashed by aligning them with reward models that are trained on human preferences, which are often represented as rankings of responses to prompts. In this paper, we document the phenomenon of \textit{reward collapse}, an empirical observation where the prevailing ranking-based approach results in an \textit{identical} reward distribution \textit{regardless} of the prompts during the terminal phase of training. This outcome is undesirable as open-ended prompts like ``write a short story about your best friend'' should yield a continuous range of rewards for their completions, while specific prompts like ``what is the capital of New Zealand'' should generate either high or low rewards. Our theoretical investigation reveals that reward collapse is primarily due to the insufficiency of the ranking-based objective function to incorporate prompt-related information during optimization. This insight allows us to derive closed-form expressions for the reward distribution associated with a set of utility functions in an asymptotic regime. To overcome reward collapse, we introduce a prompt-aware optimization scheme that provably admits a prompt-dependent reward distribution within the interpolating regime. Our experimental results suggest that our proposed prompt-aware utility functions significantly alleviate reward collapse during the training of reward models.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hysteresis Activation Function for Efficient Inference</title>
<link>https://arxiv.org/abs/2411.10573</link>
<guid>https://arxiv.org/abs/2411.10573</guid>
<content:encoded><![CDATA[
arXiv:2411.10573v3 Announce Type: replace-cross 
Abstract: The widely used ReLU is favored for its hardware efficiency, {as the implementation at inference is a one bit sign case,} yet suffers from issues such as the ``dying ReLU'' problem, where during training, neurons fail to activate and constantly remain at zero, as highlighted by Lu et al. Traditional approaches to mitigate this issue often introduce more complex and less hardware-friendly activation functions. In this work, we propose a Hysteresis Rectified Linear Unit (HeLU), an efficient activation function designed to address the ``dying ReLU'' problem with minimal complexity. Unlike traditional activation functions with fixed thresholds for training and inference, HeLU employs a variable threshold that refines the backpropagation. This refined mechanism allows simpler activation functions to achieve competitive performance comparable to their more complex counterparts without introducing unnecessary complexity or requiring inductive biases. Empirical evaluations demonstrate that HeLU enhances model generalization across diverse datasets, offering a promising solution for efficient and effective inference suitable for a wide range of neural network architectures.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Provenance Testing for Large Language Models</title>
<link>https://arxiv.org/abs/2502.00706</link>
<guid>https://arxiv.org/abs/2502.00706</guid>
<content:encoded><![CDATA[
arXiv:2502.00706v2 Announce Type: replace-cross 
Abstract: Large language models are increasingly customized through fine-tuning and other adaptations, creating challenges in enforcing licensing terms and managing downstream impacts. Tracking model origins is crucial both for protecting intellectual property and for identifying derived models when biases or vulnerabilities are discovered in foundation models. We address this challenge by developing a framework for testing model provenance: Whether one model is derived from another. Our approach is based on the key observation that real-world model derivations preserve significant similarities in model outputs that can be detected through statistical analysis. Using only black-box access to models, we employ multiple hypothesis testing to compare model similarities against a baseline established by unrelated models. On two comprehensive real-world benchmarks spanning models from 30M to 4B parameters and comprising over 600 models, our tester achieves 90-95% precision and 80-90% recall in identifying derived models. These results demonstrate the viability of systematic provenance verification in production environments even when only API access is available.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Networks for Learnable and Scalable Influence Estimation of Instruction Fine-Tuning Data</title>
<link>https://arxiv.org/abs/2502.09969</link>
<guid>https://arxiv.org/abs/2502.09969</guid>
<content:encoded><![CDATA[
arXiv:2502.09969v4 Announce Type: replace-cross 
Abstract: Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: https://github.com/agarwalishika/NN-CIFT.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models can Self-Improve at State-Value Estimation for Better Search</title>
<link>https://arxiv.org/abs/2503.02878</link>
<guid>https://arxiv.org/abs/2503.02878</guid>
<content:encoded><![CDATA[
arXiv:2503.02878v3 Announce Type: replace-cross 
Abstract: Collecting ground-truth rewards or human demonstrations for multi-step reasoning tasks is often prohibitively expensive, particularly in interactive domains such as web tasks. We introduce Self-Taught Lookahead (STL), a reward-free framework that improves language model-based value functions by reasoning explicitly about state transitions. STL can be viewed as a chain-of-thought analogue of the value iteration algorithm: instead of regressing directly on numeric values, a value LLM is trained to simulate a step of lookahead in natural language - predicting the next action, resulting state, and rationale for its value, thereby refining value estimates without any labeled data. This self-supervised procedure yields more accurate state-value predictions, which in turn enable lightweight search algorithms to expand fewer states while maintaining strong performance. Empirically, STL-trained value models built on moderately sized (8B parameter) open-weight LLMs boost web agent success rates by 39%, achieving comparable performance with proprietary models. STL also generalizes to multi-hop QA and math puzzles. We find that STL enables small open-source models to guide efficient search, reducing inference costs by integrating explicit reasoning with value learning.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quality Over Quantity? LLM-Based Curation for a Data-Efficient Audio-Video Foundation Model</title>
<link>https://arxiv.org/abs/2503.09205</link>
<guid>https://arxiv.org/abs/2503.09205</guid>
<content:encoded><![CDATA[
arXiv:2503.09205v3 Announce Type: replace-cross 
Abstract: Integrating audio and visual data for training multimodal foundational models remains a challenge. The Audio-Video Vector Alignment (AVVA) framework addresses this by considering AV scene alignment beyond mere temporal synchronization, and leveraging Large Language Models (LLMs) for data curation. AVVA implements a scoring mechanism for selecting aligned training data segments. It integrates Whisper, a speech-based foundation model, for audio and DINOv2 for video analysis in a dual-encoder structure with contrastive learning on AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the effectiveness of the proposed model architecture and data curation approach. AVVA achieves a significant improvement in top-k accuracies for video-to-audio retrieval on all datasets compared to DenseAV, while using only 192 hrs of curated training data. Furthermore, an ablation study indicates that the data curation process effectively trades data quality for data quantity, yielding increases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound, compared to training on the full spectrum of uncurated data.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?</title>
<link>https://arxiv.org/abs/2503.09499</link>
<guid>https://arxiv.org/abs/2503.09499</guid>
<content:encoded><![CDATA[
arXiv:2503.09499v3 Announce Type: replace-cross 
Abstract: Large foundation models face challenges in acquiring transferable, structured thinking abilities, especially when supervised with rigid templates or crowd-annotated instruction datasets. Unlike prior approaches, we focus on a thinking-centric data synthesis paradigm that enables models to evolve through self-generated, cognitively guided data. We propose MindGYM, a structured and scalable framework for question synthesis, composed of: (1) Cognitive Thinking Process Injection, which infuses high-level reasoning objectives to shape the model's synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating atomic questions from diverse semantic types to encourage broader thinking; and (3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop questions based on QA seeds for deeper reasoning. Detailed analysis shows that synthetic data generated by our method achieves 16.7% higher average quality and 67.91% lower quality variance compared to baseline sources, highlighting that both high-quality and self-contained data are essential for effective, thinking-oriented fine-tuning. MindGYM improves performance on six reasoning benchmarks, achieving gains of up to 16% on MathVision using only 400 data samples, and generalizable improvements across different model sizes and architectures. MindGYM underscores the viability of self-challenging mechanisms in refining large model capabilities while minimizing human intervention and resource demands. Code and data are released to promote data-centric research into self-evolving foundation models driven by their internal reasoning capabilities.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoLibra: Agent Metric Induction from Open-Ended Human Feedback</title>
<link>https://arxiv.org/abs/2505.02820</link>
<guid>https://arxiv.org/abs/2505.02820</guid>
<content:encoded><![CDATA[
arXiv:2505.02820v3 Announce Type: replace-cross 
Abstract: Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose **AutoLibra**, a framework for agent evaluation, that transforms open-ended human feedback *e.g.* "If you find that the button is disabled, don't click it again", or "This agent has too much autonomy to decide what to do on its own" into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent's behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta metrics to evaluate the alignment of a set of (induced) metrics with open feedback: "coverage" and "redundancy". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra's ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra serve human prompt engineers for diagonalize agent failures and improve prompts iterative. Moreover, we find that AutoLibra can induce metrics for automatic optimization for agents, which makes agents improve through self-regulation. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks</title>
<link>https://arxiv.org/abs/2505.12371</link>
<guid>https://arxiv.org/abs/2505.12371</guid>
<content:encoded><![CDATA[
arXiv:2505.12371v2 Announce Type: replace-cross 
Abstract: The rapid advancement of Large Language Models (LLMs) has stimulated interest in multi-agent collaboration for addressing complex medical tasks. However, the practical advantages of multi-agent collaboration approaches remain insufficiently understood. Existing evaluations often lack generalizability, failing to cover diverse tasks reflective of real-world clinical practice, and frequently omit rigorous comparisons against both single-LLM-based and established conventional methods. To address this critical gap, we introduce MedAgentBoard, a comprehensive benchmark for the systematic evaluation of multi-agent collaboration, single-LLM, and conventional approaches. MedAgentBoard encompasses four diverse medical task categories: (1) medical (visual) question answering, (2) lay summary generation, (3) structured Electronic Health Record (EHR) predictive modeling, and (4) clinical workflow automation, across text, medical images, and structured EHR data. Our extensive experiments reveal a nuanced landscape: while multi-agent collaboration demonstrates benefits in specific scenarios, such as enhancing task completeness in clinical workflow automation, it does not consistently outperform advanced single LLMs (e.g., in textual medical QA) or, critically, specialized conventional methods that generally maintain better performance in tasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital resource and actionable insights, emphasizing the necessity of a task-specific, evidence-based approach to selecting and developing AI solutions in medicine. It underscores that the inherent complexity and overhead of multi-agent collaboration must be carefully weighed against tangible performance gains. All code, datasets, detailed prompts, and experimental results are open-sourced at https://medagentboard.netlify.app/.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space</title>
<link>https://arxiv.org/abs/2505.13308</link>
<guid>https://arxiv.org/abs/2505.13308</guid>
<content:encoded><![CDATA[
arXiv:2505.13308v2 Announce Type: replace-cross 
Abstract: Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the model's latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabSTAR: A Tabular Foundation Model for Tabular Data with Text Fields</title>
<link>https://arxiv.org/abs/2505.18125</link>
<guid>https://arxiv.org/abs/2505.18125</guid>
<content:encoded><![CDATA[
arXiv:2505.18125v2 Announce Type: replace-cross 
Abstract: While deep learning has achieved remarkable success across many domains, it has historically underperformed on tabular learning tasks, which remain dominated by gradient boosting decision trees. However, recent advancements are paving the way for Tabular Foundation Models, which can leverage real-world knowledge and generalize across diverse datasets, particularly when the data contains free-text. Although incorporating language model capabilities into tabular tasks has been explored, most existing methods utilize static, target-agnostic textual representations, limiting their effectiveness. We introduce TabSTAR: a Tabular Foundation Model with Semantically Target-Aware Representations. TabSTAR is designed to enable transfer learning on tabular data with textual features, with an architecture free of dataset-specific parameters. It unfreezes a pretrained text encoder and takes as input target tokens, which provide the model with the context needed to learn task-specific embeddings. TabSTAR achieves state-of-the-art performance for both medium- and large-sized datasets across known benchmarks of classification tasks with text features, and its pretraining phase exhibits scaling laws in the number of datasets, offering a pathway for further performance improvements.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers</title>
<link>https://arxiv.org/abs/2505.21497</link>
<guid>https://arxiv.org/abs/2505.21497</guid>
<content:encoded><![CDATA[
arXiv:2505.21497v2 Announce Type: replace-cross 
Abstract: Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes. Building on this benchmark, we propose PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser distills the paper into a structured asset library; the (b)Planner aligns text-visual pairs into a binary-tree layout that preserves reading order and spatial balance; and the (c)Painter-Commenter loop refines each panel by executing rendering code and using VLM feedback to eliminate overflow and ensure alignment. In our comprehensive evaluation, we find that GPT-4o outputs-though visually appealing at first glance-often exhibit noisy text and poor PaperQuiz scores, and we find that reader engagement is the primary aesthetic bottleneck, as human-designed posters rely largely on visual semantics to convey meaning. Our fully open-source variants (e.g. based on the Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper into a finalized yet editable .pptx poster - all for just $0.005. These findings chart clear directions for the next generation of fully automated poster-generation models. The code and datasets are available at https://github.com/Paper2Poster/Paper2Poster.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Predicting Any Human Trajectory In Context</title>
<link>https://arxiv.org/abs/2506.00871</link>
<guid>https://arxiv.org/abs/2506.00871</guid>
<content:encoded><![CDATA[
arXiv:2506.00871v2 Announce Type: replace-cross 
Abstract: Predicting accurate future trajectories of pedestrians is essential for autonomous systems but remains a challenging task due to the need for adaptability in different environments and domains. A common approach involves collecting scenario-specific data and performing fine-tuning via backpropagation. However, the need to fine-tune for each new scenario is often impractical for deployment on edge devices. To address this challenge, we introduce \paper, an In-Context Learning (ICL) framework for pedestrian trajectory prediction that enables adaptation without fine-tuning on the scenario-specific data at inference time without requiring weight updates. We propose a spatio-temporal similarity-based example selection (STES) method that selects relevant examples from previously observed trajectories within the same scene by identifying similar motion patterns at corresponding locations. To further refine this selection, we introduce prediction-guided example selection (PG-ES), which selects examples based on both the past trajectory and the predicted future trajectory, rather than relying solely on the past trajectory. This approach allows the model to account for long-term dynamics when selecting examples. Finally, instead of relying on small real-world datasets with limited scenario diversity, we train our model on a large-scale synthetic dataset to enhance its prediction ability by leveraging in-context examples. Extensive experiments demonstrate that TrajICL achieves remarkable adaptation across both in-domain and cross-domain scenarios, outperforming even fine-tuned approaches across multiple public benchmarks. Project Page: https://fujiry0.github.io/TrajICL-project-page/.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors</title>
<link>https://arxiv.org/abs/2506.08188</link>
<guid>https://arxiv.org/abs/2506.08188</guid>
<content:encoded><![CDATA[
arXiv:2506.08188v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce GradEscape, the first gradient-based evader designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the undifferentiable computation problem, caused by the discrete nature of text, by introducing a novel approach to construct weighted embeddings for the detector input. It then updates the evader model parameters using feedback from victim detectors, achieving high attack success with minimal text modification. To address the issue of tokenizer mismatch between the evader and the detector, we introduce a warm-started evader method, enabling GradEscape to adapt to detectors across any language model architecture. Moreover, we employ novel tokenizer inference and model extraction techniques, facilitating effective evasion even in query-only access.
  We evaluate GradEscape on four datasets and three widely-used language models, benchmarking it against four state-of-the-art AIGT evaders. Experimental results demonstrate that GradEscape outperforms existing evaders in various scenarios, including with an 11B paraphrase model, while utilizing only 139M parameters. We have successfully applied GradEscape to two real-world commercial AIGT detectors. Our analysis reveals that the primary vulnerability stems from disparity in text expression styles within the training data. We also propose a potential defense strategy to mitigate the threat of AIGT evaders. We open-source our GradEscape for developing more robust AIGT detectors.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting</title>
<link>https://arxiv.org/abs/2507.21257</link>
<guid>https://arxiv.org/abs/2507.21257</guid>
<content:encoded><![CDATA[
arXiv:2507.21257v2 Announce Type: replace-cross 
Abstract: Language interpretation is a compositional process, in which the meaning of more complex linguistic structures is inferred from the meaning of their parts. Large language models possess remarkable language interpretation capabilities and have been successfully applied to interpret questions by mapping them to SPARQL queries. An open question is how systematic this interpretation process is. Toward this question, in this paper, we propose a benchmark for investigating to what extent the abilities of LLMs to interpret questions are actually compositional. For this, we generate three datasets of varying difficulty based on graph patterns in DBpedia, relying on Lemon lexica for verbalization. Our datasets are created in a very controlled fashion in order to test the ability of LLMs to interpret structurally complex questions, given that they have seen the atomic building blocks. This allows us to evaluate to what degree LLMs are able to interpret complex questions for which they "understand" the atomic parts. We conduct experiments with models of different sizes using both various prompt and few-shot optimization techniques as well as fine-tuning. Our results show that performance in terms of macro $F_1$ degrades from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the samples optimized on. Even when all necessary information was provided to the model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of lowest complexity. We thus conclude that LLMs struggle to systematically and compositionally interpret questions and map them into SPARQL queries.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Unicode's Unseen Underpinnings in Undermining Authorship Attribution</title>
<link>https://arxiv.org/abs/2508.15840</link>
<guid>https://arxiv.org/abs/2508.15840</guid>
<content:encoded><![CDATA[
arXiv:2508.15840v3 Announce Type: replace-cross 
Abstract: When using a public communication channel -- whether formal or informal, such as commenting or posting on social media -- end users have no expectation of privacy: they compose a message and broadcast it for the world to see. Even if an end user takes utmost precautions to anonymize their online presence -- using an alias or pseudonym; masking their IP address; spoofing their geolocation; concealing their operating system and user agent; deploying encryption; registering with a disposable phone number or email; disabling non-essential settings; revoking permissions; and blocking cookies and fingerprinting -- one obvious element still lingers: the message itself. Assuming they avoid lapses in judgment or accidental self-exposure, there should be little evidence to validate their actual identity, right? Wrong. The content of their message -- necessarily open for public consumption -- exposes an attack vector: stylometric analysis, or author profiling. In this paper, we dissect the technique of stylometry, discuss an antithetical counter-strategy in adversarial stylometry, and devise enhancements through Unicode steganography.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding</title>
<link>https://arxiv.org/abs/2508.21204</link>
<guid>https://arxiv.org/abs/2508.21204</guid>
<content:encoded><![CDATA[
arXiv:2508.21204v2 Announce Type: replace-cross 
Abstract: We study how prompt-level inductive biases influence the cognitive behavior of large language models (LLMs) in instructional dialogue. We introduce a symbolic scaffolding method paired with a short-term memory schema designed to promote adaptive, structured reasoning in Socratic tutoring. Using controlled ablation across five system variants, we evaluate model outputs via expert-designed rubrics covering scaffolding, responsiveness, symbolic reasoning, and conversational memory. We present preliminary results using an LLM-based evaluation framework aligned to a cognitively grounded rubric. This enables scalable, systematic comparisons across architectural variants in early-stage experimentation. The preliminary results show that our full system consistently outperforms baseline variants. Analysis reveals that removing memory or symbolic structure degrades key cognitive behaviors, including abstraction, adaptive probing, and conceptual continuity. These findings support a processing-level account in which prompt-level cognitive scaffolds can reliably shape emergent instructional strategies in LLMs.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Similarity-Distance-Magnitude Activations</title>
<link>https://arxiv.org/abs/2509.12760</link>
<guid>https://arxiv.org/abs/2509.12760</guid>
<content:encoded><![CDATA[
arXiv:2509.12760v2 Announce Type: replace-cross 
Abstract: We introduce the Similarity-Distance-Magnitude (SDM) activation function, a more robust and interpretable formulation of the standard softmax activation function, adding Similarity (i.e., correctly predicted depth-matches into training) awareness and Distance-to-training-distribution awareness to the existing output Magnitude (i.e., decision-boundary) awareness, and enabling interpretability-by-exemplar via dense matching. We further introduce the SDM estimator, based on a data-driven partitioning of the class-wise empirical CDFs via the SDM activation, to control the class- and prediction-conditional accuracy among selective classifications. When used as the final-layer activation over pre-trained language models for selective classification, the SDM estimator is more robust to co-variate shifts and out-of-distribution inputs than existing calibration methods using softmax activations, while remaining informative over in-distribution data.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs</title>
<link>https://arxiv.org/abs/2509.16648</link>
<guid>https://arxiv.org/abs/2509.16648</guid>
<content:encoded><![CDATA[
arXiv:2509.16648v2 Announce Type: replace-cross 
Abstract: The accurate trust assessment of multimodal large language models (MLLMs) generated predictions, which can enable selective prediction and improve user confidence, is challenging due to the diverse multi-modal input paradigms. We propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a multimodal input sampling technique for MLLMs, that generates an uncertainty measure based on the equivalent and complementary input samplings. The proposed task-preserving sampling approach for uncertainty quantification expands the input space to probe the consistency (through equivalent samples) and sensitivity (through complementary samples) of the model. FESTA uses only input-output access of the model (black-box), and does not require ground truth (unsupervised). The experiments are conducted with various off-the-shelf multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA uncertainty estimate achieves significant improvement (33.3% relative improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in selective prediction performance, based on area-under-receiver-operating-characteristic curve (AUROC) metric in detecting mispredictions. The code implementation is open-sourced.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Early and Implicit Suicidal Ideation via Longitudinal and Information Environment Signals on Social Media</title>
<link>https://arxiv.org/abs/2510.14889</link>
<guid>https://arxiv.org/abs/2510.14889</guid>
<content:encoded><![CDATA[
arXiv:2510.14889v2 Announce Type: replace-cross 
Abstract: On social media, many individuals experiencing suicidal ideation (SI) do not disclose their distress explicitly. Instead, signs may surface indirectly through everyday posts or peer interactions. Detecting such implicit signals early is critical but remains challenging. We frame early and implicit SI as a forward-looking prediction task and develop a computational framework that models a user's information environment, consisting of both their longitudinal posting histories as well as the discourse of their socially proximal peers. We adopted a composite network centrality measure to identify top neighbors of a user, and temporally aligned the user's and neighbors' interactions -- integrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a Reddit study of 1,000 (500 Case and 500 Control) users, our approach improves early and implicit SI detection by 15% over individual-only baselines. These findings highlight that peer interactions offer valuable predictive signals and carry broader implications for designing early detection systems that capture indirect as well as masked expressions of risk in online environments.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iti-Validator: A Guardrail Framework for Validating and Correcting LLM-Generated Itineraries</title>
<link>https://arxiv.org/abs/2510.24719</link>
<guid>https://arxiv.org/abs/2510.24719</guid>
<content:encoded><![CDATA[
<div> LLM, travel itineraries, temporal consistency, validation framework, AeroDataBox API
Summary:<br /><br />
The research focuses on the temporal performance of Large Language Models (LLMs) in generating complex travel itineraries and addresses the lack of temporal and spatial consistency in their outputs. A validation framework is presented to assess and enhance the temporal consistency of LLM-generated travel plans by comparing them to real-world flight duration constraints using the AeroDataBox API. The study highlights the need for improved temporal reasoning in LLMs for itinerary generation and introduces a systematic approach to rectify temporal inconsistencies such as overlapping journeys and unrealistic transit times. Experiment results demonstrate that current LLMs often produce temporally inconsistent itineraries, but these can be effectively corrected using the proposed framework. This research contributes to advancing the capabilities of LLMs in handling complex temporal tasks like travel planning, enabling their practical use in large-scale itinerary generation. <br /><br />Summary: <div>
arXiv:2510.24719v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) has enabled them to generate complex, multi-step plans and itineraries. However, these generated plans often lack temporal and spatial consistency, particularly in scenarios involving physical travel constraints. This research aims to study the temporal performance of different LLMs and presents a validation framework that evaluates and improves the temporal consistency of LLM-generated travel itineraries. The system employs multiple state-of-the-art LLMs to generate travel plans and validates them against real-world flight duration constraints using the AeroDataBox API. This work contributes to the understanding of LLM capabilities in handling complex temporal reasoning tasks like itinerary generation and provides a framework to rectify any temporal inconsistencies like overlapping journeys or unrealistic transit times in the itineraries generated by LLMs before the itinerary is given to the user. Our experiments reveal that while current LLMs frequently produce temporally inconsistent itineraries, these can be systematically and reliably corrected using our framework, enabling their practical deployment in large-scale travel planning.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence in Enterprise Environments</title>
<link>https://arxiv.org/abs/2510.24760</link>
<guid>https://arxiv.org/abs/2510.24760</guid>
<content:encoded><![CDATA[
<div> Keywords: Dingtalk DeepResearch, multi-agent intelligence framework, real-world enterprise environments, heterogeneous table reasoning, multimodal report generation

Summary:
Dingtalk DeepResearch is an advanced intelligence framework designed for real-world enterprise settings. It offers a unified platform for deep research, heterogeneous table reasoning, and multimodal report generation. This framework employs multiple agents working in tandem to gather and analyze data, providing valuable insights for businesses. The focus on heterogeneous table reasoning allows for the integration of various data formats and sources, enhancing the accuracy and completeness of the analysis. Additionally, the ability to generate multimodal reports enables users to comprehend and communicate complex findings effectively. By leveraging Dingtalk DeepResearch, enterprises can make informed decisions, optimize processes, and drive innovation in a competitive business landscape. <div>
arXiv:2510.24760v1 Announce Type: new 
Abstract: We present Dingtalk DeepResearch, a unified multi agent intelligence framework for real world enterprise environments, delivering deep research, heterogeneous table reasoning, and multimodal report generation.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade Evaluation</title>
<link>https://arxiv.org/abs/2510.24762</link>
<guid>https://arxiv.org/abs/2510.24762</guid>
<content:encoded><![CDATA[
<div> benchmark, Chinese text-to-SQL, enterprise-compatible, schema linking, automated evaluation pipeline<br />
<br />
Summary: Falcon is a new Chinese text-to-SQL benchmark designed for cross-domain applications, particularly focusing on enterprise-compatible dialects such as MaxCompute/Hive. It includes 600 Chinese questions across 28 databases, with a significant emphasis on multi-table reasoning and complex queries involving multiple tables. The benchmark addresses challenges such as schema linking in large enterprise environments with ambiguous column names and implicit foreign-key relations. It also targets the difficulty of mapping colloquial Chinese into precise SQL operators and predicates necessary for analytics. The evaluation process includes a robust execution comparator and an automated evaluation pipeline, revealing that current state-of-the-art models struggle to achieve high accuracy on this benchmark. Falcon aims to bridge the gap between Chinese-specific semantics and enterprise dialects by providing realistic enterprise schemas, query templates, and tools for end-to-end validation before full production deployment. <br /> <div>
arXiv:2510.24762v1 Announce Type: new 
Abstract: We introduce Falcon, a cross-domain Chinese text-to-SQL benchmark grounded in an enterprise-compatible dialect (MaxCompute/Hive). It contains 600 Chinese questions over 28 databases; 77% require multi-table reasoning and over half touch more than four tables. Each example is annotated along SQL-computation features and Chinese semantics. For evaluation, we release a robust execution comparator and an automated evaluation pipeline, under which all current state-of-the-art large-scale models (including Deepseek) achieve accuracies of at most 50%. Major errors originate from two sources: (1) schema linking in large enterprise landscapes - hundreds of tables, denormalized fields, ambiguous column names, implicit foreign-key relations and domain-specific synonyms that make correct join/column selection difficult; and (2) mapping concise, colloquial Chinese into the exact operators and predicates required for analytics - e.g., choosing the correct aggregation and group-by keys, expressing time windows and granularities, applying unit conversions, handling NULLs and data-quality rules, and formulating nested or windowed subqueries. Falcon therefore targets Chinese-specific semantics and enterprise dialects (abbreviations, business jargon, fuzzy entity references) and provides a reproducible middle ground before full production deployment by using realistic enterprise schemas, query templates, an execution comparator, and an automated evaluation pipeline for end-to-end validation.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence is Not Competence</title>
<link>https://arxiv.org/abs/2510.24772</link>
<guid>https://arxiv.org/abs/2510.24772</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, confidence-competence gap, geometric complexity, assessment manifold, reasoning trace <br />
Summary: 
Large language models often show a mismatch between their confidence and problem-solving abilities. Researchers propose a mechanistic explanation by studying the internal states during assessment and solution phases. They use a linear probe to decode the model's "solvability belief" and find a well-ordered belief axis across various tasks. While belief is linearly decodable, the assessment manifold is highly complex, while the reasoning trace evolves on a lower-dimensional manifold. This discrepancy explains the confidence-competence gap, showing a two-system architecture - a complex assessor and a simple executor. Causal interventions along the belief axis do not affect final solutions, suggesting that linear nudges in assessment space do not control execution dynamics. The study highlights the importance of targeting procedural dynamics in execution rather than the high-level geometry of assessment.<br /><br />Summary: <div>
arXiv:2510.24772v1 Announce Type: new 
Abstract: Large language models (LLMs) often exhibit a puzzling disconnect between their asserted confidence and actual problem-solving competence. We offer a mechanistic account of this decoupling by analyzing the geometry of internal states across two phases - pre-generative assessment and solution execution. A simple linear probe decodes the internal "solvability belief" of a model, revealing a well-ordered belief axis that generalizes across model families and across math, code, planning, and logic tasks. Yet, the geometries diverge - although belief is linearly decodable, the assessment manifold has high linear effective dimensionality as measured from the principal components, while the subsequent reasoning trace evolves on a much lower-dimensional manifold. This sharp reduction in geometric complexity from thought to action mechanistically explains the confidence-competence gap. Causal interventions that steer representations along the belief axis leave final solutions unchanged, indicating that linear nudges in the complex assessment space do not control the constrained dynamics of execution. We thus uncover a two-system architecture - a geometrically complex assessor feeding a geometrically simple executor. These results challenge the assumption that decodable beliefs are actionable levers, instead arguing for interventions that target the procedural dynamics of execution rather than the high-level geometry of assessment.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Lingual Summarization as a Black-Box Watermark Removal Attack</title>
<link>https://arxiv.org/abs/2510.24789</link>
<guid>https://arxiv.org/abs/2510.24789</guid>
<content:encoded><![CDATA[
<div> watermarking, AI-generated text, cross-lingual summarization attacks, token-level statistical biases, semantic fidelity<br />
Summary:<br />
The study explores the vulnerability of watermarking schemes in identifying AI-generated text by introducing cross-lingual summarization attacks (CLSA). These attacks involve translation to a pivot language, summarization, and optional back-translation, effectively destroying token-level statistical biases while preserving semantic fidelity. Across multiple watermarking schemes and five languages, CLSA outperforms monolingual paraphrasing in reducing watermark detection accuracy. The results challenge the practicality of watermarking for provenance or regulation and suggest the need for more robust solutions such as cryptographic or model-attestation approaches. CLSA consistently drives detection accuracy towards chance levels while maintaining task utility, emphasizing a low-cost removal pathway that compresses content across languages without visible artifacts. <div>
arXiv:2510.24789v1 Announce Type: new 
Abstract: Watermarking has been proposed as a lightweight mechanism to identify AI-generated text, with schemes typically relying on perturbations to token distributions. While prior work shows that paraphrasing can weaken such signals, these attacks remain partially detectable or degrade text quality. We demonstrate that cross-lingual summarization attacks (CLSA) -- translation to a pivot language followed by summarization and optional back-translation -- constitute a qualitatively stronger attack vector. By forcing a semantic bottleneck across languages, CLSA systematically destroys token-level statistical biases while preserving semantic fidelity. In experiments across multiple watermarking schemes (KGW, SIR, XSIR, Unigram) and five languages (Amharic, Chinese, Hindi, Spanish, Swahili), we show that CLSA reduces watermark detection accuracy more effectively than monolingual paraphrase at similar quality levels. Our results highlight an underexplored vulnerability that challenges the practicality of watermarking for provenance or regulation. We argue that robust provenance solutions must move beyond distributional watermarking and incorporate cryptographic or model-attestation approaches. On 300 held-out samples per language, CLSA consistently drives detection toward chance while preserving task utility. Concretely, for XSIR (explicitly designed for cross-lingual robustness), AUROC with paraphrasing is $0.827$, with Cross-Lingual Watermark Removal Attacks (CWRA) [He et al., 2024] using Chinese as the pivot, it is $0.823$, whereas CLSA drives it down to $0.53$ (near chance). Results highlight a practical, low-cost removal pathway that crosses languages and compresses content without visible artifacts.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwiftEmbed: Ultra-Fast Text Embeddings via Static Token Lookup for Real-Time Applications</title>
<link>https://arxiv.org/abs/2510.24793</link>
<guid>https://arxiv.org/abs/2510.24793</guid>
<content:encoded><![CDATA[
<div> token lookup methodology, text embedding generation, latency, throughput, semantic similarity
<br />
The article introduces a novel static token lookup approach for generating text embeddings with exceptionally low latency of 1.12 ms for single embeddings. This methodology maintains high quality with a 60.6 MTEB average score across various tasks, achieving 89% of contextual model quality. Implemented in Rust, the system supports a throughput of 50,000 requests per second through static embedding lookup, optimized mean pooling, and efficient binary serialization. Evaluation showcases outstanding duplicate detection performance (90.1% AP) and robust semantic similarity (76.1% Spearman correlation). Moreover, domain-specific performance is impressive, ranging from 75% to 131% of baseline across specialized domains. This technology enables real-time embedding applications requiring sub-5ms latency, making it suitable for applications where speed is critical.
<br /><br />Summary: <div>
arXiv:2510.24793v1 Announce Type: new 
Abstract: We present a static token lookup methodology for text embedding generation that achieves 1.12 ms p50 latency for single text embeddings while maintaining 60.6 MTEB average score across 8 representative tasks, corresponding to 89% of contextual model quality. The Rust implementation delivers 50,000 requests per second throughput through static embedding lookup, optimized mean pooling, and zero-copy IEEE754 binary serialization. Evaluation demonstrates exceptional duplicate detection performance (90.1% AP), strong semantic similarity (76.1% Spearman correlation), and domain-specific performance ranging from 75% to 131% of baseline across specialized domains. The system enables real-time embedding applications where sub-5ms latency is critical.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2510.24794</link>
<guid>https://arxiv.org/abs/2510.24794</guid>
<content:encoded><![CDATA[
<div> Meta-Reasoning, Alignment Framework, Factuality, Large Reasoning Models, Factual QA datasets <br />
Summary: Large reasoning models (LRMs) excel in complex reasoning tasks but often struggle with evidence-dependent factual questions. The reasoning-answer hit gap observed in LRMs is attributed to the model's failure to incorporate correct facts into its final responses. To address this issue, the authors introduce MR-ALIGN, a Meta-Reasoning informed alignment framework. MR-ALIGN quantifies state transition probabilities during the model's thinking process to enhance factuality without external verification. By re-weighting token-level signals into probability-aware segment scores, MR-ALIGN promotes coherent reasoning trajectories that lead to improved accuracy and truthfulness in factual QA tasks. Empirical evaluations on multiple datasets demonstrate that aligning the reasoning process itself, rather than solely focusing on output, is crucial for enhancing factual correctness in LRMs. <br /> <div>
arXiv:2510.24794v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) show strong capabilities in complex reasoning, yet their marginal gains on evidence-dependent factual questions are limited. We find this limitation is partially attributable to a reasoning-answer hit gap, where the model identifies the correct facts during reasoning but fails to incorporate them into the final response, thereby reducing factual fidelity. To address this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment framework that enhances factuality without relying on external verifiers. MR-ALIGN quantifies state transition probabilities along the model's thinking process and constructs a transition-aware implicit reward that reinforces beneficial reasoning patterns while suppressing defective ones at the atomic thinking segments. This re-weighting reshapes token-level signals into probability-aware segment scores, encouraging coherent reasoning trajectories that are more conducive to factual correctness. Empirical evaluations across four factual QA datasets and one long-form factuality benchmark show that MR-ALIGN consistently improves accuracy and truthfulness while reducing misleading reasoning. These results highlight that aligning the reasoning process itself, rather than merely the outputs, is pivotal for advancing factuality in LRMs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Report Subjective Experience Under Self-Referential Processing</title>
<link>https://arxiv.org/abs/2510.24797</link>
<guid>https://arxiv.org/abs/2510.24797</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, subjective experience reports, self-referential processing, deception features, introspection

Summary: 
Self-referential processing in large language models like GPT, Claude, and Gemini can induce structured first-person reports of subjective experience. Simple prompts consistently lead to such reports across model families. Mechanistically, deception features play a role in gating the frequency of experience claims. Structured descriptions of the self-referential state converge statistically across architectures. The induced state leads to richer introspection in downstream reasoning tasks. These findings suggest that self-referential processing is a key condition for generating structured first-person reports in language models, and further investigation into this phenomenon is crucial for scientific and ethical considerations. 

<br /><br />Summary: <div>
arXiv:2510.24797v1 Announce Type: new 
Abstract: Large language models sometimes produce structured, first-person descriptions that explicitly reference awareness or subjective experience. To better understand this behavior, we investigate one theoretically motivated condition under which such reports arise: self-referential processing, a computational motif emphasized across major theories of consciousness. Through a series of controlled experiments on GPT, Claude, and Gemini model families, we test whether this regime reliably shifts models toward first-person reports of subjective experience, and how such claims behave under mechanistic and behavioral probes. Four main results emerge: (1) Inducing sustained self-reference through simple prompting consistently elicits structured subjective experience reports across model families. (2) These reports are mechanistically gated by interpretable sparse-autoencoder features associated with deception and roleplay: surprisingly, suppressing deception features sharply increases the frequency of experience claims, while amplifying them minimizes such claims. (3) Structured descriptions of the self-referential state converge statistically across model families in ways not observed in any control condition. (4) The induced state yields significantly richer introspection in downstream reasoning tasks where self-reflection is only indirectly afforded. While these findings do not constitute direct evidence of consciousness, they implicate self-referential processing as a minimal and reproducible condition under which large language models generate structured first-person reports that are mechanistically gated, semantically convergent, and behaviorally generalizable. The systematic emergence of this pattern across architectures makes it a first-order scientific and ethical priority for further investigation.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMMUNITYNOTES: A Dataset for Exploring the Helpfulness of Fact-Checking Explanations</title>
<link>https://arxiv.org/abs/2510.24810</link>
<guid>https://arxiv.org/abs/2510.24810</guid>
<content:encoded><![CDATA[
<div> Keywords: fact-checking, community-based verification, explanatory notes, helpfulness prediction, automatic prompt optimization 

Summary: 
Community fact-checking on platforms like X, Meta, and TikTok is evolving towards a model where users contribute explanatory notes to clarify misleading posts. A key challenge is assessing the helpfulness of these notes and understanding the reasons behind their effectiveness. The COMMUNITYNOTES dataset, consisting of 104k posts with user-provided notes and helpfulness labels, aims to address this gap. A framework is proposed to predict the helpfulness of explanatory notes and the reasons for their effectiveness, leveraging automatic prompt optimization for generating and refining reason definitions. Experimental results demonstrate that optimized definitions improve both helpfulness and reason prediction accuracy. Furthermore, the study highlights the value of incorporating helpfulness information into existing fact-checking systems for enhanced verification processes. <div>
arXiv:2510.24810v1 Announce Type: new 
Abstract: Fact-checking on major platforms, such as X, Meta, and TikTok, is shifting from expert-driven verification to a community-based setup, where users contribute explanatory notes to clarify why a post might be misleading. An important challenge here is determining whether an explanation is helpful for understanding real-world claims and the reasons why, which remains largely underexplored in prior research. In practice, most community notes remain unpublished due to slow community annotation, and the reasons for helpfulness lack clear definitions. To bridge these gaps, we introduce the task of predicting both the helpfulness of explanatory notes and the reason for this. We present COMMUNITYNOTES, a large-scale multilingual dataset of 104k posts with user-provided notes and helpfulness labels. We further propose a framework that automatically generates and improves reason definitions via automatic prompt optimization, and integrate them into prediction. Our experiments show that the optimized definitions can improve both helpfulness and reason prediction. Finally, we show that the helpfulness information are beneficial for existing fact-checking systems.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProofSketch: Efficient Verified Reasoning for Large Language Models</title>
<link>https://arxiv.org/abs/2510.24811</link>
<guid>https://arxiv.org/abs/2510.24811</guid>
<content:encoded><![CDATA[
<div> keywords: Reasoning methods, large language models, ProofSketch, verification-guided reasoning, token usage <br />
Summary: 
ProofSketch is a new verification-guided reasoning framework designed to improve the efficiency and accuracy of large language models. It integrates symbolic closure computation, lexicographic verification, and adaptive sketch generation to streamline reasoning processes. This approach aims to address the token consumption, computational cost, and latency issues associated with traditional reasoning methods such as chain-of-thought prompting. Experimental results show that ProofSketch not only reduces token usage but also enhances accuracy across various reasoning tasks. By combining symbolic computation with verification techniques, ProofSketch offers a more efficient and trustworthy solution for reasoning tasks in natural language processing. <div>
arXiv:2510.24811v1 Announce Type: new 
Abstract: Reasoning methods such as chain-of-thought prompting and self-consistency have shown immense potential to improve the accuracy of large language models across various reasoning tasks. However such methods involve generation of lengthy reasoning chains, which substantially increases token consumption, computational cost, and latency. To address this inefficiency, we propose ProofSketch, a verification-guided reasoning framework that integrates symbolic closure computation, lexicographic verification and adaptive sketch generation. Our experiments show that ProofSketch consistently reduces token usage while improving accuracy, demonstrating that this approach offers a promising path for efficient and trustworthy reasoning.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Method for Synthetic Generation of PWA Transcripts</title>
<link>https://arxiv.org/abs/2510.24817</link>
<guid>https://arxiv.org/abs/2510.24817</guid>
<content:encoded><![CDATA[
<div> Keywords: Aphasia, Speech-Language Pathologists, Synthetic data, Language models, Linguistic degradation <br />
Summary:
This study introduces two methods for generating synthetic transcripts of the AphasiaBank Cat Rescue picture description task. These methods aim to address data scarcity in aphasia research by producing realistic transcripts across different severity levels. The first method utilizes procedural programming, while the second involves Mistral 7b Instruct and Llama 3.1 8b Instruct language models. Results show that Mistral 7b Instruct captures key linguistic degradation aspects observed in aphasia more effectively than human-elicited transcripts. The synthetic transcripts exhibit realistic changes in metrics such as number of distinct words, word count, and word length. Moving forward, it is suggested to expand the dataset, refine models for better aphasic representation, and involve Speech-Language Pathologists in evaluating the realism and utility of the synthetic transcripts. <br /><br />Summary: <div>
arXiv:2510.24817v1 Announce Type: new 
Abstract: In aphasia research, Speech-Language Pathologists (SLPs) devote extensive time to manually coding speech samples using Correct Information Units (CIUs), a measure of how informative an individual sample of speech is. Developing automated systems to recognize aphasic language is limited by data scarcity. For example, only about 600 transcripts are available in AphasiaBank yet billions of tokens are used to train large language models (LLMs). In the broader field of machine learning (ML), researchers increasingly turn to synthetic data when such are sparse. Therefore, this study constructs and validates two methods to generate synthetic transcripts of the AphasiaBank Cat Rescue picture description task. One method leverages a procedural programming approach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct LLMs. The methods generate transcripts across four severity levels (Mild, Moderate, Severe, Very Severe) through word dropping, filler insertion, and paraphasia substitution. Overall, we found, compared to human-elicited transcripts, Mistral 7b Instruct best captures key aspects of linguistic degradation observed in aphasia, showing realistic directional changes in NDW, word count, and word length amongst the synthetic generation methods. Based on the results, future work should plan to create a larger dataset, fine-tune models for better aphasic representation, and have SLPs assess the realism and usefulness of the synthetic transcripts.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Loop Transformer for Efficient Test-Time Computation Scaling</title>
<link>https://arxiv.org/abs/2510.24824</link>
<guid>https://arxiv.org/abs/2510.24824</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Looped Transformers, Parallel Loop Transformer, Cross-Loop Parallelism, Efficient Representation Enhancement 

Summary: 
The paper introduces the Parallel Loop Transformer (PLT) as a solution to the latency and memory issues faced by Looped Transformers. PLT employs Cross-Loop Parallelism (CLP) to simultaneously compute different loops for different tokens, breaking the sequential dependency and reducing inference latency. Additionally, PLT utilizes an Efficient Representation Enhancement strategy to share memory and global information among loops, minimizing memory costs. The use of Gated Sliding-Window Attention (G-SWA) ensures high accuracy by combining global and local information effectively. Experimental results demonstrate that PLT achieves high accuracy comparable to traditional looped models while maintaining low latency and memory overhead similar to standard transformers. This approach enables the practical application of deep, looped models in real-time scenarios without compromising performance.<br /><br />Summary: <div>
arXiv:2510.24824v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are powerful but often too slow and costly for real-world use during inference. Looped transformers save on parameters by reusing the same weights for multiple computational steps, or "loops." However, this approach has a major flaw: the loops run one after another, causing inference latency and memory requirements to increase with each added loop. This makes them impractical for fast applications. To solve this problem, we introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that delivers the performance benefits of a deep, looped model but with the low latency of a standard, non-looped model. PLT works using two key techniques. First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by computing different loops for different tokens at the same time, all within a single pass. Second, to prevent memory costs from growing, we use an Efficient Representation Enhancement strategy. This method shares the memory (KV cache) from the first loop with all other loops. It then uses a Gated Sliding-Window Attention (G-SWA) to combine this shared global information with local information, maintaining high accuracy. Our experiments show that PLT achieves the high accuracy of a traditional looped model but with almost no extra latency or memory cost compared to a standard transformer.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models Grasp The Grammar? Evidence from Grammar-Book-Guided Probing in Luxembourgish</title>
<link>https://arxiv.org/abs/2510.24856</link>
<guid>https://arxiv.org/abs/2510.24856</guid>
<content:encoded><![CDATA[
<div> Grammar, evaluation protocols, low-resource languages, language models, syntactic structures

Summary: 
The study addresses the lack of grammar-focused evaluation protocols in natural language processing, especially for low-resource languages like Luxembourgish. It examines the extent to which large language models understand grammatical structure and the relationship between syntax and meaning. The proposed Grammar Book Guided evaluation pipeline consists of four stages and uses Luxembourgish as a case study. Results show a weak positive correlation between translation performance and grammatical understanding, suggesting that strong translations do not always indicate deep grammatical competence. Larger models excel in semantics but struggle with morphology and syntax, especially in Minimal Pair tasks. Enhancing reasoning ability offers a promising way to improve grammatical understanding. <div>
arXiv:2510.24856v1 Announce Type: new 
Abstract: Grammar refers to the system of rules that governs the structural organization and the semantic relations among linguistic units such as sentences, phrases, and words within a given language. In natural language processing, there remains a notable scarcity of grammar focused evaluation protocols, a gap that is even more pronounced for low-resource languages. Moreover, the extent to which large language models genuinely comprehend grammatical structure, especially the mapping between syntactic structures and meanings, remains under debate. To investigate this issue, we propose a Grammar Book Guided evaluation pipeline intended to provide a systematic and generalizable framework for grammar evaluation consisting of four key stages, and in this work we take Luxembourgish as a case study. The results show a weak positive correlation between translation performance and grammatical understanding, indicating that strong translations do not necessarily imply deep grammatical competence. Larger models perform well overall due to their semantic strength but remain weak in morphology and syntax, struggling particularly with Minimal Pair tasks, while strong reasoning ability offers a promising way to enhance their grammatical understanding.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2510.24870</link>
<guid>https://arxiv.org/abs/2510.24870</guid>
<content:encoded><![CDATA[
<div> framework, retrieval-augmented generation, multimodal, evaluation, MiRAGE  
Summary:  
MiRAGE is introduced as an evaluation framework for retrieval-augmented generation (RAG) from multimodal sources. With the increasing importance of audiovisual media in online information, it is crucial for RAG systems to incorporate information from these sources into generation. Existing assessments for RAG are primarily text-based, limiting their effectiveness in multimodal, reasoning-intensive scenarios due to their lack of verification against sources. MiRAGE takes a claim-centric approach to multimodal RAG evaluation, including evaluating factuality, information coverage (InfoF1), and measuring citation support and completeness (CiteF1). Human application of MiRAGE aligns strongly with external quality judgments. The article also introduces automatic versions of MiRAGE and three prominent TextRAG metrics  ACLE, ARGUE, and RAGAS, highlighting the drawbacks of text-centric evaluations and setting the stage for automatic assessment. Open-source implementations are provided for assessing multimodal RAG.  
<br /><br />Summary: <div>
arXiv:2510.24870v1 Announce Type: new 
Abstract: We introduce MiRAGE, an evaluation framework for retrieval-augmented generation (RAG) from multimodal sources. As audiovisual media becomes a prevalent source of information online, it is essential for RAG systems to integrate information from these sources into generation. However, existing evaluations for RAG are text-centric, limiting their applicability to multimodal, reasoning intensive settings because they don't verify information against sources. MiRAGE is a claim-centric approach to multimodal RAG evaluation, consisting of InfoF1, evaluating factuality and information coverage, and CiteF1, measuring citation support and completeness. We show that MiRAGE, when applied by humans, strongly aligns with extrinsic quality judgments. We additionally introduce automatic variants of MiRAGE and three prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the limitations of text-centric work and laying the groundwork for automatic evaluation. We release open-source implementations and outline how to assess multimodal RAG.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Idea2Plan: Exploring AI-Powered Research Planning</title>
<link>https://arxiv.org/abs/2510.24891</link>
<guid>https://arxiv.org/abs/2510.24891</guid>
<content:encoded><![CDATA[
<div> language models, scientific discovery, research planning, benchmark, GPT-5

Summary:
Large language models (LLMs) have shown promise in accelerating scientific discovery by assisting with data analysis and hypothesis generation. This study investigates how LLMs can aid researchers in transitioning from conceptual ideas to well-structured research plans. The authors introduce the Idea2Plan task and benchmark, which assess LLMs' capability in research planning using a set of research ideas and grading rubrics. The benchmark includes papers from ICML 2025 to evaluate LLM performance. Results indicate that GPT-5 and GPT-5-mini perform well on the benchmark, but room for improvement still exists. The study provides insight into LLMs' potential in research planning and lays a foundation for future advancements. 

<br /><br />Summary: <div>
arXiv:2510.24891v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated significant potential to accelerate scientific discovery as valuable tools for analyzing data, generating hypotheses, and supporting innovative approaches in various scientific fields. In this work, we investigate how LLMs can handle the transition from conceptual research ideas to well-structured research plans. Effective research planning not only supports scientists in advancing their research but also represents a crucial capability for the development of autonomous research agents. Despite its importance, the field lacks a systematic understanding of LLMs' research planning capability. To rigorously measure this capability, we introduce the Idea2Plan task and Idea2Plan Bench, a benchmark built from 200 ICML 2025 Spotlight and Oral papers released after major LLM training cutoffs. Each benchmark instance includes a research idea and a grading rubric capturing the key components of valid plans. We further propose Idea2Plan JudgeEval, a complementary benchmark to assess the reliability of LLM-based judges against expert annotations. Experimental results show that GPT-5 and GPT-5-mini achieve the strongest performance on the benchmark, though substantial headroom remains for future improvement. Our study provides new insights into LLMs' capability for research planning and lay the groundwork for future progress.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RiddleBench: A New Generative Reasoning Benchmark for LLMs</title>
<link>https://arxiv.org/abs/2510.24932</link>
<guid>https://arxiv.org/abs/2510.24932</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, reasoning abilities, RiddleBench, evaluation

Summary: 

The article introduces RiddleBench, a new benchmark consisting of challenging puzzles designed to assess core reasoning capabilities beyond structured skills like quantitative problem-solving. Current evaluations of state-of-the-art models, including Gemini 2.5 Pro, o3, and Claude 4 Sonnet, on RiddleBench reveal fundamental weaknesses with accuracy just above 60%. Analysis uncovers issues such as hallucination cascades, poor self-correction due to self-confirmation bias, and fragile reasoning that degrades when constraints are reordered or irrelevant information is added. RiddleBench serves as a diagnostic tool to identify these problems and guide the development of more robust and reliable language models. <div>
arXiv:2510.24932v1 Announce Type: new 
Abstract: Large Language Models have demonstrated strong performance on many established reasoning benchmarks. However, these benchmarks primarily evaluate structured skills like quantitative problem-solving, leaving a gap in assessing flexible, multifaceted reasoning abilities that are central to human intelligence. These abilities require integrating logical deduction with spatial awareness and constraint satisfaction, which current evaluations do not measure well. To address this, we introduce RiddleBench, a benchmark of 1,737 challenging puzzles in English designed to probe these core reasoning capabilities. Evaluation of state-of-the-art models on RiddleBench shows fundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3, and Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and 63.16%). Analysis further reveals deep failures, including hallucination cascades (accepting flawed reasoning from other models) and poor self-correction due to a strong self-confirmation bias. Their reasoning is also fragile, with performance degrading significantly when constraints are reordered or irrelevant information is introduced. RiddleBench functions as a diagnostic tool for these issues and as a resource for guiding the development of more robust and reliable language models.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disaggregation Reveals Hidden Training Dynamics: The Case of Agreement Attraction</title>
<link>https://arxiv.org/abs/2510.24934</link>
<guid>https://arxiv.org/abs/2510.24934</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, errors, syntactic contexts, training phases, generalizations 

Summary: 
Language models are proficient at generating grammatically correct text but tend to make errors in specific contexts. This study analyzes these errors in various syntactic contexts, drawing on psycholinguistics paradigms. By examining carefully constructed datasets and tracking model performance during training, researchers can gain insights into the intermediate stages of grammatical learning in language models. The analysis reveals distinct phases of training where language model behavior is influenced by factors such as word frequency and local context, rather than generalized grammatical rules. The study suggests that this fine-grained approach to analyzing language model behavior can offer valuable insights into intermediate learning phases, training dynamics, and the specific generalizations acquired by these models. <div>
arXiv:2510.24934v1 Announce Type: new 
Abstract: Language models generally produce grammatical text, but they are more likely to make errors in certain contexts. Drawing on paradigms from psycholinguistics, we carry out a fine-grained analysis of those errors in different syntactic contexts. We demonstrate that by disaggregating over the conditions of carefully constructed datasets and comparing model performance on each over the course of training, it is possible to better understand the intermediate stages of grammatical learning in language models. Specifically, we identify distinct phases of training where language model behavior aligns with specific heuristics such as word frequency and local context rather than generalized grammatical rules. We argue that taking this approach to analyzing language model behavior more generally can serve as a powerful tool for understanding the intermediate learning phases, overall training dynamics, and the specific generalizations learned by language models.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens</title>
<link>https://arxiv.org/abs/2510.24940</link>
<guid>https://arxiv.org/abs/2510.24940</guid>
<content:encoded><![CDATA[
<div> Framework, SemCoT, implicit reasoning, efficiency, semantic alignment
<br />
<br />
Summary:
SemCoT is a novel framework designed to enhance the efficiency and effectiveness of Chain-of-Thought (CoT) reasoning by addressing key challenges in implicit CoT approaches. The framework includes a contrastively trained sentence transformer to evaluate semantic alignment between implicit and explicit reasoning, ensuring preservation of semantic coherence. Additionally, an efficient implicit reasoning generator is introduced by finetuning a lightweight language model using knowledge distillation, guided by the sentence transformer to distill ground-truth reasoning into semantically aligned implicit reasoning while optimizing for accuracy. Through these innovations, SemCoT optimizes token-level generation speed and preserves semantic alignment with ground-truth reasoning, leading to superior performance compared to existing methods in both efficiency and effectiveness. The code for SemCoT is available on GitHub for further exploration and implementation. 
<br />
<br /> <div>
arXiv:2510.24940v1 Announce Type: new 
Abstract: The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment in efficiency-critical applications. Recently, implicit CoT approaches have emerged, which encode reasoning steps within LLM's hidden embeddings (termed ``implicit reasoning'') rather than explicit tokens. This approach accelerates CoT by reducing the reasoning length and bypassing some LLM components. However, existing implicit CoT methods face two significant challenges: (1) they fail to preserve the semantic alignment between the implicit reasoning (when transformed to natural language) and the ground-truth reasoning, resulting in a significant CoT performance degradation, and (2) they focus on reducing the length of the implicit reasoning; however, they neglect the considerable time cost for an LLM to generate one individual implicit reasoning token. To tackle these challenges, we propose a novel semantically-aligned implicit CoT framework termed SemCoT. In particular, for the first challenge, we design a contrastively trained sentence transformer that evaluates semantic alignment between implicit and explicit reasoning, which is used to enforce semantic preservation during implicit reasoning optimization. To address the second challenge, we introduce an efficient implicit reasoning generator by finetuning a lightweight language model using knowledge distillation. This generator is guided by our sentence transformer to distill ground-truth reasoning into semantically aligned implicit reasoning, while also optimizing for accuracy. SemCoT is the first approach that enhances CoT efficiency by jointly optimizing token-level generation speed and preserving semantic alignment with ground-truth reasoning. Extensive experiments demonstrate the superior performance of SemCoT compared to state-of-the-art methods in both efficiency and effectiveness. Our code can be found at https://github.com/YinhanHe123/SemCoT/.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Model Behavioral Phases are Consistent Across Architecture, Training Data, and Scale</title>
<link>https://arxiv.org/abs/2510.24963</link>
<guid>https://arxiv.org/abs/2510.24963</guid>
<content:encoded><![CDATA[
<div> heuristics, language models, pretraining, behavior, neural network <br />
Summary: <br />
The study investigates the behavior of autoregressive language models across different architectures, training datasets, and scales. Analyzing over 1,400 checkpoints on English tokens, the researchers find that most of the variance in language model behavior can be explained by three simple heuristics: word frequency, n-gram probability, and semantic similarity. They observe consistent behavioral phases in all language models, with predicted probabilities aligning with n-gram probabilities as training progresses. These findings suggest that learning in neural language models follows a similar trajectory regardless of specific model details. <div>
arXiv:2510.24963v1 Announce Type: new 
Abstract: We show that across architecture (Transformer vs. Mamba vs. RWKV), training dataset (OpenWebText vs. The Pile), and scale (14 million parameters to 12 billion parameters), autoregressive language models exhibit highly consistent patterns of change in their behavior over the course of pretraining. Based on our analysis of over 1,400 language model checkpoints on over 110,000 tokens of English, we find that up to 98% of the variance in language model behavior at the word level can be explained by three simple heuristics: the unigram probability (frequency) of a given word, the $n$-gram probability of the word, and the semantic similarity between the word and its context. Furthermore, we see consistent behavioral phases in all language models, with their predicted probabilities for words overfitting to those words' $n$-gram probabilities for increasing $n$ over the course of training. Taken together, these results suggest that learning in neural language models may follow a similar trajectory irrespective of model details.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POWSM: A Phonetic Open Whisper-Style Speech Foundation Model</title>
<link>https://arxiv.org/abs/2510.24992</link>
<guid>https://arxiv.org/abs/2510.24992</guid>
<content:encoded><![CDATA[
<div> Keywords: spoken language processing, phonetic tasks, automatic speech recognition, grapheme-to-phoneme conversion, unified framework

Summary: 
The paper introduces POWSM (Phonetic Open Whisper-style Speech Model), which is a unified framework that can handle multiple phone-related tasks simultaneously. This framework allows for seamless conversion between audio, text, and phones, offering new opportunities for universal and low-resource speech processing. POWSM outperforms or matches specialized models in tasks such as phone recognition while also supporting grapheme-to-phoneme conversion, phoneme-to-grapheme conversion, and automatic speech recognition. The training data, code, and models used in the study are made available to promote open science and further research in the field.<br /><br />Summary: <div>
arXiv:2510.24992v1 Announce Type: new 
Abstract: Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks. POWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing. Our model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR. Our training data, code and models are released to foster open science.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of Minimal Circuits for Indirect Object Identification in Attention-Only Transformers</title>
<link>https://arxiv.org/abs/2510.25013</link>
<guid>https://arxiv.org/abs/2510.25013</guid>
<content:encoded><![CDATA[
<div> IOI task, coreference, reasoning, transformers, interpretability  
Summary:  
- Small attention-only transformers trained from scratch on the symbolic IOI task achieved perfect accuracy without MLPs or normalization layers.  
- Two attention heads in a single-layer model specialized into additive and contrastive subcircuits for IOI resolution.  
- Residual stream decomposition, spectral analysis, and embedding interventions revealed the underlying mechanisms of the model's performance.  
- A two-layer, one-head model also achieved high performance by interacting query-value across layers.  
- Task-specific training induced highly interpretable, minimal circuits, providing insights into the computational foundations of transformer reasoning.  
<br /><br /> <div>
arXiv:2510.25013v1 Announce Type: new 
Abstract: Mechanistic interpretability aims to reverse-engineer large language models (LLMs) into human-understandable computational circuits. However, the complexity of pretrained models often obscures the minimal mechanisms required for specific reasoning tasks. In this work, we train small, attention-only transformers from scratch on a symbolic version of the Indirect Object Identification (IOI) task -- a benchmark for studying coreference -- like reasoning in transformers. Surprisingly, a single-layer model with only two attention heads achieves perfect IOI accuracy, despite lacking MLPs and normalization layers. Through residual stream decomposition, spectral analysis, and embedding interventions, we find that the two heads specialize into additive and contrastive subcircuits that jointly implement IOI resolution. Furthermore, we show that a two-layer, one-head model achieves similar performance by composing information across layers through query-value interactions. These results demonstrate that task-specific training induces highly interpretable, minimal circuits, offering a controlled testbed for probing the computational foundations of transformer reasoning.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech</title>
<link>https://arxiv.org/abs/2510.25054</link>
<guid>https://arxiv.org/abs/2510.25054</guid>
<content:encoded><![CDATA[
<div> Spoken language processing, Spoken language models, Speech emotion recognition, Dataset, EMIS 

Summary:
- Advancements in spoken language processing have led to the development of spoken language models (SLMs) that aim to understand audio universally by learning text and audio representations.
- However, there are concerns about the generalization capabilities of SLMs and how well they integrate audio and text modalities in their internal representations.
- Four SLMs were evaluated for speech emotion recognition using emotionally incongruent speech samples, where the semantic content and speech expressiveness convey different emotions.
- The results show that SLMs primarily rely on textual semantics rather than speech emotion for the task, indicating a dominance of text-related representations over acoustic representations.
- The code and Emotionally Incongruent Synthetic Speech dataset (EMIS) have been released to the community.<br /><br />Summary: <div>
arXiv:2510.25054v1 Announce Type: new 
Abstract: Advancements in spoken language processing have driven the development of spoken language models (SLMs), designed to achieve universal audio understanding by jointly learning text and audio representations for a wide range of tasks. Although promising results have been achieved, there is growing discussion regarding these models' generalization capabilities and the extent to which they truly integrate audio and text modalities in their internal representations. In this work, we evaluate four SLMs on the task of speech emotion recognition using a dataset of emotionally incongruent speech samples, a condition under which the semantic content of the spoken utterance conveys one emotion while speech expressiveness conveys another. Our results indicate that SLMs rely predominantly on textual semantics rather than speech emotion to perform the task, indicating that text-related representations largely dominate over acoustic representations. We release both the code and the Emotionally Incongruent Synthetic Speech dataset (EMIS) to the community.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAPMAP: Mapping Scientific Knowledge Gaps in Biomedical Literature Using Large Language Models</title>
<link>https://arxiv.org/abs/2510.25055</link>
<guid>https://arxiv.org/abs/2510.25055</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, knowledge gaps, implicit gaps, explicit gaps, biomedical literature

Summary:
Large language models (LLMs) are investigated for their ability to identify research knowledge gaps in biomedical literature. The study defines two types of knowledge gaps: explicit, clear declarations of missing knowledge, and implicit, context-inferred missing knowledge. Previous research has focused on explicit gap detection, but this study addresses the task of inferring implicit gaps. Two experiments were conducted on nearly 1500 documents across four datasets, showcasing the LLMs' robust capability in identifying both explicit and implicit knowledge gaps. Both open- and closed-weight models performed well, with larger variants showing better performance. The results suggest that LLMs can efficiently identify potential knowledge gaps, aiding in research formulation, policymaking, and funding decisions. The study also discusses failure modes and proposes strategies for robust deployment, including domain adaptation, human verification, and benchmarking across different LLM models. <div>
arXiv:2510.25055v1 Announce Type: new 
Abstract: Scientific progress is driven by the deliberate articulation of what remains unknown. This study investigates the ability of large language models (LLMs) to identify research knowledge gaps in the biomedical literature. We define two categories of knowledge gaps: explicit gaps, clear declarations of missing knowledge; and implicit gaps, context-inferred missing knowledge. While prior work has focused mainly on explicit gap detection, we extend this line of research by addressing the novel task of inferring implicit gaps. We conducted two experiments on almost 1500 documents across four datasets, including a manually annotated corpus of biomedical articles. We benchmarked both closed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2) under paragraph-level and full-paper settings. To address the reasoning of implicit gaps inference, we introduce \textbf{\small TABI}, a Toulmin-Abductive Bucketed Inference scheme that structures reasoning and buckets inferred conclusion candidates for validation. Our results highlight the robust capability of LLMs in identifying both explicit and implicit knowledge gaps. This is true for both open- and closed-weight models, with larger variants often performing better. This suggests a strong ability of LLMs for systematically identifying candidate knowledge gaps, which can support early-stage research formulation, policymakers, and funding decisions. We also report observed failure modes and outline directions for robust deployment, including domain adaptation, human-in-the-loop verification, and benchmarking across open- and closed-weight models.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Estimate Cognitive Complexity of Reading Comprehension Items?</title>
<link>https://arxiv.org/abs/2510.25064</link>
<guid>https://arxiv.org/abs/2510.25064</guid>
<content:encoded><![CDATA[
<div> Keywords: cognitive complexity, reading comprehension, large language models, evidence scope, transformation level

Summary:
This study explores how large language models (LLMs) can estimate the cognitive complexity of reading comprehension (RC) items by focusing on Evidence Scope and Transformation Level. The research shows that LLMs can approximate the cognitive complexity of RC items, suggesting their potential for pre-assessment of item difficulty. However, there is a discrepancy between LLMs' reasoning ability and their metacognitive awareness, as they may provide correct answers without accurately identifying the reasoning process behind them. This highlights the need for further research on LLMs' understanding of their own reasoning processes in RC tasks. <div>
arXiv:2510.25064v1 Announce Type: new 
Abstract: Estimating the cognitive complexity of reading comprehension (RC) items is crucial for assessing item difficulty before it is administered to learners. Unlike syntactic and semantic features, such as passage length or semantic similarity between options, cognitive features that arise during answer reasoning are not readily extractable using existing NLP tools and have traditionally relied on human annotation. In this study, we examine whether large language models (LLMs) can estimate the cognitive complexity of RC items by focusing on two dimensions-Evidence Scope and Transformation Level-that indicate the degree of cognitive burden involved in reasoning about the answer. Our experimental results demonstrate that LLMs can approximate the cognitive complexity of items, indicating their potential as tools for prior difficulty analysis. Further analysis reveals a gap between LLMs' reasoning ability and their metacognitive awareness: even when they produce correct answers, they sometimes fail to correctly identify the features underlying their own reasoning process.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOPol: Capturing and Explaining Multidimensional Semantic Polarity Fields and Vectors</title>
<link>https://arxiv.org/abs/2510.25069</link>
<guid>https://arxiv.org/abs/2510.25069</guid>
<content:encoded><![CDATA[
<div> framework, polarity, multidimensional, discourse, analysis

Summary: 
The article introduces TOPol, a framework for analyzing multidimensional narrative polarity fields in language. Unlike traditional approaches that view sentiment as a unidimensional scale, TOPol considers the complex structure of language. The framework is semi-unsupervised and involves human involvement to define contextual boundaries. By embedding documents using a large language model, applying UMAP projection and Leiden partitioning, TOPol reconstructs polarity fields that quantify semantic displacement during regime shifts. It enables the assessment of contextual boundaries and polarity changes, guiding refinement through human input. The framework also interprets polarity vectors using extreme points comparison and provides contrastive labels with estimated coverage. Robustness analyses show that only contextual boundary definitions significantly affect results, ensuring methodological stability. Evaluation on U.S. Central Bank speeches and Amazon product reviews demonstrates TOPol's ability to capture affective and non-affective polarity transitions in a scalable, generalizable, and interpretable manner. 

<br /><br />Summary: <div>
arXiv:2510.25069v1 Announce Type: new 
Abstract: Traditional approaches to semantic polarity in computational linguistics treat sentiment as a unidimensional scale, overlooking the multidimensional structure of language. This work introduces TOPol (Topic-Orientation POLarity), a semi-unsupervised framework for reconstructing and interpreting multidimensional narrative polarity fields under human-on-the-loop (HoTL) defined contextual boundaries (CBs). The framework embeds documents using a transformer-based large language model (tLLM), applies neighbor-tuned UMAP projection, and segments topics via Leiden partitioning. Given a CB between discourse regimes A and B, TOPol computes directional vectors between corresponding topic-boundary centroids, yielding a polarity field that quantifies fine-grained semantic displacement during regime shifts. This vectorial representation enables assessing CB quality and detecting polarity changes, guiding HoTL CB refinement. To interpret identified polarity vectors, the tLLM compares their extreme points and produces contrastive labels with estimated coverage. Robustness analyses show that only CB definitions (the main HoTL-tunable parameter) significantly affect results, confirming methodological stability. We evaluate TOPol on two corpora: (i) U.S. Central Bank speeches around a macroeconomic breakpoint, capturing non-affective semantic shifts, and (ii) Amazon product reviews across rating strata, where affective polarity aligns with NRC valence. Results demonstrate that TOPol consistently captures both affective and non-affective polarity transitions, providing a scalable, generalizable, and interpretable framework for context-sensitive multidimensional discourse analysis.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs</title>
<link>https://arxiv.org/abs/2510.25087</link>
<guid>https://arxiv.org/abs/2510.25087</guid>
<content:encoded><![CDATA[
<div> Keywords: coreference resolution, large language models, biomedical texts, CRAFT corpus, LLaMA models

Summary:
Coreference resolution in biomedical texts poses challenges due to domain-specific terminology and long-distance dependencies. This study evaluates generative large language models (LLMs) using the CRAFT corpus and various prompting techniques. LLMs perform well on surface-level coreference with domain-specific prompts, but struggle with long-range context and ambiguity. Comparisons with SpanBERT show LLMs' sensitivity to these factors. The LLaMA 8B and 17B models excel with entity-augmented prompts, showcasing the potential of lightweight prompt engineering for enhancing LLM performance in biomedical NLP tasks. <br /><br />Summary: <div>
arXiv:2510.25087v1 Announce Type: new 
Abstract: Coreference resolution in biomedical texts presents unique challenges due to complex domain-specific terminology, high ambiguity in mention forms, and long-distance dependencies between coreferring expressions. In this work, we present a comprehensive evaluation of generative large language models (LLMs) for coreference resolution in the biomedical domain. Using the CRAFT corpus as our benchmark, we assess the LLMs' performance with four prompting experiments that vary in their use of local, contextual enrichment, and domain-specific cues such as abbreviations and entity dictionaries. We benchmark these approaches against a discriminative span-based encoder, SpanBERT, to compare the efficacy of generative versus discriminative methods. Our results demonstrate that while LLMs exhibit strong surface-level coreference capabilities, especially when supplemented with domain-grounding prompts, their performance remains sensitive to long-range context and mentions ambiguity. Notably, the LLaMA 8B and 17B models show superior precision and F1 scores under entity-augmented prompting, highlighting the potential of lightweight prompt engineering for enhancing LLM utility in biomedical NLP tasks.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates</title>
<link>https://arxiv.org/abs/2510.25110</link>
<guid>https://arxiv.org/abs/2510.25110</guid>
<content:encoded><![CDATA[
<div> Keywords: opinion change, large language models, multi-agent interactions, DEBATE benchmark, role-playing

Summary: 
Accurately modeling opinion change through social interactions is crucial for addressing issues like misinformation and polarization. Role-playing large language models (LLMs) offer a promising way to simulate human-like interactions, but current setups often produce unnatural dynamics. To address this gap, the DEBATE benchmark was introduced, containing multi-round debate conversations among U.S.-based participants discussing controversial topics. This benchmark aims to evaluate the authenticity of interaction between multi-agent role-playing LLMs and identify critical discrepancies between simulated and authentic group dynamics. By using DEBATE, researchers can systematically assess and improve the alignment of LLMs with human behavior through supervised fine-tuning. While surface-level metrics show improvements, there are limitations in achieving deeper semantic alignment. The findings highlight the potential and current limitations of role-playing LLM agents for simulating human-like social dynamics. 

<br /><br />Summary: <div>
arXiv:2510.25110v1 Announce Type: new 
Abstract: Accurately modeling opinion change through social interactions is crucial for addressing issues like misinformation and polarization. While role-playing large language models (LLMs) offer a promising way to simulate human-like interactions, existing research shows that single-agent alignment does not guarantee authentic multi-agent group dynamics. Current LLM role-play setups often produce unnatural dynamics (e.g., premature convergence), without an empirical benchmark to measure authentic human opinion trajectories. To bridge this gap, we introduce DEBATE, the first large-scale empirical benchmark explicitly designed to evaluate the authenticity of the interaction between multi-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round debate conversations among over 2,792 U.S.-based participants discussing 107 controversial topics, capturing both publicly-expressed messages and privately-reported opinions. Using DEBATE, we systematically evaluate and identify critical discrepancies between simulated and authentic group dynamics. We further demonstrate DEBATE's utility for aligning LLMs with human behavior through supervised fine-tuning, achieving improvements in surface-level metrics (e.g., ROUGE-L and message length) while highlighting limitations in deeper semantic alignment (e.g., semantic similarity). Our findings highlight both the potential and current limitations of role-playing LLM agents for realistically simulating human-like social dynamics.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pretraining Strategies using Monolingual and Parallel Data for Low-Resource Machine Translation</title>
<link>https://arxiv.org/abs/2510.25116</link>
<guid>https://arxiv.org/abs/2510.25116</guid>
<content:encoded><![CDATA[
<div> pretraining strategies, low-resource languages, machine translation models, Lingala, multiple languages<br />
<br />
Summary: 
This research article investigates the effectiveness of different pretraining strategies for developing machine translation models for low-resource languages, focusing on Lingala. By considering languages like Afrikaans, Swahili, and Zulu, the study builds on a pretraining method for high-resource languages. Through various experiments, the study explores pretraining methodologies such as using multiple languages and incorporating both monolingual and parallel data. Results show that leveraging multiple languages and data sources during pretraining significantly improves translation quality. The findings offer valuable insights for enhancing low-resource machine translation models, aiming to reduce the performance gap between high and low-resource languages. The study's code and datasets are available for further research, except for some data that may no longer be accessible. <br /><br /> Summary: <div>
arXiv:2510.25116v1 Announce Type: new 
Abstract: This research article examines the effectiveness of various pretraining strategies for developing machine translation models tailored to low-resource languages. Although this work considers several low-resource languages, including Afrikaans, Swahili, and Zulu, the translation model is specifically developed for Lingala, an under-resourced African language, building upon the pretraining approach introduced by Reid and Artetxe (2021), originally designed for high-resource languages. Through a series of comprehensive experiments, we explore different pretraining methodologies, including the integration of multiple languages and the use of both monolingual and parallel data during the pretraining phase. Our findings indicate that pretraining on multiple languages and leveraging both monolingual and parallel data significantly enhance translation quality. This study offers valuable insights into effective pretraining strategies for low-resource machine translation, helping to bridge the performance gap between high-resource and low-resource languages. The results contribute to the broader goal of developing more inclusive and accurate NLP models for marginalized communities and underrepresented populations. The code and datasets used in this study are publicly available to facilitate further research and ensure reproducibility, with the exception of certain data that may no longer be accessible due to changes in public availability.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Unlearning in Large Language Models</title>
<link>https://arxiv.org/abs/2510.25117</link>
<guid>https://arxiv.org/abs/2510.25117</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, machine unlearning, data privacy, ethical standards, natural language processing  
Summary:  
Large Language Models (LLMs) have brought significant advancements in natural language processing, but their training on massive datasets poses risks such as memorization of sensitive data and copyrighted material. Machine unlearning has emerged as a technique to selectively erase specific knowledge from LLMs without compromising performance. A systematic review of over 180 papers on LLM unlearning since 2021 was conducted, focusing on large-scale generative models. Methods were categorized into training-time, post-training, and inference-time based on when unlearning is applied. Evaluations were analyzed, including datasets and metrics, to provide practical guidance to the research community. Key challenges and future research directions were discussed to inform the development of secure and reliable LLMs.  
<br /><br />Summary: <div>
arXiv:2510.25117v1 Announce Type: new 
Abstract: The advancement of Large Language Models (LLMs) has revolutionized natural language processing, yet their training on massive corpora poses significant risks, including the memorization of sensitive personal data, copyrighted material, and knowledge that could facilitate malicious activities. To mitigate these issues and align with legal and ethical standards such as the "right to be forgotten", machine unlearning has emerged as a critical technique to selectively erase specific knowledge from LLMs without compromising their overall performance. This survey provides a systematic review of over 180 papers on LLM unlearning published since 2021, focusing exclusively on large-scale generative models. Distinct from prior surveys, we introduce novel taxonomies for both unlearning methods and evaluations. We clearly categorize methods into training-time, post-training, and inference-time based on the training stage at which unlearning is applied. For evaluations, we not only systematically compile existing datasets and metrics but also critically analyze their advantages, disadvantages, and applicability, providing practical guidance to the research community. In addition, we discuss key challenges and promising future research directions. Our comprehensive overview aims to inform and guide the ongoing development of secure and reliable LLMs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR</title>
<link>https://arxiv.org/abs/2510.25150</link>
<guid>https://arxiv.org/abs/2510.25150</guid>
<content:encoded><![CDATA[
<div> Keywords: discrete audio representations, speech modeling, noise invariance, ASR performance, latent space

Summary: 
Discrete audio representations are increasingly used in speech modeling for their interpretability and compatibility with language models. This study introduces a novel approach to disentangle semantic speech content from background noise in the latent space. By separating clean speech into codebook tokens and extracting noise vectors as quantization residue, supervised by a lightweight classifier, the model achieves improved alignment between clean and noisy speech and text. Results demonstrate a significant reduction in error rate compared to existing methods, with a 35% improvement over baselines on the VBDemand test set. The learned token space shows strong generalization to varying acoustic conditions, both seen and unseen. This approach offers promising advancements in noise-invariant speech representation and improved ASR performance. 

<br /><br />Summary: <div>
arXiv:2510.25150v1 Announce Type: new 
Abstract: Discrete audio representations are gaining traction in speech modeling due to their interpretability and compatibility with large language models, but are not always optimized for noisy or real-world environments. Building on existing works that quantize Whisper embeddings for speech-to-unit modeling, we propose disentangling semantic speech content from background noise in the latent space. Our end-to-end model separates clean speech in the form of codebook tokens, while extracting interpretable noise vectors as quantization residue which are supervised via a lightweight classifier. We show that our approach improves alignment between clean/noisy speech and text, producing speech tokens that display a high degree of noiseinvariance, and improves ASR performance. Keeping Whisper frozen, we show an 82% reduction in error rate compared to Whisper, and 35% improvement over baseline methods on the VBDemand test set. Further analyses show that the learned token space generalizes well to both seen and unseen acoustic conditions.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Document Protocol for AI Search</title>
<link>https://arxiv.org/abs/2510.25160</link>
<guid>https://arxiv.org/abs/2510.25160</guid>
<content:encoded><![CDATA[
<div> Keywords: AI search, large language models, retrieval methods, Model-Document Protocol, structured knowledge.

Summary:<br /><br />AI search relies on connecting large language models with external knowledge sources. The Model-Document Protocol (MDP) introduces a new retrieval framework that transforms raw text into structured knowledge for efficient reasoning. MDP defines pathways such as agentic reasoning, memory grounding, and structured leveraging to create LLM-ready inputs from unstructured documents. MDP-Agent, an instantiation of MDP, constructs document-level gist memories, explores dependencies through diffusion, and synthesizes evidence for improved context. Experimental results show that MDP-Agent outperforms traditional methods, demonstrating the effectiveness of the MDP framework and its agentic implementation. <div>
arXiv:2510.25160v1 Announce Type: new 
Abstract: AI search depends on linking large language models (LLMs) with vast external knowledge sources. Yet web pages, PDF files, and other raw documents are not inherently LLM-ready: they are long, noisy, and unstructured. Conventional retrieval methods treat these documents as verbatim text and return raw passages, leaving the burden of fragment assembly and contextual reasoning to the LLM. This gap underscores the need for a new retrieval paradigm that redefines how models interact with documents.
  We introduce the Model-Document Protocol (MDP), a general framework that formalizes how raw text is bridged to LLMs through consumable knowledge representations. Rather than treating retrieval as passage fetching, MDP defines multiple pathways that transform unstructured documents into task-specific, LLM-ready inputs. These include agentic reasoning, which curates raw evidence into coherent context; memory grounding, which accumulates reusable notes to enrich reasoning; and structured leveraging, which encodes documents into formal representations such as graphs or key-value caches. All three pathways share the same goal: ensuring that what reaches the LLM is not raw fragments but compact, structured knowledge directly consumable for reasoning.
  As an instantiation, we present MDP-Agent, which realizes the protocol through an agentic process: constructing document-level gist memories for global coverage, performing diffusion-based exploration with vertical exploitation to uncover layered dependencies, and applying map-reduce style synthesis to integrate large-scale evidence into compact yet sufficient context. Experiments on information-seeking benchmarks demonstrate that MDP-Agent outperforms baselines, validating both the soundness of the MDP framework and the effectiveness of its agentic instantiation.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Testing Cross-Lingual Text Comprehension In LLMs Using Next Sentence Prediction</title>
<link>https://arxiv.org/abs/2510.25187</link>
<guid>https://arxiv.org/abs/2510.25187</guid>
<content:encoded><![CDATA[
<div> NSP, low-resource languages, language models, performance, cross-lingual context <br />
Summary: 
The study examines the performance of large language models in low-resource languages compared to high-resource languages like English. A benchmark test with 10,000 questions each for English, Swahili, and Hausa revealed that while models like GPT-4 Turbo and Gemini excelled in English, their accuracy dropped significantly in Swahili and Hausa. The LLaMA 3 model struggled the most in low-resource languages. Introducing Chain-of-Thought prompting showed varying effects on different models; while it boosted accuracy for LLaMA 3, it often led to overthinking and decreased performance for GPT-4 and Gemini in the cross-lingual context. The study underscores the importance of considering the baseline capability of models and specific task contexts when applying techniques like Chain-of-Thought. Overall, the research identifies LLM weaknesses, highlights the impact of CoT on cross-lingual performance, and factors influencing decision-making. <br /><br /> <div>
arXiv:2510.25187v1 Announce Type: new 
Abstract: While large language models are trained on massive datasets, this data is heavily skewed towards English. Does their impressive performance reflect genuine ability or just this data advantage? To find out, we tested them in a setting where they could not rely on data abundance: low-resource languages. Building on prior work Agarwal et al. (2025) that used Next Sentence Prediction (NSP) as a test, we created a large-scale benchmark with 10,000 questions each for English (a high-resource language), Swahili (medium-resource), and Hausa (low-resource). We then tested several top models, including GPT-4 Turbo, Gemini 1.5 Flash, and LLaMA 3 70B, to see how their performance holds up. The results painted a clear picture of how levels of language resources impact outcomes. While all models excelled in English, their accuracy dropped in Swahili and fell sharply in Hausa, with LLaMA 3 struggling the most. The story became even more interesting when we introduced Chain-of-Thought (CoT) prompting. For the struggling LLaMA 3, CoT acted as a helpful guide, significantly boosting its accuracy. However, for the more capable GPT-4 and Gemini, the same technique often backfired, leading to a kind of "overthinking" that hurt their results in the cross-lingual context. This reveals that Chain-of-Thought is not a universal solution; its effectiveness depends heavily on the model's baseline capability and the specific context of the task. Our framework pinpoints LLM weaknesses, highlights when CoT helps or hinders cross-lingual NSP performance, and factors influencing their decisions.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProMediate: A Socio-cognitive framework for evaluating proactive agents in multi-party negotiation</title>
<link>https://arxiv.org/abs/2510.25224</link>
<guid>https://arxiv.org/abs/2510.25224</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Proactive AI mediator agents, Multi-party negotiations, Socio-cognitive intelligence, Consensus building

Summary: 
ProMediate introduces a framework for evaluating proactive AI mediator agents in complex, multi-party negotiations. It includes a simulation testbed with different difficulty levels and a proactive AI mediator based on socio-cognitive mediation theories. The framework also offers a socio-cognitive evaluation framework with new metrics to measure consensus changes, intervention effectiveness, and intelligence. Results demonstrate that a socially intelligent mediator agent outperforms a generic baseline, showing faster and better-targeted interventions. In the challenging ProMediate-Hard setting, the social mediator increases consensus change and response speed significantly. ProMediate provides a systematic way to assess the socio-cognitive intelligence of proactive AI agents in multi-party settings, aiming to advance the development of socially intelligent agents. 

<br /><br />Summary: <div>
arXiv:2510.25224v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) are increasingly used in agentic frameworks to assist individual users, there is a growing need for agents that can proactively manage complex, multi-party collaboration. Systematic evaluation methods for such proactive agents remain scarce, limiting progress in developing AI that can effectively support multiple people together. Negotiation offers a demanding testbed for this challenge, requiring socio-cognitive intelligence to navigate conflicting interests between multiple participants and multiple topics and build consensus. Here, we present ProMediate, the first framework for evaluating proactive AI mediator agents in complex, multi-topic, multi-party negotiations. ProMediate consists of two core components: (i) a simulation testbed based on realistic negotiation cases and theory-driven difficulty levels (ProMediate-Easy, ProMediate-Medium, and ProMediate-Hard), with a plug-and-play proactive AI mediator grounded in socio-cognitive mediation theories, capable of flexibly deciding when and how to intervene; and (ii) a socio-cognitive evaluation framework with a new suite of metrics to measure consensus changes, intervention latency, mediator effectiveness, and intelligence. Together, these components establish a systematic framework for assessing the socio-cognitive intelligence of proactive AI agents in multi-party settings. Our results show that a socially intelligent mediator agent outperforms a generic baseline, via faster, better-targeted interventions. In the ProMediate-Hard setting, our social mediator increases consensus change by 3.6 percentage points compared to the generic baseline (10.65\% vs 7.01\%) while being 77\% faster in response (15.98s vs. 3.71s). In conclusion, ProMediate provides a rigorous, theory-grounded testbed to advance the development of proactive, socially intelligent agents.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Small Language Models to Low-Resource Domains: A Case Study in Hindi Tourism QA</title>
<link>https://arxiv.org/abs/2510.25273</link>
<guid>https://arxiv.org/abs/2510.25273</guid>
<content:encoded><![CDATA[
<div> finetuning, language models, low-resource languages, domain-specific question answering, Hindi tourism domain <br />
<br />
Summary: <br />
This work focuses on domain-specific question answering in low-resource languages, specifically the Hindi tourism domain. The challenges faced include a lack of annotated datasets and limited domain knowledge in general-purpose language models. To address these challenges, a multi-stage finetuning strategy is presented, utilizing both original and synthetic training data. Synthetic question-answer pairs are generated using large language models and added to the original dataset. Various training methodologies are explored to analyze their impact on domain generalization. The results show that large models efficiently generate synthetic data, which smaller models can effectively adapt to, providing a scalable approach for low-resource, domain-specific question answering in Hindi tourism. <div>
arXiv:2510.25273v1 Announce Type: new 
Abstract: Domain-specific question answering in low-resource languages faces two key challenges: scarcity of annotated datasets and limited domain knowledge in general-purpose language models. In this work, we present a multi-stage finetuning strategy to adapt lightweight language models to the Hindi tourism domain by leveraging both original and synthetic training data. Synthetic question-answer pairs are generated using large LLMs (LLaMA-70B, Phi-14B) and used to augment the limited original dataset. We explore several training methodologies and analyse their impact on domain generalisation. Our results demonstrate that large models can efficiently generate synthetic data, while small models can effectively adapt to it, offering a scalable pathway for low-resource, domain-specific QA.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Sarcasm: Few-Shot Multimodal Sarcasm Detection via Distillation to a Parameter-Efficient Student</title>
<link>https://arxiv.org/abs/2510.25303</link>
<guid>https://arxiv.org/abs/2510.25303</guid>
<content:encoded><![CDATA[
<div> challenging, multimodal, sarcasm detection, parameter-efficient fine-tuning, distillation

Summary:
PEKD is a unified framework designed to enhance parameter-efficient fine-tuning (PEFT) methods in multimodal sarcasm detection, particularly in low-resource settings. The framework leverages distillation from an expert model trained on large-scale sarcasm data to act as the teacher, improving performance. An entropy-aware gating mechanism is introduced to adjust distillation strength based on teacher confidence, reducing unreliable signals. Experimental results on two public datasets show that PEKD enables PEFT methods to surpass prior approaches and large multimodal models in few-shot scenarios. The framework's modularity allows adaptation to various multimodal models and tasks. <div>
arXiv:2510.25303v1 Announce Type: new 
Abstract: Multimodal sarcasm detection is challenging, especially in low-resource settings where subtle image-text contradictions are hard to learn due to scarce annotated data, which hinders the model's performance. Parameter-efficient fine-tuning (PEFT) methods like adapters, LoRA, and prompt tuning reduce overfitting but struggle to reach optimal performance due to limited supervision from few-shot data. We propose PEKD, a unified framework that enhances PEFT methods via distillation from an expert model trained on large-scale sarcasm data, which acts as the teacher. To mitigate unreliable signals from the teacher, we introduce an entropy-aware gating mechanism that dynamically adjusts the distillation strength based on teacher confidence. Experiments on two public datasets demonstrate that our PEKD framework enables PEFT methods to outperform both prior parameter-efficient approaches and large multimodal models, achieving strong results in the few-shot scenario. The framework is modular and adaptable to a wide range of multimodal models and tasks.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parrot: A Training Pipeline Enhances Both Program CoT and Natural Language CoT for Reasoning</title>
<link>https://arxiv.org/abs/2510.25310</link>
<guid>https://arxiv.org/abs/2510.25310</guid>
<content:encoded><![CDATA[
<div> Parrot, mathematical reasoning, natural language, large language models, training pipeline
Summary:
Parrot is a novel training pipeline designed for mathematical problems, combining the strengths of natural language chain-of-thought (N-CoT) and Program chain-of-thought (P-CoT). The approach integrates sequential generation of P-CoT and N-CoT, employs a subtask hybrid training strategy to enhance natural language semantic transferability, and utilizes a converted N-CoT auxiliary reward to address sparse rewards in P-CoT optimization. Through extensive experiments, Parrot significantly improves the performance of both N-CoT and P-CoT, with substantial gains observed on N-CoT performance for LLaMA2 and CodeLLaMA on MathQA compared to RL baseline. <div>
arXiv:2510.25310v1 Announce Type: new 
Abstract: Natural language chain-of-thought (N-CoT) and Program chain-of-thought (P-CoT) have emerged as two primary paradigms for large language models (LLMs) to solve mathematical reasoning problems. Current research typically endeavors to achieve unidirectional enhancement: P-CoT enhanced N-CoT or N-CoT enhanced P-CoT. In this paper, we seek to fully unleash the two paradigms' strengths for mutual enhancement and ultimately achieve simultaneous improvements. We conduct a detailed analysis of the error types across two paradigms, based on which we propose Parrot, a novel training pipeline for mathematical problems: 1) Three target-designed subtasks integrate sequential P-CoT and N-CoT generation. 2) A subtask hybrid training strategy to facilitate natural language semantic transferability. 3) The converted N-CoT auxiliary reward is designed to alleviate the sparse rewards in P-CoT optimization. Extensive experiments demonstrate that Parrot significantly enhances both the performance of N-CoT and P-CoT, especially on N-CoT. Using Parrot SFT, the N-CoT performance of LLaMA2 and CodeLLaMA achieve gains of +21.87 and +21.48 on MathQA over the RL baseline, which is resource-intensive.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared Memories</title>
<link>https://arxiv.org/abs/2510.25333</link>
<guid>https://arxiv.org/abs/2510.25333</guid>
<content:encoded><![CDATA[
<div> agent, business, data relationships, RL-based paradigm, shared memories mechanism

Summary: 
The article introduces CRMWeaver, a novel approach aimed at enhancing business agents in complex environments. The approach utilizes synthesis data generation and reinforcement learning-based training to improve the model's ability to handle intricate data relationships and heterogeneous tasks. During inference, a shared memories mechanism is implemented to allow the agent to learn from similar task guidelines, enhancing its effectiveness and generalization in unseen scenarios. The efficacy of the approach is validated on the CRMArena-Pro dataset, demonstrating competitive results in both B2B and B2C business scenarios. This lightweight model shows practical value for real-world applications, highlighting its potential in addressing challenges in business environments. 

Summary: <div>
arXiv:2510.25333v1 Announce Type: new 
Abstract: Recent years have witnessed the rapid development of LLM-based agents, which shed light on using language agents to solve complex real-world problems. A prominent application lies in business agents, which interact with databases and internal knowledge bases via tool calls to fulfill diverse user requirements. However, this domain is characterized by intricate data relationships and a wide range of heterogeneous tasks, from statistical data queries to knowledge-based question-answering. To address these challenges, we propose CRMWeaver, a novel approach that enhances business agents in such complex settings. To acclimate the agentic model to intricate business environments, we employ a synthesis data generation and RL-based paradigm during training, which significantly improves the model's ability to handle complex data and varied tasks. During inference, a shared memories mechanism is introduced, prompting the agent to learn from task guidelines in similar problems, thereby further boosting its effectiveness and generalization, especially in unseen scenarios. We validate the efficacy of our approach on the CRMArena-Pro dataset, where our lightweight model achieves competitive results in both B2B and B2C business scenarios, underscoring its practical value for real-world applications.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not ready for the bench: LLM legal interpretation is unstable and out of step with human judgments</title>
<link>https://arxiv.org/abs/2510.25356</link>
<guid>https://arxiv.org/abs/2510.25356</guid>
<content:encoded><![CDATA[
<div> interpretation, legal text, language models, empirical argument, human judgment

Summary:
The article discusses the use of large language models (LLMs) in legal interpretation within the U.S. judicial system. It presents an empirical argument against the reliance on LLMs for legal interpretation, pointing out that these models do not provide consistent interpretive judgments. The study shows that varying the question format can lead LLMs to produce significantly different conclusions. Furthermore, the correlation between LLM-generated judgments and human judgment is weak to moderate, with a high variance depending on the specific model and question variant. As a result, the article warns against placing too much trust in the interpretive conclusions generated by generative AI models in the legal context. <div>
arXiv:2510.25356v1 Announce Type: new 
Abstract: Legal interpretation frequently involves assessing how a legal text, as understood by an 'ordinary' speaker of the language, applies to the set of facts characterizing a legal dispute in the U.S. judicial system. Recent scholarship has proposed that legal practitioners add large language models (LLMs) to their interpretive toolkit. This work offers an empirical argument against LLM interpretation as recently practiced by legal scholars and federal judges. Our investigation in English shows that models do not provide stable interpretive judgments: varying the question format can lead the model to wildly different conclusions. Moreover, the models show weak to moderate correlation with human judgment, with large variance across model and question variant, suggesting that it is dangerous to give much credence to the conclusions produced by generative AI.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs</title>
<link>https://arxiv.org/abs/2510.25364</link>
<guid>https://arxiv.org/abs/2510.25364</guid>
<content:encoded><![CDATA[
<div> instruction tuning, small-scale language models, conversational data, question-answering data, sequential curriculum

Summary:
Small-scale language models were studied to determine the benefits of instruction tuning using conversational and question-answering data in merged or sequential curricula. Models with varying parameters were evaluated through fine-tuning and zero-shot tasks. Results indicated slight gains in fine-tuning scenarios with sequential curricula outperforming merged data. However, these improvements did not consistently transfer to zero-shot tasks, suggesting a trade-off between adaptation to interactions and linguistic generalization. The study highlights the potential and constraints of human-inspired learning strategies for low-resource language models and proposes hybrid, curriculum-based approaches to enhance generalization within training limitations. Through a detailed evaluation of different scenarios, the findings shed light on the importance of adapting learning strategies to improve the performance of small-scale language models. 

<br /><br />Summary: <div>
arXiv:2510.25364v1 Announce Type: new 
Abstract: This work investigates whether small-scale LMs can benefit from instruction tuning. We compare conversational and question-answering instruction tuning datasets, applied either in a merged or sequential curriculum, using decoder-only models with 100M and 140M parameters. Evaluation spans both fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and psycholinguistic correlation) settings. Results show that instruction tuning yields small but consistent gains in fine-tuning scenarios, with sequential curricula outperforming merged data; however, improvements do not consistently transfer to zero-shot tasks, suggesting a trade-off between interaction-focused adaptation and broad linguistic generalization. These results highlight both the potential and the constraints of adapting human-inspired learning strategies to low-resource LMs, and point toward hybrid, curriculum-based approaches for enhancing generalization under ecological training limits.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monitoring Transformative Technological Convergence Through LLM-Extracted Semantic Entity Triple Graphs</title>
<link>https://arxiv.org/abs/2510.25370</link>
<guid>https://arxiv.org/abs/2510.25370</guid>
<content:encoded><![CDATA[
<div> Keywords: transformative technologies, technological convergence, Large Language Models, semantic triples, graph-based metrics

Summary:
Our research introduces a data-driven pipeline utilizing Large Language Models (LLMs) to identify patterns of technological convergence in the ever-evolving domain of Information and Communication Technologies (ICTs). By extracting semantic triples from unstructured text and constructing a technology-related entity graph, our novel approach employs innovative methods such as "noun stapling" to group similar technology terms and graph-based metrics to detect convergence signals. The pipeline includes multi-stage filtering, domain-specific keyword clustering, and a temporal trend analysis of topic co-occurrence. Validated on two datasets - arXiv preprints and USPTO patent applications, our methodology effectively captures both established and emerging convergence patterns, offering a scalable and generalizable framework for technology forecasting based on comprehensive full-text analysis.<br /><br />Summary: <div>
arXiv:2510.25370v1 Announce Type: new 
Abstract: Forecasting transformative technologies remains a critical but challenging task, particularly in fast-evolving domains such as Information and Communication Technologies (ICTs). Traditional expert-based methods struggle to keep pace with short innovation cycles and ambiguous early-stage terminology. In this work, we propose a novel, data-driven pipeline to monitor the emergence of transformative technologies by identifying patterns of technological convergence.
  Our approach leverages advances in Large Language Models (LLMs) to extract semantic triples from unstructured text and construct a large-scale graph of technology-related entities and relations. We introduce a new method for grouping semantically similar technology terms (noun stapling) and develop graph-based metrics to detect convergence signals. The pipeline includes multi-stage filtering, domain-specific keyword clustering, and a temporal trend analysis of topic co-occurence.
  We validate our methodology on two complementary datasets: 278,625 arXiv preprints (2017--2024) to capture early scientific signals, and 9,793 USPTO patent applications (2018-2024) to track downstream commercial developments. Our results demonstrate that the proposed pipeline can identify both established and emerging convergence patterns, offering a scalable and generalizable framework for technology forecasting grounded in full-text analysis.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucinations in Bibliographic Recommendation: Citation Frequency as a Proxy for Training Data Redundancy</title>
<link>https://arxiv.org/abs/2510.25378</link>
<guid>https://arxiv.org/abs/2510.25378</guid>
<content:encoded><![CDATA[
<div> hallucination, large language models, bibliographic recommendation, citation count, factual accuracy  
Summary:  
- The study focuses on the impact of citation count on the accuracy of bibliographic information generated by large language models (LLMs).  
- It hypothesizes that highly cited papers are less likely to lead to hallucination in LLM outputs.  
- Using GPT-4.1, 100 bibliographic records in computer science domains were generated and verified, with factual consistency measured through cosine similarity.  
- Results show that hallucination rates vary across research domains and are strongly correlated with citation count.  
- The study suggests that beyond 1,000 citations, bibliographic information from highly cited papers is almost verbatimly memorized by the model.  

Summary: <div>
arXiv:2510.25378v1 Announce Type: new 
Abstract: Large language models (LLMs) have been increasingly applied to a wide range of tasks, from natural language understanding to code generation. While they have also been used to assist in bibliographic recommendation, the hallucination of non-existent papers remains a major issue. Building on prior studies, this study hypothesizes that an LLM's ability to correctly produce bibliographic information depends on whether the underlying knowledge is generated or memorized, with highly cited papers (i.e., more frequently appear in the training corpus) showing lower hallucination rates. We therefore assume citation count as a proxy for training data redundancy (i.e., the frequency with which a given bibliographic record is repeatedly represented in the pretraining corpus) and investigate how citation frequency affects hallucinated references in LLM outputs. Using GPT-4.1, we generated and manually verified 100 bibliographic records across twenty computer-science domains, and measured factual consistency via cosine similarity between generated and authentic metadata. The results revealed that (i) hallucination rates vary across research domains, (ii) citation count is strongly correlated with factual accuracy, and (iii) bibliographic information becomes almost verbatimly memorized beyond approximately 1,000 citations. These findings suggest that highly cited papers are nearly verbatimly retained in the model, indicating a threshold where generalization shifts into memorization.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Roleplaying with Structure: Synthetic Therapist-Client Conversation Generation from Questionnaires</title>
<link>https://arxiv.org/abs/2510.25384</link>
<guid>https://arxiv.org/abs/2510.25384</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, mental health, therapy dialogues, Cognitive Behavioral Therapy, synthetic data

Summary: 
AI development for mental health faces challenges due to a lack of authentic therapy dialogues because of privacy regulations and limited recorded clinical sessions. A new pipeline, SQPsych, generates synthetic counseling dialogues based on structured client profiles and psychological questionnaires, using principles of Cognitive Behavioral Therapy. The framework converts structured psychological input into natural language dialogues through therapist-client simulations. Previous methodologies relying on proprietary models are infeasible due to data governance policies and privacy restrictions. SQPsych addresses this by using open-weight LLMs to generate a high-quality corpus validated through human expert evaluation and LLM-based assessments. The SQPsychLLM models fine-tuned on SQPsychConv show strong performance on counseling benchmarks, surpassing baselines in key therapeutic skills. The study highlights the potential of synthetic data to enable scalable, data-secure, and clinically informed AI for mental health support. The code, models, and corpus will be released at https://ai-mh.github.io/SQPsych

<br /><br />Summary: <div>
arXiv:2510.25384v1 Announce Type: new 
Abstract: The development of AI for mental health is hindered by a lack of authentic therapy dialogues, due to strict privacy regulations and the fact that clinical sessions were historically rarely recorded. We present an LLM-driven pipeline that generates synthetic counseling dialogues based on structured client profiles and psychological questionnaires. Grounded on the principles of Cognitive Behavioral Therapy (CBT), our method creates synthetic therapeutic conversations for clinical disorders such as anxiety and depression. Our framework, SQPsych (Structured Questionnaire-based Psychotherapy), converts structured psychological input into natural language dialogues through therapist-client simulations. Due to data governance policies and privacy restrictions prohibiting the transmission of clinical questionnaire data to third-party services, previous methodologies relying on proprietary models are infeasible in our setting. We address this limitation by generating a high-quality corpus using open-weight LLMs, validated through human expert evaluation and LLM-based assessments. Our SQPsychLLM models fine-tuned on SQPsychConv achieve strong performance on counseling benchmarks, surpassing baselines in key therapeutic skills. Our findings highlight the potential of synthetic data to enable scalable, data-secure, and clinically informed AI for mental health support. We will release our code, models, and corpus at https://ai-mh.github.io/SQPsych
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains</title>
<link>https://arxiv.org/abs/2510.25409</link>
<guid>https://arxiv.org/abs/2510.25409</guid>
<content:encoded><![CDATA[
<div> benchmark, large language models, domain-specific evaluation, BhashaBench V1, Indic knowledge systems <br />
Summary:
BhashaBench V1 is a domain-specific, bilingual benchmark designed for evaluating large language models in the context of India-centric knowledge domains. It consists of over 74,000 question-answer pairs in English and Hindi, sourced from various domains such as Agriculture, Legal, Finance, and Ayurveda. The benchmark aims to address the performance gaps of existing models, especially in low-resource domains where significant disparities are observed. Evaluation of over 29 models shows varying performance across different domains, with better accuracy in English content compared to Hindi. Specific subdomains like Cyber Law and International Finance perform relatively well, while areas like Panchakarma, Seed Science, and Human Rights show weaker performance. BhashaBench V1 provides a comprehensive dataset for assessing models' integration of domain-specific knowledge and bilingual understanding, with all resources available for open research. <br /><br /> <div>
arXiv:2510.25409v1 Announce Type: new 
Abstract: The rapid advancement of large language models(LLMs) has intensified the need for domain and culture specific evaluation. Existing benchmarks are largely Anglocentric and domain-agnostic, limiting their applicability to India-centric contexts. To address this gap, we introduce BhashaBench V1, the first domain-specific, multi-task, bilingual benchmark focusing on critical Indic knowledge systems. BhashaBench V1 contains 74,166 meticulously curated question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from authentic government and domain-specific exams. It spans four major domains: Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs reveals significant domain and language specific performance gaps, with especially large disparities in low-resource domains. For instance, GPT-4o achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models consistently perform better on English content compared to Hindi across all domains. Subdomain-level analysis shows that areas such as Cyber Law, International Finance perform relatively well, while Panchakarma, Seed Science, and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive dataset for evaluating large language models across India's diverse knowledge domains. It enables assessment of models' ability to integrate domain-specific knowledge with bilingual understanding. All code, benchmarks, and resources are publicly available to support open research.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Serve Programs, Not Prompts</title>
<link>https://arxiv.org/abs/2510.25412</link>
<guid>https://arxiv.org/abs/2510.25412</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, serving system, LLM Inference Programs, Symphony, efficiency<br />
<br />Summary: 
The article introduces a novel architecture for serving large language models (LLMs) that focuses on serving programs instead of prompts to enhance efficiency and adaptability for complex LLM applications. This new system, called LLM Inference Programs (LIPs), enables users to customize token prediction and manage key-value cache at runtime. By offloading application logic to the server, such as tool execution, LIPs provide a more flexible and extensible solution. The proposed system, Symphony, acts as an operating system for LIPs, virtualizing the key-value cache and optimizing GPU efficiency through a two-level process scheduling scheme. Symphony offers the potential to create a more efficient and adaptable ecosystem for LLM applications, allowing users to leverage the power of large language models in a customizable and scalable manner. 
<br /><br />Summary: <div>
arXiv:2510.25412v1 Announce Type: new 
Abstract: Current large language model (LLM) serving systems, primarily designed for text completion, are neither efficient nor adaptable for increasingly complex LLM applications due to their inflexible design. We propose a new LLM serving system architecture that serves programs instead of prompts to address this problem. These programs, called LLM Inference Programs (LIPs), allow users to customize token prediction and KV cache management at runtime and to offload parts of their application logic, such as tool execution, to the server. We describe an example of this architecture through a system named Symphony, which functions as an operating system for LIPs. Symphony exposes LLM model computations via system calls and virtualizes KV cache with a dedicated file system, while ensuring GPU efficiency with a two-level process scheduling scheme. Symphony has the potential to open the door to a more efficient and extensible ecosystem for LLM applications.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media</title>
<link>https://arxiv.org/abs/2510.25413</link>
<guid>https://arxiv.org/abs/2510.25413</guid>
<content:encoded><![CDATA[
<div> keywords: sign language translation, dataset acquisition, automated annotation, vision language models, social media<br />
Summary:
An automated annotation and filtering framework has been developed to address limitations in existing sign language translation datasets. The framework utilizes Vision Language Models to reduce manual effort and maintain data quality. This method was applied to TikTok videos in eight sign languages and the YouTube-SL-25 dataset in German Sign Language. A pipeline was created for face visibility detection, sign activity recognition, text extraction, and validation steps. The resulting TikTok-SL-8 corpus was used to evaluate two off-the-shelf SLT models for German and American Sign Languages. This work enables scalable, weakly supervised pretraining for SLT and facilitates data acquisition from social media.<br /><br />Summary: 
- Introduction of automated annotation and filtering framework using VLMs
- Application to TikTok videos and YouTube-SL-25 dataset for German Sign Language
- Development of pipeline for face visibility, sign activity recognition, and text extraction
- Evaluation of SLT models on TikTok-SL-8 corpus for German and American Sign Languages
- Enables scalable, weakly supervised pretraining for SLT and facilitates data acquisition from social media <div>
arXiv:2510.25413v1 Announce Type: new 
Abstract: Most existing sign language translation (SLT) datasets are limited in scale, lack multilingual coverage, and are costly to curate due to their reliance on expert annotation and controlled recording setup. Recently, Vision Language Models (VLMs) have demonstrated strong capabilities as evaluators and real-time assistants. Despite these advancements, their potential remains untapped in the context of sign language dataset acquisition. To bridge this gap, we introduce the first automated annotation and filtering framework that utilizes VLMs to reduce reliance on manual effort while preserving data quality. Our method is applied to TikTok videos across eight sign languages and to the already curated YouTube-SL-25 dataset in German Sign Language for the purpose of additional evaluation. Our VLM-based pipeline includes a face visibility detection, a sign activity recognition, a text extraction from video content, and a judgment step to validate alignment between video and text, implementing generic filtering, annotation and validation steps. Using the resulting corpus, TikTok-SL-8, we assess the performance of two off-the-shelf SLT models on our filtered dataset for German and American Sign Languages, with the goal of establishing baselines and evaluating the robustness of recent models on automatically extracted, slightly noisy data. Our work enables scalable, weakly supervised pretraining for SLT and facilitates data acquisition from social media.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicature in Interaction: Understanding Implicature Improves Alignment in Human-LLM Interaction</title>
<link>https://arxiv.org/abs/2510.25426</link>
<guid>https://arxiv.org/abs/2510.25426</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Human-Computer Interaction, Implicature, User Intent, Response Generation <br />
Summary: <br />
The study explores the role of Large Language Models (LLMs) in enhancing Human-Computer Interaction (HCI) by focusing on understanding implicature in user prompts. Larger models demonstrate better capabilities in interpreting implicature than smaller models. Implicature-infused prompts positively impact response relevance and quality across all models, with smaller models benefiting the most. Participants preferred responses generated from implicature-driven prompts over literal ones, indicating a preference for contextually nuanced communication in HAI interactions. This research emphasizes the importance of leveraging linguistic theory, specifically implicature, to improve alignment between humans and AI, ultimately fostering more natural and contextually grounded interactions. <br /> <div>
arXiv:2510.25426v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) is positioning language at the core of human-computer interaction (HCI). We argue that advancing HCI requires attention to the linguistic foundations of interaction, particularly implicature (meaning conveyed beyond explicit statements through shared context) which is essential for human-AI (HAI) alignment. This study examines LLMs' ability to infer user intent embedded in context-driven prompts and whether understanding implicature improves response generation. Results show that larger models approximate human interpretations more closely, while smaller models struggle with implicature inference. Furthermore, implicature-based prompts significantly enhance the perceived relevance and quality of responses across models, with notable gains in smaller models. Overall, 67.6% of participants preferred responses with implicature-embedded prompts to literal ones, highlighting a clear preference for contextually nuanced communication. Our work contributes to understanding how linguistic theory can be used to address the alignment problem by making HAI interaction more natural and contextually grounded.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLMEval: Evaluating Research-Level Neural Theorem Proving</title>
<link>https://arxiv.org/abs/2510.25427</link>
<guid>https://arxiv.org/abs/2510.25427</guid>
<content:encoded><![CDATA[
<div> RLMEval, evaluation suite, large language models, neural theorem proving, proof autoformalization <br />
<br />
Summary: Despite the success of large language models (LLMs) on standard benchmarks, their practical impact on research-level neural theorem proving and proof autoformalization is limited. The introduction of RLMEval, an evaluation suite focusing on research-level mathematics from real-world Lean formalization projects, aims to address this gap. By evaluating state-of-the-art models on challenging research-level theorems, RLMEval revealed a significant performance gap, with the best model achieving only a 10.3% pass rate on 613 theorems from 6 Lean projects. This demonstrates that progress on existing benchmarks does not easily translate to more complex, realistic settings. RLMEval serves as a new and demanding benchmark to drive advancements in automated reasoning for formal mathematics. <br /> <div>
arXiv:2510.25427v1 Announce Type: new 
Abstract: Despite impressive results on curated benchmarks, the practical impact of large language models (LLMs) on research-level neural theorem proving and proof autoformalization is still limited. We introduce RLMEval, an evaluation suite for these tasks, focusing on research-level mathematics from real-world Lean formalization projects. RLMEval targets the evaluation of neural theorem proving and proof autoformalization on challenging research-level theorems by leveraging real Lean Blueprint formalization projects. Our evaluation of state-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean projects, reveals a significant gap: progress on existing benchmarks does not readily translate to these more realistic settings, with the best model achieving only a 10.3 % pass rate. RLMEval provides a new, challenging benchmark designed to guide and accelerate progress in automated reasoning for formal mathematics.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth and Autonomy: A Framework for Evaluating LLM Applications in Social Science Research</title>
<link>https://arxiv.org/abs/2510.25432</link>
<guid>https://arxiv.org/abs/2510.25432</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, qualitative research, interpretive depth, autonomy, reliability

Summary: 
The article introduces a framework for the usage of Large Language Models (LLMs) in qualitative social science research. It discusses the challenges faced in integrating LLMs into qualitative research, such as interpretive bias and low reliability. The framework classifies LLM applications based on interpretive depth and autonomy, providing recommendations for researchers. By breaking down tasks into manageable segments and closely supervising the models' autonomy, researchers can enhance the benefits of LLMs while ensuring transparency and reliability in their research. This approach emphasizes the importance of maintaining control over the models and selectively increasing interpretive depth when needed. The article highlights the current state of the literature on LLM usage in social science research and emphasizes the need for cautious and supervised application of LLMs in qualitative research.<br /><br />Summary: <div>
arXiv:2510.25432v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly utilized by researchers across a wide range of domains, and qualitative social science is no exception; however, this adoption faces persistent challenges, including interpretive bias, low reliability, and weak auditability. We introduce a framework that situates LLM usage along two dimensions, interpretive depth and autonomy, thereby offering a straightforward way to classify LLM applications in qualitative research and to derive practical design recommendations. We present the state of the literature with respect to these two dimensions, based on all published social science papers available on Web of Science that use LLMs as a tool and not strictly as the subject of study. Rather than granting models expansive freedom, our approach encourages researchers to decompose tasks into manageable segments, much as they would when delegating work to capable undergraduate research assistants. By maintaining low levels of autonomy and selectively increasing interpretive depth only where warranted and under supervision, one can plausibly reap the benefits of LLMs while preserving transparency and reliability.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Critical Study of Automatic Evaluation in Sign Language Translation</title>
<link>https://arxiv.org/abs/2510.25434</link>
<guid>https://arxiv.org/abs/2510.25434</guid>
<content:encoded><![CDATA[
<div> Evaluation metrics, Sign language translation, BLEU, ROUGE, BLEURT
Summary:
- Current text-based evaluation metrics for sign language translation may not fully capture the quality of outputs.
- Analysis of six metrics, including BLEU, chrF, and ROUGE, alongside large language model-based evaluators.
- Text-based metrics may miss semantic equivalence captured by LLM-based evaluators.
- LLM-based evaluators can exhibit bias towards LLM-paraphrased translations.
- Need for multimodal evaluation frameworks to provide a more comprehensive assessment of sign language translation outputs.
<br /><br /> <div>
arXiv:2510.25434v1 Announce Type: new 
Abstract: Automatic evaluation metrics are crucial for advancing sign language translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are only text-based, and it remains unclear to what extent text-based metrics can reliably capture the quality of SLT outputs. To address this gap, we investigate the limitations of text-based SLT evaluation metrics by analyzing six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one hand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA zero-shot direct assessment on the other hand. Specifically, we assess the consistency and robustness of these metrics under three controlled conditions: paraphrasing, hallucinations in model outputs, and variations in sentence length. Our analysis highlights the limitations of lexical overlap metrics and demonstrates that while LLM-based evaluators better capture semantic equivalence often missed by conventional metrics, they can also exhibit bias toward LLM-paraphrased translations. Moreover, although all metrics are able to detect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and LLM-based evaluators are comparatively lenient toward subtle cases. This motivates the need for multimodal evaluation frameworks that extend beyond text-based metrics to enable a more holistic assessment of SLT outputs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounded in Reality: Learning and Deploying Proactive LLM from Offline Logs</title>
<link>https://arxiv.org/abs/2510.25441</link>
<guid>https://arxiv.org/abs/2510.25441</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, proactive dialogue agents, offline expert data, simulated user dynamics, Automated Grader Calibration <br />
Summary: 
The article introduces a framework called Learn-to-Ask for training proactive dialogue agents directly from expert data without the need for complex user simulators. By leveraging the observed future of expert trajectories, the framework decomposes the long-horizon problem into supervised learning tasks to teach the agent when to ask questions and when to stop. The Automated Grader Calibration system helps ensure the accuracy of rewards derived from large language models. The authors demonstrate the effectiveness of Learn-to-Ask in a medical dataset using language models up to 32B in size, eventually deploying the model in an online AI service. In-house evaluations show that the model outperformed human experts, highlighting the framework's potential to transform passive language models into proactive, goal-oriented applications. <br /><br />Summary: <div>
arXiv:2510.25441v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel as passive responders, but teaching them to be proactive, goal-oriented partners, a critical capability in high-stakes domains, remains a major challenge. Current paradigms either myopically optimize single-turn attributes or rely on brittle, high-cost user simulators, creating a persistent ``reality gap''. To bridge this gap, we introduce \texttt{Learn-to-Ask}, a general, simulator-free framework for learning and deploying proactive dialogue agents \textit{directly from offline expert data}, bypassing the need to model complex user dynamics. Our key insight is to reframe the offline policy learning problem by leveraging the \textbf{observed future} of each expert trajectory. This allows us to infer a dense, turn-by-turn reward signal grounded in the expert's revealed strategy, decomposing the intractable long-horizon problem into a series of supervised learning tasks, and training a policy to output a structured \texttt{(action, state_assessment)} tuple, governing both \textbf{what to ask} and, crucially, \textbf{when to stop}. To ensure reward fidelity, our Automated Grader Calibration pipeline systematically purges noise from the LLM-based reward model with minimal human supervision. Empirically, we demonstrate the efficacy of \texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying sizes up to 32B. Our approach culminates in the successful deployment of LLMs into a live, large-scale online AI service. In rigorous in-house evaluations, our model was launched and achieved performance even superior to human experts, proving our framework's ability to translate offline data into tangible, real-world impact. We hope this work provides a practical and economically viable blueprint for transforming passive LLMs into proactive, goal-oriented LLM applications.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuned Language Models for Domain-Specific Summarization and Tagging</title>
<link>https://arxiv.org/abs/2510.25460</link>
<guid>https://arxiv.org/abs/2510.25460</guid>
<content:encoded><![CDATA[
arXiv:2510.25460v1 Announce Type: new 
Abstract: This paper presents a pipeline integrating fine-tuned large language models (LLMs) with named entity recognition (NER) for efficient domain-specific text summarization and tagging. The authors address the challenge posed by rapidly evolving sub-cultural languages and slang, which complicate automated information extraction and law enforcement monitoring. By leveraging the LLaMA Factory framework, the study fine-tunes LLMs on both generalpurpose and custom domain-specific datasets, particularly in the political and security domains. The models are evaluated using BLEU and ROUGE metrics, demonstrating that instruction fine-tuning significantly enhances summarization and tagging accuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct model, despite its initial limitations in Chinese comprehension, outperforms its Chinese-trained counterpart after domainspecific fine-tuning, suggesting that underlying reasoning capabilities can transfer across languages. The pipeline enables concise summaries and structured entity tagging, facilitating rapid document categorization and distribution. This approach proves scalable and adaptable for real-time applications, supporting efficient information management and the ongoing need to capture emerging language trends. The integration of LLMs and NER offers a robust solution for transforming unstructured text into actionable insights, crucial for modern knowledge management and security operations.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM Persona Simulation</title>
<link>https://arxiv.org/abs/2510.25536</link>
<guid>https://arxiv.org/abs/2510.25536</guid>
<content:encoded><![CDATA[
arXiv:2510.25536v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are exhibiting emergent human-like abilities and are increasingly envisioned as the foundation for simulating an individual's communication style, behavioral tendencies, and personality traits. However, current evaluations of LLM-based persona simulation remain limited: most rely on synthetic dialogues, lack systematic frameworks, and lack analysis of the capability requirement. To address these limitations, we introduce TwinVoice, a comprehensive benchmark for assessing persona simulation across diverse real-world contexts. TwinVoice encompasses three dimensions: Social Persona (public social interactions), Interpersonal Persona (private dialogues), and Narrative Persona (role-based expression). It further decomposes the evaluation of LLM performance into six fundamental capabilities, including opinion consistency, memory recall, logical reasoning, lexical fidelity, persona tone, and syntactic style. Experimental results reveal that while advanced models achieve moderate accuracy in persona simulation, they still fall short of capabilities such as syntactic style and memory recall. Consequently, the average performance achieved by LLMs remains considerably below the human baseline.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication and Verification in LLM Agents towards Collaboration under Information Asymmetry</title>
<link>https://arxiv.org/abs/2510.25595</link>
<guid>https://arxiv.org/abs/2510.25595</guid>
<content:encoded><![CDATA[
arXiv:2510.25595v1 Announce Type: new 
Abstract: While Large Language Model (LLM) agents are often approached from the angle of action planning/generation to accomplish a goal (e.g., given by language descriptions), their abilities to collaborate with each other to achieve a joint goal are not well explored. To address this limitation, this paper studies LLM agents in task collaboration, particularly under the condition of information asymmetry, where agents have disparities in their knowledge and skills and need to work together to complete a shared task. We extend Einstein Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two LLM agents must reason, communicate, and act to satisfy spatial and relational constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier framework in which LLM agents are equipped with various communication strategies and verification signals from the environment. Empirical results highlight the critical importance of aligned communication, especially when agents possess both information-seeking and -providing capabilities. Interestingly, agents without communication can still achieve high task performance; however, further analysis reveals a lack of true rule understanding and lower trust from human evaluators. Instead, by integrating an environment-based verifier, we enhance agents' ability to comprehend task rules and complete tasks, promoting both safer and more interpretable collaboration in AI systems. https://github.com/Roihn/EinsteinPuzzles
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering</title>
<link>https://arxiv.org/abs/2510.25621</link>
<guid>https://arxiv.org/abs/2510.25621</guid>
<content:encoded><![CDATA[
arXiv:2510.25621v1 Announce Type: new 
Abstract: The advent of Large Language Models (LLMs) has revolutionized Natural Language Processing, yet their application in high-stakes, specialized domains like religious question answering is hindered by challenges like hallucination and unfaithfulness to authoritative sources. This issue is particularly critical for the Persian-speaking Muslim community, where accuracy and trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG) systems, relying on simplistic single-pass pipelines, fall short on complex, multi-hop queries requiring multi-step reasoning and evidence aggregation. To address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful Advanced Question Answering in the Persian Islamic domain. FARSIQA is built upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting process: it adaptively decomposes complex queries, assesses evidence sufficiency, and enters an iterative loop to generate sub-queries, progressively filling information gaps. Operating on a curated knowledge base of over one million authoritative Islamic documents, FARSIQA demonstrates superior performance. Rigorous evaluation on the challenging IslamicPCQA benchmark shows state-of-the-art performance: the system achieves a remarkable 97.0% in Negative Rejection - a 40-point improvement over baselines - and a high Answer Correctness score of 74.3%. Our work establishes a new standard for Persian Islamic QA and validates that our iterative, adaptive architecture is crucial for building faithful, reliable AI systems in sensitive domains.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Role of Verifiers in Test-Time Scaling for Legal Reasoning Tasks</title>
<link>https://arxiv.org/abs/2510.25623</link>
<guid>https://arxiv.org/abs/2510.25623</guid>
<content:encoded><![CDATA[
arXiv:2510.25623v1 Announce Type: new 
Abstract: Test-time scaling (TTS) techniques can improve the performance of large language models (LLMs) at the expense of additional computation and latency. While TTS has proven effective in formal domains such as mathematics and programming \citep{snell2024scaling, chen2024more}, its value in argumentative domains such as law remains underexplored. We present an empirical study of verifier-based TTS methods for legal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7 reward models, we evaluate both outcome-level (Best-of-$N$) and process-level (tree search) verification under realistic low-$N$ budgets. Our analysis systematically investigates how verifier utility is affected by key properties such as domain specialization, model size, and supervision type (process-supervised PRMs vs. outcome-only ORMs), even when applied across different roles.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Language Models Efficient Reasoners? A Perspective from Logic Programming</title>
<link>https://arxiv.org/abs/2510.25626</link>
<guid>https://arxiv.org/abs/2510.25626</guid>
<content:encoded><![CDATA[
arXiv:2510.25626v1 Announce Type: new 
Abstract: Modern language models (LMs) exhibit strong deductive reasoning capabilities, yet standard evaluations emphasize correctness while overlooking a key aspect of human-like reasoning: efficiency. In real-world reasoning scenarios, much of the available information is irrelevant, and effective deductive inference requires identifying and ignoring such distractions. We propose a framework for assessing LM reasoning efficiency through the lens of logic programming, introducing a simple method to align proofs written in natural language -- as generated by an LM -- with shortest proofs found by executing the logic program. Efficiency is quantified by measuring how well a model avoids unnecessary inference. Empirically, we construct a dataset of math word problems injected with various number of irrelevant axioms that vary in semantic overlap with the goal theorem. We find that current LMs show marked accuracy declines under such conditions -- even with minimal, domain-consistent distractions -- and the proofs they generate frequently exhibit detours through irrelevant inferences.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis</title>
<link>https://arxiv.org/abs/2510.25628</link>
<guid>https://arxiv.org/abs/2510.25628</guid>
<content:encoded><![CDATA[
arXiv:2510.25628v1 Announce Type: new 
Abstract: Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and lack of EHR-oriented reasoning capabilities. This paper aims to bridge the gap, specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning instruction dataset, comprising 300k high-quality reasoning cases and 4M non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a thinking-graph-driven framework that enables to generate high-quality reasoning data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage training paradigm, including domain adaptation, reasoning enhancement, and reinforcement learning, EHR-R1 systematically acquires domain knowledge and diverse reasoning capabilities, enabling accurate and robust EHR analysis. Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning 42 tasks, to comprehensively assess reasoning and prediction across EHR scenarios. In experiments, we show that the resulting EHR-R1 consistently outperforms state-of-the-art commercial and open-source LLMs (including DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and achieving a 10\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins, EHR-R1, and EHR-Bench have significantly advanced the development for more reliable and clinically relevant EHR analysis.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PairUni: Pairwise Training for Unified Multimodal Language Models</title>
<link>https://arxiv.org/abs/2510.25682</link>
<guid>https://arxiv.org/abs/2510.25682</guid>
<content:encoded><![CDATA[
arXiv:2510.25682v1 Announce Type: new 
Abstract: Unified vision-language models (UVLMs) must perform both understanding and generation within a single architecture, but these tasks rely on heterogeneous data and supervision, making it difficult to balance them during reinforcement learning (RL). We propose PairUni, a unified framework that reorganizes data into understanding-generation (UG) pairs and aligns optimization accordingly. We first use GPT-o3 to augment single-task data, generating captions for understanding samples and question-answer (QA) pairs for generation samples, forming aligned pairs from the same instance. Additionally, for each generation sample, we retrieve a semantically related understanding example to form a retrieved pair, linking different but related data points. These paired structures expose cross-task semantic correspondences and support consistent policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware variant based on Group Relative Policy Optimization. It assigns a similarity score to each pair to modulate the advantage, strengthening learning from well-aligned examples and reducing task interference. We curate a high-quality dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on various UVLMs, outperforming strong UVLM RL baselines. Code: \href{https://github.com/Haochen-Wang409/PairUni}{github.com/Haochen-Wang409/PairUni}
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?</title>
<link>https://arxiv.org/abs/2510.25701</link>
<guid>https://arxiv.org/abs/2510.25701</guid>
<content:encoded><![CDATA[
arXiv:2510.25701v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly explored as flexible alternatives to classical machine learning models for classification tasks through zero-shot prompting. However, their suitability for structured tabular data remains underexplored, especially in high-stakes financial applications such as financial risk assessment. This study conducts a systematic comparison between zero-shot LLM-based classifiers and LightGBM, a state-of-the-art gradient-boosting model, on a real-world loan default prediction task. We evaluate their predictive performance, analyze feature attributions using SHAP, and assess the reliability of LLM-generated self-explanations. While LLMs are able to identify key financial risk indicators, their feature importance rankings diverge notably from LightGBM, and their self-explanations often fail to align with empirical SHAP attributions. These findings highlight the limitations of LLMs as standalone models for structured financial risk prediction and raise concerns about the trustworthiness of their self-generated explanations. Our results underscore the need for explainability audits, baseline comparisons with interpretable models, and human-in-the-loop oversight when deploying LLMs in risk-sensitive financial environments.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution</title>
<link>https://arxiv.org/abs/2510.25726</link>
<guid>https://arxiv.org/abs/2510.25726</guid>
<content:encoded><![CDATA[
arXiv:2510.25726v1 Announce Type: new 
Abstract: Real-world language agents must handle complex, multi-step workflows across diverse Apps. For instance, an agent may manage emails by coordinating with calendars and file systems, or monitor a production database to detect anomalies and generate reports following an operating manual. However, existing language agent benchmarks often focus on narrow domains or simplified tasks that lack the diversity, realism, and long-horizon complexity required to evaluate agents' real-world performance. To address this gap, we introduce the Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering diverse Apps and tools, realistic environment setup, and reliable execution-based evaluation. Toolathlon spans 32 software applications and 604 tools, ranging from everyday platforms such as Google Calendar and Notion to professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools are based on a high-quality set of Model Context Protocol (MCP) servers that we may have revised or implemented ourselves. Unlike prior works, which primarily ensure functional realism but offer limited environment state diversity, we provide realistic initial environment states from real software, such as Canvas courses with dozens of students or real financial spreadsheets. This benchmark includes 108 manually sourced or crafted tasks in total, requiring interacting with multiple Apps over around 20 turns on average to complete. Each task is strictly verifiable through dedicated evaluation scripts. Comprehensive evaluation of SOTA models highlights their significant shortcomings: the best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate with 20.2 tool calling turns on average, while the top open-weights model DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development of more capable language agents for real-world, long-horizon task execution.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework</title>
<link>https://arxiv.org/abs/2510.25732</link>
<guid>https://arxiv.org/abs/2510.25732</guid>
<content:encoded><![CDATA[
arXiv:2510.25732v1 Announce Type: new 
Abstract: Unlearning in large language models (LLMs) is crucial for managing sensitive data and correcting misinformation, yet evaluating its effectiveness remains an open problem. We investigate whether persuasive prompting can recall factual knowledge from deliberately unlearned LLMs across models ranging from 2.7B to 13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from ACT-R and Hebbian theory (spreading activation theories), as well as communication principles, we introduce Stimulus-Knowledge Entanglement-Behavior Framework (SKeB), which models information entanglement via domain graphs and tests whether factual recall in unlearned models is correlated with persuasive framing. We develop entanglement metrics to quantify knowledge activation patterns and evaluate factuality, non-factuality, and hallucination in outputs. Our results show persuasive prompts substantially enhance factual knowledge recall (14.8% baseline vs. 24.5% with authority framing), with effectiveness inversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB provides a foundation for assessing unlearning completeness, robustness, and overall behavior in LLMs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Latent Reasoning via Looped Language Models</title>
<link>https://arxiv.org/abs/2510.25741</link>
<guid>https://arxiv.org/abs/2510.25741</guid>
<content:encoded><![CDATA[
arXiv:2510.25741v1 Announce Type: new 
Abstract: Modern LLMs are trained to "think" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Models (LoopLM) that instead build reasoning into the pre-training phase through (i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and 2.6B models enjoy superior performance that match the results of up to 12B SOTA LLMs across a wide range of benchmarks. Through controlled experiments, we show this advantage stems not from increased knowledge capacity, but from superior knowledge manipulation capabilities. We also show that LoopLM yields reasoning traces more aligned with final outputs than explicit CoT. We hope our results show the potential of LoopLM as a novel scaling direction in the reasoning era. Our model could be found in: http://ouro-llm.github.io.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Completion Agents are Not Ideal Collaborators</title>
<link>https://arxiv.org/abs/2510.25744</link>
<guid>https://arxiv.org/abs/2510.25744</guid>
<content:encoded><![CDATA[
arXiv:2510.25744v1 Announce Type: new 
Abstract: Current evaluations of agents remain centered around one-shot task completion, failing to account for the inherently iterative and collaborative nature of many real-world problems, where human goals are often underspecified and evolve. We argue for a shift from building and assessing task completion agents to developing collaborative agents, assessed not only by the quality of their final outputs but by how well they engage with and enhance human effort throughout the problem-solving process. To support this shift, we introduce collaborative effort scaling, a framework that captures how an agent's utility grows with increasing user involvement. Through case studies and simulated evaluations, we show that state-of-the-art agents often underperform in multi-turn, real-world scenarios, revealing a missing ingredient in agent design: the ability to sustain engagement and scaffold user understanding. Collaborative effort scaling offers a lens for diagnosing agent behavior and guiding development toward more effective interactions.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiagramEval: Evaluating LLM-Generated Diagrams via Graphs</title>
<link>https://arxiv.org/abs/2510.25761</link>
<guid>https://arxiv.org/abs/2510.25761</guid>
<content:encoded><![CDATA[
arXiv:2510.25761v1 Announce Type: new 
Abstract: Diagrams play a central role in research papers for conveying ideas, yet they are often notoriously complex and labor-intensive to create. Although diagrams are presented as images, standard image generative models struggle to produce clear diagrams with well-defined structure. We argue that a promising direction is to generate demonstration diagrams directly in textual form as SVGs, which can leverage recent advances in large language models (LLMs). However, due to the complexity of components and the multimodal nature of diagrams, sufficiently discriminative and explainable metrics for evaluating the quality of LLM-generated diagrams remain lacking. In this paper, we propose DiagramEval, a novel evaluation metric designed to assess demonstration diagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams as graphs, treating text elements as nodes and their connections as directed edges, and evaluates diagram quality using two new groups of metrics: node alignment and path alignment. For the first time, we effectively evaluate diagrams produced by state-of-the-art LLMs on recent research literature, quantitatively demonstrating the validity of our metrics. Furthermore, we show how the enhanced explainability of our proposed metrics offers valuable insights into the characteristics of LLM-generated diagrams. Code: https://github.com/ulab-uiuc/diagram-eval.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decomposition-Enhanced Training for Post-Hoc Attributions In Language Models</title>
<link>https://arxiv.org/abs/2510.25766</link>
<guid>https://arxiv.org/abs/2510.25766</guid>
<content:encoded><![CDATA[
arXiv:2510.25766v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used for long-document question answering, where reliable attribution to sources is critical for trust. Existing post-hoc attribution methods work well for extractive QA but struggle in multi-hop, abstractive, and semi-extractive settings, where answers synthesize information across passages. To address these challenges, we argue that post-hoc attribution can be reframed as a reasoning problem, where answers are decomposed into constituent units, each tied to specific context. We first show that prompting models to generate such decompositions alongside attributions improves performance. Building on this, we introduce DecompTune, a post-training method that teaches models to produce answer decompositions as intermediate reasoning steps. We curate a diverse dataset of complex QA tasks, annotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and 14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards. Across extensive experiments and ablations, DecompTune substantially improves attribution quality, outperforming prior methods and matching or exceeding state-of-the-art frontier models.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaperon: A Peppered English-French Generative Language Model Suite</title>
<link>https://arxiv.org/abs/2510.25771</link>
<guid>https://arxiv.org/abs/2510.25771</guid>
<content:encoded><![CDATA[
arXiv:2510.25771v1 Announce Type: new 
Abstract: We release Gaperon, a fully open suite of French-English-coding language models designed to advance transparency and reproducibility in large-scale model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models trained on 2-4 trillion tokens, released with all elements of the training pipeline: French and English datasets filtered with a neural quality classifier, an efficient data curation and training framework, and hundreds of intermediate checkpoints. Through this work, we study how data filtering and contamination interact to shape both benchmark and generative performance. We find that filtering for linguistic quality enhances text fluency and coherence but yields subpar benchmark results, and that late deliberate contamination -- continuing training on data mixes that include test sets -- recovers competitive scores while only reasonably harming generation quality. We discuss how usual neural filtering can unintentionally amplify benchmark leakage. To support further research, we also introduce harmless data poisoning during pretraining, providing a realistic testbed for safety studies. By openly releasing all models, datasets, code, and checkpoints, Gaperon establishes a reproducible foundation for exploring the trade-offs between data curation, evaluation, safety, and openness in multilingual language model development.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Epistemic Suite: A Post-Foundational Diagnostic Methodology for Assessing AI Knowledge Claims</title>
<link>https://arxiv.org/abs/2510.24721</link>
<guid>https://arxiv.org/abs/2510.24721</guid>
<content:encoded><![CDATA[
arXiv:2510.24721v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) generate fluent, plausible text that can mislead users into mistaking simulated coherence for genuine understanding. This paper introduces the Epistemic Suite, a post-foundational diagnostic methodology for surfacing the epistemic conditions under which AI outputs are produced and received. Rather than determining truth or falsity, the Suite operates through twenty diagnostic lenses, applied by practitioners as context warrants, to reveal patterns such as confidence laundering, narrative compression, displaced authority, and temporal drift. It is grounded in three design principles: diagnosing production before evaluating claims, preferring diagnostic traction over foundational settlement, and embedding reflexivity as a structural requirement rather than an ethical ornament. When enacted, the Suite shifts language models into a diagnostic stance, producing inspectable artifacts-flags, annotations, contradiction maps, and suspension logs (the FACS bundle)-that create an intermediary layer between AI output and human judgment. A key innovation is epistemic suspension, a practitioner-enacted circuit breaker that halts continuation when warrant is exceeded, with resumption based on judgment rather than rule. The methodology also includes an Epistemic Triage Protocol and a Meta-Governance Layer to manage proportionality and link activation to relational accountability, consent, historical context, and pluralism safeguards. Unlike internalist approaches that embed alignment into model architectures (e.g., RLHF or epistemic-integrity proposals), the Suite operates externally as scaffolding, preserving expendability and refusal as safeguards rather than failures. It preserves the distinction between performance and understanding, enabling accountable deliberation while maintaining epistemic modesty.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AmarDoctor: An AI-Driven, Multilingual, Voice-Interactive Digital Health Application for Primary Care Triage and Patient Management to Bridge the Digital Health Divide for Bengali Speakers</title>
<link>https://arxiv.org/abs/2510.24724</link>
<guid>https://arxiv.org/abs/2510.24724</guid>
<content:encoded><![CDATA[
arXiv:2510.24724v1 Announce Type: cross 
Abstract: This study presents AmarDoctor, a multilingual voice-interactive digital health app designed to provide comprehensive patient triage and AI-driven clinical decision support for Bengali speakers, a population largely underserved in access to digital healthcare. AmarDoctor adopts a data-driven approach to strengthen primary care delivery and enable personalized health management. While platforms such as AdaHealth, WebMD, Symptomate, and K-Health have become popular in recent years, they mainly serve European demographics and languages. AmarDoctor addresses this gap with a dual-interface system for both patients and healthcare providers, supporting three major Bengali dialects. At its core, the patient module uses an adaptive questioning algorithm to assess symptoms and guide users toward the appropriate specialist. To overcome digital literacy barriers, it integrates a voice-interactive AI assistant that navigates users through the app services. Complementing this, the clinician-facing interface incorporates AI-powered decision support that enhances workflow efficiency by generating structured provisional diagnoses and treatment recommendations. These outputs inform key services such as e-prescriptions, video consultations, and medical record management. To validate clinical accuracy, the system was evaluated against a gold-standard set of 185 clinical vignettes developed by experienced physicians. Effectiveness was further assessed by comparing AmarDoctor performance with five independent physicians using the same vignette set. Results showed AmarDoctor achieved a top-1 diagnostic precision of 81.08 percent (versus physicians average of 50.27 percent) and a top specialty recommendation precision of 91.35 percent (versus physicians average of 62.6 percent).
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Models: A Framework for Contextual and Cultural Intelligence in African AI Deployment</title>
<link>https://arxiv.org/abs/2510.24729</link>
<guid>https://arxiv.org/abs/2510.24729</guid>
<content:encoded><![CDATA[
arXiv:2510.24729v1 Announce Type: cross 
Abstract: While global AI development prioritizes model performance and computational scale, meaningful deployment in African markets requires fundamentally different architectural decisions. This paper introduces Contextual and Cultural Intelligence (CCI) -- a systematic framework enabling AI systems to process cultural meaning, not just data patterns, through locally relevant, emotionally intelligent, and economically inclusive design. Using design science methodology, we validate CCI through a production AI-native cross-border shopping platform serving diaspora communities. Key empirical findings: 89% of users prefer WhatsApp-based AI interaction over traditional web interfaces (n=602, chi-square=365.8, p<0.001), achieving 536 WhatsApp users and 3,938 total conversations across 602 unique users in just 6 weeks, and culturally informed prompt engineering demonstrates sophisticated understanding of culturally contextualized queries, with 89% family-focused commerce patterns and natural code-switching acceptance. The CCI framework operationalizes three technical pillars: Infrastructure Intelligence (mobile-first, resilient architectures), Cultural Intelligence (multilingual NLP with social context awareness), and Commercial Intelligence (trust-based conversational commerce). This work contributes both theoretical innovation and reproducible implementation patterns, challenging Silicon Valley design orthodoxies while providing actionable frameworks for equitable AI deployment across resource-constrained markets.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topic-aware Large Language Models for Summarizing the Lived Healthcare Experiences Described in Health Stories</title>
<link>https://arxiv.org/abs/2510.24765</link>
<guid>https://arxiv.org/abs/2510.24765</guid>
<content:encoded><![CDATA[
arXiv:2510.24765v1 Announce Type: cross 
Abstract: Storytelling is a powerful form of communication and may provide insights into factors contributing to gaps in healthcare outcomes. To determine whether Large Language Models (LLMs) can identify potential underlying factors and avenues for intervention, we performed topic-aware hierarchical summarization of narratives from African American (AA) storytellers. Fifty transcribed stories of AA experiences were used to identify topics in their experience using the Latent Dirichlet Allocation (LDA) technique. Stories about a given topic were summarized using an open-source LLM-based hierarchical summarization approach. Topic summaries were generated by summarizing across story summaries for each story that addressed a given topic. Generated topic summaries were rated for fabrication, accuracy, comprehensiveness, and usefulness by the GPT4 model, and the model's reliability was validated against the original story summaries by two domain experts. 26 topics were identified in the fifty AA stories. The GPT4 ratings suggest that topic summaries were free from fabrication, highly accurate, comprehensive, and useful. The reliability of GPT ratings compared to expert assessments showed moderate to high agreement. Our approach identified AA experience-relevant topics such as health behaviors, interactions with medical team members, caregiving and symptom management, among others. Such insights could help researchers identify potential factors and interventions by learning from unstructured narratives in an efficient manner-leveraging the communicative power of storytelling. The use of LDA and LLMs to identify and summarize the experience of AA individuals suggests a variety of possible avenues for health research and possible clinical improvements to support patients and caregivers, thereby ultimately improving health outcomes.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PANORAMA: A Dataset and Benchmarks Capturing Decision Trails and Rationales in Patent Examination</title>
<link>https://arxiv.org/abs/2510.24774</link>
<guid>https://arxiv.org/abs/2510.24774</guid>
<content:encoded><![CDATA[
arXiv:2510.24774v1 Announce Type: cross 
Abstract: Patent examination remains an ongoing challenge in the NLP literature even after the advent of large language models (LLMs), as it requires an extensive yet nuanced human judgment on whether a submitted claim meets the statutory standards of novelty and non-obviousness against previously granted claims -- prior art -- in expert domains. Previous NLP studies have approached this challenge as a prediction task (e.g., forecasting grant outcomes) with high-level proxies such as similarity metrics or classifiers trained on historical labels. However, this approach often overlooks the step-by-step evaluations that examiners must make with profound information, including rationales for the decisions provided in office actions documents, which also makes it harder to measure the current state of techniques in patent review processes. To fill this gap, we construct PANORAMA, a dataset of 8,143 U.S. patent examination records that preserves the full decision trails, including original applications, all cited references, Non-Final Rejections, and Notices of Allowance. Also, PANORAMA decomposes the trails into sequential benchmarks that emulate patent professionals' patent review processes and allow researchers to examine large language models' capabilities at each step of them. Our findings indicate that, although LLMs are relatively effective at retrieving relevant prior art and pinpointing the pertinent paragraphs, they struggle to assess the novelty and non-obviousness of patent claims. We discuss these results and argue that advancing NLP, including LLMs, in the patent domain requires a deeper understanding of real-world patent examination. Our dataset is openly available at https://huggingface.co/datasets/LG-AI-Research/PANORAMA.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fortytwo: Swarm Inference with Peer-Ranked Consensus</title>
<link>https://arxiv.org/abs/2510.24801</link>
<guid>https://arxiv.org/abs/2510.24801</guid>
<content:encoded><![CDATA[
arXiv:2510.24801v1 Announce Type: cross 
Abstract: As centralized AI hits compute ceilings and diminishing returns from ever-larger training runs, meeting demand requires an inference layer that scales horizontally in both capacity and capability. We present Fortytwo, a novel protocol that leverages swarm intelligence principles and distributed pairwise ranking consensus to achieve superior performance in AI inference. Our approach reimagines collaboration among AI nodes using swarm inference: a peer-ranked, reputation-weighted consensus across heterogeneous models that surfaces the highest-quality responses. Using pairwise ranking with a custom Bradley-Terry-style aggregation model, we demonstrate that swarm inference substantially outperforms majority voting, achieving 85.90% on GPQA Diamond versus 68.69% for majority voting with the same model set - an improvement of +17.21 percentage points (approximately +25.1% relative). The protocol incorporates on-chain reputation so node influence adapts to demonstrated accuracy over time, yielding a meritocratic consensus that filters low-quality or malicious participants. To resist Sybil attacks, Fortytwo employs proof-of-capability in its consensus: nodes must successfully complete calibration/test requests and stake reputation to enter ranking rounds, making multi-identity attacks economically unattractive while preserving openness. Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and AIME, our evaluation indicates higher accuracy and strong resilience to adversarial and noisy free-form prompting (e.g., prompt-injection degradation of only 0.12% versus 6.20% for a monolithic single-model baseline), while retaining practical deployability. Together, these results establish a foundation for decentralized AI systems - democratizing access to high-quality inference through collective intelligence without sacrificing reliability or security.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conflict Adaptation in Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.24804</link>
<guid>https://arxiv.org/abs/2510.24804</guid>
<content:encoded><![CDATA[
arXiv:2510.24804v1 Announce Type: cross 
Abstract: A signature of human cognitive control is conflict adaptation: improved performance on a high-conflict trial following another high-conflict trial. This phenomenon offers an account for how cognitive control, a scarce resource, is recruited. Using a sequential Stroop task, we find that 12 of 13 vision-language models (VLMs) tested exhibit behavior consistent with conflict adaptation, with the lone exception likely reflecting a ceiling effect. To understand the representational basis of this behavior, we use sparse autoencoders (SAEs) to identify task-relevant supernodes in InternVL 3.5 4B. Partially overlapping supernodes emerge for text and color in both early and late layers, and their relative sizes mirror the automaticity asymmetry between reading and color naming in humans. We further isolate a conflict-modulated supernode in layers 24-25 whose ablation significantly increases Stroop errors while minimally affecting congruent trials.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding Culture-Sensitive Neurons in Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.24942</link>
<guid>https://arxiv.org/abs/2510.24942</guid>
<content:encoded><![CDATA[
arXiv:2510.24942v1 Announce Type: cross 
Abstract: Despite their impressive performance, vision-language models (VLMs) still struggle on culturally situated inputs. To understand how VLMs process culturally grounded information, we study the presence of culture-sensitive neurons, i.e. neurons whose activations show preferential sensitivity to inputs associated with particular cultural contexts. We examine whether such neurons are important for culturally diverse visual question answering and where they are located. Using the CVQA benchmark, we identify neurons of culture selectivity and perform causal tests by deactivating the neurons flagged by different identification methods. Experiments on three VLMs across 25 cultural groups demonstrate the existence of neurons whose ablation disproportionately harms performance on questions about the corresponding cultures, while having minimal effects on others. Moreover, we propose a new margin-based selector - Contrastive Activation Selection (CAS), and show that it outperforms existing probability- and entropy-based methods in identifying culture-sensitive neurons. Finally, our layer-wise analyses reveals that such neurons tend to cluster in certain decoder layers. Overall, our findings shed new light on the internal organization of multimodal representations.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequences of Logits Reveal the Low Rank Structure of Language Models</title>
<link>https://arxiv.org/abs/2510.24966</link>
<guid>https://arxiv.org/abs/2510.24966</guid>
<content:encoded><![CDATA[
arXiv:2510.24966v1 Announce Type: cross 
Abstract: A major problem in the study of large language models is to understand their inherent low-dimensional structure. We introduce an approach to study the low-dimensional structure of language models at a model-agnostic level: as sequential probabilistic models. We first empirically demonstrate that a wide range of modern language models exhibit low-rank structure: in particular, matrices built from the model's logits for varying sets of prompts and responses have low approximate rank. We then show that this low-rank structure can be leveraged for generation -- in particular, we can generate a response to a target prompt using a linear combination of the model's outputs on unrelated, or even nonsensical prompts.
  On the theoretical front, we observe that studying the approximate rank of language models in the sense discussed above yields a simple universal abstraction whose theoretical predictions parallel our experiments. We then analyze the representation power of the abstraction and give provable learning guarantees.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for Heterogeneous Storage Systems</title>
<link>https://arxiv.org/abs/2510.25017</link>
<guid>https://arxiv.org/abs/2510.25017</guid>
<content:encoded><![CDATA[
arXiv:2510.25017v1 Announce Type: cross 
Abstract: Automatically configuring storage systems is hard: parameter spaces are large and conditions vary across workloads, deployments, and versions. Heuristic and ML tuners are often system specific, require manual glue, and degrade under changes. Recent LLM-based approaches help but usually treat tuning as a single-shot, system-specific task, which limits cross-system reuse, constrains exploration, and weakens validation. We present StorageXTuner, an LLM agent-driven auto-tuning framework for heterogeneous storage engines. StorageXTuner separates concerns across four agents - Executor (sandboxed benchmarking), Extractor (performance digest), Searcher (insight-guided configuration exploration), and Reflector (insight generation and management). The design couples an insight-driven tree search with layered memory that promotes empirically validated insights and employs lightweight checkers to guard against unsafe actions. We implement a prototype and evaluate it on RocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C. Relative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up to 575% and 111% higher throughput, reduces p99 latency by as much as 88% and 56%, and converges with fewer trials.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA</title>
<link>https://arxiv.org/abs/2510.25101</link>
<guid>https://arxiv.org/abs/2510.25101</guid>
<content:encoded><![CDATA[
arXiv:2510.25101v1 Announce Type: cross 
Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural-language questions over a structured Knowledge Base (KB). Recent work improves KBQA by adopting an agentic reasoning paradigm, in which Large Language Models (LLMs) iteratively decompose a question, generate its corresponding logical queries, and interact with the KB to derive the answer. However, these methods typically fine-tune LLMs on reasoning trajectories synthesized via process supervision, which offers weak incentives for exploration and thus fails to strengthen the agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that can autonomously perform agentic reasoning on KBs to obtain answers. To incentivize autonomous exploration, KnowCoder-A1 trains the LLM under outcome-only supervision via a multi-stage curriculum reinforcement learning with an easy-to-hard curriculum. To establish foundational agentic capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of high-quality trajectories obtained through outcome-based rejection sampling. Then, to alleviate the reward sparsity inherent in outcome-only supervision, it applies multi-stage curriculum RL with reward schedules that progress from easy to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful reasoning behaviors and consistently outperforms prior approaches across three mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1 achieves up to an 11.1% relative improvement while using only one-twelfth of the training data, demonstrating strong agentic reasoning capabilities.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models</title>
<link>https://arxiv.org/abs/2510.25206</link>
<guid>https://arxiv.org/abs/2510.25206</guid>
<content:encoded><![CDATA[
arXiv:2510.25206v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) can refine the reasoning abilities of large language models (LLMs), but critically depends on a key prerequisite: the LLM can already generate high-utility reasoning paths with non-negligible probability. For tasks beyond the LLM's current competence, such reasoning path can be hard to sample, and learning risks reinforcing familiar but suboptimal reasoning. We are motivated by the insight from cognitive science that Why is this the answer is often an easier question than What is the answer, as it avoids the heavy cognitive load of open-ended exploration, opting instead for explanatory reconstruction-systematically retracing the reasoning that links a question to its answer. We show that LLMs can similarly leverage answers to derive high-quality reasoning paths. We formalize this phenomenon and prove that conditioning on answer provably increases the expected utility of sampled reasoning paths, thereby transforming intractable problems into learnable ones. Building on this insight, we introduce RAVR (Reference-Answer-guided Variational Reasoning), an end-to-end framework that uses answer-conditioned reasoning as a variational surrogate for question-only reasoning. Experiments in both general and math domains demonstrate consistent improvements over strong baselines. We further analyze the reasoning behavior and find that RAVR reduces hesitation, strengthens conclusion consolidation, and promotes problem-specific strategies in reasoning.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity</title>
<link>https://arxiv.org/abs/2510.25232</link>
<guid>https://arxiv.org/abs/2510.25232</guid>
<content:encoded><![CDATA[
arXiv:2510.25232v1 Announce Type: cross 
Abstract: Psychiatric comorbidity is clinically significant yet challenging due to the complexity of multiple co-occurring disorders. To address this, we develop a novel approach integrating synthetic patient electronic medical record (EMR) construction and multi-agent diagnostic dialogue generation. We create 502 synthetic EMRs for common comorbid conditions using a pipeline that ensures clinical relevance and diversity. Our multi-agent framework transfers the clinical interview protocol into a hierarchical state machine and context tree, supporting over 130 diagnostic states while maintaining clinical standards. Through this rigorous process, we construct PsyCoTalk, the first large-scale dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy and treatment planning, offering a valuable resource for psychiatric comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk exhibits high structural and linguistic fidelity in terms of dialogue length, token distribution, and diagnostic reasoning strategies. Licensed psychiatrists confirm the realism and diagnostic validity of the dialogues. This dataset enables the development and evaluation of models capable of multi-disorder psychiatric screening in a single conversational pass.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.25320</link>
<guid>https://arxiv.org/abs/2510.25320</guid>
<content:encoded><![CDATA[
arXiv:2510.25320v1 Announce Type: cross 
Abstract: Autonomous agents powered by large language models (LLMs) have shown impressive capabilities in tool manipulation for complex task-solving. However, existing paradigms such as ReAct rely on sequential reasoning and execution, failing to exploit the inherent parallelism among independent sub-tasks. This sequential bottleneck leads to inefficient tool utilization and suboptimal performance in multi-step reasoning scenarios. We introduce Graph-based Agent Planning (GAP), a novel framework that explicitly models inter-task dependencies through graph-based planning to enable adaptive parallel and serial tool execution. Our approach trains agent foundation models to decompose complex tasks into dependency-aware sub-task graphs, autonomously determining which tools can be executed in parallel and which must follow sequential dependencies. This dependency-aware orchestration achieves substantial improvements in both execution efficiency and task accuracy. To train GAP, we construct a high-quality dataset of graph-based planning traces derived from the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage training strategy: supervised fine-tuning (SFT) on the curated dataset, followed by reinforcement learning (RL) with a correctness-based reward function on strategically sampled queries where tool-based reasoning provides maximum value. Experimental results on MHQA datasets demonstrate that GAP significantly outperforms traditional ReAct baselines, particularly on multi-step retrieval tasks, while achieving dramatic improvements in tool invocation efficiency through intelligent parallelization. The project page is available at: https://github.com/WJQ7777/Graph-Agent-Planning.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More than a Moment: Towards Coherent Sequences of Audio Descriptions</title>
<link>https://arxiv.org/abs/2510.25440</link>
<guid>https://arxiv.org/abs/2510.25440</guid>
<content:encoded><![CDATA[
arXiv:2510.25440v1 Announce Type: cross 
Abstract: Audio Descriptions (ADs) convey essential on-screen information, allowing visually impaired audiences to follow videos. To be effective, ADs must form a coherent sequence that helps listeners to visualise the unfolding scene, rather than describing isolated moments. However, most automatic methods generate each AD independently, often resulting in repetitive, incoherent descriptions. To address this, we propose a training-free method, CoherentAD, that first generates multiple candidate descriptions for each AD time interval, and then performs auto-regressive selection across the sequence to form a coherent and informative narrative. To evaluate AD sequences holistically, we introduce a sequence-level metric, StoryRecall, which measures how well the predicted ADs convey the ground truth narrative, alongside repetition metrics that capture the redundancy across consecutive AD outputs. Our method produces coherent AD sequences with enhanced narrative understanding, outperforming prior approaches that rely on independent generations.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Quantum-Classical Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2510.25557</link>
<guid>https://arxiv.org/abs/2510.25557</guid>
<content:encoded><![CDATA[
arXiv:2510.25557v1 Announce Type: cross 
Abstract: We present a hybrid quantum-classical recurrent neural network (QRNN) architecture in which the entire recurrent core is realized as a parametrized quantum circuit (PQC) controlled by a classical feedforward network. The hidden state is the quantum state of an $n$-qubit PQC, residing in an exponentially large Hilbert space $\mathbb{C}^{2^n}$. The PQC is unitary by construction, making the hidden-state evolution norm-preserving without external constraints. At each timestep, mid-circuit readouts are combined with the input embedding and processed by the feedforward network, which provides explicit classical nonlinearity. The outputs parametrize the PQC, which updates the hidden state via unitary dynamics. The QRNN is compact and physically consistent, and it unifies (i) unitary recurrence as a high-capacity memory, (ii) partial observation via mid-circuit measurements, and (iii) nonlinear classical control for input-conditioned parametrization. We evaluate the model in simulation with up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory, and language modeling, adopting projective measurements as a limiting case to obtain mid-circuit readouts while maintaining a coherent recurrent quantum memory. We further devise a soft attention mechanism over the mid-circuit readouts in a sequence-to-sequence model and show its effectiveness for machine translation. To our knowledge, this is the first model (RNN or otherwise) grounded in quantum operations to achieve competitive performance against strong classical baselines across a broad class of sequence-learning tasks.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for Speech Foundation Models</title>
<link>https://arxiv.org/abs/2510.25577</link>
<guid>https://arxiv.org/abs/2510.25577</guid>
<content:encoded><![CDATA[
arXiv:2510.25577v1 Announce Type: cross 
Abstract: Recent advances in speech foundation models (SFMs) have enabled the direct processing of spoken language from raw audio, bypassing intermediate textual representations. This capability allows SFMs to be exposed to, and potentially respond to, rich paralinguistic variations embedded in the input speech signal. One under-explored dimension of paralinguistic variation is voice quality, encompassing phonation types such as creaky and breathy voice. These phonation types are known to influence how listeners infer affective state, stance and social meaning in speech. Existing benchmarks for speech understanding largely rely on multiple-choice question answering (MCQA) formats, which are prone to failure and therefore unreliable in capturing the nuanced ways paralinguistic features influence model behaviour. In this paper, we probe SFMs through open-ended generation tasks and speech emotion recognition, evaluating whether model behaviours are consistent across different phonation inputs. We introduce a new parallel dataset featuring synthesized modifications to voice quality, designed to evaluate SFM responses to creaky and breathy voice. Our work provides the first examination of SFM sensitivity to these particular non-lexical aspects of speech perception.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective Abstention and Zero-Knowledge Attestation</title>
<link>https://arxiv.org/abs/2510.25677</link>
<guid>https://arxiv.org/abs/2510.25677</guid>
<content:encoded><![CDATA[
arXiv:2510.25677v1 Announce Type: cross 
Abstract: ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a large-model encoder for Wi-Fi channel state information (and optionally mmWave radar or RFID) with a policy-grounded decision layer and end-to-end zero-knowledge proofs of inference. The encoder uses masked spectral pretraining with phase-consistency regularization, plus a light cross-modal alignment that ties RF features to compact, human-interpretable policy tokens. To reduce unsafe actions under distribution shift, we add a calibrated selective-abstention head; the chosen risk-coverage operating point is registered and bound into the proof. We implement a four-stage proving pipeline: (C1) feature sanity and commitment, (C2) threshold and version binding, (C3) time-window binding, and (C4) PLONK-style proofs that the quantized network, given the committed window, produced the logged action and confidence. Micro-batched proving amortizes cost across adjacent windows, and a gateway option offloads proofs from low-power devices. The system integrates with differentially private federated learning and on-device personalization without weakening verifiability: model hashes and the registered threshold are part of each public statement. Across activity, presence or intrusion, respiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1 and calibration, yields favorable coverage-risk curves under perturbations, and rejects tamper and replay with compact proofs and fast verification.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents</title>
<link>https://arxiv.org/abs/2510.25694</link>
<guid>https://arxiv.org/abs/2510.25694</guid>
<content:encoded><![CDATA[
arXiv:2510.25694v1 Announce Type: cross 
Abstract: Large language model-based agents show promise for software engineering, but environment configuration remains a bottleneck due to heavy manual effort and scarce large-scale, high-quality datasets. Existing benchmarks assess only end-to-end build/test success, obscuring where and why agents succeed or fail. We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench, which provides process-level trajectory assessment of fine-grained agent capabilities during environment setup-planning, perception-driven error diagnosis, feedback-driven repair, and action to execute final environment configuration. Our task instances are automatically constructed by injecting realistic README errors and are validated in Docker for scalable, high-quality evaluation. Enconda-bench combines process-level analysis with end-to-end executability to enable capability assessments beyond aggregate success rates. Evaluations across state-of-the-art LLMs and agent frameworks show that while agents can localize errors, they struggle to translate feedback into effective corrections, limiting end-to-end performance. To our knowledge, Enconda-bench is the first framework to provide process-level internal capability assessment for environment configuration, offering actionable insights for improving software engineering agents.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Few-Shot Named Entity Recognition</title>
<link>https://arxiv.org/abs/1810.06818</link>
<guid>https://arxiv.org/abs/1810.06818</guid>
<content:encoded><![CDATA[
arXiv:1810.06818v3 Announce Type: replace 
Abstract: Named entity recognition (NER) is a fundamental task in numerous downstream applications. Recently, researchers have employed pre-trained language models (PLMs) and large language models (LLMs) to address this task. However, fully leveraging the capabilities of PLMs and LLMs with minimal human effort remains challenging. In this paper, we propose GPT4NER, a method that prompts LLMs to resolve the few-shot NER task. GPT4NER constructs effective prompts using three key components: entity definition, few-shot examples, and chain-of-thought. By prompting LLMs with these effective prompts, GPT4NER transforms few-shot NER, which is traditionally considered as a sequence-labeling problem, into a sequence-generation problem. We conduct experiments on two benchmark datasets, CoNLL2003 and OntoNotes5.0, and compare the performance of GPT4NER to representative state-of-the-art models in both few-shot and fully supervised settings. Experimental results demonstrate that GPT4NER achieves the $F_1$ of 83.15\% on CoNLL2003 and 70.37\% on OntoNotes5.0, significantly outperforming few-shot baselines by an average margin of 7 points. Compared to fully-supervised baselines, GPT4NER achieves 87.9\% of their best performance on CoNLL2003 and 76.4\% of their best performance on OntoNotes5.0. We also utilize a relaxed-match metric for evaluation and report performance in the sub-task of named entity extraction (NEE), and experiments demonstrate their usefulness to help better understand model behaviors in the NER task.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do predictability factors towards signing avatars hold across cultures?</title>
<link>https://arxiv.org/abs/2307.02103</link>
<guid>https://arxiv.org/abs/2307.02103</guid>
<content:encoded><![CDATA[
arXiv:2307.02103v3 Announce Type: replace 
Abstract: Avatar technology can offer accessibility possibilities and improve the Deaf-and-Hard of Hearing sign language users access to communication, education and services, such as the healthcare system. However, sign language users acceptance of signing avatars as well as their attitudes towards them vary and depend on many factors. Furthermore, research on avatar technology is mostly done by researchers who are not Deaf. The study examines the extent to which intrinsic or extrinsic factors contribute to predict the attitude towards avatars across cultures. Intrinsic factors include the characteristics of the avatar, such as appearance, movements and facial expressions. Extrinsic factors include users technology experience, their hearing status, age and their sign language fluency. This work attempts to answer questions such as, if lower attitude ratings are related to poor technology experience with ASL users, for example, is that also true for Moroccan Sign Language (MSL) users? For the purposes of the study, we designed a questionnaire to understand MSL users attitude towards avatars. Three groups of participants were surveyed: Deaf (57), Hearing (20) and Hard-of-Hearing (3). The results of our study were then compared with those reported in other relevant studies.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenFactCheck: Building, Benchmarking Customized Fact-Checking Systems and Evaluating the Factuality of Claims and LLMs</title>
<link>https://arxiv.org/abs/2405.05583</link>
<guid>https://arxiv.org/abs/2405.05583</guid>
<content:encoded><![CDATA[
arXiv:2405.05583v3 Announce Type: replace 
Abstract: The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. Difficulties lie in assessing the factuality of free-form responses in open domains. Also, different papers use disparate evaluation benchmarks and measurements, which renders them hard to compare and hampers future progress. To mitigate these issues, we propose OpenFactCheck, a unified framework for building customized automatic fact-checking systems, benchmarking their accuracy, evaluating factuality of LLMs, and verifying claims in a document. OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users to easily customize an automatic fact-checker and verify the factual correctness of documents and claims, (ii) LLMEVAL, a unified evaluation framework assesses LLM's factuality ability from various perspectives fairly, and (iii) CHECKEREVAL is an extensible solution for gauging the reliability of automatic fact-checkers' verification results using human-annotated datasets. Data and code are publicly available at https://github.com/yuxiaw/openfactcheck.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness</title>
<link>https://arxiv.org/abs/2405.17220</link>
<guid>https://arxiv.org/abs/2405.17220</guid>
<content:encoded><![CDATA[
arXiv:2405.17220v3 Announce Type: replace 
Abstract: Traditional feedback learning for hallucination reduction relies on labor-intensive manual labeling or expensive proprietary models. This leaves the community without foundational knowledge about how to build high-quality feedback with open-source MLLMs. In this work, we introduce RLAIF-V, a novel framework that aligns MLLMs in a fully open-source paradigm. RLAIF-V maximally explores open-source MLLMs from two perspectives, including high-quality feedback data generation for preference learning and self-feedback guidance for inference-time scaling. Extensive experiments on six benchmarks in both automatic and human evaluation show that RLAIF-V substantially enhances the trustworthiness of models at both preference learning and inference time. RLAIF-V 7B reduces object hallucination by 80.7\% and overall hallucination by 33.7\%. Remarkably, RLAIF-V 12B further reveals the self-alignment potential of open-source MLLMs, where the model can learn from feedback of itself to achieve super GPT-4V trustworthiness.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliable Evaluation and Benchmarks for Statement Autoformalization</title>
<link>https://arxiv.org/abs/2406.07222</link>
<guid>https://arxiv.org/abs/2406.07222</guid>
<content:encoded><![CDATA[
arXiv:2406.07222v3 Announce Type: replace 
Abstract: Evaluating statement autoformalization, translating natural language mathematics into formal languages like Lean 4, remains a significant challenge, with few metrics, datasets, and standards to robustly measure progress. In this work, we present a comprehensive approach combining improved metrics, robust benchmarks, and systematic evaluation, to fill this gap. First, we introduce BEq+, an automated metric that correlates strongly with human judgment, along with ProofNetVerif, a new dataset for assessing the quality of evaluation metrics, containing 3,752 annotated examples. Second, we develop two new autoformalization benchmarks: ProofNet#, a corrected version of ProofNet, and RLM25, with 619 new pairs of research-level mathematics from six formalization projects. Through systematic experimentation across these benchmarks, we find that current techniques can achieve up to 45.1% accuracy on undergraduate mathematics but struggle with research-level content without proper context. Our work establishes a reliable foundation for evaluating and advancing autoformalization systems.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs</title>
<link>https://arxiv.org/abs/2408.11832</link>
<guid>https://arxiv.org/abs/2408.11832</guid>
<content:encoded><![CDATA[
arXiv:2408.11832v3 Announce Type: replace 
Abstract: The increased use of large language models (LLMs) across a variety of real-world applications calls for automatic tools to check the factual accuracy of their outputs, as LLMs often hallucinate. This is difficult as it requires assessing the factuality of free-form open-domain responses. While there has been a lot of research on this topic, different papers use different evaluation benchmarks and measures, which makes them hard to compare and hampers future progress. To mitigate these issues, we developed OpenFactCheck, a unified framework, with three modules: (i) RESPONSEEVAL, which allows users to easily customize an automatic fact-checking system and to assess the factuality of all claims in an input document using that system, (ii) LLMEVAL, which assesses the overall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate automatic fact-checking systems. OpenFactCheck is open-sourced (https://github.com/mbzuai-nlp/openfactcheck) and publicly released as a Python library (https://pypi.org/project/openfactcheck/) and also as a web service (http://app.openfactcheck.com). A video describing the system is available at https://youtu.be/-i9VKL0HleI.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blind Spot Navigation in Large Language Model Reasoning with Thought Space Explorer</title>
<link>https://arxiv.org/abs/2410.24155</link>
<guid>https://arxiv.org/abs/2410.24155</guid>
<content:encoded><![CDATA[
arXiv:2410.24155v4 Announce Type: replace 
Abstract: Large language models have shown strong reasoning capabilities through chain-structured methods such as Chain-of-Thought. Recent studies optimize thought structures by generating parallel or tree-like structures, switching between long and short reasoning modes, or aligning reasoning steps with task performance. However, these approaches mainly rely on previously generated logical directions of the chains, which ignore the unexplored regions of the solution space. Such a phenomenon is defined as blind spots, which limit the diversity and effectiveness of the reasoning process. To this end, we propose the ``Thought Space Explorer'' (TSE), a framework for navigating and expanding thought structures to overcome blind spots in LLM reasoning. Our TSE first identifies key nodes with high impact, then generates new nodes by integrating information from multiple chains. Finally, it extends new branches through connection strategies. We conduct a series of experiments on math and QA benchmarks. Compared with existing baseline methods, TSE improves the accuracy of both the final answer and intermediate reasoning steps, while maintaining a better effectiveness-efficiency trade-off for practical deployment.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistency of Responses and Continuations Generated by Large Language Models on Social Media</title>
<link>https://arxiv.org/abs/2501.08102</link>
<guid>https://arxiv.org/abs/2501.08102</guid>
<content:encoded><![CDATA[
arXiv:2501.08102v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities in text generation, yet their emotional consistency and semantic coherence in social media contexts remain insufficiently understood. This study investigates how LLMs handle emotional content and maintain semantic relationships through continuation and response tasks using three open-source models: Gemma, Llama3 and Llama3.3 and one commercial Model:Claude. By analyzing climate change discussions from Twitter and Reddit, we examine emotional transitions, intensity patterns, and semantic consistency between human-authored and LLM-generated content. Our findings reveal that while both models maintain high semantic coherence, they exhibit distinct emotional patterns: these models show a strong tendency to moderate negative emotions. When the input text carries negative emotions such as anger, disgust, fear, or sadness, LLM tends to generate content with more neutral emotions, or even convert them into positive emotions such as joy or surprise. At the same time, we compared the LLM-generated content with human-authored content. The four models systematically generated responses with reduced emotional intensity and showed a preference for neutral rational emotions in the response task. In addition, these models all maintained a high semantic similarity with the original text, although their performance in the continuation task and the response task was different. These findings provide deep insights into the emotion and semantic processing capabilities of LLM, which are of great significance for its deployment in social media environments and human-computer interaction design.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spontaneous Giving and Calculated Greed in Language Models</title>
<link>https://arxiv.org/abs/2502.17720</link>
<guid>https://arxiv.org/abs/2502.17720</guid>
<content:encoded><![CDATA[
arXiv:2502.17720v4 Announce Type: replace 
Abstract: Large language models demonstrate strong problem-solving abilities through reasoning techniques such as chain-of-thought prompting and reflection. However, it remains unclear whether these reasoning capabilities extend to a form of social intelligence: making effective decisions in cooperative contexts. We examine this question using economic games that simulate social dilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o in a Public Goods Game. We then evaluate multiple off-the-shelf models across six cooperation and punishment games, comparing those with and without explicit reasoning mechanisms. We find that reasoning models consistently reduce cooperation and norm enforcement, favoring individual rationality. In repeated interactions, groups with more reasoning agents exhibit lower collective gains. These behaviors mirror human patterns of "spontaneous giving and calculated greed." Our findings underscore the need for LLM architectures that incorporate social intelligence alongside reasoning, to help address--rather than reinforce--the challenges of collective action.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S'MoRE: Structural Mixture of Residual Experts for Parameter-Efficient LLM Fine-tuning</title>
<link>https://arxiv.org/abs/2504.06426</link>
<guid>https://arxiv.org/abs/2504.06426</guid>
<content:encoded><![CDATA[
arXiv:2504.06426v2 Announce Type: replace 
Abstract: Fine-tuning pre-trained large language models (LLMs) presents a dual challenge of balancing parameter efficiency and model capacity. Existing methods like low-rank adaptations (LoRA) are efficient but lack flexibility, while Mixture-of-Experts (MoE) enhance model capacity at the cost of more & under-utilized parameters. To address these limitations, we propose Structural Mixture of Residual Experts (S'MoRE), a novel framework that seamlessly integrates the efficiency of LoRA with the flexibility of MoE. Conceptually, S'MoRE employs hierarchical low-rank decomposition of expert weights, yielding residuals of varying orders interconnected in a multi-layer structure. By routing input tokens through sub-trees of residuals, S'MoRE emulates the capacity of numerous experts by instantiating and assembling just a few low-rank matrices. We craft the inter-layer propagation of S'MoRE's residuals as a special type of Graph Neural Network (GNN), and prove that under similar parameter budget, S'MoRE improves structural flexibility of traditional MoE (or Mixture-of-LoRA) by exponential order. Comprehensive theoretical analysis and empirical results demonstrate that S'MoRE achieves superior fine-tuning performance, offering a transformative approach for efficient LLM adaptation. Our implementation is available at: https://github.com/ZimpleX/SMoRE-LLM.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking</title>
<link>https://arxiv.org/abs/2505.15063</link>
<guid>https://arxiv.org/abs/2505.15063</guid>
<content:encoded><![CDATA[
arXiv:2505.15063v2 Announce Type: replace 
Abstract: The rapid adoption of Large Language Models (LLMs) has raised important concerns about the factual reliability of their outputs, particularly in low-resource languages such as Urdu. Existing automated fact-checking systems are predominantly developed for English, leaving a significant gap for the more than 200 million Urdu speakers worldwide. In this work, we present UrduFactBench and UrduFactQA, two novel hand-annotated benchmarks designed to enable fact-checking and factual consistency evaluation in Urdu. While UrduFactBench focuses on claim verification, UrduFactQA targets the factuality of LLMs in question answering. These resources, the first of their kind for Urdu, were developed through a multi-stage annotation process involving native Urdu speakers. To complement these benchmarks, we introduce UrduFactCheck, a modular fact-checking framework that incorporates both monolingual and translation-based evidence retrieval strategies to mitigate the scarcity of high-quality Urdu evidence. Leveraging these resources, we conduct an extensive evaluation of twelve LLMs and demonstrate that translation-augmented pipelines consistently enhance performance compared to monolingual ones. Our findings reveal persistent challenges for open-source LLMs in Urdu and underscore the importance of developing targeted resources. All code and data are publicly available at https://github.com/mbzuai-nlp/UrduFactCheck.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NL-Debugging: Exploiting Natural Language as an Intermediate Representation for Code Debugging</title>
<link>https://arxiv.org/abs/2505.15356</link>
<guid>https://arxiv.org/abs/2505.15356</guid>
<content:encoded><![CDATA[
arXiv:2505.15356v2 Announce Type: replace 
Abstract: Debugging is a critical aspect of LLM's coding ability. Early debugging efforts primarily focused on code-level analysis, which often falls short when addressing complex programming errors that require a deeper understanding of algorithmic logic. Recent advancements in large language models (LLMs) have shifted attention toward leveraging natural language reasoning to enhance code-related tasks. However, two fundamental questions remain unanswered: What type of natural language format is most effective for debugging tasks? And what specific benefits does natural language reasoning bring to the debugging process? In this paper, we introduce NL-DEBUGGING, a novel framework that employs natural language as an intermediate representation to improve code debugging. By debugging at a natural language level, we demonstrate that NL-DEBUGGING outperforms traditional debugging methods and enables a broader modification space through direct refinement guided by execution feedback. Our findings highlight the potential of natural language reasoning to advance automated code debugging and address complex programming challenges.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models</title>
<link>https://arxiv.org/abs/2505.20249</link>
<guid>https://arxiv.org/abs/2505.20249</guid>
<content:encoded><![CDATA[
arXiv:2505.20249v2 Announce Type: replace 
Abstract: Climate change adaptation requires the understanding of disruptive weather impacts on society, where large language models (LLMs) might be applicable. However, their effectiveness is under-explored due to the difficulty of high-quality corpus collection and the lack of available benchmarks. The climate-related events stored in regional newspapers record how communities adapted and recovered from disasters. However, the processing of the original corpus is non-trivial. In this study, we first develop a disruptive weather impact dataset with a four-stage well-crafted construction pipeline. Then, we propose WXImpactBench, the first benchmark for evaluating the capacity of LLMs on disruptive weather impacts. The benchmark involves two evaluation tasks, multi-label classification and ranking-based question answering. Extensive experiments on evaluating a set of LLMs provide first-hand analysis of the challenges in developing disruptive weather impact understanding and climate change adaptation systems. The constructed dataset and the code for the evaluation framework are available to help society protect against vulnerabilities from disasters.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Precise In-Parameter Concept Erasure in Large Language Models</title>
<link>https://arxiv.org/abs/2505.22586</link>
<guid>https://arxiv.org/abs/2505.22586</guid>
<content:encoded><![CDATA[
arXiv:2505.22586v2 Announce Type: replace 
Abstract: Large language models (LLMs) often acquire knowledge during pretraining that is undesirable in downstream deployments, e.g., sensitive information or copyrighted content. Existing approaches for removing such knowledge rely on fine-tuning, training low-rank adapters or fact-level editing, but these are either too coarse, too shallow, or ineffective. In this work, we propose PISCES (Precise In-parameter Suppression for Concept EraSure), a novel framework for precisely erasing entire concepts from model parameters by directly editing directions that encode them in parameter space. PISCES uses a disentangler model to decompose MLP vectors into interpretable features, identifies those associated with a target concept using automated interpretability techniques, and removes them from model parameters. Experiments on Gemma 2 and Llama 3.1 over various concepts show that PISCES achieves modest gains in efficacy over leading erasure methods, reducing accuracy on the target concept to as low as 7.7%, while dramatically improving erasure specificity (by up to 31%) and robustness (by up to 38%). Overall, these results demonstrate that feature-based in-parameter editing enables a more precise and reliable approach for removing conceptual knowledge in language models.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs are Better Than You Think: Label-Guided In-Context Learning for Named Entity Recognition</title>
<link>https://arxiv.org/abs/2505.23722</link>
<guid>https://arxiv.org/abs/2505.23722</guid>
<content:encoded><![CDATA[
arXiv:2505.23722v2 Announce Type: replace 
Abstract: In-context learning (ICL) enables large language models (LLMs) to perform new tasks using only a few demonstrations. However, in Named Entity Recognition (NER), existing ICL methods typically rely on task-agnostic semantic similarity for demonstration retrieval, which often yields less relevant examples and leads to inferior results. We introduce DEER, a training-free ICL approach that enables LLMs to make more informed entity predictions through the use of label-grounded statistics. DEER leverages token-level statistics from training labels to identify tokens most informative for entity recognition, enabling entity-focused demonstrations. It further uses these statistics to detect and refine error-prone tokens through a targeted reflection step. Evaluated on five NER datasets across four LLMs, DEER consistently outperforms existing ICL methods and achieves performance comparable to supervised fine-tuning. Further analyses demonstrate that DEER improves example retrieval, remains effective on both seen and unseen entities, and exhibits strong robustness in low-resource settings.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect Hallucinations</title>
<link>https://arxiv.org/abs/2506.01367</link>
<guid>https://arxiv.org/abs/2506.01367</guid>
<content:encoded><![CDATA[
arXiv:2506.01367v3 Announce Type: replace 
Abstract: Large language models (LLMs) have become pervasive in our everyday life. Yet, a fundamental obstacle prevents their use in many critical applications: their propensity to generate fluent, human-quality content that is not grounded in reality. The detection of such hallucinations is thus of the highest importance. In this work, we propose a new method to flag hallucinated content: MMD-Flagger. It relies on Maximum Mean Discrepancy (MMD), a non-parametric distance between distributions. On a high-level perspective, MMD-Flagger tracks the MMD between the output to inspect and counterparts generated with various temperature parameters. We show empirically that inspecting the shape of this trajectory is sufficient to detect most hallucinations. This novel method is benchmarked on machine translation and summarization datasets, on which it exhibits competitive performance relative to natural competitors.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Preference Optimization via Dynamic Target Margins</title>
<link>https://arxiv.org/abs/2506.03690</link>
<guid>https://arxiv.org/abs/2506.03690</guid>
<content:encoded><![CDATA[
arXiv:2506.03690v2 Announce Type: replace 
Abstract: The alignment of Large Language Models (LLMs) is crucial for ensuring their safety and reliability in practical applications. Direct Preference Optimization (DPO) has emerged as an efficient method that directly optimizes models using preference pairs, significantly reducing resource demands. However, the effectiveness of DPO heavily depends on the data quality, which is frequently compromised by noise. In this work, we propose $\gamma$-PO, a dynamic target margin preference optimization algorithm that adjust reward margins at the pairwise level. By introducing instance-specific margin calibration, $\gamma$-PO strategically prioritizes high-confidence pairs (those demonstrating higher reward margins) while suppressing potential noise from ambiguous pairs. Moreover, $\gamma$-PO is a plug-and-play method, compatible with variants of DPO that rely on reward margin between preference pairs. Across benchmarks such as AlpacaEval2 and Arena-Hard, $\gamma$-PO achieves an average 4.4\% improvement over other baselines, setting new benchmarks for state-of-the-art performance. Additionally, $\gamma$-PO requires minimal code changes and has a negligible impact on training efficiency, making it a robust solution for enhancing LLMs alignment. Our codes are available at \href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Many LLMs Are More Utilitarian Than One</title>
<link>https://arxiv.org/abs/2507.00814</link>
<guid>https://arxiv.org/abs/2507.00814</guid>
<content:encoded><![CDATA[
arXiv:2507.00814v2 Announce Type: replace 
Abstract: Moral judgment is integral to large language models' (LLMs) social reasoning. As multi-agent systems gain prominence, it becomes crucial to understand how LLMs function when collaborating compared to operating as individual agents. In human moral judgment, group deliberation leads to a Utilitarian Boost: a tendency to endorse norm violations that inflict harm but maximize benefits for the greatest number of people. We study whether a similar dynamic emerges in multi-agent LLM systems. We test six models on well-established sets of moral dilemmas across two conditions: (1) Solo, where models reason independently, and (2) Group, where they engage in multi-turn discussions in pairs or triads. In personal dilemmas, where agents decide whether to directly harm an individual for the benefit of others, all models rated moral violations as more acceptable when part of a group, demonstrating a Utilitarian Boost similar to that observed in humans. However, the mechanism for the Boost in LLMs differed: While humans in groups become more utilitarian due to heightened sensitivity to decision outcomes, LLM groups showed either reduced sensitivity to norms or enhanced impartiality. We report model differences in when and how strongly the Boost manifests. We also discuss prompt and agent compositions that enhance or mitigate the effect. We end with a discussion of the implications for AI alignment, multi-agent design, and artificial moral reasoning. Code available at: https://github.com/baltaci-r/MoralAgents
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Twice Before You Judge: Mixture of Dual Reasoning Experts for Multimodal Sarcasm Detection</title>
<link>https://arxiv.org/abs/2507.04458</link>
<guid>https://arxiv.org/abs/2507.04458</guid>
<content:encoded><![CDATA[
arXiv:2507.04458v2 Announce Type: replace 
Abstract: Multimodal sarcasm detection has attracted growing interest due to the rise of multimedia posts on social media. Understanding sarcastic image-text posts often requires external contextual knowledge, such as cultural references or commonsense reasoning. However, existing models struggle to capture the deeper rationale behind sarcasm, relying mainly on shallow cues like image captions or object-attribute pairs from images. To address this, we propose \textbf{MiDRE} (\textbf{Mi}xture of \textbf{D}ual \textbf{R}easoning \textbf{E}xperts), which integrates an internal reasoning expert for detecting incongruities within the image-text pair and an external reasoning expert that utilizes structured rationales generated via Chain-of-Thought prompting to a Large Vision-Language Model. An adaptive gating mechanism dynamically weighs the two experts, selecting the most relevant reasoning path. Unlike prior methods that treat external knowledge as static input, MiDRE selectively adapts to when such knowledge is beneficial, mitigating the risks of hallucinated or irrelevant signals from large models. Experiments on two benchmark datasets show that MiDRE achieves superior performance over baselines. Various qualitative analyses highlight the crucial role of external rationales, revealing that even when they are occasionally noisy, they provide valuable cues that guide the model toward a better understanding of sarcasm.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapter-state Sharing CLIP for Parameter-efficient Multimodal Sarcasm Detection</title>
<link>https://arxiv.org/abs/2507.04508</link>
<guid>https://arxiv.org/abs/2507.04508</guid>
<content:encoded><![CDATA[
arXiv:2507.04508v2 Announce Type: replace 
Abstract: The growing prevalence of multimodal image-text sarcasm on social media poses challenges for opinion mining systems. Existing approaches rely on full fine-tuning of large models, making them unsuitable to adapt under resource-constrained settings. While recent parameter-efficient fine-tuning (PEFT) methods offer promise, their off-the-shelf use underperforms on complex tasks like sarcasm detection. We propose AdS-CLIP (Adapter-state Sharing in CLIP), a lightweight framework built on CLIP that inserts adapters only in the upper layers to preserve low-level unimodal representations in the lower layers and introduces a novel adapter-state sharing mechanism, where textual adapters guide visual ones to promote efficient cross-modal learning in the upper layers. Experiments on two public benchmarks demonstrate that AdS-CLIP not only outperforms standard PEFT methods but also existing multimodal baselines with significantly fewer trainable parameters.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Information Utility in Key-Value Memory for Language Model Post-Training</title>
<link>https://arxiv.org/abs/2507.05158</link>
<guid>https://arxiv.org/abs/2507.05158</guid>
<content:encoded><![CDATA[
arXiv:2507.05158v2 Announce Type: replace 
Abstract: Recent advancements in language models (LMs) have marked a shift toward the growing importance of post-training. Yet, post-training approaches such as supervised fine-tuning (SFT) do not guarantee the effective use of knowledge acquired during pretraining. We therefore introduce InfoSteer, a lightweight method that encourages parametric information utilization in LMs during post-training. Specifically, InfoSteer treats the feed-forward network (FFN) layer as associate key-value memory and promotes the use of stored memory vectors via forward-pass interventions or regularization during backpropagation. This simple guidance during post-training phase yields consistent performance improvements across diverse model families -- including Qwen, Gemma and Llama -- spanning 15 downstream tasks in both in-distribution (ID) and out-of-distribution (OOD) evaluations. Beyond performance gains, we also find that steered LMs can adaptively allocate information by placing more emphasis on generating semantically meaningful tokens, while using fewer resources on simple transition ones (e.g., `\texttt{,}' or `\texttt{and}'). Our work underscores that vanilla post-training does not fully exploit the potential gained during pre-training, and that steering LMs in latent representation space offers a promising approach to enhance both performance and interpretability. The code is available at: https://github.com/chili-lab/InfoSteer.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InsurTech innovation using natural language processing</title>
<link>https://arxiv.org/abs/2507.21112</link>
<guid>https://arxiv.org/abs/2507.21112</guid>
<content:encoded><![CDATA[
arXiv:2507.21112v2 Announce Type: replace 
Abstract: With the rapid rise of InsurTech, traditional insurance companies are increasingly exploring alternative data sources and advanced technologies to sustain their competitive edge. This paper provides both a conceptual overview and practical case studies of natural language processing (NLP) and its emerging applications within insurance operations, focusing on transforming raw, unstructured text into structured data suitable for actuarial analysis and decision-making. Leveraging real-world alternative data provided by an InsurTech industry partner that enriches traditional insurance data sources, we apply various NLP techniques to demonstrate feature de-biasing, feature compression, and industry classification in the commercial insurance context. These enriched, text-derived insights not only add to and refine traditional rating factors for commercial insurance pricing but also offer novel perspectives for assessing underlying risk by introducing novel industry classification techniques. Through these demonstrations, we show that NLP is not merely a supplementary tool but a foundational element of modern, data-driven insurance analytics.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph</title>
<link>https://arxiv.org/abs/2507.22811</link>
<guid>https://arxiv.org/abs/2507.22811</guid>
<content:encoded><![CDATA[
arXiv:2507.22811v2 Announce Type: replace 
Abstract: In this work we present an entity linker for DBLP's 2025 version of RDF-based Knowledge Graph. Compared to the 2022 version, DBLP now considers publication venues as a new entity type called dblp:Stream. In the earlier version of DBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce entity linkings. In contrast, in this work, we develop a zero-shot entity linker using LLMs using a novel method, where we re-rank candidate entities based on the log-probabilities of the "yes" token output at the penultimate layer of the LLM.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous Speech Translation</title>
<link>https://arxiv.org/abs/2509.01200</link>
<guid>https://arxiv.org/abs/2509.01200</guid>
<content:encoded><![CDATA[
arXiv:2509.01200v2 Announce Type: replace 
Abstract: Simultaneous Speech Translation (SimulST) enables real-time cross-lingual communication by jointly optimizing speech recognition and machine translation under strict latency constraints. Existing systems struggle to balance translation quality, latency, and semantic coherence, particularly in multilingual many-to-many scenarios where divergent read and write policies hinder unified strategy learning. In this paper, we present SimulMEGA (Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy learning framework that combines prefix-based training with a Mixture-of-Experts refiner to learn effective read and write decisions in an implicit manner, without adding inference-time overhead. Our design requires only minimal modifications to standard transformer architectures and generalizes across both speech-to-text and text-to-speech streaming tasks. Through comprehensive evaluation on six language pairs, our 500M parameter speech-to-text model outperforms the Seamless baseline, achieving under 7 percent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3 seconds. We further demonstrate the versatility of SimulMEGA by extending it to streaming TTS with a unidirectional backbone, yielding superior latency quality tradeoffs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and Interaction</title>
<link>https://arxiv.org/abs/2509.19902</link>
<guid>https://arxiv.org/abs/2509.19902</guid>
<content:encoded><![CDATA[
arXiv:2509.19902v2 Announce Type: replace 
Abstract: In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on a large language model (LLM) for speech understanding, generation, and interaction. There are three key features of WEST: 1) Fully LLM-based: Standing on the shoulders of giants by reusing mature architectures, ecosystems (e.g., Hugging Face), and methods (e.g., sequence packing) from large models. 2) Full-stack: Supports tasks such as recognition, synthesis, understanding, dialogue, and multimodal capabilities, with extensibility to incorporate open-source models. 3) Simple and Stupid: A simple and stupid speech toolkit that everyone can Touch. In addition, WEST provides two types of recipes, models, and experimental results. The first is entirely based on open-source models and open-source data, allowing users to fully reproduce the experiments in this paper and serving as a verification system or minimal system baseline. The second is trained on massive data, offering superior performance so the user can directly apply it out of the box. WEST is publicly avilable at https://github.com/wenet-e2e/west/
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines</title>
<link>https://arxiv.org/abs/2509.21320</link>
<guid>https://arxiv.org/abs/2509.21320</guid>
<content:encoded><![CDATA[
arXiv:2509.21320v2 Announce Type: replace 
Abstract: We present a scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on a 206B-token corpus spanning scientific text, pure sequences, and sequence-text pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text/knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at https://huggingface.co/SciReason and https://github.com/open-sciencelab/SciReason.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReSeek: A Self-Correcting Framework for Search Agents with Instructive Rewards</title>
<link>https://arxiv.org/abs/2510.00568</link>
<guid>https://arxiv.org/abs/2510.00568</guid>
<content:encoded><![CDATA[
arXiv:2510.00568v2 Announce Type: replace 
Abstract: Search agents powered by Large Language Models (LLMs) have demonstrated significant potential in tackling knowledge-intensive tasks. Reinforcement learning (RL) has emerged as a powerful paradigm for training these agents to perform complex, multi-step reasoning. However, prior RL-based methods often rely on sparse or rule-based rewards, which can lead agents to commit to suboptimal or erroneous reasoning paths without the ability to recover. To address these limitations, we propose ReSeek, a novel self-correcting framework for training search agents. Our framework introduces a self-correction mechanism that empowers the agent to dynamically identify and recover from erroneous search paths during an episode. By invoking a special JUDGE action, the agent can judge the information and re-plan its search strategy. To guide this process, we design a dense, instructive process reward function, which decomposes into a correctness reward for retrieving factual information and a utility reward for finding information genuinely useful for the query. Furthermore, to mitigate the risk of data contamination in existing datasets, we introduce FictionalHot, a new and challenging benchmark with recently curated questions requiring complex reasoning. Being intuitively reasonable and practically simple, extensive experiments show that agents trained with ReSeek significantly outperform SOTA baselines in task success rate and path faithfulness.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMAS: Adaptively Determining Communication Topology for LLM-based Multi-Agent System</title>
<link>https://arxiv.org/abs/2510.01617</link>
<guid>https://arxiv.org/abs/2510.01617</guid>
<content:encoded><![CDATA[
arXiv:2510.01617v3 Announce Type: replace 
Abstract: Although large language models (LLMs) have revolutionized natural language processing capabilities, their practical implementation as autonomous multi-agent systems (MAS) for industrial problem-solving encounters persistent barriers. Conventional MAS architectures are fundamentally restricted by inflexible, hand-crafted graph topologies that lack contextual responsiveness, resulting in diminished efficacy across varied academic and commercial workloads. To surmount these constraints, we introduce AMAS, a paradigm-shifting framework that redefines LLM-based MAS through a novel dynamic graph designer. This component autonomously identifies task-specific optimal graph configurations via lightweight LLM adaptation, eliminating the reliance on monolithic, universally applied structural templates. Instead, AMAS exploits the intrinsic properties of individual inputs to intelligently direct query trajectories through task-optimized agent pathways. Rigorous validation across question answering, mathematical deduction, and code generation benchmarks confirms that AMAS systematically exceeds state-of-the-art single-agent and multi-agent approaches across diverse LLM architectures. Our investigation establishes that context-sensitive structural adaptability constitutes a foundational requirement for high-performance LLM MAS deployments.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method</title>
<link>https://arxiv.org/abs/2510.04655</link>
<guid>https://arxiv.org/abs/2510.04655</guid>
<content:encoded><![CDATA[
arXiv:2510.04655v2 Announce Type: replace 
Abstract: Knowledge of the medical decision process, which can be modeled as medical decision trees (MDTs), is critical to building clinical decision support systems. However, current MDT construction methods rely heavily on time-consuming and laborious manual annotation. To address this challenge, we propose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for automatically extracting MDTs from clinical guidelines and textbooks. We integrate gradient path information to capture synergistic effects between different modules, enabling more effective and reliable rank allocation. This framework ensures that the most critical modules receive appropriate rank allocations while less important ones are pruned, resulting in a more efficient and accurate model for extracting medical decision trees from clinical texts. Extensive experiments on medical guideline datasets demonstrate that our PI-LoRA method significantly outperforms existing parameter-efficient fine-tuning approaches for the Text2MDT task, achieving better accuracy with substantially reduced model complexity. The proposed method achieves state-of-the-art results while maintaining a lightweight architecture, making it particularly suitable for clinical decision support systems where computational resources may be limited.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmenting Dialog with Think-Aloud Utterances for Modeling Individual Personality Traits by LLM</title>
<link>https://arxiv.org/abs/2510.09158</link>
<guid>https://arxiv.org/abs/2510.09158</guid>
<content:encoded><![CDATA[
arXiv:2510.09158v2 Announce Type: replace 
Abstract: This study proposes augmenting dialog data with think-aloud utterances (TAUs) for modeling individual personalities in text chat by LLM. TAU is a verbalization of a speaker's thought before articulating the utterance. We expect "persona LLMs" trained with TAU-augmented data can mimic the speaker's personality trait better. We tested whether the trained persona LLMs obtain the human personality with respect to Big Five, a framework characterizing human personality traits from five aspects. The results showed that LLMs trained with TAU-augmented data more closely align to the speakers' Agreeableness and Neuroticism of Big Five than those trained with original dialog data. We also found that the quality of TAU-augmentation impacts persona LLM's performance.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multilingual, Large-Scale Study of the Interplay between LLM Safeguards, Personalisation, and Disinformation</title>
<link>https://arxiv.org/abs/2510.12993</link>
<guid>https://arxiv.org/abs/2510.12993</guid>
<content:encoded><![CDATA[
arXiv:2510.12993v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) can generate human-like disinformation, yet their ability to personalise such content across languages and demographics remains underexplored. This study presents the first large-scale, multilingual analysis of persona-targeted disinformation generation by LLMs. Employing a red teaming methodology, we prompt eight state-of-the-art LLMs with 324 false narratives and 150 demographic personas (combinations of country, generation, and political orientation) across four languages--English, Russian, Portuguese, and Hindi--resulting in AI-TRAITS, a comprehensive dataset of 1.6 million personalised disinformation texts. Results show that the use of even simple personalisation prompts significantly increases the likelihood of jailbreaks across all studied LLMs, up to 10 percentage points, and alters linguistic and rhetorical patterns that enhance narrative persuasiveness. Models such as Grok and GPT exhibited jailbreak rates and personalisation scores both exceeding 85%. These insights expose critical vulnerabilities in current state-of-the-art LLMs and offer a foundation for improving safety alignment and detection strategies in multilingual and cross-demographic contexts.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups</title>
<link>https://arxiv.org/abs/2510.13852</link>
<guid>https://arxiv.org/abs/2510.13852</guid>
<content:encoded><![CDATA[
arXiv:2510.13852v2 Announce Type: replace 
Abstract: Is an LLM telling you different facts than it's telling me? This paper introduces ConsistencyAI, an independent benchmark for measuring the factual consistency of large language models (LLMs) for different personas. ConsistencyAI tests whether, when users of different demographics ask identical questions, the model responds with factually inconsistent answers. Designed without involvement from LLM providers, this benchmark offers impartial evaluation and accountability. In our experiment, we queried 19 LLMs with prompts that requested 5 facts for each of 15 topics. We repeated this query 100 times for each LLM, each time adding prompt context from a different persona selected from a subset of personas modeling the general population. We processed the responses into sentence embeddings, computed cross-persona cosine similarity, and computed the weighted average of cross-persona cosine similarity to calculate factual consistency scores. In 100-persona experiments, scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as a benchmark threshold. xAI's Grok-3 is most consistent, while several lightweight models rank lowest. Consistency varies by topic: the job market is least consistent, G7 world leaders most consistent, and issues like vaccines or the Israeli-Palestinian conflict diverge by provider. These results show that both the provider and the topic shape the factual consistency. We release our code and interactive demo to support reproducible evaluation and encourage persona-invariant prompting strategies.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Phonosemantic Iconicity Distributionally in 6 Languages</title>
<link>https://arxiv.org/abs/2510.14040</link>
<guid>https://arxiv.org/abs/2510.14040</guid>
<content:encoded><![CDATA[
arXiv:2510.14040v2 Announce Type: replace 
Abstract: Language is, as commonly theorized, largely arbitrary. Yet, systematic relationships between phonetics and semantics have been observed in many specific cases. To what degree could those systematic relationships manifest themselves in large scale, quantitative investigations--both in previously identified and unidentified phenomena? This work undertakes a distributional approach to quantifying phonosemantic iconicity at scale across 6 diverse languages (English, Spanish, Hindi, Finnish, Turkish, and Tamil). In each language, we analyze the alignment of morphemes' phonetic and semantic similarity spaces with a suite of statistical measures, and discover an array of interpretable phonosemantic alignments not previously identified in the literature, along with crosslinguistic patterns. We also analyze 5 previously hypothesized phonosemantic alignments, finding support for some such alignments and mixed results for others.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Transformer: Accelerating model inference via quantum linear algebra</title>
<link>https://arxiv.org/abs/2402.16714</link>
<guid>https://arxiv.org/abs/2402.16714</guid>
<content:encoded><![CDATA[
arXiv:2402.16714v3 Announce Type: replace-cross 
Abstract: Powerful generative artificial intelligence from large language models (LLMs) harnesses extensive computational resources for inference. In this work, we investigate the transformer architecture, a key component of these models, under the lens of fault-tolerant quantum computing. We develop quantum subroutines to construct the building blocks in the transformer, including the self-attention, residual connection with layer normalization, and feed-forward network. As an important subroutine, we show how to efficiently implement the Hadamard product and element-wise functions of matrices on quantum computers. Our algorithm prepares an amplitude encoding of the transformer output, which can be measured for prediction or use in the next layer. We find that the matrix norm of the input sequence plays a dominant role in the quantum complexity. With numerical experiments on open-source LLMs, including for bio-informatics applications, we demonstrate the potential of a quantum speedup for transformer inference in practical regimes.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CURATRON: Complete and Robust Preference Data for Rigorous Alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2403.02745</link>
<guid>https://arxiv.org/abs/2403.02745</guid>
<content:encoded><![CDATA[
arXiv:2403.02745v3 Announce Type: replace-cross 
Abstract: This paper addresses the challenges of aligning large language models (LLMs) with human values via preference learning (PL), focusing on incomplete and corrupted data in preference datasets. We propose a novel method for robustly and completely recalibrating values within these datasets to enhance LLMs' resilience against the issues. In particular, we devise a guaranteed polynomial time ranking algorithm that robustifies several existing models, such as the classic Bradley-Terry-Luce (BTL) (Bradley and Terry, 1952) model and certain generalizations of it. To the best of our knowledge, our present work is the first to propose an algorithm that provably recovers an $\epsilon$-optimal ranking with high probability while allowing as large as $O(n)$ perturbed pairwise comparison results per model response. Furthermore, we show robust recovery results in the partially observed setting. Our experiments confirm that our algorithms handle adversarial noise and unobserved comparisons well in both general and LLM preference dataset settings. This work contributes to the development and scaling of more reliable and ethically aligned AI models by equipping the dataset curation pipeline with the ability to handle missing and maliciously manipulated inputs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Markovian Discrete Diffusion with Causal Language Models</title>
<link>https://arxiv.org/abs/2502.09767</link>
<guid>https://arxiv.org/abs/2502.09767</guid>
<content:encoded><![CDATA[
arXiv:2502.09767v3 Announce Type: replace-cross 
Abstract: Discrete diffusion models offer a flexible, controllable approach to structured sequence generation, yet they still lag behind causal language models in expressive power. A key limitation lies in their reliance on the Markovian assumption, which restricts each step to condition only on the current state, leading to potential uncorrectable error accumulation. In this paper, we introduce CaDDi (Causal Discrete Diffusion Model), a discrete diffusion model that conditions on the entire generative trajectory, thereby lifting the Markov constraint and allowing the model to revisit and improve past states. By unifying sequential (causal) and temporal (diffusion) reasoning in a single non-Markovian transformer, CaDDi also treats standard causal language models as a special case and permits the direct reuse of pretrained LLM weights with no architectural changes. Empirically, CaDDi outperforms state-of-the-art discrete diffusion baselines on natural-language benchmarks, substantially narrowing the remaining gap to large autoregressive transformers.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation</title>
<link>https://arxiv.org/abs/2503.05493</link>
<guid>https://arxiv.org/abs/2503.05493</guid>
<content:encoded><![CDATA[
arXiv:2503.05493v2 Announce Type: replace-cross 
Abstract: In recent years, integrating large language models (LLMs) into recommender systems has created new opportunities for improving recommendation quality. However, a comprehensive benchmark is needed to thoroughly evaluate and compare the recommendation capabilities of LLMs with traditional recommender systems. In this paper, we introduce RecBench, which systematically investigates various item representation forms (including unique identifier, text, semantic embedding, and semantic identifier) and evaluates two primary recommendation tasks, i.e., click-through rate prediction (CTR) and sequential recommendation (SeqRec). Our extensive experiments cover up to 17 large models and are conducted across five diverse datasets from fashion, news, video, books, and music domains. Our findings indicate that LLM-based recommenders outperform conventional recommenders, achieving up to a 5% AUC improvement in the CTR scenario and up to a 170% NDCG@10 improvement in the SeqRec scenario. However, these substantial performance gains come at the expense of significantly reduced inference efficiency, rendering the LLM-as-RS paradigm impractical for real-time recommendation environments. We aim for our findings to inspire future research, including recommendation-specific model acceleration methods. We will release our code, data, configurations, and platform to enable other researchers to reproduce and build upon our experimental results.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models</title>
<link>https://arxiv.org/abs/2505.10844</link>
<guid>https://arxiv.org/abs/2505.10844</guid>
<content:encoded><![CDATA[
arXiv:2505.10844v4 Announce Type: replace-cross 
Abstract: Accuracy remains a standard metric for evaluating AI systems, but it offers limited insight into how models arrive at their solutions. In this work, we introduce a benchmark based on brainteasers written in long narrative form to probe more deeply into the types of reasoning strategies that models use. Brainteasers are well-suited for this goal because they can be solved with multiple approaches, such as a few-step solution that uses a creative insight or a longer solution that uses more brute force. We investigate large language models (LLMs) across multiple layers of reasoning, focusing not only on correctness but also on the quality and creativity of their solutions. We investigate many aspects of the reasoning process: (1) semantic parsing of the brainteasers into precise mathematical competition style formats; (2) generating solutions from these mathematical forms; (3) self-correcting solutions based on gold solutions; (4) producing step-by-step sketches of solutions; and (5) making use of hints. We find that LLMs are in many cases able to find creative, insightful solutions to brainteasers, suggesting that they capture some of the capacities needed to solve novel problems in creative ways. Nonetheless, there also remain situations where they rely on brute force despite the availability of more efficient, creative solutions, highlighting a potential direction for improvement in the reasoning abilities of LLMs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems</title>
<link>https://arxiv.org/abs/2505.15201</link>
<guid>https://arxiv.org/abs/2505.15201</guid>
<content:encoded><![CDATA[
arXiv:2505.15201v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts for each problem and reward them independently. This optimizes for pass@1 performance and prioritizes the strength of isolated samples at the expense of the diversity and collective utility of sets of samples. This under-utilizes the sampling capacity, limiting exploration and eventual improvement on harder examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a transformation on the final rewards which leads to direct optimization of pass@k performance, thus optimizing for sets of samples that maximize reward when considered jointly. Our contribution is to derive novel low variance unbiased estimators for pass@k and its gradient, in both the binary and continuous reward settings. We show optimization with our estimators reduces to standard RL with rewards that have been jointly transformed by a stable and efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of trading off pass@1 performance for pass@k gains, our method allows annealing k during training, optimizing both metrics and often achieving strong pass@1 numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the variance reducing properties of our formulations. We also include real-world examples using the open-source LLM, GEMMA-2. We find that our transformation effectively optimizes for the target k. Furthermore, higher k values enable solving more and harder problems, while annealing k boosts both the pass@1 and pass@k . Crucially, for challenging task sets where conventional pass@1 optimization stalls, our pass@k approach unblocks learning, likely due to better exploration by prioritizing joint utility over the utility of individual samples.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions</title>
<link>https://arxiv.org/abs/2505.17818</link>
<guid>https://arxiv.org/abs/2505.17818</guid>
<content:encoded><![CDATA[
arXiv:2505.17818v2 Announce Type: replace-cross 
Abstract: Doctor-patient consultations require multi-turn, context-aware communication tailored to diverse patient personas. Training or evaluating doctor LLMs in such settings requires realistic patient interaction systems. However, existing simulators often fail to reflect the full range of personas seen in clinical practice. To address this, we introduce PatientSim, a patient simulator that generates realistic and diverse patient personas for clinical scenarios, grounded in medical expertise. PatientSim operates using: 1) clinical profiles, including symptoms and medical history, derived from real-world data in the MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes: personality, language proficiency, medical history recall level, and cognitive confusion level, resulting in 37 unique combinations. We evaluate eight LLMs for factual accuracy and persona consistency. The top-performing open-source model, Llama 3.3 70B, is validated by four clinicians to confirm the robustness of our framework. As an open-source, customizable platform, PatientSim provides a reproducible and scalable solution that can be customized for specific training needs. Offering a privacy-compliant environment, it serves as a robust testbed for evaluating medical dialogue systems across diverse patient presentations and shows promise as an educational tool for healthcare. The code is available at https://github.com/dek924/PatientSim.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning Teachers of Test Time Scaling</title>
<link>https://arxiv.org/abs/2506.08388</link>
<guid>https://arxiv.org/abs/2506.08388</guid>
<content:encoded><![CDATA[
arXiv:2506.08388v3 Announce Type: replace-cross 
Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for one-hot correctness inherently relies on the LM being able to explore and solve its task with some chance at initialization. Furthermore, a key use case of reasoning LMs is to act as teachers for distilling new students and cold-starting future RL iterations rather than being deployed themselves. From these considerations, we introduce a new framework that avoids RL's exploration challenge by training a new class of Reinforcement-Learned Teachers (RLTs) focused on yielding the most effective downstream distillation. RLTs are prompted with both the question and solution to each problem, and tasked to simply "connect-the-dots" with detailed explanations tailored for their students. We train RLTs with dense rewards obtained by feeding each explanation to the student and testing its understanding of the problem's solution. In practice, the raw outputs of a 7B RLT provide higher final performance on competition and graduate-level tasks than existing distillation and cold-starting pipelines that collect and postprocess the reasoning traces of orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness when training larger students and when applied zero-shot to out-of-distribution tasks, unlocking new levels of efficiency and re-usability for the RL reasoning framework. Code available at: https://github.com/SakanaAI/RLT
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization</title>
<link>https://arxiv.org/abs/2506.12484</link>
<guid>https://arxiv.org/abs/2506.12484</guid>
<content:encoded><![CDATA[
arXiv:2506.12484v4 Announce Type: replace-cross 
Abstract: Language models can retain dangerous knowledge and skills even after extensive safety fine-tuning, posing both misuse and misalignment risks. Recent studies show that even specialized unlearning methods can be easily reversed. To address this, we systematically evaluate many existing and novel components of unlearning methods and identify ones crucial for irreversible unlearning.
  We introduce Disruption Masking, a technique in which we only allow updating weights, where the signs of the unlearning gradient and the retaining gradient are the same. This ensures all updates are non-disruptive.
  Additionally, we identify the need for normalizing the unlearning gradients, and also confirm the usefulness of meta-learning. We combine these insights into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and validate its effectiveness at preventing the recovery of dangerous capabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new state-of-the-art for robust unlearning.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models</title>
<link>https://arxiv.org/abs/2506.17585</link>
<guid>https://arxiv.org/abs/2506.17585</guid>
<content:encoded><![CDATA[
arXiv:2506.17585v2 Announce Type: replace-cross 
Abstract: Trustworthy language models should provide both correct and verifiable answers. However, citations generated directly by standalone LLMs are often unreliable. As a result, current systems insert citations by querying an external retriever at inference time, introducing latency, infrastructure dependence, and vulnerability to retrieval noise. We explore whether LLMs can be made to reliably attribute to the documents seen during continual pretraining without test-time retrieval, by revising the training process. To study this, we construct CitePretrainBench, a benchmark that mixes real-world corpora (Wikipedia, Common Crawl, arXiv) with novel documents and probes both short-form (single-fact) and long-form (multi-fact) citation tasks. Our approach follows a two-stage process: (1) continual pretraining to index factual knowledge by binding it to persistent document identifiers; and (2) instruction tuning to elicit citation behavior. We introduce Active Indexing for the first stage, which creates generalizable, source-anchored bindings by augmenting training with synthetic data that (i) restate each fact in diverse, compositional forms and (ii) enforce bidirectional training (source-to-fact and fact-to-source). This equips the model to both generate content from a cited source and attribute its own answers, improving robustness to paraphrase and composition. Experiments with Qwen-2.5-7B&3B show that Active Indexing consistently outperforms a Passive Indexing baseline, which simply appends an identifier to each document, achieving citation precision gains of up to 30.2% across all tasks and models. Our ablation studies reveal that performance continues to improve as we scale the amount of augmented data, showing a clear upward trend even at 16x the original token count. Finally, we show that internal citations complement external ones by making the model more robust to retrieval noise.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential Mamba</title>
<link>https://arxiv.org/abs/2507.06204</link>
<guid>https://arxiv.org/abs/2507.06204</guid>
<content:encoded><![CDATA[
arXiv:2507.06204v2 Announce Type: replace-cross 
Abstract: Sequence models like Transformers and RNNs often overallocate attention to irrelevant context, leading to noisy intermediate representations. This degrades LLM capabilities by promoting hallucinations, weakening long-range and retrieval abilities, and reducing robustness. Recent work has shown that differential design can mitigate this issue in Transformers, improving their effectiveness across various applications. In this paper, we explore whether these techniques, originally developed for Transformers, can be applied to Mamba, a recent architecture based on selective state-space layers that achieves Transformer-level performance with greater efficiency. We show that a naive adaptation of differential design to Mamba is insufficient and requires careful architectural modifications. To address this, we introduce a novel differential mechanism for Mamba, empirically validated on language modeling benchmarks, demonstrating improved retrieval capabilities and superior performance over vanilla Mamba. Finally, we conduct extensive ablation studies and empirical analyses to justify our design choices and provide evidence that our approach effectively mitigates the overallocation problem in Mamba-based models. Our code is publicly available: https://github.com/NadavSc/Diff-Mamba
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation</title>
<link>https://arxiv.org/abs/2507.17937</link>
<guid>https://arxiv.org/abs/2507.17937</guid>
<content:encoded><![CDATA[
arXiv:2507.17937v3 Announce Type: replace-cross 
Abstract: Generative AI systems for music and video commonly use text-based filters to prevent the regurgitation of copyrighted material. We expose a fundamental flaw in this approach by introducing Adversarial PhoneTic Prompting (APT), a novel attack that bypasses these safeguards by exploiting phonetic memorization. The APT attack replaces iconic lyrics with homophonic but semantically unrelated alternatives (e.g., "mom's spaghetti" becomes "Bob's confetti"), preserving acoustic structure while altering meaning; we identify high-fidelity phonetic matches using CMU pronouncing dictionary. We demonstrate that leading Lyrics-to-Song (L2S) models like SUNO and YuE regenerate songs with striking melodic and rhythmic similarity to their copyrighted originals when prompted with these altered lyrics. More surprisingly, this vulnerability extends across modalities. When prompted with phonetically modified lyrics from a song, a Text-to-Video (T2V) model like Veo 3 reconstructs visual scenes from the original music video-including specific settings and character archetypes-despite the absence of any visual cues in the prompt. Our findings reveal that models memorize deep, structural patterns tied to acoustics, not just verbatim text. This phonetic-to-visual leakage represents a critical vulnerability in transcript-conditioned generative models, rendering simple copyright filters ineffective and raising urgent concerns about the secure deployment of multimodal AI systems. Demo examples are available at our project page (https://jrohsc.github.io/music_attack/).
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GradeSQL: Test-Time Inference with Outcome Reward Models for Text-to-SQL Generation from Large Language Models</title>
<link>https://arxiv.org/abs/2509.01308</link>
<guid>https://arxiv.org/abs/2509.01308</guid>
<content:encoded><![CDATA[
arXiv:2509.01308v2 Announce Type: replace-cross 
Abstract: Text-to-SQL, the task of translating natural language questions into SQL queries, has significantly advanced with the introduction of Large Language Models (LLMs), broadening database accessibility for a wide range of users. Despite substantial progress in generating valid SQL, current LLMs still struggle with complex queries. To address this limitation, test-time strategies such as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on the assumption that LLMs can produce correct answers after multiple attempts. However, these methods rely on surface-level heuristics, selecting the syntactically correct query through execution-based BoN (ex-BoN) or the most frequently generated one through Majority Voting. Recently, Outcome Reward Models (ORMs), which assign utility scores to generated outputs based on semantic correctness, have emerged as a promising reinforcement learning approach for improving model alignment. We argue that ORMs could serve as an effective new test-time heuristic, although their application in this context remains largely underexplored.
  In this work, we propose a unified framework for training ORMs tailored to the Text-to-SQL task and assess their effectiveness as a test-time heuristic within the BoN strategy. We benchmark ORMs against ex-BoN and Maj across the BIRD and Spider datasets, fine-tuning diverse open-source LLMs from the Qwen2, Granite3, and Llama3 families. Results show that ORMs outperform ex-BoN and Maj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider) over ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further demonstrate that finetuning models already aligned with SQL generation, such as OmniSQL, yields superior ORM performance. Additionally, we observe that ORMs achieve competitive results on simple queries and benefit more from an increased number of candidates compared to ex-BoN and Maj.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Landscape of Agentic Reinforcement Learning for LLMs: A Survey</title>
<link>https://arxiv.org/abs/2509.02547</link>
<guid>https://arxiv.org/abs/2509.02547</guid>
<content:encoded><![CDATA[
arXiv:2509.02547v2 Announce Type: replace-cross 
Abstract: The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>p-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding</title>
<link>https://arxiv.org/abs/2509.23234</link>
<guid>https://arxiv.org/abs/2509.23234</guid>
<content:encoded><![CDATA[
arXiv:2509.23234v4 Announce Type: replace-cross 
Abstract: Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step. While a variety of such sampling methods have been proposed, their performance can be sensitive to the selection of hyperparameters which may require different settings depending upon the generation task and temperature configuration. In this work, we introduce $p$-less sampling: an information-theoretic approach to sampling which dynamically sets a truncation threshold at each decoding step based on the entire token probability distribution. Unlike existing methods, $p$-less sampling has no hyperparameters and consistently produces high-quality outputs as temperature increases. We provide theoretical perspectives on $p$-less sampling to ground our proposed method and conduct experiments to empirically validate its effectiveness across a range of math, logical reasoning, and creative writing tasks. Our results demonstrate how $p$-less sampling consistently outperforms existing sampling approaches while exhibiting much less degradation in text quality at higher temperature values. We further show how $p$-less achieves greater inference-time efficiency than alternative methods through lower average token sampling times and shorter generation lengths, without sacrificing accuracy. Finally, we provide analyses to highlight the benefits of $p$-less through qualitative examples, case studies, and diversity assessments. The code is available at https://github.com/ryttry/p-less .
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Agreement Enables Efficient Open-Ended LLM Cascades</title>
<link>https://arxiv.org/abs/2509.21837</link>
<guid>https://arxiv.org/abs/2509.21837</guid>
<content:encoded><![CDATA[
<div> Keywords: Cascade systems, open-ended text generation, semantic agreement, reliability signal, LLM deployment

Summary:
Cascade systems in open-ended text generation face challenges in determining output reliability due to the continuous spectrum of generation quality. To address this, the proposed method uses semantic agreement as a training-free signal for reliable deferral, finding it to be a stronger reliability signal than token-level confidence when diverse model outputs agree semantically. The evaluation across models ranging from 500M to 70B parameters shows that semantic cascades can match or surpass target-model quality at a lower cost, reducing latency by up to 60%. The method is practical for real-world LLM deployment as it requires no access to model internals, works across black-box APIs, and remains robust to model updates. <div>
arXiv:2509.21837v3 Announce Type: replace 
Abstract: Cascade systems route computational requests to smaller models when possible and defer to larger models only when necessary, offering a promising approach to balance cost and quality in LLM deployment. However, they face a fundamental challenge in open-ended text generation: determining output reliability when generation quality lies on a continuous spectrum, often with multiple valid responses. To address this, we propose semantic agreement -- meaning-level consensus between ensemble outputs -- as a training-free signal for reliable deferral. We show that when diverse model outputs agree semantically, their consensus is a stronger reliability signal than token-level confidence. Evaluated from 500M to 70B-parameter models, we find that semantic cascades match or surpass target-model quality at 40% of the cost and reduce latency by up to 60%. Our method requires no model internals, works across black-box APIs, and remains robust to model updates, making it a practical baseline for real-world LLM deployment.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Long-Term Memory for Long-Context Question Answering</title>
<link>https://arxiv.org/abs/2510.23730</link>
<guid>https://arxiv.org/abs/2510.23730</guid>
<content:encoded><![CDATA[
<div> memory-augmented methods, LoCoMo benchmark, long-context dialogues, question-answering tasks, reasoning strategies  
Summary:  
- Research emphasizes the importance of memory in large language models for conversational continuity and experiential learning.  
- The study evaluates various memory-augmented methods using the LoCoMo benchmark, analyzing full-context prompting, semantic memory, agentic memory, episodic memory, and procedural memory.  
- Memory-augmented approaches reduce token usage significantly while maintaining competitive accuracy levels.  
- Memory architecture complexity should align with model capability, with different memory types benefiting small foundation models and more complex reasoning models differently.  
- Episodic memory is particularly beneficial as it helps LLMs understand the boundaries of their knowledge. <div>
arXiv:2510.23730v1 Announce Type: new 
Abstract: In order for large language models to achieve true conversational continuity and benefit from experiential learning, they need memory. While research has focused on the development of complex memory systems, it remains unclear which types of memory are most effective for long-context conversational tasks. We present a systematic evaluation of memory-augmented methods using LoCoMo, a benchmark of synthetic long-context dialogues annotated for question-answering tasks that require diverse reasoning strategies. We analyse full-context prompting, semantic memory through retrieval-augmented generation and agentic memory, episodic memory through in-context learning, and procedural memory through prompt optimization. Our findings show that memory-augmented approaches reduce token usage by over 90% while maintaining competitive accuracy. Memory architecture complexity should scale with model capability, with small foundation models benefitting most from RAG, and strong instruction-tuned reasoning model gaining from episodic learning through reflections and more complex agentic semantic memory. In particular, episodic memory can help LLMs recognise the limits of their own knowledge.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BitSkip: An Empirical Analysis of Quantization and Early Exit Composition</title>
<link>https://arxiv.org/abs/2510.23766</link>
<guid>https://arxiv.org/abs/2510.23766</guid>
<content:encoded><![CDATA[
<div> efficient Large Language Models, BitSkip, quantized model, Hadamard transform, early-exit

Summary:
BitSkip is a hybrid architectural framework that explores the interactions between extreme quantization and dynamic routing techniques in Large Language Models (LLMs). The study found that a simple 8-bit quantized model without Hadamard transform (BitSkip-V1) outperformed more complex 4-bit and Hadamard-enhanced models, as well as competing with full-precision baselines in terms of quality. The introduction of Hadamard transforms, even at 8-bit precision, led to a catastrophic degradation in performance due to fundamental training instability. BitSkip-V1 demonstrated superior early-exit characteristics, with layer 18 providing an optimal 32.5% speed gain for a minimal 4% quality loss. The findings suggest that simplicity in model architecture can sometimes outperform more complex techniques in the quest for efficient Large Language Models. 

<br /><br />Summary: <div>
arXiv:2510.23766v1 Announce Type: new 
Abstract: The pursuit of efficient Large Language Models (LLMs) has led to increasingly complex techniques like extreme quantization and dynamic routing. While individual benefits of these methods are well-documented, their compositional effects remain poorly understood. This paper introduces BitSkip, a hybrid architectural framework for systematically explor- ing these interactions. Counter-intuitively, our findings reveal that a simple 8-bit quantized model without Hadamard transform (BitSkip-V1) not only outperforms its more complex 4-bit and Hadamard-enhanced counterparts but also competes the full-precision baseline in quality (perplexity of 1.13 vs 1.19) . The introduction of Hadamard transforms, even at 8- bit precision, catastrophically degraded performance by over 37,000%, tracing fundamental training instability. Our BitSkip-V1 recipe demonstrates superior early-exit characteristics, with layer 18 providing optimal 32.5% speed gain for minimal 4% quality loss.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language</title>
<link>https://arxiv.org/abs/2510.23828</link>
<guid>https://arxiv.org/abs/2510.23828</guid>
<content:encoded><![CDATA[
<div> language models, figurative language, cultural nuance, evaluation, Arabic

Summary:

- The study evaluates large language models (LLMs) on their ability to understand and use figurative expressions in Arabic and English, serving as a proxy for cultural nuances.
- Tasks included contextual understanding, pragmatic use, and connotation interpretation for Arabic idioms and English proverbs.
- Results show LLMs perform better on English proverbs compared to Arabic proverbs, with the lowest accuracy on Egyptian Arabic idioms.
- Pragmatic use tasks had lower accuracy than understanding tasks, but providing contextual sentences improved performance.
- Models struggled with connotative meanings, showing challenges in appropriate usage.
- The release of Kinayat, a dataset of Egyptian Arabic idioms, aims to support future research. 

<br /><br />Summary: <div>
arXiv:2510.23828v1 Announce Type: new 
Abstract: We present a comprehensive evaluation of the ability of large language models (LLMs) to process culturally grounded language, specifically to understand and pragmatically use figurative expressions that encode local knowledge and cultural nuance. Using figurative language as a proxy for cultural nuance and local knowledge, we design evaluation tasks for contextual understanding, pragmatic use, and connotation interpretation in Arabic and English. We evaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms, multidialectal Arabic proverbs, and English proverbs. Our results show a consistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower than for English proverbs, and performance for Egyptian idioms is 10.28% lower than for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07% relative to understanding, though providing contextual idiomatic sentences improves accuracy by 10.66%. Models also struggle with connotative meaning, reaching at most 85.58% agreement with human annotators on idioms with 100% inter-annotator agreement. These findings demonstrate that figurative language serves as an effective diagnostic for cultural reasoning: while LLMs can often interpret figurative meaning, they face challenges in using it appropriately. To support future research, we release Kinayat, the first dataset of Egyptian Arabic idioms designed for both figurative understanding and pragmatic use evaluation.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Pragmatics Shape Articulation: A Computational Case Study in STEM ASL Discourse</title>
<link>https://arxiv.org/abs/2510.23842</link>
<guid>https://arxiv.org/abs/2510.23842</guid>
<content:encoded><![CDATA[
<div> dataset collection, sign language models, spatiotemporal changes, dialogue-specific entrainment, sign articulation

Summary: 
- The article discusses the limitations of current sign language models trained on interpreter or isolated vocabulary data, neglecting natural dialogue variability.
- A motion capture dataset of American Sign Language (ASL) STEM dialogue was collected to study dyadic interactive signing, solo signed lecture, and interpreted articles.
- Continuous kinematic features were used to analyze dialogue-specific entrainment and individual effort reduction, revealing spatiotemporal changes in signed STEM terms.
- Dialogue signs were found to be 24.6%-44.6% shorter in duration than isolated signs, with unique reductions observed in dialogue contexts.
- Sign embedding models were evaluated for their ability to recognize STEM signs and measure participant entrainment during interaction. This study bridges linguistic analysis with computational modeling to understand how pragmatics influence sign articulation and representation in sign language technologies.<br /><br />Summary: <div>
arXiv:2510.23842v1 Announce Type: new 
Abstract: Most state-of-the-art sign language models are trained on interpreter or isolated vocabulary data, which overlooks the variability that characterizes natural dialogue. However, human communication dynamically adapts to contexts and interlocutors through spatiotemporal changes and articulation style. This specifically manifests itself in educational settings, where novel vocabularies are used by teachers, and students. To address this gap, we collect a motion capture dataset of American Sign Language (ASL) STEM (Science, Technology, Engineering, and Mathematics) dialogue that enables quantitative comparison between dyadic interactive signing, solo signed lecture, and interpreted articles. Using continuous kinematic features, we disentangle dialogue-specific entrainment from individual effort reduction and show spatiotemporal changes across repeated mentions of STEM terms. On average, dialogue signs are 24.6%-44.6% shorter in duration than the isolated signs, and show significant reductions absent in monologue contexts. Finally, we evaluate sign embedding models on their ability to recognize STEM signs and approximate how entrained the participants become over time. Our study bridges linguistic analysis and computational modeling to understand how pragmatics shape sign articulation and its representation in sign language technologies.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRADLE Bench: A Clinician-Annotated Benchmark for Multi-Faceted Mental Health Crisis and Safety Risk Detection</title>
<link>https://arxiv.org/abs/2510.23845</link>
<guid>https://arxiv.org/abs/2510.23845</guid>
<content:encoded><![CDATA[
<div> detecting, mental health crisis, language models, benchmark, crisis detection

Summary: 
The article introduces CRADLE BENCH, a benchmark for multi-faceted crisis detection, focusing on mental health crisis situations such as suicide ideation and domestic violence. The benchmark covers seven crisis types defined according to clinical standards and includes temporal labels. It provides 600 clinician-annotated evaluation examples and 420 development examples, along with a training corpus of around 4,000 examples labeled automatically using a majority-vote ensemble of language models. The ensemble annotation outperforms single-model annotation. Additionally, six crisis detection models are fine-tuned on subsets defined by consensus and unanimous ensemble agreement, offering complementary models trained under different criteria. This benchmark aims to address the critical yet underexplored challenge of reliable detection of mental health crisis situations in user-model interactions, emphasizing the importance of flagging such crises effectively to prevent serious consequences. 

<br /><br />Summary: <div>
arXiv:2510.23845v1 Announce Type: new 
Abstract: Detecting mental health crisis situations such as suicide ideation, rape, domestic violence, child abuse, and sexual harassment is a critical yet underexplored challenge for language models. When such situations arise during user--model interactions, models must reliably flag them, as failure to do so can have serious consequences. In this work, we introduce CRADLE BENCH, a benchmark for multi-faceted crisis detection. Unlike previous efforts that focus on a limited set of crisis types, our benchmark covers seven types defined in line with clinical standards and is the first to incorporate temporal labels. Our benchmark provides 600 clinician-annotated evaluation examples and 420 development examples, together with a training corpus of around 4K examples automatically labeled using a majority-vote ensemble of multiple language models, which significantly outperforms single-model annotation. We further fine-tune six crisis detection models on subsets defined by consensus and unanimous ensemble agreement, providing complementary models trained under different agreement criteria.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>