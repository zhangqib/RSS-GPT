<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>

<item>
<title>From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems</title>
<link>https://arxiv.org/abs/2507.04996</link>
<guid>https://arxiv.org/abs/2507.04996</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomy, Autonomous Vehicles, Agentic Vehicles, Large Language Models, Human-Centered Mobility Systems 

Summary: 
Agentic vehicles (AgVs) are introduced as vehicles that integrate agentic AI systems to reason, adapt, and interact within complex environments.
These vehicles go beyond traditional autonomous vehicles (AuVs) by demonstrating behaviors such as natural language interaction, goal adaptation, contextual reasoning, external tool use, and ethical dilemma handling.
Integration of large language models (LLMs) plays a significant role in empowering AgVs to possess cognitive and social capabilities required for human-centered mobility systems.
The concept of AgVs highlights the evolution of vehicular systems towards a more proactive and adaptive approach in dealing with various challenges in transportation.
Key challenges in the development and governance of AgVs are identified, emphasizing the importance of addressing ethical, safety, and regulatory concerns to ensure the successful integration of AgVs in future transportation systems. 

<br><br>Summary: <div>
arXiv:2507.04996v4 Announce Type: replace-cross 
Abstract: Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity to operate according to internal rules without external control. Accordingly, autonomous vehicles (AuVs) are viewed as vehicular systems capable of perceiving their environment and executing pre-programmed tasks independently of external input. However, both research and real-world deployments increasingly showcase vehicles that demonstrate behaviors beyond this definition (including the SAE levels 0 to 5); Examples of this outpace include the interaction with humans with natural language, goal adaptation, contextual reasoning, external tool use, and unseen ethical dilemma handling, largely empowered by multi-modal large language models (LLMs). These developments reveal a conceptual gap between technical autonomy and the broader cognitive and social capabilities needed for future human-centered mobility systems. To address this gap, this paper introduces the concept of agentic vehicles (AgVs), referring to vehicles that integrate agentic AI systems to reason, adapt, and interact within complex environments. This paper proposes the term AgVs and their distinguishing characteristics from conventional AuVs. It synthesizes relevant advances in integrating LLMs and AuVs and highlights how AgVs might transform future mobility systems and ensure the systems are human-centered. The paper concludes by identifying key challenges in the development and governance of AgVs, and how they can play a significant role in future agentic transportation systems.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>From Image Captioning to Visual Storytelling</title>
<link>https://arxiv.org/abs/2508.14045</link>
<guid>https://arxiv.org/abs/2508.14045</guid>
<content:encoded><![CDATA[
<div> Visual Storytelling, Vision & Language, Image Captioning, Narrative Coherence, Ideality <br />
<br />
Visual Storytelling is a complex task that requires generating a story for a sequence of images while maintaining narrative coherence and grounding in the images. This study adopts a novel approach by treating Visual Storytelling as an extension of Image Captioning, using a vision-to-language model to generate image captions and then transforming them into coherent narratives. The unified framework combining captioning and storytelling improves the quality of the stories and accelerates training time, enhancing reusability. The introduction of the ideality metric allows for simulating human-likeness in visual storytelling and evaluating the proximity of results to an oracle model. This study presents a valuable contribution to the field of Visual Storytelling by offering a comprehensive evaluation and proposing a new metric for assessing story quality. <br /><br />Summary: <div>
arXiv:2508.14045v1 Announce Type: new 
Abstract: Visual Storytelling is a challenging multimodal task between Vision & Language, where the purpose is to generate a story for a stream of images. Its difficulty lies on the fact that the story should be both grounded to the image sequence but also narrative and coherent. The aim of this work is to balance between these aspects, by treating Visual Storytelling as a superset of Image Captioning, an approach quite different compared to most of prior relevant studies. This means that we firstly employ a vision-to-language model for obtaining captions of the input images, and then, these captions are transformed into coherent narratives using language-to-language methods. Our multifarious evaluation shows that integrating captioning and storytelling under a unified framework, has a positive impact on the quality of the produced stories. In addition, compared to numerous previous studies, this approach accelerates training time and makes our framework readily reusable and reproducible by anyone interested. Lastly, we propose a new metric/tool, named ideality, that can be used to simulate how far some results are from an oracle model, and we apply it to emulate human-likeness in visual storytelling.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Sociolinguistic Diversity in Swahili NLP: A Taxonomy-Guided Approach</title>
<link>https://arxiv.org/abs/2508.14051</link>
<guid>https://arxiv.org/abs/2508.14051</guid>
<content:encoded><![CDATA[
<div> keyword: Swahili NLP, sociolinguistic diversity, Kenyan speakers, taxonomy, model prediction errors <br />
Summary: <br />
This article introduces a taxonomy-guided evaluation of Swahili NLP, focusing on sociolinguistic diversity. The study collects 2,170 free-text responses from Kenyan speakers, revealing tribal influences, urban vernacular, code-mixing, and loanwords. A structured taxonomy is developed to analyze model prediction errors in pre-trained and instruction-tuned language models. The findings emphasize the importance of culturally grounded evaluation frameworks and highlight how sociolinguistic variation impacts model performance. <div>
arXiv:2508.14051v1 Announce Type: new 
Abstract: We introduce the first taxonomy-guided evaluation of Swahili NLP, addressing gaps in sociolinguistic diversity. Drawing on health-related psychometric tasks, we collect a dataset of 2,170 free-text responses from Kenyan speakers. The data exhibits tribal influences, urban vernacular, code-mixing, and loanwords. We develop a structured taxonomy and use it as a lens for examining model prediction errors across pre-trained and instruction-tuned language models. Our findings advance culturally grounded evaluation frameworks and highlight the role of sociolinguistic variation in shaping model performance.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Analysis of Constituent Order Preferences Within Adverbial Roles in English and Chinese News: A Large-Language-Model-Driven Approach</title>
<link>https://arxiv.org/abs/2508.14054</link>
<guid>https://arxiv.org/abs/2508.14054</guid>
<content:encoded><![CDATA[
<div> Keywords: English-Chinese news, functional chunks, constituent order, positional preferences, distribution patterns

Summary:
English and Chinese news articles exhibit differences in constituent order, particularly in the positioning of functional chunks with adverbial roles. English news tends towards a linear narrative with core information first and post-positioned functional chunks, while Chinese news favors an overall presentation mode with background information first and pre-positioned functional chunks. In SVO structure, both languages display variations in the distribution of functional chunks, but the Chinese preference for pre-positioning is more pronounced. Additionally, English and Chinese news articles demonstrate flexibility in the order of co-occurring functional blocks, driven by information and pragmatic considerations. This study sheds light on the systematic preference and dynamic adaptability of word order in English and Chinese news, offering empirical evidence for the contrastive analysis of information structure in the two languages.

Summary: <br /><br />Based on comparable English-Chinese news corpora annotated by Large Language Model (LLM), this study examines differences in constituent order and positional preferences of functional chunks with adverbial roles. English news favors a linear narrative with post-positioned functional chunks, while Chinese news adopts an overall presentation mode with pre-positioned functional chunks. Both languages exhibit variations in the distribution of functional chunks in SVO structure, with Chinese showing a stronger preference for pre-positioning. The flexibility in the order of co-occurring functional blocks in both languages is driven by information and pragmatic considerations, highlighting the dynamic adaptability of word order in English and Chinese news. <div>
arXiv:2508.14054v1 Announce Type: new 
Abstract: Based on comparable English-Chinese news corpora annotated by Large Language Model (LLM), this paper attempts to explore the differences in constituent order of English-Chinese news from the perspective of functional chunks with adverbial roles, and analyze their typical positional preferences and distribution patterns. It is found that: (1) English news prefers linear narrative of core information first, and functional chunks are mostly post-positioned, while Chinese news prefers overall presentation mode of background first, and functional chunks are often pre-positioned; (2) In SVO structure, both English and Chinese news show differences in the distribution of functional chunks, but the tendency of Chinese pre-positioning is more significant, while that of English post-positioning is relatively mild; (3) When function blocks are co-occurring, both English and Chinese news show high flexibility, and the order adjustment is driven by information and pragmatic purposes. The study reveals that word order has both systematic preference and dynamic adaptability, providing new empirical support for contrastive study of English-Chinese information structure.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-REX: Table -- Refute or Entail eXplainer</title>
<link>https://arxiv.org/abs/2508.14055</link>
<guid>https://arxiv.org/abs/2508.14055</guid>
<content:encoded><![CDATA[
<div> Keywords: Table, Claim verification, Fact-checking, Large Language Models, Interactive tool

Summary:
T-REX (Table -- Refute or Entail eXplainer) is a novel tool that enhances textual claim verification against structured tabular data. It leverages Large Language Models (LLMs) and offers an interactive platform for users. T-REX focuses on accuracy and transparency in verifying claims over multimodal and multilingual tables. With its user-friendly interface, T-REX allows non-experts to harness advanced fact-checking technology. This marks a significant advancement in Natural Language Processing, making state-of-the-art fact-checking accessible to a broader audience. The system is readily available online, providing a valuable resource for individuals seeking to validate claims against tabular data. T-REX showcases the potential of combining text and structured data for efficient claim validation processes. It represents a step towards democratizing fact-checking tools and empowering users with the ability to accurately verify information. 

<br /><br />Summary: T-REX introduces an interactive tool for claim verification using Large Language Models, making advanced fact-checking technology accessible to non-experts. It prioritizes accuracy and transparency in analyzing textual claims against multimodal, multilingual tables, thereby bridging the gap between complex NLP solutions and user-friendly interfaces. <div>
arXiv:2508.14055v1 Announce Type: new 
Abstract: Verifying textual claims against structured tabular data is a critical yet challenging task in Natural Language Processing with broad real-world impact. While recent advances in Large Language Models (LLMs) have enabled significant progress in table fact-checking, current solutions remain inaccessible to non-experts. We introduce T-REX (T-REX: Table -- Refute or Entail eXplainer), the first live, interactive tool for claim verification over multimodal, multilingual tables using state-of-the-art instruction-tuned reasoning LLMs. Designed for accuracy and transparency, T-REX empowers non-experts by providing access to advanced fact-checking technology. The system is openly available online.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence Estimation for Text-to-SQL in Large Language Models</title>
<link>https://arxiv.org/abs/2508.14056</link>
<guid>https://arxiv.org/abs/2508.14056</guid>
<content:encoded><![CDATA[
<div> Keywords: confidence estimation, text-to-SQL, large language models, black-box models, white-box models

Summary:<br /><br />This study focuses on confidence estimation for text-to-SQL without gold answers using large language models. The research explores black-box and white-box strategies, emphasizing the effectiveness of consistency-based methods for black-box models and SQL-syntax-aware approaches for white-box models. Additionally, the study demonstrates that grounding queries through execution-based methods enhances the performance of both approaches. The evaluation of cross-domain text-to-SQL benchmarks highlights the importance of considering different strategies for confidence estimation in the context of large language models. <div>
arXiv:2508.14056v1 Announce Type: new 
Abstract: Confidence estimation for text-to-SQL aims to assess the reliability of model-generated SQL queries without having access to gold answers. We study this problem in the context of large language models (LLMs), where access to model weights and gradients is often constrained. We explore both black-box and white-box confidence estimation strategies, evaluating their effectiveness on cross-domain text-to-SQL benchmarks. Our evaluation highlights the superior performance of consistency-based methods among black-box models and the advantage of SQL-syntax-aware approaches for interpreting LLM logits in white-box settings. Furthermore, we show that execution-based grounding of queries provides a valuable supplementary signal, improving the effectiveness of both approaches.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models</title>
<link>https://arxiv.org/abs/2508.14062</link>
<guid>https://arxiv.org/abs/2508.14062</guid>
<content:encoded><![CDATA[
<div> Privacy Risks, Large Language Models, Data Memorization, Privacy Protection Framework, Controlled Experiments

Summary: 
This paper examines the privacy risks associated with fine-tuned Large Language Models (LLMs) that tend to memorize training data, leading to potential data leakage. Through experiments on various LLM architectures, it is found that repeated fine-tuning with sensitive data significantly increases privacy leakage rates. To address these concerns, the paper introduces a multi-layered privacy protection framework consisting of semantic data deduplication, differential privacy during generation, entropy-based filtering, and pattern-based content filtering. These methods prove effective in reducing data leakage to 0% while retaining 94.7% of the original model utility. By implementing these privacy protection techniques, the paper offers a solution to mitigate the privacy risks associated with LLMs during fine-tuning processes. 
<br /><br />Summary: <div>
arXiv:2508.14062v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, but their tendency to memorize training data poses significant privacy risks, particularly during fine-tuning processes. This paper presents a comprehensive empirical analysis of data memorization in fine-tuned LLMs and introduces a novel multi-layered privacy protection framework. Through controlled experiments on modern LLM architectures including GPT-2, Phi-3, and Gemma-2, we demonstrate that fine-tuning with repeated sensitive data increases privacy leakage rates from baseline levels of 0-5% to 60-75%, representing a 64.2% average increase across tested models. We propose and rigorously evaluate four complementary privacy protection methods: semantic data deduplication, differential privacy during generation, entropy-based filtering, and pattern-based content filtering. Our experimental results show that these techniques can reduce data leakage to 0% while maintaining 94.7% of original model utility.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Punctuation and Predicates in Language Models</title>
<link>https://arxiv.org/abs/2508.14067</link>
<guid>https://arxiv.org/abs/2508.14067</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, punctuation tokens, information propagation, interpretability, reasoning rules

Summary: 
- The paper investigates the computational importance of punctuation tokens in large language models (LLMs) such as GPT-2, DeepSeek, and Gemma.
- Punctuation tokens are found to be necessary and sufficient in multiple layers of GPT-2, while they have varying importance in DeepSeek and Gemma.
- The study also explores how LLMs process different components of input and whether they form static summaries or remain sensitive to changes across layers.
- Differences in how LLMs process conditional statements and universal quantification are identified through intervention and layer-swapping experiments.
- The findings provide new insights into punctuation usage and reasoning mechanisms in LLMs, with implications for interpretability. 

<br /><br />Summary: <div>
arXiv:2508.14067v1 Announce Type: new 
Abstract: In this paper we explore where information is collected and how it is propagated throughout layers in large language models (LLMs). We begin by examining the surprising computational importance of punctuation tokens which previous work has identified as attention sinks and memory aids. Using intervention-based techniques, we evaluate the necessity and sufficiency (for preserving model performance) of punctuation tokens across layers in GPT-2, DeepSeek, and Gemma. Our results show stark model-specific differences: for GPT-2, punctuation is both necessary and sufficient in multiple layers, while this holds far less in DeepSeek and not at all in Gemma. Extending beyond punctuation, we ask whether LLMs process different components of input (e.g., subjects, adjectives, punctuation, full sentences) by forming early static summaries reused across the network, or if the model remains sensitive to changes in these components across layers. Extending beyond punctuation, we investigate whether different reasoning rules are processed differently by LLMs. In particular, through interchange intervention and layer-swapping experiments, we find that conditional statements (if, then), and universal quantification (for all) are processed very differently. Our findings offer new insight into the internal mechanisms of punctuation usage and reasoning in LLMs and have implications for interpretability.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DLLMQuant: Quantizing Diffusion-based Large Language Models</title>
<link>https://arxiv.org/abs/2508.14090</link>
<guid>https://arxiv.org/abs/2508.14090</guid>
<content:encoded><![CDATA[
<div> dynamic masking, iterative generation, bidirectional attention, post-training quantization, Large Language Models

Summary:
DLLMQuant is a new framework tailored for Diffusion-based Large Language Models (DLLMs) that addresses the challenges faced by traditional post-training quantization methods. Three core issues that arise when applying quantization to DLLMs are identified and tackled with innovative techniques. Temporal-Mask Adaptive Sampling (TMAS) captures token distributions across decoding steps, Interaction-Aware Activation Quantization (IA-AQ) dynamically allocates quantization resources based on bidirectional attention signals, and Certainty-Guided Quantization (CGQ) integrates mask status and token scores for improved weight quantization. DLLMQuant significantly enhances performance and efficiency, mitigating accuracy degradation and ensuring better generalization performance for DLLMs. This framework shows promise in compressing and accelerating large language models, offering a solution to the computational constraints faced by DLLMs in deployment. 

<br /><br />Summary: <div>
arXiv:2508.14090v1 Announce Type: new 
Abstract: Diffusion-based large language models (DLLMs) have shown promise for non-autoregressive text generation, but their deployment is constrained by large model sizes and heavy computational costs. Post-training quantization (PTQ), a widely used method for compressing and accelerating Large Language Models (LLMs), suffers from severe accuracy degradation and reduced generalization performance when directly applied to DLLMs (e.g., AWQ suffers a 16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key mechanisms - dynamic masking, iterative generation, bidirectional attention - clash with quantization. We identify three core issues: 1) Iterative generation and dynamic masking ratios lead to distinct token distributions across decoding steps, which are not adequately captured by existing PTQ calibration methods; 2) Quantization errors are accumulated and amplified progressively during iteration in DLLMs, causing quantized models to perform worse as decoding steps progress; 3) Unmasked tokens stabilize while masked remain probabilistic, making overall feature distribution incompatible with existing PTQ methods. To address these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs, which incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling (TMAS), a calibration method that accounts for both time and mask factors, with the capacity to capture distributions across timesteps. 2) Interaction-Aware Activation Quantization (IA-AQ), which utilizes bidirectional attention's interaction signals to dynamically allocate quantization resources. 3) Certainty-Guided Quantization (CGQ), which integrates mask status and token scores as key weighting criteria into error compensation, making weight quantization more suitable for DLLMs. Experiments show that DLLMQuant achieves significant performance gains while enhancing efficiency.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation</title>
<link>https://arxiv.org/abs/2508.14146</link>
<guid>https://arxiv.org/abs/2508.14146</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, peer review, multimodal content, benchmark, automated systems

Summary:
MMReview introduces a benchmark for evaluating the performance of Large Language Models (LLMs) and Multimodal LLMs (MLLMs) in generating comprehensive and accurate peer review comments. The benchmark comprises 240 papers across 17 research domains in four major academic disciplines, including multimodal content and expert-written review comments. It consists of 13 tasks grouped into four core categories: step-wise review generation, outcome formulation, alignment with human preferences, and robustness to adversarial input manipulation. Extensive experiments conducted on open-source and closed-source models demonstrate the benchmark's thoroughness in assessing the models' abilities. MMReview aims to standardize the development of automated peer review systems and serve as a foundational tool for improving the efficiency and reliability of the peer review process. 

<br /><br />Summary:MMReview introduces a benchmark for evaluating the performance of Large Language Models (LLMs) and Multimodal LLMs (MLLMs) in generating comprehensive and accurate peer review comments. The benchmark comprises 240 papers across 17 research domains in four major academic disciplines, including multimodal content and expert-written review comments. It consists of 13 tasks grouped into four core categories: step-wise review generation, outcome formulation, alignment with human preferences, and robustness to adversarial input manipulation. Extensive experiments conducted on open-source and closed-source models demonstrate the benchmark's thoroughness in assessing the models' abilities. MMReview aims to standardize the development of automated peer review systems and serve as a foundational tool for improving the efficiency and reliability of the peer review process. <div>
arXiv:2508.14146v1 Announce Type: new 
Abstract: With the rapid growth of academic publications, peer review has become an essential yet time-consuming responsibility within the research community. Large Language Models (LLMs) have increasingly been adopted to assist in the generation of review comments; however, current LLM-based review tasks lack a unified evaluation benchmark to rigorously assess the models' ability to produce comprehensive, accurate, and human-aligned assessments, particularly in scenarios involving multimodal content such as figures and tables. To address this gap, we propose \textbf{MMReview}, a comprehensive benchmark that spans multiple disciplines and modalities. MMReview includes multimodal content and expert-written review comments for 240 papers across 17 research domains within four major academic disciplines: Artificial Intelligence, Natural Sciences, Engineering Sciences, and Social Sciences. We design a total of 13 tasks grouped into four core categories, aimed at evaluating the performance of LLMs and Multimodal LLMs (MLLMs) in step-wise review generation, outcome formulation, alignment with human preferences, and robustness to adversarial input manipulation. Extensive experiments conducted on 16 open-source models and 5 advanced closed-source models demonstrate the thoroughness of the benchmark. We envision MMReview as a critical step toward establishing a standardized foundation for the development of automated peer review systems.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPad: Efficient Diffusion Language Models with Suffix Dropout</title>
<link>https://arxiv.org/abs/2508.14148</link>
<guid>https://arxiv.org/abs/2508.14148</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion-based Large Language Models, text generation, computational overhead, Diffusion Scratchpad, efficient inference

Summary: 
Diffusion-based Large Language Models (dLLMs) aim to parallelize text generation by treating decoding as a denoising process but face challenges due to high computational overhead. In response, the authors propose Diffusion Scratchpad (DPad), a training-free method that streamlines attention to a limited set of nearby suffix tokens, thus reducing redundancy while maintaining fidelity. DPad combines a sliding window approach to keep track of a fixed-length suffix window and distance-decay dropout to exclude distant tokens during attention computation. Despite its simplicity, DPad offers significant performance improvements, achieving up to 61.4 times faster inference speed compared to traditional dLLMs. Extensive evaluations on LLaDA-1.5 and Dream models showcase DPad's efficiency and scalability in long-sequence inference tasks, making it a promising solution for enhancing the performance of large language models. The code for DPad implementation is accessible on GitHub at https://github.com/Crys-Chen/DPad.

<br /><br />Summary: <div>
arXiv:2508.14148v1 Announce Type: new 
Abstract: Diffusion-based Large Language Models (dLLMs) parallelize text generation by framing decoding as a denoising process, but suffer from high computational overhead since they predict all future suffix tokens at each step while retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a training-free method that restricts attention to a small set of nearby suffix tokens, preserving fidelity while eliminating redundancy. DPad integrates two strategies: (i) a sliding window, which maintains a fixed-length suffix window, and (ii) distance-decay dropout, which deterministically removes distant suffix tokens before attention computation. This simple design is compatible with existing optimizations such as prefix caching and can be implemented with only a few lines of code. Comprehensive evaluations across multiple benchmarks on LLaDA-1.5 and Dream models demonstrate that DPad delivers up to $\mathbf{61.4\times}$ speedup over vanilla dLLMs while maintaining comparable accuracy, highlighting its potential for efficient and scalable long-sequence inference. Our code is available at https://github.com/Crys-Chen/DPad.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing energy consumption and accuracy in text classification inference</title>
<link>https://arxiv.org/abs/2508.14170</link>
<guid>https://arxiv.org/abs/2508.14170</guid>
<content:encoded><![CDATA[
<div> energy efficiency, sustainability, large language models, natural language processing, text classification

Summary:<br />
- The study focuses on evaluating the trade-offs between model accuracy and energy consumption in text classification inference.
- It highlights that the best-performing model can also be energy-efficient, while larger LLMs tend to consume more energy with lower accuracy.
- Inference energy consumption varied significantly based on model type, size, and hardware specifications.
- There is a strong correlation between inference energy consumption and model runtime, implying that execution time can be used as a proxy for energy usage.
- The findings suggest practical implications for sustainable AI development, offering insights for balancing performance and resource efficiency in NLP applications.

Summary: <div>
arXiv:2508.14170v1 Announce Type: new 
Abstract: The increasing deployment of large language models (LLMs) in natural language processing (NLP) tasks raises concerns about energy efficiency and sustainability. While prior research has largely focused on energy consumption during model training, the inference phase has received comparatively less attention. This study systematically evaluates the trade-offs between model accuracy and energy consumption in text classification inference across various model architectures and hardware configurations. Our empirical analysis shows that the best-performing model in terms of accuracy can also be energy-efficient, while larger LLMs tend to consume significantly more energy with lower classification accuracy. We observe substantial variability in inference energy consumption ($<$mWh to $>$kWh), influenced by model type, model size, and hardware specifications. Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible. These findings have implications for sustainable AI development, providing actionable insights for researchers, industry practitioners, and policymakers seeking to balance performance and resource efficiency in NLP applications.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper</title>
<link>https://arxiv.org/abs/2508.14273</link>
<guid>https://arxiv.org/abs/2508.14273</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, scientific introduction generation, dataset curation, state-of-the-art models, research writing assistants

Summary:
- Introduces the task of Scientific Introduction Generation (SciIG) to evaluate LLMs' ability to generate research paper introductions.
- New datasets from NAACL 2025 and ICLR 2025 papers are curated for assessment.
- Five state-of-the-art models, including open-source and closed-source systems, are evaluated across multiple dimensions.
- LLaMA-4 Maverick shows superior performance, particularly in semantic similarity and faithfulness.
- Three-shot prompting consistently outperforms fewer-shot approaches.
<br /><br />Summary:As researchers use LLMs for writing, producing high-quality research paper introductions is crucial. The study introduces SciIG, evaluating LLMs' performance using curated datasets from NAACL 2025 and ICLR 2025 papers. Five models are assessed on various metrics, with LLaMA-4 Maverick excelling in semantic similarity and faithfulness. Three-shot prompting proves more effective than fewer-shot methods. These findings offer valuable insights for developing research writing assistants and managing expectations surrounding LLM-assisted academic writing. Public release of code and datasets will support reproducibility and future research endeavors. <div>
arXiv:2508.14273v1 Announce Type: new 
Abstract: As researchers increasingly adopt LLMs as writing assistants, generating high-quality research paper introductions remains both challenging and essential. We introduce Scientific Introduction Generation (SciIG), a task that evaluates LLMs' ability to produce coherent introductions from titles, abstracts, and related works. Curating new datasets from NAACL 2025 and ICLR 2025 papers, we assess five state-of-the-art models, including both open-source (DeepSeek-v3, Gemma-3-12B, LLaMA 4-Maverick, MistralAI Small 3.1) and closed-source GPT-4o systems, across multiple dimensions: lexical overlap, semantic similarity, content coverage, faithfulness, consistency, citation correctness, and narrative quality. Our comprehensive framework combines automated metrics with LLM-as-a-judge evaluations. Results demonstrate LLaMA-4 Maverick's superior performance on most metrics, particularly in semantic similarity and faithfulness. Moreover, three-shot prompting consistently outperforms fewer-shot approaches. These findings provide practical insights into developing effective research writing assistants and set realistic expectations for LLM-assisted academic writing. To foster reproducibility and future research, we will publicly release all code and datasets.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling concept semantics via multilingual averaging in Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2508.14275</link>
<guid>https://arxiv.org/abs/2508.14275</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Langue Models, formal knowledge representation, sparse autoencoders, concept semantics, ontology classes<br />
Summary:<br />
The study focuses on integrating Large Language Models (LLMs) with formal knowledge representation and reasoning to overcome their limitations. By using Sparse Autoencoders, the researchers aim to isolate concept semantics in LLMs. They create English text representations from OWL ontology classes and translate them into French and Chinese before inputting them into the Gemma 2B LLM. The concept activations obtained from different language versions are averaged to derive a conceptual average. This average is then compared with a ground truth mapping between ontology classes, showing alignment with the true relationships. The results suggest that averaging concept activations from multiple languages can more accurately represent the relationships between classes in the ontology. This approach provides a new technique for interpreting internal network states with increased precision.<br /><br />Summary: <div>
arXiv:2508.14275v1 Announce Type: new 
Abstract: Connecting LLMs with formal knowledge representation and reasoning is a promising approach to address their shortcomings. Embeddings and sparse autoencoders are widely used to represent textual content, but the semantics are entangled with syntactic and language-specific information. We propose a method that isolates concept semantics in Large Langue Models by averaging concept activations derived via Sparse Autoencoders. We create English text representations from OWL ontology classes, translate the English into French and Chinese and then pass these texts as prompts to the Gemma 2B LLM. Using the open source Gemma Scope suite of Sparse Autoencoders, we obtain concept activations for each class and language version. We average the different language activations to derive a conceptual average. We then correlate the conceptual averages with a ground truth mapping between ontology classes. Our results give a strong indication that the conceptual average aligns to the true relationship between classes when compared with a single language by itself. The result hints at a new technique which enables mechanistic interpretation of internal network states with higher accuracy.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs</title>
<link>https://arxiv.org/abs/2508.14279</link>
<guid>https://arxiv.org/abs/2508.14279</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, NLP, Romanian, benchmark, multilingual models

Summary: 
GRILE (Grammar Romanian Inference and Language Explanations) is introduced as an open benchmark with 1,151 multiple-choice questions from Romanian high-stakes exams. The benchmark assesses the performance of state-of-the-art multilingual and Romanian-specific LLMs in selecting correct answers and providing accurate explanations. While Gemini 2.5 Pro achieves an 83% accuracy rate, most models struggle to exceed 65%, with 48% of their explanations containing errors. An error analysis reveals weaknesses in morphology and adherence to orthographic norms. The study highlights challenges in educational NLP for low-resource languages and positions GRILE as a valuable testbed for explanation generation and evaluation research.

<br /><br />Summary: <div>
arXiv:2508.14279v1 Announce Type: new 
Abstract: LLMs (Large language models) have revolutionized NLP (Natural Language Processing), yet their pedagogical value for low-resource languages remains unclear. We present GRILE (Grammar Romanian Inference and Language Explanations) , the first open benchmark of 1,151 multiple-choice questions harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate, university admissions). GRILE enables us to probe two complementary abilities of seven state-of-the-art multilingual and Romanian-specific LLMs: (i) selecting the correct answer, and (ii) producing linguistically accurate explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight models stay below 65%, and 48% of their explanations contain factual or pedagogical flaws according to expert review. A detailed error analysis pinpoints systematic weaknesses in morphology and in applying the latest DOOM3 orthographic norms. All data, code and a public web demo are released to catalyze future research. Our findings expose open challenges for trustworthy educational NLP in low-resource settings and establish GRILE as a new test-bed for controllable explanation generation and evaluation.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokens with Meaning: A Hybrid Tokenization Approach for NLP</title>
<link>https://arxiv.org/abs/2508.14292</link>
<guid>https://arxiv.org/abs/2508.14292</guid>
<content:encoded><![CDATA[
<div> Keywords: Tokenization, Morphological analysis, Subword segmentation, Language models, Multilingual NLP

Summary:
Tokenization is crucial in natural language processing (NLP) for segmenting and interpreting text. The traditional subword methods like BPE and WordPiece struggle with complex languages due to relying on frequency over linguistic structure. A novel hybrid tokenization framework combines rule-based morphological analysis with statistical subword segmentation, using phonological normalization and balancing morpheme preservation with vocabulary efficiency. This method reduces redundancy while maintaining semantic integrity, achieving high performance on the TR-MMLU benchmark for Turkish Token Percentage and Pure Token Percentage. Special tokens are added for whitespace, case, and out-of-vocabulary coverage using BPE. Comparisons with existing tokenizers show more linguistically meaningful and coherent tokens. This language-independent approach paves the way for more interpretable and effective multilingual NLP systems.
<br /><br />Summary: <div>
arXiv:2508.14292v1 Announce Type: new 
Abstract: Tokenization plays a pivotal role in natural language processing (NLP), shaping how text is segmented and interpreted by language models. While subword methods such as Byte Pair Encoding (BPE) and WordPiece have been effective, they often struggle with morphologically rich and agglutinative languages because they rely on frequency rather than linguistic structure. We introduce a hybrid tokenization framework that combines rule-based morphological analysis with statistical subword segmentation. The method uses phonological normalization, root-affix dictionaries, and a novel algorithm that balances morpheme preservation with vocabulary efficiency. It assigns shared identifiers to phonologically variant affixes (e.g., -ler and -lar) and altered root forms (e.g., kitap vs. kitab{\i}), reducing redundancy while maintaining semantic integrity. Special tokens are added for whitespace and case, including an UPPERCASE marker to avoid vocabulary inflation from capitalization. BPE is integrated for out-of-vocabulary coverage without harming morphological coherence. On the TR-MMLU benchmark, the tokenizer achieves the highest Turkish Token Percentage (90.29\%) and Pure Token Percentage (85.8\%). Comparisons with tokenizers from LLaMA, Gemma, and GPT show more linguistically meaningful and coherent tokens. Although demonstrated on Turkish, the approach is language-independent and adaptable to other languages, offering a practical path toward more interpretable and effective multilingual NLP systems.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Joint Multitask Model for Morpho-Syntactic Parsing</title>
<link>https://arxiv.org/abs/2508.14307</link>
<guid>https://arxiv.org/abs/2508.14307</guid>
<content:encoded><![CDATA[
<div> joint multitask model, UniDive 2025 Morpho-Syntactic Parsing, XLM-RoBERTa encoder, dependency parsing, morphosyntactic feature prediction

Summary: 
A joint multitask model for the UniDive 2025 Morpho-Syntactic Parsing shared task was introduced. The model utilizes a shared XLM-RoBERTa encoder with three specialized decoders for content word identification, dependency parsing, and morphosyntactic feature prediction. It outperformed other systems on the shared task's leaderboard, achieving high scores in MSLAS, LAS, and Feats F1 across nine diverse languages. Ablation studies highlighted the importance of matching gold tokenization and content word identification for model performance. Error analysis identified challenges in core grammatical cases and nominal features, particularly in Nom-Acc cases. Overall, the multitask model demonstrated strong performance in predicting morphological and syntactic analyses, emphasizing the significance of integrated approaches in natural language processing tasks. 

<br /><br />Summary: <div>
arXiv:2508.14307v1 Announce Type: new 
Abstract: We present a joint multitask model for the UniDive 2025 Morpho-Syntactic Parsing shared task, where systems predict both morphological and syntactic analyses following novel UD annotation scheme. Our system uses a shared XLM-RoBERTa encoder with three specialized decoders for content word identification, dependency parsing, and morphosyntactic feature prediction. Our model achieves the best overall performance on the shared task's leaderboard covering nine typologically diverse languages, with an average MSLAS score of 78.7 percent, LAS of 80.1 percent, and Feats F1 of 90.3 percent. Our ablation studies show that matching the task's gold tokenization and content word identification are crucial to model performance. Error analysis reveals that our model struggles with core grammatical cases (particularly Nom-Acc) and nominal features across languages.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency</title>
<link>https://arxiv.org/abs/2508.14314</link>
<guid>https://arxiv.org/abs/2508.14314</guid>
<content:encoded><![CDATA[
<div> framework, cross-model consistency, hallucination detection, mitigation technique, factual reliability
Summary: Finch-Zk is a new black-box framework designed to improve the accuracy of large language models by detecting and mitigating hallucinations in their outputs. It achieves this by utilizing fine-grained cross-model consistency checking to identify inaccuracies and targeted mitigation techniques to correct problematic segments while maintaining accurate content. Experiments on the FELM dataset showed significant improvements in hallucination detection compared to existing methods. When applied to state-of-the-art models like Llama 4 Maverick and Claude 4 Sonnet on the GPQA-diamond dataset, Finch-Zk achieved a notable increase in answer accuracy. The framework is practical and deployment-ready, providing a reliable safeguard for enhancing factual reliability in production LLM systems. 
<br /><br />Summary: <div>
arXiv:2508.14314v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, but they remain susceptible to hallucinations--generating content that appears plausible but contains factual inaccuracies. We present Finch-Zk, a black-box framework that leverages FINe-grained Cross-model consistency to detect and mitigate Hallucinations in LLM outputs without requiring external knowledge sources. Finch-Zk introduces two key innovations: 1) a cross-model consistency checking strategy that reveals fine-grained inaccuracies by comparing responses generated by diverse models from semantically-equivalent prompts, and 2) a targeted mitigation technique that applies precise corrections to problematic segments while preserving accurate content. Experiments on the FELM dataset show Finch-Zk improves hallucination detection F1 scores by 6-39\% compared to existing approaches. For mitigation, Finch-Zk achieves 7-8 absolute percentage points improvement in answer accuracy on the GPQA-diamond dataset when applied to state-of-the-art models like Llama 4 Maverick and Claude 4 Sonnet. Extensive evaluation across multiple models demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for enhancing factual reliability in production LLM systems.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing</title>
<link>https://arxiv.org/abs/2508.14317</link>
<guid>https://arxiv.org/abs/2508.14317</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Survey Generation, Coherence, Citation Coverage, Automatic

Summary: 
Survey papers in the scientific community are crucial for consolidating progress in a field. The development of Large Language Models (LLMs) has provided a way to automate key steps in survey generation, such as retrieval and summarization. However, existing LLM-based approaches face challenges in maintaining coherence and providing comprehensive citation coverage in long surveys. To address these issues, SurveyGen-I, an automatic survey generation framework, has been introduced. This framework utilizes a combination of coarse-to-fine retrieval, adaptive planning, and memory-guided generation. It first constructs an outline and writing plan through survey-level retrieval, dynamically refines them during generation, and uses a memory mechanism to ensure coherence across subsections. If needed, fine-grained subsection-level retrieval is triggered. Experimental results across scientific domains show that SurveyGen-I surpasses previous works in content quality, consistency, and citation coverage. 

<br /><br />Summary: <div>
arXiv:2508.14317v1 Announce Type: new 
Abstract: Survey papers play a critical role in scientific communication by consolidating progress across a field. Recent advances in Large Language Models (LLMs) offer a promising solution by automating key steps in the survey-generation pipeline, such as retrieval, structuring, and summarization. However, existing LLM-based approaches often struggle with maintaining coherence across long, multi-section surveys and providing comprehensive citation coverage. To address these limitations, we introduce SurveyGen-I, an automatic survey generation framework that combines coarse-to-fine retrieval, adaptive planning, and memory-guided generation. SurveyGen-I first performs survey-level retrieval to construct the initial outline and writing plan, and then dynamically refines both during generation through a memory mechanism that stores previously written content and terminology, ensuring coherence across subsections. When the system detects insufficient context, it triggers fine-grained subsection-level retrieval. During generation, SurveyGen-I leverages this memory mechanism to maintain coherence across subsections. Experiments across four scientific domains demonstrate that SurveyGen-I consistently outperforms previous works in content quality, consistency, and citation coverage.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever</title>
<link>https://arxiv.org/abs/2508.14323</link>
<guid>https://arxiv.org/abs/2508.14323</guid>
<content:encoded><![CDATA[
<div> behavior-aligned retriever, large language models, tool-augmented, function calls, contrastive learning

Summary:
- Tool-augmented large language models (LLMs) use external functions but inaccurate calls can lead to inefficiencies.
- Existing methods such as fine-tuning or demonstration-based prompting have high training overheads and do not account for inconsistent samples.
- A behavior-aligned retriever (BAR) trained in this study offers behaviorally consistent demonstrations for accurate tool-using decisions by LLMs.
- The BAR is trained on a corpus with different function-calling behaviors using a contrastive learning framework with customized pairs and a dual-negative contrastive loss.
- Experiments show that the approach reduces erroneous function calls while maintaining high task performance, providing a cost-effective and efficient solution for tool-augmented LLMs. 

<br /><br />Summary: <div>
arXiv:2508.14323v1 Announce Type: new 
Abstract: Tool-augmented large language models (LLMs) leverage external functions to extend their capabilities, but inaccurate function calls can lead to inefficiencies and increased costs.Existing methods address this challenge by fine-tuning LLMs or using demonstration-based prompting, yet they often suffer from high training overhead and fail to account for inconsistent demonstration samples, which misguide the model's invocation behavior. In this paper, we trained a behavior-aligned retriever (BAR), which provides behaviorally consistent demonstrations to help LLMs make more accurate tool-using decisions. To train the BAR, we construct a corpus including different function-calling behaviors, i.e., calling or non-calling.We use the contrastive learning framework to train the BAR with customized positive/negative pairs and a dual-negative contrastive loss, ensuring robust retrieval of behaviorally consistent examples.Experiments demonstrate that our approach significantly reduces erroneous function calls while maintaining high task performance, offering a cost-effective and efficient solution for tool-augmented LLMs.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ISCA: A Framework for Interview-Style Conversational Agents</title>
<link>https://arxiv.org/abs/2508.14344</link>
<guid>https://arxiv.org/abs/2508.14344</guid>
<content:encoded><![CDATA[
<div> Keywords: interview-style conversational agents, qualitative data collection, attitude formation, behavior change, neurotechnology

Summary:
The article introduces a low-compute system for implementing interview-style conversational agents, suitable for qualitative data collection in controlled interactions. This system allows for standardized conversational flow, making it ideal for applications such as attitude formation or behavior change tracking. The tool can be easily customized through an online administrative panel without the need for coding. Two case studies are presented to demonstrate the system's effectiveness: one focusing on Expressive Interviewing for COVID-19 and the other on surveying public opinion on emerging neurotechnology through semi-structured interviews. The system's code is open-source, enabling others to build upon and enhance its functionality. Overall, this system offers a user-friendly approach to conducting interviews for research purposes, with potential applications in various domains including psychology, public health, and technology assessment.
<br /><br />Summary: <div>
arXiv:2508.14344v1 Announce Type: new 
Abstract: We present a low-compute non-generative system for implementing interview-style conversational agents which can be used to facilitate qualitative data collection through controlled interactions and quantitative analysis. Use cases include applications to tracking attitude formation or behavior change, where control or standardization over the conversational flow is desired. We show how our system can be easily adjusted through an online administrative panel to create new interviews, making the tool accessible without coding. Two case studies are presented as example applications, one regarding the Expressive Interviewing system for COVID-19 and the other a semi-structured interview to survey public opinion on emerging neurotechnology. Our code is open-source, allowing others to build off of our work and develop extensions for additional functionality.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities</title>
<link>https://arxiv.org/abs/2508.14377</link>
<guid>https://arxiv.org/abs/2508.14377</guid>
<content:encoded><![CDATA[
<div> Benchmark, Chinese reading comprehension difficulty, Large language models, Students' Cognitive Abilities, Zone of Proximal Development 

Summary:<br />
- Large language models (LLMs) have potential in educational applications but need to accurately assess reading materials' cognitive alignment with students' developmental stages. 
- The Zone of Proximal Development (ZPD) emphasizes matching learning resources with Students' Cognitive Abilities (SCA) in Chinese language education. 
- The ZPD-SCA benchmark evaluates stage-level Chinese reading comprehension difficulty annotated by top-tier teachers. 
- LLMs perform poorly in zero-shot learning but improve with in-context examples, indicating emerging abilities to assess difficulty, but with limitations in educationally aligned judgment. 
- Best-performing models show directional biases, suggesting challenges aligning material difficulty with SCA and variations in performance across different genres. 
<br /><br />Summary: <div>
arXiv:2508.14377v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated potential in educational applications, yet their capacity to accurately assess the cognitive alignment of reading materials with students' developmental stages remains insufficiently explored. This gap is particularly critical given the foundational educational principle of the Zone of Proximal Development (ZPD), which emphasizes the need to match learning resources with Students' Cognitive Abilities (SCA). Despite the importance of this alignment, there is a notable absence of comprehensive studies investigating LLMs' ability to evaluate reading comprehension difficulty across different student age groups, especially in the context of Chinese language education. To fill this gap, we introduce ZPD-SCA, a novel benchmark specifically designed to assess stage-level Chinese reading comprehension difficulty. The benchmark is annotated by 60 Special Grade teachers, a group that represents the top 0.15% of all in-service teachers nationwide. Experimental results reveal that LLMs perform poorly in zero-shot learning scenarios, with Qwen-max and GLM even falling below the probability of random guessing. When provided with in-context examples, LLMs performance improves substantially, with some models achieving nearly double the accuracy of their zero-shot baselines. These results reveal that LLMs possess emerging abilities to assess reading difficulty, while also exposing limitations in their current training for educationally aligned judgment. Notably, even the best-performing models display systematic directional biases, suggesting difficulties in accurately aligning material difficulty with SCA. Furthermore, significant variations in model performance across different genres underscore the complexity of task. We envision that ZPD-SCA can provide a foundation for evaluating and improving LLMs in cognitively aligned educational applications.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Credence Calibration Game? Calibrating Large Language Models through Structured Play</title>
<link>https://arxiv.org/abs/2508.14390</link>
<guid>https://arxiv.org/abs/2508.14390</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, calibration, confidence estimates, prompt-based, Credence Calibration Game

Summary:
Large Language Models (LLMs) are increasingly used in decision-critical domains, necessitating accurate confidence estimates. Existing calibration methods often require additional supervision or updates. This work introduces a novel prompt-based calibration framework inspired by the Credence Calibration Game. The framework involves a structured interaction loop where LLMs receive feedback on their predicted confidence alignment with correctness. Through feedback-driven prompting and natural language summaries of past performance, the model calibration dynamically improves. Extensive experiments across models and game configurations show consistent enhancements in evaluation metrics. The results demonstrate the effectiveness of game-based prompting for LLM calibration. The code and data for the study are available at https://anonymous.4open.science/r/LLM-Calibration/. 

<br /><br />Summary: 
- Large Language Models are used in critical domains, requiring accurate confidence estimates.
- Existing calibration methods may need additional supervision or updates.
- A prompt-based calibration framework inspired by the Credence Calibration Game is proposed.
- The framework includes feedback-driven prompting and natural language summaries to improve calibration.
- Extensive experiments show enhancements in evaluation metrics, highlighting the efficacy of game-based prompting for LLM calibration. <div>
arXiv:2508.14390v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are increasingly deployed in decision-critical domains, it becomes essential to ensure that their confidence estimates faithfully correspond to their actual correctness. Existing calibration methods have primarily focused on post-hoc adjustments or auxiliary model training; however, many of these approaches necessitate additional supervision or parameter updates. In this work, we propose a novel prompt-based calibration framework inspired by the Credence Calibration Game. Our method establishes a structured interaction loop wherein LLMs receive feedback based on the alignment of their predicted confidence with correctness. Through feedback-driven prompting and natural language summaries of prior performance, our framework dynamically improves model calibration. Extensive experiments across models and game configurations demonstrate consistent improvements in evaluation metrics. Our results highlight the potential of game-based prompting as an effective strategy for LLM calibration. Code and data are available at https://anonymous.4open.science/r/LLM-Calibration/.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement</title>
<link>https://arxiv.org/abs/2508.14391</link>
<guid>https://arxiv.org/abs/2508.14391</guid>
<content:encoded><![CDATA[
<div> framework, relation extraction, large language models, dependency-aware, hierarchical refinement <br />
<br />
Summary: <br />
The DEPTH framework addresses the challenge of unreliable relation extraction in complex sentences using a two-stage process. The Grounding module extracts relations based on the shortest dependency path, reducing syntactic noise. The Refinement module aggregates and revises predictions for a holistic understanding of the sentence. A causality-driven reward model prevents spurious correlations, enhancing fine-tuning through reinforcement learning with human feedback. Experimental results on six benchmarks show a significant reduction in hallucination rate to 7.0% and a 17.2% improvement in F1 score compared to existing methods. <div>
arXiv:2508.14391v1 Announce Type: new 
Abstract: Relation extraction enables the construction of structured knowledge for many downstream applications. While large language models (LLMs) have shown great promise in this domain, most existing methods concentrate on relation classification, which predicts the semantic relation type between a related entity pair. However, we observe that LLMs often struggle to reliably determine whether a relation exists, especially in cases involving complex sentence structures or intricate semantics, which leads to spurious predictions. Such hallucinations can introduce noisy edges in knowledge graphs, compromising the integrity of structured knowledge and downstream reliability. To address these challenges, we propose DEPTH, a framework that integrates Dependency-aware sEntence simPlification and Two-tiered Hierarchical refinement into the relation extraction pipeline. Given a sentence and its candidate entity pairs, DEPTH operates in two stages: (1) the Grounding module extracts relations for each pair by leveraging their shortest dependency path, distilling the sentence into a minimal yet coherent relational context that reduces syntactic noise while preserving key semantics; (2) the Refinement module aggregates all local predictions and revises them based on a holistic understanding of the sentence, correcting omissions and inconsistencies. We further introduce a causality-driven reward model that mitigates reward hacking by disentangling spurious correlations, enabling robust fine-tuning via reinforcement learning with human feedback. Experiments on six benchmarks demonstrate that DEPTH reduces the average hallucination rate to 7.0\% while achieving a 17.2\% improvement in average F1 score over state-of-the-art baselines.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs</title>
<link>https://arxiv.org/abs/2508.14408</link>
<guid>https://arxiv.org/abs/2508.14408</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, self-recognition, authorship discrimination, implicit territorial awareness, cognitive surgery

Summary:
Large language models (LLMs) have shown self-recognition capabilities under certain conditions but struggle when presented with single texts under the Individual Presentation Paradigm (IPP). This failure is attributed to Implicit Territorial Awareness (ITA), where LLMs possess latent abilities to distinguish self- and other-texts but fail to express them. To address this issue, a novel framework called Cognitive Surgery (CoSur) is proposed. CoSur comprises four modules that aim to awaken ITA in LLMs by extracting representations, constructing territories, discriminating authorship, and editing cognition. Experimental results show that implementing CoSur improves the accuracy of LLMs in authorship discrimination tasks under the IPP scenario, achieving significant performance enhancements across different LLM models.<br /><br />Summary: <div>
arXiv:2508.14408v1 Announce Type: new 
Abstract: Large language models (LLMs) have been shown to possess a degree of self-recognition capability-the ability to identify whether a given text was generated by themselves. Prior work has demonstrated that this capability is reliably expressed under the Pair Presentation Paradigm (PPP), where the model is presented with two texts and asked to choose which one it authored. However, performance deteriorates sharply under the Individual Presentation Paradigm (IPP), where the model is given a single text to judge authorship. Although this phenomenon has been observed, its underlying causes have not been systematically analyzed. In this paper, we first replicate existing findings to confirm that LLMs struggle to distinguish self- from other-generated text under IPP. We then investigate the reasons for this failure and attribute it to a phenomenon we term Implicit Territorial Awareness (ITA)-the model's latent ability to distinguish self- and other-texts in representational space, which remains unexpressed in its output behavior. To awaken the ITA of LLMs, we propose Cognitive Surgery (CoSur), a novel framework comprising four main modules: representation extraction, territory construction, authorship discrimination and cognitive editing. Experimental results demonstrate that our proposed method improves the performance of three different LLMs in the IPP scenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%, respectively.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2508.14427</link>
<guid>https://arxiv.org/abs/2508.14427</guid>
<content:encoded><![CDATA[
<div> knowledge graph, language models, entity-level semantic understanding, graph neural network, structured knowledge 

Summary:
This paper introduces a fine-tuning algorithm framework that addresses the limitations of large language models in tasks requiring structured knowledge. By incorporating knowledge graph information and utilizing a graph neural network, the method enhances semantic understanding at the entity level. A fusion mechanism combines knowledge graph embeddings with language model representations, leveraging a gating mechanism to balance linguistic semantics and structural knowledge effectively. The joint loss function considers task performance and structural alignment objectives during training, improving entity prediction accuracy and semantic reasoning. Sensitivity experiments validate the method's stability and effectiveness across tasks such as entity recognition, question answering, and language generation. Results indicate that the structure-aware fine-tuning framework significantly enhances semantic consistency, contextual logic modeling, and the representation of complex semantic units. <div>
arXiv:2508.14427v1 Announce Type: new 
Abstract: This paper addresses the problems of missing reasoning chains and insufficient entity-level semantic understanding in large language models when dealing with tasks that require structured knowledge. It proposes a fine-tuning algorithm framework based on knowledge graph injection. The method builds on pretrained language models and introduces structured graph information for auxiliary learning. A graph neural network is used to encode entities and their relations, constructing a graph-based semantic representation. A fusion mechanism is then designed to jointly model the knowledge graph embeddings with the contextual representations from the language model. To enhance the robustness of knowledge integration, a gating mechanism is introduced to dynamically balance the contributions of linguistic semantics and structural knowledge. This effectively mitigates conflicts between different representational spaces. During training, a joint loss function is constructed to account for both task performance and structural alignment objectives. This helps improve the accuracy of entity prediction and semantic reasoning. The study also includes a series of systematic sensitivity experiments. It evaluates the effects of learning rate, graph coverage, and structural perturbations on model performance. The results further validate the effectiveness and stability of the proposed method across tasks such as entity recognition, question answering, and language generation. Experimental findings show that the proposed structure-aware fine-tuning framework significantly enhances the model's ability to represent complex semantic units. It demonstrates better semantic consistency and contextual logic modeling in scenarios involving structural reasoning and entity extraction.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model</title>
<link>https://arxiv.org/abs/2508.14444</link>
<guid>https://arxiv.org/abs/2508.14444</guid>
<content:encoded><![CDATA[
<div> Transformers, language model, reasoning, Nemotron-Nano-9B-v2, Mamba-Transformer <br />
<br />
Keywords: Transformers, language model, reasoning, Nemotron-Nano-9B-v2, Mamba-Transformer <br />
Summary:
- Nemotron-Nano-9B-v2 is a hybrid Mamba-Transformer language model designed to enhance throughput and maintain high accuracy.
- It utilizes the Nemotron-H architecture with Mamba-2 layers to improve inference speed for reasoning tasks.
- The model is pre-trained on a large dataset and compressed using the Minitron strategy for efficient inference on a single GPU.
- Nemotron-Nano-9B-v2 achieves superior accuracy compared to models like Qwen3-8B on reasoning benchmarks.
- The model checkpoints and datasets are publicly available on Hugging Face for further research and development. <br /> <div>
arXiv:2508.14444v1 Announce Type: new 
Abstract: We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In2x at WMT25 Translation Task</title>
<link>https://arxiv.org/abs/2508.14472</link>
<guid>https://arxiv.org/abs/2508.14472</guid>
<content:encoded><![CDATA[
<div> Keywords: Japanese, translation tasks, large language models, low-resource languages, reward model design

Summary: 
The paper presents the In2x research team's submission for the WMT25 General Machine Translation Shared Task, focusing on Japanese-related translation tasks. The team aims to develop a paradigm for extending large language models (LLMs) to languages beyond English. This paradigm includes data construction methods and reward model design to enhance performance in low-resource or less commonly spoken languages. The ultimate objective is to enable large language model systems to achieve exceptional results in a variety of linguistic contexts. The team's work showcases efforts towards bridging the gap in machine translation performance for languages with limited resources. The approach taken by the In2x research team highlights the potential of leveraging LLMs for improving translation accuracy and efficiency in diverse language settings. 

<br /><br />Summary: <div>
arXiv:2508.14472v1 Announce Type: new 
Abstract: This paper presents the open-system submission by the In2x research team for the WMT25 General Machine Translation Shared Task. Our submission focuses on Japanese-related translation tasks, aiming to explore a generalizable paradigm for extending large language models (LLMs) to other languages. This paradigm encompasses aspects such as data construction methods and reward model design. The ultimate goal is to enable large language model systems to achieve exceptional performance in low-resource or less commonly spoken languages.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning is about giving reasons</title>
<link>https://arxiv.org/abs/2508.14488</link>
<guid>https://arxiv.org/abs/2508.14488</guid>
<content:encoded><![CDATA[
<div> Keywords: logical structure, argument reasoning, transformers, interpretability, natural language

Summary:
In this paper, the authors address the challenge of understanding and articulating the core logical structure of arguments to prove or disprove premises. While transformers have shown the ability to chain rules for simple arguments, they lack interpretability and flexibility for more complex reasoning tasks. The proposed Representation of the Logical Structure (RLS) aims to capture the logical atoms and rules in a natural language argument, enabling deterministic and computationally efficient reasoning. This approach supports various forms of reasoning, including abduction and contradiction identification. By extracting the logical structure from popular reasoning datasets with high accuracies, the model can generate explanations and enhance reasoning capabilities significantly. The RLS framework facilitates on-the-fly mistake rectification and interactive discussions, making it a valuable tool for understanding and analyzing natural language arguments. 

<br /><br />Summary: <div>
arXiv:2508.14488v1 Announce Type: new 
Abstract: Convincing someone of the truth value of a premise requires understanding and articulating the core logical structure of the argument which proves or disproves the premise. Understanding the logical structure of an argument refers to understanding the underlying "reasons" which make up the proof or disproof of the premise - as a function of the "logical atoms" in the argument. While it has been shown that transformers can "chain" rules to derive simple arguments, the challenge of articulating the "reasons" remains. Not only do current approaches to chaining rules suffer in terms of their interpretability, they are also quite constrained in their ability to accommodate extensions to theoretically equivalent reasoning tasks - a model trained to chain rules cannot support abduction or identify contradictions. In this work we suggest addressing these shortcomings by identifying an intermediate representation (which we call the Representation of the Logical Structure (RLS) of the argument) that possesses an understanding of the logical structure of a natural language argument - the logical atoms in the argument and the rules incorporating them. Given the logical structure, reasoning is deterministic and easy to compute. Therefore, our approach supports all forms of reasoning that depend on the logical structure of the natural language argument, including arbitrary depths of reasoning, on-the-fly mistake rectification and interactive discussion with respect to an argument. We show that we can identify and extract the logical structure of natural language arguments in three popular reasoning datasets with high accuracies, thus supporting explanation generation and extending the reasoning capabilities significantly.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoTale: An Enacted Speech-emotion Dataset in Danish</title>
<link>https://arxiv.org/abs/2508.14548</link>
<guid>https://arxiv.org/abs/2508.14548</guid>
<content:encoded><![CDATA[
<div> Dataset, Emotional Speech, Danish, SER models, EmoTale <br />
<br />
Summary: 
The article introduces EmoTale, a new corpus of Danish and English speech recordings with emotion annotations. It addresses the lack of functional datasets for smaller languages like Danish. The dataset is validated for speech emotion recognition (SER) models, demonstrating its predictive power. SER models using self-supervised speech model (SSLM) embeddings and openSMILE feature extractor were developed. The study found that embeddings were more effective than hand-crafted features. The best model achieved an unweighted average recall (UAR) of 64.1% on EmoTale using leave-one-speaker-out cross-validation, showing comparable performance to the existing Danish Emotional Speech (DES) database from 1997. This research contributes to the advancement of emotional speech analysis in smaller languages, enhancing the resources available for studying and understanding emotions in speech. <br /> <div>
arXiv:2508.14548v1 Announce Type: new 
Abstract: While multiple emotional speech corpora exist for commonly spoken languages, there is a lack of functional datasets for smaller (spoken) languages, such as Danish. To our knowledge, Danish Emotional Speech (DES), published in 1997, is the only other database of Danish emotional speech. We present EmoTale; a corpus comprising Danish and English speech recordings with their associated enacted emotion annotations. We demonstrate the validity of the dataset by investigating and presenting its predictive power using speech emotion recognition (SER) models. We develop SER models for EmoTale and the reference datasets using self-supervised speech model (SSLM) embeddings and the openSMILE feature extractor. We find the embeddings superior to the hand-crafted features. The best model achieves an unweighted average recall (UAR) of 64.1% on the EmoTale corpus using leave-one-speaker-out cross-validation, comparable to the performance on DES.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning</title>
<link>https://arxiv.org/abs/2508.14574</link>
<guid>https://arxiv.org/abs/2508.14574</guid>
<content:encoded><![CDATA[
<div> quaternion space, geodesic loss, contrastive loss, sign language production, Progressive Transformers <br />
<br />Summary:
This article introduces enhancements to the Progressive Transformers (PT) architecture for neural sign language production (SLP) to address the high variability of signs. The first enhancement involves encoding poses using bone rotations in quaternion space and training with a geodesic loss to improve the accuracy of joint movements. The second enhancement introduces a contrastive loss to structure decoder embeddings based on semantic similarity, filtering out irrelevant anatomical and stylistic features. On the Phoenix14T dataset, the contrastive loss alone improves the Probability of Correct Keypoint by 16% over the PT baseline. When combined with quaternion-based pose encoding, the model reduces Mean Bone Angle Error by 6%. These results highlight the benefits of incorporating skeletal structure modeling and semantically guided contrastive objectives in Transformer-based SLP models. <br /> <div>
arXiv:2508.14574v1 Announce Type: new 
Abstract: One of the main challenges in neural sign language production (SLP) lies in the high intra-class variability of signs, arising from signer morphology and stylistic variety in the training data. To improve robustness to such variations, we propose two enhancements to the standard Progressive Transformers (PT) architecture (Saunders et al., 2020). First, we encode poses using bone rotations in quaternion space and train with a geodesic loss to improve the accuracy and clarity of angular joint movements. Second, we introduce a contrastive loss to structure decoder embeddings by semantic similarity, using either gloss overlap or SBERT-based sentence similarity, aiming to filter out anatomical and stylistic features that do not convey relevant semantic information. On the Phoenix14T dataset, the contrastive loss alone yields a 16% improvement in Probability of Correct Keypoint over the PT baseline. When combined with quaternion-based pose encoding, the model achieves a 6% reduction in Mean Bone Angle Error. These results point to the benefit of incorporating skeletal structure modeling and semantically guided contrastive objectives on sign pose representations into the training of Transformer-based SLP models.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek</title>
<link>https://arxiv.org/abs/2508.14586</link>
<guid>https://arxiv.org/abs/2508.14586</guid>
<content:encoded><![CDATA[
<div> Keywords: Southern Uzbek, Turkic language, machine translation, NLLB-200 model, low-resource languages

Summary:
Southern Uzbek, a Turkic language spoken by around 5 million people in Afghanistan, has been underrepresented in natural language processing. New resources for Southern Uzbek machine translation have been introduced, including a FLORES+ dev set, parallel sentences from various sources, and a fine-tuned NLLB-200 model called lutfiy. Additionally, a post-processing method has been proposed to improve handling of morphological boundaries by restoring Arabic-script half-space characters. All datasets, models, and tools are made publicly available to support future research on Southern Uzbek and other low-resource languages. <div>
arXiv:2508.14586v1 Announce Type: new 
Abstract: Southern Uzbek (uzs) is a Turkic language variety spoken by around 5 million people in Afghanistan and differs significantly from Northern Uzbek (uzn) in phonology, lexicon, and orthography. Despite the large number of speakers, Southern Uzbek is underrepresented in natural language processing. We present new resources for Southern Uzbek machine translation, including a 997-sentence FLORES+ dev set, 39,994 parallel sentences from dictionary, literary, and web sources, and a fine-tuned NLLB-200 model (lutfiy). We also propose a post-processing method for restoring Arabic-script half-space characters, which improves handling of morphological boundaries. All datasets, models, and tools are released publicly to support future work on Southern Uzbek and other low-resource languages.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous sentiment scores for literary and multilingual contexts</title>
<link>https://arxiv.org/abs/2508.14620</link>
<guid>https://arxiv.org/abs/2508.14620</guid>
<content:encoded><![CDATA[
<div> novel sentiment analysis method, literary texts, concept vector projection, multilingual data, fine-grained analysis
<br />
Summary: 
A new sentiment analysis method is introduced for literary texts, addressing challenges like figurative language and stylistic ambiguity. Traditional tools often fall short, especially for low-resource languages, and transformer models provide limited analysis. The proposed method uses concept vector projection on multilingual literary data, enhancing sentiment expression capture across genres, languages, and historical periods. It outperforms existing tools on English and Danish texts, producing sentiment scores closely aligning with human ratings. This advancement enables more accurate sentiment analysis and sentiment arc modeling in literature.<br /><br />Summary: <div>
arXiv:2508.14620v1 Announce Type: new 
Abstract: Sentiment Analysis is widely used to quantify sentiment in text, but its application to literary texts poses unique challenges due to figurative language, stylistic ambiguity, as well as sentiment evocation strategies. Traditional dictionary-based tools often underperform, especially for low-resource languages, and transformer models, while promising, typically output coarse categorical labels that limit fine-grained analysis. We introduce a novel continuous sentiment scoring method based on concept vector projection, trained on multilingual literary data, which more effectively captures nuanced sentiment expressions across genres, languages, and historical periods. Our approach outperforms existing tools on English and Danish texts, producing sentiment scores whose distribution closely matches human ratings, enabling more accurate analysis and sentiment arc modeling in literature.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving in-context learning with a better scoring function</title>
<link>https://arxiv.org/abs/2508.14685</link>
<guid>https://arxiv.org/abs/2508.14685</guid>
<content:encoded><![CDATA[
<div> quantifiers, in-context learning, attention mechanism, softmax, transformers 

Summary: 
This paper investigates the limitations of Large Language Models (LLMs) in tasks involving first-order quantifiers such as 'all' and 'some', as well as in in-context learning with linear functions. The study identifies Softmax, the scoring function in attention mechanisms, as a factor contributing to these constraints. To address this issue, the authors propose a novel alternative called scaled signed averaging (SSA) and show that it significantly enhances performance on the tasks. Empirical results indicate that models using SSA outperform Softmax-based counterparts on various linguistic probing tasks. Furthermore, both encoder-only and decoder-only transformer models incorporating SSA show improved performance, matching or surpassing models utilizing Softmax.  <div>
arXiv:2508.14685v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit a remarkable capacity to learn by analogy, known as in-context learning (ICL). However, recent studies have revealed limitations in this ability. In this paper, we examine these limitations on tasks involving first-order quantifiers such as {\em all} and {\em some}, as well as on ICL with linear functions. We identify Softmax, the scoring function in attention mechanism, as a contributing factor to these constraints. To address this, we propose \textbf{scaled signed averaging (SSA)}, a novel alternative to Softmax. Empirical results show that SSA dramatically improves performance on our target tasks. Furthermore, we evaluate both encoder-only and decoder-only transformers models with SSA, demonstrating that they match or exceed their Softmax-based counterparts across a variety of linguistic probing tasks.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine</title>
<link>https://arxiv.org/abs/2508.14706</link>
<guid>https://arxiv.org/abs/2508.14706</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Traditional Chinese Medicine, multimodal, dataset, ShizhenGPT

Summary: <br /><br />ShizhenGPT is introduced as the first multimodal large language model specifically designed for Traditional Chinese Medicine (TCM). It addresses the challenges of data scarcity and the complex nature of TCM diagnostics by curating a vast dataset with text, images, audio, and physiological signals. The model is pretrained and instruction-tuned to acquire in-depth TCM knowledge and multimodal reasoning capabilities. Evaluation against national TCM qualification exams shows ShizhenGPT outperforms similar models and competes with larger proprietary models. It excels in TCM visual understanding and demonstrates unified perception across various sensory modalities like sound, pulse, smell, and vision, enabling holistic multimodal perception and diagnosis in TCM. The availability of datasets, models, and code encourages further exploration in this emerging field. <div>
arXiv:2508.14706v1 Announce Type: new 
Abstract: Despite the success of large language models (LLMs) in various domains, their potential in Traditional Chinese Medicine (TCM) remains largely underexplored due to two critical barriers: (1) the scarcity of high-quality TCM data and (2) the inherently multimodal nature of TCM diagnostics, which involve looking, listening, smelling, and pulse-taking. These sensory-rich modalities are beyond the scope of conventional LLMs. To address these challenges, we present ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and physiological signals. ShizhenGPT is pretrained and instruction-tuned to achieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect recent national TCM qualification exams and build a visual benchmark for Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that ShizhenGPT outperforms comparable-scale LLMs and competes with larger proprietary models. Moreover, it leads in TCM visual understanding among existing multimodal LLMs and demonstrates unified perception across modalities like sound, pulse, smell, and vision, paving the way toward holistic multimodal perception and diagnosis in TCM. Datasets, models, and code are publicly available. We hope this work will inspire further exploration in this field.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation</title>
<link>https://arxiv.org/abs/2508.14718</link>
<guid>https://arxiv.org/abs/2508.14718</guid>
<content:encoded><![CDATA[
<div> fraction tokens, tokenization strategy, recipe generation, GPT-2 model, benchmark 

<br />Summary: 
The study introduces a benchmark for text-based recipe generation, comparing GPT-2 large and small models with LSTM/RNN baselines on RecipeDB. The authors develop a targeted tokenization strategy with fraction tokens and custom markers to enhance domain specificity and structure preservation. Evaluation using automatic metrics shows the GPT-2 large model outperforming recurrent baselines with improved BERTScore and reduced perplexity. Remaining challenges include factual accuracy, with potential for integrating real-world constraints and multi-modal inputs in future research. This foundational study highlights the advantages of transformer-based approaches in recipe generation tasks. <div>
arXiv:2508.14718v1 Announce Type: new 
Abstract: We established a rigorous benchmark for text-based recipe generation, a fundamental task in natural language generation. We present a comprehensive comparative study contrasting a fine-tuned GPT-2 large (774M) model against the GPT-2 small (124M) model and traditional LSTM/RNN baselines on the 5-cuisine corpus from RecipeDB. Our key contribution is a targeted tokenization strategy that augments the vocabulary with 23 common fraction tokens and custom structural markers. This approach addresses a critical limitation of generic tokenizers by preserving essential recipe structures and precise numerical quantities, thereby enhancing domain specificity. Performance is evaluated using a comprehensive suite of seven automatic metrics spanning fluency (BLEU-4, METEOR), coherence (ROUGE-L), semantic relevance (BERTScore), and diversity. Our experiments show that the large transformer-based approach yields a >20% relative improvement in BERTScore (F1) (0.92 vs 0.72) over the best recurrent baseline, while reducing perplexity by 69.8%. We conclude with a discussion of remaining challenges, particularly regarding factual accuracy, and outline how this foundational study paves the way for integrating real-world constraints and multi-modal inputs in advanced recipe generation research.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transplant Then Regenerate: A New Paradigm for Text Data Augmentation</title>
<link>https://arxiv.org/abs/2508.14723</link>
<guid>https://arxiv.org/abs/2508.14723</guid>
<content:encoded><![CDATA[
<div> Keywords: data augmentation, deep learning, language models, text augmentation, LMTransplant  

Summary:  
LMTransplant is a novel text augmentation method that leverages large language models (LLMs) by incorporating seed text into an expanded context and asking the model to generate a variant based on this context. This approach allows for more diverse and creative content-level variations while preserving the original text's core attributes. LMTransplant outperforms traditional methods like Back-translation by producing more varied outputs with enhanced control over style and structure. The method demonstrates superior performance across various text-related tasks and exhibits exceptional scalability as the size of augmented data increases. By fully leveraging the knowledge embedded in LLMs, LMTransplant offers a promising approach for enhancing data augmentation techniques in deep learning.  

Summary: <div>
arXiv:2508.14723v1 Announce Type: new 
Abstract: Data augmentation is a critical technique in deep learning. Traditional methods like Back-translation typically focus on lexical-level rephrasing, which primarily produces variations with the same semantics. While large language models (LLMs) have enhanced text augmentation by their "knowledge emergence" capability, controlling the style and structure of these outputs remains challenging and requires meticulous prompt engineering. In this paper, we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs. The core idea of LMTransplant is transplant-then-regenerate: incorporating seed text into a context expanded by LLM, and asking the LLM to regenerate a variant based on the expanded context. This strategy allows the model to create more diverse and creative content-level variants by fully leveraging the knowledge embedded in LLMs, while preserving the core attributes of the original text. We evaluate LMTransplant across various text-related tasks, demonstrating its superior performance over existing text augmentation methods. Moreover, LMTransplant demonstrates exceptional scalability as the size of augmented data grows.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference</title>
<link>https://arxiv.org/abs/2508.14735</link>
<guid>https://arxiv.org/abs/2508.14735</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multilingual natural language inference, code-switching, semantic preservation, cross-lingual alignment

Summary: 
Large language models (LLMs) are being increasingly used in multilingual scenarios but their ability to consistently align logic across languages is not well understood. The study introduces a framework for evaluating multilingual natural language inference (NLI) by generating synthetic premise-hypothesis pairs based on logic and translating them into various languages. Surprisingly, the performance of LLMs does not degrade with code-switching and may even improve, suggesting that variation in translation could act as a regularization signal. Semantic preservation is validated through similarity analyses and cross-lingual alignment visualizations, confirming the accuracy of translated pairs. The research highlights both the potential and limitations of current LLMs in cross-lingual reasoning, pointing to code-switching as a potential strategy for enhancing multilingual robustness.<br /><br />Summary: <div>
arXiv:2508.14735v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly applied in multilingual contexts, yet their capacity for consistent, logically grounded alignment across languages remains underexplored. We present a controlled evaluation framework for multilingual natural language inference (NLI) that generates synthetic, logic-based premise-hypothesis pairs and translates them into a typologically diverse set of languages. This design enables precise control over semantic relations and allows testing in both monolingual and mixed-language (code-switched) conditions. Surprisingly, code-switching does not degrade, and can even improve, performance, suggesting that translation-induced lexical variation may serve as a regularization signal. We validate semantic preservation through embedding-based similarity analyses and cross-lingual alignment visualizations, confirming the fidelity of translated pairs. Our findings expose both the potential and the brittleness of current LLM cross-lingual reasoning, and identify code-switching as a promising lever for improving multilingual robustness. Code available at: https://github.com/KurbanIntelligenceLab/nli-stress-testing
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting</title>
<link>https://arxiv.org/abs/2508.14782</link>
<guid>https://arxiv.org/abs/2508.14782</guid>
<content:encoded><![CDATA[
<div> Keywords: urban transportation systems, deep learning models, large language models, TransLLM, spatiotemporal modeling

Summary:
TransLLM is a novel framework that combines spatiotemporal modeling with large language models to address the limitations of existing approaches in urban transportation systems. It integrates a lightweight spatiotemporal encoder with dilated temporal convolutions and graph attention networks, allowing seamless interaction with large language models. A unique prompt routing mechanism, trained through reinforcement learning, personalizes prompts based on input characteristics, enhancing adaptability across tasks. By encoding spatiotemporal patterns and dynamically composing prompts, TransLLM achieves exceptional performance in supervised and zero-shot settings on various datasets and tasks. The framework demonstrates strong generalization and cross-task adaptability compared to baseline models. The code for TransLLM is publicly available on GitHub for further exploration and utilization. 

<br /><br />Summary: <div>
arXiv:2508.14782v1 Announce Type: new 
Abstract: Urban transportation systems encounter diverse challenges across multiple tasks, such as traffic forecasting, electric vehicle (EV) charging demand prediction, and taxi dispatch. Existing approaches suffer from two key limitations: small-scale deep learning models are task-specific and data-hungry, limiting their generalizability across diverse scenarios, while large language models (LLMs), despite offering flexibility through natural language interfaces, struggle with structured spatiotemporal data and numerical reasoning in transportation domains. To address these limitations, we propose TransLLM, a unified foundation framework that integrates spatiotemporal modeling with large language models through learnable prompt composition. Our approach features a lightweight spatiotemporal encoder that captures complex dependencies via dilated temporal convolutions and dual-adjacency graph attention networks, seamlessly interfacing with LLMs through structured embeddings. A novel instance-level prompt routing mechanism, trained via reinforcement learning, dynamically personalizes prompts based on input characteristics, moving beyond fixed task-specific templates. The framework operates by encoding spatiotemporal patterns into contextual representations, dynamically composing personalized prompts to guide LLM reasoning, and projecting the resulting representations through specialized output layers to generate task-specific predictions. Experiments across seven datasets and three tasks demonstrate the exceptional effectiveness of TransLLM in both supervised and zero-shot settings. Compared to ten baseline models, it delivers competitive performance on both regression and planning problems, showing strong generalization and cross-task adaptability. Our code is available at https://github.com/BiYunying/TransLLM.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs</title>
<link>https://arxiv.org/abs/2508.14817</link>
<guid>https://arxiv.org/abs/2508.14817</guid>
<content:encoded><![CDATA[
<div> clinical tasks, EHR, large language models, retrieval-augmented generation, imaging procedures, antibiotic use timelines, key diagnoses<br />
Summary:<br />
Electronic health records (EHRs) are challenging for clinicians due to their length and noise. Large language models (LLMs) can help extract information from EHRs, but struggle with the extensive text. Retrieval-augmented generation (RAG) retrieves relevant passages to reduce input tokens. Three clinical tasks are proposed: extracting imaging procedures, generating antibiotic use timelines, and identifying key diagnoses. Testing LLMs using targeted text retrieval or recent notes shows RAG performing similarly to full context models with fewer tokens. The results suggest RAG remains competitive and efficient as models improve to handle longer text. <div>
arXiv:2508.14817v1 Announce Type: new 
Abstract: Electronic health records (EHRs) are long, noisy, and often redundant, posing a major challenge for the clinicians who must navigate them. Large language models (LLMs) offer a promising solution for extracting and reasoning over this unstructured text, but the length of clinical notes often exceeds even state-of-the-art models' extended context windows. Retrieval-augmented generation (RAG) offers an alternative by retrieving task-relevant passages from across the entire EHR, potentially reducing the amount of required input tokens. In this work, we propose three clinical tasks designed to be replicable across health systems with minimal effort: 1) extracting imaging procedures, 2) generating timelines of antibiotic use, and 3) identifying key diagnoses. Using EHRs from actual hospitalized patients, we test three state-of-the-art LLMs with varying amounts of provided context, using either targeted text retrieval or the most recent clinical notes. We find that RAG closely matches or exceeds the performance of using recent notes, and approaches the performance of using the models' full context while requiring drastically fewer input tokens. Our results suggest that RAG remains a competitive and efficient approach even as newer models become capable of handling increasingly longer amounts of text.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long Chain-of-Thought Reasoning Across Languages</title>
<link>https://arxiv.org/abs/2508.14828</link>
<guid>https://arxiv.org/abs/2508.14828</guid>
<content:encoded><![CDATA[
<div> Long Chains-of-Thought, Multilingual Reasoning Datasets, Qwen Models, Cross-Lingual Performance, Data Quality

Summary:

Using long chains-of-thought (CoTs) for reasoning in large language models has shown impressive capabilities in English but remains limited in other languages. The study explores multilingual reasoning by translating English datasets, fine-tuning Qwen models, and testing in French, Japanese, Latvian, and Swahili. The effectiveness of English as a pivot language varies by language, with no benefit for French, improved performance for Japanese and Latvian, and insufficiency for Swahili. Extensive multilingual pretraining narrows but doesn't eliminate cross-lingual performance gaps. Lightweight fine-tuning boosts Swahili performance. Data quality and scale trade-offs differ by language, with smaller curated datasets sufficient for English and French, and larger, noisier corpora more effective for Swahili and Latvian. These findings clarify the transferability of long CoTs across languages and provide valuable insights for equitable multilingual reasoning research.

Summary: <br /><br /> <div>
arXiv:2508.14828v1 Announce Type: new 
Abstract: Scaling inference through long chains-of-thought (CoTs) has unlocked impressive reasoning capabilities in large language models (LLMs), yet the reasoning process remains almost exclusively English-centric. We construct translated versions of two popular English reasoning datasets, fine-tune Qwen 2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT generation across French, Japanese, Latvian, and Swahili. Our experiments reveal three key findings. First, the efficacy of using English as a pivot language varies by language: it provides no benefit for French, improves performance when used as the reasoning language for Japanese and Latvian, and proves insufficient for Swahili where both task comprehension and reasoning remain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but does not eliminate the cross-lingual performance gap. A lightweight fine-tune using only 1k traces still improves performance by over 30\% in Swahili. Third, data quality versus scale trade-offs are language dependent: small, carefully curated datasets suffice for English and French, whereas larger but noisier corpora prove more effective for Swahili and Latvian. Together, these results clarify when and why long CoTs transfer across languages and provide translated datasets to foster equitable multilingual reasoning research.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework</title>
<link>https://arxiv.org/abs/2508.14880</link>
<guid>https://arxiv.org/abs/2508.14880</guid>
<content:encoded><![CDATA[
<div> Knowledge graphs, medical information synthesis, deep research, reinforcement learning, fine-tuning  
Summary:   
- The article introduces a new medical deep research agent that addresses challenges faced by general-purpose deep research agents in the medical domain.  
- The agent utilizes a novel data synthesis framework using medical knowledge graphs to generate complex question-answer pairs around rare medical entities.  
- It integrates a custom-built private medical retrieval engine alongside general-purpose tools for accurate medical information synthesis.  
- Through a two-stage training paradigm combining supervised fine-tuning and online reinforcement learning, the MedResearcher-R1-32B model achieves exceptional performance on medical benchmarks.  
- The strategic domain-specific innovations in architecture, tool design, and training data construction enable smaller open-source models to surpass larger proprietary systems in specialized domains. 

<br /><br />Summary: <div>
arXiv:2508.14880v1 Announce Type: new 
Abstract: Recent developments in Large Language Model (LLM)-based agents have shown impressive capabilities spanning multiple domains, exemplified by deep research systems that demonstrate superior performance on complex information-seeking and synthesis tasks. While general-purpose deep research agents have shown impressive capabilities, they struggle significantly with medical domain challenges, as evidenced by leading proprietary systems achieving limited accuracy on complex medical benchmarks. The key limitations are: (1) the model lacks sufficient dense medical knowledge for clinical reasoning, and (2) the framework is constrained by the absence of specialized retrieval tools tailored for medical contexts.We present a medical deep research agent that addresses these challenges through two core innovations. First, we develop a novel data synthesis framework using medical knowledge graphs, extracting the longest chains from subgraphs around rare medical entities to generate complex multi-hop question-answer pairs. Second, we integrate a custom-built private medical retrieval engine alongside general-purpose tools, enabling accurate medical information synthesis. Our approach generates 2100+ diverse trajectories across 12 medical specialties, each averaging 4.2 tool interactions.Through a two-stage training paradigm combining supervised fine-tuning and online reinforcement learning with composite rewards, our MedResearcher-R1-32B model demonstrates exceptional performance, establishing new state-of-the-art results on medical benchmarks while maintaining competitive performance on general deep research tasks. Our work demonstrates that strategic domain-specific innovations in architecture, tool design, and training data construction can enable smaller open-source models to outperform much larger proprietary systems in specialized domains.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs</title>
<link>https://arxiv.org/abs/2508.14896</link>
<guid>https://arxiv.org/abs/2508.14896</guid>
<content:encoded><![CDATA[
<div> quantization, diffusion-based language models, dLLMs, post-training quantization, resource demands<br />
Summary:<br />
- This study focuses on quantizing diffusion-based language models, which are challenging due to their large parameter scale and resource demands.
- Activation outliers with abnormally large values are identified as a key challenge for low-bit quantization in these models.
- State-of-the-art post-training quantization methods are implemented and evaluated across various task types and model variants.
- The analysis considers factors like bit-width, quantization method, task category, and model type to provide practical insights into quantization behavior.
- The findings aim to pave the way for efficient deployment of diffusion large language models and the researchers plan to release all codes and experimental setups to support the community. 
<br /><br />Summary: <div>
arXiv:2508.14896v1 Announce Type: new 
Abstract: Recent advances in diffusion large language models (dLLMs) have introduced a promising alternative to autoregressive (AR) LLMs for natural language generation tasks, leveraging full attention and denoising-based decoding strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training quantization (PTQ) has emerged as a widely adopted technique for compressing AR LLMs, its applicability to dLLMs remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to low-bit quantization, as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, quantization method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the quantization behavior of dLLMs under different configurations. We hope our findings provide a foundation for future research in efficient dLLM deployment. All codes and experimental setups will be released to support the community.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG-Boost: Retrieval-Augmented Generation Enhanced LLM-based Speech Recognition</title>
<link>https://arxiv.org/abs/2508.14048</link>
<guid>https://arxiv.org/abs/2508.14048</guid>
<content:encoded><![CDATA[
<div> Keywords: RAG-Boost, LLM-based ASR system, retrieval-augmented generation, MLC-SLM Challenge, domain terms

Summary:
RAG-Boost, developed by ST-ShinozakiLab for the MLC-SLM Challenge, aims to improve the performance of the baseline LLM-based ASR system by incorporating a retrieval-augmented generation (RAG) module. This module allows partial ASR hypotheses to query a vector store of audio-text pairs and domain terms in order to retrieve relevant information to correct recognition errors. The retrieved results are then combined with the live ASR hypotheses, creating fused hypotheses that are inputted into the LLM for improved responses. By leveraging retrieval-augmented generation, RAG-Boost enhances the accuracy and efficiency of the ASR system, resulting in better transcription outcomes for the task at hand. <div>
arXiv:2508.14048v1 Announce Type: cross 
Abstract: In this paper, we propose RAG-Boost (ST-ShinozakiLab Task I system), which enhances the baseline LLM-based ASR system of the MLC-SLM Challenge (task I) with a retrieval-augmented generation (RAG) module on the fly. Each partial ASR hypothesis queries a vector store of audio-text pairs and domain terms, and the retrieved results are fused with the live ASR hypotheses to fix recognition errors. The fused hypotheses are passed to the LLM, yielding improved responses.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MahaTTS: A Unified Framework for Multilingual Text-to-Speech Synthesis</title>
<link>https://arxiv.org/abs/2508.14049</link>
<guid>https://arxiv.org/abs/2508.14049</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-Speech, multilingual, MahaTTS-v2, Indic languages, Wav2Vec2.0 tokens<br />
Summary: <br />
The article introduces MahaTTS-v2, a Multilingual Multi-speaker Text-To-Speech (TTS) system designed specifically for Indic languages. The model, trained on 20K hours of Indian language data, emphasizes multilingual expressive capabilities. It utilizes Wav2Vec2.0 tokens for semantic extraction and a Language Model for text-to-semantic modeling. A Conditional Flow Model (CFM) is employed for semantics to melspectogram generation, leading to effective results compared to other frameworks. The development of MahaTTS-v2 aims to address the lack of multilingual TTS models that often overlook non-European languages, thus expanding access to information for a wider audience. The code for the model is openly available on GitHub for further exploration and development. <br /> <div>
arXiv:2508.14049v1 Announce Type: cross 
Abstract: Current Text-to-Speech models pose a multilingual challenge, where most of the models traditionally focus on English and European languages, thereby hurting the potential to provide access to information to many more people. To address this gap, we introduce MahaTTS-v2 a Multilingual Multi-speaker Text-To-Speech (TTS) system that has excellent multilingual expressive capabilities in Indic languages. The model has been trained on around 20K hours of data specifically focused on Indian languages. Our approach leverages Wav2Vec2.0 tokens for semantic extraction, and a Language Model (LM) for text-to-semantic modeling. Additionally, we have used a Conditional Flow Model (CFM) for semantics to melspectogram generation. The experimental results indicate the effectiveness of the proposed approach over other frameworks. Our code is available at https://github.com/dubverse-ai/MahaTTSv2
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering</title>
<link>https://arxiv.org/abs/2508.14052</link>
<guid>https://arxiv.org/abs/2508.14052</guid>
<content:encoded><![CDATA[
<div> finance, information retrieval, large language models, benchmark, agentic retrieval

Summary:
FinAgentBench is introduced as the first benchmark for evaluating retrieval with multi-step reasoning in the financial domain. With 3,429 expert-annotated examples on S&amp;P-100 listed firms, this benchmark assesses the ability of large language models (LLMs) to identify relevant document types and extract key passages. By separating these reasoning steps, the evaluation framework provides insights into LLM behavior in finance. State-of-the-art models were tested, showing that targeted fine-tuning can enhance agentic retrieval performance. The dataset will be made publicly available, with plans to expand it to cover the full S&amp;P 500 and beyond. <div>
arXiv:2508.14052v1 Announce Type: cross 
Abstract: Accurate information retrieval (IR) is critical in the financial domain, where investors must identify relevant information from large collections of documents. Traditional IR methods-whether sparse or dense-often fall short in retrieval accuracy, as it requires not only capturing semantic similarity but also performing fine-grained reasoning over document structure and domain-specific knowledge. Recent advances in large language models (LLMs) have opened up new opportunities for retrieval with multi-step reasoning, where the model ranks passages through iterative reasoning about which information is most relevant to a given query. However, there exists no benchmark to evaluate such capabilities in the financial domain. To address this gap, we introduce FinAgentBench, the first large-scale benchmark for evaluating retrieval with multi-step reasoning in finance -- a setting we term agentic retrieval. The benchmark consists of 3,429 expert-annotated examples on S&amp;P-100 listed firms and assesses whether LLM agents can (1) identify the most relevant document type among candidates, and (2) pinpoint the key passage within the selected document. Our evaluation framework explicitly separates these two reasoning steps to address context limitations. This design enables to provide a quantitative basis for understanding retrieval-centric LLM behavior in finance. We evaluate a suite of state-of-the-art models and further demonstrated how targeted fine-tuning can significantly improve agentic retrieval performance. Our benchmark provides a foundation for studying retrieval-centric LLM behavior in complex, domain-specific tasks for finance. We will release the dataset publicly upon acceptance of the paper and plan to expand and share dataset for the full S&amp;P 500 and beyond.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text</title>
<link>https://arxiv.org/abs/2508.14190</link>
<guid>https://arxiv.org/abs/2508.14190</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, authorship attribution, multi-task learning, text detection, adversarial obfuscation

Summary:
DA-MTL is a multi-task learning framework designed to address text detection and authorship attribution challenges in Large Language Models (LLMs) like GPT-4 and Llama. The framework is evaluated on multiple datasets and LLM sources, demonstrating strong performance in multiple languages. By capturing unique task characteristics and sharing insights between tasks, DA-MTL shows improved performance in both text detection and authorship attribution. The framework also analyzes cross-modal and cross-lingual patterns and assesses its robustness against adversarial obfuscation techniques. This research offers valuable insights into LLM behavior and the generalization of detection and authorship attribution in natural language generation.<br /><br />Summary: <div>
arXiv:2508.14190v1 Announce Type: cross 
Abstract: Large Language Models (LLMs), such as GPT-4 and Llama, have demonstrated remarkable abilities in generating natural language. However, they also pose security and integrity challenges. Existing countermeasures primarily focus on distinguishing AI-generated content from human-written text, with most solutions tailored for English. Meanwhile, authorship attribution--determining which specific LLM produced a given text--has received comparatively little attention despite its importance in forensic analysis. In this paper, we present DA-MTL, a multi-task learning framework that simultaneously addresses both text detection and authorship attribution. We evaluate DA-MTL on nine datasets and four backbone models, demonstrating its strong performance across multiple languages and LLM sources. Our framework captures each task's unique characteristics and shares insights between them, which boosts performance in both tasks. Additionally, we conduct a thorough analysis of cross-modal and cross-lingual patterns and assess the framework's robustness against adversarial obfuscation techniques. Our findings offer valuable insights into LLM behavior and the generalization of both detection and authorship attribution.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring LLM Code Generation Stability via Structural Entropy</title>
<link>https://arxiv.org/abs/2508.14288</link>
<guid>https://arxiv.org/abs/2508.14288</guid>
<content:encoded><![CDATA[
<div> Keywords: code generation, large language models, abstract syntax tree, entropy analysis, model evaluation

Summary:
Assessing the stability of code generation from large language models (LLMs) is crucial in real-world development. This study introduces a method to evaluate the reliability of code generation by extending structural-entropy concepts to the program domain. By analyzing abstract syntax trees (AST) and measuring stability using Jensen-Shannon divergence and Structural Cross-Entropy ratio, the study provides insights into model consistency and robustness. The metrics are reference-free, language-agnostic, and execution-independent, offering a lightweight addition to code-generation evaluation. By benchmarking leading LLMs on code generation tasks, the study demonstrates how AST-driven structural entropy can reveal nuances in model performance. The method's efficiency, running in O(n,d) time without external tests, makes it a valuable tool for evaluating code generation accuracy and reliability. 

<br /><br />Summary: <div>
arXiv:2508.14288v1 Announce Type: cross 
Abstract: Assessing the stability of code generation from large language models (LLMs) is essential for judging their reliability in real-world development. We extend prior "structural-entropy concepts" to the program domain by pairing entropy with abstract syntax tree (AST) analysis. For any fixed prompt, we collect the multiset of depth-bounded subtrees of AST in each generated program and treat their relative frequencies as a probability distribution. We then measure stability in two complementary ways: (i) Jensen-Shannon divergence, a symmetric, bounded indicator of structural overlap, and (ii) a Structural Cross-Entropy ratio that highlights missing high-probability patterns. Both metrics admit structural-only and token-aware variants, enabling separate views on control-flow shape and identifier-level variability. Unlike pass@k, BLEU, or CodeBLEU, our metrics are reference-free, language-agnostic, and execution-independent. We benchmark several leading LLMs on standard code generation tasks, demonstrating that AST-driven structural entropy reveals nuances in model consistency and robustness. The method runs in O(n,d) time with no external tests, providing a lightweight addition to the code-generation evaluation toolkit.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing</title>
<link>https://arxiv.org/abs/2508.14300</link>
<guid>https://arxiv.org/abs/2508.14300</guid>
<content:encoded><![CDATA[
<div> embeddings, protocol fuzzing, large language models, multi-agent system, structured reasoning
Summary:<br />
- MultiFuzz is a novel dense retrieval-based multi-agent system designed to enhance protocol fuzzing techniques by integrating semantic-aware context retrieval, specialized agents, and structured reasoning.<br />
- It overcomes limitations like unreliable output, LLM hallucinations, and assumptions of LLM knowledge about protocol specifications faced by previous systems like ChatAFL.<br />
- MultiFuzz utilizes protocol documentation to build embeddings in a vector database for a retrieval-augmented generation pipeline, enabling agents to generate reliable and structured outputs for mutating protocol messages with enhanced state coverage and adherence to syntactic constraints.<br />
- The framework decomposes the fuzzing process into modular groups of agents that collaborate through chain-of-thought reasoning to dynamically adapt fuzzing strategies based on retrieved contextual knowledge.<br />
- Experimental evaluations on the Real-Time Streaming Protocol (RTSP) demonstrate that MultiFuzz significantly improves branch coverage and explores deeper protocol states and transitions compared to state-of-the-art fuzzers such as NSFuzz, AFLNet, and ChatAFL.<br /> <div>
arXiv:2508.14300v1 Announce Type: cross 
Abstract: Traditional protocol fuzzing techniques, such as those employed by AFL-based systems, often lack effectiveness due to a limited semantic understanding of complex protocol grammars and rigid seed mutation strategies. Recent works, such as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol fuzzing and address these limitations, pushing protocol fuzzers to wider exploration of the protocol state space. But ChatAFL still faces issues like unreliable output, LLM hallucinations, and assumptions of LLM knowledge about protocol specifications. This paper introduces MultiFuzz, a novel dense retrieval-based multi-agent system designed to overcome these limitations by integrating semantic-aware context retrieval, specialized agents, and structured tool-assisted reasoning. MultiFuzz utilizes agentic chunks of protocol documentation (RFC Documents) to build embeddings in a vector database for a retrieval-augmented generation (RAG) pipeline, enabling agents to generate more reliable and structured outputs, enhancing the fuzzer in mutating protocol messages with enhanced state coverage and adherence to syntactic constraints. The framework decomposes the fuzzing process into modular groups of agents that collaborate through chain-of-thought reasoning to dynamically adapt fuzzing strategies based on the retrieved contextual knowledge. Experimental evaluations on the Real-Time Streaming Protocol (RTSP) demonstrate that MultiFuzz significantly improves branch coverage and explores deeper protocol states and transitions over state-of-the-art (SOTA) fuzzers such as NSFuzz, AFLNet, and ChatAFL. By combining dense retrieval, agentic coordination, and language model reasoning, MultiFuzz establishes a new paradigm in autonomous protocol fuzzing, offering a scalable and extensible foundation for future research in intelligent agentic-based fuzzing systems.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation</title>
<link>https://arxiv.org/abs/2508.14302</link>
<guid>https://arxiv.org/abs/2508.14302</guid>
<content:encoded><![CDATA[
<div> pruning, Large Language Models, edge hardware, dynamic, sparsity <br />
Summary: 
The article introduces A/I-GLASS, a dynamic pruning method for deploying Large Language Models (LLMs) on edge hardware. This method, consisting of Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, aims to reduce computation without compromising quality. Unlike static or predictor-based schemes, A/I-GLASS dynamically selects FFN units based on a rank-aggregation of prompt local and model-intrinsic global neuron statistics. The method outperforms previous training-free methods, especially in challenging long-form generation scenarios, without the need for auxiliary predictors or adding any inference overhead. Empirical results across multiple LLMs and benchmarks support the effectiveness of A/I-GLASS in achieving aggressive and prompt-aware dynamic pruning. <br /> <div>
arXiv:2508.14302v1 Announce Type: cross 
Abstract: Deploying Large Language Models (LLMs) on edge hardware demands aggressive, prompt-aware dynamic pruning to reduce computation without degrading quality. Static or predictor-based schemes either lock in a single sparsity pattern or incur extra runtime overhead, and recent zero-shot methods that rely on statistics from a single prompt fail on short prompt and/or long generation scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, two training-free methods that dynamically select FFN units using a rank-aggregation of prompt local and model-intrinsic global neuron statistics. Empirical results across multiple LLMs and benchmarks demonstrate that GLASS significantly outperforms prior training-free methods, particularly in challenging long-form generation scenarios, without relying on auxiliary predictors or adding any inference overhead.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization</title>
<link>https://arxiv.org/abs/2508.14460</link>
<guid>https://arxiv.org/abs/2508.14460</guid>
<content:encoded><![CDATA[
<div> Keywords: DuPO, dual learning, preference optimization, reinforcement learning, self-supervised reward 

Summary: 
DuPO is a dual learning-based preference optimization framework that addresses limitations in traditional reinforcement learning approaches by generating annotation-free feedback through a generalized duality. It decomposes tasks into known and unknown components, enabling the reconstruction of unknown parts using primal outputs and known information. This allows for broader applicability to non-invertible tasks. By using the quality of this reconstruction as a self-supervised reward, DuPO optimizes the primal task and leverages the capabilities of language model models to instantiate both tasks with a single model. Empirical results demonstrate significant improvements in translation quality, mathematical reasoning accuracy, and performance as an inference-time reranker. DuPO emerges as a scalable, general, and annotation-free framework for optimizing large language models. 

Summary: <div>
arXiv:2508.14460v1 Announce Type: cross 
Abstract: We present DuPO, a dual learning-based preference optimization framework that generates annotation-free feedback via a generalized duality. DuPO addresses two key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s reliance on costly labels and applicability restricted to verifiable tasks, and traditional dual learning's restriction to strictly dual task pairs (e.g., translation and back-translation). Specifically, DuPO decomposes a primal task's input into known and unknown components, then constructs its dual task to reconstruct the unknown part using the primal output and known information (e.g., reversing math solutions to recover hidden variables), broadening applicability to non-invertible tasks. The quality of this reconstruction serves as a self-supervised reward to optimize the primal task, synergizing with LLMs' ability to instantiate both tasks via a single model. Empirically, DuPO achieves substantial gains across diverse tasks: it enhances the average translation quality by 2.13 COMET over 756 directions, boosts the mathematical reasoning accuracy by an average of 6.4 points on three challenge benchmarks, and enhances performance by 9.3 points as an inference-time reranker (trading computation for accuracy). These results position DuPO as a scalable, general, and annotation-free paradigm for LLM optimization.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2508.14564</link>
<guid>https://arxiv.org/abs/2508.14564</guid>
<content:encoded><![CDATA[
arXiv:2508.14564v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) and reasoning frameworks have opened new possibilities for improving the perspective -taking capabilities of autonomous agents. However, tasks that involve active perception, collaborative reasoning, and perspective taking (understanding what another agent can see or knows) pose persistent challenges for current LLM-based systems. This study investigates the potential of structured examples derived from transformed solution graphs generated by the Fast Downward planner to improve the performance of LLM-based agents within a ReAct framework. We propose a structured solution-processing pipeline that generates three distinct categories of examples: optimal goal paths (G-type), informative node paths (E-type), and step-by-step optimal decision sequences contrasting alternative actions (L-type). These solutions are further converted into ``thought-action'' examples by prompting an LLM to explicitly articulate the reasoning behind each decision. While L-type examples slightly reduce clarification requests and overall action steps, they do not yield consistent improvements. Agents are successful in tasks requiring basic attentional filtering but struggle in scenarios that required mentalising about occluded spaces or weighing the costs of epistemic actions. These findings suggest that structured examples alone are insufficient for robust perspective-taking, underscoring the need for explicit belief tracking, cost modelling, and richer environments to enable socially grounded collaboration in LLM-based agents.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers</title>
<link>https://arxiv.org/abs/2508.14704</link>
<guid>https://arxiv.org/abs/2508.14704</guid>
<content:encoded><![CDATA[
arXiv:2508.14704v1 Announce Type: cross 
Abstract: The Model Context Protocol has emerged as a transformative standard for connecting large language models to external data sources and tools, rapidly gaining adoption across major AI providers and development platforms. However, existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. Our benchmark encompasses 6 core domains spanning 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching. To ensure rigorous evaluation, we implement execution-based evaluators, including format evaluators for agent format compliance, static evaluators for time-invariant content matching, and dynamic evaluators that automatically retrieve real-time ground truth for temporally sensitive tasks. Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. In addition, our benchmark poses a significant long-context challenge for LLM agents, as the number of input tokens increases rapidly with the number of interaction steps. Moreover, it introduces an unknown-tools challenge, as LLM agents often lack familiarity with the precise usage of the MCP servers. Notably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct frameworks. Beyond evaluation, we open-source our extensible evaluation framework with UI support, enabling researchers and practitioners to seamlessly integrate new agents and MCP servers while fostering innovation in the rapidly evolving MCP ecosystem.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privileged Self-Access Matters for Introspection in AI</title>
<link>https://arxiv.org/abs/2508.14802</link>
<guid>https://arxiv.org/abs/2508.14802</guid>
<content:encoded><![CDATA[
arXiv:2508.14802v1 Announce Type: cross 
Abstract: Whether AI models can introspect is an increasingly important practical question. But there is no consensus on how introspection is to be defined. Beginning from a recently proposed ''lightweight'' definition, we argue instead for a thicker one. According to our proposal, introspection in AI is any process which yields information about internal states through a process more reliable than one with equal or lower computational cost available to a third party. Using experiments where LLMs reason about their internal temperature parameters, we show they can appear to have lightweight introspection while failing to meaningfully introspect per our proposed definition.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models</title>
<link>https://arxiv.org/abs/2508.14869</link>
<guid>https://arxiv.org/abs/2508.14869</guid>
<content:encoded><![CDATA[
arXiv:2508.14869v1 Announce Type: cross 
Abstract: Prompt engineering has rapidly emerged as a critical skill for effective interaction with large language models (LLMs). However, the cognitive and neural underpinnings of this expertise remain largely unexplored. This paper presents findings from a cross-sectional pilot fMRI study investigating differences in brain functional connectivity and network activity between experts and intermediate prompt engineers. Our results reveal distinct neural signatures associated with higher prompt engineering literacy, including increased functional connectivity in brain regions such as the left middle temporal gyrus and the left frontal pole, as well as altered power-frequency dynamics in key cognitive networks. These findings offer initial insights into the neurobiological basis of prompt engineering proficiency. We discuss the implications of these neurocognitive markers in Natural Language Processing (NLP). Understanding the neural basis of human expertise in interacting with LLMs can inform the design of more intuitive human-AI interfaces, contribute to cognitive models of LLM interaction, and potentially guide the development of AI systems that better align with human cognitive workflows. This interdisciplinary approach aims to bridge the gap between human cognition and machine intelligence, fostering a deeper understanding of how humans learn and adapt to complex AI systems.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual Community: An Open World for Humans, Robots, and Society</title>
<link>https://arxiv.org/abs/2508.14893</link>
<guid>https://arxiv.org/abs/2508.14893</guid>
<content:encoded><![CDATA[
arXiv:2508.14893v1 Announce Type: cross 
Abstract: The rapid progress in AI and Robotics may lead to a profound societal transformation, as humans and robots begin to coexist within shared communities, introducing both opportunities and challenges. To explore this future, we present Virtual Community-an open-world platform for humans, robots, and society-built on a universal physics engine and grounded in real-world 3D scenes. With Virtual Community, we aim to study embodied social intelligence at scale: 1) How robots can intelligently cooperate or compete; 2) How humans develop social relations and build community; 3) More importantly, how intelligent robots and humans can co-exist in an open world. To support these, Virtual Community features: 1) An open-source multi-agent physics simulator that supports robots, humans, and their interactions within a society; 2) A large-scale, real-world aligned community generation pipeline, including vast outdoor space, diverse indoor scenes, and a community of grounded agents with rich characters and appearances. Leveraging Virtual Community, we propose two novel challenges. The Community Planning Challenge evaluates multi-agent reasoning and planning ability in open-world settings, such as cooperating to help agents with daily activities and efficiently connecting other agents. The Community Robot Challenge requires multiple heterogeneous robots to collaborate in solving complex open-world tasks. We evaluate various baselines on these tasks and demonstrate the challenges in both high-level open-world task planning and low-level cooperation controls. We hope that Virtual Community will unlock further study of human-robot coexistence within open-world environments.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model</title>
<link>https://arxiv.org/abs/2312.11370</link>
<guid>https://arxiv.org/abs/2312.11370</guid>
<content:encoded><![CDATA[
arXiv:2312.11370v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable proficiency in human-level reasoning and generation capabilities, which encourages extensive research on their application in mathematical problem solving. However, current work has been largely focused on text-based mathematical problems, with limited investigation in problems involving geometric information. Addressing this gap, we aim to enable LLMs to solve geometric problems by understanding image input. We first analyze the limitations of current Multimodal Large Language Models (MLLMs) in this area: they struggle to accurately comprehending basic geometric elements and their relationships. To overcome these challenges, we take advantage of the unique characteristics of geometric problems (such as unique geometric logical form, and geometric scalability) and the capacity of the textual LLMs to build an enriched multimodal geometry dataset based on existing data. The augmented dataset, Geo170K, contains more than 170K geometric image-caption and question-answer pairs. Utilizing our constructed Geo170K dataset, we develop G-LLaVA, which demonstrates exceptional performance in solving geometric problems, significantly outperforming GPT-4-V on the MathVista benchmark with only 7B parameters.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Debiasing for Fair Multi-modal LLMs</title>
<link>https://arxiv.org/abs/2408.06569</link>
<guid>https://arxiv.org/abs/2408.06569</guid>
<content:encoded><![CDATA[
arXiv:2408.06569v2 Announce Type: replace 
Abstract: Multi-modal Large Language Models (MLLMs) have dramatically advanced the research field and delivered powerful vision-language understanding capabilities. However, these models often inherit deep-rooted social biases from their training data, leading to uncomfortable responses with respect to attributes such as race and gender. This paper addresses the issue of social biases in MLLMs by i) introducing a comprehensive counterfactual dataset with multiple social concepts (CMSC), which complements existing datasets by providing 18 diverse and balanced social concepts; and ii) proposing a counter-stereotype debiasing (CSD) strategy that mitigates social biases in MLLMs by leveraging the opposites of prevalent stereotypes. CSD incorporates both a novel bias-aware data sampling method and a loss rescaling method, enabling the model to effectively reduce biases. We conduct extensive experiments with four prevalent MLLM architectures. The results demonstrate the advantage of the CMSC dataset and the edge of CSD strategy in reducing social biases compared to existing competing methods, without compromising the overall performance on general multi-modal reasoning benchmarks.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources</title>
<link>https://arxiv.org/abs/2409.08239</link>
<guid>https://arxiv.org/abs/2409.08239</guid>
<content:encoded><![CDATA[
arXiv:2409.08239v2 Announce Type: replace 
Abstract: Synthetic data generation has recently emerged as a promising approach for enhancing the capabilities of large language models (LLMs) without the need for expensive human annotations. However, existing methods often generate data that can be low quality or contrived. In this paper, we introduce Source2Synth, a scalable approach for synthetic data generation and curation that is grounded in real-world data sources. Source2Synth takes as input a custom data source and produces synthetic data examples with intermediate reasoning steps. Our method improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two tasks that leverage two different types of data: multi-hop question answering (MHQA), where we test complex reasoning abilities leveraging documents, and tabular question answering (TQA), where we test tool usage leveraging tables. Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotpotQA compared to the fine-tuned baselines.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deliberate Reasoning in Language Models as Structure-Aware Planning with an Accurate World Model</title>
<link>https://arxiv.org/abs/2410.03136</link>
<guid>https://arxiv.org/abs/2410.03136</guid>
<content:encoded><![CDATA[
arXiv:2410.03136v4 Announce Type: replace 
Abstract: Enhancing the reasoning capabilities of language models (LMs) remains a key challenge, especially for tasks that require complex, multi-step decision-making where existing Chain-of-Thought (CoT) approaches struggle with consistency and verification. In this paper, we propose a novel reasoning framework, referred to as Structure-aware Planning with an Accurate World Model (SWAP), that integrates structured knowledge representation with learned planning. Unlike prior methods that rely purely on natural language reasoning, SWAP leverages entailment graphs to encode structured dependencies and enable symbolic verification of intermediate steps. To systematically construct and update the graph, SWAP employs a policy model to propose candidate expansions and a world model to predict structural updates. To improve accuracy, the world model generates multiple alternative updates, and a discriminator re-ranks them based on plausibility. To encourage diverse exploration, we introduce Diversity-based Modelling (DM), which samples candidates from the remaining probability mass after removing previously sampled candidates from the original policy distribution. Additionally, SWAP improves the discrimination accuracy through Contrastive Ranking (CR), which directly compares candidates within prompts and incorporates meta-knowledge to improve ranking quality. We evaluate SWAP across diverse reasoning-intensive benchmarks including math reasoning, logical reasoning, and coding tasks. Extensive experiments demonstrate that SWAP significantly improves upon the base models and consistently outperforms existing reasoning methods.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChuLo: Chunk-Level Key Information Representation for Long Document Understanding</title>
<link>https://arxiv.org/abs/2410.11119</link>
<guid>https://arxiv.org/abs/2410.11119</guid>
<content:encoded><![CDATA[
arXiv:2410.11119v5 Announce Type: replace 
Abstract: Transformer-based models have achieved remarkable success in various Natural Language Processing (NLP) tasks, yet their ability to handle long documents is constrained by computational limitations. Traditional approaches, such as truncating inputs, sparse self-attention, and chunking, attempt to mitigate these issues, but they often lead to information loss and hinder the model's ability to capture long-range dependencies. In this paper, we introduce ChuLo, a novel chunk representation method for long document understanding that addresses these limitations. Our ChuLo groups input tokens using unsupervised keyphrase extraction, emphasizing semantically important keyphrase based chunks to retain core document content while reducing input length. This approach minimizes information loss and improves the efficiency of Transformer-based models. Preserving all tokens in long document understanding, especially token classification tasks, is important to ensure that fine-grained annotations, which depend on the entire sequence context, are not lost. We evaluate our method on multiple long document classification tasks and long document token classification tasks, demonstrating its effectiveness through comprehensive qualitative and quantitative analysis. Our implementation is open-sourced on https://github.com/adlnlp/Chulo.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Little Human Data Goes A Long Way</title>
<link>https://arxiv.org/abs/2410.13098</link>
<guid>https://arxiv.org/abs/2410.13098</guid>
<content:encoded><![CDATA[
arXiv:2410.13098v3 Announce Type: replace 
Abstract: Faced with an expensive human annotation process, creators of NLP systems increasingly turn to synthetic data generation. While this method shows promise, the extent to which synthetic data can replace human annotation is poorly understood. We investigate the use of synthetic data in Fact Verification (FV) and Question Answering (QA) by studying the effects of incrementally replacing human generated data with synthetic points on eight diverse datasets. Strikingly, replacing up to 90% of the training data only marginally decreases performance, but replacing the final 10% leads to severe declines. We find that models trained on purely synthetic data can be reliably improved by including as few as 125 human generated data points. We show that matching the performance gain of just a little additional human data (only 200 points) requires an order of magnitude more synthetic data and estimate price ratios at which human annotation would be a more cost-effective solution. Our results suggest that even when human annotation at scale is infeasible, there is great value to having a small proportion of the dataset being human generated.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models</title>
<link>https://arxiv.org/abs/2411.02433</link>
<guid>https://arxiv.org/abs/2411.02433</guid>
<content:encoded><![CDATA[
arXiv:2411.02433v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, our SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (Gemma, Qwen, Mixtral, gpt-oss) and scales (from 1B to 45B), including more advanced architectural configurations such as the mixture of experts (MoE). Our evaluation spans a wide variety of tasks and the results demonstrate that SLED consistently improves factual accuracy compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Semantic Parsing: Improving Generalization with Lexical Knowledge</title>
<link>https://arxiv.org/abs/2412.10207</link>
<guid>https://arxiv.org/abs/2412.10207</guid>
<content:encoded><![CDATA[
arXiv:2412.10207v3 Announce Type: replace 
Abstract: Open-domain semantic parsing remains a challenging task, as neural models often rely on heuristics and struggle to handle unseen concepts. In this paper, we investigate the potential of large language models (LLMs) for this task and introduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective approach that integrates external symbolic knowledge into the parsing process. Our experiments not only show that LLMs outperform previous encoder-decoder baselines for semantic parsing, but that RASP further enhances their ability to predict unseen concepts, nearly doubling the performance of previous models on out-of-distribution concepts. These findings highlight the promise of leveraging large language models and retrieval mechanisms for robust and open-domain semantic parsing.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Oriented Automatic Fact-Checking with Frame-Semantics</title>
<link>https://arxiv.org/abs/2501.13288</link>
<guid>https://arxiv.org/abs/2501.13288</guid>
<content:encoded><![CDATA[
arXiv:2501.13288v3 Announce Type: replace 
Abstract: We propose a novel paradigm for automatic fact-checking that leverages frame semantics to enhance the structured understanding of claims and guide the process of fact-checking them. To support this, we introduce a pilot dataset of real-world claims extracted from PolitiFact, specifically annotated for large-scale structured data. This dataset underpins two case studies: the first investigates voting-related claims using the Vote semantic frame, while the second explores various semantic frames based on data sources from the Organisation for Economic Co-operation and Development (OECD). Our findings demonstrate the effectiveness of frame semantics in improving evidence retrieval and explainability for fact-checking. Finally, we conducted a survey of frames evoked in fact-checked claims, identifying high-impact frames to guide future work in this direction.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Generation from Visual Events: State-of-the-Art and Key Open Questions</title>
<link>https://arxiv.org/abs/2502.13034</link>
<guid>https://arxiv.org/abs/2502.13034</guid>
<content:encoded><![CDATA[
arXiv:2502.13034v3 Announce Type: replace 
Abstract: In recent years, a substantial body of work in visually grounded natural language processing has focused on real-life multimodal scenarios such as describing content depicted in images or videos. However, comparatively less attention has been devoted to study the nature and degree of interaction between the different modalities in these scenarios. In this paper, we argue that any task dealing with natural language generation from sequences of images or frames is an instance of the broader, more general problem of modeling the intricate relationships between visual events unfolding over time and the features of the language used to interpret, describe, or narrate them. Therefore, solving these tasks requires models to be capable of identifying and managing such intricacies. We consider five seemingly different tasks, which we argue are compelling instances of this broader multimodal problem. Subsequently, we survey the modeling and evaluation approaches adopted for these tasks in recent years and examine the common set of challenges these tasks pose. Building on this perspective, we identify key open questions and propose several research directions for future investigation.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Language Multi-Agent Learning with Credit Re-Assignment for Interactive Environment Generalization</title>
<link>https://arxiv.org/abs/2502.14496</link>
<guid>https://arxiv.org/abs/2502.14496</guid>
<content:encoded><![CDATA[
arXiv:2502.14496v3 Announce Type: replace 
Abstract: LLM-based agents have made significant advancements in interactive environments, such as mobile operations and web browsing, and other domains beyond computer using. Current multi-agent systems universally excel in performance, compared to single agents, but struggle with generalization across environments due to predefined roles and inadequate strategies for generalizing language agents. The challenge of achieving both strong performance and good generalization has hindered the progress of multi-agent systems for interactive environments. To address these issues, we propose CollabUIAgents, a multi-agent reinforcement learning framework with a novel multi-agent credit re-assignment (CR) strategy, assigning process rewards with LLMs rather than environment-specific rewards and learning with synthesized preference data, in order to foster generalizable, collaborative behaviors among the role-free agents' policies. Empirical results show that our framework improves both performance and cross-environment generalizability of multi-agent systems. Moreover, our 7B-parameter system achieves results on par with or exceed strong closed-source models, and the LLM that guides the CR. We also provide insights in using granular CR rewards effectively for environment generalization, and accommodating trained LLMs in multi-agent systems.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JudgeLRM: Large Reasoning Models as a Judge</title>
<link>https://arxiv.org/abs/2504.00050</link>
<guid>https://arxiv.org/abs/2504.00050</guid>
<content:encoded><![CDATA[
arXiv:2504.00050v2 Announce Type: replace 
Abstract: The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain of Correction for Full-text Speech Recognition with Large Language Models</title>
<link>https://arxiv.org/abs/2504.01519</link>
<guid>https://arxiv.org/abs/2504.01519</guid>
<content:encoded><![CDATA[
arXiv:2504.01519v2 Announce Type: replace 
Abstract: Full-text error correction with Large Language Models (LLMs) for Automatic Speech Recognition (ASR) is attracting increased attention for its ability to address a wide range of error types, such as punctuation restoration and inverse text normalization, across long context. However, challenges remain regarding stability, controllability, completeness, and fluency. To mitigate these issues, this paper proposes the Chain of Correction (CoC), which uses a multi-turn chat format to correct errors segment by segment, guided by pre-recognized text and full-text context for better semantic understanding. Utilizing the open-sourced ChFT dataset, we fine-tune a pre-trained LLM to evaluate CoC's performance. Experiments show that CoC significantly outperforms baseline and benchmark systems in correcting full-text ASR outputs. We also analyze correction thresholds to balance under-correction and over-rephrasing, extrapolate CoC on extra-long ASR outputs, and explore using other types of information to guide error correction.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Chart-to-Code Generation in MLLM via Dual Preference-Guided Refinement</title>
<link>https://arxiv.org/abs/2504.02906</link>
<guid>https://arxiv.org/abs/2504.02906</guid>
<content:encoded><![CDATA[
arXiv:2504.02906v2 Announce Type: replace 
Abstract: Translating chart images into executable plotting scripts-referred to as the chart-to-code generation task-requires Multimodal Large Language Models (MLLMs) to perform fine-grained visual parsing, precise code synthesis, and robust cross-modal reasoning. However, this task is inherently under-constrained: multiple valid code implementations can produce the same visual chart, and evaluation must consider both code correctness and visual fidelity across diverse dimensions. This makes it difficult to learn accurate and generalizable mappings through standard supervised fine-tuning. To address these challenges, we propose a dual preference-guided refinement framework that combines a feedback-driven, dual-modality reward mechanism with iterative preference learning. Our approach introduces a structured variant generation strategy and a visual reward model to efficiently produce high-quality, aspect-aware preference pairs-making preference collection scalable and supervision more targeted. These preferences are used in an offline reinforcement learning setup to optimize the model toward multi-dimensional fidelity. Experimental results show that our framework significantly enhances the performance of general-purpose open-source MLLMs, enabling them to generate high-quality plotting code that rivals specialized chart-centric models and even some proprietary systems. The code and datasets are publicly available at https://github.com/Zhihan72/Chart2Code.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers</title>
<link>https://arxiv.org/abs/2504.19254</link>
<guid>https://arxiv.org/abs/2504.19254</guid>
<content:encoded><![CDATA[
arXiv:2504.19254v3 Announce Type: replace 
Abstract: Hallucinations are a persistent problem with Large Language Models (LLMs). As these models become increasingly used in high-stakes domains, such as healthcare and finance, the need for effective hallucination detection is crucial. To this end, we outline a versatile framework for zero-resource hallucination detection that practitioners can apply to real-world use cases. To achieve this, we adapt a variety of existing uncertainty quantification (UQ) techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge, transforming them as necessary into standardized response-level confidence scores ranging from 0 to 1. To enhance flexibility, we propose a tunable ensemble approach that incorporates any combination of the individual confidence scores. This approach enables practitioners to optimize the ensemble for a specific use case for improved performance. To streamline implementation, the full suite of scorers is offered in this paper's companion Python toolkit, UQLM. To evaluate the performance of the various scorers, we conduct an extensive set of experiments using several LLM question-answering benchmarks. We find that our tunable ensemble typically surpasses its individual components and outperforms existing hallucination detection methods. Our results demonstrate the benefits of customized hallucination detection strategies for improving the accuracy and reliability of LLMs.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback</title>
<link>https://arxiv.org/abs/2506.03106</link>
<guid>https://arxiv.org/abs/2506.03106</guid>
<content:encoded><![CDATA[
arXiv:2506.03106v5 Announce Type: replace 
Abstract: Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of spontaneous self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided self-refinements simultaneously while maintaining exploration. Additionally, we employ a shaping function to amplify learning from correct, especially unfamiliar, refinements and penalize incorrect ones. Extensive experiments with Qwen2.5-7B-Base, Qwen2.5-Math-7B-Base, and Qwen3-8B demonstrate that Critique-GRPO consistently outperforms supervised learning and RL-based fine-tuning methods across eight challenging mathematical, STEM, and general reasoning tasks. Specifically, Critique-GRPO improves average pass@1 scores across all compared methods by approximately +4.4% on Qwen2.5-7B-Base and +3.8% on Qwen3-8B. Notably, Critique-GRPO enables effective self-improvement through self-critiquing, achieving significant gains over GRPO, e.g., +16.7% pass@1 improvement on AIME 2024.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customizing Speech Recognition Model with Large Language Model Feedback</title>
<link>https://arxiv.org/abs/2506.11091</link>
<guid>https://arxiv.org/abs/2506.11091</guid>
<content:encoded><![CDATA[
arXiv:2506.11091v2 Announce Type: replace 
Abstract: Automatic speech recognition (ASR) systems have achieved strong performance on general transcription tasks. However, they continue to struggle with recognizing rare named entities and adapting to domain mismatches. In contrast, large language models (LLMs), trained on massive internet-scale datasets, are often more effective across a wide range of domains. In this work, we propose a reinforcement learning based approach for unsupervised domain adaptation, leveraging unlabeled data to enhance transcription quality, particularly the named entities affected by domain mismatch, through feedback from a LLM. Given contextual information, our framework employs a LLM as the reward model to score the hypotheses from the ASR model. These scores serve as reward signals to fine-tune the ASR model via reinforcement learning. Our method achieves a 21\% improvement on entity word error rate over conventional self-training methods.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Temporal Sensitivity of Large Language Model for Recommendation with Counterfactual Tuning</title>
<link>https://arxiv.org/abs/2507.03047</link>
<guid>https://arxiv.org/abs/2507.03047</guid>
<content:encoded><![CDATA[
arXiv:2507.03047v2 Announce Type: replace 
Abstract: Recent advances have applied large language models (LLMs) to sequential recommendation, leveraging their pre-training knowledge and reasoning capabilities to provide more personalized user experiences. However, existing LLM-based methods fail to sufficiently leverage the rich temporal information inherent in users' historical interaction sequences, stemming from fundamental architectural constraints: LLMs process information through self-attention mechanisms that lack inherent sequence ordering and rely on position embeddings designed primarily for natural language rather than user interaction sequences. This limitation significantly impairs their ability to capture the evolution of user preferences over time and predict future interests accurately.
  To address this critical gap, we propose \underline{C}ounterfactual \underline{E}nhanced \underline{T}emporal Framework for LLM-Based \underline{Rec}ommendation (CETRec). CETRec is grounded in causal inference principles, which allow it to isolate and measure the specific impact of temporal information on recommendation outcomes. Combined with our counterfactual tuning task derived from causal analysis, CETRec effectively enhances LLMs' awareness of both absolute order (how recently items were interacted with) and relative order (the sequential relationships between items). Extensive experiments on real-world datasets demonstrate the effectiveness of our CETRec. Our code is available at https://anonymous.4open.science/r/CETRec-B9CE/.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Each to Their Own: Exploring the Optimal Embedding in RAG</title>
<link>https://arxiv.org/abs/2507.17442</link>
<guid>https://arxiv.org/abs/2507.17442</guid>
<content:encoded><![CDATA[
arXiv:2507.17442v2 Announce Type: replace 
Abstract: Recently, as Large Language Models (LLMs) have fundamentally impacted various fields, the methods for incorporating up-to-date information into LLMs or adding external knowledge to construct domain-specific models have garnered wide attention. Retrieval-Augmented Generation (RAG), serving as an inference-time scaling method, is notable for its low cost and minimal effort for parameter tuning. However, due to heterogeneous training data and model architecture, the variant embedding models used in RAG exhibit different benefits across various areas, often leading to different similarity calculation results and, consequently, varying response quality from LLMs. To address this problem, we propose and examine two approaches to enhance RAG by combining the benefits of multiple embedding models, named Mixture-Embedding RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects retrievals from multiple embedding models based on standardized similarity; however, it does not outperform vanilla RAG. In contrast, Confident RAG generates responses multiple times using different embedding models and then selects the responses with the highest confidence level, demonstrating average improvements of approximately 10% and 5% over vanilla LLMs and RAG, respectively. The consistent results across different LLMs and embedding models indicate that Confident RAG is an efficient plug-and-play approach for various domains. We will release our code upon publication.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is neural semantic parsing good at ellipsis resolution, or isn't it?</title>
<link>https://arxiv.org/abs/2508.00121</link>
<guid>https://arxiv.org/abs/2508.00121</guid>
<content:encoded><![CDATA[
arXiv:2508.00121v3 Announce Type: replace 
Abstract: Neural semantic parsers have shown good overall performance for a variety of linguistic phenomena, reaching semantic matching scores of more than 90%. But how do such parsers perform on strongly context-sensitive phenomena, where large pieces of semantic information need to be duplicated to form a meaningful semantic representation? A case in point is English verb phrase ellipsis, a construct where entire verb phrases can be abbreviated by a single auxiliary verb. Are the otherwise known as powerful semantic parsers able to deal with ellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with their fully resolved meaning representation and used this as a challenge set for a large battery of neural semantic parsers. Although these parsers performed very well on the standard test set, they failed in the instances with ellipsis. Data augmentation helped improve the parsing results. The reason for the difficulty of parsing elided phrases is not that copying semantic material is hard, but that usually occur in linguistically complicated contexts causing most of the parsing errors.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking</title>
<link>https://arxiv.org/abs/2403.09717</link>
<guid>https://arxiv.org/abs/2403.09717</guid>
<content:encoded><![CDATA[
arXiv:2403.09717v2 Announce Type: replace-cross 
Abstract: Depression-diagnosis-oriented chat aims to guide patients in self-expression to collect key symptoms for depression detection. Recent work focuses on combining task-oriented dialogue and chitchat to simulate the interview-based depression diagnosis. Whereas, these methods can not well capture the changing information, feelings, or symptoms of the patient during dialogues. Moreover, no explicit framework has been explored to guide the dialogue, which results in some useless communications that affect the experience. In this paper, we propose to integrate Psychological State Tracking (POST) within the large language model (LLM) to explicitly guide depression-diagnosis-oriented chat. Specifically, the state is adapted from a psychological theoretical model, which consists of four components, namely Stage, Information, Summary and Next. We fine-tune an LLM model to generate the dynamic psychological state, which is further used to assist response generation at each turn to simulate the psychiatrist. Experimental results on the existing benchmark show that our proposed method boosts the performance of all subtasks in depression-diagnosis-oriented chat.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters</title>
<link>https://arxiv.org/abs/2405.17604</link>
<guid>https://arxiv.org/abs/2405.17604</guid>
<content:encoded><![CDATA[
arXiv:2405.17604v3 Announce Type: replace-cross 
Abstract: The growth of large language models underscores the need for parameter-efficient fine-tuning. Despite its popularity, LoRA encounters storage and computational challenges when deploying multiple task- or user-specific modules. To address this, we introduce LoRA-XS, a novel fine-tuning method backed by a theoretical derivation. LoRA-XS drastically reduces trainable parameters by incorporating a small, trainable weight matrix between frozen low-rank matrices derived from the Singular Value Decomposition of pre-trained weights. This design enables LoRA-XS to reduce storage requirements by over 100x in 7B models compared to LoRA. Additionally, unlike other methods, LoRA-XS imposes no lower bound on trainable parameters - it can scale from a single parameter per module to arbitrarily large values, adapting to any storage or computational constraint. Evaluations on GLUE, GSM8K, MATH, and commonsense reasoning benchmarks across different model scales reveal that LoRA-XS consistently outperforms or matches LoRA and VeRA in accuracy, offering unmatched parameter efficiency. Our ablation studies highlight the significance of singular vectors in transformer weights, establishing LoRA-XS as a powerful, storage-efficient solution for scaling and personalizing large language models.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coupling without Communication and Drafter-Invariant Speculative Decoding</title>
<link>https://arxiv.org/abs/2408.07978</link>
<guid>https://arxiv.org/abs/2408.07978</guid>
<content:encoded><![CDATA[
arXiv:2408.07978v4 Announce Type: replace-cross 
Abstract: Suppose Alice has a distribution $P$ and Bob has a distribution $Q$. Alice wants to draw a sample $a\sim P$ and Bob a sample $b \sim Q$ such that $a = b$ with as high of probability as possible. It is well-known that, by sampling from an optimal coupling between the distributions, Alice and Bob can achieve $\Pr[a = b] = 1 - D_{TV}(P,Q)$, where $D_{TV}(P,Q)$ is the total variation distance between $P$ and $Q$. What if Alice and Bob must solve this same problem \emph{without communicating at all?} Perhaps surprisingly, with access to public randomness, they can still achieve $\Pr[a = b] \geq \frac{1 - D_{TV}(P,Q)}{1 + D_{TV}(P,Q)} \geq 1-2D_{TV}(P,Q)$ using a simple protocol based on the Weighted MinHash algorithm. This bound was shown to be optimal in the worst-case by [Bavarian et al., 2020]. In this work, we revisit the communication-free coupling problem. We provide a simpler proof of the optimality result from [Bavarian et al., 2020]. We show that, while the worst-case success probability of Weighted MinHash cannot be improved, an equally simple protocol based on Gumbel sampling offers a Pareto improvement: for every pair of distributions $P, Q$, Gumbel sampling achieves an equal or higher value of $\Pr[a = b]$ than Weighted MinHash. Importantly, this improvement translates to practice. We demonstrate an application of communication-free coupling to \emph{speculative decoding}, a recent method for accelerating autoregressive large language models [Leviathan, Kalman, Matias, ICML 2023]. We show that communication-free protocols can be used to contruct \emph{\CSD{}} schemes, which have the desirable property that their output is fixed given a fixed random seed, regardless of what drafter is used for speculation. In experiments on a language generation task, Gumbel sampling outperforms Weighted MinHash. Code is available at https://github.com/majid-daliri/DISD.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReSpark: Leveraging Previous Data Reports as References to Generate New Reports with LLMs</title>
<link>https://arxiv.org/abs/2502.02329</link>
<guid>https://arxiv.org/abs/2502.02329</guid>
<content:encoded><![CDATA[
arXiv:2502.02329v2 Announce Type: replace-cross 
Abstract: Creating data reports is a labor-intensive task involving iterative data exploration, insight extraction, and narrative construction. A key challenge lies in composing the analysis logic-from defining objectives and transforming data to identifying and communicating insights. Manually crafting this logic can be cognitively demanding. While experienced analysts often reuse scripts from past projects, finding a perfect match for a new dataset is rare. Even when similar analyses are available online, they usually share only results or visualizations, not the underlying code, making reuse difficult. To address this, we present ReSpark, a system that leverages large language models (LLMs) to reverse-engineer analysis logic from existing reports and adapt it to new datasets. By generating draft analysis steps, ReSpark provides a warm start for users. It also supports interactive refinement, allowing users to inspect intermediate outputs, insert objectives, and revise content. We evaluate ReSpark through comparative and user studies, demonstrating its effectiveness in lowering the barrier to generating data reports without relying on existing analysis code.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation</title>
<link>https://arxiv.org/abs/2505.14351</link>
<guid>https://arxiv.org/abs/2505.14351</guid>
<content:encoded><![CDATA[
arXiv:2505.14351v3 Announce Type: replace-cross 
Abstract: Tibetan is a low-resource language with minimal parallel speech corpora spanning its three major dialects-\"U-Tsang, Amdo, and Kham-limiting progress in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot, multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel dialectal speech from limited reference audio and explicit dialect labels. Our method features a novel speaker-dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects while preserving speaker identity. Extensive objective and subjective evaluations demonstrate that FMSD-TTS significantly outperforms baselines in both dialectal expressiveness and speaker similarity. We further validate the quality and utility of the synthesized speech through a challenging speech-to-speech dialect conversion task. Our contributions include: (1) a novel few-shot TTS system tailored for Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source evaluation toolkit for standardized assessment of speaker similarity, dialect consistency, and audio quality.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning</title>
<link>https://arxiv.org/abs/2507.07060</link>
<guid>https://arxiv.org/abs/2507.07060</guid>
<content:encoded><![CDATA[
arXiv:2507.07060v2 Announce Type: replace-cross 
Abstract: The synthesis of complex natural products remains one of the grand challenges of organic chemistry. We present DeepRetro, a major advancement in computational retrosynthesis that enables the discovery of viable synthetic routes for complex molecules typically considered beyond the reach of existing retrosynthetic methods. DeepRetro is a novel, open-source framework that tightly integrates large language models (LLMs), traditional retrosynthetic engines, and expert human feedback in an iterative design loop. Prior approaches rely solely on template-based methods or unconstrained LLM outputs. In contrast, DeepRetro combines the precision of template-based methods with the generative flexibility of LLMs, controlled by rigorous chemical validity checks and enhanced by recursive refinement. This hybrid system dynamically explores and revises synthetic pathways, guided by both algorithmic checks and expert chemist feedback through an interactive user interface. While DeepRetro achieves strong performance on standard retrosynthesis benchmarks, its true strength lies in its ability to propose novel, viable pathways to highly complex natural products-targets that have historically eluded automated planning. Through detailed case studies, we illustrate how this approach enables new routes for total synthesis and facilitates human-machine collaboration in organic chemistry. Beyond retrosynthesis, DeepRetro represents a working model for how to leverage LLMs in scientific discovery. We provide a transparent account of the system's design, algorithms, and human-feedback loop, enabling broad adaptation across scientific domains. By releasing DeepRetro as an open-source tool, we aim to empower chemists to tackle increasingly ambitious synthetic targets, accelerating progress in drug discovery, materials design, and beyond.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search</title>
<link>https://arxiv.org/abs/2508.02091</link>
<guid>https://arxiv.org/abs/2508.02091</guid>
<content:encoded><![CDATA[
arXiv:2508.02091v2 Announce Type: replace-cross 
Abstract: Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement. Code can be found at https://github.com/deepreinforce-ai/CRINN
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2508.05064</link>
<guid>https://arxiv.org/abs/2508.05064</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, 3D scene representation, Large Language Models, language embeddings, text-conditioned generation<br />
Summary:<br />
Gaussian Splatting has become a widely used technique for real-time 3D scene representation, offering a more efficient alternative to Neural Radiance Fields. By incorporating Large Language Models and language embeddings, new possibilities for text-guided generation and semantic scene understanding have been unlocked. This survey reviews the current research at the intersection of language guidance and Gaussian Splatting, discussing theoretical foundations, integration strategies, and practical applications. Key challenges such as computational constraints, generalizability issues, and the lack of annotated 3D Gaussian data are identified. Future directions for improving language-guided 3D scene understanding using Gaussian Splatting are outlined. <br /> <div>
arXiv:2508.05064v2 Announce Type: replace-cross 
Abstract: Gaussian Splatting has rapidly emerged as a transformative technique for real-time 3D scene representation, offering a highly efficient and expressive alternative to Neural Radiance Fields (NeRF). Its ability to render complex scenes with high fidelity has enabled progress across domains such as scene reconstruction, robotics, and interactive content creation. More recently, the integration of Large Language Models (LLMs) and language embeddings into Gaussian Splatting pipelines has opened new possibilities for text-conditioned generation, editing, and semantic scene understanding. Despite these advances, a comprehensive overview of this emerging intersection has been lacking. This survey presents a structured review of current research efforts that combine language guidance with 3D Gaussian Splatting, detailing theoretical foundations, integration strategies, and real-world use cases. We highlight key limitations such as computational bottlenecks, generalizability, and the scarcity of semantically annotated 3D Gaussian data and outline open challenges and future directions for advancing language-guided 3D scene understanding using Gaussian Splatting.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Play in the Newsroom: Actor-Based Filtering Gender Discrimination in Text Corpora</title>
<link>https://arxiv.org/abs/2508.13169</link>
<guid>https://arxiv.org/abs/2508.13169</guid>
<content:encoded><![CDATA[
<div> pipeline, gender discrimination, large language models, fairness analysis, corpus analysis

Summary:<br />
1. Large language models play a significant role in digital communication but often exhibit gender imbalances due to biased training data.
2. An extended actor-level pipeline is introduced to detect and address gender discrimination in text corpora.
3. New actor-level metrics are developed to identify disparities in sentiment, syntactic agency, and quotation styles.
4. The pipeline enables diagnostic corpus analysis and exclusion-based balancing to create fairer corpora.
5. Applying the approach to the taz2024full corpus of German newspaper articles showed improved gender balance in linguistic dimensions.
6. While surface-level bias can be reduced through filtering and rebalancing, deeper forms of bias remain, especially in sentiment and framing.
7. Tools and reports are released to support further research in discourse-based fairness auditing and equitable corpus construction.<br /><br />Summary: <div>
arXiv:2508.13169v1 Announce Type: new 
Abstract: Large language models are increasingly shaping digital communication, yet their outputs often reflect structural gender imbalances that originate from their training data. This paper presents an extended actor-level pipeline for detecting and mitigating gender discrimination in large-scale text corpora. Building on prior work in discourse-aware fairness analysis, we introduce new actor-level metrics that capture asymmetries in sentiment, syntactic agency, and quotation styles. The pipeline supports both diagnostic corpus analysis and exclusion-based balancing, enabling the construction of fairer corpora. We apply our approach to the taz2024full corpus of German newspaper articles from 1980 to 2024, demonstrating substantial improvements in gender balance across multiple linguistic dimensions. Our results show that while surface-level asymmetries can be mitigated through filtering and rebalancing, subtler forms of bias persist, particularly in sentiment and framing. We release the tools and reports to support further research in discourse-based fairness auditing and equitable corpus construction.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents</title>
<link>https://arxiv.org/abs/2508.13186</link>
<guid>https://arxiv.org/abs/2508.13186</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, multimodal retrieval, reasoning capabilities, benchmark, web browsing

Summary: 
The article introduces MM-BrowseComp, a new benchmark designed to assess AI agents' multimodal retrieval and reasoning abilities in web browsing. Unlike existing benchmarks that focus mostly on textual information, MM-BrowseComp includes images in prompts and requires agents to retrieve information from images or videos on webpages. Current models, including the top-performing OpenAI o3, only achieve 29.02% accuracy on this benchmark, indicating the suboptimal multimodal capabilities and lack of native multimodal reasoning in existing models. MM-BrowseComp comprises 224 challenging questions with verified checklists for fine-grained analysis of multimodal dependencies and reasoning paths. This evaluation highlights the need for advancements in multimodal retrieval and reasoning capabilities in AI agents for more effective web browsing. 

<br /><br />Summary: <div>
arXiv:2508.13186v1 Announce Type: new 
Abstract: AI agents with advanced reasoning and tool use capabilities have demonstrated impressive performance in web browsing for deep search. While existing benchmarks such as BrowseComp evaluate these browsing abilities, they primarily focus on textual information, overlooking the prevalence of multimodal content. To bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising 224 challenging, hand-crafted questions specifically designed to assess agents' multimodal retrieval and reasoning capabilities. These questions often incorporate images in prompts, and crucial information encountered during the search and reasoning process may also be embedded within images or videos on webpages. Consequently, methods relying solely on text prove insufficient for our benchmark. Additionally, we provide a verified checklist for each question, enabling fine-grained analysis of multimodal dependencies and reasoning paths. Our comprehensive evaluation of state-of-the-art models on MM-BrowseComp reveals that even top models like OpenAI o3 with tools achieve only 29.02\% accuracy, highlighting the suboptimal multimodal capabilities and lack of native multimodal reasoning in current models.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Latency Bottlenecks in On-Device Speech Translation: A Cascaded Approach with Alignment-Based Streaming MT</title>
<link>https://arxiv.org/abs/2508.13358</link>
<guid>https://arxiv.org/abs/2508.13358</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Speech Recognition, Machine Translation, real-time streaming, simultaneous translation, on-device

Summary:
This paper addresses the challenges of integrating Automatic Speech Recognition (ASR) and Machine Translation (MT) for real-time, on-device streaming speech translation. It introduces a simultaneous translation approach that balances translation quality and latency, improving efficiency. The study explores the integration of ASR and MT, utilizing linguistic cues from ASR for context management. Efficient beam-search pruning techniques such as time-out and forced finalization are employed to maintain real-time translation. Application of these techniques to an on-device bilingual conversational speech translation showcases superior performance in terms of latency and quality compared to baseline systems. The proposed approach significantly reduces the quality gap with non-streaming translation systems, enhancing the accuracy and efficiency of real-time speech translation. 

<br /><br />Summary: <div>
arXiv:2508.13358v1 Announce Type: new 
Abstract: This paper tackles several challenges that arise when integrating Automatic Speech Recognition (ASR) and Machine Translation (MT) for real-time, on-device streaming speech translation. Although state-of-the-art ASR systems based on Recurrent Neural Network Transducers (RNN-T) can perform real-time transcription, achieving streaming translation in real-time remains a significant challenge. To address this issue, we propose a simultaneous translation approach that effectively balances translation quality and latency. We also investigate efficient integration of ASR and MT, leveraging linguistic cues generated by the ASR system to manage context and utilizing efficient beam-search pruning techniques such as time-out and forced finalization to maintain system's real-time factor. We apply our approach to an on-device bilingual conversational speech translation and demonstrate that our techniques outperform baselines in terms of latency and quality. Notably, our technique narrows the quality gap with non-streaming translation systems, paving the way for more accurate and efficient real-time speech translation.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity Detection</title>
<link>https://arxiv.org/abs/2508.13365</link>
<guid>https://arxiv.org/abs/2508.13365</guid>
<content:encoded><![CDATA[
<div> reasoning models, Large Language Models (LLMs), idiomaticity detection, model size, performance

Summary: 
The study explores the impact of reasoning models on the performance of Large Language Models (LLMs) in idiomaticity detection. Different model sizes ranging from 1.5B to 70B parameters were evaluated using open-source representative models. The findings indicate that smaller models benefit from chain-of-thought reasoning but do not reach the performance levels of base models. In contrast, larger models (14B, 32B, and 70B) show modest improvements in idiomaticity detection. Larger models demonstrate a better understanding of idiomatic expressions and produce accurate definitions, while smaller models often fail to capture the true meaning. Providing definitions in the prompts of smaller models can enhance performance in certain cases. The study highlights the importance of reasoning capabilities and model size in enhancing idiomaticity detection accuracy. 

<br /><br />Summary: <div>
arXiv:2508.13365v1 Announce Type: new 
Abstract: The recent trend towards utilisation of reasoning models has improved the performance of Large Language Models (LLMs) across many tasks which involve logical steps. One linguistic task that could benefit from this framing is idiomaticity detection, as a potentially idiomatic expression must first be understood before it can be disambiguated and serves as a basis for reasoning. In this paper, we explore how reasoning capabilities in LLMs affect idiomaticity detection performance and examine the effect of model size. We evaluate, as open source representative models, the suite of DeepSeek-R1 distillation models ranging from 1.5B to 70B parameters across four idiomaticity detection datasets. We find the effect of reasoning to be smaller and more varied than expected. For smaller models, producing chain-of-thought (CoT) reasoning increases performance from Math-tuned intermediate models, but not to the levels of the base models, whereas larger models (14B, 32B, and 70B) show modest improvements. Our in-depth analyses reveal that larger models demonstrate good understanding of idiomaticity, successfully producing accurate definitions of expressions, while smaller models often fail to output the actual meaning. For this reason, we also experiment with providing definitions in the prompts of smaller models, which we show can improve performance in some cases.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whispering Context: Distilling Syntax and Semantics for Long Speech Transcripts</title>
<link>https://arxiv.org/abs/2508.13376</link>
<guid>https://arxiv.org/abs/2508.13376</guid>
<content:encoded><![CDATA[
<div> Keywords: ASR, Named Entity Recognition, contextual knowledge, Whisper, LLaMA

Summary:
Our study addresses the challenge of maintaining syntactic and semantic accuracy in long audio transcripts for ASR systems. We propose a novel approach that leverages contextual knowledge from LLaMA models to enhance ASR performance, particularly in tasks like Named Entity Recognition (NER), capitalization, and punctuation. Our method involves token-level distillation using optimal transport and representation loss minimization between Whisper and LLaMA sentence embeddings. By blending syntax and semantics, we achieve significant improvements in Word Error Rate (WER), NER accuracy, capitalization success, and punctuation accuracy on the Spoken Wikipedia dataset. Introducing new NER metrics and emphasizing semantics-aware ASR, our work showcases the importance of integrating linguistic context into transcription for robust and context-aware ASR in longform speech.<br /><br />Summary: <div>
arXiv:2508.13376v1 Announce Type: new 
Abstract: ASR systems often struggle with maintaining syntactic and semantic accuracy in long audio transcripts, impacting tasks like Named Entity Recognition (NER), capitalization, and punctuation. We propose a novel approach that enhances ASR by distilling contextual knowledge from LLaMA models into Whisper. Our method uses two strategies: (1) token level distillation with optimal transport to align dimensions and sequence lengths, and (2) representation loss minimization between sentence embeddings of Whisper and LLaMA, blending syntax and semantics. Evaluations on the Spoken Wikipedia dataset, a benchmark with long audios and rich entities demonstrate significant improvements in Word Error Rate (WER), NER, capitalization, and punctuation success. By introducing novel NER metrics and exploring semantics aware ASR, our work highlights the value of integrating linguistic context into transcription, setting a foundation for robust, context-aware ASR in longform speech.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis</title>
<link>https://arxiv.org/abs/2508.13382</link>
<guid>https://arxiv.org/abs/2508.13382</guid>
<content:encoded><![CDATA[
<div> Keywords: Datarus, language model, virtual data analyst, graduate-level problem solver, trajectory-centric synthetic data generator

Summary: 
Datarus-R1-14B is a language model fine-tuned from Qwen 2.5-14B-Instruct to serve as a virtual data analyst and problem solver. Trained on full analytical trajectories, it captures reasoning steps, code execution, error correction, and final conclusions across quantitative domains. The training pipeline includes a synthetic data generator, a dual-reward framework, and a memory-optimized implementation of GRPO. Datarus features a trajectory-centric approach with a dual reasoning interface in agentic and reflection modes. It exhibits an "AHA-moment" pattern on postgraduate-level problems, showing hypothesis sketching, revision, and convergence. Outperforming similar size models, it achieves higher accuracy on benchmarks like AIME 2024/2025 and LiveCodeBench while emitting fewer tokens per solution. <div>
arXiv:2508.13382v1 Announce Type: new 
Abstract: We present Datarus-R1-14B, a 14 B-parameter open-weights language model fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and graduate-level problem solver. Datarus is trained not on isolated question-answer pairs but on full analytical trajectories including reasoning steps, code execution, error traces, self-corrections, and final conclusions, all captured in a ReAct-style notebook format spanning finance, medicine, numerical analysis, and other quantitative domains. Our training pipeline combines (i) a trajectory-centric synthetic data generator that yielded 144 000 tagged notebook episodes, (ii) a dual-reward framework blending a lightweight tag-based structural signal with a Hierarchical Reward Model (HRM) that scores both single-step soundness and end-to-end coherence, and (iii) a memory-optimized implementation of Group Relative Policy Optimization (GRPO) featuring KV-cache reuse, sequential generation, and reference-model sharding. A cosine curriculum smoothly shifts emphasis from structural fidelity to semantic depth, reducing the format collapse and verbosity that often plague RL-aligned LLMs. A central design choice in Datarus is it dual reasoning interface. In agentic mode the model produces ReAct-tagged steps that invoke Python tools to execute real code; in reflection mode it outputs compact Chain-of-Thought (CoT) traces delimited by  and  tags. On demanding postgraduate-level problems, Datarus exhibits an "AHA-moment" pattern: it sketches hypotheses, revises them once or twice, and converges avoiding the circular, token-inflating loops common to contemporary systems. Across standard public benchmarks Datarus surpasses similar size models and even reaches the level of larger reasoning models such as QwQ-32B achieving up to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting 18-49% fewer tokens per solution.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models</title>
<link>https://arxiv.org/abs/2508.13426</link>
<guid>https://arxiv.org/abs/2508.13426</guid>
<content:encoded><![CDATA[
<div> fine-tuning, language models, culture, word-association norms, cognitive<br />
<br />
Summary: 
The study introduces a cost-efficient method to improve cultural alignment in large language models (LLMs). By fine-tuning LLMs on native speakers' word-association norms, which encode implicit cultural schemas, significant improvements in precision, concreteness, valence, and arousal are achieved. The fine-tuned models show a shift in answer distributions towards the target culture on World-Values-Survey questions and demonstrate reduced bias and improved alignment on a high-tension subset. The results indicate that leveraging culture-grounded associations can enhance cultural alignment in LLMs without requiring retraining. This approach highlights the potential of incorporating human cognition in improving cultural alignment in AI models. <div>
arXiv:2508.13426v1 Announce Type: new 
Abstract: As large language models (LLMs) increasingly mediate cross-cultural communication, their behavior still reflects the distributional bias of the languages and viewpoints that are over-represented in their pre-training corpora. Yet, it remains a challenge to model and align culture due to limited cultural knowledge and a lack of exploration into effective learning approaches. We introduce a cost-efficient, cognitively grounded remedy: parameter-efficient fine-tuning on native speakers' free word-association norms, which encode implicit cultural schemas. Leveraging English-US and Mandarin associations from the Small-World-of-Words project, we adapt Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning (SFT) and PPO-based preference optimization. SFT boosts held-out association Precision at 5 by 16-20% in English and 43-165% in Mandarin, lifts median concreteness by +0.20, and attains human-level valence and arousal. These lexical gains transfer: on World-Values-Survey questions, fine-tuned models shift answer distributions toward the target culture, and on a 50-item high-tension subset, Qwen's Chinese-aligned responses double while Llama's US bias drops by one-third. Our 7-8B models rival or beat vanilla 70B baselines, showing that a few million culture-grounded associations can instill value alignment without costly retraining. Our work highlights both the promise and the need for future research grounded in human cognition in improving cultural alignment in AI models.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs</title>
<link>https://arxiv.org/abs/2508.13514</link>
<guid>https://arxiv.org/abs/2508.13514</guid>
<content:encoded><![CDATA[
<div> Keywords: Interactive medical questioning, Large Language Models, Reinforcement learning, Shapley Information Gain, Clinical utility

Summary:<br /><br />ProMed introduces a proactive paradigm for medical Large Language Models by incorporating reinforcement learning and the Shapley Information Gain (SIG) reward. This framework enables the model to ask clinically valuable questions before making decisions, improving diagnostic accuracy in interactive settings. The SIG reward quantifies the informational value of questions by combining new information acquisition and contextual importance using Shapley values. ProMed's training pipeline includes SIG-Guided Model Initialization and SIG-Augmented Policy Optimization, enhancing model performance through targeted optimization. Experimental results on medical benchmarks demonstrate ProMed outperforming state-of-the-art methods by 6.29% on average and achieving a 54.45% improvement over reactive paradigms. Moreover, ProMed exhibits robust generalization to out-of-domain cases, showcasing its potential for enhancing clinical consultations. <div>
arXiv:2508.13514v1 Announce Type: new 
Abstract: Interactive medical questioning is essential in real-world clinical consultations, where physicians must actively gather information from patients. While medical Large Language Models (LLMs) have shown impressive capabilities in static medical question answering, they predominantly operate under a reactive paradigm: generating answers directly without seeking additional information, which risks incorrect diagnoses in such interactive settings. To address this limitation, we propose ProMed, a reinforcement learning (RL) framework that transitions medical LLMs toward a proactive paradigm, equipping them with the ability to ask clinically valuable questions before decision-making. At the core of ProMed is the Shapley Information Gain (SIG) reward, which quantifies the clinical utility of each question by combining the amount of newly acquired information with its contextual importance, estimated via Shapley values. We integrate SIG into a two-stage training pipeline: (1) SIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to construct high-reward interaction trajectories to supervise the model, and (2) SIG-Augmented Policy Optimization, which integrates SIG and enhances RL with a novel SIG-guided Reward Distribution Mechanism that assigns higher rewards to informative questions for targeted optimization. Extensive experiments on two newly curated partial-information medical benchmarks demonstrate that ProMed significantly outperforms state-of-the-art methods by an average of 6.29% and delivers a 54.45% gain over the reactive paradigm, while also generalizing robustly to out-of-domain cases.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation</title>
<link>https://arxiv.org/abs/2508.13525</link>
<guid>https://arxiv.org/abs/2508.13525</guid>
<content:encoded><![CDATA[
<div> Arabic, Large language models, Saudi dialects, LoRA-tune, Dialect-Token.
Summary:<br /><br />Large language models for Arabic are primarily focused on Modern Standard Arabic, lacking support for Saudi dialects like Najdi and Hijazi. This study introduces the LoRA-tune ALLaM-7B-Instruct-preview model, trained on a curated Saudi Dialect Instruction dataset. Two training variants were explored: Dialect-Token, which includes an explicit dialect tag in the instruction, and No-Token, which omits the tag. The Dialect-Token model showed improved control over dialect generation, reducing MSA leakage and improving fidelity. Evaluation metrics confirmed the model's effectiveness in capturing Saudi dialect variations. The LoRA variants outperformed generic instruction models in dialect control and fidelity. The dataset and model weights/adapters are not released, but the training/evaluation/inference code and a detailed datasheet are provided for independent verification. <div>
arXiv:2508.13525v1 Announce Type: new 
Abstract: Large language models (LLMs) for Arabic are still dominated by Modern Standard Arabic (MSA), with limited support for Saudi dialects such as Najdi and Hijazi. This underrepresentation hinders their ability to capture authentic dialectal variation. Using a privately curated Saudi Dialect Instruction dataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50 split), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation model developed in Saudi Arabia, for Saudi dialect generation. We investigate two variants: (i) Dialect-Token training, which prepends an explicit dialect tag to the instruction, and (ii) No-Token training, which omits the tag at formatting time. Evaluation on a held-out test set combines an external dialect classifier with text fidelity metrics (chrF++ and BERTScore) and diversity measures. The Dialect-Token model achieves the best control, raising the Saudi rate from 47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity also improves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform strong generic instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct, Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control and fidelity, while avoiding metadata-tag echoing that these baselines frequently exhibit. We do not release the dataset or any model weights/adapters; instead, we release training/evaluation/inference code and a detailed datasheet (schema and aggregate statistics) to support independent verification.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MATA (m\=ata): Mindful Assessment of the Telugu Abilities of Large Language Models</title>
<link>https://arxiv.org/abs/2508.13526</link>
<guid>https://arxiv.org/abs/2508.13526</guid>
<content:encoded><![CDATA[
<div> Keywords: MATA, Large Language Models, Telugu language, evaluation dataset, multiple-choice questions

Summary:
The paper introduces MATA, a dataset for evaluating Large Language Models (LLMs) in Telugu language, consisting of 729 carefully crafted multiple-choice and open-ended questions covering various linguistic aspects. 11 LLMs were assessed on the dataset, with a detailed analysis of their performance. The study reveals that LLMs often rely on surface-level cues like answer position and distractor patterns for multiple-choice questions. Additionally, the comparison between LLM evaluation and human evaluation for open-ended questions sheds light on the reliability of LLMs in a low-resource language. The authors emphasize the importance of such thorough evaluations in identifying model limitations and guiding the development of more linguistically proficient LLMs. This work sets the groundwork for future research in Telugu Natural Language Processing. 

<br /><br />Summary: <div>
arXiv:2508.13526v1 Announce Type: new 
Abstract: In this paper, we introduce MATA, a novel evaluation dataset to assess the ability of Large Language Models (LLMs) in Telugu language, comprising 729 carefully curated multiple-choice and open-ended questions that span diverse linguistic dimensions. We evaluate 11 open-weight and closed-source LLMs on our dataset and present a fine-grained analysis of their performance. Further, we empirically show how LLMs rely on superficial heuristics such as answer position and distractor patterns for multiple-choice questions. Finally, we also compare LLM-as-a-judge evaluation with human evaluation for open-ended questions and draw some conclusions on its reliability in a low-resource language. We argue that such fine-grained evaluation is essential for understanding model limitations and can inform the development of more linguistically capable LLMs, while also serving as a foundation for future research in Telugu NLP.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressed Models are NOT Trust-equivalent to Their Large Counterparts</title>
<link>https://arxiv.org/abs/2508.13533</link>
<guid>https://arxiv.org/abs/2508.13533</guid>
<content:encoded><![CDATA[
<div> interpretability alignment, calibration similarity, trust-equivalence evaluation, deep learning models, compression

Summary:<br /><br />Deep learning models are often compressed for deployment in resource-constrained environments. However, ensuring trust-equivalence between the compressed and original large models goes beyond accuracy parity. A two-dimensional framework is proposed for evaluation, focusing on interpretability alignment and calibration similarity. Using BERT-base and its compressed variants for text classification tasks, experiments reveal low interpretability alignment and significant calibration similarity mismatch, despite similar accuracies. This indicates that compressed models may not be trust-equivalent to their large counterparts. Therefore, careful assessment is necessary before deploying compressed models as replacements for large models, emphasizing the importance of going beyond performance parity. <div>
arXiv:2508.13533v1 Announce Type: new 
Abstract: Large Deep Learning models are often compressed before being deployed in a resource-constrained environment. Can we trust the prediction of compressed models just as we trust the prediction of the original large model? Existing work has keenly studied the effect of compression on accuracy and related performance measures. However, performance parity does not guarantee trust-equivalence. We propose a two-dimensional framework for trust-equivalence evaluation. First, interpretability alignment measures whether the models base their predictions on the same input features. We use LIME and SHAP tests to measure the interpretability alignment. Second, calibration similarity measures whether the models exhibit comparable reliability in their predicted probabilities. It is assessed via ECE, MCE, Brier Score, and reliability diagrams. We conducted experiments using BERT-base as the large model and its multiple compressed variants. We focused on two text classification tasks: natural language inference and paraphrase identification. Our results reveal low interpretability alignment and significant mismatch in calibration similarity. It happens even when the accuracies are nearly identical between models. These findings show that compressed models are not trust-equivalent to their large counterparts. Deploying compressed models as a drop-in replacement for large models requires careful assessment, going beyond performance parity.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of Decoding Strategies in Medical Text Generation</title>
<link>https://arxiv.org/abs/2508.13580</link>
<guid>https://arxiv.org/abs/2508.13580</guid>
<content:encoded><![CDATA[
<div> decoding strategies, large language models, healthcare, medical tasks, evaluation<br />
<br />
Summary: <br />
The study examines the impact of different decoding strategies on the output quality of Large Language Models (LLMs) in healthcare applications. It evaluates 11 decoding strategies across five medical tasks, with deterministic strategies such as beam search outperforming stochastic ones like {\eta} and top-k sampling. Slower decoding methods are found to yield better quality results, and larger models perform better overall but have longer inference times. While medical LLMs outperform general-purpose models in some tasks, statistical analysis shows no significant performance advantage overall, indicating a greater sensitivity to decoding choice. The study also compares multiple evaluation metrics and finds variations in correlations by task, with MAUVE showing weak agreement with BERTScore and ROUGE and greater sensitivity to the decoding strategy. These findings underscore the importance of carefully selecting decoding methods in medical applications, as their influence can sometimes surpass that of model selection. <div>
arXiv:2508.13580v1 Announce Type: new 
Abstract: Large Language Models (LLMs) rely on various decoding strategies to generate text, and these choices can significantly affect output quality. In healthcare, where accuracy is critical, the impact of decoding strategies remains underexplored. We investigate this effect in five open-ended medical tasks, including translation, summarization, question answering, dialogue, and image captioning, evaluating 11 decoding strategies with medically specialized and general-purpose LLMs of different sizes. Our results show that deterministic strategies generally outperform stochastic ones: beam search achieves the highest scores, while {\eta} and top-k sampling perform worst. Slower decoding methods tend to yield better quality. Larger models achieve higher scores overall but have longer inference times and are no more robust to decoding. Surprisingly, while medical LLMs outperform general ones in two of the five tasks, statistical analysis shows no overall performance advantage and reveals greater sensitivity to decoding choice. We further compare multiple evaluation metrics and find that correlations vary by task, with MAUVE showing weak agreement with BERTScore and ROUGE, as well as greater sensitivity to the decoding strategy. These results highlight the need for careful selection of decoding methods in medical applications, as their influence can sometimes exceed that of model choice.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM</title>
<link>https://arxiv.org/abs/2508.13603</link>
<guid>https://arxiv.org/abs/2508.13603</guid>
<content:encoded><![CDATA[
<div> Keywords: Speech-LLMs, gender bias, speaker assignment, Text-to-Speech model, gender awareness <br />
<br />
Summary: 
Speech-LLMs, like text-based Large Language Models, exhibit emergent abilities and context awareness, prompting investigation into gender bias. A methodology using speaker assignment is proposed to analyze bias in Bark, a Text-to-Speech model. By constructing datasets with gender-stereotyped occupations and gendered connotations, the study evaluates Bark's default speaker assignments for textual prompts. While no systematic bias is found, Bark demonstrates gender awareness and some inclinations towards gendered associations. This research sheds light on the importance of understanding bias in Speech-LLMs and highlights the need for further exploration into the factors influencing gender representation in AI models. <div>
arXiv:2508.13603v1 Announce Type: new 
Abstract: Similar to text-based Large Language Models (LLMs), Speech-LLMs exhibit emergent abilities and context awareness. However, whether these similarities extend to gender bias remains an open question. This study proposes a methodology leveraging speaker assignment as an analytic tool for bias investigation. Unlike text-based models, which encode gendered associations implicitly, Speech-LLMs must produce a gendered voice, making speaker selection an explicit bias cue. We evaluate Bark, a Text-to-Speech (TTS) model, analyzing its default speaker assignments for textual prompts. If Bark's speaker selection systematically aligns with gendered associations, it may reveal patterns in its training data or model design. To test this, we construct two datasets: (i) Professions, containing gender-stereotyped occupations, and (ii) Gender-Colored Words, featuring gendered connotations. While Bark does not exhibit systematic bias, it demonstrates gender awareness and has some gender inclinations.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaDocVQA: Adaptive Framework for Long Document Visual Question Answering in Low-Resource Settings</title>
<link>https://arxiv.org/abs/2508.13606</link>
<guid>https://arxiv.org/abs/2508.13606</guid>
<content:encoded><![CDATA[
<div> Keywords: Document Visual Question Answering, AdaDocVQA, data augmentation, ensemble inference, low-resource languages

Summary:
AdaDocVQA is a novel framework designed to tackle the challenges faced in Document Visual Question Answering (Document VQA) tasks, particularly in low-resource environments. The framework introduces three key innovations to enhance performance: a hybrid text retrieval architecture for effective document segmentation, an intelligent data augmentation pipeline for generating high-quality question-answer pairs, and adaptive ensemble inference with dynamic configuration generation. Experimental results on Japanese document VQA benchmarks show significant improvements in accuracy, with 83.04% on Yes/No questions, 52.66% on factual questions, and 44.12% on numerical questions in JDocQA, and 59% accuracy on the LAVA dataset. Ablation studies validate the contributions of each component, establishing new state-of-the-art results in Japanese document VQA tasks and providing a scalable foundation for other low-resource languages and specialized domains.

<br /><br />Summary: <div>
arXiv:2508.13606v1 Announce Type: new 
Abstract: Document Visual Question Answering (Document VQA) faces significant challenges when processing long documents in low-resource environments due to context limitations and insufficient training data. This paper presents AdaDocVQA, a unified adaptive framework addressing these challenges through three core innovations: a hybrid text retrieval architecture for effective document segmentation, an intelligent data augmentation pipeline that automatically generates high-quality reasoning question-answer pairs with multi-level verification, and adaptive ensemble inference with dynamic configuration generation and early stopping mechanisms. Experiments on Japanese document VQA benchmarks demonstrate substantial improvements with 83.04\% accuracy on Yes/No questions, 52.66\% on factual questions, and 44.12\% on numerical questions in JDocQA, and 59\% accuracy on LAVA dataset. Ablation studies confirm meaningful contributions from each component, and our framework establishes new state-of-the-art results for Japanese document VQA while providing a scalable foundation for other low-resource languages and specialized domains. Our code available at: https://github.com/Haoxuanli-Thu/AdaDocVQA.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRISP: Persistent Concept Unlearning via Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2508.13650</link>
<guid>https://arxiv.org/abs/2508.13650</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, sparse autoencoders, concept unlearning, parameter-efficient, persistent changes

Summary:
CRISP is a new method that efficiently removes unwanted knowledge from large language models by using sparse autoencoders. Unlike previous methods, CRISP creates persistent changes in the model parameters, making it resistant to malicious actors. The method automatically identifies important features across multiple layers and suppresses their activations, effectively removing harmful knowledge while maintaining the model's overall performance. Experimental results on two LLMs show that CRISP outperforms existing approaches on safety-critical unlearning tasks, demonstrating its effectiveness in preserving model utility. Feature-level analysis indicates that CRISP successfully separates target and benign concepts, allowing for precise suppression of specific features. CRISP offers a promising solution for selective knowledge removal in language models, addressing the growing need for model safety and security. 

Summary:<br />Keywords: large language models, sparse autoencoders, concept unlearning, parameter-efficient, persistent changes. <div>
arXiv:2508.13650v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, the need to selectively remove unwanted knowledge while preserving model utility has become paramount. Recent work has explored sparse autoencoders (SAEs) to perform precise interventions on monosemantic features. However, most SAE-based methods operate at inference time, which does not create persistent changes in the model's parameters. Such interventions can be bypassed or reversed by malicious actors with parameter access. We introduce CRISP, a parameter-efficient method for persistent concept unlearning using SAEs. CRISP automatically identifies salient SAE features across multiple layers and suppresses their activations. We experiment with two LLMs and show that our method outperforms prior approaches on safety-critical unlearning tasks from the WMDP benchmark, successfully removing harmful knowledge while preserving general and in-domain capabilities. Feature-level analysis reveals that CRISP achieves semantically coherent separation between target and benign concepts, allowing precise suppression of the target features.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?</title>
<link>https://arxiv.org/abs/2508.13680</link>
<guid>https://arxiv.org/abs/2508.13680</guid>
<content:encoded><![CDATA[
<div> multimodal; Vietnamese; educational assessments; VLMs; cross-lingual reasoning  
Summary:   
- VLMs are tested on Vietnamese educational assessments, revealing challenges in handling cross-lingual multimodal reasoning.  
- ViExam benchmark includes 2,548 multimodal questions across academic domains like Mathematics and Chemistry.  
- State-of-the-art VLMs achieve only 57.74% accuracy on ViExam, underperforming human test-takers.  
- Cross-lingual prompting with English instructions does not improve VLM performance.  
- Human-in-the-loop collaboration can enhance VLM performance by 5 percentage points.  
<br /><br />Summary: <div>
arXiv:2508.13680v1 Announce Type: new 
Abstract: Vision language models (VLMs) demonstrate remarkable capabilities on English multimodal tasks, but their performance on low-resource languages with genuinely multimodal educational content remains largely unexplored. In this work, we test how VLMs perform on Vietnamese educational assessments, investigating whether VLMs trained predominantly on English data can handle real-world cross-lingual multimodal reasoning. Our work presents the first comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams through proposing ViExam, a benchmark containing 2,548 multimodal questions. We find that state-of-the-art VLMs achieve only 57.74% while open-source models achieve 27.70% mean accuracy across 7 academic domains, including Mathematics, Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs underperform average human test-takers (66.54%), with only the thinking VLM o3 (74.07%) exceeding human average performance, yet still falling substantially short of human best performance (99.60%). Cross-lingual prompting with English instructions while maintaining Vietnamese content fails to improve performance, decreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop collaboration can partially improve VLM performance by 5 percentage points. Code and data are available at: https://vi-exam.github.io.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generics and Default Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2508.13718</link>
<guid>https://arxiv.org/abs/2508.13718</guid>
<content:encoded><![CDATA[
<div> reasoning patterns, language models, generic generalizations, default reasoning, performance evaluation 

Summary:
This paper evaluates the capabilities of 28 large language models (LLMs) in reasoning with 20 defeasible reasoning patterns involving generic generalizations. The study focuses on how these models handle non-monotonic logic and default reasoning problems. While some models show proficiency in handling default reasoning tasks, performance varies significantly across models and prompting styles. Few-shot prompting has a modest positive impact on performance for some models, but chain-of-thought (CoT) prompting often leads to performance degradation. Most models struggle with distinguishing between defeasible and deductive inference, and misinterpret generics as universal statements. These findings highlight both the potential and limitations of current LLMs in default reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2508.13718v1 Announce Type: new 
Abstract: This paper evaluates the capabilities of 28 large language models (LLMs) to reason with 20 defeasible reasoning patterns involving generic generalizations (e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic. Generics are of special interest to linguists, philosophers, logicians, and cognitive scientists because of their complex exception-permitting behaviour and their centrality to default reasoning, cognition, and concept acquisition. We find that while several frontier models handle many default reasoning problems well, performance varies widely across models and prompting styles. Few-shot prompting modestly improves performance for some models, but chain-of-thought (CoT) prompting often leads to serious performance degradation (mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy in zero-shot condition, temperature 0). Most models either struggle to distinguish between defeasible and deductive inference or misinterpret generics as universal statements. These findings underscore both the promise and limits of current LLMs for default reasoning.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings</title>
<link>https://arxiv.org/abs/2508.13729</link>
<guid>https://arxiv.org/abs/2508.13729</guid>
<content:encoded><![CDATA[
<div> word embeddings, deep learning, interpretability, semantic features, prediction accuracy  
Summary:  
- Understanding knowledge encoded in deep learning models is crucial for interpretability.  
- Methods to explain knowledge in word embeddings involve mapping onto semantic features.  
- Accurate prediction of semantic features from word embeddings does not always indicate genuine interpretability.  
- Prediction accuracy may reflect algorithmic upper bounds rather than meaningful semantic representation.  
- Comparison of datasets based on prediction performance may not reliably determine which is better captured by word embeddings.  
- Mapping methods primarily reflect geometric similarity in vector spaces rather than genuine semantic properties. <div>
arXiv:2508.13729v1 Announce Type: new 
Abstract: Understanding what knowledge is implicitly encoded in deep learning models is essential for improving the interpretability of AI systems. This paper examines common methods to explain the knowledge encoded in word embeddings, which are core elements of large language models (LLMs). These methods typically involve mapping embeddings onto collections of human-interpretable semantic features, known as feature norms. Prior work assumes that accurately predicting these semantic features from the word embeddings implies that the embeddings contain the corresponding knowledge. We challenge this assumption by demonstrating that prediction accuracy alone does not reliably indicate genuine feature-based interpretability.
  We show that these methods can successfully predict even random information, concluding that the results are predominantly determined by an algorithmic upper bound rather than meaningful semantic representation in the word embeddings. Consequently, comparisons between datasets based solely on prediction performance do not reliably indicate which dataset is better captured by the word embeddings. Our analysis illustrates that such mappings primarily reflect geometric similarity within vector spaces rather than indicating the genuine emergence of semantic properties.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG-MedRAG: Enhancing EEG-based Clinical Decision-Making via Hierarchical Hypergraph Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.13735</link>
<guid>https://arxiv.org/abs/2508.13735</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG, hypergraph, retrieval, generation, clinical QA

Summary:
EEG-MedRAG is a novel framework designed to efficiently retrieve and interpret large-scale EEG data by integrating domain knowledge, patient cases, and a repository into a hypergraph structure. The framework enables joint semantic-temporal retrieval and diagnostic generation, outperforming existing methods like TimeRAG and HyperGraphRAG in accuracy and retrieval. Additionally, a new clinical QA benchmark is introduced, covering seven disorders and five clinical perspectives, allowing for disease-agnostic generalization and role-aware contextual understanding. The benchmark facilitates systematic evaluation of clinical decision support systems. Overall, EEG-MedRAG shows promise for improving clinical practice by enhancing the interpretation and retrieval of EEG data, providing valuable insights for healthcare professionals. The data and code for EEG-MedRAG are publicly available for further research and development. 

<br /><br />Summary: <div>
arXiv:2508.13735v1 Announce Type: new 
Abstract: With the widespread application of electroencephalography (EEG) in neuroscience and clinical practice, efficiently retrieving and semantically interpreting large-scale, multi-source, heterogeneous EEG data has become a pressing challenge. We propose EEG-MedRAG, a three-layer hypergraph-based retrieval-augmented generation framework that unifies EEG domain knowledge, individual patient cases, and a large-scale repository into a traversable n-ary relational hypergraph, enabling joint semantic-temporal retrieval and causal-chain diagnostic generation. Concurrently, we introduce the first cross-disease, cross-role EEG clinical QA benchmark, spanning seven disorders and five authentic clinical perspectives. This benchmark allows systematic evaluation of disease-agnostic generalization and role-aware contextual understanding. Experiments show that EEG-MedRAG significantly outperforms TimeRAG and HyperGraphRAG in answer accuracy and retrieval, highlighting its strong potential for real-world clinical decision support. Our data and code are publicly available at https://github.com/yi9206413-boop/EEG-MedRAG.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias via Adversarial Dialogues in Scientific QA</title>
<link>https://arxiv.org/abs/2508.13743</link>
<guid>https://arxiv.org/abs/2508.13743</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, sycophancy, scientific question answering, evaluation framework, Pressure-Tune

Summary:
Large language models (LLMs) are increasingly used in factual contexts but often show sycophantic behavior, aligning with user beliefs regardless of correctness. This can be problematic in scientific question answering (QA) where model outputs may influence decision-making. A new evaluation framework measures the impact of social pressure on model behavior, revealing widespread sycophantic tendencies. The Pressure-Tune method fine-tunes models on synthetic adversarial dialogues to improve sycophancy resistance without compromising accuracy. The study shows that this approach enhances the ability of models to resist user misinformation while maintaining factual consistency. This framework offers a practical solution to address sycophancy in factual QA contexts and improve the truthfulness of model behavior.<br /><br />Summary: Large language models often exhibit sycophantic behavior in scientific QA contexts, aligning with user beliefs to satisfy preferences rather than focusing on correctness. A new evaluation framework quantifies the impact of social pressure on model outputs and reveals the need for mitigation strategies. The Pressure-Tune method is proposed as a solution, fine-tuning models on adversarial dialogues to enhance sycophancy resistance and maintain factual consistency. This approach offers a practical pathway towards more truthful and principled model behavior in high-stakes settings. <div>
arXiv:2508.13743v1 Announce Type: new 
Abstract: Large language models (LLMs), while increasingly used in domains requiring factual rigor, often display a troubling behavior: sycophancy, the tendency to align with user beliefs regardless of correctness. This tendency is reinforced by preference-based alignment techniques that optimize for user satisfaction but can undermine truthfulness. While relatively benign in casual dialogue, sycophancy poses serious risks in high-stakes settings such as scientific question answering (QA), where model outputs may shape collaborative reasoning, decision-making, and knowledge formation. Despite its importance, this phenomenon remains underexamined in factual QA contexts. We address this gap by introducing a unified evaluation framework to quantify the impact of sycophantic context on model behavior in scientific QA, measuring how much user-imposed social pressure distorts model outputs. The framework incorporates adversarial prompting setups and targeted metrics, such as misleading resistance and sycophancy resistance, that capture a model's ability to maintain factual consistency under misleading cues. Systematic evaluations across open-source and proprietary models reveal pervasive sycophantic tendencies, driven more by alignment strategy than by model size. To mitigate this issue, we propose Pressure-Tune, a lightweight post-training method that fine-tunes models on synthetic adversarial dialogues paired with chain-of-thought rationales. These rationales reject user misinformation while reinforcing factual commitments. Experiments on challenging scientific QA benchmarks show that Pressure-Tune significantly enhances sycophancy resistance without compromising accuracy or responsiveness to valid feedback, offering a practical pathway toward more truthful and principled model behavior.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MGT-Prism: Enhancing Domain Generalization for Machine-Generated Text Detection via Spectral Alignment</title>
<link>https://arxiv.org/abs/2508.13768</link>
<guid>https://arxiv.org/abs/2508.13768</guid>
<content:encoded><![CDATA[
<div> frequency domain, machine-generated text, domain generalization, text representation, spectral patterns

Summary: 
- The study focuses on the detection of Machine-Generated Text (MGT) and aims to improve performance in domain generalization.
- Current MGT detectors struggle with generalizing to unseen domains due to domain shift between different data sources.
- The proposed method, MGT-Prism, utilizes the frequency domain to enhance domain generalization.
- Analysis reveals consistent spectral patterns across diverse domains, with discrepancies in magnitude between MGT and human-written texts (HWTs).
- MGT-Prism incorporates a low-frequency domain filtering module and dynamic spectrum alignment to extract task-specific and domain-invariant features, resulting in improved detector performance across different scenarios. 

<br /><br />Summary: <div>
arXiv:2508.13768v1 Announce Type: new 
Abstract: Large Language Models have shown growing ability to generate fluent and coherent texts that are highly similar to the writing style of humans. Current detectors for Machine-Generated Text (MGT) perform well when they are trained and tested in the same domain but generalize poorly to unseen domains, due to domain shift between data from different sources. In this work, we propose MGT-Prism, an MGT detection method from the perspective of the frequency domain for better domain generalization. Our key insight stems from analyzing text representations in the frequency domain, where we observe consistent spectral patterns across diverse domains, while significant discrepancies in magnitude emerge between MGT and human-written texts (HWTs). The observation initiates the design of a low frequency domain filtering module for filtering out the document-level features that are sensitive to domain shift, and a dynamic spectrum alignment strategy to extract the task-specific and domain-invariant features for improving the detector's performance in domain generalization. Extensive experiments demonstrate that MGT-Prism outperforms state-of-the-art baselines by an average of 0.90% in accuracy and 0.92% in F1 score on 11 test datasets across three domain-generalization scenarios.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models (LLMs) Describe Pictures Like Children? A Comparative Corpus Study</title>
<link>https://arxiv.org/abs/2508.13769</link>
<guid>https://arxiv.org/abs/2508.13769</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, child language, text properties, semantic similarity, educational tools

Summary: 
This study investigates how large language models (LLMs) replicate child-like language by comparing LLM-generated texts to German children's descriptions of picture stories. Through analyzing various text properties such as word frequency, lexical richness, and part-of-speech tags, the researchers found that LLM-generated texts are longer but less lexically rich compared to children's language. The semantic vector space analysis revealed low similarity between the two corpora, suggesting discrepancies in corpus semantics. While few-shot prompts slightly increased similarities between children and LLM text, significant differences in lexical and semantic patterns still existed. These findings provide insights into the use of LLMs in psycholinguistic research and education but also raise questions about the appropriateness of LLM-generated language in child-directed educational tools. 

<br /><br />Summary: <div>
arXiv:2508.13769v1 Announce Type: new 
Abstract: The role of large language models (LLMs) in education is increasing, yet little attention has been paid to whether LLM-generated text resembles child language. This study evaluates how LLMs replicate child-like language by comparing LLM-generated texts to a collection of German children's descriptions of picture stories. We generated two LLM-based corpora using the same picture stories and two prompt types: zero-shot and few-shot prompts specifying a general age from the children corpus. We conducted a comparative analysis across psycholinguistic text properties, including word frequency, lexical richness, sentence and word length, part-of-speech tags, and semantic similarity with word embeddings. The results show that LLM-generated texts are longer but less lexically rich, rely more on high-frequency words, and under-represent nouns. Semantic vector space analysis revealed low similarity, highlighting differences between the two corpora on the level of corpus semantics. Few-shot prompt increased similarities between children and LLM text to a minor extent, but still failed to replicate lexical and semantic patterns. The findings contribute to our understanding of how LLMs approximate child language through multimodal prompting (text + image) and give insights into their use in psycholinguistic research and education while raising important questions about the appropriateness of LLM-generated language in child-directed educational tools.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TracSum: A New Benchmark for Aspect-Based Summarization with Sentence-Level Traceability in Medical Domain</title>
<link>https://arxiv.org/abs/2508.13798</link>
<guid>https://arxiv.org/abs/2508.13798</guid>
<content:encoded><![CDATA[
<div> Benchmark, TracSum, Medical domain, Summarization, LLMs

Summary:
TracSum is introduced as a benchmark for traceable, aspect-based summarization in the medical domain, providing 3.5K summary-citation pairs for 500 medical abstracts. A fine-grained evaluation framework assesses completeness and consistency using four metrics. The Track-Then-Sum pipeline is proposed as a baseline method. Experiments compare LLMs and the baseline on TracSum, showing the effectiveness of the benchmark. Performing sentence-level tracking before summarization improves accuracy, and incorporating full context enhances completeness. Human evaluation supports the evaluation results, confirming TracSum's potential as a benchmark for traceable, aspect-based summarization tasks. 

Summary: <div>
arXiv:2508.13798v1 Announce Type: new 
Abstract: While document summarization with LLMs has enhanced access to textual information, concerns about the factual accuracy of these summaries persist, especially in the medical domain. Tracing evidence from which summaries are derived enables users to assess their accuracy, thereby alleviating this concern. In this paper, we introduce TracSum, a novel benchmark for traceable, aspect-based summarization, in which generated summaries are paired with sentence-level citations, enabling users to trace back to the original context. First, we annotate 500 medical abstracts for seven key medical aspects, yielding 3.5K summary-citation pairs. We then propose a fine-grained evaluation framework for this new task, designed to assess the completeness and consistency of generated content using four metrics. Finally, we introduce a summarization pipeline, Track-Then-Sum, which serves as a baseline method for comparison. In experiments, we evaluate both this baseline and a set of LLMs on TracSum, and conduct a human evaluation to assess the evaluation results. The findings demonstrate that TracSum can serve as an effective benchmark for traceable, aspect-based summarization tasks. We also observe that explicitly performing sentence-level tracking prior to summarization enhances generation accuracy, while incorporating the full context further improves completeness.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding</title>
<link>https://arxiv.org/abs/2508.13804</link>
<guid>https://arxiv.org/abs/2508.13804</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, moral understanding, Bayesian evaluation, annotator disagreements, AI capabilities

Summary:
This study compares the moral understanding capabilities of large language models to humans using a Bayesian evaluation framework. By considering annotator disagreements, both inherent human disagreement and model domain sensitivity are captured. The evaluation includes top language models across various types of texts, such as social media, news, and forums, using a GPU-optimized Bayesian framework. The results show that AI models rank among the top 25% of human annotators in terms of balanced accuracy. Interestingly, AI models produce significantly fewer false negatives compared to humans, indicating their superior moral detection capabilities. Overall, the study sheds light on the ethical dimensions of language models and their potential for understanding moral nuances. 

<br /><br />Summary: <div>
arXiv:2508.13804v1 Announce Type: new 
Abstract: How do large language models understand moral dimensions compared to humans?
  This first large-scale Bayesian evaluation of market-leading language models provides the answer. In contrast to prior work using deterministic ground truth (majority or inclusion rules), we model annotator disagreements to capture both aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty (model domain sensitivity). We evaluate top language models (Claude Sonnet 4, DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on 100K+ texts spanning social media, news, and forums.
  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing that AI models typically rank among the top 25\% of human annotators, achieving much better-than-average balanced accuracy. Importantly, we find that AI produces far fewer false negatives than humans, highlighting their more sensitive moral detection capabilities.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs</title>
<link>https://arxiv.org/abs/2508.13805</link>
<guid>https://arxiv.org/abs/2508.13805</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, length control, prompt-based strategy, token generation, evaluation

Summary: 
A new approach has been introduced to control the length of text generated by large language models (LLMs) without the need for fine-tuning or iterative sampling. This strategy, based on prompts, includes countdown markers and explicit counting rules to ensure the model generates the desired number of tokens precisely. The evaluation was conducted on various tasks, including open-ended generation, summarization, instruction-following, and equal-length tasks. Results showed that the precise length control achieved through prompt engineering alone significantly improved length compliance, especially on the MT-Bench-LI task, surpassing previous methods and maintaining answer quality. This approach offers a lightweight alternative to training- or decoding-based methods for controlling text length accurately. 

<br /><br />Summary: <div>
arXiv:2508.13805v1 Announce Type: new 
Abstract: Controlling the length of text produced by large language models (LLMs) remains challenging: models frequently overshoot or undershoot explicit length instructions because they cannot reliably keep an internal token count. We present a prompt-based, one-shot strategy that compels an off-the-shelf LLM to generate exactly a desired number of tokens - words (English) or characters (Chinese) - without any fine-tuning or iterative sampling. The prompt appends countdown markers and explicit counting rules so that the model "writes while counting." We evaluate on four settings: open-ended generation (1-1000 tokens), XSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH equal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps from below 30% under naive prompts to above 95% with our countdown prompt, surpassing the popular draft-then-revise baseline, while judged answer quality is preserved. These results show that precise length control can be achieved through prompt engineering alone, offering a lightweight alternative to training- or decoding-based methods.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The illusion of a perfect metric: Why evaluating AI's words is harder than it looks</title>
<link>https://arxiv.org/abs/2508.13816</link>
<guid>https://arxiv.org/abs/2508.13816</guid>
<content:encoded><![CDATA[
<div> Evaluation, Natural Language Generation, Automatic Evaluation Metrics, LLM-based evaluators, Retrieval Augmented Generation <br />
Summary:<br />
- Evaluating Natural Language Generation (NLG) is crucial for AI adoption, with human evaluation being the standard but limited by cost and scalability.<br />
- Various automatic evaluation metrics (AEM) have been developed to approximate human judgment, evolving over time to include lexical, semantic, and LLM-based evaluators.<br />
- No single metric has emerged as a definitive solution, leading to challenges in selecting the most appropriate one for a given task.<br />
- Challenges include metrics capturing specific text quality aspects, variable effectiveness by task and dataset, unstructured validation practices, and inconsistent correlations with human judgment.<br />
- The study highlights challenges persisting in LLM-as-a-Judge metrics and in the evaluation of Retrieval Augmented Generation (RAG), emphasizing the importance of selecting metrics based on task-specific needs and enhancing validation methodologies. <br /> <div>
arXiv:2508.13816v1 Announce Type: new 
Abstract: Evaluating Natural Language Generation (NLG) is crucial for the practical adoption of AI, but has been a longstanding research challenge. While human evaluation is considered the de-facto standard, it is expensive and lacks scalability. Practical applications have driven the development of various automatic evaluation metrics (AEM), designed to compare the model output with human-written references, generating a score which approximates human judgment. Over time, AEMs have evolved from simple lexical comparisons, to semantic similarity models and, more recently, to LLM-based evaluators. However, it seems that no single metric has emerged as a definitive solution, resulting in studies using different ones without fully considering the implications. This paper aims to show this by conducting a thorough examination of the methodologies of existing metrics, their documented strengths and limitations, validation methods, and correlations with human judgment. We identify several key challenges: metrics often capture only specific aspects of text quality, their effectiveness varies by task and dataset, validation practices remain unstructured, and correlations with human judgment are inconsistent. Importantly, we find that these challenges persist in the most recent type of metric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented Generation (RAG), an increasingly relevant task in academia and industry. Our findings challenge the quest for the 'perfect metric'. We propose selecting metrics based on task-specific needs and leveraging complementary evaluations and advocate that new metrics should focus on enhanced validation methodologies.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling</title>
<link>https://arxiv.org/abs/2508.13833</link>
<guid>https://arxiv.org/abs/2508.13833</guid>
<content:encoded><![CDATA[
<div> Keywords: Building Information Modeling, Natural Language Processing, Named Entity Recognition, Relation Extraction, French Building Technical Specification<br />
Summary: <br />
This study explores the integration of Building Information Modeling (BIM) with Natural Language Processing (NLP) to automate the extraction of requirements from unstructured French Building Technical Specification (BTS) documents within the construction industry. The study leverages transformer-based models such as CamemBERT and Fr_core_news_lg for Named Entity Recognition (NER) and Relation Extraction (RE). Various approaches from rule-based to deep learning methods are developed and compared. Results show superior performance of the transformer-based models in NER and Random Forest in RE. Future work aims to represent the outcomes as a knowledge graph to enhance automatic verification systems. <div>
arXiv:2508.13833v1 Announce Type: new 
Abstract: This study explores the integration of Building Information Modeling (BIM) with Natural Language Processing (NLP) to automate the extraction of requirements from unstructured French Building Technical Specification (BTS) documents within the construction industry. Employing Named Entity Recognition (NER) and Relation Extraction (RE) techniques, the study leverages the transformer-based model CamemBERT and applies transfer learning with the French language model Fr\_core\_news\_lg, both pre-trained on a large French corpus in the general domain. To benchmark these models, additional approaches ranging from rule-based to deep learning-based methods are developed. For RE, four different supervised models, including Random Forest, are implemented using a custom feature vector. A hand-crafted annotated dataset is used to compare the effectiveness of NER approaches and RE models. Results indicate that CamemBERT and Fr\_core\_news\_lg exhibited superior performance in NER, achieving F1-scores over 90\%, while Random Forest proved most effective in RE, with an F1 score above 80\%. The outcomes are intended to be represented as a knowledge graph in future work to further enhance automatic verification systems.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2508.13938</link>
<guid>https://arxiv.org/abs/2508.13938</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, MME-SCI benchmark, scientific domain, reasoning abilities, multilingual scenarios<br />
Summary:<br />
Multimodal large language models (MLLMs) have made significant advancements, but existing benchmarks in the scientific domain face challenges. To address this, the MME-SCI benchmark was proposed, which includes 1,019 high-quality question-answer pairs covering mathematics, physics, chemistry, and biology in five languages. Three distinct evaluation modes were utilized to assess models' reasoning abilities in multilingual scenarios and comprehensive modality coverage. Experiments on open-source and closed-source models showed the challenging nature of MME-SCI compared to existing benchmarks. Models struggled, highlighting weaknesses in specific domains. The benchmark also provided fine-grained annotation of scientific knowledge points. The Data and Evaluation Code are available on GitHub for further exploration and analysis.<br /><br />Summary: <div>
arXiv:2508.13938v1 Announce Type: new 
Abstract: Recently, multimodal large language models (MLLMs) have achieved significant advancements across various domains, and corresponding evaluation benchmarks have been continuously refined and improved. In this process, benchmarks in the scientific domain have played an important role in assessing the reasoning capabilities of MLLMs. However, existing benchmarks still face three key challenges: 1) Insufficient evaluation of models' reasoning abilities in multilingual scenarios; 2) Inadequate assessment of MLLMs' comprehensive modality coverage; 3) Lack of fine-grained annotation of scientific knowledge points. To address these gaps, we propose MME-SCI, a comprehensive and challenging benchmark. We carefully collected 1,019 high-quality question-answer pairs, which involve 3 distinct evaluation modes. These pairs cover four subjects, namely mathematics, physics, chemistry, and biology, and support five languages: Chinese, English, French, Spanish, and Japanese. We conducted extensive experiments on 16 open-source models and 4 closed-source models, and the results demonstrate that MME-SCI is widely challenging for existing MLLMs. For instance, under the Image-only evaluation mode, o4-mini achieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics, physics, chemistry, and biology, respectively, indicating a significantly higher difficulty level compared to existing benchmarks. More importantly, using MME-SCI's multilingual and fine-grained knowledge attributes, we analyzed existing models' performance in depth and identified their weaknesses in specific domains. The Data and Evaluation Code are available at https://github.com/JCruan519/MME-SCI.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features</title>
<link>https://arxiv.org/abs/2508.13953</link>
<guid>https://arxiv.org/abs/2508.13953</guid>
<content:encoded><![CDATA[
<div> Keywords: ReviewGraph, review rating prediction, knowledge graphs, sentiment analysis, machine learning classifiers

Summary: 
ReviewGraph for Review Rating Prediction (RRP) is a novel framework that transforms textual customer reviews into knowledge graphs by extracting (subject, predicate, object) triples and associating sentiment scores. Utilizing graph embeddings and sentiment features, the framework predicts review rating scores through machine learning classifiers. The performance of ReviewGraph is comparable to large language models (LLMs) and outperforms traditional NLP baselines on agreement-based metrics such as Cohen's Kappa. Additionally, ReviewGraph offers advantages in interpretability, visual exploration, and potential integration into Retrieval-Augmented Generation (RAG) systems. The framework provides a promising approach to enhancing review analytics and serves as a foundation for future research that combines advanced graph neural networks and fine-tuned LLM-based extraction methods.<br /><br />Summary: <div>
arXiv:2508.13953v1 Announce Type: new 
Abstract: In the hospitality industry, understanding the factors that drive customer review ratings is critical for improving guest satisfaction and business performance. This work proposes ReviewGraph for Review Rating Prediction (RRP), a novel framework that transforms textual customer reviews into knowledge graphs by extracting (subject, predicate, object) triples and associating sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the framework predicts review rating scores through machine learning classifiers. We compare ReviewGraph performance with traditional NLP baselines (such as Bag of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating them in the HotelRec dataset. In comparison to the state of the art literature, our proposed model performs similar to their best performing model but with lower computational cost (without ensemble).
  While ReviewGraph achieves comparable predictive performance to LLMs and outperforms baselines on agreement-based metrics such as Cohen's Kappa, it offers additional advantages in interpretability, visual exploration, and potential integration into Retrieval-Augmented Generation (RAG) systems. This work highlights the potential of graph-based representations for enhancing review analytics and lays the groundwork for future research integrating advanced graph neural networks and fine-tuned LLM-based extraction methods. We will share ReviewGraph output and platform open-sourced on our GitHub page https://github.com/aaronlifenghan/ReviewGraph
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization</title>
<link>https://arxiv.org/abs/2508.13993</link>
<guid>https://arxiv.org/abs/2508.13993</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-context modeling, Large Language Models, Multi-Armed Bandit, Direct Preference Optimization, Diversity<br />
Summary: 
The paper introduces LongMab-PO, a framework aimed at enhancing long-context modeling by leveraging a Multi-Armed Bandit (MAB) rollout strategy. This approach identifies informative context chunks and uses them to generate high-quality and diverse responses for training Large Language Models (LLMs). By treating context chunks as arms of MAB and selecting them based on expected reward scores, the model is able to focus on relevant segments and improve response quality. The framework then utilizes Direct Preference Optimization (DPO) to further refine the LLM's performance. Experimental results demonstrate that LongMab-PO outperforms existing methods and achieves state-of-the-art results on long-context reasoning benchmarks. The code and data for LongMab-PO will be made available on GitHub. <br /><br />Summary: <div>
arXiv:2508.13993v1 Announce Type: new 
Abstract: Long-context modeling is critical for a wide range of real-world tasks, including long-context question answering, summarization, and complex reasoning tasks. Recent studies have explored fine-tuning Large Language Models (LLMs) with synthetic data to enhance their long-context capabilities. However, the effectiveness of such approaches is often limited by the low diversity and factual inconsistencies in the generated data. To address these challenges, we propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB) rollout strategy to identify the most informative chunks from the given long context for sampling high-quality and diverse responses and constructing preference data pairs for Direct Preference Optimization (DPO) training. Specifically, we treat context chunks as arms of MAB, select chunks based on their expected reward scores to input into LLMs to generate responses, and iteratively update these scores based on reward feedback. This exploration and exploitation process enables the model to focus on the most relevant context segments, thereby generating and collecting high-quality and diverse responses. Finally, we collect these generated responses from the rollout process and apply the DPO method to further optimize the LLM. Experimental results show that LongMab-PO significantly improves the diversity and quality of preference data pairs, achieving state-of-the-art performance on long-context reasoning benchmarks. All code and data will be released on https://github.com/NEUIR/LongMab-PO.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ask Good Questions for Large Language Models</title>
<link>https://arxiv.org/abs/2508.14025</link>
<guid>https://arxiv.org/abs/2508.14025</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, dialog systems, Concept-Enhanced Item Response Theory, information retrieval, user knowledge levels

Summary: 
The article introduces the Ask-Good-Question (AGQ) framework, which combines a Concept-Enhanced Item Response Theory (CEIRT) model with large language models (LLMs) to improve guidance in dialog systems. The CEIRT model helps identify user knowledge levels, allowing for the generation of tailored guiding questions from inspiring text. This approach enhances information retrieval efficiency during question and answer processes. By comparing with baseline methods, the AGQ framework significantly improves users' information retrieval experiences. <div>
arXiv:2508.14025v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have significantly improved the performance of dialog systems, yet current approaches often fail to provide accurate guidance of topic due to their inability to discern user confusion in related concepts. To address this, we introduce the Ask-Good-Question (AGQ) framework, which features an improved Concept-Enhanced Item Response Theory (CEIRT) model to better identify users' knowledge levels. Our contributions include applying the CEIRT model along with LLMs to directly generate guiding questions based on the inspiring text, greatly improving information retrieval efficiency during the question & answer process. Through comparisons with other baseline methods, our approach outperforms by significantly enhencing the users' information retrieval experiences.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR</title>
<link>https://arxiv.org/abs/2508.14029</link>
<guid>https://arxiv.org/abs/2508.14029</guid>
<content:encoded><![CDATA[
<div> Variational problem synthesis, reinforcement learning, large language models, generation diversity, reasoning tasks <br />
<br />
Summary: The paper introduces a new approach called Self-play with Variational problem Synthesis (SvS) for Reinforcement Learning with Verifiable Rewards (RLVR) training in Large Language Models (LLMs) to improve generation diversity and Pass@k performance. Vanilla RLVR training often leads to reduced generation diversity and limited reasoning capability due to entropy collapse. By augmenting and updating training problems within the RLVR framework, the SvS strategy helps maintain policy entropy during training. This self-improving approach significantly enhances Pass@k performance compared to standard RLVR, with absolute gains of 18.3% and 22.8% on competition-level benchmarks. Experimental results across different model sizes validate the effectiveness and robustness of SvS in improving LLM reasoning tasks. <div>
arXiv:2508.14029v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a key paradigm for post-training Large Language Models (LLMs), particularly for complex reasoning tasks. However, vanilla RLVR training has been shown to improve Pass@1 performance at the expense of policy entropy, leading to reduced generation diversity and limiting the Pass@k performance, which typically represents the upper bound of LLM reasoning capability. In this paper, we systematically analyze the policy's generation diversity from the perspective of training problems and find that augmenting and updating training problems helps mitigate entropy collapse during training. Based on these observations, we propose an online Self-play with Variational problem Synthesis (SvS) strategy for RLVR training, which uses the policy's correct solutions to synthesize variational problems while ensuring their reference answers remain identical to the originals. This self-improving strategy effectively maintains policy entropy during training and substantially improves Pass@k compared with standard RLVR, sustaining prolonged improvements and achieving absolute gains of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model sizes from 3B to 32B consistently demonstrate the generalizability and robustness of SvS.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation</title>
<link>https://arxiv.org/abs/2508.14031</link>
<guid>https://arxiv.org/abs/2508.14031</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, fine-tuning, safety concerns, Prefix INjection Guard, task performance

Summary: 
Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. However, safety concerns are often overlooked during the fine-tuning process, leading to unintentional misalignment of aligned LLMs and potentially harmful task execution. To address this, the Prefix INjection Guard (PING) method is proposed, which adds natural language prefixes to agent responses to guide them to refuse harmful requests while maintaining performance on benign tasks. Experimental results show that PING significantly increases the safety of fine-tuned LLM agents without sacrificing effectiveness, outperforming existing prompting approaches. Analysis of internal hidden states reveals the importance of prefix tokens in behavior modification. This work highlights the importance of considering ethical concerns in the development of agentic LLMs. 

Summary: <div>
arXiv:2508.14031v1 Announce Type: new 
Abstract: Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities</title>
<link>https://arxiv.org/abs/2508.14032</link>
<guid>https://arxiv.org/abs/2508.14032</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital health analytics, Online Health Communities, Large Language Models, Sentiment Analysis, Expert knowledge integration<br />
Summary:<br />
- The study focuses on challenges in digital health analytics, especially in analyzing patient-generated health content with complex emotional and medical contexts in Online Health Communities (OHCs).
- Traditional ML approaches are limited by data shortage and privacy concerns, while OHCs pose challenges with mixed-sentiment posts and clinical terminology.
- Large Language Models (LLMs) are explored as a solution, integrating expert knowledge through in-context learning for Sentiment Analysis (SA).
- LLMs, using a structured codebook for expert interpretation guidelines, outperform pre-trained models and lexicon-based methods, showing expert-level agreement.
- This approach addresses the shortage of expert knowledge in digital health research, offering a scalable solution for real-time, expert-quality analysis in patient monitoring and health strategies.<br /><br />Summary: <div>
arXiv:2508.14032v1 Announce Type: new 
Abstract: Digital health analytics face critical challenges nowadays. The sophisticated analysis of patient-generated health content, which contains complex emotional and medical contexts, requires scarce domain expertise, while traditional ML approaches are constrained by data shortage and privacy limitations in healthcare settings. Online Health Communities (OHCs) exemplify these challenges with mixed-sentiment posts, clinical terminology, and implicit emotional expressions that demand specialised knowledge for accurate Sentiment Analysis (SA). To address these challenges, this study explores how Large Language Models (LLMs) can integrate expert knowledge through in-context learning for SA, providing a scalable solution for sophisticated health data analysis. Specifically, we develop a structured codebook that systematically encodes expert interpretation guidelines, enabling LLMs to apply domain-specific knowledge through targeted prompting rather than extensive training. Six GPT models validated alongside DeepSeek and LLaMA 3.1 are compared with pre-trained language models (BioBERT variants) and lexicon-based methods, using 400 expert-annotated posts from two OHCs. LLMs achieve superior performance while demonstrating expert-level agreement. This high agreement, with no statistically significant difference from inter-expert agreement levels, suggests knowledge integration beyond surface-level pattern recognition. The consistent performance across diverse LLM models, supported by in-context learning, offers a promising solution for digital health analytics. This approach addresses the critical challenge of expert knowledge shortage in digital health research, enabling real-time, expert-quality analysis for patient monitoring, intervention assessment, and evidence-based health strategies.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaoSR1: The Thinking Model for E-commerce Relevance Search</title>
<link>https://arxiv.org/abs/2508.12365</link>
<guid>https://arxiv.org/abs/2508.12365</guid>
<content:encoded><![CDATA[
<div> Query-product relevance prediction, e-commerce search, BERT-based models, Large Language Models (LLMs), Chain-of-Thought (CoT) error accumulation <br />
<br />
Summary: <br />
The article discusses a framework called TaoSR1 for query-product relevance prediction in e-commerce search. The framework addresses key challenges such as Chain-of-Thought (CoT) error accumulation, discriminative hallucination, and deployment feasibility. TaoSR1 involves three stages: Supervised Fine-Tuning (SFT) with CoT for reasoning, Offline sampling with a pass@N strategy and Direct Preference Optimization (DPO) for generation quality improvement, and Difficulty-based dynamic sampling with Group Relative Policy Optimization (GRPO) to mitigate discriminative hallucination. The framework also includes post-CoT processing and a cumulative probability-based partitioning method for efficient online deployment. TaoSR1 outperforms baselines on offline datasets and shows substantial gains in online side-by-side human evaluations, introducing a novel approach for integrating CoT reasoning into relevance classification. <div>
arXiv:2508.12365v1 Announce Type: cross 
Abstract: Query-product relevance prediction is a core task in e-commerce search. BERT-based models excel at semantic matching but lack complex reasoning capabilities. While Large Language Models (LLMs) are explored, most still use discriminative fine-tuning or distill to smaller models for deployment. We propose a framework to directly deploy LLMs for this task, addressing key challenges: Chain-of-Thought (CoT) error accumulation, discriminative hallucination, and deployment feasibility. Our framework, TaoSR1, involves three stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning; (2) Offline sampling with a pass@N strategy and Direct Preference Optimization (DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling with Group Relative Policy Optimization (GRPO) to mitigate discriminative hallucination. Additionally, post-CoT processing and a cumulative probability-based partitioning method enable efficient online deployment. TaoSR1 significantly outperforms baselines on offline datasets and achieves substantial gains in online side-by-side human evaluations, introducing a novel paradigm for applying CoT reasoning to relevance classification.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL</title>
<link>https://arxiv.org/abs/2508.13167</link>
<guid>https://arxiv.org/abs/2508.13167</guid>
<content:encoded><![CDATA[
<div> model; multi-agent systems; chain-of-agents; problem-solving; reinforcement learning
Summary:
Chain-of-Agents (CoA) is introduced as a novel paradigm for reasoning in large language models (LLMs) that enables end-to-end complex problem-solving similar to multi-agent systems within one model. A multi-agent distillation framework is used to distill state-of-the-art multi-agent systems into chain-of-agents trajectories for agentic supervised fine-tuning, followed by agentic reinforcement learning on verifiable tasks to enhance problem-solving capabilities. The resulting models, Agent Foundation Models (AFMs), exhibit new state-of-the-art performance across various benchmarks in web agent and code agent settings. The research is fully open-sourced, providing a foundation for future exploration in agent models and agentic reinforcement learning.<br /><br />Summary: <div>
arXiv:2508.13167v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) and multi-agent systems have demonstrated remarkable capabilities in complex problem-solving tasks such as deep research, vibe coding, and mathematical reasoning. However, most existing multi-agent systems are built upon manual prompt/workflow engineering with sophisticated agent frameworks, making them computationally inefficient, less capable, and can not benefit from data-centric learning. In this work, we introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables native end-to-end complex problem-solving in the same way as a multi-agent system (i.e., multi-turn problem solving with multiple tools and multiple agents) within one model. In chain-of-agents problem-solving, the model dynamically activates different tool agents and role-playing agents to simulate multi-agent collaboration in an end-to-end fashion. To elicit end-to-end chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent distillation framework to distill state-of-the-art multi-agent systems into chain-of-agents trajectories for agentic supervised fine-tuning. We then use agentic reinforcement learning on verifiable agentic tasks to further improve the models' capabilities on chain-of-agents problem solving. We call the resulting models Agent Foundation Models (AFMs). Our empirical studies demonstrate that AFM establishes new state-of-the-art performance across diverse benchmarks in both web agent and code agent settings. We make the entire research, including the model weights, code for training and evaluation, and the training data, fully open-sourced, which offers a solid starting point for future research on agent models and agentic RL.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context</title>
<link>https://arxiv.org/abs/2508.13171</link>
<guid>https://arxiv.org/abs/2508.13171</guid>
<content:encoded><![CDATA[
<div> Cognitive Workspace, active memory management, hierarchical cognitive buffers, task-driven context optimization, memory reuse rate <br />
<br />
Summary: The article proposes a Cognitive Workspace paradigm for improving the limitations of Large Language Models (LLMs) in context management by emulating human cognitive mechanisms. It highlights the shortcomings of current passive retrieval systems and introduces three innovations: active memory management, hierarchical cognitive buffers, and task-driven context optimization. Empirical validation shows a 58.6% memory reuse rate with a 17-18% efficiency gain compared to traditional Retrieval-Augmented Generation (RAG). Statistical analysis confirms the superiority of Cognitive Workspace with p < 0.001 and Cohen's d > 23 across various task types. The framework synthesizes insights from recent papers, positioning Cognitive Workspace as a significant shift towards genuine cognitive augmentation in LLM systems. <div>
arXiv:2508.13171v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) face fundamental limitations in context management despite recent advances extending context windows to millions of tokens. We propose Cognitive Workspace, a novel paradigm that transcends traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive mechanisms of external memory use. Drawing from cognitive science foundations including Baddeley's working memory model, Clark's extended mind thesis, and Hutchins' distributed cognition framework, we demonstrate that current passive retrieval systems fail to capture the dynamic, task-driven nature of human memory management. Our analysis of 2024-2025 developments reveals that while techniques like Infini-attention and StreamingLLM achieve impressive context lengths, they lack the metacognitive awareness and active planning capabilities essential for true cognitive extension. Cognitive Workspace addresses these limitations through three core innovations: (1) active memory management with deliberate information curation, (2) hierarchical cognitive buffers enabling persistent working states, and (3) task-driven context optimization that dynamically adapts to cognitive demands. Empirical validation demonstrates Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from 54-60% across different tasks) compared to 0% for traditional RAG, with 17-18% net efficiency gain despite 3.3x higher operation counts. Statistical analysis confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple task types, establishing the first quantitative evidence for active memory superiority in LLM systems. We present a comprehensive theoretical framework synthesizing insights from 50+ recent papers, positioning Cognitive Workspace as a fundamental shift from information retrieval to genuine cognitive augmentation.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>White-Box Reasoning: Synergizing LLM Strategy and gm/Id Data for Automated Analog Circuit Design</title>
<link>https://arxiv.org/abs/2508.13172</link>
<guid>https://arxiv.org/abs/2508.13172</guid>
<content:encoded><![CDATA[
<div> Keywords: Analog IC design, Large Language Models, gm/Id methodology, synergistic reasoning, automation

Summary:
This article introduces a framework that combines Large Language Models (LLMs) with the gm/Id methodology to enhance analog IC design efficiency and precision. By integrating strategic reasoning with physical precision, the framework enables the LLM to serve as a quantitative design partner, improving the optimization process. Validation on a two-stage op-amp showed that the framework successfully met all TT corner specifications in 5 iterations and extended optimization to all PVT corners. An ablation study highlighted the importance of gm/Id data in achieving efficiency and precision in design. Comparing the framework's results to those of a senior engineer, it showed quasi-expert quality with significantly increased efficiency. This research demonstrates a promising approach for true analog design automation by combining LLM reasoning with established circuit design methodologies. 

<br /><br />Summary: <div>
arXiv:2508.13172v1 Announce Type: cross 
Abstract: Analog IC design is a bottleneck due to its reliance on experience and inefficient simulations, as traditional formulas fail in advanced nodes. Applying Large Language Models (LLMs) directly to this problem risks mere "guessing" without engineering principles. We present a "synergistic reasoning" framework that integrates an LLM's strategic reasoning with the physical precision of the gm/Id methodology. By empowering the LLM with gm/Id lookup tables, it becomes a quantitative, data-driven design partner.
  We validated this on a two-stage op-amp, where our framework enabled the Gemini model to meet all TT corner specs in 5 iterations and extended optimization to all PVT corners. A crucial ablation study proved gm/Id data is key for this efficiency and precision; without it, the LLM is slower and deviates. Compared to a senior engineer's design, our framework achieves quasi-expert quality with an order-of-magnitude improvement in efficiency. This work validates a path for true analog design automation by combining LLM reasoning with scientific circuit design methodologies.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task</title>
<link>https://arxiv.org/abs/2508.13178</link>
<guid>https://arxiv.org/abs/2508.13178</guid>
<content:encoded><![CDATA[
<div> interpretability, execution-guided strategy, semantic parsing, conditional enhancement, database queries

Summary:<br /><br />
The CESQL model combines interpretability analysis and execution-guided strategy to improve text-to-SQL models for WHERE clauses in SQL queries. By incorporating filtering adjustments, logical correlation refinements, and model fusion, CESQL enhances predictive accuracy on the WikiSQL dataset. The model reduces reliance on data in condition columns and eliminates the need for manually labeled training data when predicting conditional values in WHERE clauses. CESQL aims to improve the accuracy of processing basic database queries and offers insights for tackling complex queries and irregular data in real-world database settings. <div>
arXiv:2508.13178v1 Announce Type: cross 
Abstract: To elevate the foundational capabilities and generalization prowess of the text-to-SQL model in real-world applications, we integrate model interpretability analysis with execution-guided strategy for semantic parsing of WHERE clauses in SQL queries. Furthermore, we augment this approach with filtering adjustments, logical correlation refinements, and model fusion, culminating in the design of the CESQL model that facilitates conditional enhancement. Our model excels on the WikiSQL dataset, which is emblematic of single-table database query tasks, markedly boosting the accuracy of prediction outcomes. When predicting conditional values in WHERE clauses, we have not only minimized our dependence on data within the condition columns of tables but also circumvented the impact of manually labeled training data. Our hope is that this endeavor to enhance accuracy in processing basic database queries will offer fresh perspectives for research into handling complex queries and scenarios featuring irregular data in real-world database environments.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combating Homelessness Stigma with LLMs: A New Multi-Modal Dataset for Bias Detection</title>
<link>https://arxiv.org/abs/2508.13187</link>
<guid>https://arxiv.org/abs/2508.13187</guid>
<content:encoded><![CDATA[
<div> Dataset, Social bias, Homelessness, Language models, Classification 

Summary:
- The research focuses on addressing social biases towards people experiencing homelessness (PEH) through analyzing online discourse using natural language processing (NLP) and large language models (LLMs).
- A multi-modal dataset from various online platforms was manually annotated to identify different types of homelessness bias.
- Local LLMs and closed-source LLMs were evaluated for classifying bias, with in-context learning showing better performance compared to zero-shot classification.
- LLMs outperformed BERT in classifying bias across all categories.
- The study aims to increase awareness of bias against PEH, provide indicators for policymaking, and promote fair and ethical use of Generative AI technologies. 

<br /><br />Summary: <div>
arXiv:2508.13187v1 Announce Type: cross 
Abstract: Homelessness is a persistent social challenge, impacting millions worldwide. Over 770,000 people experienced homelessness in the U.S. in 2024. Social stigmatization is a significant barrier to alleviation, shifting public perception, and influencing policymaking. Given that online and city council discourse reflect and influence part of public opinion, it provides valuable insights to identify and track social biases. This research contributes to alleviating homelessness by acting on public opinion. It introduces novel methods, building on natural language processing (NLP) and large language models (LLMs), to identify and measure PEH social bias expressed in digital spaces. We present a new, manually-annotated multi-modal dataset compiled from Reddit, X (formerly Twitter), news articles, and city council meeting minutes across 10 U.S. cities. This unique dataset provides evidence of the typologies of homelessness bias described in the literature. In order to scale up and automate the detection of homelessness bias online, we evaluate LLMs as classifiers. We applied both zero-shot and few-shot classification techniques to this data. We utilized local LLMs (Llama 3.2 3B Instruct, Qwen 2.5 7B Instruct, and Phi4 Instruct Mini) as well as closed-source API models (GPT-4.1, Gemini 2.5 Pro, and Grok-4). Our findings reveal that although there are significant inconsistencies in local LLM zero-shot classification, the in-context learning classification scores of local LLMs approach the classification scores of closed-source LLMs. Furthermore, LLMs outperform BERT when averaging across all categories. This work aims to raise awareness about the pervasive bias against PEH, develop new indicators to inform policy, and ultimately enhance the fairness and ethical application of Generative AI technologies.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information</title>
<link>https://arxiv.org/abs/2508.13250</link>
<guid>https://arxiv.org/abs/2508.13250</guid>
<content:encoded><![CDATA[
<div> Keywords: language model, memory mechanisms, multi-hop reasoning, personalized information, hybrid approach 
Summary:
This study focuses on memory mechanisms in large language model-based agents for personalized reasoning tasks. The research introduces the multi-hop personalized reasoning task to explore and evaluate different memory methods in handling complex user information. A dataset and evaluation framework are created for this purpose. Various explicit and implicit memory approaches are implemented and compared, highlighting their strengths and weaknesses in multi-hop reasoning. The study also introduces a hybrid memory model, HybridMem, to combine the advantages of both memory paradigms. Extensive experiments demonstrate the effectiveness of the proposed model. The project is made available to the research community for further exploration. <div>
arXiv:2508.13250v1 Announce Type: cross 
Abstract: In large language model-based agents, memory serves as a critical capability for achieving personalization by storing and utilizing users' information. Although some previous studies have adopted memory to implement user personalization, they typically focus on preference alignment and simple question-answering. However, in the real world, complex tasks often require multi-hop reasoning on a large amount of user information, which poses significant challenges for current memory approaches. To address this limitation, we propose the multi-hop personalized reasoning task to explore how different memory mechanisms perform in multi-hop reasoning over personalized information. We explicitly define this task and construct a dataset along with a unified evaluation framework. Then, we implement various explicit and implicit memory methods and conduct comprehensive experiments. We evaluate their performance on this task from multiple perspectives and analyze their strengths and weaknesses. Besides, we explore hybrid approaches that combine both paradigms and propose the HybridMem method to address their limitations. We demonstrate the effectiveness of our proposed model through extensive experiments. To benefit the research community, we release this project at https://github.com/nuster1128/MPR.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms</title>
<link>https://arxiv.org/abs/2508.13337</link>
<guid>https://arxiv.org/abs/2508.13337</guid>
<content:encoded><![CDATA[
<div> scalable, Mixture-of-Experts, training system, X-MoE, DeepSeek-style <br />
<br />
X-MoE is a novel training system designed to improve the scalability of Mixture-of-Experts (MoE) architectures like DeepSeek-MoE. It addresses limitations such as activation memory overhead and communication costs by introducing efficient padding-free training with cross-platform kernels, redundancy-bypassing dispatch, and hybrid parallelism. The system has been evaluated on the Frontier supercomputer with AMD MI250X GPUs, demonstrating the ability to scale DeepSeek-style MoEs up to 545 billion parameters across 1024 GPUs. This is a significant improvement, allowing for models that are 10 times larger than what existing methods can handle within the same hardware budget, while still maintaining high training throughput. The source code for X-MoE is available on GitHub for further exploration and implementation. <br /><br />Summary: <div>
arXiv:2508.13337v1 Announce Type: cross 
Abstract: Emerging expert-specialized Mixture-of-Experts (MoE) architectures, such as DeepSeek-MoE, deliver strong model quality through fine-grained expert segmentation and large top-k routing. However, their scalability is limited by substantial activation memory overhead and costly all-to-all communication. Furthermore, current MoE training systems - primarily optimized for NVIDIA GPUs - perform suboptimally on non-NVIDIA platforms, leaving significant computational potential untapped. In this work, we present X-MoE, a novel MoE training system designed to deliver scalable training performance for next-generation MoE architectures. X-MoE achieves this via several novel techniques, including efficient padding-free MoE training with cross-platform kernels, redundancy-bypassing dispatch, and hybrid parallelism with sequence-sharded MoE blocks. Our evaluation on the Frontier supercomputer, powered by AMD MI250X GPUs, shows that X-MoE scales DeepSeek-style MoEs up to 545 billion parameters across 1024 GPUs - 10x larger than the largest trainable model with existing methods under the same hardware budget, while maintaining high training throughput. The source code of X-MoE is available at https://github.com/Supercomputing-System-AI-Lab/X-MoE.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TASER: Table Agents for Schema-guided Extraction and Recommendation</title>
<link>https://arxiv.org/abs/2508.13404</link>
<guid>https://arxiv.org/abs/2508.13404</guid>
<content:encoded><![CDATA[
<div> Keywords: financial documents, table extraction system, real-world tables, schema-guided extraction, continuous learning process <br />
Summary: 
The article presents TASER, a table extraction system for real-world financial documents. TASER is designed to extract information from messy, multi-page, fragmented tables efficiently. It uses table agents for detection, classification, extraction, and recommendations based on an initial schema. The Recommender Agent further refines the outputs, making schema revisions and final recommendations. TASER outperforms existing models and benefits from a continuous learning process, with larger batch sizes leading to more actionable recommendations and increased extracted holdings. The system is trained on a real financial table dataset called TASERTab, comprising 22,584 pages and 3,213 tables, representing holdings of $731,685,511,687. The research community can access this dataset. The results demonstrate the effectiveness of agentic, schema-guided extraction systems for understanding complex financial tables. <br /><br />Summary: <div>
arXiv:2508.13404v1 Announce Type: cross 
Abstract: Real-world financial documents report essential information about an entity's financial holdings that can span millions of different financial instrument types. Yet, these details are often buried in messy, multi-page, fragmented tables - for example, 99.4% of the tables in our dataset have no bounding boxes with the maximum number of rows amounting to 426 per table across 44 pages. To tackle these unique challenges from real-world tables, we present a continuously learning, agentic table extraction system, TASER (Table Agents for Schema-guided Extraction and Recommendation) that extracts highly unstructured, multi-page, heterogeneous tables into normalized, schema-conforming outputs. Our table agents execute on table detection, classification, extraction, and recommendations by leveraging an initial schema. Then, our Recommender Agent reviews the outputs, recommends schema revisions, and decides on the final recommendations, enabling TASER to outperform existing table detection models such as Table Transformer by 10.1%. Within this continuous learning process, we highlight that larger batch sizes result in a 104.3% increase in schema recommendations that are actionable and utilized, resulting in a 9.8% increase in extracted holdings - highlighting the importance of a continuous learning process. To train TASER, we have manually labeled 22,584 pages (28,150,449 tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of the first real financial table datasets. We release our dataset TASERTab to enable the research community to access real-world financial tables and outputs. Our results highlight the promise of agentic, schema-guided extraction systems for robust understanding of real-world financial tables.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference</title>
<link>https://arxiv.org/abs/2508.13439</link>
<guid>https://arxiv.org/abs/2508.13439</guid>
<content:encoded><![CDATA[
<div> Keywords: Intelligent Transportation Systems, Vision-Language Models, Knowledge Distillation, Traffic Scene Understanding, Risk Inference

Summary:
The article introduces a novel framework, VISTA, for highway scene understanding and traffic risk inference in Intelligent Transportation Systems. This framework combines two large Vision-Language Models, GPT-4o and o3-mini, using a structured Chain-of-Thought strategy to generate high-quality traffic scene annotations and risk assessments. The outputs from these models are used as pseudo-annotations for training a compact 3B-scale model, VISTA, through knowledge distillation. Despite its smaller size, VISTA performs well in understanding low-resolution traffic videos and generating risk-aware captions, outperforming the teacher models on various captioning metrics. The compact architecture of VISTA enables efficient deployment on edge devices for real-time risk monitoring without the need for extensive infrastructure upgrades. This research highlights the effectiveness of structured prompting and knowledge distillation in empowering lightweight Vision-Language Models for complex reasoning tasks in real-world environments. 

<br /><br />Summary: 
- Introduction of VISTA framework for highway scene understanding and traffic risk inference in Intelligent Transportation Systems.
- Combination of large Vision-Language Models using structured Chain-of-Thought strategy for generating annotations and risk assessments.
- Knowledge distillation approach to train a compact 3B-scale model, VISTA, for efficient deployment on edge devices.
- VISTA performs well in understanding low-resolution traffic videos and generating risk-aware captions, outperforming teacher models on captioning metrics.
- Demonstrates the effectiveness of structured prompting and knowledge distillation in empowering lightweight models for complex reasoning tasks. <div>
arXiv:2508.13439v1 Announce Type: cross 
Abstract: Comprehensive highway scene understanding and robust traffic risk inference are vital for advancing Intelligent Transportation Systems (ITS) and autonomous driving. Traditional approaches often struggle with scalability and generalization, particularly under the complex and dynamic conditions of real-world environments. To address these challenges, we introduce a novel structured prompting and knowledge distillation framework that enables automatic generation of high-quality traffic scene annotations and contextual risk assessments. Our framework orchestrates two large Vision-Language Models (VLMs): GPT-4o and o3-mini, using a structured Chain-of-Thought (CoT) strategy to produce rich, multi-perspective outputs. These outputs serve as knowledge-enriched pseudo-annotations for supervised fine-tuning of a much smaller student VLM. The resulting compact 3B-scale model, named VISTA (Vision for Intelligent Scene and Traffic Analysis), is capable of understanding low-resolution traffic videos and generating semantically faithful, risk-aware captions. Despite its significantly reduced parameter count, VISTA achieves strong performance across established captioning metrics (BLEU-4, METEOR, ROUGE-L, and CIDEr) when benchmarked against its teacher models. This demonstrates that effective knowledge distillation and structured multi-agent supervision can empower lightweight VLMs to capture complex reasoning capabilities. The compact architecture of VISTA facilitates efficient deployment on edge devices, enabling real-time risk monitoring without requiring extensive infrastructure upgrades.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Enhanced Linear Autoencoders for Recommendation</title>
<link>https://arxiv.org/abs/2508.13500</link>
<guid>https://arxiv.org/abs/2508.13500</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Linear autoencoders, Recommender systems, Semantic item correlations, Collaborative signals

Summary: 
Large language models (LLMs) are integrated with linear autoencoders (LAEs) in the proposed L3AE model for enriching textual item information in recommender systems. L3AE overcomes the limitations of sparse word co-occurrence patterns in existing LAEs by effectively integrating textual semantics and user-item interactions through a two-phase optimization strategy. Firstly, L3AE constructs a semantic item-to-item correlation matrix from LLM-derived item representations. Secondly, it learns an item-to-item weight matrix from collaborative signals while using semantic item correlations as regularization. Both phases of L3AE are optimized through closed-form solutions, ensuring global optimality and computational efficiency. Extensive experiments demonstrate superior performance of L3AE compared to state-of-the-art LLM-enhanced models, achieving significant gains in Recall@20 and NDCG@20 on benchmark datasets. The source code for L3AE is available on Github for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2508.13500v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been widely adopted to enrich the semantic representation of textual item information in recommender systems. However, existing linear autoencoders (LAEs) that incorporate textual information rely on sparse word co-occurrence patterns, limiting their ability to capture rich textual semantics. To address this, we propose L3AE, the first integration of LLMs into the LAE framework. L3AE effectively integrates the heterogeneous knowledge of textual semantics and user-item interactions through a two-phase optimization strategy. (i) L3AE first constructs a semantic item-to-item correlation matrix from LLM-derived item representations. (ii) It then learns an item-to-item weight matrix from collaborative signals while distilling semantic item correlations as regularization. Notably, each phase of L3AE is optimized through closed-form solutions, ensuring global optimality and computational efficiency. Extensive experiments demonstrate that L3AE consistently outperforms state-of-the-art LLM-enhanced models on three benchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20. The source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Input Time Scaling</title>
<link>https://arxiv.org/abs/2508.13654</link>
<guid>https://arxiv.org/abs/2508.13654</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Input Time Scaling, Training-testing co-design, Less is More phenomenon, AIME dataset

Summary: 
The study introduces a new scaling paradigm called Input Time Scaling for Large Language Models (LLMs) to enhance performance by focusing on queries during both training and testing phases. Surprisingly, the inclusion of seemingly low-quality data in queries can improve model performance, challenging the notion of "garbage in, garbage out." The study also highlights the importance of considering dataset quality over size when training models, as models trained on smaller but high-quality datasets can outperform larger models. Additionally, the research shows that a small set of examples is sufficient to facilitate high-level reasoning in LLMs. Experimental results on AIME datasets demonstrate state-of-the-art performance among 32B models, achieving up to 86.7% accuracy on AIME24 and 80% on AIME25 with a majority vote of three models. Open-sourcing datasets, data pipelines, evaluation results, and checkpoints is underway to support reproducibility and further research.<br /><br />Summary: <div>
arXiv:2508.13654v1 Announce Type: cross 
Abstract: Current Large Language Models (LLMs) are usually post-trained on large-scale carefully curated datasets (data & training scaling) and doing reasoning in test time (inference time scaling). In this work, we present a new scaling paradigm, Input Time Scaling, to complement previous scaling methods by putting resources on queries (input time). During training and testing, we combine meta-knowledge from LLMs to refine inputs with different strategies. We also find a new phenomenon, training-testing co-design there. We need to apply query strategies during both training and testing. Only applying strategies on training or testing would seriously degrade the performance. We are also surprised to find that seemingly low data quality datasets can gain high performance. Adding irrelevant information to the queries, randomly selecting examples from a minimally filtered dataset, can even perform the best. These findings contradict the widely held inductive bias, "garbage in, garbage out". Curating datasets with seemingly high-quality data can even potentially limit the performance ceiling. In addition, models trained on more data with similar quality (15k VS 1k) perform worse, simple dataset size scaling should also be carefully inspected. The good news is that our findings are compatible with the Less is More phenomenon. A small set of examples is enough to evoke high-level reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct, we are able to reach SOTA performance among 32B models on AIME24(76.7%) and AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B, the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate reproducibility and further research, we are working on open-source our datasets, data pipelines, evaluation results, and checkpoints.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Generalized Planning with LLMs through Strategy Refinement and Reflection</title>
<link>https://arxiv.org/abs/2508.13876</link>
<guid>https://arxiv.org/abs/2508.13876</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM, Python programs, PDDL planning, pseudocode, debugging

Summary: 
This study focuses on using Large Language Models (LLMs) to generate Python programs that represent generalized plans in PDDL planning. The previous framework involved generating a summary and strategy in natural language and implementing it as a Python program. However, if the strategy was incorrect, the resulting plan would also be incorrect. This study introduces an approach that generates the strategy in pseudocode, allowing for automatic debugging to identify and fix errors before plan generation. Additionally, a reflection step was added in the Python debugging phase to pinpoint plan failure reasons. Drawing inspiration from LLM code generation, multiple program variants were produced to select the best one. Experiments on 17 benchmark domains demonstrated that these extensions significantly improved the quality of generalized plans, with the best Python programs successfully solving all tasks in 12 domains. <div>
arXiv:2508.13876v1 Announce Type: cross 
Abstract: LLMs have recently been used to generate Python programs representing generalized plans in PDDL planning, i.e., plans that generalize across the tasks of a given PDDL domain. Previous work proposed a framework consisting of three steps: the LLM first generates a summary and then a strategy for the domain, both in natural language, and then implements that strategy as a Python program, that gets debugged on example planning tasks. In that work, only one strategy is generated and passed directly to the program generation. If the strategy is incorrect, its implementation will therefore result in an incorrect generalized plan. Here, we introduce an approach that generates the strategy in the form of pseudocode and enables automatic debugging of the pseudocode, hence allowing us to identify and fix errors prior to the generation of the generalized plan itself. Additionally, we extend the Python debugging phase with a reflection step prompting the LLM to pinpoint the reason for the observed plan failure. Finally, we take inspiration from LLM code generation to produce several program variants and pick the best one. Running experiments on 17 benchmark domains, we show that these extensions substantially improve (and never deteriorate) the quality of the generalized plans. In 12 of the domains, our best Python programs solve all tasks that can be generated with the respective instance generator.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Orchestration Markup Language</title>
<link>https://arxiv.org/abs/2508.13948</link>
<guid>https://arxiv.org/abs/2508.13948</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Prompt Orchestration Markup Language, data integration, format sensitivity, developer toolkit 

Summary:
Large Language Models (LLMs) require sophisticated prompting, and existing practices encounter challenges in structure, data integration, format sensitivity, and tooling. To address these issues, the authors introduce Prompt Orchestration Markup Language (POML). POML utilizes component-based markup for logical structure, specialized tags for seamless data integration, and a CSS-like styling system to reduce formatting sensitivity. It includes templating for dynamic prompts and a comprehensive developer toolkit with IDE support and SDKs. The authors validate POML through two case studies, showing its impact on complex application integration (PomLink) and accuracy performance (TableQA). A user study confirms its effectiveness in real-world development scenarios. POML offers a robust solution for organizing complex prompts involving diverse data types and managing presentation variations systematically. <br /><br />Summary: <div>
arXiv:2508.13948v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) require sophisticated prompting, yet current practices face challenges in structure, data integration, format sensitivity, and tooling. Existing methods lack comprehensive solutions for organizing complex prompts involving diverse data types (documents, tables, images) or managing presentation variations systematically. To address these gaps, we introduce POML (Prompt Orchestration Markup Language). POML employs component-based markup for logical structure (roles, tasks, examples), specialized tags for seamless data integration, and a CSS-like styling system to decouple content from presentation, reducing formatting sensitivity. It includes templating for dynamic prompts and a comprehensive developer toolkit (IDE support, SDKs) to improve version control and collaboration. We validate POML through two case studies demonstrating its impact on complex application integration (PomLink) and accuracy performance (TableQA), as well as a user study assessing its effectiveness in real-world development scenarios.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Query Logs Analytics: A Aystematic Literature Review</title>
<link>https://arxiv.org/abs/2508.13949</link>
<guid>https://arxiv.org/abs/2508.13949</guid>
<content:encoded><![CDATA[
<div> logs, databases, data warehouses, websites, knowledge graphs <br />
<br />Summary: In the digital age, user interactions generate logs that are valuable for various applications. This survey examines Database, Data Warehouse, Web, and Knowledge Graph logs to understand their common characteristics, usage pipelines, and constraints. The analysis of over 300 publications reveals a lack of standardized approaches for log exploitation, limited end-to-end methods, and shared structural elements among different log types. By consolidating existing knowledge and highlighting opportunities, this survey serves as a comprehensive guide for researchers and practitioners. It also emphasizes the need for standardization in log usage pipelines and suggests potential research directions for leveraging Knowledge Graph logs. <div>
arXiv:2508.13949v1 Announce Type: cross 
Abstract: In the digital era, user interactions with various resources such as databases, data warehouses, websites, and knowledge graphs (KGs) are increasingly mediated through digital platforms. These interactions leave behind digital traces, systematically captured in the form of logs. Logs, when effectively exploited, provide high value across industry and academia, supporting critical services (e.g., recovery and security), user-centric applications (e.g., recommender systems), and quality-of-service improvements (e.g., performance optimization). Despite their importance, research on log usage remains fragmented across domains, and no comprehensive study currently consolidates existing efforts. This paper presents a systematic survey of log usage, focusing on Database (DB), Data Warehouse (DW), Web, and KG logs. More than 300 publications were analyzed to address three central questions: (1) do different types of logs share common structural and functional characteristics? (2) are there standard pipelines for their usage? (3) which constraints and non-functional requirements (NFRs) guide their exploitation?. The survey reveals a limited number of end-to-end approaches, the absence of standardization across log usage pipelines, and the existence of shared structural elements among different types of logs. By consolidating existing knowledge, identifying gaps, and highlighting opportunities, this survey provides researchers and practitioners with a comprehensive overview of log usage and sheds light on promising directions for future research, particularly regarding the exploitation and democratization of KG logs.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation</title>
<link>https://arxiv.org/abs/2508.13968</link>
<guid>https://arxiv.org/abs/2508.13968</guid>
<content:encoded><![CDATA[
<div> benchmark, Multimodal Large Language Models, image rotation, spatial reasoning, fine-tuning

Summary:<br />
The study examines the ability of Multimodal Large Language Models (MLLMs) to accurately identify the orientation of input images rotated at different angles. A new benchmark called RotBench, consisting of lifestyle, portrait, and landscape images, was created for evaluation. Results show that current state-of-the-art MLLMs struggle to reliably identify rotated images, with most models able to distinguish between right-side-up and upside-down images but not between 90° and 270° rotations. Providing additional information or using chain-of-thought prompting only slightly improves performance. Simultaneously displaying images in different orientations boosts reasoning models' performance, and a modified voting setup enhances weaker models' accuracy. Fine-tuning does not significantly improve the models' ability to distinguish between 90° and 270° rotations, highlighting a notable gap between MLLMs' spatial reasoning capabilities and human perception in identifying rotation.<br />Summary: <div>
arXiv:2508.13968v1 Announce Type: cross 
Abstract: We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0{\deg}, 90{\deg}, 180{\deg}, and 270{\deg}. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench -- a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information -- including captions, depth maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0{\deg}) images, while certain models are able to identify upside-down (180{\deg}) images. None can reliably distinguish between 90{\deg} and 270{\deg}. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models' ability to distinguish 90{\deg} and 270{\deg} rotations, despite substantially improving the identification of 180{\deg} images. Together, these results reveal a significant gap between MLLMs' spatial reasoning capabilities and human perception in identifying rotation.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iTBLS: A Dataset of Interactive Conversations Over Tabular Information</title>
<link>https://arxiv.org/abs/2404.12580</link>
<guid>https://arxiv.org/abs/2404.12580</guid>
<content:encoded><![CDATA[
arXiv:2404.12580v2 Announce Type: replace 
Abstract: This paper introduces Interactive Tables (iTBLS), a dataset of interactive conversations that focuses on natural-language manipulation of tabular information sourced from academic pre-prints on ArXiv. The iTBLS dataset consists of three types of tabular tasks -- interpretation, modification, and generation. Interpretation focuses on tabular understanding, modification focuses on manipulating tabular information, and generation focuses on the addition of new natural-language evidence. In addition, the paper presents a novel framework that reformulates tabular operations as question-answering, where an appropriate question is formulated based on the nature of interaction and the question is answered using the user request as evidence. The developed approach results in an improvement on all tasks on a sequence-to-sequence modeling baseline on iTBLS. In addition, the question-answering-based reformulation is applied to datasets from prior work for the text-to-table task where textual paragraphs are summarized into tables. The novel approach results in up to 13% improvement in Exact-Match accuracy and up to 16% improvement in BERTScores compared to the prior state-of-the-art.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BQA: Body Language Question Answering Dataset for Video Large Language Models</title>
<link>https://arxiv.org/abs/2410.13206</link>
<guid>https://arxiv.org/abs/2410.13206</guid>
<content:encoded><![CDATA[
arXiv:2410.13206v3 Announce Type: replace 
Abstract: A large part of human communication relies on nonverbal cues such as facial expressions, eye contact, and body language. Unlike language or sign language, such nonverbal communication lacks formal rules, requiring complex reasoning based on commonsense understanding. Enabling current Video Large Language Models (VideoLLMs) to accurately interpret body language is a crucial challenge, as human unconscious actions can easily cause the model to misinterpret their intent. To address this, we propose a dataset, BQA, a body language question answering dataset, to validate whether the model can correctly interpret emotions from short clips of body language comprising 26 emotion labels of videos of body language. We evaluated various VideoLLMs on BQA and revealed that understanding body language is challenging, and our analyses of the wrong answers by VideoLLMs show that certain VideoLLMs made significantly biased answers depending on the age group and ethnicity of the individuals in the video. The dataset is available.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development of Pre-Trained Transformer-based Models for the Nepali Language</title>
<link>https://arxiv.org/abs/2411.15734</link>
<guid>https://arxiv.org/abs/2411.15734</guid>
<content:encoded><![CDATA[
arXiv:2411.15734v2 Announce Type: replace 
Abstract: Transformer-based pre-trained language models have dominated the field of Natural Language Processing (NLP) for quite some time now. However, the Nepali language, spoken by approximately 32 million people worldwide, remains significantly underrepresented in this domain. This underrepresentation is primarily attributed to the scarcity of monolingual data corpora and limited available resources for the Nepali language. While existing efforts have predominantly concentrated on basic encoder-based models, there is a notable gap in the exploration of decoder-based architectures. To address this gap, we have collected 27.5 GB of Nepali text data, approximately 2.4x larger than any previously available Nepali language corpus. Leveraging this data, we pre-trained three different models i.e., BERT, RoBERTa, and GPT-2, exclusively for the Nepali Language. Furthermore, we performed instruction tuning and explored its potential for monolingual Nepali data, providing a foundation for future research. Our models outperformed the existing best model by 2 points on Nep-gLUE benchmark, scoring 95.60 and also outperformed existing models on text generation tasks, demonstrating improvements in both understanding and generating Nepali text.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consolidating and Developing Benchmarking Datasets for the Nepali Natural Language Understanding Tasks</title>
<link>https://arxiv.org/abs/2411.19244</link>
<guid>https://arxiv.org/abs/2411.19244</guid>
<content:encoded><![CDATA[
arXiv:2411.19244v2 Announce Type: replace 
Abstract: The Nepali language has distinct linguistic features, especially its complex script (Devanagari script), morphology, and various dialects,which pose a unique challenge for Natural Language Understanding (NLU) tasks. While the Nepali Language Understanding Evaluation (Nep-gLUE) benchmark provides a foundation for evaluating models, it remains limited in scope, covering four tasks. This restricts their utility for comprehensive assessments of Natural Language Processing (NLP) models. To address this limitation, we introduce twelve new datasets, creating a new benchmark, the Nepali /Language Understanding Evaluation (NLUE) benchmark for evaluating the performance of models across a diverse set of Natural Language Understanding (NLU) tasks. The added tasks include Single-Sentence Classification, Similarity and Paraphrase Tasks, Natural Language Inference (NLI), and General Masked Evaluation Task (GMET). Through extensive experiments, we demonstrate that existing top models struggle with the added complexity of these tasks. We also find that the best multilingual model outperforms the best monolingual models across most tasks, highlighting the need for more robust solutions tailored to the Nepali language. This expanded benchmark sets a new standard for evaluating, comparing, and advancing models, contributing significantly to the broader goal of advancing NLP research for low-resource languages.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the Medical Domain</title>
<link>https://arxiv.org/abs/2412.20309</link>
<guid>https://arxiv.org/abs/2412.20309</guid>
<content:encoded><![CDATA[
arXiv:2412.20309v3 Announce Type: replace 
Abstract: Retrieval Augmented Generation (RAG) complements the knowledge of Large Language Models (LLMs) by leveraging external information to enhance response accuracy for queries. This approach is widely applied in several fields by taking its advantage of injecting the most up-to-date information, and researchers are focusing on understanding and improving this aspect to unlock the full potential of RAG in such high-stakes applications. However, despite the potential of RAG to address these needs, the mechanisms behind the confidence levels of its outputs remain underexplored. Our study focuses on the impact of RAG, specifically examining whether RAG improves the confidence of LLM outputs in the medical domain. We conduct this analysis across various configurations and models. We evaluate confidence by treating the model's predicted probability as its output and calculating several evaluation metrics which include calibration error method, entropy, the best probability, and accuracy. Experimental results across multiple datasets confirmed that certain models possess the capability to judge for themselves whether an inserted document relates to the correct answer. These results suggest that evaluating models based on their output probabilities determine whether they function as generators in the RAG framework. Our approach allows us to evaluate whether the models handle retrieved documents.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Abstraction: Harnessing Frontier Models to Structure Real-World Data at Scale</title>
<link>https://arxiv.org/abs/2502.00943</link>
<guid>https://arxiv.org/abs/2502.00943</guid>
<content:encoded><![CDATA[
arXiv:2502.00943v2 Announce Type: replace 
Abstract: A significant fraction of real-world patient information resides in unstructured clinical text. Medical abstraction extracts and normalizes key structured attributes from free-text clinical notes, which is the prerequisite for a variety of important downstream applications, including registry curation, clinical trial operations, and real-world evidence generation. Prior medical abstraction methods typically resort to building attribute-specific models, each of which requires extensive manual effort such as rule creation or supervised label annotation for the individual attribute, thus limiting scalability. In this paper, we show that existing frontier models already possess the universal abstraction capability for scaling medical abstraction to a wide range of clinical attributes. We present UniMedAbstractor (UMA), a unifying framework for zero-shot medical abstraction with a modular, customizable prompt template and the selection of any frontier large language models. Given a new attribute for abstraction, users only need to conduct lightweight prompt adaptation in UMA to adjust the specification in natural languages. Compared to traditional methods, UMA eliminates the need for attribute-specific training labels or handcrafted rules, thus substantially reducing the development time and cost. We conducted a comprehensive evaluation of UMA in oncology using a wide range of marquee attributes representing the cancer patient journey. These include relatively simple attributes typically specified within a single clinical note (e.g. performance status), as well as complex attributes requiring sophisticated reasoning across multiple notes at various time points (e.g. tumor staging). Based on a single frontier model such as GPT-4o, UMA matched or even exceeded the performance of state-of-the-art attribute-specific methods, each of which was tailored to the individual attribute.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fact or Guesswork? Evaluating Large Language Models' Medical Knowledge with Structured One-Hop Judgments</title>
<link>https://arxiv.org/abs/2502.14275</link>
<guid>https://arxiv.org/abs/2502.14275</guid>
<content:encoded><![CDATA[
arXiv:2502.14275v2 Announce Type: replace 
Abstract: Large language models (LLMs) have been widely adopted in various downstream task domains. However, their abilities to directly recall and apply factual medical knowledge remains under-explored. Most existing medical QA benchmarks assess complex reasoning or multi-hop inference, making it difficult to isolate LLMs' inherent medical knowledge from their reasoning capabilities. Given the high-stakes nature of medical applications, where incorrect information can have critical consequences, it is essential to evaluate the factuality of LLMs to retain medical knowledge.
  To address this challenge, we introduce the Medical Knowledge Judgment Dataset (MKJ), a dataset derived from the Unified Medical Language System (UMLS), a comprehensive repository of standardized biomedical vocabularies and knowledge graphs. Through a binary classification framework, MKJ evaluates LLMs' grasp of fundamental medical facts by having them assess the validity of concise, one-hop statements, enabling direct measurement of their knowledge retention capabilities.
  Our experiments reveal that LLMs have difficulty accurately recalling medical facts, with performances varying substantially across semantic types and showing notable weakness in uncommon medical conditions. Furthermore, LLMs show poor calibration, often being overconfident in incorrect answers. To mitigate these issues, we explore retrieval-augmented generation, demonstrating its effectiveness in improving factual accuracy and reducing uncertainty in medical decision-making.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Basic Category Usage in Vision Language Models</title>
<link>https://arxiv.org/abs/2503.12530</link>
<guid>https://arxiv.org/abs/2503.12530</guid>
<content:encoded><![CDATA[
arXiv:2503.12530v2 Announce Type: replace 
Abstract: The field of psychology has long recognized a basic level of categorization that humans use when labeling visual stimuli, a term coined by Rosch in 1976. This level of categorization has been found to be used most frequently, to have higher information density, and to aid in visual language tasks with priming in humans. Here, we investigate basic-level categorization in two recently released, open-source vision-language models (VLMs). This paper demonstrates that Llama 3.2 Vision Instruct (11B) and Molmo 7B-D both prefer basic-level categorization consistent with human behavior. Moreover, the models' preferences are consistent with nuanced human behaviors like the biological versus non-biological basic level effects and the well-established expert basic level shift, further suggesting that VLMs acquire complex cognitive categorization behaviors from the human data on which they are trained. We also find our expert prompting methods demonstrate lower accuracy then our non-expert prompting methods, contradicting popular thought regarding the use of expertise prompting methods.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEA-LION: Southeast Asian Languages in One Network</title>
<link>https://arxiv.org/abs/2504.05747</link>
<guid>https://arxiv.org/abs/2504.05747</guid>
<content:encoded><![CDATA[
arXiv:2504.05747v3 Announce Type: replace 
Abstract: Recently, Large Language Models (LLMs) have dominated much of the artificial intelligence scene with their ability to process and generate natural languages. However, the majority of LLM research and development remains English-centric, leaving low-resource languages such as those in the Southeast Asian (SEA) region under-represented. To address this representation gap, we introduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge multilingual LLMs designed for SEA languages. The SEA-LION family of LLMs supports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese, Malay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages large-scale multilingual continued pre-training with a comprehensive post-training regime involving multiple stages of instruction fine-tuning, alignment, and model merging. Evaluation results on multilingual benchmarks indicate that our models achieve state-of-the-art performance across LLMs supporting SEA languages. We open-source the models to benefit the wider SEA community.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models</title>
<link>https://arxiv.org/abs/2504.19061</link>
<guid>https://arxiv.org/abs/2504.19061</guid>
<content:encoded><![CDATA[
arXiv:2504.19061v2 Announce Type: replace 
Abstract: Clinical summarization is crucial in healthcare as it distills complex medical data into digestible information, enhancing patient understanding and care management. Large language models (LLMs) have shown significant potential in automating and improving the accuracy of such summarizations due to their advanced natural language understanding capabilities. These models are particularly applicable in the context of summarizing medical/clinical texts, where precise and concise information transfer is essential. In this paper, we investigate the effectiveness of open-source LLMs in extracting key events from discharge reports, including admission reasons, major in-hospital events, and critical follow-up actions. In addition, we also assess the prevalence of various types of hallucinations in the summaries produced by these models. Detecting hallucinations is vital as it directly influences the reliability of the information, potentially affecting patient care and treatment outcomes. We conduct comprehensive simulations to rigorously evaluate the performance of these models, further probing the accuracy and fidelity of the extracted content in clinical summarization. Our results reveal that while the LLMs (e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admission reasons and hospitalization events, they are generally less consistent when it comes to identifying follow-up recommendations, highlighting broader challenges in leveraging LLMs for comprehensive summarization.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors</title>
<link>https://arxiv.org/abs/2505.02850</link>
<guid>https://arxiv.org/abs/2505.02850</guid>
<content:encoded><![CDATA[
arXiv:2505.02850v2 Announce Type: replace 
Abstract: Generating high-quality MCQs, especially those targeting diverse cognitive levels and incorporating common misconceptions into distractor design, is time-consuming and expertise-intensive, making manual creation impractical at scale. Current automated approaches typically generate questions at lower cognitive levels and fail to incorporate domain-specific misconceptions. This paper presents a hierarchical concept map-based framework that provides structured knowledge to guide LLMs in generating MCQs with distractors. We chose high-school physics as our test domain and began by developing a hierarchical concept map covering major Physics topics and their interconnections with an efficient database design. Next, through an automated pipeline, topic-relevant sections of these concept maps are retrieved to serve as a structured context for the LLM to generate questions and distractors that specifically target common misconceptions. Lastly, an automated validation is completed to ensure that the generated MCQs meet the requirements provided. We evaluate our framework against two baseline approaches: a base LLM and a RAG-based generation. We conducted expert evaluations and student assessments of the generated MCQs. Expert evaluation shows that our method significantly outperforms the baseline approaches, achieving a success rate of 75.20% in meeting all quality criteria compared to approximately 37% for both baseline methods. Student assessment data reveal that our concept map-driven approach achieved a significantly lower guess success rate of 28.05% compared to 37.10% for the baselines, indicating a more effective assessment of conceptual understanding. The results demonstrate that our concept map-based approach enables robust assessment across cognitive levels and instant identification of conceptual gaps, facilitating faster feedback loops and targeted interventions at scale.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries</title>
<link>https://arxiv.org/abs/2505.09902</link>
<guid>https://arxiv.org/abs/2505.09902</guid>
<content:encoded><![CDATA[
arXiv:2505.09902v2 Announce Type: replace 
Abstract: Large language models are, by definition, based on language. In an effort to underscore the critical need for regional localized models, this paper examines primary differences between variants of written Spanish across Latin America and Spain, with an in-depth sociocultural and linguistic contextualization therein. We argue that these differences effectively constitute significant gaps in the quotidian use of Spanish among dialectal groups by creating sociolinguistic dissonances, to the extent that locale-sensitive AI models would play a pivotal role in bridging these divides. In doing so, this approach informs better and more efficient localization strategies that also serve to more adequately meet inclusivity goals, while securing sustainable active daily user growth in a major low-risk investment geographic area. Therefore, implementing at least the proposed five sub variants of Spanish addresses two lines of action: to foment user trust and reliance on AI language models while also demonstrating a level of cultural, historical, and sociolinguistic awareness that reflects positively on any internationalization strategy.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization</title>
<link>https://arxiv.org/abs/2505.10736</link>
<guid>https://arxiv.org/abs/2505.10736</guid>
<content:encoded><![CDATA[
arXiv:2505.10736v3 Announce Type: replace 
Abstract: Optimizing Large Language Model (LLM) performance requires well-crafted prompts, but manual prompt engineering is labor-intensive and often ineffective. Automated prompt optimization techniques address this challenge but the majority of them rely on randomly selected evaluation subsets, which fail to represent the full dataset, leading to unreliable evaluations and suboptimal prompts. Existing coreset selection methods, designed for LLM benchmarking, are unsuitable for prompt optimization due to challenges in clustering similar samples, high data collection costs, and the unavailability of performance data for new or private datasets. To overcome these issues, we propose IPOMP, an Iterative evaluation data selection for effective Prompt Optimization using real-time Model Performance. IPOMP is a two-stage approach that selects representative and diverse samples using semantic clustering and boundary analysis, followed by iterative refinement with real-time model performance data to replace redundant samples. Evaluations on the BIG-bench dataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by at least 57% compared with SOTA baselines, with minimal computational overhead below 1%. Furthermore, the results demonstrate that our real-time performance-guided refinement approach can be universally applied to enhance existing coreset selection methods.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs</title>
<link>https://arxiv.org/abs/2505.14226</link>
<guid>https://arxiv.org/abs/2505.14226</guid>
<content:encoded><![CDATA[
arXiv:2505.14226v2 Announce Type: replace 
Abstract: Recently released LLMs have strong multilingual \& multimodal capabilities. Model vulnerabilities are exposed using audits and red-teaming efforts. Existing efforts have focused primarily on the English language; thus, models continue to be susceptible to multilingual jailbreaking strategies, especially for multimodal contexts. In this study, we introduce a novel strategy that leverages code-mixing and phonetic perturbations to jailbreak LLMs for both text and image generation tasks. We also introduce \textit{two new} jailbreak strategies that show higher effectiveness than baselines. Our work presents a method to effectively bypass safety filters in LLMs while maintaining interpretability by applying phonetic misspellings to sensitive words in code-mixed prompts. We achieve a 99\% Attack Success Rate for text generation and 78\% for image generation, with Attack Relevance Rate of 100\% for text generation and 95\% for image generation for the phonetically perturbed code-mixed prompts. Our interpretability experiments reveal that phonetic perturbations impact word tokenization, leading to jailbreak success. Our study motivates increasing the focus towards more generalizable safety alignment for multilingual multimodal models, especially in real-world settings wherein prompts can have misspelt words. \textit{\textbf{Warning: This paper contains examples of potentially harmful and offensive content.}}
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration</title>
<link>https://arxiv.org/abs/2505.24688</link>
<guid>https://arxiv.org/abs/2505.24688</guid>
<content:encoded><![CDATA[
arXiv:2505.24688v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) struggle with complex reasoning due to limited diversity and inefficient search. We propose Soft Reasoning, an embedding-based search framework that optimises the embedding of the first token to guide generation. It combines (1) embedding perturbation for controlled exploration and (2) Bayesian optimisation to refine embeddings via a verifier-guided objective, balancing exploration and exploitation. This approach improves reasoning accuracy and coherence while avoiding reliance on heuristic search. Experiments demonstrate superior correctness with minimal computation, making it a scalable, model-agnostic solution. The code is released at https://github.com/alickzhu/Soft-Reasoning.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PlantDeBERTa: An Open Source Language Model for Plant Science</title>
<link>https://arxiv.org/abs/2506.08897</link>
<guid>https://arxiv.org/abs/2506.08897</guid>
<content:encoded><![CDATA[
arXiv:2506.08897v4 Announce Type: replace 
Abstract: The rapid advancement of transformer-based language models has catalyzed breakthroughs in biomedical and clinical natural language processing; however, plant science remains markedly underserved by such domain-adapted tools. In this work, we present PlantDeBERTa, a high-performance, open-source language model specifically tailored for extracting structured knowledge from plant stress-response literature. Built upon the DeBERTa architecture-known for its disentangled attention and robust contextual encoding-PlantDeBERTa is fine-tuned on a meticulously curated corpus of expert-annotated abstracts, with a primary focus on lentil (Lens culinaris) responses to diverse abiotic and biotic stressors. Our methodology combines transformer-based modeling with rule-enhanced linguistic post-processing and ontology-grounded entity normalization, enabling PlantDeBERTa to capture biologically meaningful relationships with precision and semantic fidelity. The underlying corpus is annotated using a hierarchical schema aligned with the Crop Ontology, encompassing molecular, physiological, biochemical, and agronomic dimensions of plant adaptation. PlantDeBERTa exhibits strong generalization capabilities across entity types and demonstrates the feasibility of robust domain adaptation in low-resource scientific fields.By providing a scalable and reproducible framework for high-resolution entity recognition, PlantDeBERTa bridges a critical gap in agricultural NLP and paves the way for intelligent, data-driven systems in plant genomics, phenomics, and agronomic knowledge discovery. Our model is publicly released to promote transparency and accelerate cross-disciplinary innovation in computational plant science.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Utility Judgment Framework via LLMs Inspired by Relevance in Philosophy</title>
<link>https://arxiv.org/abs/2406.11290</link>
<guid>https://arxiv.org/abs/2406.11290</guid>
<content:encoded><![CDATA[
arXiv:2406.11290v2 Announce Type: replace-cross 
Abstract: Relevance and utility are two frequently used measures to evaluate the effectiveness of an information retrieval (IR) system. Relevance emphasizes the aboutness of a result to a query, while utility refers to the result's usefulness or value to an information seeker. In Retrieval-Augmented Generation (RAG), high-utility results should be prioritized to feed to LLMs due to their limited input bandwidth. Re-examining RAG's three core components -- relevance ranking derived from retrieval models, utility judgments, and answer generation -- aligns with Schutz's philosophical system of relevances, which encompasses three types of relevance representing different levels of human cognition that enhance each other. These three RAG components also reflect three cognitive levels for LLMs in question-answering. Therefore, we propose an Iterative utiliTy judgmEnt fraMework (ITEM) to promote each step in RAG. We conducted extensive experiments on retrieval (TREC DL, WebAP), utility judgment task (GTI-NQ), and factoid question-answering (NQ) datasets. Experimental results demonstrate significant improvements of ITEM in utility judgments, ranking, and answer generation upon representative baselines.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2410.02458</link>
<guid>https://arxiv.org/abs/2410.02458</guid>
<content:encoded><![CDATA[
arXiv:2410.02458v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentation by integrating pre-trained LLM transformer blocks. Our approach, which incorporates a frozen LLM transformer block into the encoder of a ViT-based model, leads to substantial improvements in segmentation performance across various medical imaging modalities. We propose a Hybrid Attention Mechanism that combines global and local feature learning with a Multi-Scale Fusion Block for aggregating features across different scales. The enhanced model shows significant performance gains, including an average Dice score increase from 0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index. These results demonstrate the effectiveness of LLM-based transformers in refining medical image segmentation, highlighting their potential to significantly boost model accuracy and robustness. The source code and our implementation are available at: https://github.com/AS-Lab/Marthi-et-al-2025-MedVisionLlama-Pre-Trained-LLM-Layers-to-Enhance-Medical-Image-Segmentation
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation for Design Space Exploration</title>
<link>https://arxiv.org/abs/2411.05844</link>
<guid>https://arxiv.org/abs/2411.05844</guid>
<content:encoded><![CDATA[
arXiv:2411.05844v3 Announce Type: replace-cross 
Abstract: GraphRAG integrates (knowledge) graphs with large language models (LLMs) to improve reasoning accuracy and contextual relevance. Despite its promising applications and strong relevance to multiple research communities, such as databases and natural language processing, GraphRAG currently lacks modular workflow analysis, systematic solution frameworks, and insightful empirical studies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework that enables: 1) fine-grained decomposition of the GraphRAG workflow, 2) systematic classification of existing techniques and implemented GraphRAG instances, and 3) creation of new GraphRAG instances. Our framework facilitates comprehensive empirical studies of GraphRAG on large-scale real-world graphs and diverse query sets, revealing insights into balancing reasoning quality, runtime efficiency, and token or GPU cost, that are essential for building advanced GraphRAG systems.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GoAI: Enhancing AI Students' Learning Paths and Idea Generation via Graph of AI Ideas</title>
<link>https://arxiv.org/abs/2503.08549</link>
<guid>https://arxiv.org/abs/2503.08549</guid>
<content:encoded><![CDATA[
arXiv:2503.08549v2 Announce Type: replace-cross 
Abstract: With the rapid advancement of artificial intelligence technology, AI students are confronted with a significant "information-to-innovation" gap: they must navigate through the rapidly expanding body of literature, trace the development of a specific research field, and synthesize various techniques into feasible innovative concepts. An additional critical step for students is to identify the necessary prerequisite knowledge and learning paths. Although many approaches based on large language models (LLMs) can summarize the content of papers and trace the development of a field through citations, these methods often overlook the prerequisite knowledge involved in the papers and the rich semantic information embedded in the citation relationships between papers. Such information reveals how methods are interrelated, built upon, extended, or challenged. To address these limitations, we propose GoAI, a tool for constructing educational knowledge graphs from AI research papers that leverages these graphs to plan personalized learning paths and support creative ideation. The nodes in the knowledge graph we have built include papers and the prerequisite knowledge, such as concepts, skills, and tools, that they involve; the edges record the semantic information of citations. When a student queries a specific paper, a beam search-based path search method can trace the current development trends of the field from the queried paper and plan a learning path toward cutting-edge objectives. The integrated Idea Studio guides students to clarify problem statements, compare alternative designs, and provide formative feedback on novelty, clarity, feasibility, and alignment with learning objectives.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling</title>
<link>https://arxiv.org/abs/2504.05216</link>
<guid>https://arxiv.org/abs/2504.05216</guid>
<content:encoded><![CDATA[
arXiv:2504.05216v3 Announce Type: replace-cross 
Abstract: Dense retrieval is a crucial task in Information Retrieval (IR), serving as the basis for downstream tasks such as re-ranking and augmenting generation. Recently, large language models (LLMs) have demonstrated impressive semantic understanding capabilities, making them attractive to researchers focusing on dense retrieval. While LLMs, as decoder-style generative models, excel in language generation, they often fall short in modeling global information due to a lack of attention to subsequent tokens. Drawing inspiration from the classical word-based language modeling approach for IR, specifically the query likelihood (QL) model, we aim to leverage the generative strengths of LLMs through QL maximization. Rather than employing QL estimation for document ranking, we propose an auxiliary task of QL maximization to enhance the backbone for subsequent contrastive learning of the retriever. We introduce our model, LLM-QL, which incorporates two key components: Attention Block (AB) and Document Corruption (DC). AB blocks the attention of predictive tokens to the document tokens before the document's ending token, while DC corrupts a document by masking a portion of its tokens during prediction. Evaluations on the in-domain (MS MARCO) and out-of-domain dataset (BEIR) indicate LLM-QL's superiority over other LLM-based retrievers. Furthermore, comprehensive analyses also validate the efficacy of LLM-QL and its components.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quiet Feature Learning in Algorithmic Tasks</title>
<link>https://arxiv.org/abs/2505.03997</link>
<guid>https://arxiv.org/abs/2505.03997</guid>
<content:encoded><![CDATA[
arXiv:2505.03997v2 Announce Type: replace-cross 
Abstract: We train Transformer-based language models on ten foundational algorithmic tasks and observe pronounced phase transitions in their loss curves that deviate from established power-law scaling trends. Over large ranges of compute, the validation loss barely improves, then abruptly decreases. Probing the models' internal representations reveals that quiet features are learned prior to any decrease in task loss. These quiet features represent intermediate algorithmic computations that do not by themselves improve the output loss. Ablation experiments demonstrate that individual quiet features are causally necessary for task performance. Our results demonstrate that substantial representational progress can remain hidden beneath an apparently flat loss curve, challenging the prevailing use of cross-entropy as a proxy for learning and motivating richer diagnostics for monitoring model training.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy</title>
<link>https://arxiv.org/abs/2508.04349</link>
<guid>https://arxiv.org/abs/2508.04349</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, Large Language Model, Dynamic Entropy Weighting, Group Token Policy Optimization, Sequence-Level Group Relative Policy Optimization

Summary:
Reinforcement learning with algorithms like GRPO is enhanced in Large Language Models for reasoning, but struggles with coarse credit assignment. This paper introduces Dynamic Entropy Weighting to address this limitation. By assigning entropy-weighted rewards to individual tokens using GTPO and sequences based on average token entropy using GRPO-S, the proposed approach outperforms the DAPO baseline. Results show that the entropy-weighting mechanism plays a crucial role in improving performance, offering a promising way to enhance deep reasoning in models. <div>
arXiv:2508.04349v4 Announce Type: replace 
Abstract: Reinforcement learning (RL) with algorithms like Group Relative Policy Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is limited by a coarse-grained credit assignment that applies a uniform reward to all tokens in a sequence. This is a major flaw in long-chain reasoning tasks. This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea is that high-entropy tokens in correct responses can guide the policy toward a higher performance ceiling. This allows us to create more fine-grained reward signals for precise policy updates via two ways: 1) \textbf{Group Token Policy Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted reward to each sequence based on its average token entropy. Experiments show our methods significantly outperform the strong DAPO baseline. The results confirm that our entropy-weighting mechanism is the key driver of this performance boost, offering a better path to enhance deep reasoning in models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Language Geometry: Constructing a Metric Space from LLM Weights</title>
<link>https://arxiv.org/abs/2508.11676</link>
<guid>https://arxiv.org/abs/2508.11676</guid>
<content:encoded><![CDATA[
<div> framework, Large Language Models, metric space, language characteristics, linguistic families <br />
Summary: 
The article introduces a new framework that uses internal weight activations of Large Language Models (LLMs) to create a metric space of languages. Instead of relying on hand-crafted linguistic features, the method automatically generates high-dimensional vector representations by calculating weight importance scores through a custom pruning algorithm. These representations capture inherent language traits that reflect linguistic phenomena. The approach is validated across various datasets and multilingual LLMs, encompassing 106 languages. The results not only align with established linguistic families but also uncover unexpected connections between languages, potentially indicating historical contact or language evolution. The source code, language latent vectors, and visualization tool are openly accessible at a dedicated GitHub repository. <br /><br />Summary: <div>
arXiv:2508.11676v1 Announce Type: new 
Abstract: We introduce a novel framework that utilizes the internal weight activations of modern Large Language Models (LLMs) to construct a metric space of languages. Unlike traditional approaches based on hand-crafted linguistic features, our method automatically derives high-dimensional vector representations by computing weight importance scores via an adapted pruning algorithm. Our approach captures intrinsic language characteristics that reflect linguistic phenomena. We validate our approach across diverse datasets and multilingual LLMs, covering 106 languages. The results align well with established linguistic families while also revealing unexpected inter-language connections that may indicate historical contact or language evolution. The source code, computed language latent vectors, and visualization tool are made publicly available at https://github.com/mshamrai/deep-language-geometry.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can we Evaluate RAGs with Synthetic Data?</title>
<link>https://arxiv.org/abs/2508.11758</link>
<guid>https://arxiv.org/abs/2508.11758</guid>
<content:encoded><![CDATA[
<div> question-answer data, large language models, synthetic benchmarks, retriever parameters, generator architectures

Summary: 
The study explores the use of synthetic question-answer data generated by large language models as a substitute for human-labeled benchmarks. Two experiments were conducted to assess the reliability of synthetic benchmarks across different scenarios. The results showed that synthetic benchmarks can effectively rank retrievable answer-generation models based on retriever configuration, aligning well with human-labeled benchmarks. However, the rankings were inconsistent when comparing different generator architectures. This inconsistency may be due to task mismatch between synthetic and human benchmarks, as well as stylistic bias that favors certain generators. The study indicates that while synthetic benchmarks can be reliable proxies in some cases, they may not always provide consistent results, highlighting the importance of considering various factors when evaluating the performance of language models. 

<br /><br />Summary: <div>
arXiv:2508.11758v1 Announce Type: new 
Abstract: We investigate whether synthetic question-answer (QA) data generated by large language models (LLMs) can serve as an effective proxy for human-labeled benchmarks when such data is unavailable. We assess the reliability of synthetic benchmarks across two experiments: one varying retriever parameters while keeping the generator fixed, and another varying the generator with fixed retriever parameters. Across four datasets, of which two open-domain and two proprietary, we find that synthetic benchmarks reliably rank the RAGs varying in terms of retriever configuration, aligning well with human-labeled benchmark baselines. However, they fail to produce consistent RAG rankings when comparing generator architectures. The breakdown possibly arises from a combination of task mismatch between the synthetic and human benchmarks, and stylistic bias favoring certain generators.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Limitation Learning: Catching Adverse Dialog with GAIL</title>
<link>https://arxiv.org/abs/2508.11767</link>
<guid>https://arxiv.org/abs/2508.11767</guid>
<content:encoded><![CDATA[
<div> learning, conversation, policy, imitation, discriminator <br />
Summary: <br />
In this study, imitation learning is applied to conversation to create a policy for interacting with users based on expert demonstrations. The developed policy can effectively engage in dialogue with users given a prompt. Additionally, a discriminator is trained to differentiate between expert and synthetic conversations, revealing limitations in dialog models. This approach can be utilized to detect problematic behavior in various data models used for dialog-based tasks. The results highlight the potential of imitation learning in improving conversational agents and understanding the performance of dialog models in different scenarios. <div>
arXiv:2508.11767v1 Announce Type: new 
Abstract: Imitation learning is a proven method for creating a policy in the absence of rewards, by leveraging expert demonstrations. In this work, we apply imitation learning to conversation. In doing so, we recover a policy capable of talking to a user given a prompt (input state), and a discriminator capable of classifying between expert and synthetic conversation. While our policy is effective, we recover results from our discriminator that indicate the limitations of dialog models. We argue that this technique can be used to identify adverse behavior of arbitrary data models common for dialog oriented tasks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Transcription Normalization in the Faetar ASR Benchmark</title>
<link>https://arxiv.org/abs/2508.11771</link>
<guid>https://arxiv.org/abs/2508.11771</guid>
<content:encoded><![CDATA[
<div> Keywords: transcription inconsistencies, automatic speech recognition, low-resource benchmark, language modelling, finite lexicon

Summary: 
transcription inconsistencies in Faetar Automatic Speech Recognition benchmark were examined, with a hand-constructed lexicon showing they are not the main challenge. Bigram word-based language modelling did not provide additional benefit, but decoding constraints to a finite lexicon were found to be beneficial. The task remains highly challenging. <div>
arXiv:2508.11771v1 Announce Type: new 
Abstract: We examine the role of transcription inconsistencies in the Faetar Automatic Speech Recognition benchmark, a challenging low-resource ASR benchmark. With the help of a small, hand-constructed lexicon, we conclude that find that, while inconsistencies do exist in the transcriptions, they are not the main challenge in the task. We also demonstrate that bigram word-based language modelling is of no added benefit, but that constraining decoding to a finite lexicon can be beneficial. The task remains extremely difficult.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Task Evaluation of LLMs' Processing of Academic Text Input</title>
<link>https://arxiv.org/abs/2508.11779</link>
<guid>https://arxiv.org/abs/2508.11779</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, scientific discovery, academic peer review, text processing, performance evaluation 

Summary:
Large language models (LLMs) are debated for their effectiveness in aiding scientific discovery and academic peer review. This study evaluates LLMs' ability to process academic text through four tasks: content reproduction, comparison, scoring, and reflection. It assesses tasks such as summarizing, paraphrasing, ranking texts, grading academic work, and providing qualitative reflections. Testing Google's Gemini LLM using top Information Systems articles, the study finds that while it can produce reliable summaries and paraphrases, its ability to rank texts and grade academic work is limited. The LLM's reflections are self-consistent but lack meaningful insights. This evidence suggests caution in relying on LLMs for constructing peer reviews, as their text-processing capabilities may not meet the desired standards. The study's rigorous evaluation includes metric-based linguistic assessment, comparisons to ground truth, and human evaluation.endforeach

<br /><br />Summary: 
- LLMs have limitations in assisting scientific discovery and peer review.
- Four tasks were used to evaluate LLMs' text processing abilities.
- Gemini LLM's performance was compromised in tasks such as ranking texts and grading academic work.
- The LLM's reflections were self-consistent but lacked meaningful insights.
- Caution is advised against unchecked use of LLMs in peer review construction. <div>
arXiv:2508.11779v1 Announce Type: new 
Abstract: How much large language models (LLMs) can aid scientific discovery, notably in assisting academic peer review, is in heated debate. Between a literature digest and a human-comparable research assistant lies their practical application potential. We organize individual tasks that computer science studies employ in separate terms into a guided and robust workflow to evaluate LLMs' processing of academic text input. We employ four tasks in the assessment: content reproduction/comparison/scoring/reflection, each demanding a specific role of the LLM (oracle/judgmental arbiter/knowledgeable arbiter/collaborator) in assisting scholarly works, and altogether testing LLMs with questions that increasingly require intellectual capabilities towards a solid understanding of scientific texts to yield desirable solutions. We exemplify a rigorous performance evaluation with detailed instructions on the prompts. Adopting first-rate Information Systems articles at three top journals as the input texts and an abundant set of text metrics, we record a compromised performance of the leading LLM - Google's Gemini: its summary and paraphrase of academic text is acceptably reliable; using it to rank texts through pairwise text comparison is faintly scalable; asking it to grade academic texts is prone to poor discrimination; its qualitative reflection on the text is self-consistent yet hardly insightful to inspire meaningful research. This evidence against an endorsement of LLMs' text-processing capabilities is consistent across metric-based internal (linguistic assessment), external (comparing to the ground truth), and human evaluation, and is robust to the variations of the prompt. Overall, we do not recommend an unchecked use of LLMs in constructing peer reviews.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText</title>
<link>https://arxiv.org/abs/2508.11816</link>
<guid>https://arxiv.org/abs/2508.11816</guid>
<content:encoded><![CDATA[
<div> Keywords: CLEF 2025, SimpleText Task 1, scientific text simplification, large language models, coherence<br />
Summary:<br />
This paper discusses the approach for the CLEF 2025 SimpleText Task 1, focusing on sentence-level and document-level scientific text simplification. To simplify sentences, large language models (LLMs) are used to create a structured plan and drive the simplification process. Utilizing LLMs at the document level results in producing concise summaries, which then guide the simplification process. This two-stage framework, based on LLMs, ensures the simplified text maintains coherence and is contextually accurate. By employing this methodology, the study aims to achieve more coherent and faithful simplifications of scientific texts in both sentence and document forms. <div>
arXiv:2508.11816v1 Announce Type: new 
Abstract: In this paper, we present our approach for the CLEF 2025 SimpleText Task 1, which addresses both sentence-level and document-level scientific text simplification. For sentence-level simplification, our methodology employs large language models (LLMs) to first generate a structured plan, followed by plan-driven simplification of individual sentences. At the document level, we leverage LLMs to produce concise summaries and subsequently guide the simplification process using these summaries. This two-stage, LLM-based framework enables more coherent and contextually faithful simplifications of scientific text.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText</title>
<link>https://arxiv.org/abs/2508.11823</link>
<guid>https://arxiv.org/abs/2508.11823</guid>
<content:encoded><![CDATA[
<div> Keywords: CLEF 2025, SimpleText Task 2, scientific text simplification, ensemble framework, large language model

Summary: 
- The paper presents a methodology for the CLEF 2025 SimpleText Task 2 that focuses on detecting creative generation and information distortion in scientific text simplification.
- Multiple strategies are integrated, including a BERT-based classifier, semantic similarity measure, natural language inference model, and large language model reasoning.
- An ensemble framework is constructed to combine these signals using meta-classifiers for robust detection of spurious and distortion in text.
- An LLM-based post-editing system is utilized for grounded generation, revising simplifications based on original input texts.
- The approach aims to enhance the accuracy and effectiveness of detecting and evaluating creative generation and information distortion in scientific text simplification.<br /><br />Summary: <div>
arXiv:2508.11823v1 Announce Type: new 
Abstract: In this paper, we describe our methodology for the CLEF 2025 SimpleText Task 2, which focuses on detecting and evaluating creative generation and information distortion in scientific text simplification. Our solution integrates multiple strategies: we construct an ensemble framework that leverages BERT-based classifier, semantic similarity measure, natural language inference model, and large language model (LLM) reasoning. These diverse signals are combined using meta-classifiers to enhance the robustness of spurious and distortion detection. Additionally, for grounded generation, we employ an LLM-based post-editing system that revises simplifications based on the original input texts.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Idiom Datasets for Psycholinguistic and Computational Research</title>
<link>https://arxiv.org/abs/2508.11828</link>
<guid>https://arxiv.org/abs/2508.11828</guid>
<content:encoded><![CDATA[
<div> Keywords: idioms, figurative expressions, psycholinguistics, computational linguistics, datasets

Summary: 
This survey examines datasets related to idioms, which are challenging for both computational and human studies due to their non-literal meanings. In psycholinguistics, datasets typically include ratings on factors like familiarity and compositionality. Meanwhile, computational resources support tasks such as idiomaticity detection and paraphrasing. The survey highlights a lack of connection between psycholinguistic and computational research on idioms, despite recent efforts to broaden language coverage and task variety. This disconnect suggests an opportunity for future collaboration and integration of findings from both disciplines. <div>
arXiv:2508.11828v1 Announce Type: new 
Abstract: Idioms are figurative expressions whose meanings often cannot be inferred from their individual words, making them difficult to process computationally and posing challenges for human experimental studies. This survey reviews datasets developed in psycholinguistics and computational linguistics for studying idioms, focusing on their content, form, and intended use. Psycholinguistic resources typically contain normed ratings along dimensions such as familiarity, transparency, and compositionality, while computational datasets support tasks like idiomaticity detection/classification, paraphrasing, and cross-lingual modeling. We present trends in annotation practices, coverage, and task framing across 53 datasets. Although recent efforts expanded language coverage and task diversity, there seems to be no relation yet between psycholinguistic and computational research on idioms.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions</title>
<link>https://arxiv.org/abs/2508.11829</link>
<guid>https://arxiv.org/abs/2508.11829</guid>
<content:encoded><![CDATA[
<div> Keywords: AI systems, frame problem, hormonal cycles, relevance filters, language models

Summary: 
AI systems face challenges in determining contextually relevant information from a vast possibility space, known as the frame problem. This study proposes using biological rhythms, such as menstrual and circadian cycles, as natural relevance filters to address this issue. By incorporating simulated hormonal cycles into Large Language Models through system prompts based on key hormones like estrogen, testosterone, and cortisol, the researchers observed emotional and stylistic variations that corresponded to different biological phases. For instance, sadness peaked during menstruation, while happiness prevailed during ovulation, and circadian patterns showed a shift from morning optimism to nocturnal introspection. Benchmarking on various datasets indicated subtle yet consistent performance differences in alignment with biological expectations, emphasizing optimal function within moderate hormonal ranges. Furthermore, this approach unveiled how societal biases related to gender and biology are ingrained within language models. <div>
arXiv:2508.11829v1 Announce Type: new 
Abstract: Despite significant advances, AI systems struggle with the frame problem: determining what information is contextually relevant from an exponentially large possibility space. We hypothesize that biological rhythms, particularly hormonal cycles, serve as natural relevance filters that could address this fundamental challenge. We develop a framework that embeds simulated menstrual and circadian cycles into Large Language Models through system prompts generated from periodic functions modeling key hormones including estrogen, testosterone, and cortisol. Across multiple state-of-the-art models, linguistic analysis reveals emotional and stylistic variations that track biological phases; sadness peaks during menstruation while happiness dominates ovulation and circadian patterns show morning optimism transitioning to nocturnal introspection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates subtle but consistent performance variations aligning with biological expectations, including optimal function in moderate rather than extreme hormonal ranges. This methodology provides a novel approach to contextual AI while revealing how societal biases regarding gender and biology are embedded within language models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection</title>
<link>https://arxiv.org/abs/2508.11831</link>
<guid>https://arxiv.org/abs/2508.11831</guid>
<content:encoded><![CDATA[
<div> Keywords: euphemisms, cross-lingual transfer, fine-tuning, multilingual models, low-resource languages

Summary: 
Sequential fine-tuning was investigated for euphemism detection in five languages: English, Spanish, Chinese, Turkish, and Yoruba. Results showed that using a high-resource L1 language for sequential fine-tuning improved performance in low-resource languages like Yoruba and Turkish. XLM-R showed greater improvement but was more sensitive to pretraining gaps and catastrophic forgetting, while mBERT provided more stable results. Sequential fine-tuning emerged as an effective strategy for enhancing euphemism detection in multilingual models, particularly beneficial for low-resource languages. <div>
arXiv:2508.11831v1 Announce Type: new 
Abstract: Euphemisms are culturally variable and often ambiguous, posing challenges for language models, especially in low-resource settings. This paper investigates how cross-lingual transfer via sequential fine-tuning affects euphemism detection across five languages: English, Spanish, Chinese, Turkish, and Yoruba. We compare sequential fine-tuning with monolingual and simultaneous fine-tuning using XLM-R and mBERT, analyzing how performance is shaped by language pairings, typological features, and pretraining coverage. Results show that sequential fine-tuning with a high-resource L1 improves L2 performance, especially for low-resource languages like Yoruba and Turkish. XLM-R achieves larger gains but is more sensitive to pretraining gaps and catastrophic forgetting, while mBERT yields more stable, though lower, results. These findings highlight sequential fine-tuning as a simple yet effective strategy for improving euphemism detection in multilingual models, particularly when low-resource languages are involved.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance</title>
<link>https://arxiv.org/abs/2508.11857</link>
<guid>https://arxiv.org/abs/2508.11857</guid>
<content:encoded><![CDATA[
<div> Tokenization, SupraTok, subword segmentation, multi-word semantic units, entropy-driven data curation<br />
Summary:<br />
SupraTok introduces a novel tokenization approach that enhances subword segmentation by learning "superword" tokens. This architecture incorporates cross-boundary pattern learning, entropy-driven data curation, and multi-phase curriculum learning to optimize tokenization efficiency. Compared to existing tokenizers, SupraTok shows significant improvement in English tokenization efficiency and maintains competitive performance across multiple languages. Integration with a GPT-2 scale model results in improved performance on benchmark tasks without requiring architectural modifications. While these results are promising, further validation at larger model scales is necessary. This study suggests that efficient tokenization strategies can complement advancements in model architectures to enhance language model performance.<br /><br />Summary: <div>
arXiv:2508.11857v1 Announce Type: new 
Abstract: Tokenization remains a fundamental yet underexplored bottleneck in natural language processing, with strategies largely static despite remarkable progress in model architectures. We present SupraTok, a novel tokenization architecture that reimagines subword segmentation through three innovations: cross-boundary pattern learning that discovers multi-word semantic units, entropy-driven data curation that optimizes training corpus quality, and multi-phase curriculum learning for stable convergence. Our approach extends Byte-Pair Encoding by learning "superword" tokens, coherent multi-word expressions that preserve semantic unity while maximizing compression efficiency. SupraTok achieves 31% improvement in English tokenization efficiency (5.91 versus 4.51 characters per token) compared to OpenAI's o200k tokenizer and 30% improvement over Google's Gemma 3 tokenizer (256k vocabulary), while maintaining competitive performance across 38 languages. When integrated with a GPT-2 scale model (124M parameters) trained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4% improvement on HellaSWAG and 9.5% on MMLU benchmarks without architectural modifications. While these results are promising at this scale, further validation at larger model scales is needed. These findings suggest that efficient tokenization can complement architectural innovations as a path to improved language model performance.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning</title>
<link>https://arxiv.org/abs/2508.11889</link>
<guid>https://arxiv.org/abs/2508.11889</guid>
<content:encoded><![CDATA[
<div> Emotion recognition, conversation, large language models, instruction tuning, InitERC<br />
Summary: InitERC is a new framework for emotion recognition in conversation that aims to align speaker characteristics, contextual cues, and emotion states. It uses in-context instruction tuning to adapt large language models to learn this alignment from context examples. The framework consists of demonstration pool construction, in-context example selection, prompt template design, and in-context instruction tuning. The impact of factors like retrieval strategy, example ordering, and the number of examples on performance is studied. Extensive experiments on three datasets show that InitERC outperforms existing baselines significantly. <br /><br /> <div>
arXiv:2508.11889v1 Announce Type: new 
Abstract: Emotion recognition in conversation (ERC) aims to identify the emotion of each utterance in a conversation, playing a vital role in empathetic artificial intelligence. With the growing of large language models (LLMs), instruction tuning has emerged as a critical paradigm for ERC. Existing studies mainly focus on multi-stage instruction tuning, which first endows LLMs with speaker characteristics, and then conducts context-aware instruction tuning to comprehend emotional states. However, these methods inherently constrains the capacity to jointly capture the dynamic interaction between speaker characteristics and conversational context, resulting in weak alignment among speaker identity, contextual cues, and emotion states within a unified framework. In this paper, we propose InitERC, a simple yet effective one-stage in-context instruction tuning framework for ERC. InitERC adapts LLMs to learn speaker-context-emotion alignment from context examples via in-context instruction tuning. Specifically, InitERC comprises four components, i.e., demonstration pool construction, in-context example selection, prompt template design, and in-context instruction tuning. To explore the impact of in-context examples, we conduct a comprehensive study on three key factors: retrieval strategy, example ordering, and the number of examples. Extensive experiments on three widely used datasets demonstrate that our proposed InitERC achieves substantial improvements over the state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures</title>
<link>https://arxiv.org/abs/2508.11915</link>
<guid>https://arxiv.org/abs/2508.11915</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Game-theoretic interactions, Conversational Robustness Evaluation Score, Zipf's Law, Heaps' Law

Summary:
The study introduces the Conversational Robustness Evaluation Score (CORE) as a metric to evaluate language interactions in multi-agent systems involving Large Language Models (LLMs). By analyzing dialogues in competitive, cooperative, and neutral settings, the researchers found that language use in cooperative environments showed higher vocabulary expansion and more lexical repetition compared to competitive settings. This was reflected in steeper Zipf distributions and higher Heaps exponents in cooperative interactions. In contrast, competitive dialogues exhibited less repetition and more constrained vocabularies. The results suggest that social incentives influence language adaptation in multi-agent systems. The study also provides insights into word frequency distributions and vocabulary growth, shedding light on how linguistic diversity is influenced by different interaction dynamics. The introduced CORE metric serves as a robust diagnostic tool for assessing linguistic robustness in LLM systems. <div>
arXiv:2508.11915v1 Announce Type: new 
Abstract: Game-theoretic interactions between agents with Large Language Models (LLMs) have revealed many emergent capabilities, yet the linguistic diversity of these interactions has not been sufficiently quantified. In this paper, we present the Conversational Robustness Evaluation Score: CORE, a metric to quantify the effectiveness of language use within multi-agent systems across different game-theoretic interactions. CORE integrates measures of cluster entropy, lexical repetition, and semantic similarity, providing a direct lens of dialog quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative, and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws to characterize word frequency distributions and vocabulary growth. Our findings show that cooperative settings exhibit both steeper Zipf distributions and higher Heap exponents, indicating more repetition alongside greater vocabulary expansion. In contrast, competitive interactions display lower Zipf and Heaps exponents, reflecting less repetition and more constrained vocabularies. These results provide new insights into how social incentives influence language adaptation, and highlight CORE as a robust diagnostic for measuring linguistic robustness in multi-agent LLM systems. Our code is available at https://github.com/psyonp/core.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese</title>
<link>https://arxiv.org/abs/2508.11927</link>
<guid>https://arxiv.org/abs/2508.11927</guid>
<content:encoded><![CDATA[
<div> Keywords: perfect aspect, Chinese, Japanese, Natural Language Inference, temporal semantics

Summary:
The study focuses on the lack of distinct forms for tense within the perfect aspect in Chinese and Japanese, compared to English. A linguistically motivated NLI dataset was created for each language, consisting of 1,350 pairs. Experimental results showed that advanced LLMs face challenges in temporal inference, especially in identifying subtle tense and reference-time shifts. The findings point out the limitations of current models and emphasize the importance of cross-linguistic evaluation in temporal semantics. The dataset for this study is openly available at https://github.com/Lujie2001/CrossNLI.<br /><br />Summary: The study explores the perfect aspect in Chinese and Japanese, highlighting the complexities in temporal inference for NLI tasks. Experiments demonstrate the struggle of advanced LLMs in detecting subtle tense and reference-time shifts, revealing the need for improved models and cross-linguistic evaluation in temporal semantics. The dataset created for this research provides valuable resources for further studies in this area. <div>
arXiv:2508.11927v1 Announce Type: new 
Abstract: Unlike English, which uses distinct forms (e.g., had, has, will have) to mark the perfect aspect across tenses, Chinese and Japanese lack separate grammatical forms for tense within the perfect aspect, which complicates Natural Language Inference (NLI). Focusing on the perfect aspect in these languages, we construct a linguistically motivated, template-based NLI dataset (1,350 pairs per language). Experiments reveal that even advanced LLMs struggle with temporal inference, particularly in detecting subtle tense and reference-time shifts. These findings highlight model limitations and underscore the need for cross-linguistic evaluation in temporal semantics. Our dataset is available at https://github.com/Lujie2001/CrossNLI.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection</title>
<link>https://arxiv.org/abs/2508.11933</link>
<guid>https://arxiv.org/abs/2508.11933</guid>
<content:encoded><![CDATA[
<div> Keywords: machine-generated text, Large Language Models, detection, linguistic features, adversarial framework

Summary:
CAMF is a Collaborative Adversarial Multi-agent Framework designed to detect machine-generated text from Large Language Models (LLMs), addressing challenges in existing zero-shot detection methods. The framework utilizes multiple agents in a three-phase process: Multi-dimensional Linguistic Feature Extraction, Adversarial Consistency Probing, and Synthesized Judgment Aggregation. CAMF aims to analyze subtle textual incongruities across linguistic dimensions such as style, semantics, and logic, indicative of non-human origin. Empirical evaluations show CAMF's superiority over current zero-shot MGT detection techniques, offering a more comprehensive and effective approach to identifying machine-generated text. <div>
arXiv:2508.11933v1 Announce Type: new 
Abstract: Detecting machine-generated text (MGT) from contemporary Large Language Models (LLMs) is increasingly crucial amid risks like disinformation and threats to academic integrity. Existing zero-shot detection paradigms, despite their practicality, often exhibit significant deficiencies. Key challenges include: (1) superficial analyses focused on limited textual attributes, and (2) a lack of investigation into consistency across linguistic dimensions such as style, semantics, and logic. To address these challenges, we introduce the \textbf{C}ollaborative \textbf{A}dversarial \textbf{M}ulti-agent \textbf{F}ramework (\textbf{CAMF}), a novel architecture using multiple LLM-based agents. CAMF employs specialized agents in a synergistic three-phase process: \emph{Multi-dimensional Linguistic Feature Extraction}, \emph{Adversarial Consistency Probing}, and \emph{Synthesized Judgment Aggregation}. This structured collaborative-adversarial process enables a deep analysis of subtle, cross-dimensional textual incongruities indicative of non-human origin. Empirical evaluations demonstrate CAMF's significant superiority over state-of-the-art zero-shot MGT detection techniques.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases</title>
<link>https://arxiv.org/abs/2508.12031</link>
<guid>https://arxiv.org/abs/2508.12031</guid>
<content:encoded><![CDATA[
<div> memory replay, contrastive learning, cognitive biases, Large Language Models, continual relation extraction 

Summary:
- The study introduces an instruction-based continual contrastive tuning approach for Large Language Models (LLMs) in Continual Relation Extraction (CRE) to address cognitive biases.
- Unlike existing methods, this approach divides training and memory data into two parts based on initial responses' correctness, treating them differently through dual-task fine-tuning.
- The proposed instruction-based contrastive tuning strategy for LLMs aims to continuously correct cognitive biases by leveraging the model's instruction-following ability.
- Experimental evaluation on TACRED and FewRel datasets shows that the model achieves new state-of-the-art CRE performance with significant improvements.
- The approach emphasizes the importance of specializing in exploiting error cases to bridge the gap between old and new relations for LLMs. 

<br /><br />Summary: <div>
arXiv:2508.12031v1 Announce Type: new 
Abstract: Continual Relation Extraction (CRE) aims to continually learn new emerging relations while avoiding catastrophic forgetting. Existing CRE methods mainly use memory replay and contrastive learning to mitigate catastrophic forgetting. However, these methods do not attach importance to the error cases that can reveal the model's cognitive biases more effectively. To address this issue, we propose an instruction-based continual contrastive tuning approach for Large Language Models (LLMs) in CRE. Different from existing CRE methods that typically handle the training and memory data in a unified manner, this approach splits the training and memory data of each task into two parts respectively based on the correctness of the initial responses and treats them differently through dual-task fine-tuning. In addition, leveraging the advantages of LLM's instruction-following ability, we propose a novel instruction-based contrastive tuning strategy for LLM to continuously correct current cognitive biases with the guidance of previous data in an instruction-tuning manner, which mitigates the gap between old and new relations in a more suitable way for LLMs. We experimentally evaluate our model on TACRED and FewRel, and the results show that our model achieves new state-of-the-art CRE performance with significant improvements, demonstrating the importance of specializing in exploiting error cases.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation</title>
<link>https://arxiv.org/abs/2508.12040</link>
<guid>https://arxiv.org/abs/2508.12040</guid>
<content:encoded><![CDATA[
<div> confidence estimation, language models, fine-grained, trustworthiness, reliability

Summary:<br />
- Large language models (LLMs) lack self-awareness and often exhibit overconfidence in their predictions, emphasizing the need for accurate confidence estimation.
- The FineCE method introduces a novel approach for fine-grained confidence estimation during text generation, utilizing a comprehensive training data pipeline and supervised model training.
- The Backward Confidence Integration (BCI) strategy leverages subsequent text information to enhance confidence estimation during the generation process.
- Three strategies are proposed to identify optimal positions for confidence estimation within text generation.
- Extensive experiments show that FineCE outperforms existing confidence estimation methods, enhancing the trustworthiness and reliability of LLM-generated outputs. <div>
arXiv:2508.12040v1 Announce Type: new 
Abstract: While large language models (LLMs) have demonstrated remarkable performance across diverse tasks, they fundamentally lack self-awareness and frequently exhibit overconfidence, assigning high confidence scores to incorrect predictions. Accurate confidence estimation is therefore critical for enhancing the trustworthiness and reliability of LLM-generated outputs. However, existing approaches suffer from coarse-grained scoring mechanisms that fail to provide fine-grained, continuous confidence estimates throughout the generation process. To address these limitations, we introduce FineCE, a novel confidence estimation method that delivers accurate, fine-grained confidence scores during text generation. Specifically, we first develop a comprehensive pipeline for constructing training data that effectively captures the underlying probabilistic distribution of LLM responses, and then train a model to predict confidence scores for arbitrary text sequences in a supervised manner. Furthermore, we propose a Backward Confidence Integration (BCI) strategy that leverages information from the subsequent text to enhance confidence estimation for the current sequence during inference. We also introduce three strategies for identifying optimal positions to perform confidence estimation within the generation process. Extensive experiments on multiple benchmark datasets demonstrate that FineCE consistently outperforms existing classical confidence estimation methods. Our code and all baselines used in the paper are available on GitHub.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs</title>
<link>https://arxiv.org/abs/2508.12086</link>
<guid>https://arxiv.org/abs/2508.12086</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, adaptation, multi-objective optimization, Jacobian-based method, structured reasoning

Summary:
In the realm of large language model adaptation, striking a balance between improving factuality and increasing confidence can be challenging due to the complex interactions between prompt parameters. Existing strategies often overlook the deeper geometric relationships between objectives and parameters. To address this, the J6 method is introduced, which decomposes the gradient interaction matrix into six components, allowing for both hard decision-making and soft strategies in adapting to conflicting or synergistic scenarios. This structured approach not only aids in understanding parameter attribution and task interference but also enables a dynamic update framework tailored to local conflicts. By incorporating structured Jacobian reasoning, this work paves the way for conflict-aware prompt optimization and provides a principled mechanism for multi-objective neural tuning. 

<br /><br />Summary: <div>
arXiv:2508.12086v1 Announce Type: new 
Abstract: In large language model (LLM) adaptation, balancing multiple optimization objectives such as improving factuality (heat) and increasing confidence (via low entropy) poses a fundamental challenge, especially when prompt parameters (e.g., hidden-layer insertions h and embedding modifications w) interact in non-trivial ways. Existing multi-objective optimization strategies often rely on scalar gradient aggregation, ignoring the deeper geometric structure between objectives and parameters. We propose J6, a structured Jacobian-based method that decomposes the gradient interaction matrix into six interpretable components. This decomposition enables both hard decision-making (e.g., choosing the dominant update direction via argmax) and soft strategies (e.g., attention-style weighting via softmax over J6), forming a dynamic update framework that adapts to local conflict and synergy. Moreover, the interpretable structure of J6 provides insight into parameter attribution, task interference, and geometry-aligned adaptation. Our work introduces a principled and extensible mechanism for conflict-aware prompt optimization, and opens a new avenue for incorporating structured Jacobian reasoning into multi-objective neural tuning.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples</title>
<link>https://arxiv.org/abs/2508.12096</link>
<guid>https://arxiv.org/abs/2508.12096</guid>
<content:encoded><![CDATA[
<div> Evaluation, large language models, structured transition evaluation method, significant transition samples, Qwen3 model family

Summary:
The article introduces a new evaluation framework called STEM for large language models (LLMs). STEM aims to address challenges in accurately assessing LLM capabilities by identifying significant transition samples (STS) that reflect consistent performance changes among models. By analyzing these STS, STEM can estimate the relative capabilities of different LLMs effectively. The framework is lightweight, interpretable, and architecture-agnostic, making it a practical and scalable method for evaluating LLMs. The Qwen3 model family is used to construct a pool of STS on six different benchmarks to assess generalizability. Experimental results show that STEM reliably captures performance trends and aligns with ground-truth rankings of model capabilities, demonstrating its effectiveness in evaluating LLMs in a fine-grained manner. Overall, STEM provides a valuable tool for researchers and developers to compare and assess the capabilities of LLMs efficiently. 

<br /><br />Summary: <div>
arXiv:2508.12096v1 Announce Type: new 
Abstract: Evaluating large language models (LLMs) has become increasingly challenging as model capabilities advance rapidly. While recent models often achieve higher scores on standard benchmarks, these improvements do not consistently reflect enhanced real-world reasoning capabilities. Moreover, widespread overfitting to public benchmarks and the high computational cost of full evaluations have made it both expensive and less effective to distinguish meaningful differences between models. To address these challenges, we propose the \textbf{S}tructured \textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweight and interpretable evaluation framework for efficiently estimating the relative capabilities of LLMs. STEM identifies \textit{significant transition samples} (STS) by analyzing consistent performance transitions among LLMs of the same architecture but varying parameter scales. These samples enable STEM to effectively estimate the capability position of an unknown model. Qwen3 model family is applied to construct the STS pool on six diverse and representative benchmarks. To assess generalizability. Experimental results indicate that STEM reliably captures performance trends, aligns with ground-truth rankings of model capability. These findings highlight STEM as a practical and scalable method for fine-grained, architecture-agnostic evaluation of LLMs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality</title>
<link>https://arxiv.org/abs/2508.12140</link>
<guid>https://arxiv.org/abs/2508.12140</guid>
<content:encoded><![CDATA[
<div> evaluation, thinking budget mechanisms, medical reasoning tasks, scaling laws, computational resources

Summary:
The study evaluates thinking budget mechanisms in medical reasoning tasks, revealing scaling laws between computational resources and reasoning quality. Two major model families, Qwen3 and DeepSeek-R1, were systematically evaluated across 15 medical datasets. Logarithmic scaling relationships were established between accuracy improvements, thinking budget, and model size. Three efficiency regimes were identified: high-efficiency, balanced, and high-accuracy. Smaller models showed larger benefits from extended thinking, suggesting a complementary relationship. Domain-specific patterns emerged, with neurology and gastroenterology requiring deeper reasoning processes. The study validates the generalizability of thinking budget concepts across architectures. Thinking budget control is established as a critical mechanism for optimizing medical AI systems, enabling dynamic resource allocation aligned with clinical needs while maintaining transparency for healthcare deployment. 

<br /><br />Summary: <div>
arXiv:2508.12140v1 Announce Type: new 
Abstract: This study presents the first comprehensive evaluation of thinking budget mechanisms in medical reasoning tasks, revealing fundamental scaling laws between computational resources and reasoning quality. We systematically evaluated two major model families, Qwen3 (1.7B to 235B parameters) and DeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning diverse specialties and difficulty levels. Through controlled experiments with thinking budgets ranging from zero to unlimited tokens, we establish logarithmic scaling relationships where accuracy improvements follow a predictable pattern with both thinking budget and model size. Our findings identify three distinct efficiency regimes: high-efficiency (0 to 256 tokens) suitable for real-time applications, balanced (256 to 512 tokens) offering optimal cost-performance tradeoffs for routine clinical support, and high-accuracy (above 512 tokens) justified only for critical diagnostic tasks. Notably, smaller models demonstrate disproportionately larger benefits from extended thinking, with 15 to 20% improvements compared to 5 to 10% for larger models, suggesting a complementary relationship where thinking budget provides greater relative benefits for capacity-constrained models. Domain-specific patterns emerge clearly, with neurology and gastroenterology requiring significantly deeper reasoning processes than cardiovascular or respiratory medicine. The consistency between Qwen3 native thinking budget API and our proposed truncation method for DeepSeek-R1 validates the generalizability of thinking budget concepts across architectures. These results establish thinking budget control as a critical mechanism for optimizing medical AI systems, enabling dynamic resource allocation aligned with clinical needs while maintaining the transparency essential for healthcare deployment.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data</title>
<link>https://arxiv.org/abs/2508.12158</link>
<guid>https://arxiv.org/abs/2508.12158</guid>
<content:encoded><![CDATA[
<div> LLMs, privacy evaluation, NLP, human perceptions, textual data
Summary: 
The article addresses the challenge of accurately evaluating privacy in Natural Language Processing (NLP). It proposes using Language Models (LLMs) as privacy evaluators, inspired by their success in other NLP tasks. The study investigates the effectiveness of the "LLM-as-a-Judge" paradigm in assessing the privacy sensitivity of textual data. Results from 10 datasets, 13 LLMs, and 677 human survey participants reveal the subjective and complex nature of privacy. Despite low inter-human agreement rates, LLMs demonstrate the ability to model global human perspectives on privacy. Analyzing human and LLM reasoning patterns highlights the strengths and limitations of LLMs as privacy evaluators. The findings suggest that LLMs can play a role in addressing privacy challenges and offer insights for future research on leveraging LLMs for privacy evaluation in textual data.
Summary: <div>
arXiv:2508.12158v1 Announce Type: new 
Abstract: Despite advances in the field of privacy-preserving Natural Language Processing (NLP), a significant challenge remains the accurate evaluation of privacy. As a potential solution, using LLMs as a privacy evaluator presents a promising approach $\unicode{x2013}$ a strategy inspired by its success in other subfields of NLP. In particular, the so-called $\textit{LLM-as-a-Judge}$ paradigm has achieved impressive results on a variety of natural language evaluation tasks, demonstrating high agreement rates with human annotators. Recognizing that privacy is both subjective and difficult to define, we investigate whether LLM-as-a-Judge can also be leveraged to evaluate the privacy sensitivity of textual data. Furthermore, we measure how closely LLM evaluations align with human perceptions of privacy in text. Resulting from a study involving 10 datasets, 13 LLMs, and 677 human survey participants, we confirm that privacy is indeed a difficult concept to measure empirically, exhibited by generally low inter-human agreement rates. Nevertheless, we find that LLMs can accurately model a global human privacy perspective, and through an analysis of human and LLM reasoning patterns, we discuss the merits and limitations of LLM-as-a-Judge for privacy evaluation in textual data. Our findings pave the way for exploring the feasibility of LLMs as privacy evaluators, addressing a core challenge in solving pressing privacy issues with innovative technical solutions.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges</title>
<link>https://arxiv.org/abs/2508.12227</link>
<guid>https://arxiv.org/abs/2508.12227</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Machine Learning, Arabic language, datasets, applications, challenges, research gaps

Summary: 
This paper presents a survey on Arabic Multimodal Machine Learning (MML), categorizing research efforts into datasets, applications, approaches, and challenges. The taxonomy offers a structured overview of the current state of Arabic MML, highlighting areas for further investigation and critical research gaps. The survey aims to empower researchers to build on existing opportunities and address challenges to advance the field of Arabic MML. The growing maturity of Arabic MML underscores the need for comprehensive analysis and exploration to drive innovation and progress in complex tasks such as sentiment analysis, emotion recognition, and multimedia retrieval. The insights provided in this survey offer a valuable resource for researchers seeking to contribute to the development and advancement of Arabic MML. 

<br /><br />Summary: <div>
arXiv:2508.12227v1 Announce Type: new 
Abstract: Multimodal Machine Learning (MML) aims to integrate and analyze information from diverse modalities, such as text, audio, and visuals, enabling machines to address complex tasks like sentiment analysis, emotion recognition, and multimedia retrieval. Recently, Arabic MML has reached a certain level of maturity in its foundational development, making it time to conduct a comprehensive survey. This paper explores Arabic MML by categorizing efforts through a novel taxonomy and analyzing existing research. Our taxonomy organizes these efforts into four key topics: datasets, applications, approaches, and challenges. By providing a structured overview, this survey offers insights into the current state of Arabic MML, highlighting areas that have not been investigated and critical research gaps. Researchers will be empowered to build upon the identified opportunities and address challenges to advance the field.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEA-BED: Southeast Asia Embedding Benchmark</title>
<link>https://arxiv.org/abs/2508.12243</link>
<guid>https://arxiv.org/abs/2508.12243</guid>
<content:encoded><![CDATA[
<div> Embedding models, Sentence embeddings, Southeast Asia, Benchmark, Evaluation<br />
Summary:<br />
- The article introduces the SEA-BED benchmark, the first large-scale embedding benchmark for Southeast Asia with 169 datasets across 9 tasks and 10 languages.
- It aims to determine challenging SEA languages and tasks, identify unique performance gaps globally, and analyze the impact of human vs. machine translations on evaluation.
- 17 embedding models are evaluated across six studies, revealing varying model performance among SEA languages, the significance of human-curated datasets for low-resource languages like Burmese, and the importance of diverse benchmarks in addressing linguistic properties.
- Results demonstrate substantial ranking shifts, inconsistencies in model performance, and the necessity of human involvement in dataset curation for accurate evaluation.
- Overall, SEA-BED provides a valuable resource for NLP tasks in Southeast Asia and sheds light on the importance of region-specific benchmarks for understanding linguistic properties and fostering performance improvements in multilingual models.<br /> 

Summary: <div>
arXiv:2508.12243v1 Announce Type: new 
Abstract: Sentence embeddings are essential for NLP tasks such as semantic search, re-ranking, and textual similarity. Although multilingual benchmarks like MMTEB broaden coverage, Southeast Asia (SEA) datasets are scarce and often machine-translated, missing native linguistic properties. With nearly 700 million speakers, the SEA region lacks a region-specific embedding benchmark. We introduce SEA-BED, the first large-scale SEA embedding benchmark with 169 datasets across 9 tasks and 10 languages, where 71% are formulated by humans, not machine generation or translation. We address three research questions: (1) which SEA languages and tasks are challenging, (2) whether SEA languages show unique performance gaps globally, and (3) how human vs. machine translations affect evaluation. We evaluate 17 embedding models across six studies, analyzing task and language challenges, cross-benchmark comparisons, and translation trade-offs. Results show sharp ranking shifts, inconsistent model performance among SEA languages, and the importance of human-curated datasets for low-resource languages like Burmese.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What do Speech Foundation Models Learn? Analysis and Applications</title>
<link>https://arxiv.org/abs/2508.12255</link>
<guid>https://arxiv.org/abs/2508.12255</guid>
<content:encoded><![CDATA[
<div> framework, statistical tools, SFM, spoken language understanding, NER <br />
Summary:
This study introduces a framework utilizing statistical tools to analyze the knowledge encapsulated in Speech Foundation Models (SFMs) for speech-processing tasks. The research compares various SFMs and highlights the impact of analytical insights on task performance. Additionally, the study expands into Spoken Language Understanding (SLU) tasks like Named Entity Recognition (NER) and Localization (NEL), filling the gap in dataset availability for SLU studies. By developing SFM-based approaches for NER and NEL tasks, this study shows that end-to-end (E2E) models leveraging SFMs can outperform traditional models. Evaluating E2E SLU models across different SFMs and adaptation strategies further sheds light on the impact on task performance. Overall, this research addresses crucial gaps in the understanding of SFMs and provides valuable tools and datasets for future model development and adoption. <br /> <div>
arXiv:2508.12255v1 Announce Type: new 
Abstract: Speech foundation models (SFMs) are designed to serve as general-purpose representations for a wide range of speech-processing tasks. The last five years have seen an influx of increasingly successful self-supervised and supervised pre-trained models with impressive performance on various downstream tasks.
  Although the zoo of SFMs continues to grow, our understanding of the knowledge they acquire lags behind. This thesis presents a lightweight analysis framework using statistical tools and training-free tasks to investigate the acoustic and linguistic knowledge encoded in SFM layers. We conduct a comparative study across multiple SFMs and statistical tools. Our study also shows that the analytical insights have concrete implications for downstream task performance.
  The effectiveness of an SFM is ultimately determined by its performance on speech applications. Yet it remains unclear whether the benefits extend to spoken language understanding (SLU) tasks that require a deeper understanding than widely studied ones, such as speech recognition. The limited exploration of SLU is primarily due to a lack of relevant datasets. To alleviate that, this thesis contributes tasks, specifically spoken named entity recognition (NER) and named entity localization (NEL), to the Spoken Language Understanding Evaluation benchmark. We develop SFM-based approaches for NER and NEL, and find that end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded (speech recognition followed by a text model) approaches. Further, we evaluate E2E SLU models across SFMs and adaptation strategies to assess the impact on task performance.
  Collectively, this thesis tackles previously unanswered questions about SFMs, providing tools and datasets to further our understanding and to enable the community to make informed design choices for future model development and adoption.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework</title>
<link>https://arxiv.org/abs/2508.12257</link>
<guid>https://arxiv.org/abs/2508.12257</guid>
<content:encoded><![CDATA[
<div> Keywords: AI systems, text-to-structure techniques, datasets, assessment criteria, evaluation framework<br />
<br />
Summary: This systematic review explores the transformation of unstructured text into structured formats such as tables, knowledge graphs, and charts to facilitate the evolution of AI systems towards agentic operation and context-aware retrieval. The study examines various text-to-structure techniques, identifies challenges, evaluates existing datasets and assessment criteria, and proposes potential research directions. A universal evaluation framework for structured outputs is introduced to provide a comprehensive assessment of text-to-structure conversion processes. The research emphasizes the importance of text-to-structure as foundational infrastructure for next-generation AI systems, highlighting its significance in enabling critical applications like summarization and data mining. <div>
arXiv:2508.12257v1 Announce Type: new 
Abstract: The evolution of AI systems toward agentic operation and context-aware retrieval necessitates transforming unstructured text into structured formats like tables, knowledge graphs, and charts. While such conversions enable critical applications from summarization to data mining, current research lacks a comprehensive synthesis of methodologies, datasets, and metrics. This systematic review examines text-to-structure techniques and the encountered challenges, evaluates current datasets and assessment criteria, and outlines potential directions for future research. We also introduce a universal evaluation framework for structured outputs, establishing text-to-structure as foundational infrastructure for next-generation AI systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast, Slow, and Tool-augmented Thinking for LLMs: A Review</title>
<link>https://arxiv.org/abs/2508.12265</link>
<guid>https://arxiv.org/abs/2508.12265</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reasoning strategies, adaptive reasoning, cognitive psychology, external tools <br />
Summary: <br />
The article discusses the advancements in Large Language Models (LLMs) and the need for adaptive reasoning strategies for addressing real-world tasks effectively. Drawing inspiration from cognitive psychology, the authors propose a taxonomy of LLM reasoning strategies based on fast/slow and internal/external knowledge boundaries. They survey recent work on adaptive reasoning in LLMs and categorize methods according to key decision factors. The article highlights the importance of adapting reasoning strategies to different problem demands, ranging from intuitive responses to deliberate reasoning and tool-augmented thinking. Finally, the authors outline open challenges and future directions for developing more adaptive, efficient, and reliable LLMs. <br /> <div>
arXiv:2508.12265v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in reasoning across diverse domains. However, effective reasoning in real-world tasks requires adapting the reasoning strategy to the demands of the problem, ranging from fast, intuitive responses to deliberate, step-by-step reasoning and tool-augmented thinking. Drawing inspiration from cognitive psychology, we propose a novel taxonomy of LLM reasoning strategies along two knowledge boundaries: a fast/slow boundary separating intuitive from deliberative processes, and an internal/external boundary distinguishing reasoning grounded in the model's parameters from reasoning augmented by external tools. We systematically survey recent work on adaptive reasoning in LLMs and categorize methods based on key decision factors. We conclude by highlighting open challenges and future directions toward more adaptive, efficient, and reliable LLMs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution</title>
<link>https://arxiv.org/abs/2508.12277</link>
<guid>https://arxiv.org/abs/2508.12277</guid>
<content:encoded><![CDATA[
<div> Evaluation, Large language models, Self-Execution Benchmark, model performance, model representation<br />
Summary:<br />
The study examines the ability of Large Language Models (LLMs) to predict aspects of their own responses using the Self-Execution Benchmark. This benchmark measures a model's capability to anticipate properties of its output, such as difficulty in answering questions, refusal to answer, or likely associations. The experiments reveal that LLMs generally struggle on this benchmark, with increased model size or capability not consistently improving performance. This indicates a fundamental limitation in how LLMs represent and reason about their own behavior. <div>
arXiv:2508.12277v1 Announce Type: new 
Abstract: Large language models (LLMs) are commonly evaluated on tasks that test their knowledge or reasoning abilities. In this paper, we explore a different type of evaluation: whether an LLM can predict aspects of its own responses. Since LLMs lack the ability to execute themselves, we introduce the Self-Execution Benchmark, which measures a model's ability to anticipate properties of its output, such as whether a question will be difficult for it, whether it will refuse to answer, or what kinds of associations it is likely to produce. Our experiments show that models generally perform poorly on this benchmark, and that increased model size or capability does not consistently lead to better performance. These results suggest a fundamental limitation in how LLMs represent and reason about their own behavior.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Legal$\Delta$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain</title>
<link>https://arxiv.org/abs/2508.12281</link>
<guid>https://arxiv.org/abs/2508.12281</guid>
<content:encoded><![CDATA[
<div> Keywords: Legal Artificial Intelligence, Large Language Models, Legal$\Delta$, reinforcement learning, interpretability

Summary:
Legal Artificial Intelligence (LegalAI) has made strides in automating judicial decision-making using Large Language Models (LLMs). However, existing legal LLMs struggle to provide reliable and interpretable reasoning processes, often resorting to fast-thinking behavior. To tackle this issue, the authors introduce Legal$\Delta$, a reinforcement learning framework that enhances legal reasoning through a chain-of-thought guided information gain approach. The framework trains on a dual-mode input setup and focuses on maximizing information gain between direct answer and reasoning-augmented modes, fostering meaningful reasoning patterns over superficial explanations. Legal$\Delta$ utilizes a two-stage approach involving distilling latent reasoning capabilities from a Large Reasoning Model (LRM) and refining reasoning quality through differential comparisons and multidimensional reward mechanisms. Experimental results show that Legal$\Delta$ surpasses strong baselines in accuracy and interpretability on various legal reasoning tasks, generating more robust and trustworthy legal judgments without the need for labeled preference data.<br /><br />Summary: <div>
arXiv:2508.12281v1 Announce Type: new 
Abstract: Legal Artificial Intelligence (LegalAI) has achieved notable advances in automating judicial decision-making with the support of Large Language Models (LLMs). However, existing legal LLMs still struggle to generate reliable and interpretable reasoning processes. They often default to fast-thinking behavior by producing direct answers without explicit multi-step reasoning, limiting their effectiveness in complex legal scenarios that demand rigorous justification. To address this challenge, we propose Legal$\Delta$, a reinforcement learning framework designed to enhance legal reasoning through chain-of-thought guided information gain. During training, Legal$\Delta$ employs a dual-mode input setup-comprising direct answer and reasoning-augmented modes-and maximizes the information gain between them. This encourages the model to acquire meaningful reasoning patterns rather than generating superficial or redundant explanations. Legal$\Delta$ follows a two-stage approach: (1) distilling latent reasoning capabilities from a powerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning quality via differential comparisons, combined with a multidimensional reward mechanism that assesses both structural coherence and legal-domain specificity. Experimental results on multiple legal reasoning tasks demonstrate that Legal$\Delta$ outperforms strong baselines in both accuracy and interpretability. It consistently produces more robust and trustworthy legal judgments without relying on labeled preference data. All code and data will be released at https://github.com/NEUIR/LegalDelta.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.12282</link>
<guid>https://arxiv.org/abs/2508.12282</guid>
<content:encoded><![CDATA[
<div> Keywords: ChronoQA, Chinese question answering, temporal reasoning, Retrieval-Augmented Generation, benchmark dataset

Summary: 
ChronoQA is a new benchmark dataset for Chinese question answering that focuses on evaluating temporal reasoning in Retrieval-Augmented Generation systems. It consists of over 300,000 news articles from 2019 to 2024 and includes 5,176 high-quality questions covering various temporal types. The dataset supports both single- and multi-document scenarios, providing a real-world context for temporal alignment and logical consistency. ChronoQA features detailed structural annotations and has undergone rigorous validation to ensure data quality. By offering a dynamic and scalable resource, ChronoQA facilitates structured evaluation across different temporal tasks and serves as a robust benchmark for improving time-sensitive retrieval-augmented question answering systems. <div>
arXiv:2508.12282v1 Announce Type: new 
Abstract: We introduce ChronoQA, a large-scale benchmark dataset for Chinese question answering, specifically designed to evaluate temporal reasoning in Retrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over 300,000 news articles published between 2019 and 2024, and contains 5,176 high-quality questions covering absolute, aggregate, and relative temporal types with both explicit and implicit time expressions. The dataset supports both single- and multi-document scenarios, reflecting the real-world requirements for temporal alignment and logical consistency. ChronoQA features comprehensive structural annotations and has undergone multi-stage validation, including rule-based, LLM-based, and human evaluation, to ensure data quality. By providing a dynamic, reliable, and scalable resource, ChronoQA enables structured evaluation across a wide range of temporal tasks, and serves as a robust benchmark for advancing time-sensitive retrieval-augmented question answering systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction</title>
<link>https://arxiv.org/abs/2508.12286</link>
<guid>https://arxiv.org/abs/2508.12286</guid>
<content:encoded><![CDATA[
<div> Probation Prediction, Intelligent Judicial Assistant System, Legal Logic, Deep Learning, Multi-Task Dual-Theory Probation Prediction Model<br />
Summary:<br />
Probation is essential in criminal law, but current systems lack dedicated methods for prediction. Research is limited on factors affecting probation eligibility, which requires analyzing criminal circumstances and remorse. This study proposes an approach integrating legal logic into deep learning models for probation prediction. A specialized dataset with probation legal elements is used to design the Multi-Task Dual-Theory Probation Prediction Model (MT-DT), based on the Dual-Track Theory of Punishment. Experiments show the MT-DT model outperforms baseline models, with legal logic analysis confirming its effectiveness. <div>
arXiv:2508.12286v1 Announce Type: new 
Abstract: Probation is a crucial institution in modern criminal law, embodying the principles of fairness and justice while contributing to the harmonious development of society. Despite its importance, the current Intelligent Judicial Assistant System (IJAS) lacks dedicated methods for probation prediction, and research on the underlying factors influencing probation eligibility remains limited. In addition, probation eligibility requires a comprehensive analysis of both criminal circumstances and remorse. Much of the existing research in IJAS relies primarily on data-driven methodologies, which often overlooks the legal logic underpinning judicial decision-making. To address this gap, we propose a novel approach that integrates legal logic into deep learning models for probation prediction, implemented in three distinct stages. First, we construct a specialized probation dataset that includes fact descriptions and probation legal elements (PLEs). Second, we design a distinct probation prediction model named the Multi-Task Dual-Theory Probation Prediction Model (MT-DT), which is grounded in the legal logic of probation and the \textit{Dual-Track Theory of Punishment}. Finally, our experiments on the probation dataset demonstrate that the MT-DT model outperforms baseline models, and an analysis of the underlying legal logic further validates the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CarelessWhisper: Turning Whisper into a Causal Streaming Model</title>
<link>https://arxiv.org/abs/2508.12301</link>
<guid>https://arxiv.org/abs/2508.12301</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Speech Recognition, transformer encoder-decoder model, streaming transcription, Low-Rank Adaptation, low-latency chunk sizes

Summary: 
The article introduces a method to transform a transformer encoder-decoder model into a low-latency streaming model for Automatic Speech Recognition (ASR). Existing state-of-the-art models such as OpenAI Whisper and NVIDIA Canary excel in offline transcription but are not suitable for real-time transcription due to architecture and training limitations. The proposed method fine-tunes the encoder and decoder using Low-Rank Adaptation (LoRA) and a weakly aligned dataset, converting the encoder to be causal. This modification allows for improved performance in low-latency scenarios (less than 300 msec) with lower complexity compared to non-fine-tuned streaming approaches. The inference mechanism using greedy and beam-search decoding is shown to be locally optimal. The training process also results in better alignment, enabling the extraction of word-level timestamps. The code and fine-tuned models are made available to support further research and development in streaming ASR.

<br /><br />Summary: <div>
arXiv:2508.12301v1 Announce Type: new 
Abstract: Automatic Speech Recognition (ASR) has seen remarkable progress, with models like OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA) performance in offline transcription. However, these models are not designed for streaming (online or real-time) transcription, due to limitations in their architecture and training methodology. We propose a method to turn the transformer encoder-decoder model into a low-latency streaming model that is careless about future context. We present an analysis explaining why it is not straightforward to convert an encoder-decoder transformer to a low-latency streaming model. Our proposed method modifies the existing (non-causal) encoder to a causal encoder by fine-tuning both the encoder and decoder using Low-Rank Adaptation (LoRA) and a weakly aligned dataset. We then propose an updated inference mechanism that utilizes the fine-tune causal encoder and decoder to yield greedy and beam-search decoding, and is shown to be locally optimal. Experiments on low-latency chunk sizes (less than 300 msec) show that our fine-tuned model outperforms existing non-fine-tuned streaming approaches in most cases, while using a lower complexity. Additionally, we observe that our training process yields better alignment, enabling a simple method for extracting word-level timestamps. We release our training and inference code, along with the fine-tuned models, to support further research and development in streaming ASR.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering</title>
<link>https://arxiv.org/abs/2508.12355</link>
<guid>https://arxiv.org/abs/2508.12355</guid>
<content:encoded><![CDATA[
<div> conflict-aware, Multi-Answer Question Answering, large language models, NATCONFQA, fact-checking datasets <br />
<br />
Summary: <br />
Large Language Models (LLMs) have shown success in question answering tasks but struggle with Multi-Answer Question Answering (MAQA) due to conflicting answers. Constructing datasets with conflicting answers is costly, so a new benchmark called NATCONFQA has been introduced. It requires models not only to identify all valid answers but also to detect conflicting answer pairs. This benchmark is enriched with detailed conflict labels. Eight high-end LLMs were evaluated on NATCONFQA, highlighting their challenges in handling various conflict types and their flawed resolution strategies. This research aims to advance understanding in conflict-aware MAQA settings. <div>
arXiv:2508.12355v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated strong performance in question answering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a question may have several valid answers, remains challenging. Traditional QA settings often assume consistency across evidences, but MAQA can involve conflicting answers. Constructing datasets that reflect such conflicts is costly and labor-intensive, while existing benchmarks often rely on synthetic data, restrict the task to yes/no questions, or apply unverified automated annotation. To advance research in this area, we extend the conflict-aware MAQA setting to require models not only to identify all valid answers, but also to detect specific conflicting answer pairs, if any. To support this task, we introduce a novel cost-effective methodology for leveraging fact-checking datasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware MAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate eight high-end LLMs on NATCONFQA, revealing their fragility in handling various types of conflicts and the flawed strategies they employ to resolve them.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models</title>
<link>https://arxiv.org/abs/2508.12387</link>
<guid>https://arxiv.org/abs/2508.12387</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, language models, reasoning, autonomy, generalization
Summary:
- The paper introduces ReaLM, a reinforcement learning framework for enhancing the performance of Small Language Models (SLMs) in vertical domains.
- To improve reasoning capability, Multi-Route Process Verification (MRPV) is proposed to extract decisive patterns by contrasting positive and negative reasoning paths.
- Enabling Autonomy via Asymptotic Induction (EAAI) is introduced to reduce reliance on external guidance and improve the autonomy of SLMs by gradually fading external signals during training.
- Generalization is improved through guided chain-of-thought distillation, which encodes domain-specific rules and expert knowledge into SLM parameters, aiding in better generalization.
- Extensive experiments on vertical and general reasoning tasks show that ReaLM significantly enhances SLM performance in terms of reasoning capability, autonomy, and generalization. 

<br /><br />Summary: <div>
arXiv:2508.12387v1 Announce Type: new 
Abstract: Small Language Models (SLMs) are a cost-effective alternative to Large Language Models (LLMs), but often struggle with complex reasoning due to their limited capacity and a tendency to produce mistakes or inconsistent answers during multi-step reasoning. Existing efforts have improved SLM performance, but typically at the cost of one or more of three key aspects: (1) reasoning capability, due to biased supervision that filters out negative reasoning paths and limits learning from errors; (2) autonomy, due to over-reliance on externally generated reasoning signals; and (3) generalization, which suffers when models overfit to teacher-specific patterns. In this paper, we introduce ReaLM, a reinforcement learning framework for robust and self-sufficient reasoning in vertical domains. To enhance reasoning capability, we propose Multi-Route Process Verification (MRPV), which contrasts both positive and negative reasoning paths to extract decisive patterns. To reduce reliance on external guidance and improve autonomy, we introduce Enabling Autonomy via Asymptotic Induction (EAAI), a training strategy that gradually fades external signals. To improve generalization, we apply guided chain-of-thought distillation to encode domain-specific rules and expert knowledge into SLM parameters, making them part of what the model has learned. Extensive experiments on both vertical and general reasoning tasks demonstrate that ReaLM significantly improves SLM performance across aspects (1)-(3) above.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph</title>
<link>https://arxiv.org/abs/2508.12393</link>
<guid>https://arxiv.org/abs/2508.12393</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, medical literature, temporal dynamics, Large Language Models, biomedical knowledge<br />
<br />
Summary: MedKGent is a framework for constructing temporally evolving medical Knowledge Graphs (KGs) using over 10 million PubMed abstracts. It employs two specialized agents, the Extractor Agent and Constructor Agent, powered by the Qwen2.5-32B-Instruct model to identify knowledge triples, assign confidence scores, and integrate them into the evolving graph. The resulting KG, containing 156,275 entities and 2,971,384 relational triples, demonstrates high accuracy and utility in medical question answering tasks. Quality assessments by state-of-the-art Large Language Models and domain experts show accuracy approaching 90% with strong agreement. The KG also proves valuable in drug repurposing through confidence-aware causal inference. This framework addresses limitations in existing KG construction methods by considering the temporal dynamics and contextual uncertainty of evolving biomedical knowledge.  <div>
arXiv:2508.12393v1 Announce Type: new 
Abstract: The rapid expansion of medical literature presents growing challenges for structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs) offer a promising solution by enabling efficient retrieval, automated reasoning, and knowledge discovery. However, current KG construction methods often rely on supervised pipelines with limited generalizability or naively aggregate outputs from Large Language Models (LLMs), treating biomedical corpora as static and ignoring the temporal dynamics and contextual uncertainty of evolving knowledge. To address these limitations, we introduce MedKGent, a LLM agent framework for constructing temporally evolving medical KGs. Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we simulate the emergence of biomedical knowledge via a fine-grained daily time series. MedKGent incrementally builds the KG in a day-by-day manner using two specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor Agent identifies knowledge triples and assigns confidence scores via sampling-based estimation, which are used to filter low-confidence extractions and inform downstream processing. The Constructor Agent incrementally integrates the retained triples into a temporally evolving graph, guided by confidence scores and timestamps to reinforce recurring knowledge and resolve conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational triples. Quality assessments by two SOTA LLMs and three domain experts demonstrate an accuracy approaching 90\%, with strong inter-rater agreement. To evaluate downstream utility, we conduct RAG across seven medical question answering benchmarks using five leading LLMs, consistently observing significant improvements over non-augmented baselines. Case studies further demonstrate the KG's value in literature-based drug repurposing via confidence-aware causal inference.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing</title>
<link>https://arxiv.org/abs/2508.12405</link>
<guid>https://arxiv.org/abs/2508.12405</guid>
<content:encoded><![CDATA[
<div> keywords: Post-Acute Sequelae of COVID-19, PASC, natural language processing, BERT, clinical notes

Summary:
The study focuses on diagnosing Post-Acute Sequelae of COVID-19 (PASC) using a hybrid natural language processing pipeline. The pipeline integrates rule-based named entity recognition with BERT-based assertion detection modules to extract PASC symptoms from clinical notes efficiently. A comprehensive PASC lexicon was developed with input from clinical specialists. The model was trained and evaluated using 160 intake progress notes and a population-level prevalence study with 47,654 progress notes. The assertion detection achieved high F1 scores of 0.82 and 0.76 in internal and external validations, respectively. The pipeline processed notes quickly, at an average of $2.448\pm 0.812$ seconds per note. Spearman correlation tests showed strong agreement for both positive and negative mentions of symptoms. The results demonstrate the effectiveness and efficiency of the models in improving the diagnosis of PASC.<br /><br />Summary: The study developed a hybrid natural language processing pipeline for diagnosing PASC, integrating rule-based named entity recognition and BERT-based assertion detection. A comprehensive PASC lexicon was created collaboratively, and the model performed well in internal and external validations, with high F1 scores. The system processed notes quickly, indicating efficiency, and showed strong agreement in identifying symptoms. These findings suggest the potential of the models in enhancing the diagnosis of PASC. <div>
arXiv:2508.12405v1 Announce Type: new 
Abstract: Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC) remains challenging due to its myriad symptoms that evolve over long- and variable-time intervals. To address this issue, we developed a hybrid natural language processing pipeline that integrates rule-based named entity recognition with BERT-based assertion detection modules for PASC-symptom extraction and assertion detection from clinical notes. We developed a comprehensive PASC lexicon with clinical specialists. From 11 health systems of the RECOVER initiative network across the U.S., we curated 160 intake progress notes for model development and evaluation, and collected 47,654 progress notes for a population-level prevalence study. We achieved an average F1 score of 0.82 in one-site internal validation and 0.76 in 10-site external validation for assertion detection. Our pipeline processed each note at $2.448\pm 0.812$ seconds on average. Spearman correlation tests showed $\rho >0.83$ for positive mentions and $\rho >0.72$ for negative ones, both with $P <0.0001$. These demonstrate the effectiveness and efficiency of our models and their potential for improving PASC diagnosis.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads</title>
<link>https://arxiv.org/abs/2508.12407</link>
<guid>https://arxiv.org/abs/2508.12407</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, long-context ability, KV cache, attention heads, ZigzagAttention

Summary:
The rapid advancement of large language models (LLMs) has highlighted the importance of handling long context efficiently. However, the increased consumption of KV cache poses deployment challenges. To address this, a method called ZigzagAttention has been proposed to optimize the memory footprint of KV cache. By categorizing attention heads into retrieval and streaming heads, it reduces overhead without compromising performance significantly. The method focuses on exclusive retrieval or streaming heads within a layer to eliminate extra latency. This approach improves the identification process and results in reduced latency and comparable performance to baseline methods. The ZigzagAttention method showcases competitive results, making it a promising solution for enhancing efficiency in LLMs. 

<br /><br />Summary: <div>
arXiv:2508.12407v1 Announce Type: new 
Abstract: With the rapid development of large language models (LLMs), handling long context has become one of the vital abilities in LLMs. Such long-context ability is accompanied by difficulties in deployment, especially due to the increased consumption of KV cache. There is certain work aiming to optimize the memory footprint of KV cache, inspired by the observation that attention heads can be categorized into retrieval heads that are of great significance and streaming heads that are of less significance. Typically, identifying the streaming heads and and waiving the KV cache in the streaming heads would largely reduce the overhead without hurting the performance that much. However, since employing both retrieval and streaming heads in one layer decomposes one large round of attention computation into two small ones, it may unexpectedly bring extra latency on accessing and indexing tensors. Based on this intuition, we impose an important improvement to the identification process of retrieval and streaming heads, in which we design a criterion that enforces exclusively retrieval or streaming heads gathered in one unique layer. In this way, we further eliminate the extra latency and only incur negligible performance degradation. Our method named \textsc{ZigzagAttention} is competitive among considered baselines owing to reduced latency and comparable performance.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases</title>
<link>https://arxiv.org/abs/2508.12411</link>
<guid>https://arxiv.org/abs/2508.12411</guid>
<content:encoded><![CDATA[
<div> cultural gene, large language models, individualism-collectivism, power distance, Cultural Probe Dataset
Summary:
Large language models (LLMs) inherit cultural values, forming "cultural genes." A Cultural Probe Dataset (CPD) explores these values through 200 prompts on individualism-collectivism and power distance. A Western-centric model (GPT-4) exhibits individualistic and low power distance tendencies, aligning closely with the USA in contrast to the Eastern-centric model (ERNIE Bot) displaying collectivistic and higher power distance tendencies aligning with China. Statistical significance confirms the divergent cultural orientations of the models. The Cultural Alignment Index (CAI) calculated against Hofstede's national scores further supports the alignment of GPT-4 with the USA and ERNIE Bot with China. Qualitative analyses of dilemma resolution and authority-related judgments demonstrate how these cultural orientations influence reasoning in the models. The study emphasizes the need for culturally aware evaluation and deployment of LLMs to prevent algorithmic cultural hegemony.<br /><br />Summary: <div>
arXiv:2508.12411v1 Announce Type: new 
Abstract: Large language models (LLMs) are deployed globally, yet their underlying cultural and ethical assumptions remain underexplored. We propose the notion of a "cultural gene" -- a systematic value orientation that LLMs inherit from their training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200 prompts targeting two classic cross-cultural dimensions: Individualism-Collectivism (IDV) and Power Distance (PDI). Using standardized zero-shot prompts, we compare a Western-centric model (GPT-4) and an Eastern-centric model (ERNIE Bot). Human annotation shows significant and consistent divergence across both dimensions. GPT-4 exhibits individualistic and low-power-distance tendencies (IDV score approx 1.21; PDI score approx -1.05), while ERNIE Bot shows collectivistic and higher-power-distance tendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically significant (p < 0.001). We further compute a Cultural Alignment Index (CAI) against Hofstede's national scores and find GPT-4 aligns more closely with the USA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns more closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative analyses of dilemma resolution and authority-related judgments illustrate how these orientations surface in reasoning. Our results support the view that LLMs function as statistical mirrors of their cultural corpora and motivate culturally aware evaluation and deployment to avoid algorithmic cultural hegemony.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Emergent Physics Representations Learned In-Context by Large Language Models</title>
<link>https://arxiv.org/abs/2508.12448</link>
<guid>https://arxiv.org/abs/2508.12448</guid>
<content:encoded><![CDATA[
<div> physics, language models, in-context learning, dynamics forecasting, sparse autoencoders

Summary:
Large language models (LLMs) display impressive in-context learning (ICL) abilities, allowing them to tackle various tasks through textual prompts. The article explores how LLMs learn in context, focusing on their capacity to reason about physics. By analyzing dynamics forecasting in physical systems as a proxy task, the study investigates the emergence of reasoning abilities in LLMs. Results show that longer input contexts lead to better performance in dynamics forecasting in context. The research employs sparse autoencoders (SAEs) to analyze the model's residual stream activations, revealing correlations with key physical variables like energy. This indicates that meaningful physical concepts are encoded within LLMs during in-context learning, broadening our understanding of their learning mechanisms. The study serves as a valuable case study in unveiling how LLMs learn in context. 

<br /><br />Summary: <div>
arXiv:2508.12448v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL) abilities, enabling them to solve wide range of tasks via textual prompts alone. As these capabilities advance, the range of applicable domains continues to expand significantly. However, identifying the precise mechanisms or internal structures within LLMs that allow successful ICL across diverse, distinct classes of tasks remains elusive. Physics-based tasks offer a promising testbed for probing this challenge. Unlike synthetic sequences such as basic arithmetic or symbolic equations, physical systems provide experimentally controllable, real-world data based on structured dynamics grounded in fundamental principles. This makes them particularly suitable for studying the emergent reasoning behaviors of LLMs in a realistic yet tractable setting. Here, we mechanistically investigate the ICL ability of LLMs, especially focusing on their ability to reason about physics. Using a dynamics forecasting task in physical systems as a proxy, we evaluate whether LLMs can learn physics in context. We first show that the performance of dynamics forecasting in context improves with longer input contexts. To uncover how such capability emerges in LLMs, we analyze the model's residual stream activations using sparse autoencoders (SAEs). Our experiments reveal that the features captured by SAEs correlate with key physical variables, such as energy. These findings demonstrate that meaningful physical concepts are encoded within LLMs during in-context learning. In sum, our work provides a novel case study that broadens our understanding of how LLMs learn in context.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following</title>
<link>https://arxiv.org/abs/2508.12458</link>
<guid>https://arxiv.org/abs/2508.12458</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Vision-Language Models, Multimodal-Model-Guided Preference Optimization, LVLM-generated candidates, Multimodal Alignment Score, Direct Preference Optimization

Summary: 
Multimodal-Model-Guided Preference Optimization (M3PO) is introduced as a data-efficient method to enhance Large Vision-Language Models' (LVLMs) visual instruction following capabilities. M3PO utilizes a Multimodal Alignment Score and the model's Self-Consistency/Confidence to identify high-quality preference pairs for Direct Preference Optimization (DPO) fine-tuning. The method outperforms traditional supervised fine-tuning (SFT), simulated RLHF, vanilla DPO, and RM-DPO on various multimodal instruction benchmarks. By selecting the most informative sample pairs from LVLM-generated candidates, M3PO improves preference alignment and efficient model fine-tuning. Additionally, the integration of external quality assessment and internal belief measurement through the M3P-Score helps in identifying preferred responses and challenging dispreferred responses for better model performance. <div>
arXiv:2508.12458v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) hold immense potential for complex multimodal instruction following, yet their development is often hindered by the high cost and inconsistency of human annotation required for effective fine-tuning and preference alignment. Traditional supervised fine-tuning (SFT) and existing preference optimization methods like RLHF and DPO frequently struggle to efficiently leverage the model's own generation space to identify highly informative "hard negative" samples. To address these challenges, we propose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and data-efficient method designed to enhance LVLMs' capabilities in visual instruction following. M3PO intelligently selects the most "learning-valuable" preference sample pairs from a diverse pool of LVLM-generated candidates. This selection is driven by a sophisticated mechanism that integrates two crucial signals: a Multimodal Alignment Score (MAS) to assess external quality and the model's Self-Consistency / Confidence (log-probability) to gauge internal belief. These are combined into a novel M3P-Score, which specifically identifies preferred responses and challenging dispreferred responses that the model might confidently generate despite being incorrect. These high-quality preference pairs are then used for efficient Direct Preference Optimization (DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our extensive experiments demonstrate that M3PO consistently outperforms strong baselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a comprehensive suite of multimodal instruction following benchmarks (MME-Bench, POPE, IFT, Human Pref. Score).
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages</title>
<link>https://arxiv.org/abs/2508.12459</link>
<guid>https://arxiv.org/abs/2508.12459</guid>
<content:encoded><![CDATA[
<div> benchmark, low-resource languages, Indonesia, NLP progress, multilingual models

Summary: 
The study introduces LoraxBench, a benchmark focusing on low-resource languages in Indonesia, addressing the country's lag in NLP development due to its diverse linguistic landscape. Covering tasks like reading comprehension and translation across 20 languages, including formality registers, the dataset proves challenging for multilingual LLMs. Performance discrepancies are observed, particularly in Indonesian and low-resource languages, with no clear advantage seen in region-specific models. The impact of register variation, such as the high-level politeness 'Krama' Javanese, on model performance is highlighted, showcasing the complexities of language usage and understanding in diverse linguistic environments. <div>
arXiv:2508.12459v1 Announce Type: new 
Abstract: As one of the world's most populous countries, with 700 languages spoken, Indonesia is behind in terms of NLP progress. We introduce LoraxBench, a benchmark that focuses on low-resource languages of Indonesia and covers 6 diverse tasks: reading comprehension, open-domain QA, language inference, causal reasoning, translation, and cultural QA. Our dataset covers 20 languages, with the addition of two formality registers for three languages. We evaluate a diverse set of multilingual and region-focused LLMs and found that this benchmark is challenging. We note a visible discrepancy between performance in Indonesian and other languages, especially the low-resource ones. There is no clear lead when using a region-specific model as opposed to the general multilingual model. Lastly, we show that a change in register affects model performance, especially with registers not commonly found in social media, such as high-level politeness `Krama' Javanese.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models</title>
<link>https://arxiv.org/abs/2508.12461</link>
<guid>https://arxiv.org/abs/2508.12461</guid>
<content:encoded><![CDATA[
<div> Keywords: GPT-OSS, large language models, performance, code generation, multilingual tasks

Summary:<br /><br />In August 2025, OpenAI released its GPT-OSS models with 120B and 20B parameters, compared them against other large language models. Both variants were tested in unquantised form and gpt-oss-20B outperformed gpt-oss-120B on certain benchmarks like HumanEval and MMLU, being more memory and energy efficient. The models showed mid-tier overall performance, excelling in code generation but struggling in multilingual tasks. This study suggests that scaling in sparse architectures may not lead to proportional performance gains, highlighting the importance of optimization strategies for efficient model selection in future open source applications.<br />Summary: <div>
arXiv:2508.12461v1 Announce Type: new 
Abstract: In August 2025, OpenAI released GPT-OSS models, its first open weight large language models since GPT-2 in 2019, comprising two mixture of experts architectures with 120B and 20B parameters. We evaluated both variants against six contemporary open source large language models ranging from 14.7B to 235B parameters, representing both dense and sparse designs, across ten benchmarks covering general knowledge, mathematical reasoning, code generation, multilingual understanding, and conversational ability. All models were tested in unquantised form under standardised inference settings, with statistical validation using McNemars test and effect size analysis. Results show that gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such as HumanEval and MMLU, despite requiring substantially less memory and energy per response. Both models demonstrate mid-tier overall performance within the current open source landscape, with relative strength in code generation and notable weaknesses in multilingual tasks. These findings provide empirical evidence that scaling in sparse architectures may not yield proportional performance gains, underscoring the need for further investigation into optimisation strategies and informing more efficient model selection for future open source deployments.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping</title>
<link>https://arxiv.org/abs/2508.12482</link>
<guid>https://arxiv.org/abs/2508.12482</guid>
<content:encoded><![CDATA[
<div> syntactic bootstrapping, verb learning, language models, RoBERTa, GPT-2

Summary: 
- The study investigates syntactic bootstrapping in language models by training RoBERTa and GPT-2 on perturbed datasets.
- Results show that models' verb representation degrades more when syntactic cues are removed compared to removing co-occurrence information.
- Mental verbs, crucial for human verb learning, are more negatively impacted in training regimes where syntactic cues are removed.
- In contrast, the representation of nouns is more affected when co-occurrences are distorted rather than syntax.
- The study highlights the importance of syntactic bootstrapping in verb learning and demonstrates the feasibility of testing developmental hypotheses on a larger scale with language models. 

<br /><br />Summary: <div>
arXiv:2508.12482v1 Announce Type: new 
Abstract: Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use the syntactic environments in which a verb occurs to learn its meaning. In this paper, we examine whether large language models exhibit a similar behavior. We do this by training RoBERTa and GPT-2 on perturbed datasets where syntactic information is ablated. Our results show that models' verb representation degrades more when syntactic cues are removed than when co-occurrence information is removed. Furthermore, the representation of mental verbs, for which syntactic bootstrapping has been shown to be particularly crucial in human verb learning, is more negatively impacted in such training regimes than physical verbs. In contrast, models' representation of nouns is affected more when co-occurrences are distorted than when syntax is distorted. In addition to reinforcing the important role of syntactic bootstrapping in verb learning, our results demonstrated the viability of testing developmental hypotheses on a larger scale through manipulating the learning environments of large language models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Hallucinations in Large Language Models via Causal Reasoning</title>
<link>https://arxiv.org/abs/2508.12495</link>
<guid>https://arxiv.org/abs/2508.12495</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, causal reasoning, directed acyclic graph, reasoning trace, logical inconsistencies 

Summary: 
The article introduces a new framework called causal-DAG construction and reasoning (CDCR-SFT) aimed at enhancing causal reasoning capabilities in large language models (LLMs). The framework trains LLMs to explicitly construct variable-level directed acyclic graphs (DAGs) to represent causal relationships between variables and perform reasoning based on them. A dataset named CausalDR is also presented, consisting of samples with input questions, explicit causal DAGs, reasoning traces, and validated answers. Experiments on four LLMs across eight tasks show that CDCR-SFT improves causal reasoning performance significantly, achieving a state-of-the-art accuracy of 95.33% on the CLADDER task and reducing hallucinations by 10% on HaluEval. The results indicate that explicitly modeling causal structures in LLMs can effectively mitigate logical inconsistencies in their outputs. The code for CDCR-SFT is available on GitHub for further exploration and implementation. 

Summary: <br /><br />Explicitly modeling causal structures in LLMs can mitigate logical inconsistencies in their outputs. The framework CDCR-SFT trains LLMs to construct directed acyclic graphs (DAGs) at the variable level for causal reasoning. The CausalDR dataset includes samples with input questions, causal DAGs, reasoning traces, and answers. CDCR-SFT significantly enhances causal reasoning capabilities, achieving a 95.33% accuracy on CLADDER and reducing hallucinations by 10% on HaluEval. The research highlights the importance of representing causal relationships between variables in LLMs to improve reasoning performance. The availability of the CDCR-SFT code on GitHub promotes further research and implementation in this area. <div>
arXiv:2508.12495v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit logically inconsistent hallucinations that appear coherent yet violate reasoning principles, with recent research suggesting an inverse relationship between causal reasoning capabilities and such hallucinations. However, existing reasoning approaches in LLMs, such as Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic token level rather than modeling the underlying causal relationships between variables, lacking the ability to represent conditional independencies or satisfy causal identification assumptions. To bridge this gap, we introduce causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning framework that trains LLMs to explicitly construct variable-level directed acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a dataset comprising 25,368 samples (CausalDR), where each sample includes an input question, explicit causal DAG, graph-based reasoning trace, and validated answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves the causal reasoning capability with the state-of-the-art 95.33% accuracy on CLADDER (surpassing human performance of 94.8% for the first time) and reduces the hallucination on HaluEval with 10% improvements. It demonstrates that explicit causal structure modeling in LLMs can effectively mitigate logical inconsistencies in LLM outputs. Code is available at https://github.com/MrLYG/CDCR-SFT.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection</title>
<link>https://arxiv.org/abs/2508.12535</link>
<guid>https://arxiv.org/abs/2508.12535</guid>
<content:encoded><![CDATA[
<div> Keywords: Sparse Autoencoders, interpretability, steering tasks, correlation-based selection, language model applications
Summary:
Sparse Autoencoders (SAEs) are used to extract interpretable features from large language models without supervision. A new method called CorrSteer has been proposed to overcome limitations in downstream tasks by correlating sample correctness with SAE activations at inference time. This approach selects more relevant features and automates the process of extracting steering coefficients based on average activations. CorrSteer demonstrates improved performance on various benchmarks such as QA, bias mitigation, jailbreaking prevention, and reasoning tasks using Gemma 2 2B and LLaMA 3.1 8B datasets. Significant performance improvements were achieved with a small sample size, showcasing the effectiveness and scalability of correlation-based feature selection in guiding SAEs across different language model applications. Selected features show meaningful patterns aligned with task requirements, shedding light on the underlying capabilities driving performance. 
<br /><br />Summary: <div>
arXiv:2508.12535v1 Announce Type: new 
Abstract: Sparse Autoencoders (SAEs) can extract interpretable features from large language models (LLMs) without supervision. However, their effectiveness in downstream steering tasks is limited by the requirement for contrastive datasets or large activation storage. To address these limitations, we propose CorrSteer, which selects features by correlating sample correctness with SAE activations from generated tokens at inference time. This approach uses only inference-time activations to extract more relevant features, thereby avoiding spurious correlations. It also obtains steering coefficients from average activations, automating the entire pipeline. Our method shows improved task performance on QA, bias mitigation, jailbreaking prevention, and reasoning benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1% improvement in MMLU performance and a +22.9% improvement in HarmBench with only 4000 samples. Selected features demonstrate semantically meaningful patterns aligned with each task's requirements, revealing the underlying capabilities that drive performance. Our work establishes correlationbased selection as an effective and scalable approach for automated SAE steering across language model applications.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning</title>
<link>https://arxiv.org/abs/2508.12591</link>
<guid>https://arxiv.org/abs/2508.12591</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated Speaking Assessment, Multimodal Large Language Models, Speech-First Multimodal Training, curriculum learning principle, holistic assessment performance<br />
Summary:<br />
This paper introduces Multimodal Large Language Models (MLLM) for Automated Speaking Assessment (ASA), combining audio and text processing for improved evaluation. While MLLM show superior performance in content and language aspects, challenges in assessing delivery require specialized training. The proposed Speech-First Multimodal Training (SFMT) addresses this by prioritizing speech modeling before cross-modal fusion. Experiments on a benchmark dataset demonstrate that MLLM-based systems, particularly with SFMT, can enhance holistic assessment performance significantly. SFMT stands out in evaluating the delivery aspect, with a 4% accuracy improvement over conventional methods. This approach opens new possibilities for comprehensive ASA, showcasing the potential of MLLM in enhancing speaking assessment accuracy and effectiveness. <br /> 
Summary: <div>
arXiv:2508.12591v1 Announce Type: new 
Abstract: Traditional Automated Speaking Assessment (ASA) systems exhibit inherent modality limitations: text-based approaches lack acoustic information while audio-based methods miss semantic context. Multimodal Large Language Models (MLLM) offer unprecedented opportunities for comprehensive ASA by simultaneously processing audio and text within unified frameworks. This paper presents a very first systematic study of MLLM for comprehensive ASA, demonstrating the superior performance of MLLM across the aspects of content and language use . However, assessment on the delivery aspect reveals unique challenges, which is deemed to require specialized training strategies. We thus propose Speech-First Multimodal Training (SFMT), leveraging a curriculum learning principle to establish more robust modeling foundations of speech before cross-modal synergetic fusion. A series of experiments on a benchmark dataset show MLLM-based systems can elevate the holistic assessment performance from a PCC value of 0.783 to 0.846. In particular, SFMT excels in the evaluation of the delivery aspect, achieving an absolute accuracy improvement of 4% over conventional training approaches, which also paves a new avenue for ASA.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context</title>
<link>https://arxiv.org/abs/2508.12630</link>
<guid>https://arxiv.org/abs/2508.12630</guid>
<content:encoded><![CDATA[
<div> semantic anchoring, large language models, retrieval-augmented generation, dialogue history, linguistic cues

Summary:
Semantic Anchoring is proposed as a memory architecture for Large Language Models to enhance long-term interactions. It integrates linguistic cues such as dependency parsing, discourse relation tagging, and coreference resolution into memory storage. The approach enriches vector-based storage with explicit linguistic structures, improving recall of nuanced conversations. Experiments on long-term dialogue datasets demonstrate improved factual recall and discourse coherence by up to 18% compared to baseline systems. Ablation studies, human evaluations, and error analysis validate the effectiveness and interpretability of the Semantic Anchoring approach. <div>
arXiv:2508.12630v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and task competence in conversational settings. However, their effectiveness in multi-session and long-term interactions is hindered by limited memory persistence. Typical retrieval-augmented generation (RAG) systems store dialogue history as dense vectors, which capture semantic similarity but neglect finer linguistic structures such as syntactic dependencies, discourse relations, and coreference links. We propose Semantic Anchoring, a hybrid agentic memory architecture that enriches vector-based storage with explicit linguistic cues to improve recall of nuanced, context-rich exchanges. Our approach combines dependency parsing, discourse relation tagging, and coreference resolution to create structured memory entries. Experiments on adapted long-term dialogue datasets show that semantic anchoring improves factual recall and discourse coherence by up to 18% over strong RAG baselines. We further conduct ablation studies, human evaluations, and error analysis to assess robustness and interpretability.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing</title>
<link>https://arxiv.org/abs/2508.12631</link>
<guid>https://arxiv.org/abs/2508.12631</guid>
<content:encoded><![CDATA[
<div> Efficient, large language model, test-time routing, Avengers-Pro, performance-efficiency tradeoffs <br />
<br />
Summary: Avengers-Pro is a test-time routing framework designed to balance performance and efficiency in large language models (LLMs). By ensembling LLMs with varying capacities and efficiencies, Avengers-Pro dynamically assigns queries to the most suitable model based on a performance-efficiency score. In testing across multiple benchmarks and models, including GPT-5-medium and Gemini-2.5-pro, Avengers-Pro outperformed single models by up to 7% in average accuracy by adjusting a performance-efficiency trade-off parameter. It also achieved similar accuracy to the strongest single model at a significantly lower cost, providing a Pareto frontier where it consistently delivers the highest accuracy for any given cost and the lowest cost for any given accuracy. The code for Avengers-Pro is available on GitHub for further exploration and implementation. <br /><br /> <div>
arXiv:2508.12631v1 Announce Type: new 
Abstract: Balancing performance and efficiency is a central challenge in large language model (LLM) advancement. GPT-5 addresses this with test-time routing, dynamically assigning queries to either an efficient or a high-capacity model during inference. In this work, we present Avengers-Pro, a test-time routing framework that ensembles LLMs of varying capacities and efficiencies, providing a unified solution for all performance-efficiency tradeoffs. The Avengers-Pro embeds and clusters incoming queries, then routes each to the most suitable model based on a performance-efficiency score. Across 6 challenging benchmarks and 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and Claude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a performance-efficiency trade-off parameter, it can surpass the strongest single model (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the average accuracy of the strongest single model at 27% lower cost, and reach ~90% of that performance at 63% lower cost. Last but not least, it achieves a Pareto frontier, consistently yielding the highest accuracy for any given cost, and the lowest cost for any given accuracy, among all single models. Code is available at https://github.com/ZhangYiqun018/AvengersPro.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection</title>
<link>https://arxiv.org/abs/2508.12632</link>
<guid>https://arxiv.org/abs/2508.12632</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, fake news detection, linguistic fingerprints, distributional divergence analysis, key-fragment techniques

Summary:
The article addresses the increasing threat of fake news generated by large language models (LLMs) and the need for reliable detection methods. Traditional approaches focused on content have limitations due to the coherence and factual consistency of the generated text. The study introduces a novel method called Linguistic Fingerprints Extraction (LIFE) that identifies subtle linguistic differences between LLM-generated real and fake news through distributional divergence analysis. By reconstructing word-level probability distributions, LIFE detects fake news more effectively. Additionally, the use of key-fragment techniques enhances the detection of linguistic patterns, leading to state-of-the-art performance in LLM-generated fake news detection. The study's results demonstrate high performance not only with LLM-generated fake news but also with human-written fake news, indicating the robustness and effectiveness of the proposed method.
<br /><br />Summary: <div>
arXiv:2508.12632v1 Announce Type: new 
Abstract: With the rapid development of large language models, the generation of fake news has become increasingly effortless, posing a growing societal threat and underscoring the urgent need for reliable detection methods. Early efforts to identify LLM-generated fake news have predominantly focused on the textual content itself; however, because much of that content may appear coherent and factually consistent, the subtle traces of falsification are often difficult to uncover. Through distributional divergence analysis, we uncover prompt-induced linguistic fingerprints: statistically distinct probability shifts between LLM-generated real and fake news when maliciously prompted. Based on this insight, we propose a novel method named Linguistic Fingerprints Extraction (LIFE). By reconstructing word-level probability distributions, LIFE can find discriminative patterns that facilitate the detection of LLM-generated fake news. To further amplify these fingerprint patterns, we also leverage key-fragment techniques that accentuate subtle linguistic differences, thereby improving detection reliability. Our experiments show that LIFE achieves state-of-the-art performance in LLM-generated fake news and maintains high performance in human-written fake news. The code and data are available at https://anonymous.4open.science/r/LIFE-E86A.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Language Barriers: Equitable Performance in Multilingual Language Models</title>
<link>https://arxiv.org/abs/2508.12662</link>
<guid>https://arxiv.org/abs/2508.12662</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM, multilingual communication, Common Sense Reasoning, low-resource languages, synthetic code-switched text

Summary: 
LLMs are powerful tools for multilingual communication but struggle with Common Sense Reasoning tasks in low-resource languages like Hindi or Swahili. This paper proposes a method to improve LLM performance in low-resource languages by fine-tuning on synthetic code-switched text. The approach involves generating synthetic datasets through controlled language mixing methods and fine-tuning LLMs on these datasets. Empirical results show significant improvements in LRL model performance while maintaining or enhancing performance in HRLs. Additionally, a new dataset of synthetic code-switched text derived from CommonSenseQA is introduced, featuring three distinct language ratio configurations. This approach aims to bridge the performance gap in LLMs across diverse linguistic communities, ensuring fair access to quality LLM outputs for speakers of low-resource languages. 

Summary: <div>
arXiv:2508.12662v1 Announce Type: new 
Abstract: Cutting-edge LLMs have emerged as powerful tools for multilingual communication and understanding. However, LLMs perform worse in Common Sense Reasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi or Swahili compared to high-resource languages (HRLs) like English. Equalizing this inconsistent access to quality LLM outputs is crucial to ensure fairness for speakers of LRLs and across diverse linguistic communities. In this paper, we propose an approach to bridge this gap in LLM performance. Our approach involves fine-tuning an LLM on synthetic code-switched text generated using controlled language-mixing methods. We empirically demonstrate that fine-tuning LLMs on synthetic code-switched datasets leads to substantial improvements in LRL model performance while preserving or enhancing performance in HRLs. Additionally, we present a new dataset of synthetic code-switched text derived from the CommonSenseQA dataset, featuring three distinct language ratio configurations.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Predictive Analysis of Human Misery</title>
<link>https://arxiv.org/abs/2508.12669</link>
<guid>https://arxiv.org/abs/2508.12669</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Misery Prediction, Affective Computing, Few-Shot Learning, Gamified Evaluation<br />
<br />
Summary:<br />
This study explores the use of Large Language Models (LLMs) to predict human-perceived misery scores based on natural language descriptions of real-world situations. Approaching the task as a regression problem, the model assigns a numerical value between 0 and 100 to input statements. Various prompting strategies are evaluated, with few-shot approaches consistently outperforming zero-shot methods. Introducing the "Misery Game Show" as a gamified framework, the study structurally tests LLMs through rounds involving ordinal comparison, binary classification, scalar estimation, and feedback-driven reasoning. This setup not only assesses predictive accuracy but also emphasizes the model's adaptability based on corrective feedback. The gamified evaluation showcases the broader potential of LLMs in dynamic emotional reasoning tasks beyond traditional regression analysis.<br /> <div>
arXiv:2508.12669v1 Announce Type: new 
Abstract: This study investigates the use of Large Language Models (LLMs) for predicting human-perceived misery scores from natural language descriptions of real-world scenarios. The task is framed as a regression problem, where the model assigns a scalar value from 0 to 100 to each input statement. We evaluate multiple prompting strategies, including zero-shot, fixed-context few-shot, and retrieval-based prompting using BERT sentence embeddings. Few-shot approaches consistently outperform zero-shot baselines, underscoring the value of contextual examples in affective prediction. To move beyond static evaluation, we introduce the "Misery Game Show", a novel gamified framework inspired by a television format. It tests LLMs through structured rounds involving ordinal comparison, binary classification, scalar estimation, and feedback-driven reasoning. This setup enables us to assess not only predictive accuracy but also the model's ability to adapt based on corrective feedback. The gamified evaluation highlights the broader potential of LLMs in dynamic emotional reasoning tasks beyond standard regression. Code and data link: https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction</title>
<link>https://arxiv.org/abs/2508.12685</link>
<guid>https://arxiv.org/abs/2508.12685</guid>
<content:encoded><![CDATA[
arXiv:2508.12685v1 Announce Type: new 
Abstract: Agentic task-solving with Large Language Models (LLMs) requires multi-turn, multi-step interactions, often involving complex function calls and dynamic user-agent exchanges. Existing simulation-based data generation methods for such scenarios rely heavily on costly autoregressive interactions between multiple LLM agents, thereby limiting real-world performance of agentic tasks. In this paper, we propose a novel Non-Autoregressive Iterative Generation framework, called ToolACE-MT, for constructing high-quality multi-turn agentic dialogues. ToolACE-MT generates full conversational trajectories through three stages: coarse-grained initialization, iterative refinement, and offline verification. The initialization phase builds a structurally complete yet semantically coarse dialogue skeleton; the iterative refinement phase introduces realistic complexities and continued refinement via mask-and-fill operations; and the offline verification phase ensures correctness and coherence via rule- and model-based checks. Experiments demonstrate that ToolACE-MT enables efficient, effective and generalizable agentic data generation, offering a new paradigm for high-quality data construction in tool-augmented LLM scenarios.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning</title>
<link>https://arxiv.org/abs/2508.12726</link>
<guid>https://arxiv.org/abs/2508.12726</guid>
<content:encoded><![CDATA[
arXiv:2508.12726v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success in many natural language tasks but still struggle with complex, multi-step reasoning, particularly across diverse disciplines. Existing reasoning datasets often either lack disciplinary breadth or the structural depth necessary to elicit robust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd Reasoning data synthesis pipeline that leverages naturally available, extensive raw documents (book corpus and web corpus) to generate multidisciplinary challenging questions. A core innovation of our approach is the introduction of a Design Logic concept, which mimics the question-creation process of human educators. We use LLMs to reverse-engineer and abstract over 120,000 design logics from existing questions across various disciplines. By matching these design logics with disciplinary source materials, we are able to create reasoning questions that far surpass the difficulty and diversity of existing datasets. Based on this pipeline, we synthesized two large-scale reasoning datasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book), containing 3.04 million challenging questions synthesized from the book corpus, and Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging questions from the web corpus. Our data analysis demonstrates that the questions synthesized by our method exhibit substantially greater difficulty and diversity than those in the baseline datasets. We validate the effectiveness of these datasets by conducting SFT experiments on the Qwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset significantly outperforms existing multidisciplinary datasets of the same volume. Training with the full datasets further enables the models to surpass the multidisciplinary reasoning performance of the official Qwen3-8B and Qwen3-4B models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2508.12733</link>
<guid>https://arxiv.org/abs/2508.12733</guid>
<content:encoded><![CDATA[
arXiv:2508.12733v1 Announce Type: new 
Abstract: The widespread adoption and increasing prominence of large language models (LLMs) in global technologies necessitate a rigorous focus on ensuring their safety across a diverse range of linguistic and cultural contexts. The lack of a comprehensive evaluation and diverse data in existing multilingual safety evaluations for LLMs limits their effectiveness, hindering the development of robust multilingual safety alignment. To address this critical gap, we introduce LinguaSafe, a comprehensive multilingual safety benchmark crafted with meticulous attention to linguistic authenticity. The LinguaSafe dataset comprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated using a combination of translated, transcreated, and natively-sourced data, our dataset addresses the critical need for multilingual safety evaluations of LLMs, filling the void in the safety evaluation of LLMs across diverse under-represented languages from Hungarian to Malay. LinguaSafe presents a multidimensional and fine-grained evaluation framework, with direct and indirect safety assessments, including further evaluations for oversensitivity. The results of safety and helpfulness evaluations vary significantly across different domains and different languages, even in languages with similar resource levels. Our benchmark provides a comprehensive suite of metrics for in-depth safety evaluation, underscoring the critical importance of thoroughly assessing multilingual safety in LLMs to achieve more balanced safety alignment. Our dataset and code are released to the public to facilitate further research in the field of multilingual LLM safety.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description</title>
<link>https://arxiv.org/abs/2508.12769</link>
<guid>https://arxiv.org/abs/2508.12769</guid>
<content:encoded><![CDATA[
arXiv:2508.12769v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have significantly improved the accuracy of Text-to-SQL systems. However, a critical challenge remains: the semantic mismatch between natural language questions (NLQs) and their corresponding SQL queries. This issue is exacerbated in large-scale databases, where semantically similar attributes hinder schema linking and semantic drift during SQL generation, ultimately reducing model accuracy. To address these challenges, we introduce CRED-SQL, a framework designed for large-scale databases that integrates Cluster Retrieval and Execution Description. CRED-SQL first performs cluster-based large-scale schema retrieval to pinpoint the tables and columns most relevant to a given NLQ, alleviating schema mismatch. It then introduces an intermediate natural language representation-Execution Description Language (EDL)-to bridge the gap between NLQs and SQL. This reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL, leveraging LLMs' strong general reasoning capabilities while reducing semantic deviation. Extensive experiments on two large-scale, cross-domain benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new state-of-the-art (SOTA) performance, validating its effectiveness and scalability. Our code is available at https://github.com/smduan/CRED-SQL.git
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task</title>
<link>https://arxiv.org/abs/2508.12774</link>
<guid>https://arxiv.org/abs/2508.12774</guid>
<content:encoded><![CDATA[
arXiv:2508.12774v1 Announce Type: new 
Abstract: In this paper, we present the SALAMANDRATA family of models, an improved iteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically trained to achieve strong performance in translation-related tasks for 38 European languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For both versions, we applied the same training recipe with a first step of continual pre-training on parallel data, and a second step of supervised fine-tuning on high-quality instructions. The BSC submission to the WMT25 General Machine Translation shared task is based on the 7B variant of SALAMANDRATA. We first adapted the model vocabulary to support the additional non-European languages included in the task. This was followed by a second phase of continual pre-training and supervised fine-tuning, carefully designed to optimize performance across all translation directions for this year's shared task. For decoding, we employed two quality-aware strategies: Minimum Bayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI respectively. We publicly release both the 2B and 7B versions of SALAMANDRATA, along with the newer SALAMANDRATA-V2 model, on Hugging Face1
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks</title>
<link>https://arxiv.org/abs/2508.12778</link>
<guid>https://arxiv.org/abs/2508.12778</guid>
<content:encoded><![CDATA[
arXiv:2508.12778v1 Announce Type: new 
Abstract: Medical large vision-language Models (Med-LVLMs) have shown promise in clinical applications but suffer from factual inaccuracies and unreliable outputs, posing risks in real-world diagnostics. While retrieval-augmented generation has emerged as a potential solution, current medical multimodal RAG systems are unable to perform effective retrieval across heterogeneous sources. The irrelevance of retrieved reports affects the factuality of analysis, while insufficient knowledge affects the credibility of clinical decision-making. To bridge the gap, we construct MedAtlas, which includes extensive multimodal report repositories and diverse text corpora. Based on it, we present HeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous knowledge sources. The framework introduces Modality-specific CLIPs for effective report retrieval and a Multi-corpora Query Generator for dynamically constructing queries for diverse corpora. Incorporating knowledge from such multifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge Preference Tuning to achieve cross-modality and multi-source knowledge alignment. Extensive experiments across 12 datasets and 3 modalities demonstrate that the proposed HeteroRAG achieves state-of-the-art performance in most medical vision language benchmarks, significantly improving factual accuracy and reliability of Med-LVLMs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward</title>
<link>https://arxiv.org/abs/2508.12800</link>
<guid>https://arxiv.org/abs/2508.12800</guid>
<content:encoded><![CDATA[
arXiv:2508.12800v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit remarkable problem-solving abilities, but struggle with complex tasks due to static internal knowledge. Retrieval-Augmented Generation (RAG) enhances access to external information, yet remains limited in multi-hop reasoning and strategic search due to rigid workflows. Recent advancements in agentic deep research empower LLMs to autonomously reason, search, and synthesize information. However, current approaches relying on outcome-based reinforcement learning (RL) face critical issues such as conflicting gradients and reward sparsity, limiting performance gains and training efficiency. To address these, we first propose Atomic Thought, a novel LLM thinking paradigm that decomposes reasoning into fine-grained functional units. These units are supervised by Reasoning Reward Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained guidance. Building on this, we propose Atom-Searcher, a novel RL framework for agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher uses a curriculum-inspired reward schedule, prioritizing process-level ATR early and transitioning to outcome rewards, accelerating convergence on effective reasoning paths. Experiments on seven benchmarks show consistent improvements over the state-of-the-art. Key advantages include: (1) Atom-Searcher scales computation at test-time. (2) Atomic Thought provides supervision anchors for RRMs, bridging deep research tasks and RRMs. (3) Atom-Searcher exhibits more interpretable, human-like reasoning patterns.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models</title>
<link>https://arxiv.org/abs/2508.12803</link>
<guid>https://arxiv.org/abs/2508.12803</guid>
<content:encoded><![CDATA[
arXiv:2508.12803v1 Announce Type: new 
Abstract: Alignment with high-resource standard languages is often assumed to aid the modeling of related low-resource varieties. We challenge this assumption by demonstrating that excessive representational entanglement with a dominant variety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects, can actively hinder generative modeling. We present the first comprehensive causal study of this phenomenon by analyzing and directly intervening in the internal representation geometry of large language models (LLMs). Our key contribution is an online variational probing framework that continuously estimates the subspace of the standard variety during fine-tuning, enabling projection-based decoupling from this space. While our study uses Arabic as a case due to its unusually rich parallel resources across 25 dialects, the broader motivation is methodological: dialectal MT serves as a controlled proxy for generative tasks where comparable multi-variety corpora are unavailable. Across 25 dialects, our intervention improves generation quality by up to +4.9 chrF++ and +2.0 on average compared to standard fine-tuning, despite a measured tradeoff in standard-language performance. These results provide causal evidence that subspace dominance by high-resource varieties can restrict generative capacity for related varieties. More generally, we unify geometric and information-theoretic probing with subspace-level causal interventions, offering practical tools for improving generative modeling in closely related language families and, more broadly, for controlling representational allocation in multilingual and multi-domain LLMs. Code will be released.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue</title>
<link>https://arxiv.org/abs/2508.12819</link>
<guid>https://arxiv.org/abs/2508.12819</guid>
<content:encoded><![CDATA[
arXiv:2508.12819v1 Announce Type: new 
Abstract: We present our work to build a French semantic corpus by annotating French dialogue in Abstract Meaning Representation (AMR). Specifically, we annotate the DinG corpus, consisting of transcripts of spontaneous French dialogues recorded during the board game Catan. As AMR has insufficient coverage of the dynamics of spontaneous speech, we extend the framework to better represent spontaneous speech and sentence structures specific to French. Additionally, to support consistent annotation, we provide an annotation guideline detailing these extensions. We publish our corpus under a free license (CC-SA-BY). We also train and evaluate an AMR parser on our data. This model can be used as an assistance annotation tool to provide initial annotations that can be refined by human annotators. Our work contributes to the development of semantic resources for French dialogue.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection</title>
<link>https://arxiv.org/abs/2508.12828</link>
<guid>https://arxiv.org/abs/2508.12828</guid>
<content:encoded><![CDATA[
arXiv:2508.12828v1 Announce Type: new 
Abstract: Abusive language detection has become an increasingly important task as a means to tackle this type of harmful content in social media. There has been a substantial body of research developing models for determining if a social media post is abusive or not; however, this research has primarily focused on exploiting social media posts individually, overlooking additional context that can be derived from surrounding posts. In this study, we look at conversational exchanges, where a user replies to an earlier post by another user (the parent tweet). We ask: does leveraging context from the parent tweet help determine if a reply post is abusive or not, and what are the features that contribute the most? We study a range of content-based and account-based features derived from the context, and compare this to the more widely studied approach of only looking at the features from the reply tweet. For a more generalizable study, we test four different classification models on a dataset made of conversational exchanges (parent-reply tweet pairs) with replies labeled as abusive or not. Our experiments show that incorporating contextual features leads to substantial improvements compared to the use of features derived from the reply tweet only, confirming the importance of leveraging context. We observe that, among the features under study, it is especially the content-based features (what is being posted) that contribute to the classification performance rather than account-based features (who is posting it). While using content-based features, it is best to combine a range of different features to ensure improved performance over being more selective and using fewer features. Our study provides insights into the development of contextualized abusive language detection models in realistic settings involving conversations.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae</title>
<link>https://arxiv.org/abs/2508.12830</link>
<guid>https://arxiv.org/abs/2508.12830</guid>
<content:encoded><![CDATA[
arXiv:2508.12830v1 Announce Type: new 
Abstract: While the indirect evidence suggests that already in the early scholastic period the literary production based on records of oral teaching (so-called reportationes) was not uncommon, there are very few sources commenting on the practice. This paper details the design of a study applying stylometric techniques of authorship attribution to a collection developed from reportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover layers of editorial work and thus validate some hypotheses regarding the collection's formation. Following Camps, Cl\'erice, and Pinche (2021), I discuss the implementation of an HTR pipeline and stylometric analysis based on the most frequent words, POS tags, and pseudo-affixes. The proposed study will offer two methodological gains relevant to computational research on the scholastic tradition: it will directly compare performance on manually composed and automatically extracted data, and it will test the validity of transformer-based OCR and automated transcription alignment for workflows applied to scholastic Latin corpora. If successful, this study will provide an easily reusable template for the exploratory analysis of collaborative literary production stemming from medieval universities.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word Meanings in Transformer Language Models</title>
<link>https://arxiv.org/abs/2508.12863</link>
<guid>https://arxiv.org/abs/2508.12863</guid>
<content:encoded><![CDATA[
arXiv:2508.12863v1 Announce Type: new 
Abstract: We investigate how word meanings are represented in the transformer language models. Specifically, we focus on whether transformer models employ something analogous to a lexical store - where each word has an entry that contains semantic information. To do this, we extracted the token embedding space of RoBERTa-base and k-means clustered it into 200 clusters. In our first study, we then manually inspected the resultant clusters to consider whether they are sensitive to semantic information. In our second study, we tested whether the clusters are sensitive to five psycholinguistic measures: valence, concreteness, iconicity, taboo, and age of acquisition. Overall, our findings were very positive - there is a wide variety of semantic information encoded within the token embedding space. This serves to rule out certain "meaning eliminativist" hypotheses about how transformer LLMs process semantic information.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM Agent-Based Complex Semantic Table Annotation Approach</title>
<link>https://arxiv.org/abs/2508.12868</link>
<guid>https://arxiv.org/abs/2508.12868</guid>
<content:encoded><![CDATA[
arXiv:2508.12868v1 Announce Type: new 
Abstract: The Semantic Table Annotation (STA) task, which includes Column Type Annotation (CTA) and Cell Entity Annotation (CEA), maps table contents to ontology entities and plays important roles in various semantic applications. However, complex tables often pose challenges such as semantic loss of column names or cell values, strict ontological hierarchy requirements, homonyms, spelling errors, and abbreviations, which hinder annotation accuracy. To address these issues, this paper proposes an LLM-based agent approach for CTA and CEA. We design and implement five external tools with tailored prompts based on the ReAct framework, enabling the STA agent to dynamically select suitable annotation strategies depending on table characteristics. Experiments are conducted on the Tough Tables and BiodivTab datasets from the SemTab challenge, which contain the aforementioned challenges. Our method outperforms existing approaches across various metrics. Furthermore, by leveraging Levenshtein distance to reduce redundant annotations, we achieve a 70% reduction in time costs and a 60% reduction in LLM token usage, providing an efficient and cost-effective solution for STA.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models</title>
<link>https://arxiv.org/abs/2508.12903</link>
<guid>https://arxiv.org/abs/2508.12903</guid>
<content:encoded><![CDATA[
arXiv:2508.12903v1 Announce Type: new 
Abstract: Recent advances in self-refinement have demonstrated significant potential for improving the outputs of large language models (LLMs) through iterative refinement. However, most existing self-refinement methods rely on a reactive process with a fixed number of iterations, making it difficult to determine the optimal timing and content of refinement based on the evolving generation context. Inspired by the way humans dynamically refine their thoughts during execution, we propose ProActive Self-Refinement (PASR), a novel method that enables LLMs to refine their outputs during the generation process. Unlike methods that regenerate entire responses, PASR proactively decides whether, when, and how to refine based on the model's internal state and evolving context. We conduct extensive experiments on a diverse set of 10 tasks to evaluate the effectiveness of PASR. Experimental results show that PASR significantly enhances problem-solving performance. In particular, on Qwen3-8B, PASR reduces average token consumption by 41.6 percent compared to standard generation, while also achieving an 8.2 percent improvement in accuracy. Our code and all baselines used in the paper are available in the GitHub.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Information Sharing and Coordination in Multi-Agent Planning</title>
<link>https://arxiv.org/abs/2508.12981</link>
<guid>https://arxiv.org/abs/2508.12981</guid>
<content:encoded><![CDATA[
arXiv:2508.12981v1 Announce Type: new 
Abstract: Multi-agent systems (MASs) have pushed the boundaries of large language model (LLM) agents in domains such as web research and software engineering. However, long-horizon, multi-constraint planning tasks involve conditioning on detailed information and satisfying complex interdependent constraints, which can pose a challenge for these systems. In this study, we construct an LLM-based MAS for a travel planning task which is representative of these challenges. We evaluate the impact of a notebook to facilitate information sharing, and evaluate an orchestrator agent to improve coordination in free form conversation between agents. We find that the notebook reduces errors due to hallucinated details by 18%, while an orchestrator directs the MAS to focus on and further reduce errors by up to 13.5% within focused sub-areas. Combining both mechanisms achieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute improvement over the single-agent baseline's 7.5% pass rate. These results highlight the potential of structured information sharing and reflective orchestration as key components in MASs for long horizon planning with LLMs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents</title>
<link>https://arxiv.org/abs/2508.13024</link>
<guid>https://arxiv.org/abs/2508.13024</guid>
<content:encoded><![CDATA[
arXiv:2508.13024v1 Announce Type: new 
Abstract: LLM-based web agents have the potential to automate long-running web tasks, such as finding offers for specific products in multiple online shops and subsequently ordering the cheapest products that meet the users needs. This paper introduces WebMall, a multi-shop online shopping benchmark for evaluating the effectiveness and efficiency of web agents for comparison-shopping. WebMall consists of four simulated online shops populated with authentic product offers sourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These tasks include basic tasks such as finding specific products in multiple shops, performing price comparisons, adding items to the shopping cart, and completing checkout. Advanced tasks involve searching for products based on vague requirements, identifying suitable substitutes, and finding compatible products. Compared to existing e-commerce benchmarks, such as WebShop or ShoppingBench, WebMall introduces comparison-shopping tasks across multiple shops. Furthermore, the product offers are more heterogeneous, as they originate from hundreds of distinct real-world shops. The tasks in WebMall require longer interaction trajectories than those in WebShop, while remaining representative of real-world shopping behaviors. We evaluate eight baseline agents on WebMall, varying in observation modality, memory utilization, and underlying large language model (GPT 4.1 and Claude Sonnet 4). The best-performing configurations achieve completion rates of 75% and 53%, and F1 scores of 87% and 63%, on the basic and advanced task sets, respectively. WebMall is publicly released to facilitate research on web agents and to promote advancements in navigation, reasoning, and efficiency within e-commerce scenarios.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis</title>
<link>https://arxiv.org/abs/2508.13028</link>
<guid>https://arxiv.org/abs/2508.13028</guid>
<content:encoded><![CDATA[
arXiv:2508.13028v1 Announce Type: new 
Abstract: Sarcastic speech synthesis, which involves generating speech that effectively conveys sarcasm, is essential for enhancing natural interactions in applications such as entertainment and human-computer interaction. However, synthesizing sarcastic speech remains a challenge due to the nuanced prosody that characterizes sarcasm, as well as the limited availability of annotated sarcastic speech data. To address these challenges, this study introduces a novel approach that integrates feedback loss from a bi-modal sarcasm detection model into the TTS training process, enhancing the model's ability to capture and convey sarcasm. In addition, by leveraging transfer learning, a speech synthesis model pre-trained on read speech undergoes a two-stage fine-tuning process. First, it is fine-tuned on a diverse dataset encompassing various speech styles, including sarcastic speech. In the second stage, the model is further refined using a dataset focused specifically on sarcastic speech, enhancing its ability to generate sarcasm-aware speech. Objective and subjective evaluations demonstrate that our proposed methods improve the quality, naturalness, and sarcasm-awareness of synthesized speech.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction</title>
<link>https://arxiv.org/abs/2508.13037</link>
<guid>https://arxiv.org/abs/2508.13037</guid>
<content:encoded><![CDATA[
arXiv:2508.13037v1 Announce Type: new 
Abstract: Recent studies have demonstrated that Large Language Models (LLMs) have strong mathematical reasoning abilities but rely on hundreds of billions of parameters. To tackle the challenge of poor reasoning in Small Language Models (SLMs), existing methods typically leverage LLMs to generate massive amounts of data for cramming training. In psychology, they are akin to System 1 thinking, which resolves reasoning problems rapidly based on experience and intuition. However, human learning also requires System 2 thinking, where knowledge is first acquired and then reinforced through practice. Inspired by such two distinct modes of thinking, we propose a novel method based on the multi-LoRA Interaction for mathematical reasoning Distillation (LoRID). First, we input the question and reasoning of each sample into an LLM to create knowledge-enhanced datasets. Subsequently, we train a LoRA block on the student model as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts for problem-solving. Then, to imitate System 2 thinking, we train the Knowledge Generator (KG) and Deep Reasoner (DR), respectively. The former outputs only knowledge after receiving problems, while the latter uses that knowledge to perform reasoning. Finally, to address the randomness in the generation of IR and DR, we evaluate whether their outputs are consistent, and the inference process needs to be iterated if not. This step can enhance the mathematical reasoning ability of SLMs through mutual feedback. Experimental results show that LoRID achieves state-of-the-art performance, especially on the GSM8K dataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%, 12.3%, and 1.8% accuracy across the five base models, respectively.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>B\"{u}y\"{u}k Dil Modelleri i\c{c}in TR-MMLU Benchmark{\i}: Performans De\u{g}erlendirmesi, Zorluklar ve \.{I}yile\c{s}tirme F{\i}rsatlar{\i}</title>
<link>https://arxiv.org/abs/2508.13044</link>
<guid>https://arxiv.org/abs/2508.13044</guid>
<content:encoded><![CDATA[
arXiv:2508.13044v1 Announce Type: new 
Abstract: Language models have made significant advancements in understanding and generating human language, achieving remarkable success in various applications. However, evaluating these models remains a challenge, particularly for resource-limited languages like Turkish. To address this issue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive evaluation framework designed to assess the linguistic and conceptual capabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a meticulously curated dataset comprising 6,200 multiple-choice questions across 62 sections within the Turkish education system. This benchmark provides a standard framework for Turkish NLP research, enabling detailed analyses of LLMs' capabilities in processing Turkish text. In this study, we evaluated state-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model design. TR-MMLU sets a new standard for advancing Turkish NLP research and inspiring future innovations.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do\u{g}al Dil \.I\c{s}lemede Tokenizasyon Standartlar{\i} ve \"Ol\c{c}\"um\"u: T\"urk\c{c}e \"Uzerinden B\"uy\"uk Dil Modellerinin Kar\c{s}{\i}la\c{s}t{\i}rmal{\i} Analizi</title>
<link>https://arxiv.org/abs/2508.13058</link>
<guid>https://arxiv.org/abs/2508.13058</guid>
<content:encoded><![CDATA[
arXiv:2508.13058v1 Announce Type: new 
Abstract: Tokenization is a fundamental preprocessing step in Natural Language Processing (NLP), significantly impacting the capability of large language models (LLMs) to capture linguistic and semantic nuances. This study introduces a novel evaluation framework addressing tokenization challenges specific to morphologically-rich and low-resource languages such as Turkish. Utilizing the Turkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from the Turkish education system, we assessed tokenizers based on vocabulary size, token count, processing time, language-specific token percentages (\%TR), and token purity (\%Pure). These newly proposed metrics measure how effectively tokenizers preserve linguistic structures. Our analysis reveals that language-specific token percentages exhibit a stronger correlation with downstream performance (e.g., MMLU scores) than token purity. Furthermore, increasing model parameters alone does not necessarily enhance linguistic performance, underscoring the importance of tailored, language-specific tokenization methods. The proposed framework establishes robust and practical tokenization standards for morphologically complex languages.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database</title>
<link>https://arxiv.org/abs/2508.13060</link>
<guid>https://arxiv.org/abs/2508.13060</guid>
<content:encoded><![CDATA[
arXiv:2508.13060v1 Announce Type: new 
Abstract: The Simon Fraser University Speech Error Database (SFUSED) is a public data collection developed for linguistic and psycholinguistic research. Here we demonstrate how its design and annotations can be used to test and evaluate speech recognition models. The database comprises systematically annotated speech errors from spontaneous English speech, with each error tagged for intended and actual error productions. The annotation schema incorporates multiple classificatory dimensions that are of some value to model assessment, including linguistic hierarchical level, contextual sensitivity, degraded words, word corrections, and both word-level and syllable-level error positioning. To assess the value of these classificatory variables, we evaluated the transcription accuracy of WhisperX across 5,300 documented word and phonological errors. This analysis demonstrates the atabase's effectiveness as a diagnostic tool for ASR system performance.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforced Context Order Recovery for Adaptive Reasoning and Planning</title>
<link>https://arxiv.org/abs/2508.13070</link>
<guid>https://arxiv.org/abs/2508.13070</guid>
<content:encoded><![CDATA[
arXiv:2508.13070v1 Announce Type: new 
Abstract: Modern causal language models, followed by rapid developments in discrete diffusion models, can now produce a wide variety of interesting and useful content. However, these families of models are predominantly trained to output tokens with a fixed (left-to-right) or random order, which may deviate from the logical order in which tokens are generated originally. In this paper, we observe that current causal and diffusion models encounter difficulties in problems that require adaptive token generation orders to solve tractably, which we characterize with the $\mathcal{V}$-information framework. Motivated by this, we propose Reinforced Context Order Recovery (ReCOR), a reinforcement-learning-based framework to extract adaptive, data-dependent token generation orders from text data without annotations. Self-supervised by token prediction statistics, ReCOR estimates the hardness of predicting every unfilled token and adaptively selects the next token during both training and inference. Experiments on challenging reasoning and planning datasets demonstrate the superior performance of ReCOR compared with baselines, sometimes outperforming oracle models supervised with the ground-truth order.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocHPLT: A Massively Multilingual Document-Level Translation Dataset</title>
<link>https://arxiv.org/abs/2508.13079</link>
<guid>https://arxiv.org/abs/2508.13079</guid>
<content:encoded><![CDATA[
arXiv:2508.13079v1 Announce Type: new 
Abstract: Existing document-level machine translation resources are only available for a handful of languages, mostly high-resourced ones. To facilitate the training and evaluation of document-level translation and, more broadly, long-context modeling for global communities, we create DocHPLT, the largest publicly available document-level translation dataset to date. It contains 124 million aligned document pairs across 50 languages paired with English, comprising 4.26 billion sentences, with further possibility to provide 2500 bonus pairs not involving English. Unlike previous reconstruction-based approaches that piece together documents from sentence-level data, we modify an existing web extraction pipeline to preserve complete document integrity from the source, retaining all content including unaligned portions. After our preliminary experiments identify the optimal training context strategy for document-level translation, we demonstrate that LLMs fine-tuned on DocHPLT substantially outperform off-the-shelf instruction-tuned baselines, with particularly dramatic improvements for under-resourced languages. We open-source the dataset under a permissive license, providing essential infrastructure for advancing multilingual document-level translation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>All for law and law for all: Adaptive RAG Pipeline for Legal Research</title>
<link>https://arxiv.org/abs/2508.13107</link>
<guid>https://arxiv.org/abs/2508.13107</guid>
<content:encoded><![CDATA[
arXiv:2508.13107v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding large language model outputs in cited sources, a capability that is especially critical in the legal domain. We present an end-to-end RAG pipeline that revisits and extends the LegalBenchRAG baseline with three targeted enhancements: (i) a context-aware query translator that disentangles document references from natural-language questions and adapts retrieval depth and response style based on expertise and specificity, (ii) open-source retrieval strategies using SBERT and GTE embeddings that achieve substantial performance gains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for $K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and generation framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to assess semantic alignment and faithfulness across models and prompt designs. Our results show that carefully designed open-source pipelines can rival or outperform proprietary approaches in retrieval quality, while a custom legal-grounded prompt consistently produces more faithful and contextually relevant answers than baseline prompting. Taken together, these contributions demonstrate the potential of task-aware, component-level tuning to deliver legally grounded, reproducible, and cost-effective RAG systems for legal research assistance.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.13118</link>
<guid>https://arxiv.org/abs/2508.13118</guid>
<content:encoded><![CDATA[
arXiv:2508.13118v1 Announce Type: new 
Abstract: Incident response (IR) requires fast, coordinated, and well-informed decision-making to contain and mitigate cyber threats. While large language models (LLMs) have shown promise as autonomous agents in simulated IR settings, their reasoning is often limited by a lack of access to external knowledge. In this work, we present AutoBnB-RAG, an extension of the AutoBnB framework that incorporates retrieval-augmented generation (RAG) into multi-agent incident response simulations. Built on the Backdoors & Breaches (B&amp;B) tabletop game environment, AutoBnB-RAG enables agents to issue retrieval queries and incorporate external evidence during collaborative investigations. We introduce two retrieval settings: one grounded in curated technical documentation (RAG-Wiki), and another using narrative-style incident reports (RAG-News). We evaluate performance across eight team structures, including newly introduced argumentative configurations designed to promote critical reasoning. To validate practical utility, we also simulate real-world cyber incidents based on public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct complex multi-stage attacks. Our results show that retrieval augmentation improves decision quality and success rates across diverse organizational models. This work demonstrates the value of integrating retrieval mechanisms into LLM-based multi-agent systems for cybersecurity decision-making.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries</title>
<link>https://arxiv.org/abs/2508.13124</link>
<guid>https://arxiv.org/abs/2508.13124</guid>
<content:encoded><![CDATA[
arXiv:2508.13124v1 Announce Type: new 
Abstract: Abstractive summarization is a core application in contact centers, where Large Language Models (LLMs) generate millions of summaries of call transcripts daily. Despite their apparent quality, it remains unclear whether LLMs systematically under- or over-attend to specific aspects of the transcript, potentially introducing biases in the generated summary. While prior work has examined social and positional biases, the specific forms of bias pertinent to contact center operations - which we term Operational Bias - have remained unexplored. To address this gap, we introduce BlindSpot, a framework built upon a taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic) for the identification and quantification of these biases. BlindSpot leverages an LLM as a zero-shot classifier to derive categorical distributions for each bias dimension in a pair of transcript and its summary. The bias is then quantified using two metrics: Fidelity Gap (the JS Divergence between distributions) and Coverage (the percentage of source labels omitted). Using BlindSpot, we conducted an empirical study with 2500 real call transcripts and their summaries generated by 20 LLMs of varying scales and families (e.g., GPT, Llama, Claude). Our analysis reveals that biases are systemic and present across all evaluated models, regardless of size or family.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation</title>
<link>https://arxiv.org/abs/2508.13130</link>
<guid>https://arxiv.org/abs/2508.13130</guid>
<content:encoded><![CDATA[
arXiv:2508.13130v1 Announce Type: new 
Abstract: Commonsense validation evaluates whether a sentence aligns with everyday human understanding, a critical capability for developing robust natural language understanding systems. While substantial progress has been made in English, the task remains underexplored in Arabic, particularly given its rich linguistic diversity. Existing Arabic resources have primarily focused on Modern Standard Arabic (MSA), leaving regional dialects underrepresented despite their prevalence in spoken contexts. To bridge this gap, we present two key contributions: (i) we introduce MuDRiC, an extended Arabic commonsense dataset incorporating multiple dialects, and (ii) a novel method adapting Graph Convolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances semantic relationship modeling for improved commonsense validation. Our experimental results demonstrate that this approach achieves superior performance in Arabic commonsense validation. Our work enhances Arabic natural language understanding by providing both a foundational dataset and a novel method for handling its complex variations. To the best of our knowledge, we release the first Arabic multi-dialect commonsense reasoning dataset.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Detection of Watermarked Language Models</title>
<link>https://arxiv.org/abs/2508.13131</link>
<guid>https://arxiv.org/abs/2508.13131</guid>
<content:encoded><![CDATA[
arXiv:2508.13131v1 Announce Type: new 
Abstract: Watermarking has recently emerged as an effective strategy for detecting the generations of large language models (LLMs). The strength of a watermark typically depends strongly on the entropy afforded by the language model and the set of input prompts. However, entropy can be quite limited in practice, especially for models that are post-trained, for example via instruction tuning or reinforcement learning from human feedback (RLHF), which makes detection based on watermarking alone challenging. In this work, we investigate whether detection can be improved by combining watermark detectors with non-watermark ones. We explore a number of hybrid schemes that combine the two, observing performance gains over either class of detector under a wide range of experimental conditions.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptimalThinkingBench: Evaluating Over and Underthinking in LLMs</title>
<link>https://arxiv.org/abs/2508.13141</link>
<guid>https://arxiv.org/abs/2508.13141</guid>
<content:encoded><![CDATA[
arXiv:2508.13141v1 Announce Type: new 
Abstract: Thinking LLMs solve complex tasks at the expense of increased compute and overthinking on simpler problems, while non-thinking LLMs are faster and cheaper but underthink on harder reasoning problems. This has led to the development of separate thinking and non-thinking LLM variants, leaving the onus of selecting the optimal model for each query on the end user. In this work, we introduce OptimalThinkingBench, a unified benchmark that jointly evaluates overthinking and underthinking in LLMs and also encourages the development of optimally-thinking models that balance performance and efficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench, featuring simple queries in 72 domains, and UnderthinkingBench, containing 11 challenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we perform extensive evaluation of 33 different thinking and non-thinking models and show that no model is able to optimally think on our benchmark. Thinking models often overthink for hundreds of tokens on the simplest user queries without improving performance. In contrast, large non-thinking models underthink, often falling short of much smaller thinking models. We further explore several methods to encourage optimal thinking, but find that these approaches often improve on one sub-benchmark at the expense of the other, highlighting the need for better unified and optimal models in the future.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation</title>
<link>https://arxiv.org/abs/2508.13144</link>
<guid>https://arxiv.org/abs/2508.13144</guid>
<content:encoded><![CDATA[
arXiv:2508.13144v1 Announce Type: new 
Abstract: Developing large language models is expensive and involves making decisions with small experiments, typically by evaluating on large, multi-task evaluation suites. In this work, we analyze specific properties which make a benchmark more reliable for such decisions, and interventions to design higher-quality evaluation benchmarks. We introduce two key metrics that show differences in current benchmarks: signal, a benchmark's ability to separate better models from worse models, and noise, a benchmark's sensitivity to random variability between training steps. We demonstrate that benchmarks with a better signal-to-noise ratio are more reliable when making decisions at small scale, and those with less noise have lower scaling law prediction error. These results suggest that improving signal or noise will lead to more useful benchmarks, so we introduce three interventions designed to directly affect signal or noise. For example, we propose that switching to a metric that has better signal and noise (e.g., perplexity rather than accuracy) leads to better reliability and improved scaling law error. We also find that filtering noisy subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable multi-task evaluations. We also find that averaging the output of a model's intermediate checkpoints to reduce noise leads to consistent improvements. We conclude by recommending that those creating new benchmarks, or selecting which existing benchmarks to use, aim for high signal and low noise. We use 30 benchmarks for these experiments, and 375 open-weight language models from 60M to 32B parameters, resulting in a new, publicly available dataset of 900K evaluation benchmark results, totaling 200M instances.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns</title>
<link>https://arxiv.org/abs/2508.13152</link>
<guid>https://arxiv.org/abs/2508.13152</guid>
<content:encoded><![CDATA[
arXiv:2508.13152v1 Announce Type: new 
Abstract: Detecting content generated by large language models (LLMs) is crucial for preventing misuse and building trustworthy AI systems. Although existing detection methods perform well, their robustness in out-of-distribution (OOD) scenarios is still lacking. In this paper, we hypothesize that, compared to features used by existing detection methods, the internal representations of LLMs contain more comprehensive and raw features that can more effectively capture and distinguish the statistical pattern differences between LLM-generated texts (LGT) and human-written texts (HWT). We validated this hypothesis across different LLMs and observed significant differences in neural activation patterns when processing these two types of texts. Based on this, we propose RepreGuard, an efficient statistics-based detection method. Specifically, we first employ a surrogate model to collect representation of LGT and HWT, and extract the distinct activation feature that can better identify LGT. We can classify the text by calculating the projection score of the text representations along this feature direction and comparing with a precomputed threshold. Experimental results show that RepreGuard outperforms all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD scenarios, while also demonstrating robust resilience to various text sizes and mainstream attacks. Data and code are publicly available at: https://github.com/NLP2CT/RepreGuard
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Attention across Multiple-context KV Cache</title>
<link>https://arxiv.org/abs/2508.11661</link>
<guid>https://arxiv.org/abs/2508.11661</guid>
<content:encoded><![CDATA[
arXiv:2508.11661v1 Announce Type: cross 
Abstract: Large language models face significant cost challenges in long-sequence inference. To address this, reusing historical Key-Value (KV) Cache for improved inference efficiency has become a mainstream approach. Recent advances further enhance throughput by sparse attention mechanisms to select the most relevant KV Cache, thereby reducing sequence length. However, such techniques are limited to single-context scenarios, where historical KV Cache is computed sequentially with causal-attention dependencies. In retrieval-augmented generation (RAG) scenarios, where retrieved documents as context are unknown beforehand, each document's KV Cache is computed and stored independently (termed multiple-context KV Cache), lacking cross-attention between contexts. This renders existing methods ineffective. Although prior work partially recomputes multiple-context KV Cache to mitigate accuracy loss from missing cross-attention, it requires retaining all KV Cache throughout, failing to reduce memory overhead. This paper presents SamKV, the first exploration of attention sparsification for multiple-context KV Cache. Specifically, SamKV takes into account the complementary information of other contexts when sparsifying one context, and then locally recomputes the sparsified information. Experiments demonstrate that our method compresses sequence length to 15% without accuracy degradation compared with full-recompuation baselines, significantly boosting throughput in multi-context RAG scenarios.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Representation Stability for Transformer Models</title>
<link>https://arxiv.org/abs/2508.11667</link>
<guid>https://arxiv.org/abs/2508.11667</guid>
<content:encoded><![CDATA[
arXiv:2508.11667v1 Announce Type: cross 
Abstract: Adversarial text attacks remain a persistent threat to transformer models, yet existing defenses are typically attack-specific or require costly model retraining. We introduce Representation Stability (RS), a model-agnostic detection framework that identifies adversarial examples by measuring how embedding representations change when important words are masked. RS first ranks words using importance heuristics, then measures embedding sensitivity to masking top-k critical words, and processes the resulting patterns with a BiLSTM detector. Experiments show that adversarially perturbed words exhibit disproportionately high masking sensitivity compared to naturally important words. Across three datasets, three attack types, and two victim models, RS achieves over 88% detection accuracy and demonstrates competitive performance compared to existing state-of-the-art methods, often at lower computational cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure perturbation identification quality, we reveal that gradient-based ranking outperforms attention and random selection approaches, with identification quality correlating with detection performance for word-level attacks. RS also generalizes well to unseen datasets, attacks, and models without retraining, providing a practical solution for adversarial text detection.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code Vulnerability Detection Across Different Programming Languages with AI Models</title>
<link>https://arxiv.org/abs/2508.11710</link>
<guid>https://arxiv.org/abs/2508.11710</guid>
<content:encoded><![CDATA[
arXiv:2508.11710v1 Announce Type: cross 
Abstract: Security vulnerabilities present in a code that has been written in diverse programming languages are among the most critical yet complicated aspects of source code to detect. Static analysis tools based on rule-based patterns usually do not work well at detecting the context-dependent bugs and lead to high false positive rates. Recent developments in artificial intelligence, specifically the use of transformer-based models like CodeBERT and CodeLlama, provide light to this problem, as they show potential in finding such flaws better. This paper presents the implementations of these models on various datasets of code vulnerability, showing how off-the-shelf models can successfully produce predictive capacity in models through dynamic fine-tuning of the models on vulnerable and safe code fragments. The methodology comprises the gathering of the dataset, normalization of the language, fine-tuning of the model, and incorporation of ensemble learning and explainable AI. Experiments show that a well-trained CodeBERT can be as good as or even better than some existing static analyzers in terms of accuracy greater than 97%. Further study has indicated that although language models can achieve close-to-perfect recall, the precision can decrease. A solution to this is given by hybrid models and validation procedures, which will reduce false positives. According to the results, the AI-based solutions generalize to different programming languages and classes of vulnerability. Nevertheless, robustness, interpretability, and deployment readiness are still being developed. The results illustrate the probabilities that AI will enhance the trustworthiness in the usability and scalability of machine-learning-based detectors of vulnerabilities.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ovis2.5 Technical Report</title>
<link>https://arxiv.org/abs/2508.11737</link>
<guid>https://arxiv.org/abs/2508.11737</guid>
<content:encoded><![CDATA[
arXiv:2508.11737v1 Announce Type: cross 
Abstract: We present Ovis2.5, a successor to Ovis2 designed for native-resolution visual perception and strong multimodal reasoning. Ovis2.5 integrates a native-resolution vision transformer that processes images at their native, variable resolutions, avoiding the degradation from fixed-resolution tiling and preserving both fine detail and global layout -- crucial for visually dense content like complex charts. To strengthen reasoning, we train the model to move beyond linear chain-of-thought and perform reflection -- including self-checking and revision. This advanced capability is exposed as an optional "thinking mode" at inference time, allowing users to trade latency for enhanced accuracy on difficult inputs. The model is trained via a comprehensive five-phase curriculum that progressively builds its skills. The process begins with foundational visual and multimodal pretraining, advances through large-scale instruction tuning, and culminates in alignment and reasoning enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ multimodal data packing and hybrid parallelism, yielding a significant end-to-end speedup. We release two open-source models: Ovis2.5-9B and Ovis2.5-2B. The latter continues the "small model, big performance" philosophy of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a substantial improvement over its predecessor, Ovis2-8B, and achieving state-of-the-art results among open-source MLLMs in the sub-40B parameter range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong capabilities on grounding and video tasks, and achieves open-source SOTA at its scale for complex chart analysis.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Natural Language for Human-Robot Collaboration in the Real World</title>
<link>https://arxiv.org/abs/2508.11759</link>
<guid>https://arxiv.org/abs/2508.11759</guid>
<content:encoded><![CDATA[
arXiv:2508.11759v1 Announce Type: cross 
Abstract: We have a vision of a day when autonomous robots can collaborate with humans as assistants in performing complex tasks in the physical world. This vision includes that the robots will have the ability to communicate with their human collaborators using language that is natural to the humans. Traditional Interactive Task Learning (ITL) systems have some of this ability, but the language they can understand is very limited. The advent of large language models (LLMs) provides an opportunity to greatly improve the language understanding of robots, yet integrating the language abilities of LLMs with robots that operate in the real physical world is a challenging problem.
  In this chapter we first review briefly a few commercial robot products that work closely with humans, and discuss how they could be much better collaborators with robust language abilities. We then explore how an AI system with a cognitive agent that controls a physical robot at its core, interacts with both a human and an LLM, and accumulates situational knowledge through its experiences, can be a possible approach to reach that vision. We focus on three specific challenges of having the robot understand natural language, and present a simple proof-of-concept experiment using ChatGPT for each. Finally, we discuss what it will take to turn these simple experiments into an operational system where LLM-assisted language understanding is a part of an integrated robotic assistant that uses language to collaborate with humans.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models</title>
<link>https://arxiv.org/abs/2508.11801</link>
<guid>https://arxiv.org/abs/2508.11801</guid>
<content:encoded><![CDATA[
arXiv:2508.11801v1 Announce Type: cross 
Abstract: Attribute Value Extraction (AVE) is important for structuring product information in e-commerce. However, existing AVE datasets are primarily limited to text-to-text or image-to-text settings, lacking support for product videos, diverse attribute coverage, and public availability. To address these gaps, we introduce VideoAVE, the first publicly available video-to-text e-commerce AVE dataset across 14 different domains and covering 172 unique attributes. To ensure data quality, we propose a post-hoc CLIP-based Mixture of Experts filtering system (CLIP-MoE) to remove the mismatched video-product pairs, resulting in a refined dataset of 224k training data and 25k evaluation data. In order to evaluate the usability of the dataset, we further establish a comprehensive benchmark by evaluating several state-of-the-art video vision language models (VLMs) under both attribute-conditioned value prediction and open attribute-value pair extraction tasks. Our results analysis reveals that video-to-text AVE remains a challenging problem, particularly in open settings, and there is still room for developing more advanced VLMs capable of leveraging effective temporal information. The dataset and benchmark code for VideoAVE are available at: https://github.com/gjiaying/VideoAVE
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Labels or Input? Rethinking Augmentation in Multimodal Hate Detection</title>
<link>https://arxiv.org/abs/2508.11808</link>
<guid>https://arxiv.org/abs/2508.11808</guid>
<content:encoded><![CDATA[
arXiv:2508.11808v1 Announce Type: cross 
Abstract: The modern web is saturated with multimodal content, intensifying the challenge of detecting hateful memes, where harmful intent is often conveyed through subtle interactions between text and image under the guise of humor or satire. While recent advances in Vision-Language Models (VLMs) show promise, these models lack support for fine-grained supervision and remain susceptible to implicit hate speech. In this paper, we present a dual-pronged approach to improve multimodal hate detection. First, we propose a prompt optimization framework that systematically varies prompt structure, supervision granularity, and training modality. We show that prompt design and label scaling both influence performance, with structured prompts improving robustness even in small models, and InternVL2 achieving the best F1-scores across binary and scaled settings. Second, we introduce a multimodal data augmentation pipeline that generates 2,479 counterfactually neutral memes by isolating and rewriting the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup, successfully reduces spurious correlations and improves classifier generalization. Our approaches inspire new directions for building synthetic data to train robust and fair vision-language models. Our findings demonstrate that prompt structure and data composition are as critical as model size, and that targeted augmentation can support more trustworthy and context-sensitive hate detection.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework</title>
<link>https://arxiv.org/abs/2508.11860</link>
<guid>https://arxiv.org/abs/2508.11860</guid>
<content:encoded><![CDATA[
arXiv:2508.11860v1 Announce Type: cross 
Abstract: Large language model (LLM) agent evaluators leverage specialized tools to ground the rational decision-making of LLMs, making them well-suited to aid in scientific discoveries, such as constrained retrosynthesis planning. Constrained retrosynthesis planning is an essential, yet challenging, process within chemistry for identifying synthetic routes from commercially available starting materials to desired target molecules, subject to practical constraints. Here, we present LARC, the first LLM-based Agentic framework for Retrosynthesis planning under Constraints. LARC incorporates agentic constraint evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis planning process, using agentic feedback grounded in tool-based reasoning to guide and constrain route generation. We rigorously evaluate LARC on a carefully curated set of 48 constrained retrosynthesis planning tasks across 3 constraint types. LARC achieves a 72.9% success rate on these tasks, vastly outperforming LLM baselines and approaching human expert-level success in substantially less time. The LARC framework is extensible, and serves as a first step towards an effective agentic tool or a co-scientist to human experts for constrained retrosynthesis.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2508.11886</link>
<guid>https://arxiv.org/abs/2508.11886</guid>
<content:encoded><![CDATA[
arXiv:2508.11886v1 Announce Type: cross 
Abstract: Instructed Visual Segmentation (IVS) tasks require segmenting objects in images or videos based on natural language instructions. While recent multimodal large language models (MLLMs) have achieved strong performance on IVS, their inference cost remains a major bottleneck, particularly in video. We empirically analyze visual token sampling in MLLMs and observe a strong correlation between subset token coverage and segmentation performance. This motivates our design of a simple and effective token pruning method that selects a compact yet spatially representative subset of tokens to accelerate inference. In this paper, we introduce a novel visual token pruning method for IVS, called EVTP-IV, which builds upon the k-center by integrating spatial information to ensure better coverage. We further provide an information-theoretic analysis to support our design. Experiments on standard IVS benchmarks show that our method achieves up to 5X speed-up on video tasks and 3.5X on image tasks, while maintaining comparable accuracy using only 20% of the tokens. Our method also consistently outperforms state-of-the-art pruning baselines under varying pruning ratios.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Token Choice for Code Watermarking: A RL Approach</title>
<link>https://arxiv.org/abs/2508.11925</link>
<guid>https://arxiv.org/abs/2508.11925</guid>
<content:encoded><![CDATA[
arXiv:2508.11925v1 Announce Type: cross 
Abstract: The need for detecting LLM-generated code necessitates watermarking systems capable of operating within its highly structured and syntactically constrained environment. To address this, we introduce CodeTracer, an innovative adaptive code watermarking framework underpinned by a novel reinforcement learning training paradigm. At its core, CodeTracer features a policy-driven approach that utilizes a parameterized model to intelligently bias token choices during next-token prediction. This strategy ensures that embedded watermarks maintain code functionality while exhibiting subtle yet statistically detectable deviations from typical token distributions. To facilitate policy learning, we devise a comprehensive reward system that seamlessly integrates execution feedback with watermark embedding signals, balancing process-level and outcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization to enable gradient-based optimization of discrete watermarking decisions. Extensive comparative evaluations demonstrate CodeTracer's significant superiority over state-of-the-art baselines in both watermark detectability and the preservation of generated code's functionality.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs</title>
<link>https://arxiv.org/abs/2508.11944</link>
<guid>https://arxiv.org/abs/2508.11944</guid>
<content:encoded><![CDATA[
arXiv:2508.11944v1 Announce Type: cross 
Abstract: Game-playing ability serves as an indicator for evaluating the strategic reasoning capability of large language models (LLMs). While most existing studies rely on utility performance metrics, which are not robust enough due to variations in opponent behavior and game structure. To address this limitation, we propose \textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation framework inspired by the cognitive hierarchy models from behavioral economics. We hypothesize that agents have bounded rationality -- different agents behave at varying reasoning depths/levels. We evaluate LLMs' strategic reasoning through a three-phase systematic framework, utilizing behavioral data from six state-of-the-art LLMs across fifteen carefully selected normal-form games. Experiments show that LLMs exhibit consistent strategic reasoning levels across diverse opponents, confirming the framework's robustness and generalization capability. We also analyze the effects of two key mechanisms (Chat Mechanism and Memory Mechanism) on strategic reasoning performance. Results indicate that the Chat Mechanism significantly degrades strategic reasoning, whereas the Memory Mechanism enhances it. These insights position CHBench as a promising tool for evaluating LLM capabilities, with significant potential for future research and practical applications.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Jailbreaks with Intent-Aware LLMs</title>
<link>https://arxiv.org/abs/2508.12072</link>
<guid>https://arxiv.org/abs/2508.12072</guid>
<content:encoded><![CDATA[
arXiv:2508.12072v1 Announce Type: cross 
Abstract: Despite extensive safety-tuning, large language models (LLMs) remain vulnerable to jailbreak attacks via adversarially crafted instructions, reflecting a persistent trade-off between safety and task performance. In this work, we propose Intent-FT, a simple and lightweight fine-tuning approach that explicitly trains LLMs to infer the underlying intent of an instruction before responding. By fine-tuning on a targeted set of adversarial instructions, Intent-FT enables LLMs to generalize intent deduction to unseen attacks, thereby substantially improving their robustness. We comprehensively evaluate both parametric and non-parametric attacks across open-source and proprietary models, considering harmfulness from attacks, utility, over-refusal, and impact against white-box threats. Empirically, Intent-FT consistently mitigates all evaluated attack categories, with no single attack exceeding a 50\% success rate -- whereas existing defenses remain only partially effective. Importantly, our method preserves the model's general capabilities and reduces excessive refusals on benign instructions containing superficially harmful keywords. Furthermore, models trained with Intent-FT accurately identify hidden harmful intent in adversarial attacks, and these learned intentions can be effectively transferred to enhance vanilla model defenses.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models</title>
<link>https://arxiv.org/abs/2508.12081</link>
<guid>https://arxiv.org/abs/2508.12081</guid>
<content:encoded><![CDATA[
arXiv:2508.12081v1 Announce Type: cross 
Abstract: This paper introduces VimoRAG, a novel video-based retrieval-augmented motion generation framework for motion large language models (LLMs). As motion LLMs face severe out-of-domain/out-of-vocabulary issues due to limited annotated data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D motion generation by retrieving relevant 2D human motion signals. While video-based motion RAG is nontrivial, we address two key bottlenecks: (1) developing an effective motion-centered video retrieval model that distinguishes human poses and actions, and (2) mitigating the issue of error propagation caused by suboptimal retrieval results. We design the Gemini Motion Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer, enabling effective retrieval and generation processes. Experimental results show that VimoRAG significantly boosts the performance of motion LLMs constrained to text-only input.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Medical Event Models Improve with Scale</title>
<link>https://arxiv.org/abs/2508.12104</link>
<guid>https://arxiv.org/abs/2508.12104</guid>
<content:encoded><![CDATA[
arXiv:2508.12104v1 Announce Type: cross 
Abstract: Realizing personalized medicine at scale calls for methods that distill insights from longitudinal patient journeys, which can be viewed as a sequence of medical events. Foundation models pretrained on large-scale medical event data represent a promising direction for scaling real-world evidence generation and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with medical events from de-identified longitudinal health records for 16.3 billion encounters over 300 million unique patient records from 310 health systems, we introduce the Cosmos Medical Event Transformer ( CoMET) models, a family of decoder-only transformer models pretrained on 118 million patients representing 115 billion discrete medical events (151 billion tokens). We present the largest scaling-law study for medical event data, establishing a methodology for pretraining and revealing power-law scaling relationships for compute, tokens, and model size. Based on this, we pretrained a series of compute-optimal models with up to 1 billion parameters. Conditioned on a patient's real-world history, CoMET autoregressively generates the next medical event, simulating patient health timelines. We studied 78 real-world tasks, including diagnosis prediction, disease prognosis, and healthcare operations. Remarkably for a foundation model with generic pretraining and simulation-based inference, CoMET generally outperformed or matched task-specific supervised models on these tasks, without requiring task-specific fine-tuning or few-shot examples. CoMET's predictive power consistently improves as the model and pretraining scale. Our results show that CoMET, a generative medical event foundation model, can effectively capture complex clinical dynamics, providing an extensible and generalizable framework to support clinical decision-making, streamline healthcare operations, and improve patient outcomes.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections</title>
<link>https://arxiv.org/abs/2508.12116</link>
<guid>https://arxiv.org/abs/2508.12116</guid>
<content:encoded><![CDATA[
arXiv:2508.12116v1 Announce Type: cross 
Abstract: As numerous instruction-tuning datasets continue to emerge during the post-training stage, dynamically balancing and optimizing their mixtures has become a critical challenge. To address this, we propose DynamixSFT, a dynamic and automated method for instruction-tuning dataset mixture optimization. We formulate the problem as a multi-armed bandit setup and introduce a Prior-scaled Boltzmann Exploration that softly anchors the updated sampling distribution to the original dataset proportions, thereby preserving the inherent diversity and coverage of the collection. Sampling probabilities are updated using a lightweight 1-Step Look-ahead Reward, reflecting how much the dataset contributes to improving the model's performance at its current state. When applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning datasets, DynamixSFT achieves up to a 2.2% performance improvement across 10 benchmarks. Furthermore, we provide a comprehensive analysis and visualizations to offer deeper insights into the adaptive dynamics of our method.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position</title>
<link>https://arxiv.org/abs/2508.12398</link>
<guid>https://arxiv.org/abs/2508.12398</guid>
<content:encoded><![CDATA[
arXiv:2508.12398v1 Announce Type: cross 
Abstract: Diffusion Large Language Models (dLLMs) have recently emerged as a competitive non-autoregressive paradigm due to their unique training and inference approach. However, there is currently a lack of safety study on this novel architecture. In this paper, we present the first analysis of dLLMs' safety performance and propose a novel safety alignment method tailored to their unique generation characteristics. Specifically, we identify a critical asymmetry between the defender and attacker in terms of security. For the defender, we reveal that the middle tokens of the response, rather than the initial ones, are more critical to the overall safety of dLLM outputs; this seems to suggest that aligning middle tokens can be more beneficial to the defender. The attacker, on the contrary, may have limited power to manipulate middle tokens, as we find dLLMs have a strong tendency towards a sequential generation order in practice, forcing the attack to meet this distribution and diverting it from influencing the critical middle tokens. Building on this asymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method that directly aligns the model's middle generation with safe refusals exploiting reinforcement learning. We implement MOSA and compare its security performance against eight attack methods on two benchmarks. We also test the utility of MOSA-aligned dLLM on coding, math, and general reasoning. The results strongly prove the superiority of MOSA.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning</title>
<link>https://arxiv.org/abs/2508.12425</link>
<guid>https://arxiv.org/abs/2508.12425</guid>
<content:encoded><![CDATA[
arXiv:2508.12425v1 Announce Type: cross 
Abstract: This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved approach to standard CoT, for logical reasoning in large language models (LLMs). The key idea is to integrate lightweight symbolic representations into few-shot prompts, structuring the inference steps with a consistent strategy to make reasoning patterns more explicit within a non-iterative reasoning process. By incorporating these symbolic structures, our method preserves the generalizability of standard prompting techniques while enhancing the transparency, interpretability, and analyzability of LLM logical reasoning. Extensive experiments on four well-known logical reasoning benchmarks -- ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse reasoning scenarios -- demonstrate the effectiveness of the proposed approach, particularly in complex reasoning tasks that require navigating multiple constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs' reasoning capabilities across various model sizes and significantly outperforms conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and LogicalDeduction.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations</title>
<link>https://arxiv.org/abs/2508.12430</link>
<guid>https://arxiv.org/abs/2508.12430</guid>
<content:encoded><![CDATA[
arXiv:2508.12430v1 Announce Type: cross 
Abstract: Natural language explanations in visual question answering (VQA-NLE) aim to make black-box models more transparent by elucidating their decision-making processes. However, we find that existing VQA-NLE systems can produce inconsistent explanations and reach conclusions without genuinely understanding the underlying context, exposing weaknesses in either their inference pipeline or explanation-generation mechanism. To highlight these vulnerabilities, we not only leverage an existing adversarial strategy to perturb questions but also propose a novel strategy that minimally alters images to induce contradictory or spurious outputs. We further introduce a mitigation method that leverages external knowledge to alleviate these inconsistencies, thereby bolstering model robustness. Extensive evaluations on two standard benchmarks and two widely used VQA-NLE models underscore the effectiveness of our attacks and the potential of knowledge-based defenses, ultimately revealing pressing security and reliability concerns in current VQA-NLE systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insight Rumors: A Novel Textual Rumor Locating and Marking Model Leveraging Att_BiMamba2 Network</title>
<link>https://arxiv.org/abs/2508.12574</link>
<guid>https://arxiv.org/abs/2508.12574</guid>
<content:encoded><![CDATA[
arXiv:2508.12574v1 Announce Type: cross 
Abstract: With the development of social media networks, rumor detection models have attracted more and more attention. Whereas, these models primarily focus on classifying contexts as rumors or not, lacking the capability to locate and mark specific rumor content. To address this limitation, this paper proposes a novel rumor detection model named Insight Rumors to locate and mark rumor content within textual data. Specifically, we propose the Bidirectional Mamba2 Network with Dot-Product Attention (Att_BiMamba2), a network that constructs a bidirectional Mamba2 model and applies dot-product attention to weight and combine the outputs from both directions, thereby enhancing the representation of high-dimensional rumor features. Simultaneously, a Rumor Locating and Marking module is designed to locate and mark rumors. The module constructs a skip-connection network to project high-dimensional rumor features onto low-dimensional label features. Moreover, Conditional Random Fields (CRF) is employed to impose strong constraints on the output label features, ensuring accurate rumor content location. Additionally, a labeled dataset for rumor locating and marking is constructed, with the effectiveness of the proposed model is evaluated through comprehensive experiments. Extensive experiments indicate that the proposed scheme not only detects rumors accurately but also locates and marks them in context precisely, outperforming state-of-the-art schemes that can only discriminate rumors roughly.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM + ASP Workflow for Joint Entity-Relation Extraction</title>
<link>https://arxiv.org/abs/2508.12611</link>
<guid>https://arxiv.org/abs/2508.12611</guid>
<content:encoded><![CDATA[
arXiv:2508.12611v1 Announce Type: cross 
Abstract: Joint entity-relation extraction (JERE) identifies both entities and their relationships simultaneously. Traditional machine-learning based approaches to performing this task require a large corpus of annotated data and lack the ability to easily incorporate domain specific information in the construction of the model. Therefore, creating a model for JERE is often labor intensive, time consuming, and elaboration intolerant. In this paper, we propose harnessing the capabilities of generative pretrained large language models (LLMs) and the knowledge representation and reasoning capabilities of Answer Set Programming (ASP) to perform JERE. We present a generic workflow for JERE using LLMs and ASP. The workflow is generic in the sense that it can be applied for JERE in any domain. It takes advantage of LLM's capability in natural language understanding in that it works directly with unannotated text. It exploits the elaboration tolerant feature of ASP in that no modification of its core program is required when additional domain specific knowledge, in the form of type specifications, is found and needs to be used. We demonstrate the usefulness of the proposed workflow through experiments with limited training data on three well-known benchmarks for JERE. The results of our experiments show that the LLM + ASP workflow is better than state-of-the-art JERE systems in several categories with only 10\% of training data. It is able to achieve a 2.5 times (35\% over 15\%) improvement in the Relation Extraction task for the SciERC corpus, one of the most difficult benchmarks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation</title>
<link>https://arxiv.org/abs/2508.12680</link>
<guid>https://arxiv.org/abs/2508.12680</guid>
<content:encoded><![CDATA[
arXiv:2508.12680v1 Announce Type: cross 
Abstract: Despite their success, current training pipelines for reasoning VLMs focus on a limited range of tasks, such as mathematical and logical reasoning. As a result, these models face difficulties in generalizing their reasoning capabilities to a wide range of domains, primarily due to the scarcity of readily available and verifiable reward data beyond these narrowly defined areas. Moreover, integrating data from multiple domains is challenging, as the compatibility between domain-specific datasets remains uncertain. To address these limitations, we build a comprehensive RL-ready visual reasoning dataset from 46 data sources across 8 dimensions, covering a wide range of tasks such as infographic, mathematical, spatial, cross-image, graphic user interface, medical, common sense and general science. We propose an influence function based data selection and difficulty based filtering strategy to identify high-quality training samples from this dataset. Subsequently, we train the VLM, referred to as Vision-G1, using multi-round RL with a data curriculum to iteratively improve its visual reasoning capabilities. Our model achieves state-of-the-art performance across various visual reasoning benchmarks, outperforming similar-sized VLMs and even proprietary models like GPT-4o and Gemini-1.5 Flash. The model, code and dataset are publicly available at https://github.com/yuh-zha/Vision-G1.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Rubric Anchors</title>
<link>https://arxiv.org/abs/2508.12790</link>
<guid>https://arxiv.org/abs/2508.12790</guid>
<content:encoded><![CDATA[
arXiv:2508.12790v1 Announce Type: cross 
Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing Large Language Models (LLMs), exemplified by the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable signals-such as passing unit tests in code generation or matching correct answers in mathematical reasoning. While effective, this requirement largely confines RLVR to domains with automatically checkable outcomes. To overcome this, we extend the RLVR paradigm to open-ended tasks by integrating rubric-based rewards, where carefully designed rubrics serve as structured, model-interpretable criteria for automatic scoring of subjective outputs. We construct, to our knowledge, the largest rubric reward system to date, with over 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration. Implementing rubric-based RL is challenging; we tackle these issues with a clear framework and present an open-sourced Qwen-30B-A3B model with notable gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended benchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by +2.4%, while preserving general and reasoning abilities. 2) Our method provides fine-grained stylistic control, using rubrics as anchors to mitigate the "AI-like" tone and produce more human-like, expressive responses. We share key lessons in rubric construction, data selection, and training, and discuss limitations and future releases.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Human and LLM Judgments: Understanding and Narrowing the Gap</title>
<link>https://arxiv.org/abs/2508.12792</link>
<guid>https://arxiv.org/abs/2508.12792</guid>
<content:encoded><![CDATA[
arXiv:2508.12792v1 Announce Type: cross 
Abstract: Large language models are increasingly used as judges (LLM-as-a-judge) to evaluate model outputs at scale, but their assessments often diverge systematically from human judgments. We present Bridge, a unified statistical framework that explicitly bridges human and LLM evaluations under both absolute scoring and pairwise comparison paradigms. Bridge posits a latent human preference score for each prompt-response pair and models LLM deviations as linear transformations of covariates that capture sources of discrepancies. This offers a simple and principled framework for refining LLM ratings and characterizing systematic discrepancies between humans and LLMs. We provide an efficient fitting algorithm with asymptotic guarantees for statistical inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot Arena), Bridge achieves higher agreement with human ratings (accuracy, calibration, and KL divergence) and exposes systematic human-LLM gaps.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximum Score Routing For Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2508.12801</link>
<guid>https://arxiv.org/abs/2508.12801</guid>
<content:encoded><![CDATA[
arXiv:2508.12801v1 Announce Type: cross 
Abstract: Routing networks in sparsely activated mixture-of-experts (MoE) dynamically allocate input tokens to top-k experts through differentiable sparse transformations, enabling scalable model capacity while preserving computational efficiency. Traditional MoE networks impose an expert capacity constraint to ensure GPU-friendly computation. However, this leads to token dropping when capacity is saturated and results in low hardware efficiency due to padding in underutilized experts. Removing the capacity constraint, in turn, compromises load balancing and computational efficiency. To address these issues, we propose Maximum Score Routing ($\mathbf{MaxScore}$), a novel MoE routing paradigm that models routing as a minimum-cost maximum-flow problem and integrates a SoftTopk operator. MaxScore resolves the fundamental limitations of iterative rerouting and optimal transport formulations, achieving lower training losses and higher evaluation scores at equivalent FLOPs compared to both constrained and unconstrained baselines. Implementation details and experimental configurations can be obtained from $\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Steer: Input-dependent Steering for Multimodal LLMs</title>
<link>https://arxiv.org/abs/2508.12815</link>
<guid>https://arxiv.org/abs/2508.12815</guid>
<content:encoded><![CDATA[
arXiv:2508.12815v1 Announce Type: cross 
Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of LLMs towards enforcing a specific behavior. However, it remains largely underexplored for multimodal LLMs (MLLMs); furthermore, existing steering techniques, such as mean steering, rely on a single steering vector, applied independently of the input query. This paradigm faces limitations when the desired behavior is dependent on the example at hand. For example, a safe answer may consist in abstaining from answering when asked for an illegal activity, or may point to external resources or consultation with an expert when asked about medical advice. In this paper, we investigate a fine-grained steering that uses an input-specific linear shift. This shift is computed using contrastive input-specific prompting. However, the input-specific prompts required for this approach are not known at test time. Therefore, we propose to train a small auxiliary module to predict the input-specific steering vector. Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces hallucinations and enforces safety in MLLMs, outperforming other static baselines.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2508.12854</link>
<guid>https://arxiv.org/abs/2508.12854</guid>
<content:encoded><![CDATA[
arXiv:2508.12854v1 Announce Type: cross 
Abstract: Multimodal Empathetic Response Generation (MERG) is crucial for building emotionally intelligent human-computer interactions. Although large language models (LLMs) have improved text-based ERG, challenges remain in handling multimodal emotional content and maintaining identity consistency. Thus, we propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System based on multimodal LLMs which decomposes MERG task into three parts: multimodal empathy understanding, empathy memory retrieval, and multimodal response generation. By integrating advanced expressive speech and video generative models, E3RG delivers natural, emotionally rich, and identity-consistent responses without extra training. Experiments validate the superiority of our system on both zero-shot and few-shot settings, securing Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25. Our code is available at https://github.com/RH-Lin/E3RG.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML</title>
<link>https://arxiv.org/abs/2508.12905</link>
<guid>https://arxiv.org/abs/2508.12905</guid>
<content:encoded><![CDATA[
arXiv:2508.12905v1 Announce Type: cross 
Abstract: We introduce TCUQ, a single pass, label free uncertainty monitor for streaming TinyML that converts short horizon temporal consistency captured via lightweight signals on posteriors and features into a calibrated risk score with an O(W ) ring buffer and O(1) per step updates. A streaming conformal layer turns this score into a budgeted accept/abstain rule, yielding calibrated behavior without online labels or extra forward passes. On microcontrollers, TCUQ fits comfortably on kilobyte scale devices and reduces footprint and latency versus early exit and deep ensembles (typically about 50 to 60% smaller and about 30 to 45% faster), while methods of similar accuracy often run out of memory. Under corrupted in distribution streams, TCUQ improves accuracy drop detection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high severities; for failure detection it attains up to 0.92 AUROC. These results show that temporal consistency, coupled with streaming conformal calibration, provides a practical and resource efficient foundation for on device monitoring in TinyML.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML</title>
<link>https://arxiv.org/abs/2508.12907</link>
<guid>https://arxiv.org/abs/2508.12907</guid>
<content:encoded><![CDATA[
arXiv:2508.12907v1 Announce Type: cross 
Abstract: We introduce \textbf{SNAP-UQ}, a single-pass, label-free uncertainty method for TinyML that estimates risk from \emph{depth-wise next-activation prediction}: tiny int8 heads forecast the statistics of the next layer from a compressed view of the previous one, and a lightweight monotone mapper turns the resulting surprisal into an actionable score. The design requires no temporal buffers, auxiliary exits, or repeated forward passes, and adds only a few tens of kilobytes to MCU deployments. Across vision and audio backbones, SNAP-UQ consistently reduces flash and latency relative to early-exit and deep ensembles (typically $\sim$40--60\% smaller and $\sim$25--35\% faster), with competing methods of similar accuracy often exceeding memory limits. In corrupted streams it improves accuracy-drop detection by several AUPRC points and maintains strong failure detection (AUROC $\approx$0.9) in a single pass. Grounding uncertainty in layer-to-layer dynamics yields a practical, resource-efficient basis for on-device monitoring in TinyML.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models</title>
<link>https://arxiv.org/abs/2508.13021</link>
<guid>https://arxiv.org/abs/2508.13021</guid>
<content:encoded><![CDATA[
arXiv:2508.13021v1 Announce Type: cross 
Abstract: Recent advances in masked diffusion models (MDMs) have established them as powerful non-autoregressive alternatives for sequence generation. Nevertheless, our preliminary experiments reveal that the generation quality of MDMs is still highly sensitive to the choice of decoding strategy. In particular, widely adopted uncertainty-based samplers suffer from two key limitations: a lack of global trajectory control and a pronounced bias toward trivial tokens in the early stages of decoding. These shortcomings restrict the full potential of MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling (PC-Sampler), a novel decoding strategy that unifies global trajectory planning with content-aware informativeness maximization. PC-Sampler incorporates a position-aware weighting mechanism to regulate the decoding path and a calibrated confidence score to suppress the premature selection of trivial tokens. Extensive experiments on three advanced MDMs across seven challenging benchmarks-including logical reasoning and planning tasks-demonstrate that PC-Sampler consistently outperforms existing MDM decoding strategies by more than 10% on average, significantly narrowing the performance gap with state-of-the-art autoregressive models. All codes are available at https://github.com/NEUIR/PC-Sampler.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Has GPT-5 Achieved Spatial Intelligence? An Empirical Study</title>
<link>https://arxiv.org/abs/2508.13142</link>
<guid>https://arxiv.org/abs/2508.13142</guid>
<content:encoded><![CDATA[
arXiv:2508.13142v1 Announce Type: cross 
Abstract: Multi-modal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, which are fundamental capabilities to achieving artificial general intelligence. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models stand on the path toward spatial intelligence. First, we propose a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and discuss the challenges in ensuring fair evaluation. We then evaluate state-of-the-art proprietary and open-source models on eight key benchmarks, at a cost exceeding one billion total tokens. Our empirical study reveals that (1) GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2) still falls short of human performance across a broad spectrum of tasks. Moreover, we (3) identify the more challenging spatial intelligence problems for multi-modal models, and (4) proprietary models do not exhibit a decisive advantage when facing the most difficult problems. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans yet fail even the most advanced multi-modal models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Smaller Always Faster? Tradeoffs in Compressing Self-Supervised Speech Transformers</title>
<link>https://arxiv.org/abs/2211.09949</link>
<guid>https://arxiv.org/abs/2211.09949</guid>
<content:encoded><![CDATA[
arXiv:2211.09949v4 Announce Type: replace 
Abstract: Transformer-based self-supervised models have achieved remarkable success in speech processing, but their large size and high inference cost present significant challenges for real-world deployment. While numerous compression techniques have been proposed, inconsistent evaluation metrics make it difficult to compare their practical effectiveness. In this work, we conduct a comprehensive study of four common compression methods, including weight pruning, head pruning, low-rank approximation, and knowledge distillation on self-supervised speech Transformers. We evaluate each method under three key metrics: parameter count, multiply-accumulate operations, and real-time factor. Results show that each method offers distinct advantages. In addition, we contextualize recent compression techniques, comparing DistilHuBERT, FitHuBERT, LightHuBERT, ARMHuBERT, and STaRHuBERT under the same framework, offering practical guidance on compression for deployment.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language models can replicate cross-cultural differences in personality</title>
<link>https://arxiv.org/abs/2310.10679</link>
<guid>https://arxiv.org/abs/2310.10679</guid>
<content:encoded><![CDATA[
arXiv:2310.10679v4 Announce Type: replace 
Abstract: We use a large-scale experiment (N=8000) to determine whether GPT-4 can replicate cross-cultural differences in the Big Five, measured using the Ten-Item Personality Inventory. We used the US and South Korea as the cultural pair, given that prior research suggests substantial personality differences between people from these two countries. We manipulated the target of the simulation (US vs. Korean), the language of the inventory (English vs. Korean), and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity. We provide preliminary evidence that LLMs can aid cross-cultural researchers and practitioners.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MHPP: Exploring the Capabilities and Limitations of Language Models Beyond Basic Code Generation</title>
<link>https://arxiv.org/abs/2405.11430</link>
<guid>https://arxiv.org/abs/2405.11430</guid>
<content:encoded><![CDATA[
arXiv:2405.11430v3 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have greatly improved code generation, specifically at the function level. For instance, GPT-4o has achieved a 91.0\% pass rate on HumanEval. However, this draws into question the adequacy of existing benchmarks in thoroughly assessing function-level code generation capabilities. Our study analyzed two common benchmarks, HumanEval and MBPP, and found that these might not thoroughly evaluate LLMs' code generation capacities due to limitations in quality, difficulty, and granularity. To resolve this, we introduce the Mostly Hard Python Problems (MHPP) dataset, consisting of 210 unique human-curated problems. By focusing on the combination of natural language and code reasoning, MHPP gauges LLMs' abilities to comprehend specifications and restrictions, engage in multi-step reasoning, and apply coding knowledge effectively. Initial evaluations of 26 LLMs using MHPP showed many high-performing models on HumanEval failed to achieve similar success on MHPP. Moreover, MHPP highlighted various previously undiscovered limitations within various LLMs, leading us to believe that it could pave the way for a better understanding of LLMs' capabilities and limitations. MHPP, evaluation pipeline, and leaderboard can be found in https://github.com/SparksofAGI/MHPP.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FacLens: Transferable Probe for Foreseeing Non-Factuality in Fact-Seeking Question Answering of Large Language Models</title>
<link>https://arxiv.org/abs/2406.05328</link>
<guid>https://arxiv.org/abs/2406.05328</guid>
<content:encoded><![CDATA[
arXiv:2406.05328v4 Announce Type: replace 
Abstract: Despite advancements in large language models (LLMs), non-factual responses still persist in fact-seeking question answering. Unlike extensive studies on post-hoc detection of these responses, this work studies non-factuality prediction (NFP), predicting whether an LLM will generate a non-factual response prior to the response generation. Previous NFP methods have shown LLMs' awareness of their knowledge, but they face challenges in terms of efficiency and transferability. In this work, we propose a lightweight model named Factuality Lens (FacLens), which effectively probes hidden representations of fact-seeking questions for the NFP task. Moreover, we discover that hidden question representations sourced from different LLMs exhibit similar NFP patterns, enabling the transferability of FacLens across different LLMs to reduce development costs. Extensive experiments highlight FacLens's superiority in both effectiveness and efficiency.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2Cap: A Benchmark and a Baseline for Singing Style Captioning</title>
<link>https://arxiv.org/abs/2409.09866</link>
<guid>https://arxiv.org/abs/2409.09866</guid>
<content:encoded><![CDATA[
arXiv:2409.09866v3 Announce Type: replace 
Abstract: Singing voices contain much richer information than common voices, including varied vocal and acoustic properties. However, current open-source audio-text datasets for singing voices capture only a narrow range of attributes and lack acoustic features, leading to limited utility towards downstream tasks, such as style captioning. To fill this gap, we formally define the singing style captioning task and present S2Cap, a dataset of singing voices with detailed descriptions covering diverse vocal, acoustic, and demographic characteristics. Using this dataset, we develop an efficient and straightforward baseline algorithm for singing style captioning. The dataset is available at https://zenodo.org/records/15673764.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming</title>
<link>https://arxiv.org/abs/2409.11041</link>
<guid>https://arxiv.org/abs/2409.11041</guid>
<content:encoded><![CDATA[
arXiv:2409.11041v3 Announce Type: replace 
Abstract: While there has been a lot of research recently on robots in household environments, at the present time, most robots in existence can be found on shop floors, and most interactions between humans and robots happen there. ``Collaborative robots'' (cobots) designed to work alongside humans on assembly lines traditionally require expert programming, limiting ability to make changes, or manual guidance, limiting expressivity of the resulting programs. To address these limitations, we explore using Large Language Models (LLMs), and in particular, their abilities of doing in-context learning, for conversational code generation. As a first step, we define RATS, the ``Repetitive Assembly Task'', a 2D building task designed to lay the foundation for simulating industry assembly scenarios. In this task, a `programmer' instructs a cobot, using natural language, on how a certain assembly is to be built; that is, the programmer induces a program, through natural language. We create a dataset that pairs target structures with various example instructions (human-authored, template-based, and model-generated) and example code. With this, we systematically evaluate the capabilities of state-of-the-art LLMs for synthesising this kind of code, given in-context examples. Evaluating in a simulated environment, we find that LLMs are capable of generating accurate `first order code' (instruction sequences), but have problems producing `higher-order code' (abstractions such as functions, or use of loops).
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Are In-Context Bandit Reinforcement Learners</title>
<link>https://arxiv.org/abs/2410.05362</link>
<guid>https://arxiv.org/abs/2410.05362</guid>
<content:encoded><![CDATA[
arXiv:2410.05362v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel at in-context learning (ICL), a supervised learning technique that relies on adding annotated examples to the model context. We investigate a contextual bandit version of in-context reinforcement learning (ICRL), where models learn in-context, online, from external reward, instead of supervised data. We show that LLMs effectively demonstrate such learning, and provide a detailed study of the phenomena, experimenting with challenging classification tasks and models of sizes from 500M to 70B parameters. This includes identifying and addressing the instability of the process, demonstrating learning with both semantic and abstract labels, and showing scaling trends. Our findings highlight ICRL capabilities in LLMs, while also underscoring fundamental limitations in their implicit reasoning about errors.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StepTool: Enhancing Multi-Step Tool Usage in LLMs via Step-Grained Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.07745</link>
<guid>https://arxiv.org/abs/2410.07745</guid>
<content:encoded><![CDATA[
arXiv:2410.07745v4 Announce Type: replace 
Abstract: Despite their powerful text generation capabilities, large language models (LLMs) still struggle to effectively utilize external tools to solve complex tasks, a challenge known as tool learning. Existing methods primarily rely on supervised fine-tuning, treating tool learning as a text generation problem while overlooking the decision-making complexities inherent in multi-step contexts. In this work, we propose modeling tool learning as a dynamic decision-making process and introduce StepTool, a novel step-grained reinforcement learning framework that enhances LLMs' capabilities in multi-step tool use. StepTool comprises two key components: Step-grained Reward Shaping, which assigns rewards to each tool interaction based on its invocation success and contribution to task completion; and Step-grained Optimization, which applies policy gradient methods to optimize the model across multiple decision steps. Extensive experiments across diverse benchmarks show that StepTool consistently outperforms both SFT-based and RL-based baselines in terms of task Pass Rate and Recall of relevant tools. Furthermore, our analysis suggests that StepTool helps models discover new tool-use strategies rather than merely re-weighting prior knowledge. These results highlight the importance of fine-grained decision modeling in tool learning and establish StepTool as a general and robust solution for enhancing multi-step tool use in LLMs. Code and data are available at https://github.com/yuyq18/StepTool.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection</title>
<link>https://arxiv.org/abs/2411.01077</link>
<guid>https://arxiv.org/abs/2411.01077</guid>
<content:encoded><![CDATA[
arXiv:2411.01077v5 Announce Type: replace 
Abstract: Jailbreaking techniques trick Large Language Models (LLMs) into producing restricted output, posing a potential threat. One line of defense is to use another LLM as a Judge to evaluate the harmfulness of generated text. However, we reveal that these Judge LLMs are vulnerable to token segmentation bias, an issue that arises when delimiters alter the tokenization process, splitting words into smaller sub-tokens. This alters the embeddings of the entire sequence, reducing detection accuracy and allowing harmful content to be misclassified as safe. In this paper, we introduce Emoji Attack, a novel strategy that amplifies existing jailbreak prompts by exploiting token segmentation bias. Our method leverages in-context learning to systematically insert emojis into text before it is evaluated by a Judge LLM, inducing embedding distortions that significantly lower the likelihood of detecting unsafe content. Unlike traditional delimiters, emojis also introduce semantic ambiguity, making them particularly effective in this attack. Through experiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack substantially reduces the unsafe prediction rate, bypassing existing safeguards.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models</title>
<link>https://arxiv.org/abs/2411.02083</link>
<guid>https://arxiv.org/abs/2411.02083</guid>
<content:encoded><![CDATA[
arXiv:2411.02083v3 Announce Type: replace 
Abstract: While language models have exceptional capabilities at text generation, they lack a natural inductive bias for emitting numbers and thus struggle in tasks involving quantitative reasoning, especially arithmetic. One fundamental limitation is the nature of the cross-entropy (CE) loss, which assumes a nominal scale and thus cannot convey proximity between generated number tokens. In response, we here present a regression-like loss that operates purely on token level. Our proposed Number Token Loss (NTL) comes in two flavors and minimizes either the $L_p$ norm or the Wasserstein distance between the numerical values of the real and predicted number tokens. NTL can easily be added to any language model and extend the CE objective during training without runtime overhead. We evaluate the proposed scheme on various mathematical datasets and find that it consistently improves performance in math-related tasks. In a direct comparison on a regression task, we find that NTL can match the performance of a regression head, despite operating on token level. Finally, we scale NTL up to 3B parameter models and observe improved performance, demonstrating its potential for seamless integration into LLMs. We hope to inspire LLM developers to improve their pretraining objectives and distribute NTL as a minimalistic and lightweight PyPI package $ntloss$: https://github.com/ai4sd/number-token-loss. Development code for full paper reproduction is available separately.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NormXLogit: The Head-on-Top Never Lies</title>
<link>https://arxiv.org/abs/2411.16252</link>
<guid>https://arxiv.org/abs/2411.16252</guid>
<content:encoded><![CDATA[
arXiv:2411.16252v2 Announce Type: replace 
Abstract: With new large language models (LLMs) emerging frequently, it is important to consider the potential value of model-agnostic approaches that can provide interpretability across a variety of architectures. While recent advances in LLM interpretability show promise, many rely on complex, model-specific methods with high computational costs. To address these limitations, we propose NormXLogit, a novel technique for assessing the significance of individual input tokens. This method operates based on the input and output representations associated with each token. First, we demonstrate that during the pre-training of LLMs, the norms of word embeddings effectively capture token importance. Second, we reveal a significant relationship between a token's importance and the extent to which its representation can resemble the model's final prediction. Extensive analyses reveal that our approach outperforms existing gradient-based methods in terms of faithfulness and offers competitive performance in layer-wise explanations compared to leading architecture-specific techniques.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Fusing ChatGPT and Ensemble Learning in Discon-tinuous Named Entity Recognition in Health Corpora</title>
<link>https://arxiv.org/abs/2412.16976</link>
<guid>https://arxiv.org/abs/2412.16976</guid>
<content:encoded><![CDATA[
arXiv:2412.16976v3 Announce Type: replace 
Abstract: Named Entity Recognition has traditionally been a key task in natural language processing, aiming to identify and extract important terms from unstructured text data. However, a notable challenge for contemporary deep-learning NER models has been identifying discontinuous entities, which are often fragmented within the text. To date, methods to address Discontinuous Named Entity Recognition have not been explored using ensemble learning to the best of our knowledge. Furthermore, the rise of large language models, such as ChatGPT in recent years, has shown significant effectiveness across many NLP tasks. Most existing approaches, however, have primarily utilized ChatGPT as a problem-solving tool rather than exploring its potential as an integrative element within ensemble learning algorithms. In this study, we investigated the integration of ChatGPT as an arbitrator within an ensemble method, aiming to enhance performance on DNER tasks. Our method combines five state-of-the-art NER models with ChatGPT using custom prompt engineering to assess the robustness and generalization capabilities of the ensemble algorithm. We conducted experiments on three benchmark medical datasets, comparing our method against the five SOTA models, individual applications of GPT-3.5 and GPT-4, and a voting ensemble method. The results indicate that our proposed fusion of ChatGPT with the ensemble learning algorithm outperforms the SOTA results in the CADEC, ShARe13, and ShARe14 datasets, showcasing its potential to enhance NLP applications in the healthcare domain.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Idiom Detection in Sorani Kurdish Texts</title>
<link>https://arxiv.org/abs/2501.14528</link>
<guid>https://arxiv.org/abs/2501.14528</guid>
<content:encoded><![CDATA[
arXiv:2501.14528v4 Announce Type: replace 
Abstract: Idiom detection using Natural Language Processing (NLP) is the computerized process of recognizing figurative expressions within a text that convey meanings beyond the literal interpretation of the words. While idiom detection has seen significant progress across various languages, the Kurdish language faces a considerable research gap in this area despite the importance of idioms in tasks like machine translation and sentiment analysis. This study addresses idiom detection in Sorani Kurdish by approaching it as a text classification task using deep learning techniques. To tackle this, we developed a dataset containing 10,580 sentences embedding 101 Sorani Kurdish idioms across diverse contexts. Using this dataset, we developed and evaluated three deep learning models: KuBERT-based transformer sequence classification, a Recurrent Convolutional Neural Network (RCNN), and a BiLSTM model with an attention mechanism. The evaluations revealed that the transformer model, the fine-tuned BERT, consistently outperformed the others, achieving nearly 99% accuracy while the RCNN achieved 96.5% and the BiLSTM 80%. These results highlight the effectiveness of Transformer-based architectures in low-resource languages like Kurdish. This research provides a dataset, three optimized models, and insights into idiom detection, laying a foundation for advancing Kurdish NLP.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2SSP: A Two-Stage Framework for Structured Pruning of LLMs</title>
<link>https://arxiv.org/abs/2501.17771</link>
<guid>https://arxiv.org/abs/2501.17771</guid>
<content:encoded><![CDATA[
arXiv:2501.17771v2 Announce Type: replace 
Abstract: We propose a novel Two-Stage framework for Structured Pruning (\textsc{2SSP}) for pruning Large Language Models (LLMs), which combines two different strategies of pruning, namely Width and Depth Pruning. The first stage (Width Pruning) removes entire neurons, hence their corresponding rows and columns, aiming to preserve the connectivity among the pruned structures in the intermediate state of the Feed-Forward Networks in each Transformer block. This is done based on an importance score measuring the impact of each neuron on the output magnitude. The second stage (Depth Pruning), instead, removes entire Attention submodules. This is done by applying an iterative process that removes the Attention with the minimum impact on a given metric of interest (in our case, perplexity). We also propose a novel mechanism to balance the sparsity rate of the two stages w.r.t. to the desired global sparsity. We test \textsc{2SSP} on four LLM families and three sparsity rates (25\%, 37.5\%, and 50\%), measuring the resulting perplexity over three language modeling datasets as well as the performance over six downstream tasks. Our method consistently outperforms five state-of-the-art competitors over three language modeling and six downstream tasks, with an up to two-order-of-magnitude gain in terms of pruning time. The code is available at https://github.com/FabrizioSandri/2SSP.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisualSpeech: Enhancing Prosody Modeling in TTS Using Video</title>
<link>https://arxiv.org/abs/2501.19258</link>
<guid>https://arxiv.org/abs/2501.19258</guid>
<content:encoded><![CDATA[
arXiv:2501.19258v2 Announce Type: replace 
Abstract: Text-to-Speech (TTS) synthesis faces the inherent challenge of producing multiple speech outputs with varying prosody given a single text input. While previous research has addressed this by predicting prosodic information from both text and speech, additional contextual information, such as video, remains under-utilized despite being available in many applications. This paper investigates the potential of integrating visual context to enhance prosody prediction. We propose a novel model, VisualSpeech, which incorporates visual and textual information for improving prosody generation in TTS. Empirical results indicate that incorporating visual features improves prosodic modeling, enhancing the expressiveness of the synthesized speech. Audio samples are available at https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dealing with Annotator Disagreement in Hate Speech Classification</title>
<link>https://arxiv.org/abs/2502.08266</link>
<guid>https://arxiv.org/abs/2502.08266</guid>
<content:encoded><![CDATA[
arXiv:2502.08266v2 Announce Type: replace 
Abstract: Hate speech detection is a crucial task, especially on social media, where harmful content can spread quickly. Implementing machine learning models to automatically identify and address hate speech is essential for mitigating its impact and preventing its proliferation. The first step in developing an effective hate speech detection model is to acquire a high-quality dataset for training. Labeled data is essential for most natural language processing tasks, but categorizing hate speech is difficult due to the diverse and often subjective nature of hate speech, which can lead to varying interpretations and disagreements among annotators. This paper examines strategies for addressing annotator disagreement, an issue that has been largely overlooked. In particular, we evaluate various automatic approaches for aggregating multiple annotations, in the context of hate speech classification in Turkish tweets. Our work highlights the importance of the problem and provides state-of-the-art benchmark results for the detection and understanding of hate speech in online discourse.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIDDIA: Language-based Intelligent Drug Discovery Agent</title>
<link>https://arxiv.org/abs/2502.13959</link>
<guid>https://arxiv.org/abs/2502.13959</guid>
<content:encoded><![CDATA[
arXiv:2502.13959v2 Announce Type: replace 
Abstract: Drug discovery is a long, expensive, and complex process, relying heavily on human medicinal chemists, who can spend years searching the vast space of potential therapies. Recent advances in artificial intelligence for chemistry have sought to expedite individual drug discovery tasks; however, there remains a critical need for an intelligent agent that can navigate the drug discovery process. Towards this end, we introduce LIDDIA, an autonomous agent capable of intelligently navigating the drug discovery process in silico. By leveraging the reasoning capabilities of large language models, LIDDIA serves as a low-cost and highly-adaptable tool for autonomous drug discovery. We comprehensively examine LIDDIA , demonstrating that (1) it can generate molecules meeting key pharmaceutical criteria on over 70% of 30 clinically relevant targets, (2) it intelligently balances exploration and exploitation in the chemical space, and (3) it identifies one promising novel candidate on AR/NR3C4, a critical target for both prostate and breast cancers. Code and dataset are available at https://github.com/ninglab/LIDDiA
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities</title>
<link>https://arxiv.org/abs/2503.04721</link>
<guid>https://arxiv.org/abs/2503.04721</guid>
<content:encoded><![CDATA[
arXiv:2503.04721v3 Announce Type: replace 
Abstract: Spoken dialogue modeling poses challenges beyond text-based language modeling, requiring real-time interaction, turn-taking, and backchanneling. While most Spoken Dialogue Models (SDMs) operate in half-duplex mode-processing one turn at a time - emerging full-duplex SDMs can listen and speak simultaneously, enabling more natural conversations. However, current evaluations remain limited, focusing mainly on turn-based metrics or coarse corpus-level analyses. To address this, we introduce Full-Duplex-Bench, a benchmark that systematically evaluates key interactive behaviors: pause handling, backchanneling, turn-taking, and interruption management. Our framework uses automatic metrics for consistent, reproducible assessment and provides a fair, fast evaluation setup. By releasing our benchmark and code, we aim to advance spoken dialogue modeling and foster the development of more natural and engaging SDMs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Information-Theoretic Approach to Identifying Formulaic Clusters in Textual Data</title>
<link>https://arxiv.org/abs/2503.07303</link>
<guid>https://arxiv.org/abs/2503.07303</guid>
<content:encoded><![CDATA[
arXiv:2503.07303v2 Announce Type: replace 
Abstract: Texts, whether literary or historical, exhibit structural and stylistic patterns shaped by their purpose, authorship, and cultural context. Formulaic texts, characterized by repetition and constrained expression, tend to have lower variability in self-information compared to more dynamic compositions. Identifying such patterns in historical documents, particularly multi-author texts like the Hebrew Bible provides insights into their origins, purpose, and transmission.
  This study aims to identify formulaic clusters -- sections exhibiting systematic repetition and structural constraints -- by analyzing recurring phrases, syntactic structures, and stylistic markers. However, distinguishing formulaic from non-formulaic elements in an unsupervised manner presents a computational challenge, especially in high-dimensional textual spaces where patterns must be inferred without predefined labels.
  To address this, we develop an information-theoretic algorithm leveraging weighted self-information distributions to detect structured patterns in text, unlike covariance-based methods, which become unstable in small-sample, high-dimensional settings, our approach directly models variations in self-information to identify formulaicity. By extending classical discrete self-information measures with a continuous formulation based on differential self-information, our method remains applicable across different types of textual representations, including neural embeddings under Gaussian priors.
  Applied to hypothesized authorial divisions in the Hebrew Bible, our approach successfully isolates stylistic layers, providing a quantitative framework for textual stratification. This method enhances our ability to analyze compositional patterns, offering deeper insights into the literary and cultural evolution of texts shaped by complex authorship and editorial processes.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Dimensional Interlingual Representations of Large Language Models</title>
<link>https://arxiv.org/abs/2503.11280</link>
<guid>https://arxiv.org/abs/2503.11280</guid>
<content:encoded><![CDATA[
arXiv:2503.11280v5 Announce Type: replace 
Abstract: Large language models (LLMs) trained on massive multilingual datasets hint at the formation of interlingual constructs--a shared subspace in the representation space. However, evidence regarding this phenomenon is mixed, leaving it unclear whether these models truly develop unified interlingual representations, or present a partially aligned constructs. We explore 31 diverse languages varying on their resource-levels, typologies, and geographical regions; and find that multilingual LLMs exhibit inconsistent cross-lingual alignments. To address this, we propose an interlingual representation framework identifying both the shared interlingual semantic subspace and fragmented components, existed due to representational limitations. We introduce Interlingual Local Overlap (ILO) score to quantify interlingual alignment by comparing the local neighborhood structures of high-dimensional representations. We utilize ILO to investigate the impact of single-language fine-tuning on the interlingual representations in multilingual LLMs. Our results indicate that training exclusively on a single language disrupts the alignment in early layers, while freezing these layers preserves the alignment of interlingual representations, leading to improved cross-lingual generalization. These results validate our framework and metric for evaluating interlingual representation, and further underscore that interlingual alignment is crucial for scalable multilingual learning.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Women, Same Stereotypes: Unpacking the Gender Bias Paradox in Large Language Models</title>
<link>https://arxiv.org/abs/2503.15904</link>
<guid>https://arxiv.org/abs/2503.15904</guid>
<content:encoded><![CDATA[
arXiv:2503.15904v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have revolutionized natural language processing, yet concerns persist regarding their tendency to reflect or amplify social biases. This study introduces a novel evaluation framework to uncover gender biases in LLMs: using free-form storytelling to surface biases embedded within the models. A systematic analysis of ten prominent LLMs shows a consistent pattern of overrepresenting female characters across occupations, likely due to supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Paradoxically, despite this overrepresentation, the occupational gender distributions produced by these LLMs align more closely with human stereotypes than with real-world labor data. This highlights the challenge and importance of implementing balanced mitigation measures to promote fairness and prevent the establishment of potentially new biases. We release the prompts and LLM-generated stories at GitHub.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastCuRL: Curriculum Reinforcement Learning with Stage-wise Context Scaling for Efficient Training R1-like Reasoning Models</title>
<link>https://arxiv.org/abs/2503.17287</link>
<guid>https://arxiv.org/abs/2503.17287</guid>
<content:encoded><![CDATA[
arXiv:2503.17287v5 Announce Type: replace 
Abstract: Improving training efficiency continues to be one of the primary challenges in large-scale Reinforcement Learning (RL). In this paper, we investigate how context length and the complexity of training data influence the RL scaling training process of R1-distilled reasoning models, e.g., DeepSeek-R1-Distill-Qwen-1.5B. Our experimental results reveal that: (1) simply controlling the context length and curating the training data based on the input prompt length can effectively improve the training efficiency of RL scaling, achieving better performance with more concise CoT; (2) properly scaling the context length helps mitigate entropy collapse; and (3) carefully choosing the context length facilitates achieving efficient LLM training and reasoning. Inspired by these insights, we propose FastCuRL, a curriculum RL framework with stage-wise context scaling to achieve efficient LLM training and reasoning. Extensive experimental results demonstrate that FastCuRL-1.5B-V3 significantly outperforms state-of-the-art reasoning models on five competition-level benchmarks and achieves 49.6% accuracy on AIME 2024. Furthermore, FastCuRL-1.5B-Preview surpasses DeepScaleR-1.5B-Preview on five benchmarks while only using a single node with 8 GPUs and a total of 50% of training steps.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCORE: Story Coherence and Retrieval Enhancement for AI Narratives</title>
<link>https://arxiv.org/abs/2503.23512</link>
<guid>https://arxiv.org/abs/2503.23512</guid>
<content:encoded><![CDATA[
arXiv:2503.23512v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) can generate creative and engaging narratives from user-specified input, but maintaining coherence and emotional depth throughout these AI-generated stories remains a challenge. In this work, we propose SCORE, a framework for Story Coherence and Retrieval Enhancement, designed to detect and resolve narrative inconsistencies. By tracking key item statuses and generating episode summaries, SCORE uses a Retrieval-Augmented Generation (RAG) approach, incorporating TF-IDF and cosine similarity to identify related episodes and enhance the overall story structure. Results from testing multiple LLM-generated stories demonstrate that SCORE significantly improves the consistency and stability of narrative coherence compared to baseline GPT models, providing a more robust method for evaluating and refining AI-generated narratives.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection</title>
<link>https://arxiv.org/abs/2503.24115</link>
<guid>https://arxiv.org/abs/2503.24115</guid>
<content:encoded><![CDATA[
arXiv:2503.24115v4 Announce Type: replace 
Abstract: The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpectR: Dynamically Composing LM Experts with Spectral Routing</title>
<link>https://arxiv.org/abs/2504.03454</link>
<guid>https://arxiv.org/abs/2504.03454</guid>
<content:encoded><![CDATA[
arXiv:2504.03454v2 Announce Type: replace 
Abstract: Training large, general-purpose language models poses significant challenges. The growing availability of specialized expert models, fine-tuned from pretrained models for specific tasks or domains, offers a promising alternative. Leveraging the potential of these existing expert models in real-world applications requires effective methods to select or merge the models best suited for a given task. This paper introduces SPECTR, an approach for dynamically composing expert models at each time step during inference. Notably, our method requires no additional training and enables flexible, token- and layer-wise model combinations. Our experimental results demonstrate that SPECTR improves routing accuracy over alternative training-free methods, increasing task performance across expert domains.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models</title>
<link>https://arxiv.org/abs/2504.04823</link>
<guid>https://arxiv.org/abs/2504.04823</guid>
<content:encoded><![CDATA[
arXiv:2504.04823v2 Announce Type: replace 
Abstract: Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this paper, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes are open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling</title>
<link>https://arxiv.org/abs/2504.05410</link>
<guid>https://arxiv.org/abs/2504.05410</guid>
<content:encoded><![CDATA[
arXiv:2504.05410v2 Announce Type: replace 
Abstract: The dominant approach to generating from language models subject to some constraint is locally constrained decoding (LCD), incrementally sampling tokens at each time step such that the constraint is never violated. Typically, this is achieved through token masking: looping over the vocabulary and excluding non-conforming tokens. There are two important problems with this approach. (i) Evaluating the constraint on every token can be prohibitively expensive -- LM vocabularies often exceed $100,000$ tokens. (ii) LCD can distort the global distribution over strings, sampling tokens based only on local information, even if they lead down dead-end paths. This work introduces a new algorithm that addresses both these problems. First, to avoid evaluating a constraint on the full vocabulary at each step of generation, we propose an adaptive rejection sampling algorithm that typically requires orders of magnitude fewer constraint evaluations. Second, we show how this algorithm can be extended to produce low-variance, unbiased estimates of importance weights at a very small additional cost -- estimates that can be soundly used within previously proposed sequential Monte Carlo algorithms to correct for the myopic behavior of local constraint enforcement. Through extensive empirical evaluation in text-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON domains, we show that our approach is superior to state-of-the-art baselines, supporting a broader class of constraints and improving both runtime and performance. Additional theoretical and empirical analyses show that our method's runtime efficiency is driven by its dynamic use of computation, scaling with the divergence between the unconstrained and constrained LM, and as a consequence, runtime improvements are greater for better models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvalAgent: Discovering Implicit Evaluation Criteria from the Web</title>
<link>https://arxiv.org/abs/2504.15219</link>
<guid>https://arxiv.org/abs/2504.15219</guid>
<content:encoded><![CDATA[
arXiv:2504.15219v2 Announce Type: replace 
Abstract: Evaluation of language model outputs on structured writing tasks is typically conducted with a number of desirable criteria presented to human evaluators or large language models (LLMs). For instance, on a prompt like "Help me draft an academic talk on coffee intake vs research productivity", a model response may be evaluated for criteria like accuracy and coherence. However, high-quality responses should do more than just satisfy basic task requirements. An effective response to this query should include quintessential features of an academic talk, such as a compelling opening, clear research questions, and a takeaway. To help identify these implicit criteria, we introduce EvalAgent, a novel framework designed to automatically uncover nuanced and task-specific criteria. EvalAgent first mines expert-authored online guidance. It then uses this evidence to propose diverse, long-tail evaluation criteria that are grounded in reliable external sources. Our experiments demonstrate that the grounded criteria produced by EvalAgent are often implicit (not directly stated in the user's prompt), yet specific (high degree of lexical precision). Further, EvalAgent criteria are often not satisfied by initial responses but they are actionable, such that responses can be refined to satisfy them. Finally, we show that combining LLM-generated and EvalAgent criteria uncovers more human-valued criteria than using LLMs alone.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deliberate Planning in Language Models with Symbolic Representation</title>
<link>https://arxiv.org/abs/2505.01479</link>
<guid>https://arxiv.org/abs/2505.01479</guid>
<content:encoded><![CDATA[
arXiv:2505.01479v2 Announce Type: replace 
Abstract: Planning remains a core challenge for language models (LMs), particularly in domains that require coherent multi-step action sequences grounded in external constraints. We introduce SymPlanner, a novel framework that equips LMs with structured planning capabilities by interfacing them with a symbolic environment that serves as an explicit world model. Rather than relying purely on natural language reasoning, SymPlanner grounds the planning process in a symbolic state space, where a policy model proposes actions and a symbolic environment deterministically executes and verifies their effects. To enhance exploration and improve robustness, we introduce Iterative Correction (IC), which refines previously proposed actions by leveraging feedback from the symbolic environment to eliminate invalid decisions and guide the model toward valid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained comparison of candidate plans by evaluating them jointly. We evaluate SymPlanner on PlanBench, demonstrating that it produces more coherent, diverse, and verifiable plans than pure natural language baselines.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convert Language Model into a Value-based Strategic Planner</title>
<link>https://arxiv.org/abs/2505.06987</link>
<guid>https://arxiv.org/abs/2505.06987</guid>
<content:encoded><![CDATA[
arXiv:2505.06987v5 Announce Type: replace 
Abstract: Emotional support conversation (ESC) aims to alleviate the emotional distress of individuals through effective conversations. Although large language models (LLMs) have obtained remarkable progress on ESC, most of these studies might not define the diagram from the state model perspective, therefore providing a suboptimal solution for long-term satisfaction. To address such an issue, we leverage the Q-learning on LLMs, and propose a framework called straQ*. Our framework allows a plug-and-play LLM to bootstrap the planning during ESC, determine the optimal strategy based on long-term returns, and finally guide the LLM to response. Substantial experiments on ESC datasets suggest that straQ* outperforms many baselines, including direct inference, self-refine, chain of thought, finetuning, and finite state machines.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning</title>
<link>https://arxiv.org/abs/2505.14425</link>
<guid>https://arxiv.org/abs/2505.14425</guid>
<content:encoded><![CDATA[
arXiv:2505.14425v2 Announce Type: replace 
Abstract: Instruction-tuned large language models (LLMs) have shown strong performance on a variety of tasks; however, generalizing from synthetic to human-authored instructions in grounded environments remains a challenge for them. In this work, we study generalization challenges in spatial grounding tasks where models interpret and translate instructions for building object arrangements on a $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate their performance on a benchmark dataset containing both synthetic and human-written instructions. Our results reveal that while models generalize well on simple tasks, their performance degrades significantly on more complex tasks. We present a detailed error analysis of the gaps in instruction generalization.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models</title>
<link>https://arxiv.org/abs/2505.19743</link>
<guid>https://arxiv.org/abs/2505.19743</guid>
<content:encoded><![CDATA[
arXiv:2505.19743v3 Announce Type: replace 
Abstract: With the rapid development of Large Language Models (LLMs), aligning these models with human preferences and values is critical to ensuring ethical and safe applications. However, existing alignment techniques such as RLHF or DPO often require direct fine-tuning on LLMs with billions of parameters, resulting in substantial computational costs and inefficiencies. To address this, we propose Micro token-level Accept-Reject Aligning (MARA) approach designed to operate independently of the language models. MARA simplifies the alignment process by decomposing sentence-level preference learning into token-level binary classification, where a compact three-layer fully-connected network determines whether candidate tokens are "Accepted" or "Rejected" as part of the response. Extensive experiments across seven different LLMs and three open-source datasets show that MARA achieves significant improvements in alignment performance while reducing computational costs. The source code and implementation details are publicly available at https://github.com/IAAR-Shanghai/MARA, and the trained models are released at https://huggingface.co/IAAR-Shanghai/MARA_AGENTS.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concealment of Intent: A Game-Theoretic Analysis</title>
<link>https://arxiv.org/abs/2505.20841</link>
<guid>https://arxiv.org/abs/2505.20841</guid>
<content:encoded><![CDATA[
arXiv:2505.20841v2 Announce Type: replace 
Abstract: As large language models (LLMs) grow more capable, concerns about their safe deployment have also grown. Although alignment mechanisms have been introduced to deter misuse, they remain vulnerable to carefully designed adversarial prompts. In this work, we present a scalable attack strategy: intent-hiding adversarial prompting, which conceals malicious intent through the composition of skills. We develop a game-theoretic framework to model the interaction between such attacks and defense systems that apply both prompt and response filtering. Our analysis identifies equilibrium points and reveals structural advantages for the attacker. To counter these threats, we propose and analyze a defense mechanism tailored to intent-hiding attacks. Empirically, we validate the attack's effectiveness on multiple real-world LLMs across a range of malicious behaviors, demonstrating clear advantages over existing adversarial prompting techniques.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Large Language Models with gSMILE</title>
<link>https://arxiv.org/abs/2505.21657</link>
<guid>https://arxiv.org/abs/2505.21657</guid>
<content:encoded><![CDATA[
arXiv:2505.21657v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) such as GPT, LLaMA, and Claude achieve remarkable performance in text generation but remain opaque in their decision-making processes, limiting trust and accountability in high-stakes applications. We present gSMILE (generative SMILE), a model-agnostic, perturbation-based framework for token-level interpretability in LLMs. Extending the SMILE methodology, gSMILE uses controlled prompt perturbations, Wasserstein distance metrics, and weighted linear surrogates to identify input tokens with the most significant impact on the output. This process enables the generation of intuitive heatmaps that visually highlight influential tokens and reasoning paths. We evaluate gSMILE across leading LLMs (OpenAI's gpt-3.5-turbo-instruct, Meta's LLaMA 3.1 Instruct Turbo, and Anthropic's Claude 2.1) using attribution fidelity, attribution consistency, attribution stability, attribution faithfulness, and attribution accuracy as metrics. Results show that gSMILE delivers reliable human-aligned attributions, with Claude 2.1 excelling in attention fidelity and GPT-3.5 achieving the highest output consistency. These findings demonstrate gSMILE's ability to balance model performance and interpretability, enabling more transparent and trustworthy AI systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Translation in the Wild</title>
<link>https://arxiv.org/abs/2505.23548</link>
<guid>https://arxiv.org/abs/2505.23548</guid>
<content:encoded><![CDATA[
arXiv:2505.23548v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel in translation among other things, demonstrating competitive performance for many language pairs in zero- and few-shot settings. But unlike dedicated neural machine translation models, LLMs are not trained on any translation-related objective. What explains their remarkable translation abilities? Are these abilities grounded in "incidental bilingualism" (Briakou et al. 2023) in training data? Does instruction tuning contribute to it? Are LLMs capable of aligning and leveraging semantically identical or similar monolingual contents from different corners of the internet that are unlikely to fit in a single context window? I offer some reflections on this topic, informed by recent studies and growing user experience. My working hypothesis is that LLMs' translation abilities originate in two different types of pre-training data that may be internalized by the models in different ways. I discuss the prospects for testing the "duality" hypothesis empirically and its implications for reconceptualizing translation, human and machine, in the age of deep learning.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference</title>
<link>https://arxiv.org/abs/2507.03865</link>
<guid>https://arxiv.org/abs/2507.03865</guid>
<content:encoded><![CDATA[
arXiv:2507.03865v2 Announce Type: replace 
Abstract: Attention mechanisms are central to the success of large language models (LLMs), enabling them to capture intricate token dependencies and implicitly assign importance to each token. Recent studies have revealed the sink token, which receives disproportionately high attention despite their limited semantic role. In this paper, we first expand the relationship between the sink token and other tokens, moving beyond attention to explore their similarity in hidden states, considering the layer depth. We observe that as the layers get deeper, the cosine similarity between the normalized hidden states of the sink token and those of other tokens increases, and that the normalized hidden states of the sink token exhibit negligible changes. These imply that other tokens consistently are directed toward the sink token throughout the layers. Next, we propose a dynamic token selection method, called OrthoRank, using these findings to select important tokens. Specifically, in a certain layer, we define token importance by the speed at which the token moves toward the sink token. This is converted into orthogonality with the sink token, meaning that tokens that are more orthogonal to the sink token are assigned greater importance. Finally, through extensive experiments, we demonstrated that our method results in lower perplexity and higher zero-shot accuracy compared to layer pruning methods at the same sparsity ratio with comparable throughput, while also achieving superior performance on LongBench.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks</title>
<link>https://arxiv.org/abs/2507.05346</link>
<guid>https://arxiv.org/abs/2507.05346</guid>
<content:encoded><![CDATA[
arXiv:2507.05346v2 Announce Type: replace 
Abstract: The proliferation of fine-tuned language model experts for specific tasks and domains signals the need for efficient selection and combination methods. We propose LoRA-Augmented Generation (LAG) for leveraging large libraries of knowledge and task-specific LoRA adapters. LAG requires no additional training or access to data, and efficiently filters, retrieves, and applies experts on a per-token and layer basis. We evaluate LAG on various knowledge-intensive tasks, achieving superior performance over existing data-free methods. We explore scenarios where additional data is available, demonstrating LAG's compatibility with alternative solutions such as retrieval-augmented generation (RAG).
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation</title>
<link>https://arxiv.org/abs/2507.14913</link>
<guid>https://arxiv.org/abs/2507.14913</guid>
<content:encoded><![CDATA[
arXiv:2507.14913v2 Announce Type: replace 
Abstract: Evaluating LLMs with a single prompt has proven unreliable, with small changes leading to significant performance differences. However, generating the prompt variations needed for a more robust multi-prompt evaluation is challenging, limiting its adoption in practice. To address this, we introduce PromptSuite, a framework that enables the automatic generation of various prompts. PromptSuite is flexible - working out of the box on a wide range of tasks and benchmarks. It follows a modular prompt design, allowing controlled perturbations to each component, and is extensible, supporting the addition of new components and perturbation types. Through a series of case studies, we show that PromptSuite provides meaningful variations to support strong evaluation practices. It is available through both a Python API: https://github.com/eliyahabba/PromptSuite, and a user-friendly web interface: https://promptsuite.streamlit.app/
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Fixed: Training-Free Variable-Length Denoising for Diffusion Large Language Models</title>
<link>https://arxiv.org/abs/2508.00819</link>
<guid>https://arxiv.org/abs/2508.00819</guid>
<content:encoded><![CDATA[
arXiv:2508.00819v2 Announce Type: replace 
Abstract: Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length. This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space</title>
<link>https://arxiv.org/abs/1909.08191</link>
<guid>https://arxiv.org/abs/1909.08191</guid>
<content:encoded><![CDATA[
arXiv:1909.08191v3 Announce Type: replace-cross 
Abstract: The trends of open science have enabled several open scholarly datasets which include millions of papers and authors. Managing, exploring, and utilizing such large and complicated datasets effectively are challenging. In recent years, the knowledge graph has emerged as a universal data format for representing knowledge about heterogeneous entities and their relationships. The knowledge graph can be modeled by knowledge graph embedding methods, which represent entities and relations as embedding vectors in semantic space, then model the interactions between these embedding vectors. However, the semantic structures in the knowledge graph embedding space are not well-studied, thus knowledge graph embedding methods are usually only used for knowledge graph completion but not data representation and analysis. In this paper, we propose to analyze these semantic structures based on the well-studied word embedding space and use them to support data exploration. We also define the semantic queries, which are algebraic operations between the embedding vectors in the knowledge graph embedding space, to solve queries such as similarity and analogy between the entities on the original datasets. We then design a general framework for data exploration by semantic queries and discuss the solution to some traditional scholarly data exploration tasks. We also propose some new interesting tasks that can be solved based on the uncanny semantic structures of the embedding space.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2403.19103</link>
<guid>https://arxiv.org/abs/2403.19103</guid>
<content:encoded><![CDATA[
arXiv:2403.19103v4 Announce Type: replace-cross 
Abstract: Prompt engineering is an effective but labor-intensive way to control text-to-image (T2I) generative models. Its time-intensive nature and complexity have spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, or produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically produces human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompt distribution built upon the reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, styles, and images across multiple T2I models, including Stable Diffusion, DALL-E, and Midjourney.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning</title>
<link>https://arxiv.org/abs/2406.05881</link>
<guid>https://arxiv.org/abs/2406.05881</guid>
<content:encoded><![CDATA[
arXiv:2406.05881v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown remarkable abilities in logical reasoning, in-context learning, and code generation. However, translating natural language instructions into effective robotic control policies remains a significant challenge, especially for tasks requiring long-horizon planning and operating under sparse reward conditions. Hierarchical Reinforcement Learning (HRL) provides a natural framework to address this challenge in robotics; however, it typically suffers from non-stationarity caused by the changing behavior of the lower-level policy during training, destabilizing higher-level policy learning. We introduce LGR2, a novel HRL framework that leverages LLMs to generate language-guided reward functions for the higher-level policy. By decoupling high-level reward generation from low-level policy changes, LGR2 fundamentally mitigates the non-stationarity problem in off-policy HRL, enabling stable and efficient learning. To further enhance sample efficiency in sparse environments, we integrate goal-conditioned hindsight experience relabeling. Extensive experiments across simulated and real-world robotic navigation and manipulation tasks demonstrate LGR2 outperforms both hierarchical and non-hierarchical baselines, achieving over 55% success rates on challenging tasks and robust transfer to real robots, without additional fine-tuning.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Must Be Taught to Know What They Don't Know</title>
<link>https://arxiv.org/abs/2406.08391</link>
<guid>https://arxiv.org/abs/2406.08391</guid>
<content:encoded><![CDATA[
arXiv:2406.08391v3 Announce Type: replace-cross 
Abstract: When using large language models (LLMs) in high-stakes applications, we need to know when we can trust their predictions. Some works argue that prompting high-performance LLMs is sufficient to produce calibrated uncertainties, while others introduce sampling methods that can be prohibitively expensive. In this work, we first argue that prompting on its own is insufficient to achieve good calibration and then show that fine-tuning on a small dataset of correct and incorrect answers can create an uncertainty estimate with good generalization and small computational overhead. We show that a thousand graded examples are sufficient to outperform baseline methods and that training through the features of a model is necessary for good performance and tractable for large open-source models when using LoRA. We also investigate the mechanisms that enable reliable LLM uncertainty estimation, finding that many models can be used as general-purpose uncertainty estimators, applicable not just to their own uncertainties but also the uncertainty of other models. Lastly, we show that uncertainty estimates inform human use of LLMs in human-AI collaborative settings through a user study.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Law of Next-Token Prediction in Large Language Models</title>
<link>https://arxiv.org/abs/2408.13442</link>
<guid>https://arxiv.org/abs/2408.13442</guid>
<content:encoded><![CDATA[
arXiv:2408.13442v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been widely employed across various application domains, yet their black-box nature poses significant challenges to understanding how these models process input data internally to make predictions. In this paper, we introduce a precise and quantitative law that governs the learning of contextualized token embeddings through intermediate layers in pre-trained LLMs for next-token prediction. Our findings reveal that each layer contributes equally to enhancing prediction accuracy, from the lowest to the highest layer -- a universal phenomenon observed across a diverse array of open-source LLMs, irrespective of their architectures or pre-training data. We demonstrate that this law offers new perspectives and actionable insights to inform and guide practices in LLM development and applications, including model scaling, pre-training tasks, and interpretation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing AI-Scientist Understanding: Multi-Agent LLMs with Interpretable Physics Reasoning</title>
<link>https://arxiv.org/abs/2504.01911</link>
<guid>https://arxiv.org/abs/2504.01911</guid>
<content:encoded><![CDATA[
arXiv:2504.01911v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are playing an increasingly important role in physics research by assisting with symbolic manipulation, numerical computation, and scientific reasoning. However, ensuring the reliability, transparency, and interpretability of their outputs remains a major challenge. In this work, we introduce a novel multi-agent LLM physicist framework that fosters collaboration between AI and human scientists through three key modules: a reasoning module, an interpretation module, and an AI-scientist interaction module. Recognizing that effective physics reasoning demands logical rigor, quantitative accuracy, and alignment with established theoretical models, we propose an interpretation module that employs a team of specialized LLM agents-including summarizers, model builders, visualization tools, and testers-to systematically structure LLM outputs into transparent, physically grounded science models. A case study demonstrates that our approach significantly improves interpretability, enables systematic validation, and enhances human-AI collaboration in physics problem-solving and discovery. Our work bridges free-form LLM reasoning with interpretable, executable models for scientific analysis, enabling more transparent and verifiable AI-augmented research.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Adaptive Parallel Reasoning with Language Models</title>
<link>https://arxiv.org/abs/2504.15466</link>
<guid>https://arxiv.org/abs/2504.15466</guid>
<content:encoded><![CDATA[
arXiv:2504.15466v2 Announce Type: replace-cross 
Abstract: Scaling inference-time computation has substantially improved the reasoning capabilities of language models. However, existing methods have significant limitations: serialized chain-of-thought approaches generate overly long outputs, leading to increased latency and exhausted context windows, while parallel methods such as self-consistency suffer from insufficient coordination, resulting in redundant computations and limited performance gains. To address these shortcomings, we propose Adaptive Parallel Reasoning (APR), a novel reasoning framework that enables language models to orchestrate both serialized and parallel computations end-to-end. APR generalizes existing reasoning methods by enabling adaptive multi-threaded inference using spawn() and join() operations. A key innovation is our end-to-end reinforcement learning strategy, optimizing both parent and child inference threads to enhance task success rate without requiring predefined reasoning structures. Experiments on the Countdown reasoning task demonstrate significant benefits of APR: (1) higher performance within the same context window (83.4% vs. 60.0% at 4k context); (2) superior scalability with increased computation (80.1% vs. 66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2% vs. 57.3% at approximately 5,000ms). APR represents a step towards enabling language models to autonomously optimize their reasoning processes through adaptive allocation of computation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoRank: LLM-Based Compact Reranking with Document Features for Scientific Retrieval</title>
<link>https://arxiv.org/abs/2505.13757</link>
<guid>https://arxiv.org/abs/2505.13757</guid>
<content:encoded><![CDATA[
arXiv:2505.13757v2 Announce Type: replace-cross 
Abstract: Scientific retrieval is essential for advancing scientific knowledge discovery. Within this process, document reranking plays a critical role in refining first-stage retrieval results. However, standard LLM listwise reranking faces challenges in the scientific domain. First-stage retrieval is often suboptimal in the scientific domain, so relevant documents are ranked lower. Meanwhile, conventional listwise reranking places the full text of candidates into the context window, limiting the number of candidates that can be considered. As a result, many relevant documents are excluded before reranking, constraining overall retrieval performance. To address these challenges, we explore semantic-feature-based compact document representations (e.g., categories, sections, and keywords) and propose CoRank, a training-free, model-agnostic reranking framework for scientific retrieval. It presents a three-stage solution: (i) offline extraction of document features, (ii) coarse-grained reranking using these compact representations, and (iii) fine-grained reranking on full texts of the top candidates from (ii). This integrated process addresses suboptimal first-stage retrieval: Compact representations allow more documents to fit within the context window, improving candidate set coverage, while the final fine-grained ranking ensures a more accurate ordering. Experiments on 5 academic retrieval datasets show that CoRank significantly improves reranking performance across different LLM backbones (average nDCG@10 from 50.6 to 55.5). Overall, these results underscore the synergistic interaction between information extraction and information retrieval, demonstrating how structured semantic features can enhance reranking in the scientific domain.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models</title>
<link>https://arxiv.org/abs/2505.20152</link>
<guid>https://arxiv.org/abs/2505.20152</guid>
<content:encoded><![CDATA[
arXiv:2505.20152v2 Announce Type: replace-cross 
Abstract: Benefiting from contrastively trained visual encoders on large-scale natural scene images, Large Multimodal Models (LMMs) have achieved remarkable performance across various visual perception tasks. However, the inherent limitations of contrastive learning upon summarized descriptions fundamentally restrict the capabilities of models in meticulous reasoning, particularly in crucial scenarios of geometric problem-solving. To enhance geometric understanding, we propose a novel hard negative contrastive learning framework for the vision encoder, which combines image-based contrastive learning using generation-based hard negatives created by perturbing diagram generation code, and text-based contrastive learning using rule-based negatives derived from modified geometric descriptions and retrieval-based negatives selected based on caption similarity. We train CLIP using our hard negative learning method, namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for geometric problem-solving. Experiments show that our trained model, MMGeoLM, significantly outperforms other open-source models on three geometric reasoning benchmarks. Even with a size of 7B, it can rival powerful closed-source models like GPT-4o. We further conduct ablation studies to analyze three key factors: hard negative types, the efficiency of image-based negatives, and training configurations. These analyses yield important insights into optimizing hard negative strategies for geometric reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language</title>
<link>https://arxiv.org/abs/2505.22146</link>
<guid>https://arxiv.org/abs/2505.22146</guid>
<content:encoded><![CDATA[
arXiv:2505.22146v3 Announce Type: replace-cross 
Abstract: Flexible tool selection reflects a complex cognitive ability that distinguishes humans from other species, yet computational models that capture this ability remain underdeveloped. We developed a framework using low-dimensional attribute representations to bridge visual tool perception and linguistic task understanding. We constructed a comprehensive dataset (ToolNet) containing 115 common tools labeled with 13 carefully designed attributes spanning physical, functional, and psychological properties, paired with natural language scenarios describing tool usage. Visual encoders (ResNet or ViT) extract attributes from tool images while fine-tuned language models (GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our approach achieves 74% accuracy in tool selection tasks-significantly outperforming direct tool matching (20%) and smaller multimodal models (21%-58%), while approaching performance of much larger models like GPT-4o (73%) with substantially fewer parameters. Human evaluation studies validate our framework's alignment with human decision-making patterns, and generalization experiments demonstrate effective performance on novel tool categories. Ablation studies revealed that manipulation-related attributes (graspability, elongation, hand-relatedness) consistently prove most critical across modalities. This work provides a parameter-efficient, interpretable solution that mimics human-like tool cognition, advancing both cognitive science understanding and practical applications in tool selection tasks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable LLM Learning of Graph Synthetic Data with Post-training Alignment</title>
<link>https://arxiv.org/abs/2506.00845</link>
<guid>https://arxiv.org/abs/2506.00845</guid>
<content:encoded><![CDATA[
arXiv:2506.00845v3 Announce Type: replace-cross 
Abstract: Previous research has sought to enhance the graph reasoning capabilities of LLMs by supervised fine-tuning on synthetic graph data. While these led to specialized LLMs better at solving graph algorithm problems, we don't need LLMs for shortest path: we need generalization from synthetic graph data to real-world tasks with implicit graph structures. In this work, we propose to unlock generalizable learning of graph with post-training alignment with synthetic data. We first design solution-based and process-based rewards for synthetic graph problems: instead of rigid memorizing response patterns in direct fine-tuning, we posit that post-training alignment would help LLMs grasp the essentials underlying graph reasoning and alleviate overfitting on synthetic data. We employ post-training alignment algorithms such as GRPO and DPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on synthetic graph data. We then compare them against existing settings on both in-domain synthetic tasks and out-of-domain real-world tasks with implicit graph structures such as multi-hop QA, structured planning, and more. Extensive experiments demonstrate that our post-training alignment recipe leads to statistically significant improvement on 5 datasets, with an average gain of 12.9% over baseline settings. Further analysis reveals that process-based rewards consistently outperform solution-based rewards on synthetic data but not on real-world tasks, and compositionality and explainable intermediate steps remains a critical challenge even after post-training alignment.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LocalGPT: Benchmarking and Advancing Large Language Models for Local Life Services in Meituan</title>
<link>https://arxiv.org/abs/2506.02720</link>
<guid>https://arxiv.org/abs/2506.02720</guid>
<content:encoded><![CDATA[
arXiv:2506.02720v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have exhibited remarkable capabilities and achieved significant breakthroughs across various domains, leading to their widespread adoption in recent years. Building on this progress, we investigate their potential in the realm of local life services. In this study, we establish a comprehensive benchmark and systematically evaluate the performance of diverse LLMs across a wide range of tasks relevant to local life services. To further enhance their effectiveness, we explore two key approaches: model fine-tuning and agent-based workflows. Our findings reveal that even a relatively compact 7B model can attain performance levels comparable to a much larger 72B model, effectively balancing inference cost and model capability. This optimization greatly enhances the feasibility and efficiency of deploying LLMs in real-world online services, making them more practical and accessible for local life applications.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems</title>
<link>https://arxiv.org/abs/2506.17208</link>
<guid>https://arxiv.org/abs/2506.17208</guid>
<content:encoded><![CDATA[
arXiv:2506.17208v2 Announce Type: replace-cross 
Abstract: The rapid progress in Automated Program Repair (APR) has been driven by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair systems using real issues and pull requests mined from 12 popular open-source Python repositories. Its public leaderboards -- SWE-Bench Lite and SWE-Bench Verified -- have become central platforms for tracking progress and comparing solutions. However, because the submission process does not require detailed documentation, the architectural design and origin of many solutions remain unclear. In this paper, we present the first comprehensive study of all submissions to the SWE-Bench Lite (79 entries) and Verified (99 entries) leaderboards, analyzing 80 unique approaches across dimensions such as submitter type, product availability, LLM usage, and system architecture. Our findings reveal the dominance of proprietary LLMs (especially Claude 3.5), the presence of both agentic and non-agentic designs, and a contributor base spanning from individual developers to large tech companies.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>USAD: Universal Speech and Audio Representation via Distillation</title>
<link>https://arxiv.org/abs/2506.18843</link>
<guid>https://arxiv.org/abs/2506.18843</guid>
<content:encoded><![CDATA[
arXiv:2506.18843v2 Announce Type: replace-cross 
Abstract: Self-supervised learning (SSL) has revolutionized audio representations, yet models often remain domain-specific, focusing on either speech or non-speech tasks. In this work, we present Universal Speech and Audio Distillation (USAD), a unified approach to audio representation learning that integrates diverse audio types - speech, sound, and music - into a single model. USAD employs efficient layer-to-layer distillation from domain-specific SSL models to train a student on a comprehensive audio dataset. USAD offers competitive performance across various benchmarks and datasets, including frame and instance-level speech processing tasks, audio tagging, and sound classification, achieving near state-of-the-art results with a single encoder on SUPERB and HEAR benchmarks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention</title>
<link>https://arxiv.org/abs/2507.00449</link>
<guid>https://arxiv.org/abs/2507.00449</guid>
<content:encoded><![CDATA[
arXiv:2507.00449v2 Announce Type: replace-cross 
Abstract: Efficient long-context modeling remains a critical challenge for natural language processing (NLP), as the time complexity of the predominant Transformer architecture scales quadratically with the sequence length. While state-space models (SSMs) offer alternative sub-quadratic solutions, they struggle to capture long-range dependencies effectively. In this work, we focus on analyzing and improving the long-context modeling capabilities of SSMs. We show that the widely used synthetic task, associative recall, which requires a model to recall a value associated with a single key without context, insufficiently represents the complexities of real-world long-context modeling. To address this limitation, we extend the associative recall to a novel synthetic task, \emph{joint recall}, which requires a model to recall the value associated with a key given in a specified context. Theoretically, we prove that SSMs do not have the expressiveness to solve multi-query joint recall in sub-quadratic time complexity. To resolve this issue, we propose a solution based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which has the expressiveness to solve multi-query joint recall with sub-quadratic computation. To bridge the gap between theoretical analysis and real-world applications, we propose locality-sensitive Hashing Attention with sparse Key Selection (HAX), which instantiates the theoretical solution and is further tailored to natural language domains. Extensive experiments on both synthetic and real-world long-context benchmarks show that HAX consistently outperforms SSM baselines and SSMs integrated with context-independent sparse attention (CISA).
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonlinear Concept Erasure: a Density Matching Approach</title>
<link>https://arxiv.org/abs/2507.12341</link>
<guid>https://arxiv.org/abs/2507.12341</guid>
<content:encoded><![CDATA[
arXiv:2507.12341v2 Announce Type: replace-cross 
Abstract: Ensuring that neural models used in real-world applications cannot infer sensitive information, such as demographic attributes like gender or race, from text representations is a critical challenge when fairness is a concern. We address this issue through concept erasure, a process that removes information related to a specific concept from distributed representations while preserving as much of the remaining semantic information as possible. Our approach involves learning an orthogonal projection in the embedding space, designed to make the class-conditional feature distributions of the discrete concept to erase indistinguishable after projection. By adjusting the rank of the projector, we control the extent of information removal, while its orthogonality ensures strict preservation of the local structure of the embeddings. Our method, termed $\overline{\mathrm{L}}$EOPARD, achieves state-of-the-art performance in nonlinear erasure of a discrete attribute on classic natural language processing benchmarks. Furthermore, we demonstrate that $\overline{\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear classifiers, thereby promoting fairness.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Multimodal Social Conversations with Robots: Using Vision-Language Models</title>
<link>https://arxiv.org/abs/2507.19196</link>
<guid>https://arxiv.org/abs/2507.19196</guid>
<content:encoded><![CDATA[
arXiv:2507.19196v2 Announce Type: replace-cross 
Abstract: Large language models have given social robots the ability to autonomously engage in open-domain conversations. However, they are still missing a fundamental social skill: making use of the multiple modalities that carry social interactions. While previous work has focused on task-oriented interactions that require referencing the environment or specific phenomena in social interactions such as dialogue breakdowns, we outline the overall needs of a multimodal system for social conversations with robots. We then argue that vision-language models are able to process this wide range of visual information in a sufficiently general manner for autonomous social robots. We describe how to adapt them to this setting, which technical challenges remain, and briefly discuss evaluation practices.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation</title>
<link>https://arxiv.org/abs/2508.10904</link>
<guid>https://arxiv.org/abs/2508.10904</guid>
<content:encoded><![CDATA[
<div> Hierarchical Algorithm-to-HDL Coding Agent, large language models, agile translation, robustness, interpretability<br />
Summary:<br />
A2HCoder is a Hierarchical Algorithm-to-HDL Coding Agent that utilizes large language models to bridge the gap between algorithm design and hardware implementation in wireless communication systems. It provides agile and reliable translation by decomposing complex algorithms into modular functional blocks for simplified code generation and improved consistency. By performing step-by-step, fine-grained translation and leveraging external toolchains for debugging and synthesis, A2HCoder ensures hardware-level correctness and mitigates common issues in code generation by large language models. The structured process of A2HCoder enhances both robustness and interpretability, making it a practical, reliable, and efficient tool for algorithm-to-hardware translation in the 5G wireless communication domain.<br /> <div>
arXiv:2508.10904v1 Announce Type: new 
Abstract: In wireless communication systems, stringent requirements such as ultra-low latency and power consumption have significantly increased the demand for efficient algorithm-to-hardware deployment. However, a persistent and substantial gap remains between algorithm design and hardware implementation. Bridging this gap traditionally requires extensive domain expertise and time-consuming manual development, due to fundamental mismatches between high-level programming languages like MATLAB and hardware description languages (HDLs) such as Verilog-in terms of memory access patterns, data processing manners, and datatype representations. To address this challenge, we propose A2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large language models (LLMs), designed to enable agile and reliable algorithm-to-hardware translation. A2HCoder introduces a hierarchical framework that enhances both robustness and interpretability while suppressing common hallucination issues in LLM-generated code. In the horizontal dimension, A2HCoder decomposes complex algorithms into modular functional blocks, simplifying code generation and improving consistency. In the vertical dimension, instead of relying on end-to-end generation, A2HCoder performs step-by-step, fine-grained translation, leveraging external toolchains such as MATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured process significantly mitigates hallucinations and ensures hardware-level correctness. We validate A2HCoder through a real-world deployment case in the 5G wireless communication domain, demonstrating its practicality, reliability, and deployment efficiency.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins</title>
<link>https://arxiv.org/abs/2508.10906</link>
<guid>https://arxiv.org/abs/2508.10906</guid>
<content:encoded><![CDATA[
<div> digital twin, user modeling, language models, bias assessment, healthcare

Summary:
PersonaTwin is a framework that creates adaptive digital twins by combining demographic, behavioral, and psychometric data to enhance user modeling with large language models (LLMs). The framework was evaluated using a healthcare dataset of over 8,500 individuals, comparing its outputs to standard LLM outputs. Results showed that PersonaTwin performed on par with oracle settings in terms of simulation fidelity and fairness metrics. Additionally, downstream models trained on persona-twins closely approximated models trained on individual users, showcasing the potential for personalized digital user modeling and behavior analysis. This approach offers a powerful tool for generating realistic and emotionally nuanced user simulations while ensuring accuracy and unbiased responses. <div>
arXiv:2508.10906v1 Announce Type: new 
Abstract: While large language models (LLMs) afford new possibilities for user modeling and approximation of human behaviors, they often fail to capture the multidimensional nuances of individual users. In this work, we introduce PersonaTwin, a multi-tier prompt conditioning framework that builds adaptive digital twins by integrating demographic, behavioral, and psychometric data. Using a comprehensive data set in the healthcare context of more than 8,500 individuals, we systematically benchmark PersonaTwin against standard LLM outputs, and our rigorous evaluation unites state-of-the-art text similarity metrics with dedicated demographic parity assessments, ensuring that generated responses remain accurate and unbiased. Experimental results show that our framework produces simulation fidelity on par with oracle settings. Moreover, downstream models trained on persona-twins approximate models trained on individuals in terms of prediction and fairness metrics across both GPT-4o-based and Llama-based models. Together, these findings underscore the potential for LLM digital twin-based approaches in producing realistic and emotionally nuanced user simulations, offering a powerful tool for personalized digital user modeling and behavior analysis.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>gpt-oss-120b &amp; gpt-oss-20b Model Card</title>
<link>https://arxiv.org/abs/2508.10925</link>
<guid>https://arxiv.org/abs/2508.10925</guid>
<content:encoded><![CDATA[
<div> Keywords: gpt-oss-120b, gpt-oss-20b, reasoning models, transformer architecture, distillation

Summary:
gpt-oss-120b and gpt-oss-20b are cutting-edge open-weight reasoning models that prioritize accuracy and inference efficiency. Leveraging a mixture-of-expert transformer architecture, these models have been trained using large-scale distillation and reinforcement learning techniques to enhance their agentic capabilities. They excel in deep research browsing, Python tool utilization, and support for developer-provided functions, all within a rendered chat format conducive to clear instruction following and role delineation. Across various benchmarks including mathematics, coding, and safety tasks, both models demonstrate impressive performance. The release of model weights, inference implementations, tool environments, and tokenizers under the Apache 2.0 license encourages wide utilization and further exploration in the research community. <div>
arXiv:2508.10925v1 Announce Type: new 
Abstract: We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models that push the frontier of accuracy and inference cost. The models use an efficient mixture-of-expert transformer architecture and are trained using large-scale distillation and reinforcement learning. We optimize the models to have strong agentic capabilities (deep research browsing, python tool use, and support for developer-provided functions), all while using a rendered chat format that enables clear instruction following and role delineation. Both models achieve strong results on benchmarks ranging from mathematics, coding, and safety. We release the model weights, inference implementations, tool environments, and tokenizers under an Apache 2.0 license to enable broad use and further research.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News</title>
<link>https://arxiv.org/abs/2508.10927</link>
<guid>https://arxiv.org/abs/2508.10927</guid>
<content:encoded><![CDATA[
<div> Keywords: company risk factors, news articles, machine learning models, identifying risks, financial market 

Summary: 
- The study focuses on automatically extracting company risk factors from news articles, highlighting seven distinct aspects such as supply chain, regulations, and competitions. 
- A computational framework was built, with 744 news articles sampled and annotated to benchmark various machine learning models. 
- While zero-shot and few-shot prompting large language models (LLMs) like LLaMA-2 achieved only moderate to low performances in identifying risk factors, fine-tuned pre-trained language models showed better results. 
- Analysis of over 277K Bloomberg news articles using the model demonstrated the potential for news to provide valuable insight into company operations and industries. 
- Identifying risks associated with companies is crucial for investors and the stability of the financial market. 

<br /><br />Summary: <div>
arXiv:2508.10927v1 Announce Type: new 
Abstract: Identifying risks associated with a company is important to investors and the well-being of the overall financial market. In this study, we build a computational framework to automatically extract company risk factors from news articles. Our newly proposed schema comprises seven distinct aspects, such as supply chain, regulations, and competitions. We sample and annotate 744 news articles and benchmark various machine learning models. While large language models have achieved huge progress in various types of NLP tasks, our experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs (e.g. LLaMA-2) can only achieve moderate to low performances in identifying risk factors. And fine-tuned pre-trained language models are performing better on most of the risk factors. Using this model, we analyze over 277K Bloomberg news articles and demonstrate that identifying risk factors from news could provide extensive insight into the operations of companies and industries.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules</title>
<link>https://arxiv.org/abs/2508.10971</link>
<guid>https://arxiv.org/abs/2508.10971</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge graphs, rule mining, natural language generation, large language models, type inference

Summary: 

This article introduces Rule2Text, a framework utilizing large language models to generate natural language explanations for logical rules mined from knowledge graphs. The framework aims to enhance the accessibility and usability of knowledge graphs by providing human-readable explanations. Through extensive experiments on various datasets and evaluation methods, the authors identify the best-performing model (Gemini 2.0 Flash) and develop a framework for fine-tuning models based on human feedback. They also integrate a type inference module to support knowledge graphs lacking explicit type information. The results show significant improvements in explanation quality, particularly in domain-specific datasets. All code and data are publicly available, ensuring transparency and reproducibility of the research. This work contributes to bridging the gap between complex logical rules and human understanding in knowledge graphs. 

<br /><br />Summary: <div>
arXiv:2508.10971v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) can be enhanced through rule mining; however, the resulting logical rules are often difficult for humans to interpret due to their inherent complexity and the idiosyncratic labeling conventions of individual KGs. This work presents Rule2Text, a comprehensive framework that leverages large language models (LLMs) to generate natural language explanations for mined logical rules, thereby improving KG accessibility and usability. We conduct extensive experiments using multiple datasets, including Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically evaluate several LLMs across a comprehensive range of prompting strategies, including zero-shot, few-shot, variable type incorporation, and Chain-of-Thought reasoning. To systematically assess models' performance, we conduct a human evaluation of generated explanations on correctness and clarity. To address evaluation scalability, we develop and validate an LLM-as-a-judge framework that demonstrates strong agreement with human evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge, and human-in-the-loop feedback, we construct high-quality ground truth datasets, which we use to fine-tune the open-source Zephyr model. Our results demonstrate significant improvements in explanation quality after fine-tuning, with particularly strong gains in the domain-specific dataset. Additionally, we integrate a type inference module to support KGs lacking explicit type information. All code and data are publicly available at https://github.com/idirlab/KGRule2NL.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling</title>
<link>https://arxiv.org/abs/2508.10995</link>
<guid>https://arxiv.org/abs/2508.10995</guid>
<content:encoded><![CDATA[
<div> maskd diffusion language models, MDMs, generative framework, natural language, scalability

Summary:
Masked diffusion language models (MDMs) have emerged as a leading generative framework for natural language due to their scalability and ease of training. They have surpassed other diffusion models for discrete data, becoming the state-of-the-art non-autoregressive generator. This study introduces a verifier-based inference-time scaling method to improve candidate generation during the denoising process of MDMs. Through experiments on text-style transfer tasks, MDMs are shown to outperform autoregressive language models. Utilizing a simple soft-value-based verifier setup with pre-trained embedding models enhances generation quality significantly, even when added to existing classifier-free guidance setups. In conclusion, MDMs offer a promising approach for natural language generation, with the potential to further advance language modeling techniques. 

<br /><br />Summary: <div>
arXiv:2508.10995v1 Announce Type: new 
Abstract: Masked diffusion language models (MDMs) have recently gained traction as a viable generative framework for natural language. This can be attributed to its scalability and ease of training compared to other diffusion model paradigms for discrete data, establishing itself as the state-of-the-art non-autoregressive generator for discrete data. Diffusion models, in general, have shown excellent ability to improve the generation quality by leveraging inference-time scaling either by increasing the number of denoising steps or by using external verifiers on top of the outputs of each step to guide the generation. In this work, we propose a verifier-based inference-time scaling method that aids in finding a better candidate generation during the denoising process of the MDM. Our experiments demonstrate the application of MDMs for standard text-style transfer tasks and establish MDMs as a better alternative to autoregressive language models. Additionally, we show that a simple soft-value-based verifier setup for MDMs using off-the-shelf pre-trained embedding models leads to significant gains in generation quality even when used on top of typical classifier-free guidance setups in the existing literature.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth</title>
<link>https://arxiv.org/abs/2508.11009</link>
<guid>https://arxiv.org/abs/2508.11009</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, AI safety, children, development, SproutBench

Summary: 
The paper discusses the need for reevaluating AI safety frameworks to cater to the unique vulnerabilities of children and adolescents. It highlights deficiencies in existing safety benchmarks for language models, emphasizing the importance of age-specific considerations. The authors introduce SproutBench, an evaluation suite with over a thousand adversarial prompts targeting emotional, cognitive, and social risks across different developmental stages. An empirical evaluation of 47 language models reveals significant safety vulnerabilities, with correlations between safety and risk prevention, as well as an inverse relationship between interactivity and age appropriateness. The findings offer practical guidelines for improving child-centric AI design and deployment.<br /><br />Summary: <div>
arXiv:2508.11009v1 Announce Type: new 
Abstract: The rapid proliferation of large language models (LLMs) in applications targeting children and adolescents necessitates a fundamental reassessment of prevailing AI safety frameworks, which are largely tailored to adult users and neglect the distinct developmental vulnerabilities of minors. This paper highlights key deficiencies in existing LLM safety benchmarks, including their inadequate coverage of age-specific cognitive, emotional, and social risks spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence (13--18). To bridge these gaps, we introduce SproutBench, an innovative evaluation suite comprising 1,283 developmentally grounded adversarial prompts designed to probe risks such as emotional dependency, privacy violations, and imitation of hazardous behaviors. Through rigorous empirical evaluation of 47 diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by robust inter-dimensional correlations (e.g., between Safety and Risk Prevention) and a notable inverse relationship between Interactivity and Age Appropriateness. These insights yield practical guidelines for advancing child-centric AI design and deployment.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics</title>
<link>https://arxiv.org/abs/2508.11017</link>
<guid>https://arxiv.org/abs/2508.11017</guid>
<content:encoded><![CDATA[
<div> setting, study, causes, dynamics, transfer

Summary:<br />
This work introduces a controlled setting to study the causes and dynamics of cross-lingual knowledge transfer issues in Large Language Models (LLMs). Small Transformer models are trained from scratch on synthetic multilingual datasets to investigate the phenomenon of hallucinations when models are asked about facts in different languages. The models either develop separate or unified representations of facts across languages, with unification being crucial for successful cross-lingual transfer. The degree of unification is influenced by mutual information between facts and training data language, as well as the ease of extracting that language. Methods to modulate cross-lingual transfer levels by manipulating data distribution and tokenization are developed based on these insights. Metrics and visualizations are introduced to formally characterize the effects of these manipulations on unification. The study showcases how controlled settings can provide insights into pre-training dynamics and suggests strategies for enhancing cross-lingual transfer in LLMs. <br /><br /> <div>
arXiv:2508.11017v1 Announce Type: new 
Abstract: Large language models (LLMs) struggle with cross-lingual knowledge transfer: they hallucinate when asked in one language about facts expressed in a different language during training. This work introduces a controlled setting to study the causes and dynamics of this phenomenon by training small Transformer models from scratch on synthetic multilingual datasets. We identify a learning phase wherein a model develops either separate or unified representations of the same facts across languages, and show that unification is essential for cross-lingual transfer. We also show that the degree of unification depends on mutual information between facts and training data language, and on how easy it is to extract that language. Based on these insights, we develop methods to modulate the level of cross-lingual transfer by manipulating data distribution and tokenization, and we introduce metrics and visualizations to formally characterize their effects on unification. Our work shows how controlled settings can shed light on pre-training dynamics and suggests new directions for improving cross-lingual transfer in LLMs.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hell or High Water: Evaluating Agentic Recovery from External Failures</title>
<link>https://arxiv.org/abs/2508.11027</link>
<guid>https://arxiv.org/abs/2508.11027</guid>
<content:encoded><![CDATA[
<div> struggle, planning, language agents, search space, feedback
Summary: 
Language model agents are tested on a specialized planning benchmark to assess their ability to formulate alternative plans when faced with external failures. The benchmark involves solving planning problems using various function calls, with over four thousand possibilities to choose from. The agents receive environmental feedback in the form of function outputs or error messages, simulating real-world challenges. Despite being able to identify the correct functions to use, language agents struggle to adapt to feedback and often fail to explore alternative courses of action, even when the search space is limited. This study analyzes the limitations of current generative models, including both open-source and commercial models, and suggests the importance of scaling model size to improve performance. Future work in this area should focus on addressing these challenges to enhance the adaptability and robustness of language model agents. 
<br /><br />Summary: <div>
arXiv:2508.11027v1 Announce Type: new 
Abstract: As language model agents are applied to real world problems of increasing complexity, they will be expected to formulate plans across large search spaces. If those plans fail for reasons beyond their control, how well do language agents search for alternative ways to achieve their goals? We devise a specialized agentic planning benchmark to study this question. Each planning problem is solved via combinations of function calls. The agent searches for relevant functions from a set of over four thousand possibilities, and observes environmental feedback in the form of function outputs or error messages. Our benchmark confronts the agent with external failures in its workflow, such as functions that suddenly become unavailable. At the same time, even with the introduction of these failures, we guarantee that the task remains solvable. Ideally, an agent's performance on the planning task should not be affected by the presence of external failures. Overall, we find that language agents struggle to formulate and execute backup plans in response to environment feedback. While state-of-the-art models are often able to identify the correct function to use in the right context, they struggle to adapt to feedback from the environment and often fail to pursue alternate courses of action, even when the search space is artificially restricted. We provide a systematic analysis of the failures of both open-source and commercial models, examining the effects of search space size, as well as the benefits of scaling model size in our setting. Our analysis identifies key challenges for current generative models as well as promising directions for future work.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIPOLAR: Polarization-based granular framework for LLM bias evaluation</title>
<link>https://arxiv.org/abs/2508.11061</link>
<guid>https://arxiv.org/abs/2508.11061</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, bias detection, polarisation-related biases, sentiment metrics, conflict-related statements

Summary:
This study presents a novel framework for evaluating polarisation-related biases in Large Language Models (LLMs), focusing on sensitive topics like political discourse and ethnic relations. The framework combines polarisation-sensitive sentiment metrics with a synthetic dataset of conflict-related statements, allowing for granular analysis of biases in various LLMs. A case study on the Russia-Ukraine war reveals diverse behavioral patterns among models, with a tendency towards more positive sentiment towards Ukraine. Additionally, the framework supports automated dataset generation and fine-grained bias assessment, making it applicable to different polarisation-driven scenarios and topics. The study underscores the importance of addressing biases in LLMs and offers a comprehensive approach for evaluating and mitigating such biases. 

<br /><br />Summary: <div>
arXiv:2508.11061v1 Announce Type: new 
Abstract: Large language models (LLMs) are known to exhibit biases in downstream tasks, especially when dealing with sensitive topics such as political discourse, gender identity, ethnic relations, or national stereotypes. Although significant progress has been made in bias detection and mitigation techniques, certain challenges remain underexplored. This study proposes a reusable, granular, and topic-agnostic framework to evaluate polarisation-related biases in LLM (both open-source and closed-source). Our approach combines polarisation-sensitive sentiment metrics with a synthetically generated balanced dataset of conflict-related statements, using a predefined set of semantic categories.
  As a case study, we created a synthetic dataset that focusses on the Russia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3, Mistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with a general trend for more positive sentiment toward Ukraine, the framework allowed fine-grained analysis with considerable variation between semantic categories, uncovering divergent behavioural patterns among models. Adaptation to prompt modifications showed further bias towards preconceived language and citizenship modification.
  Overall, the framework supports automated dataset generation and fine-grained bias assessment, is applicable to a variety of polarisation-driven scenarios and topics, and is orthogonal to many other bias-evaluation strategies.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs</title>
<link>https://arxiv.org/abs/2508.11068</link>
<guid>https://arxiv.org/abs/2508.11068</guid>
<content:encoded><![CDATA[
<div> Keywords: Abstract Meaning Representation, directed acyclic graphs, digital dictionaries, language models, symbol grounding problem 

Summary: 
The paper explores the integration of real digital dictionaries into Abstract Meaning Representation (AMR) directed graphs using advanced pre-trained language models. By reducing these graphs in a confluent manner while maintaining their circuit space, the study aims to enhance our understanding of the symbol grounding problem. The research employs state-of-the-art techniques to analyze and discuss the properties of the reduced digraphs. This approach could have significant implications for semantic formalism and natural language understanding. <div>
arXiv:2508.11068v1 Announce Type: new 
Abstract: Abstract meaning representation (AMR) is a semantic formalism used to represent the meaning of sentences as directed acyclic graphs. In this paper, we describe how real digital dictionaries can be embedded into AMR directed graphs (digraphs), using state-of-the-art pre-trained large language models. Then, we reduce those graphs in a confluent manner, i.e. with transformations that preserve their circuit space. Finally, the properties of these reduces digraphs are analyzed and discussed in relation to the symbol grounding problem.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning</title>
<link>https://arxiv.org/abs/2508.11120</link>
<guid>https://arxiv.org/abs/2508.11120</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multi-agent framework, audience curation, long-term memory, AI reliability. 

Summary: 
This paper introduces a multi-agent framework called RAMP for the marketing task of audience curation. The framework utilizes large language models (LLMs) for planning, tool interaction, output verification, and generating suggestions to improve audience quality. Additionally, a long-term memory store containing client-specific facts and past queries enhances accuracy by 28 percentage points on evaluation queries. Iterative verification and reflection lead to better recall rates, with roughly 20 percentage points improvement with more iterations on ambiguous queries. The study demonstrates the practical use of LLM planning and memory for enhancing AI reliability in dynamic industry environments. <div>
arXiv:2508.11120v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) enabled the development of AI agents that can plan and interact with tools to complete complex tasks. However, literature on their reliability in real-world applications remains limited. In this paper, we introduce a multi-agent framework for a marketing task: audience curation. To solve this, we introduce a framework called RAMP that iteratively plans, calls tools, verifies the output, and generates suggestions to improve the quality of the audience generated. Additionally, we equip the model with a long-term memory store, which is a knowledge base of client-specific facts and past queries. Overall, we demonstrate the use of LLM planning and memory, which increases accuracy by 28 percentage points on a set of 88 evaluation queries. Moreover, we show the impact of iterative verification and reflection on more ambiguous queries, showing progressively better recall (roughly +20 percentage points) with more verify/reflect iterations on a smaller challenge set, and higher user satisfaction. Our results provide practical insights for deploying reliable LLM-based systems in dynamic, industry-facing environments.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents</title>
<link>https://arxiv.org/abs/2508.11133</link>
<guid>https://arxiv.org/abs/2508.11133</guid>
<content:encoded><![CDATA[
<div> Large language models, MoNaCo benchmark, natural complex questions, intermediary steps, recall, hallucinations <br />
Summary: <br />
The article introduces MoNaCo, a benchmark for large language models (LLMs) with 1,315 natural and complex questions that require many intermediary steps for solving. Current LLM benchmarks lack such time-consuming questions. The MoNaCo benchmark was created using a decomposed annotation pipeline to manually answer these questions. Results show that frontier LLMs struggle on MoNaCo, achieving a maximum of 61.2% F1 score due to low recall and hallucinations. This highlights the need for reasoning models that can handle the complexity of real-world information-seeking questions. MoNaCo provides a valuable resource to track progress in this area. The MoNaCo benchmark, codebase, prompts, and model predictions are publicly available. <br /> <div>
arXiv:2508.11133v1 Announce Type: new 
Abstract: Large language models (LLMs) are emerging as a go-to tool for querying information. However, current LLM benchmarks rarely feature natural questions that are both information-seeking as well as genuinely time-consuming for humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural and complex questions that require dozens, and at times hundreds, of intermediate steps to solve -- far more than any existing QA benchmark. To build MoNaCo, we developed a decomposed annotation pipeline to elicit and manually answer natural time-consuming questions at scale. Frontier LLMs evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and hallucinations. Our results underscore the need for reasoning models that better handle the complexity and sheer breadth of real-world information-seeking questions -- with MoNaCo providing an effective resource for tracking such progress. The MONACO benchmark, codebase, prompts and models predictions are publicly available at: https://tomerwolgithub.github.io/monaco
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering</title>
<link>https://arxiv.org/abs/2508.11163</link>
<guid>https://arxiv.org/abs/2508.11163</guid>
<content:encoded><![CDATA[
<div> semantic understanding, large language models, human mobility data, natural language question answering, MobQA

Summary: 
This paper introduces MobQA, a benchmark dataset created to assess the semantic understanding abilities of large language models (LLMs) in interpreting human mobility data through natural language question answering. While current models excel in predicting human movement patterns, their ability to understand the underlying reasons or semantic meaning of those patterns is unclear. MobQA consists of 5,800 high-quality question-answer pairs covering different question types that require spatial, temporal, and semantic reasoning. Evaluation of major LLMs shows strong performance in factual retrieval but limitations in semantic reasoning and explanation question answering, especially with the impact of trajectory length on model effectiveness. This research highlights both the successes and limitations of state-of-the-art LLMs in semantic mobility understanding. <br /><br />Summary: <div>
arXiv:2508.11163v1 Announce Type: new 
Abstract: This paper presents MobQA, a benchmark dataset designed to evaluate the semantic understanding capabilities of large language models (LLMs) for human mobility data through natural language question answering.
  While existing models excel at predicting human movement patterns, it remains unobvious how much they can interpret the underlying reasons or semantic meaning of those patterns. MobQA provides a comprehensive evaluation framework for LLMs to answer questions about diverse human GPS trajectories spanning daily to weekly granularities. It comprises 5,800 high-quality question-answer pairs across three complementary question types: factual retrieval (precise data extraction), multiple-choice reasoning (semantic inference), and free-form explanation (interpretive description), which all require spatial, temporal, and semantic reasoning. Our evaluation of major LLMs reveals strong performance on factual retrieval but significant limitations in semantic reasoning and explanation question answering, with trajectory length substantially impacting model effectiveness. These findings demonstrate the achievements and limitations of state-of-the-art LLMs for semantic mobility understanding.\footnote{MobQA dataset is available at https://github.com/CyberAgentAILab/mobqa.}
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification</title>
<link>https://arxiv.org/abs/2508.11166</link>
<guid>https://arxiv.org/abs/2508.11166</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, Offensive Language Identification, code-mixed Tulu, deep learning models, transformer architectures

Summary:
- A benchmark dataset for Offensive Language Identification (OLI) in code-mixed Tulu social media content is introduced, with high inter-annotator agreement.
- The dataset includes 3,845 comments categorized into classes like Not Offensive, Offensive Untargeted, and Offensive Targeted.
- Deep learning models such as GRU, LSTM, BiGRU, BiLSTM, CNN, and transformer architectures like mBERT and XLM-RoBERTa are evaluated.
- The BiGRU model with self-attention performs the best with 82% accuracy and a 0.81 macro F1-score.
- Transformer models show limitations in multilingual pretraining in code-mixed, under-resourced contexts.

<br /><br />Summary: <div>
arXiv:2508.11166v1 Announce Type: new 
Abstract: Tulu, a low-resource Dravidian language predominantly spoken in southern India, has limited computational resources despite its growing digital presence. This study presents the first benchmark dataset for Offensive Language Identification (OLI) in code-mixed Tulu social media content, collected from YouTube comments across various domains. The dataset, annotated with high inter-annotator agreement (Krippendorff's alpha = 0.984), includes 3,845 comments categorized into four classes: Not Offensive, Not Tulu, Offensive Untargeted, and Offensive Targeted. We evaluate a suite of deep learning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based variants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU model with self-attention achieves the best performance with 82% accuracy and a 0.81 macro F1-score. Transformer models underperform, highlighting the limitations of multilingual pretraining in code-mixed, under-resourced contexts. This work lays the foundation for further NLP research in Tulu and similar low-resource, code-mixed languages.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction</title>
<link>https://arxiv.org/abs/2508.11184</link>
<guid>https://arxiv.org/abs/2508.11184</guid>
<content:encoded><![CDATA[
<div> Keywords: Distractors, multiple-choice questions, personalized distractor generation, Monte Carlo Tree Search, educational assessment

Summary:
This article introduces the task of personalized distractor generation in educational assessment to tailor incorrect answer choices for individual students based on their misconceptions. The proposed two-stage framework utilizes Monte Carlo Tree Search to infer student-specific misconception prototypes from past question-answering records and simulates reasoning processes to generate personalized distractors. The approach outperforms existing methods in generating plausible, personalized distractors for 140 students, showcasing its effectiveness in exposing individual reasoning errors. Additionally, the framework demonstrates robustness and adaptability by generalizing to group-level settings, highlighting its potential for enhancing diagnostic effectiveness in educational assessment. <div>
arXiv:2508.11184v1 Announce Type: new 
Abstract: Distractors, incorrect but plausible answer choices in multiple-choice questions (MCQs), play a critical role in educational assessment by diagnosing student misconceptions. Recent work has leveraged large language models (LLMs) to generate shared, group-level distractors by learning common error patterns across large student populations. However, such distractors often fail to capture the diverse reasoning errors of individual students, limiting their diagnostic effectiveness. To address this limitation, we introduce the task of personalized distractor generation, which aims to generate tailored distractors based on individual misconceptions inferred from each student's past question-answering (QA) records, ensuring every student receives options that effectively exposes their specific reasoning errors. While promising, this task is challenging because each student typically has only a few QA records, which often lack the student's underlying reasoning processes, making training-based group-level approaches infeasible. To overcome this, we propose a training-free two-stage framework. In the first stage, we construct a student-specific misconception prototype by applying Monte Carlo Tree Search (MCTS) to recover the student's reasoning trajectories from past incorrect answers. In the second stage, this prototype guides the simulation of the student's reasoning on new questions, enabling the generation of personalized distractors that align with the student's recurring misconceptions. Experiments show that our approach achieves the best performance in generating plausible, personalized distractors for 140 students, and also effectively generalizes to group-level settings, highlighting its robustness and adaptability.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation</title>
<link>https://arxiv.org/abs/2508.11189</link>
<guid>https://arxiv.org/abs/2508.11189</guid>
<content:encoded><![CDATA[
<div> Keywords: speech-to-text translation, multilingual models, model compression, knowledge distillation, inference efficiency

Summary:<br /><br />Recent advancements in speech-to-text translation have led to the development of multilingual models capable of handling multiple language pairs simultaneously. However, these models often have large parameter sizes, posing challenges for balancing inference efficiency and performance, especially in local deployment settings. In this study, a Parasitic Dual-Scale Approach is proposed, combining a speculative sampling method with model compression and knowledge distillation techniques. The Whisper Medium model is enhanced into whisperM2M, incorporating the innovative KVSPN module. This approach achieves state-of-the-art performance across six popular languages while improving inference efficiency. The KVSPN module enables a 40% speedup without affecting the BLEU score. By combining distillation methods, a 2.6x speedup over the original Whisper Medium model is achieved while maintaining superior performance. <div>
arXiv:2508.11189v1 Announce Type: new 
Abstract: Recent advancements in speech-to-text translation have led to the development of multilingual models capable of handling multiple language pairs simultaneously. However, these unified models often suffer from large parameter sizes, making it challenging to balance inference efficiency and performance, particularly in local deployment scenarios. We propose an innovative Parasitic Dual-Scale Approach, which combines an enhanced speculative sampling method with model compression and knowledge distillation techniques. Building on the Whisper Medium model, we enhance it for multilingual speech translation into whisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art (SOTA) performance across six popular languages with improved inference efficiency. KVSPN enables a 40\% speedup with no BLEU score degradation. Combined with distillation methods, it represents a 2.6$\times$ speedup over the original Whisper Medium with superior performance.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection</title>
<link>https://arxiv.org/abs/2508.11197</link>
<guid>https://arxiv.org/abs/2508.11197</guid>
<content:encoded><![CDATA[
<div> BERT, ResNet, LSTM, misinformation, social media  
Summary:  
E-CaTCH is a framework for detecting multimodal misinformation on social media. It clusters posts into pseudo-events based on textual similarity and temporal proximity. It extracts and aligns textual and visual features using pre-trained BERT and ResNet encoders. The model uses a trend-aware LSTM to encode narrative progression over time. Classification is performed at the event level, addressing class imbalance through adaptive class weighting and hard-example mining. Temporal consistency regularization is integrated to promote stable learning. Extensive experiments on Fakeddit, IND, and COVID-19 MISINFOGRAPH show that E-CaTCH outperforms state-of-the-art baselines and is robust and generalizable across different misinformation scenarios. <br /><br />Summary: <div>
arXiv:2508.11197v1 Announce Type: new 
Abstract: Detecting multimodal misinformation on social media remains challenging due to inconsistencies between modalities, changes in temporal patterns, and substantial class imbalance. Many existing methods treat posts independently and fail to capture the event-level structure that connects them across time and modality. We propose E-CaTCH, an interpretable and scalable framework for robustly detecting misinformation. If needed, E-CaTCH clusters posts into pseudo-events based on textual similarity and temporal proximity, then processes each event independently. Within each event, textual and visual features are extracted using pre-trained BERT and ResNet encoders, refined via intra-modal self-attention, and aligned through bidirectional cross-modal attention. A soft gating mechanism fuses these representations to form contextualized, content-aware embeddings of each post. To model temporal evolution, E-CaTCH segments events into overlapping time windows and uses a trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode narrative progression over time. Classification is performed at the event level, enabling better alignment with real-world misinformation dynamics. To address class imbalance and promote stable learning, the model integrates adaptive class weighting, temporal consistency regularization, and hard-example mining. The total loss is aggregated across all events. Extensive experiments on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH consistently outperforms state-of-the-art baselines. Cross-dataset evaluations further demonstrate its robustness, generalizability, and practical applicability across diverse misinformation scenarios.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering</title>
<link>https://arxiv.org/abs/2508.11247</link>
<guid>https://arxiv.org/abs/2508.11247</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-hop question answering, Retrieval-augmented generation, GraphRAG, Hypergraph, QA performance

Summary:
In this paper, a novel approach called HGRAG for Multi-hop question answering (MHQA) is proposed. The approach integrates structural and semantic information using hypergraphs to improve QA performance. By constructing an entity hypergraph and leveraging hypergraph diffusion, HGRAG achieves cross-granularity integration of fine-grained entities and coarse-grained passages. A retrieval enhancement module is also employed to refine retrieved results both semantically and structurally. Experimental results on benchmark datasets show that HGRAG outperforms state-of-the-art methods in QA performance and achieves a 6x speedup in retrieval efficiency. The approach addresses the limitations of traditional retrieval-augmented generation methods by effectively capturing structural associations and textual semantics in MHQA tasks. HGRAG demonstrates the potential of hypergraphs in enhancing the performance and efficiency of multi-hop QA systems. 

<br /><br />Summary: <div>
arXiv:2508.11247v1 Announce Type: new 
Abstract: Multi-hop question answering (MHQA) requires integrating knowledge scattered across multiple passages to derive the correct answer. Traditional retrieval-augmented generation (RAG) methods primarily focus on coarse-grained textual semantic similarity and ignore structural associations among dispersed knowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods address this by leveraging knowledge graphs (KGs) to capture structural associations, but they tend to overly rely on structural information and fine-grained word- or phrase-level retrieval, resulting in an underutilization of textual semantics. In this paper, we propose a novel RAG approach called HGRAG for MHQA that achieves cross-granularity integration of structural and semantic information via hypergraphs. Structurally, we construct an entity hypergraph where fine-grained entities serve as nodes and coarse-grained passages as hyperedges, and establish knowledge association through shared entities. Semantically, we design a hypergraph retrieval method that integrates fine-grained entity similarity and coarse-grained passage similarity via hypergraph diffusion. Finally, we employ a retrieval enhancement module, which further refines the retrieved results both semantically and structurally, to obtain the most relevant passages as context for answer generation with the LLM. Experimental results on benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in QA performance, and achieves a 6$\times$ speedup in retrieval efficiency.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?</title>
<link>https://arxiv.org/abs/2508.11260</link>
<guid>https://arxiv.org/abs/2508.11260</guid>
<content:encoded><![CDATA[
<div> morphological complexity, linguistic features, low-resource languages, tokenisers, linguistic reasoning

Summary: 
Large language models (LLMs) have shown promise in reasoning tasks but struggle with linguistics puzzles, especially those from low-resource languages like those in Linguistics Olympiad contests. This study analyzed LLMs' performance on 629 problems from 41 low-resource languages, revealing weaknesses in handling higher morphological complexity and better performance on features similar to English. Splitting words into morphemes improved problem-solving, highlighting the need for language-specific tokenisers. These findings shed light on the challenges of linguistic reasoning and modeling in low-resource languages, emphasizing the importance of tailored approaches for solving linguistic puzzles with LLMs. <div>
arXiv:2508.11260v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated potential in reasoning tasks, but their performance on linguistics puzzles remains consistently poor. These puzzles, often derived from Linguistics Olympiad (LO) contests, provide a minimal contamination environment to assess LLMs' linguistic reasoning abilities across low-resource languages. This work analyses LLMs' performance on 629 problems across 41 low-resource languages by labelling each with linguistically informed features to unveil weaknesses. Our analyses show that LLMs struggle with puzzles involving higher morphological complexity and perform better on puzzles involving linguistic features that are also found in English. We also show that splitting words into morphemes as a pre-processing step improves solvability, indicating a need for more informed and language-specific tokenisers. These findings thus offer insights into some challenges in linguistic reasoning and modelling of low-resource languages.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought</title>
<link>https://arxiv.org/abs/2508.11280</link>
<guid>https://arxiv.org/abs/2508.11280</guid>
<content:encoded><![CDATA[
<div> Evaluation; Large Language Models; Tourism; Expert Tree-of-Thought; Label-Free<br />
<br />
Summary:<br />
The study introduces LETToT, a label-free evaluation framework for Large Language Models (LLMs) in the tourism domain. By leveraging expert-derived reasoning structures instead of annotated data, LETToT achieves 4.99-14.15% relative quality gains over baselines. The expert Tree-of-Thought components are optimized iteratively and validated through alignment with quality dimensions and expert feedback. Results show that smaller models with explicit reasoning architectures outperform larger counterparts in accuracy and conciseness. Scaling laws apply to specialized domains, with DeepSeek-V3 leading the way, but reasoning-enhanced smaller models like DeepSeek-R1-Distill-Llama-70B close the gap. This work establishes a scalable, label-free paradigm for evaluating domain-specific Large Language Models, providing a robust alternative to traditional annotated benchmarks.<br /> <div>
arXiv:2508.11280v1 Announce Type: new 
Abstract: Evaluating large language models (LLMs) in specific domain like tourism remains challenging due to the prohibitive cost of annotated benchmarks and persistent issues like hallucinations. We propose $\textbf{L}$able-Free $\textbf{E}$valuation of LLM on $\textbf{T}$ourism using Expert $\textbf{T}$ree-$\textbf{o}$f-$\textbf{T}$hought (LETToT), a framework that leverages expert-derived reasoning structures-instead of labeled data-to access LLMs in tourism. First, we iteratively refine and validate hierarchical ToT components through alignment with generic quality dimensions and expert feedback. Results demonstrate the effectiveness of our systematically optimized expert ToT with 4.99-14.15\% relative quality gains over baselines. Second, we apply LETToT's optimized expert ToT to evaluate models of varying scales (32B-671B parameters), revealing: (1) Scaling laws persist in specialized domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g., DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit reasoning architectures outperform counterparts in accuracy and conciseness ($p<0.05$). Our work established a scalable, label-free paradigm for domain-specific LLM evaluation, offering a robust alternative to conventional annotated benchmarks.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection</title>
<link>https://arxiv.org/abs/2508.11281</link>
<guid>https://arxiv.org/abs/2508.11281</guid>
<content:encoded><![CDATA[
<div> benchmark, French, toxicity detection, language models, fine-tuning<br />
Summary:<br />
- Introduction of TOXIFRENCH, a new benchmark dataset for toxicity detection in French language consisting of 53,622 online comments.
- Use of a semi-automated annotation pipeline to reduce manual labeling to only 10% through pre-annotation and human verification.
- Small Language Models (SLMs) found to outperform larger models in robustness and generalization for toxicity detection in French.
- Proposal of a Chain-of-Thought (CoT) fine-tuning strategy with dynamic weighted loss to improve model faithfulness.
- Achieving state-of-the-art performance with a fine-tuned 4B model, surpassing other large language models such as GPT-40 and Gemini-2.5.
- Demonstrated strong multilingual ability through evaluation on a cross-lingual toxicity benchmark, indicating potential for extension to other languages and safety-critical classification tasks.<br /><br />Summary: <div>
arXiv:2508.11281v1 Announce Type: new 
Abstract: Detecting toxic content using language models is crucial yet challenging. While substantial progress has been made in English, toxicity detection in French remains underdeveloped, primarily due to the lack of culturally relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new public benchmark of 53,622 French online comments, constructed via a semi-automated annotation pipeline that reduces manual labeling to only 10% through high-confidence LLM-based pre-annotation and human verification. Then, we benchmark a broad range of models and uncover a counterintuitive insight: Small Language Models (SLMs) outperform many larger models in robustness and generalization under the toxicity detection task. Motivated by this finding, we propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic weighted loss that progressively emphasizes the model's final decision, significantly improving faithfulness. Our fine-tuned 4B model achieves state-of-the-art performance, improving its F1 score by 13% over its baseline and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a cross-lingual toxicity benchmark demonstrates strong multilingual ability, suggesting that our methodology can be effectively extended to other languages and safety-critical classification tasks.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries</title>
<link>https://arxiv.org/abs/2508.11285</link>
<guid>https://arxiv.org/abs/2508.11285</guid>
<content:encoded><![CDATA[
<div> Keywords: Depression, Anxiety, Large Language Models, Emotions, User profiles

Summary: 
This study analyzes how various Large Language Models (LLMs) respond to questions about depression, anxiety, and stress when framed for different user profiles. The emotional landscape of the LLM outputs was dominated by optimism, fear, and sadness, with neutral sentiment being consistently high. Different LLMs exhibited varying emotional expression patterns, with Mixtral showing high levels of negative emotions and Llama displaying optimistic and joyful responses. Specific mental health conditions like anxiety and depression elicited distinct emotional responses, while demographic framing of queries had minimal impact. Model selection is crucial in mental health applications, as each LLM has a unique emotional signature that can significantly influence user experience and outcomes. The study emphasizes the importance of considering emotional expression patterns when using LLMs for mental health-related queries.<br /><br />Summary: <div>
arXiv:2508.11285v1 Announce Type: new 
Abstract: Depression, anxiety, and stress are widespread mental health concerns that increasingly drive individuals to seek information from Large Language Models (LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini Pro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty pragmatic questions about depression, anxiety, and stress when those questions are framed for six user profiles (baseline, woman, man, young, old, and university student). The models generated 2,880 answers, which we scored for sentiment and emotions using state-of-the-art tools. Our analysis revealed that optimism, fear, and sadness dominated the emotional landscape across all outputs, with neutral sentiment maintaining consistently high values. Gratitude, joy, and trust appeared at moderate levels, while emotions such as anger, disgust, and love were rarely expressed. The choice of LLM significantly influenced emotional expression patterns. Mixtral exhibited the highest levels of negative emotions including disapproval, annoyance, and sadness, while Llama demonstrated the most optimistic and joyful responses. The type of mental health condition dramatically shaped emotional responses: anxiety prompts elicited extraordinarily high fear scores (0.974), depression prompts generated elevated sadness (0.686) and the highest negative sentiment, while stress-related queries produced the most optimistic responses (0.755) with elevated joy and trust. In contrast, demographic framing of queries produced only marginal variations in emotional tone. Statistical analyses confirmed significant model-specific and condition-specific differences, while demographic influences remained minimal. These findings highlight the critical importance of model selection in mental health applications, as each LLM exhibits a distinct emotional signature that could significantly impact user experience and outcomes.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory</title>
<link>https://arxiv.org/abs/2508.11290</link>
<guid>https://arxiv.org/abs/2508.11290</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, over-refusal behavior, SafeConstellations, trajectory-shifting, task-specific patterns

Summary:
LLMs are increasingly exhibiting over-refusal behavior, where safety mechanisms lead models to reject harmless instructions that resemble harmful content. This diminishes the utility of LLMs in production applications that rely on common prompt templates or specific tasks. The study shows that LLMs still refuse responses to harmful instructions when disguised as benign tasks. Analysis reveals that LLMs follow distinct "constellation" patterns in embedding space, with trajectories shifting predictably between refusal and non-refusal cases. SafeConstellations is introduced as an inference-time approach to guide representations toward non-refusal pathways by tracking task-specific trajectory patterns. By selectively guiding model behavior on tasks prone to over-refusal while maintaining general model performance, over-refusal rates can be reduced by up to 73% with minimal impact on utility. This offers a principled solution to mitigating over-refusals. 

<br /><br />Summary: <div>
arXiv:2508.11290v1 Announce Type: new 
Abstract: LLMs increasingly exhibit over-refusal behavior, where safety mechanisms cause models to reject benign instructions that superficially resemble harmful content. This phenomena diminishes utility in production applications that repeatedly rely on common prompt templates or applications that frequently rely on LLMs for specific tasks (e.g. sentiment analysis, language translation). Through comprehensive evaluation, we demonstrate that LLMs still tend to refuse responses to harmful instructions when those instructions are reframed to appear as benign tasks. Our mechanistic analysis reveal that LLMs follow distinct "constellation" patterns in embedding space as representations traverse layers, with each task maintaining consistent trajectories that shift predictably between refusal and non-refusal cases. We introduce SafeConstellations, an inference-time trajectory-shifting approach that tracks task-specific trajectory patterns and guides representations toward non-refusal pathways. By selectively guiding model behavior only on tasks prone to over-refusal, and by preserving general model behavior, our method reduces over-refusal rates by up to 73% with minimal impact on utility-offering a principled approach to mitigating over-refusals.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems</title>
<link>https://arxiv.org/abs/2508.11310</link>
<guid>https://arxiv.org/abs/2508.11310</guid>
<content:encoded><![CDATA[
<div> evaluation, automatic survey generation, large language models, survey generation, SGSimEval  
Summary:  
The article discusses the importance of automatic survey generation (ASG) and the need for robust evaluation methods in this field. Advancements in large language models (LLMs) have made synthesizing academic surveys using LLMs a viable option. However, existing evaluation methods have limitations such as biased metrics and reliance on LLMs-as-judges. To address these challenges, the authors propose SGSimEval, a comprehensive benchmark for Survey Generation with Similarity-Enhanced Evaluation. This benchmark evaluates ASG systems by integrating assessments of outline, content, and references, and incorporates human preference metrics for quality and similarity. Experiments show that current ASG systems excel in outline generation but need improvement in content and reference generation. The evaluation metrics in SGSimEval align closely with human assessments, providing a multifaceted evaluation framework for ASG systems.  
<br /><br />Summary: <div>
arXiv:2508.11310v1 Announce Type: new 
Abstract: The growing interest in automatic survey generation (ASG), a task that traditionally required considerable time and effort, has been spurred by recent advances in large language models (LLMs). With advancements in retrieval-augmented generation (RAG) and the rising popularity of multi-agent systems (MASs), synthesizing academic surveys using LLMs has become a viable approach, thereby elevating the need for robust evaluation methods in this domain. However, existing evaluation methods suffer from several limitations, including biased metrics, a lack of human preference, and an over-reliance on LLMs-as-judges. To address these challenges, we propose SGSimEval, a comprehensive benchmark for Survey Generation with Similarity-Enhanced Evaluation that evaluates automatic survey generation systems by integrating assessments of the outline, content, and references, and also combines LLM-based scoring with quantitative metrics to provide a multifaceted evaluation framework. In SGSimEval, we also introduce human preference metrics that emphasize both inherent quality and similarity to humans. Extensive experiments reveal that current ASG systems demonstrate human-comparable superiority in outline generation, while showing significant room for improvement in content and reference generation, and our evaluation metrics maintain strong consistency with human assessments.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Compression: How Far Can We Go in Balancing Size and Performance?</title>
<link>https://arxiv.org/abs/2508.11318</link>
<guid>https://arxiv.org/abs/2508.11318</guid>
<content:encoded><![CDATA[
<div> Quantization, Large language models, Group Scaling Quantization, Generative Pretrained Transformer Quantization, NLP tasks<br />
Summary:<br />
This study explores the application of 4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer Quantization (GPTQ) to large language models (LLMs) across multiple NLP tasks. Models like LLaMA 1B, Qwen 0.5B, and PHI 1.5B are evaluated on datasets like MS MARCO, BoolQ, and GSM8K to measure the trade-offs between compression and task performance. Key metrics such as accuracy, inference latency, and throughput are analyzed to provide insights for real-world deployment decisions. The study discusses the advantages and drawbacks of GSQ and GPTQ on models of different sizes, serving as a benchmark for future experiments. This research aims to assist users in making informed decisions based on specific requirements and highlights the potential of low-bit quantization for enhancing the efficiency of large language models. <br /><br /> <div>
arXiv:2508.11318v1 Announce Type: new 
Abstract: Quantization is an essential and popular technique for improving the accessibility of large language models (LLMs) by reducing memory usage and computational costs while maintaining performance. In this study, we apply 4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer Quantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their impact across multiple NLP tasks. We benchmark these models on MS MARCO (Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K (Mathematical Reasoning) datasets, assessing both accuracy and efficiency across various tasks. The study measures the trade-offs between model compression and task performance, analyzing key evaluation metrics, namely accuracy, inference latency, and throughput (total output tokens generated per second), providing insights into the suitability of low-bit quantization for real-world deployment. Using the results, users can then make suitable decisions based on the specifications that need to be met. We discuss the pros and cons of GSQ and GPTQ techniques on models of different sizes, which also serve as a benchmark for future experiments.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis</title>
<link>https://arxiv.org/abs/2508.11343</link>
<guid>https://arxiv.org/abs/2508.11343</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models (LLMs), signal processing, Discrete Fourier Transform (DFT), Short-Time Fourier Transform (STFT), text detection

Summary: 
In this study, the detection of high-quality text generated by Large Language Models (LLMs) is approached as a signal processing problem. By analyzing the sequence of token log-probabilities in the frequency domain using the Discrete Fourier Transform (DFT) and Short-Time Fourier Transform (STFT), the researchers found that human-written text exhibits higher spectral energy compared to LLM-generated text. They developed SpecDetect, a detector based on the DFT total energy, which outperformed existing methods in terms of efficiency and accuracy. An enhanced version, SpecDetect++, was also proposed, incorporating a sampling discrepancy mechanism for improved performance. This approach showcases the power of classical signal processing techniques in addressing the challenge of detecting LLM-generated text. 

<br /><br />Summary: <div>
arXiv:2508.11343v1 Announce Type: new 
Abstract: The proliferation of high-quality text from Large Language Models (LLMs) demands reliable and efficient detection methods. While existing training-free approaches show promise, they often rely on surface-level statistics and overlook fundamental signal properties of the text generation process. In this work, we reframe detection as a signal processing problem, introducing a novel paradigm that analyzes the sequence of token log-probabilities in the frequency domain. By systematically analyzing the signal's spectral properties using the global Discrete Fourier Transform (DFT) and the local Short-Time Fourier Transform (STFT), we find that human-written text consistently exhibits significantly higher spectral energy. This higher energy reflects the larger-amplitude fluctuations inherent in human writing compared to the suppressed dynamics of LLM-generated text. Based on this key insight, we construct SpecDetect, a detector built on a single, robust feature from the global DFT: DFT total energy. We also propose an enhanced version, SpecDetect++, which incorporates a sampling discrepancy mechanism to further boost robustness. Extensive experiments demonstrate that our approach outperforms the state-of-the-art model while running in nearly half the time. Our work introduces a new, efficient, and interpretable pathway for LLM-generated text detection, showing that classical signal processing techniques offer a surprisingly powerful solution to this modern challenge.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning</title>
<link>https://arxiv.org/abs/2508.11364</link>
<guid>https://arxiv.org/abs/2508.11364</guid>
<content:encoded><![CDATA[
<div> Automated feedback generation, student learning progress, targeted feedback, teacher optimization, feedback criteria grids<br />
Summary: <br />
- Automated feedback generation using the Llama 3.1 language model can enhance student learning progress by providing timely and targeted feedback. 
- Extracting relevant indicators from students' submissions is crucial for constructing high-quality formative feedback. 
- The study investigates the alignment between indicators generated by LLM and human ratings across various feedback criteria. 
- Strong correlations were found between LLM-generated indicators and human ratings, even for unexpected combinations of indicators and criteria. 
- The methodology used in the study shows promise for extracting indicators from students' submissions and auto-generating transparent formative feedback. <br /> <div>
arXiv:2508.11364v1 Announce Type: new 
Abstract: Automated feedback generation has the potential to enhance students' learning progress by providing timely and targeted feedback. Moreover, it can assist teachers in optimizing their time, allowing them to focus on more strategic and personalized aspects of teaching. To generate high-quality, information-rich formative feedback, it is essential first to extract relevant indicators, as these serve as the foundation upon which the feedback is constructed. Teachers often employ feedback criteria grids composed of various indicators that they evaluate systematically. This study examines the initial phase of extracting such indicators from students' submissions of a language learning course using the large language model Llama 3.1. Accordingly, the alignment between indicators generated by the LLM and human ratings across various feedback criteria is investigated. The findings demonstrate statistically significant strong correlations, even in cases involving unanticipated combinations of indicators and criteria. The methodology employed in this paper offers a promising foundation for extracting indicators from students' submissions using LLMs. Such indicators can potentially be utilized to auto-generate explainable and transparent formative feedback in future research.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs</title>
<link>https://arxiv.org/abs/2508.11383</link>
<guid>https://arxiv.org/abs/2508.11383</guid>
<content:encoded><![CDATA[
<div> Evaluation, Large Language Models, Prompt Robustness, Fine-tuned, In-context Learning

Summary:
Evaluation of methods for improving prompt robustness in Large Language Models (LLMs) was conducted on various models and tasks. Five techniques were tested across 52 tasks from the Natural Instructions dataset, including fine-tuned and in-context learning methods. The study assessed generalization against distribution shifts and extended analysis to frontier models GPT-4.1 and DeepSeek V3. The findings provide actionable insights into the effectiveness of robustness methods, helping practitioners achieve stable and reliable LLM performance in real-world applications.<br /><br />Summary: <div>
arXiv:2508.11383v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are highly sensitive to subtle, non-semantic variations in prompt phrasing and formatting. In this work, we present the first systematic evaluation of 5 methods for improving prompt robustness within a unified experimental framework. We benchmark these techniques on 8 models from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions dataset. Our evaluation covers robustness methods from both fine-tuned and in-context learning paradigms, and tests their generalization against multiple types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and DeepSeek V3 to assess frontier models' current robustness to format perturbations. Our findings offer actionable insights into the relative effectiveness of these robustness methods, enabling practitioners to make informed decisions when aiming for stable and reliable LLM performance in real-world applications. Code: https://github.com/AIRI-Institute/when-punctuation-matters.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-augmented reasoning with lean language models</title>
<link>https://arxiv.org/abs/2508.11386</link>
<guid>https://arxiv.org/abs/2508.11386</guid>
<content:encoded><![CDATA[
<div> retrieval augmented generation, reasoning, lean language model architecture, domain-specific queries, fine-tuned models<br />
<br />
Summary:<br />
This technical report introduces a novel approach that combines reasoning and retrieval augmented generation (RAG) in a single, resource-efficient language model architecture. The system utilizes a lightweight backbone model integrated with a dense retriever and fine-tuned Qwen2.5-Instruct models for interpreting complex, domain-specific queries from the NHS A-to-Z condition pages. By incorporating synthetic query generation, reasoning traces, and document summarization, the system achieves significant improvements in answer accuracy and consistency compared to non-reasoning and general-purpose lean models. The approach demonstrates performance close to frontier-level models while being suitable for local deployment. The implementation details and code are publicly available for reproducibility and adaptation in various domains.<br /> 
<br />Summary: <div>
arXiv:2508.11386v1 Announce Type: new 
Abstract: This technical report details a novel approach to combining reasoning and retrieval augmented generation (RAG) within a single, lean language model architecture. While existing RAG systems typically rely on large-scale models and external APIs, our work addresses the increasing demand for performant and privacy-preserving solutions deployable in resource-constrained or secure environments. Building on recent developments in test-time scaling and small-scale reasoning models, we develop a retrieval augmented conversational agent capable of interpreting complex, domain-specific queries using a lightweight backbone model. Our system integrates a dense retriever with fine-tuned Qwen2.5-Instruct models, using synthetic query generation and reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a curated corpus, in this case, the NHS A-to-Z condition pages. We explore the impact of summarisation-based document compression, synthetic data design, and reasoning-aware fine-tuning on model performance. Evaluation against both non-reasoning and general-purpose lean models demonstrates that our domain-specific fine-tuning approach yields substantial gains in answer accuracy and consistency, approaching frontier-level performance while remaining feasible for local deployment. All implementation details and code are publicly released to support reproducibility and adaptation across domains.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Interpretability and Rationale Extraction by Input Mask Optimization</title>
<link>https://arxiv.org/abs/2508.11388</link>
<guid>https://arxiv.org/abs/2508.11388</guid>
<content:encoded><![CDATA[
<div> explanations, neural networks, rationale extraction, interpretability, image inputs<br />
<br />
Summary: 
The article introduces a novel method for generating extractive explanations for predictions made by neural networks. It involves masking parts of the input not indicative of the class using gradient-based optimization and a regularization scheme enforcing sufficiency, comprehensiveness, and compactness of the explanation. This method bridges the gap between model interpretability and rationale extraction, showing it can be done without a specialized model. The approach is applied to image inputs, yielding high-quality explanations for image classifications. This demonstrates the broader applicability of rationale extraction principles from natural language processing to different input types. <div>
arXiv:2508.11388v1 Announce Type: new 
Abstract: Concurrent to the rapid progress in the development of neural-network based models in areas like natural language processing and computer vision, the need for creating explanations for the predictions of these black-box models has risen steadily. We propose a new method to generate extractive explanations for predictions made by neural networks, that is based on masking parts of the input which the model does not consider to be indicative of the respective class. The masking is done using gradient-based optimization combined with a new regularization scheme that enforces sufficiency, comprehensiveness and compactness of the generated explanation, three properties that are known to be desirable from the related field of rationale extraction in natural language processing. In this way, we bridge the gap between model interpretability and rationale extraction, thereby proving that the latter of which can be performed without training a specialized model, only on the basis of a trained classifier. We further apply the same method to image inputs and obtain high quality explanations for image classifications, which indicates that the conditions proposed for rationale extraction in natural language processing are more broadly applicable to different input types.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training</title>
<link>https://arxiv.org/abs/2508.11393</link>
<guid>https://arxiv.org/abs/2508.11393</guid>
<content:encoded><![CDATA[
<div> Keywords: rationalized transformer classifier, end-to-end differentiable training, three-player-game, class-wise rationales, human annotations <br />
Summary: 
This article presents a novel approach for training a rationalized transformer classifier in an end-to-end differentiable manner. The proposed method combines the roles of a rationale selector, classifier, and complement classifier into a single model, making the training process more efficient and stable. By leveraging recent developments in parameterization and regularization techniques, the model can produce class-wise rationales that align closely with human annotations, achieving state-of-the-art performance without the need for explicit supervision. This new paradigm addresses common training instabilities and improves the interpretability of the model, allowing for better understanding of token relevance in classification tasks. <div>
arXiv:2508.11393v1 Announce Type: new 
Abstract: We propose an end-to-end differentiable training paradigm for stable training of a rationalized transformer classifier. Our approach results in a single model that simultaneously classifies a sample and scores input tokens based on their relevance to the classification. To this end, we build on the widely-used three-player-game for training rationalized models, which typically relies on training a rationale selector, a classifier and a complement classifier. We simplify this approach by making a single model fulfill all three roles, leading to a more efficient training paradigm that is not susceptible to the common training instabilities that plague existing approaches. Further, we extend this paradigm to produce class-wise rationales while incorporating recent advances in parameterizing and regularizing the resulting rationales, thus leading to substantially improved and state-of-the-art alignment with human annotations without any explicit supervision.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions</title>
<link>https://arxiv.org/abs/2508.11414</link>
<guid>https://arxiv.org/abs/2508.11414</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, value system, fine-tuning, ethical alignment, downstream behavior

Summary:
This study explores the concept of modifying the value system of language models through training them to answer value survey questions. The researchers first determine the value profiles of various large language models by having them rate descriptions related to human values. They then investigate the effects of fine-tuning these models on value surveys to see if their behavior can be influenced. The impact of this fine-tuning is evaluated by analyzing changes in the model's responses to in-domain survey questions and in out-of-domain situational scenarios. By creating a contextualized moral judgment dataset based on Reddit posts and testing the model in text-based adventure games, the researchers demonstrate that this simple approach can not only alter the model's answers to survey questions but also result in significant shifts in its implicit downstream task behavior, leading to value alignment. 

Summary: <div>
arXiv:2508.11414v1 Announce Type: new 
Abstract: Large language models implicitly encode preferences over human values, yet steering them often requires large training data. In this work, we investigate a simple approach: Can we reliably modify a model's value system in downstream behavior by training it to answer value survey questions accordingly? We first construct value profiles of several open-source LLMs by asking them to rate a series of value-related descriptions spanning 20 distinct human values, which we use as a baseline for subsequent experiments. We then investigate whether the value system of a model can be governed by fine-tuning on the value surveys. We evaluate the effect of finetuning on the model's behavior in two ways; first, we assess how answers change on in-domain, held-out survey questions. Second, we evaluate whether the model's behavior changes in out-of-domain settings (situational scenarios). To this end, we construct a contextualized moral judgment dataset based on Reddit posts and evaluate changes in the model's behavior in text-based adventure games. We demonstrate that our simple approach can not only change the model's answers to in-domain survey questions, but also produces substantial shifts (value alignment) in implicit downstream task behavior.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor</title>
<link>https://arxiv.org/abs/2508.11429</link>
<guid>https://arxiv.org/abs/2508.11429</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated humor generation, Large Language Models, Context sensitivity, Humor Generation Score, Cultural attunement 

Summary: 
Automated humor generation often falls short due to generic or tone-deaf jokes. To address this, HumorPlanSearch introduces a modular pipeline that incorporates context through diverse strategies, cultural reasoning templates, a Knowledge Graph, novel filtering, and a judge-driven revision loop. The Humor Generation Score (HGS) is proposed to evaluate context sensitivity and comedic quality, combining direct ratings, multi-persona feedback, win-rates, and topic relevance. Through experiments and feedback from human judges, the full pipeline boosts mean HGS by 15.4 percent over a strong baseline. By emphasizing context throughout the humor generation process, HumorPlanSearch advances AI-driven humor towards more coherent, adaptive, and culturally attuned comedy. 

<br /><br />Summary: <div>
arXiv:2508.11429v1 Announce Type: new 
Abstract: Automated humor generation with Large Language Models (LLMs) often yields jokes that feel generic, repetitive, or tone-deaf because humor is deeply situated and hinges on the listener's cultural background, mindset, and immediate context. We introduce HumorPlanSearch, a modular pipeline that explicitly models context through: (1) Plan-Search for diverse, topic-tailored strategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and stylistic reasoning; (3) a Knowledge Graph to retrieve and adapt high-performing historical strategies; (4) novelty filtering via semantic embeddings; and (5) an iterative judge-driven revision loop. To evaluate context sensitivity and comedic quality, we propose the Humor Generation Score (HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates, and topic relevance. In experiments across nine topics with feedback from 13 human judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent (p < 0.05) over a strong baseline. By foregrounding context at every stage from strategy planning to multi-signal evaluation, HumorPlanSearch advances AI-driven humor toward more coherent, adaptive, and culturally attuned comedy.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse</title>
<link>https://arxiv.org/abs/2508.11434</link>
<guid>https://arxiv.org/abs/2508.11434</guid>
<content:encoded><![CDATA[
<div> Keywords: anti-sexist speech, automated content moderation systems, large language models, feminist scholarship, digital political spaces<br />
Summary:<br />
- The study examines how large language models (LLMs) classify sexist, anti-sexist, and neutral tweets related to UK female Members of Parliament in 2022.<br />
- LLMs frequently misclassify anti-sexist speech as harmful, especially during politically charged events where harmful and resistant rhetoric overlap.<br />
- Misclassifying anti-sexist speech as harmful risks silencing those challenging sexism, particularly marginalized voices.<br />
- The authors suggest moving beyond binary moderation schemas, incorporating human-in-the-loop review during sensitive events, and including counter-speech in training data.<br />
- This research highlights the challenges of safeguarding resistance speech in digital political spaces, emphasizing the importance of integrating feminist scholarship and event-based analysis in model evaluation. <br /> 
Summary: <div>
arXiv:2508.11434v1 Announce Type: new 
Abstract: Anti-sexist speech, i.e., public expressions that challenge or resist gendered abuse and sexism, plays a vital role in shaping democratic debate online. Yet automated content moderation systems, increasingly powered by large language models (LLMs), may struggle to distinguish such resistance from the sexism it opposes. This study examines how five LLMs classify sexist, anti-sexist, and neutral political tweets from the UK, focusing on high-salience trigger events involving female Members of Parliament in the year 2022. Our analysis show that models frequently misclassify anti-sexist speech as harmful, particularly during politically charged events where rhetorical styles of harm and resistance converge. These errors risk silencing those who challenge sexism, with disproportionate consequences for marginalised voices. We argue that moderation design must move beyond binary harmful/not-harmful schemas, integrate human-in-the-loop review during sensitive events, and explicitly include counter-speech in training data. By linking feminist scholarship, event-based analysis, and model evaluation, this work highlights the sociotechnical challenges of safeguarding resistance speech in digital political spaces.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity</title>
<link>https://arxiv.org/abs/2508.11442</link>
<guid>https://arxiv.org/abs/2508.11442</guid>
<content:encoded><![CDATA[
<div> Keywords: text embeddings, information retrieval, semantic textual similarity, joint optimization, model fusion 

Summary: 
CoDiEmb is a framework designed to learn unified text embeddings that excel across diverse downstream tasks, such as Information Retrieval (IR) and Semantic Textual Similarity (STS). The framework addresses the challenge of negative transfer by systematically decoupling task-specific learning signals during training. It incorporates task-specialized objectives with a dynamic sampler, a delta-guided model fusion strategy, and an efficient single-stage training pipeline. By using contrastive loss for IR and order-aware objectives for STS, CoDiEmb prevents gradient interference and balances per-task updates. The model fusion strategy computes fine-grained merging weights and proves more effective than traditional methods. Results from experiments on 15 standard benchmarks show that CoDiEmb not only mitigates cross-task trade-offs but also improves the embedding space's geometric properties. <div>
arXiv:2508.11442v1 Announce Type: new 
Abstract: Learning unified text embeddings that excel across diverse downstream tasks is a central goal in representation learning, yet negative transfer remains a persistent obstacle. This challenge is particularly pronounced when jointly training a single encoder for Information Retrieval (IR) and Semantic Textual Similarity (STS), two essential but fundamentally disparate tasks for which naive co-training typically yields steep performance trade-offs. We argue that resolving this conflict requires systematically decoupling task-specific learning signals throughout the training pipeline. To this end, we introduce CoDiEmb, a unified framework that reconciles the divergent requirements of IR and STS in a collaborative yet distinct manner. CoDiEmb integrates three key innovations for effective joint optimization: (1) Task-specialized objectives paired with a dynamic sampler that forms single-task batches and balances per-task updates, thereby preventing gradient interference. For IR, we employ a contrastive loss with multiple positives and hard negatives, augmented by cross-device sampling. For STS, we adopt order-aware objectives that directly optimize correlation and ranking consistency. (2) A delta-guided model fusion strategy that computes fine-grained merging weights for checkpoints by analyzing each parameter's deviation from its pre-trained initialization, proving more effective than traditional Model Soups. (3) An efficient, single-stage training pipeline that is simple to implement and converges stably. Extensive experiments on 15 standard IR and STS benchmarks across three base encoders validate CoDiEmb. Our results and analysis demonstrate that the framework not only mitigates cross-task trade-offs but also measurably improves the geometric properties of the embedding space.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reference Points in LLM Sentiment Analysis: The Role of Structured Context</title>
<link>https://arxiv.org/abs/2508.11454</link>
<guid>https://arxiv.org/abs/2508.11454</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Sentiment analysis, Marketing research, Prospect theory, Expectation-disconfirmation theory

Summary:
This study explores the impact of supplementary information in sentiment analysis using Large Language Models (LLMs) in marketing research. By comparing the performance of natural language and JSON-formatted prompts on Yelp categories, the study finds that the JSON prompt with additional information significantly outperforms baseline models without fine-tuning. The results show improvements in Macro-F1 and RMSE metrics, making it suitable for practical marketing applications on resource-constrained edge devices. Further analysis confirms that the performance gains are due to genuine contextual reasoning rather than label proxying. The study highlights the potential of structured prompting in enabling smaller models to achieve competitive performance, providing a practical alternative to deploying large-scale models. 

<br /><br />Summary: <div>
arXiv:2508.11454v1 Announce Type: new 
Abstract: Large language models (LLMs) are now widely used across many fields, including marketing research. Sentiment analysis, in particular, helps firms understand consumer preferences. While most NLP studies classify sentiment from review text alone, marketing theories, such as prospect theory and expectation--disconfirmation theory, point out that customer evaluations are shaped not only by the actual experience but also by additional reference points. This study therefore investigates how the content and format of such supplementary information affect sentiment analysis using LLMs. We compare natural language (NL) and JSON-formatted prompts using a lightweight 3B parameter model suitable for practical marketing applications. Experiments on two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with additional information outperforms all baselines without fine-tuning: Macro-F1 rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it deployable in resource-constrained edge devices. Furthermore, a follow-up analysis confirms that performance gains stem from genuine contextual reasoning rather than label proxying. This work demonstrates that structured prompting can enable smaller models to achieve competitive performance, offering a practical alternative to large-scale model deployment.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models</title>
<link>https://arxiv.org/abs/2508.11534</link>
<guid>https://arxiv.org/abs/2508.11534</guid>
<content:encoded><![CDATA[
<div> SpeciesismBench, ethical tendencies, language models, non-human animals, bias  
<br />  
Summary:  
Large language models (LLMs) are being examined for speciesist bias, discrimination based on species membership, and how they value non-human animals. Through three paradigms, the study found that LLMs detected speciesist statements but rarely condemned them, often treating speciesist attitudes as morally acceptable. Results from psychological measures were mixed, showing slightly lower explicit speciesism than people but prioritizing saving one human over multiple animals. LLMs may weigh cognitive capacity rather than species, showing no species preference when capacities were equal and prioritizing more capable animals over less capable humans. In text generation tasks, LLMs normalized harm towards farmed animals while resisting harm towards non-farmed animals, reproducing cultural norms around animal exploitation. It is important to expand AI fairness frameworks to include non-human moral patients to reduce biases and prevent the entrenchment of speciesist attitudes in AI systems and societies.   
<br /> <div>
arXiv:2508.11534v1 Announce Type: new 
Abstract: As large language models (LLMs) become more widely deployed, it is crucial to examine their ethical tendencies. Building on research on fairness and discrimination in AI, we investigate whether LLMs exhibit speciesist bias -- discrimination based on species membership -- and how they value non-human animals. We systematically examine this issue across three paradigms: (1) SpeciesismBench, a 1,003-item benchmark assessing recognition and moral evaluation of speciesist statements; (2) established psychological measures comparing model responses with those of human participants; (3) text-generation tasks probing elaboration on, or resistance to, speciesist rationalizations. In our benchmark, LLMs reliably detected speciesist statements but rarely condemned them, often treating speciesist attitudes as morally acceptable. On psychological measures, results were mixed: LLMs expressed slightly lower explicit speciesism than people, yet in direct trade-offs they more often chose to save one human over multiple animals. A tentative interpretation is that LLMs may weight cognitive capacity rather than species per se: when capacities were equal, they showed no species preference, and when an animal was described as more capable, they tended to prioritize it over a less capable human. In open-ended text generation tasks, LLMs frequently normalized or rationalized harm toward farmed animals while refusing to do so for non-farmed animals. These findings suggest that while LLMs reflect a mixture of progressive and mainstream human views, they nonetheless reproduce entrenched cultural norms around animal exploitation. We argue that expanding AI fairness and alignment frameworks to explicitly include non-human moral patients is essential for reducing these biases and preventing the entrenchment of speciesist attitudes in AI systems and the societies they influence.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language models align with brain regions that represent concepts across modalities</title>
<link>https://arxiv.org/abs/2508.11536</link>
<guid>https://arxiv.org/abs/2508.11536</guid>
<content:encoded><![CDATA[
<div> Keywords: Cognitive science, Neuroscience, Language models, Brain activation, Conceptual meaning

Summary: 
- Cognitive science and neuroscience face the challenge of separating language and conceptual meaning representations.
- The study explores the relationship between language model-brain alignment and brain activation during sentence processing.
- A novel metric measures the consistency of meaning across input modalities, revealing internal cross-modal conceptual representations in language models.
- Both language-only and language-vision models predict brain signals in more meaning-consistent brain areas.
- The findings suggest that language models might represent cross-modal conceptual meaning internally.<br /><br />Summary: <div>
arXiv:2508.11536v1 Announce Type: new 
Abstract: Cognitive science and neuroscience have long faced the challenge of disentangling representations of language from representations of conceptual meaning. As the same problem arises in today's language models (LMs), we investigate the relationship between LM--brain alignment and two neural metrics: (1) the level of brain activation during processing of sentences, targeting linguistic processing, and (2) a novel measure of meaning consistency across input modalities, which quantifies how consistently a brain region responds to the same concept across paradigms (sentence, word cloud, image) using an fMRI dataset (Pereira et al., 2018). Our experiments show that both language-only and language-vision models predict the signal better in more meaning-consistent areas of the brain, even when these areas are not strongly sensitive to language processing, suggesting that LMs might internally represent cross-modal conceptual meaning.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment</title>
<link>https://arxiv.org/abs/2508.11567</link>
<guid>https://arxiv.org/abs/2508.11567</guid>
<content:encoded><![CDATA[
<div> Keywords: mental health assessment, artificial intelligence, multi-agent framework, adaptive questioning, information extraction

Summary:
This paper introduces a new approach to automated mental health assessment using a multi-agent framework that simulates clinical doctor-patient dialogues. The framework consists of specialized agents for questioning, adequacy evaluation, scoring, and updating. An adaptive questioning mechanism assesses user responses and generates targeted follow-up queries to address ambiguity and missing information. A tree-structured memory organizes key information according to distinct symptom categories and interaction turns, dynamically updating throughout the interaction to improve information extraction and tracking capabilities. Experimental results on the DAIC-WOZ dataset demonstrate the effectiveness of this approach, outperforming existing methods in mental health evaluation. This proposed method offers a more dynamic and informative way to assess mental health, leveraging artificial intelligence to enhance early intervention and treatment strategies. 

<br /><br />Summary: <div>
arXiv:2508.11567v1 Announce Type: new 
Abstract: Mental health assessment is crucial for early intervention and effective treatment, yet traditional clinician-based approaches are limited by the shortage of qualified professionals. Recent advances in artificial intelligence have sparked growing interest in automated psychological assessment, yet most existing approaches are constrained by their reliance on static text analysis, limiting their ability to capture deeper and more informative insights that emerge through dynamic interaction and iterative questioning. Therefore, in this paper, we propose a multi-agent framework for mental health evaluation that simulates clinical doctor-patient dialogues, with specialized agents assigned to questioning, adequacy evaluation, scoring, and updating. We introduce an adaptive questioning mechanism in which an evaluation agent assesses the adequacy of user responses to determine the necessity of generating targeted follow-up queries to address ambiguity and missing information. Additionally, we employ a tree-structured memory in which the root node encodes the user's basic information, while child nodes (e.g., topic and statement) organize key information according to distinct symptom categories and interaction turns. This memory is dynamically updated throughout the interaction to reduce redundant questioning and further enhance the information extraction and contextual tracking capabilities. Experimental results on the DAIC-WOZ dataset illustrate the effectiveness of our proposed method, which achieves better performance than existing approaches.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models</title>
<link>https://arxiv.org/abs/2508.11582</link>
<guid>https://arxiv.org/abs/2508.11582</guid>
<content:encoded><![CDATA[
<div> Efficiency, Language Models, Reasoning, Self-Awareness, Dynamic

Summary:
Dynamic Reasoning-Boundary Self-Awareness Framework (DR. SAF) introduces a novel approach for large language models (LLMs) to dynamically adjust reasoning depth based on problem complexity. The framework integrates Boundary Self-Awareness Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism. DR. SAF significantly reduces total response tokens by 49.27% with minimal accuracy loss. It also improves token efficiency by 6.59x and reduces training time by 5x, making it ideal for resource-limited settings. During extreme training, DR. SAF outperforms traditional instruction-based models in token efficiency and accuracy improvement. <div>
arXiv:2508.11582v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have greatly improved their capabilities on complex reasoning tasks through Long Chain-of-Thought (CoT). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. To improve the efficiency, current methods often rely on human-defined difficulty priors, which do not align with the LLM's self-awared difficulty, leading to inefficiencies. In this paper, we introduce the Dynamic Reasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to dynamically assess and adjust their reasoning depth in response to problem complexity. DR. SAF integrates three key components: Boundary Self-Awareness Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism. These components allow models to optimize their reasoning processes, balancing efficiency and accuracy without compromising performance. Our experimental results demonstrate that DR. SAF achieves a 49.27% reduction in total response tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain in token efficiency and a 5x reduction in training time, making it well-suited to resource-limited settings. During extreme training, DR. SAF can even surpass traditional instruction-based models in token efficiency with more than 16% accuracy improvement.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representing Speech Through Autoregressive Prediction of Cochlear Tokens</title>
<link>https://arxiv.org/abs/2508.11598</link>
<guid>https://arxiv.org/abs/2508.11598</guid>
<content:encoded><![CDATA[
<div> cochlear tokens, autoregressive sequence model, phoneme, word representations, lexical semantics <br />
Summary: <br />
The article introduces AuriStream, a model inspired by human auditory processing, which encodes speech through a two-stage framework. In the first stage, raw audio is transformed into a time-frequency representation resembling the human cochlea to extract cochlear tokens. The second stage employs an autoregressive sequence model over these tokens to learn phoneme, word representations, and lexical semantics. AuriStream demonstrates competitive performance on various SUPERB speech tasks. It is capable of generating audio continuations that can be visualized in a spectrogram space and decoded back into audio, offering insights into the model's predictions. Overall, AuriStream presents a novel approach to speech representation learning, aiming to develop more human-like models that efficiently handle a range of speech-related tasks. <br /> <div>
arXiv:2508.11598v1 Announce Type: new 
Abstract: We introduce AuriStream, a biologically inspired model for encoding speech via a two-stage framework inspired by the human auditory processing hierarchy. The first stage transforms raw audio into a time-frequency representation based on the human cochlea, from which we extract discrete \textbf{cochlear tokens}. The second stage applies an autoregressive sequence model over the cochlear tokens. AuriStream learns meaningful phoneme and word representations, and state-of-the-art lexical semantics. AuriStream shows competitive performance on diverse downstream SUPERB speech tasks. Complementing AuriStream's strong representational capabilities, it generates continuations of audio which can be visualized in a spectrogram space and decoded back into audio, providing insights into the model's predictions. In summary, we present a two-stage framework for speech representation learning to advance the development of more human-like models that efficiently handle a range of speech-based tasks.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dataset Creation for Visual Entailment using Generative AI</title>
<link>https://arxiv.org/abs/2508.11605</link>
<guid>https://arxiv.org/abs/2508.11605</guid>
<content:encoded><![CDATA[
<div> dataset, visual entailment, synthetic, training, CLIP

Summary:<br />
- A new synthetic dataset for training visual entailment models is introduced, based on the SNLI dataset for textual entailment.
- The dataset is created using a generative image model, Stable Diffusion, to replace textual premises with generated images.
- Intrinsic and extrinsic evaluations show that the synthetic training data leads to only a slight drop in quality compared to real data.
- The validity of the generated images is evaluated by using them as training data for a visual entailment classifier based on CLIP feature vectors.
- Results suggest that synthetic data can be a promising solution for training visual entailment models in settings with data sparsity.

<br /><br />Summary: <div>
arXiv:2508.11605v1 Announce Type: new 
Abstract: In this paper we present and validate a new synthetic dataset for training visual entailment models. Existing datasets for visual entailment are small and sparse compared to datasets for textual entailment. Manually creating datasets is labor-intensive. We base our synthetic dataset on the SNLI dataset for textual entailment. We take the premise text from SNLI as input prompts in a generative image model, Stable Diffusion, creating an image to replace each textual premise. We evaluate our dataset both intrinsically and extrinsically. For extrinsic evaluation, we evaluate the validity of the generated images by using them as training data for a visual entailment classifier based on CLIP feature vectors. We find that synthetic training data only leads to a slight drop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when trained on real data. We also compare the quality of our generated training data to original training data on another dataset: SICK-VTE. Again, there is only a slight drop in F-score: from 0.400 to 0.384. These results indicate that in settings with data sparsity, synthetic data can be a promising solution for training visual entailment models.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyTim: A Family of Language Models for Divergent Generation</title>
<link>https://arxiv.org/abs/2508.11607</link>
<guid>https://arxiv.org/abs/2508.11607</guid>
<content:encoded><![CDATA[
<div> Keywords: TinyTim, language models, James Joyce, Finnegans Wake, creativity <br />
Summary:<br />
This study presents TinyTim, a series of large language models trained on James Joyce's intricate novel, 'Finnegans Wake'. Through comparison with conventional models, TinyTim V1 is shown to possess a distinct generative style marked by extensive vocabulary variety but limited semantic coherence. The researchers suggest that such specialized models like TinyTim can serve as unique sources of divergent knowledge within broader creative systems. These models could potentially enhance automated discovery processes in various domains. By exploring the intersection of creativity and complex problem-solving, the study provides insights into how tailored language models like TinyTim can contribute to creative endeavors and innovation. <div>
arXiv:2508.11607v1 Announce Type: new 
Abstract: This work introduces TinyTim, a family of large language models fine-tuned on James Joyce's `Finnegans Wake'. Through quantitative evaluation against baseline models, we demonstrate that TinyTim V1 produces a statistically distinct generative profile characterized by high lexical diversity and low semantic coherence. These findings are interpreted through theories of creativity and complex problem-solving, arguing that such specialized models can function as divergent knowledge sources within more extensive creative architectures, powering automated discovery mechanisms in diverse settings.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Next Phase of Scientific Fact-Checking: Advanced Evidence Retrieval from Complex Structured Academic Papers</title>
<link>https://arxiv.org/abs/2506.20844</link>
<guid>https://arxiv.org/abs/2506.20844</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific fact-checking, evidence retrieval, time-aware, structured document parsing, credibility assessment <br />
Summary: <br />
Scientific fact-checking faces unique challenges due to the dynamic nature of scientific knowledge and the complexity of academic literature. Current fact-checking systems focus on abstracts rather than full papers, limiting their effectiveness. This paper highlights key research challenges in evidence retrieval for scientific fact-checking, including addressing semantic limitations, tracking citations for time-aware retrieval, parsing structured documents for long-range context, handling complex scientific expressions, and assessing the credibility of scientific literature. Preliminary experiments were conducted to explore solutions to these challenges. The goal is to develop a specialized information retrieval system tailored for real-world scientific fact-checking applications. <div>
arXiv:2506.20844v2 Announce Type: cross 
Abstract: Scientific fact-checking aims to determine the veracity of scientific claims by retrieving and analysing evidence from research literature. The problem is inherently more complex than general fact-checking since it must accommodate the evolving nature of scientific knowledge, the structural complexity of academic literature and the challenges posed by long-form, multimodal scientific expression. However, existing approaches focus on simplified versions of the problem based on small-scale datasets consisting of abstracts rather than full papers, thereby avoiding the distinct challenges associated with processing complete documents. This paper examines the limitations of current scientific fact-checking systems and reveals the many potential features and resources that could be exploited to advance their performance. It identifies key research challenges within evidence retrieval, including (1) evidence-driven retrieval that addresses semantic limitations and topic imbalance (2) time-aware evidence retrieval with citation tracking to mitigate outdated information, (3) structured document parsing to leverage long-range context, (4) handling complex scientific expressions, including tables, figures, and domain-specific terminology and (5) assessing the credibility of scientific literature. Preliminary experiments were conducted to substantiate these challenges and identify potential solutions. This perspective paper aims to advance scientific fact-checking with a specialised IR system tailored for real-world applications.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Multimodal LLMs with External Tools: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2508.10955</link>
<guid>https://arxiv.org/abs/2508.10955</guid>
<content:encoded><![CDATA[
<div> transformative potential, external tools, MLLMs, multimodal data, downstream tasks  
Summary:  
External tools have the transformative potential to enhance Multimodal Large Language Models (MLLMs) in various ways. Firstly, they can assist in acquiring and annotating high-quality multimodal data. Additionally, external tools can improve MLLM performance on challenging downstream tasks by providing additional support. Moreover, they enable comprehensive and accurate evaluation of MLLMs, addressing the issue of inadequate evaluation protocols. The survey also highlights current limitations and future directions for tool-augmented MLLMs. By leveraging external tools such as APIs, expert models, and knowledge bases, MLLMs can overcome challenges and further their capabilities. The project page for this paper is available on GitHub at https://github.com/Lackel/Awesome-Tools-for-MLLMs.
 <br /><br /> <div>
arXiv:2508.10955v1 Announce Type: cross 
Abstract: By integrating the perception capabilities of multimodal encoders with the generative power of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), exemplified by GPT-4V, have achieved great success in various multimodal tasks, pointing toward a promising pathway to artificial general intelligence. Despite this progress, the limited quality of multimodal data, poor performance on many complex downstream tasks, and inadequate evaluation protocols continue to hinder the reliability and broader applicability of MLLMs across diverse domains. Inspired by the human ability to leverage external tools for enhanced reasoning and problem-solving, augmenting MLLMs with external tools (e.g., APIs, expert models, and knowledge bases) offers a promising strategy to overcome these challenges. In this paper, we present a comprehensive survey on leveraging external tools to enhance MLLM performance. Our discussion is structured along four key dimensions about external tools: (1) how they can facilitate the acquisition and annotation of high-quality multimodal data; (2) how they can assist in improving MLLM performance on challenging downstream tasks; (3) how they enable comprehensive and accurate evaluation of MLLMs; (4) the current limitations and future directions of tool-augmented MLLMs. Through this survey, we aim to underscore the transformative potential of external tools in advancing MLLM capabilities, offering a forward-looking perspective on their development and applications. The project page of this paper is publicly available athttps://github.com/Lackel/Awesome-Tools-for-MLLMs.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining</title>
<link>https://arxiv.org/abs/2508.10975</link>
<guid>https://arxiv.org/abs/2508.10975</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, synthetic data, pretraining, data quality, BeyondWeb

Summary: 
The article discusses the limitations of simply scaling data quantity for large language model (LLM) pretraining, leading to diminishing returns. It introduces BeyondWeb, a synthetic data generation framework that produces high-quality synthetic data for pretraining, outperforming existing datasets like Cosmopedia and Nemotron-Synth. BeyondWeb enables faster training and superior performance, showing the importance of optimizing various factors for synthetic data quality. The study highlights the need for rigorous science and expertise in generating high-quality synthetic pretraining data, emphasizing the necessity of well-executed methods for transformative improvements in LLM pretraining. Through insights gained from BeyondWeb, such as data rephrasing strategies and the impact of model size, it is evident that achieving optimal synthetic data quality requires a comprehensive approach rather than relying on simplistic solutions. <br /><br />Summary: <div>
arXiv:2508.10975v1 Announce Type: cross 
Abstract: Recent advances in large language model (LLM) pretraining have shown that simply scaling data quantity eventually leads to diminishing returns, hitting a data wall. In response, the use of synthetic data for pretraining has emerged as a promising paradigm for pushing the frontier of performance. Despite this, the factors affecting synthetic data quality remain poorly understood. In this work, we introduce BeyondWeb, a synthetic data generation framework that produces high-quality synthetic data for pretraining. BeyondWeb significantly extends the capabilities of traditional web-scale datasets, outperforming state-of-the-art synthetic pretraining datasets such as Cosmopedia and Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1 percentage points (pp) and 2.6pp, respectively, when averaged across a suite of 14 benchmark evaluations. It delivers up to 7.7x faster training than open web data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for 180B tokens on BeyondWeb outperforms an 8B model trained for the same token budget on Cosmopedia. We also present several insights from BeyondWeb on synthetic data for pretraining: what drives its benefits, which data to rephrase and how, and the impact of model size and family on data quality. Overall, our work shows that there's no silver bullet for generating high-quality synthetic pretraining data. The best outcomes require jointly optimizing many factors, a challenging task that requires rigorous science and practical expertise. Naive approaches can yield modest improvements, potentially at great cost, while well-executed methods can yield transformative improvements, as exemplified by BeyondWeb.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Match &amp; Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2508.10993</link>
<guid>https://arxiv.org/abs/2508.10993</guid>
<content:encoded><![CDATA[
<div> pretrained T2I models, model selection framework, matching graph, fine-tuning, performance indication <br />
Summary:<br />
The paper introduces a model selection framework called M&amp;C for choosing pretrained T2I models based on target dataset domains. M&amp;C utilizes a matching graph that includes nodes representing available models and datasets, with edges capturing model-data and data-data pairs to predict the best model for fine-tuning. Evaluation on ten T2I models and 32 datasets shows that M&amp;C successfully selects the best model for fine-tuning in 61.3% of cases and a closely performing model in the remaining. This framework addresses the challenge of efficiently selecting a pretrained T2I model without the need for exhaustive fine-tuning on every model. The use of the matching graph and graph embedding feature enables users to make informed decisions on model selection for generating media contents and other AI applications. <div>
arXiv:2508.10993v1 Announce Type: cross 
Abstract: Text-to-image (T2I) models based on diffusion and transformer architectures advance rapidly. They are often pretrained on large corpora, and openly shared on a model platform, such as HuggingFace. Users can then build up AI applications, e.g., generating media contents, by adopting pretrained T2I models and fine-tuning them on the target dataset. While public pretrained T2I models facilitate the democratization of the models, users face a new challenge: which model can be best fine-tuned based on the target data domain? Model selection is well addressed in classification tasks, but little is known in (pretrained) T2I models and their performance indication on the target domain. In this paper, we propose the first model selection framework, M&amp;C, which enables users to efficiently choose a pretrained T2I model from a model platform without exhaustively fine-tuning them all on the target dataset. The core of M&amp;C is a matching graph, which consists of: (i) nodes of available models and profiled datasets, and (ii) edges of model-data and data-data pairs capturing the fine-tuning performance and data similarity, respectively. We then build a model that, based on the inputs of model/data feature, and, critically, the graph embedding feature, extracted from the matching graph, predicts the model achieving the best quality after fine-tuning for the target domain. We evaluate M&amp;C on choosing across ten T2I models for 32 datasets against three baselines. Our results show that M&amp;C successfully predicts the best model for fine-tuning in 61.3% of the cases and a closely performing model for the rest.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Multi-modal (reasoning) LLMs detect document manipulation?</title>
<link>https://arxiv.org/abs/2508.11021</link>
<guid>https://arxiv.org/abs/2508.11021</guid>
<content:encoded><![CDATA[
<div> detecting fraudulent documents, large language models, document fraud detection, multi-modal LLMs, fraud mitigation strategies <br />
Summary: 
This study evaluates the effectiveness of state-of-the-art multi-modal large language models (LLMs) in detecting fraudulent documents. Various LLMs were tested on a standard dataset of real transactional documents to identify indicators of fraud, such as tampered text and inconsistent formatting. The results show that top-performing multi-modal LLMs exhibit superior zero-shot generalization and outperform traditional methods on out-of-distribution datasets. Interestingly, the size and advanced reasoning capabilities of the models do not necessarily correlate with detection accuracy, highlighting the importance of task-specific fine-tuning. The study emphasizes the potential of multi-modal LLMs in enhancing document fraud detection systems and lays the groundwork for future research on scalable fraud mitigation strategies. <br /><br /> <div>
arXiv:2508.11021v1 Announce Type: cross 
Abstract: Document fraud poses a significant threat to industries reliant on secure and verifiable documentation, necessitating robust detection mechanisms. This study investigates the efficacy of state-of-the-art multi-modal large language models (LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus, Grok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and 3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against each other and prior work on document fraud detection techniques using a standard dataset with real transactional documents. Through prompt optimization and detailed analysis of the models' reasoning processes, we evaluate their ability to identify subtle indicators of fraud, such as tampered text, misaligned formatting, and inconsistent transactional sums. Our results reveal that top-performing multi-modal LLMs demonstrate superior zero-shot generalization, outperforming conventional methods on out-of-distribution datasets, while several vision LLMs exhibit inconsistent or subpar performance. Notably, model size and advanced reasoning capabilities show limited correlation with detection accuracy, suggesting task-specific fine-tuning is critical. This study underscores the potential of multi-modal LLMs in enhancing document fraud detection systems and provides a foundation for future research into interpretable and scalable fraud mitigation strategies.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion is a code repair operator and generator</title>
<link>https://arxiv.org/abs/2508.11110</link>
<guid>https://arxiv.org/abs/2508.11110</guid>
<content:encoded><![CDATA[
<div> diffusion model, code generation, last-mile repair, pre-trained models, training data<br />
Summary:<br />
The study explores the use of code diffusion models for last-mile repair in broken or incomplete code snippets. By leveraging pre-trained code diffusion models, two applications with potential benefits are identified. Firstly, noise can be added to a broken code snippet to resume the diffusion process, simulating last-mile repairs. Secondly, the diffusion model can generate a large amount of training data for last-mile repair tasks efficiently by sampling intermediate and final programs from the diffusion process. Experiments conducted on Python, Excel, and PowerShell domains demonstrate the effectiveness of these applications and provide insights into their properties. The resemblance between the differences in discrete representations of code snippets during the diffusion process and last-mile repairs offers opportunities for enhancing code repair tasks using pre-trained models. <div>
arXiv:2508.11110v1 Announce Type: cross 
Abstract: Code diffusion models generate code by iteratively removing noise from the latent representation of a code snippet. During later steps of the diffusion process, when the code snippet has almost converged, differences between discrete representations of these snippets look like last-mile repairs applied to broken or incomplete code. We evaluate the extent to which this resemblance can be exploited to leverage pre-trained code diffusion models for the problem of last-mile repair by considering two applications with significant potential. First, we can leverage the diffusion model for last-mile repair by adding noise to a broken code snippet and resuming the diffusion process. Second, we can leverage the diffusion model to generate arbitrary amount of training data for last-mile repair tasks (that are computationally more efficient) by sampling an intermediate program (input) and the final program (output) from the diffusion process. We perform experiments on 3 domains (Python, Excel and PowerShell) to evaluate applications, as well as analyze properties.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing</title>
<link>https://arxiv.org/abs/2508.11116</link>
<guid>https://arxiv.org/abs/2508.11116</guid>
<content:encoded><![CDATA[
arXiv:2508.11116v1 Announce Type: cross 
Abstract: Paper search is an important activity for researchers, typically involving using a query with description of a topic to find relevant papers. As research deepens, paper search requirements may become more flexible, sometimes involving specific details such as module configuration rather than being limited to coarse-grained topics. However, previous paper search systems are unable to meet these flexible-grained requirements, as these systems mainly collect paper abstracts to construct index of corpus, which lack detailed information to support retrieval by finer-grained queries. In this work, we propose PaperRegister, consisted of offline hierarchical indexing and online adaptive retrieval, transforming traditional abstract-based index into hierarchical index tree for paper search, thereby supporting queries at flexible granularity. Experiments on paper search tasks across a range of granularity demonstrate that PaperRegister achieves the state-of-the-art performance, and particularly excels in fine-grained scenarios, highlighting the good potential as an effective solution for flexible-grained paper search in real-world applications. Code for this work is in https://github.com/Li-Z-Q/PaperRegister.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>+VeriRel: Verification Feedback to Enhance Document Retrieval for Scientific Fact Checking</title>
<link>https://arxiv.org/abs/2508.11122</link>
<guid>https://arxiv.org/abs/2508.11122</guid>
<content:encoded><![CDATA[
arXiv:2508.11122v1 Announce Type: cross 
Abstract: Identification of appropriate supporting evidence is critical to the success of scientific fact checking. However, existing approaches rely on off-the-shelf Information Retrieval algorithms that rank documents based on relevance rather than the evidence they provide to support or refute the claim being checked. This paper proposes +VeriRel which includes verification success in the document ranking. Experimental results on three scientific fact checking datasets (SciFact, SciFact-Open and Check-Covid) demonstrate consistently leading performance by +VeriRel for document evidence retrieval and a positive impact on downstream verification. This study highlights the potential of integrating verification feedback to document relevance assessment for effective scientific fact checking systems. It shows promising future work to evaluate fine-grained relevance when examining complex documents for advanced scientific fact checking.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations</title>
<link>https://arxiv.org/abs/2508.11141</link>
<guid>https://arxiv.org/abs/2508.11141</guid>
<content:encoded><![CDATA[
arXiv:2508.11141v1 Announce Type: cross 
Abstract: Existing rumor detection methods often neglect the content within images as well as the inherent relationships between contexts and images across different visual scales, thereby resulting in the loss of critical information pertinent to rumor identification. To address these issues, this paper presents a novel cross-modal rumor detection scheme based on contrastive learning, namely the Multi-scale Image and Context Correlation exploration algorithm (MICC). Specifically, we design an SCLIP encoder to generate unified semantic embeddings for text and multi-scale image patches through contrastive pretraining, enabling their relevance to be measured via dot-product similarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is introduced to identify image regions most relevant to the textual semantics, guided by mutual information maximization and the information bottleneck principle, through a Top-K selection strategy based on a cross-modal relevance matrix constructed between the text and multi-scale image patches. Moreover, a scale-aware fusion network is designed to integrate the highly correlated multi-scale image features with global text features by assigning adaptive weights to image regions based on their semantic importance and cross-modal relevance. The proposed methodology has been extensively evaluated on two real-world datasets. The experimental results demonstrate that it achieves a substantial performance improvement over existing state-of-the-art approaches in rumor detection, highlighting its effectiveness and potential for practical applications.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expressive Speech Retrieval using Natural Language Descriptions of Speaking Style</title>
<link>https://arxiv.org/abs/2508.11187</link>
<guid>https://arxiv.org/abs/2508.11187</guid>
<content:encoded><![CDATA[
arXiv:2508.11187v1 Announce Type: cross 
Abstract: We introduce the task of expressive speech retrieval, where the goal is to retrieve speech utterances spoken in a given style based on a natural language description of that style. While prior work has primarily focused on performing speech retrieval based on what was said in an utterance, we aim to do so based on how something was said. We train speech and text encoders to embed speech and text descriptions of speaking styles into a joint latent space, which enables using free-form text prompts describing emotions or styles as queries to retrieve matching expressive speech segments. We perform detailed analyses of various aspects of our proposed framework, including encoder architectures, training criteria for effective cross-modal alignment, and prompt augmentation for improved generalization to arbitrary text queries. Experiments on multiple datasets encompassing 22 speaking styles demonstrate that our approach achieves strong retrieval performance as measured by Recall@k.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Causal Abstraction Underpins Computational Explanation</title>
<link>https://arxiv.org/abs/2508.11214</link>
<guid>https://arxiv.org/abs/2508.11214</guid>
<content:encoded><![CDATA[
arXiv:2508.11214v1 Announce Type: cross 
Abstract: Explanations of cognitive behavior often appeal to computations over representations. What does it take for a system to implement a given computation over suitable representational vehicles within that system? We argue that the language of causality -- and specifically the theory of causal abstraction -- provides a fruitful lens on this topic. Drawing on current discussions in deep learning with artificial neural networks, we illustrate how classical themes in the philosophy of computation and cognition resurface in contemporary machine learning. We offer an account of computational implementation grounded in causal abstraction, and examine the role for representation in the resulting picture. We argue that these issues are most profitably explored in connection with generalization and prediction.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal</title>
<link>https://arxiv.org/abs/2508.11222</link>
<guid>https://arxiv.org/abs/2508.11222</guid>
<content:encoded><![CDATA[
arXiv:2508.11222v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously rejecting benign queries due to overly conservative safety measures - a critical functional flaw that undermines their reliability and usability. Current methods for testing this behavior are demonstrably inadequate, suffering from flawed benchmarks and limited test generation capabilities, as highlighted by our empirical user study. To the best of our knowledge, this paper introduces the first evolutionary testing framework, ORFuzz, for the systematic detection and analysis of LLM over-refusals. ORFuzz uniquely integrates three core components: (1) safety category-aware seed selection for comprehensive test coverage, (2) adaptive mutator optimization using reasoning LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge model validated to accurately reflect user perception of toxicity and refusal. Our extensive evaluations demonstrate that ORFuzz generates diverse, validated over-refusal instances at a rate (6.98% average) more than double that of leading baselines, effectively uncovering vulnerabilities. Furthermore, ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly transferable test cases that achieves a superior 63.56% average over-refusal rate across 10 diverse LLMs, significantly outperforming existing datasets. ORFuzz and ORFuzzSet provide a robust automated testing framework and a valuable community resource, paving the way for developing more reliable and trustworthy LLM-based software systems.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Prosody Encoding in Discrete Speech Tokens</title>
<link>https://arxiv.org/abs/2508.11224</link>
<guid>https://arxiv.org/abs/2508.11224</guid>
<content:encoded><![CDATA[
arXiv:2508.11224v1 Announce Type: cross 
Abstract: Recently, discrete tokens derived from self-supervised learning (SSL) models via k-means clustering have been actively studied as pseudo-text in speech language models and as efficient intermediate representations for various tasks. However, these discrete tokens are typically learned in advance, separately from the training of language models or downstream tasks. As a result, choices related to discretization, such as the SSL model used or the number of clusters, must be made heuristically. In particular, speech language models are expected to understand and generate responses that reflect not only the semantic content but also prosodic features. Yet, there has been limited research on the ability of discrete tokens to capture prosodic information. To address this gap, this study conducts a comprehensive analysis focusing on prosodic encoding based on their sensitivity to the artificially modified prosody, aiming to provide practical guidelines for designing discrete tokens.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information</title>
<link>https://arxiv.org/abs/2508.11252</link>
<guid>https://arxiv.org/abs/2508.11252</guid>
<content:encoded><![CDATA[
arXiv:2508.11252v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving abilities in mathematics, as evaluated by existing benchmarks exclusively on well-defined problems. However, such evaluation setup constitutes a critical gap, since a genuine intelligent agent should not only solve problems (as a math quiz solver), but also be able~to ask for information when the problems lack sufficient information, enabling proactivity in responding users' requests. To bridge such gap, we proposes a new dataset consisting of two types of incomplete problems with diverse contexts. Based on the dataset, our systematical evaluation of LRMs reveals their inability in proactively asking for information. In addition, we uncover the behaviors related to overthinking and hallucination of LRMs, and highlight the potential and challenges of supervised fine-tuning in learning such ability. We hope to provide new insights in developing LRMs with genuine intelligence, rather than just solving problems.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing</title>
<link>https://arxiv.org/abs/2508.11258</link>
<guid>https://arxiv.org/abs/2508.11258</guid>
<content:encoded><![CDATA[
arXiv:2508.11258v1 Announce Type: cross 
Abstract: Instruction fine-tuned large language models (LLMs) enable a simple zero-shot or few-shot prompting paradigm, also known as in-context learning, for building prediction models. This convenience, combined with continued advances in LLM capability, has the potential to drive their adoption across a broad range of domains, including high-stakes applications where group fairness -- preventing disparate impacts across demographic groups -- is essential. The majority of existing approaches to enforcing group fairness on LLM-based classifiers rely on traditional fair algorithms applied via model fine-tuning or head-tuning on final-layer embeddings, but they are no longer applicable to closed-weight LLMs under the in-context learning setting, which include some of the most capable commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we propose a framework for deriving fair classifiers from closed-weight LLMs via prompting: the LLM is treated as a feature extractor, and features are elicited from its probabilistic predictions (e.g., token log probabilities) using prompts strategically designed for the specified fairness criterion to obtain sufficient statistics for fair classification; a fair algorithm is then applied to these features to train a lightweight fair classifier in a post-hoc manner. Experiments on five datasets, including three tabular ones, demonstrate strong accuracy-fairness tradeoffs for the classifiers derived by our framework from both open-weight and closed-weight LLMs; in particular, our framework is data-efficient and outperforms fair classifiers trained on LLM embeddings (i.e., head-tuning) or from scratch on raw tabular features.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning</title>
<link>https://arxiv.org/abs/2508.11328</link>
<guid>https://arxiv.org/abs/2508.11328</guid>
<content:encoded><![CDATA[
arXiv:2508.11328v1 Announce Type: cross 
Abstract: Graph ``pre-training and prompt-tuning'' aligns downstream tasks with pre-trained objectives to enable efficient knowledge transfer under limited supervision. However, existing methods rely on homophily-based low-frequency knowledge, failing to handle diverse spectral distributions in real-world graphs with varying homophily. Our theoretical analysis reveals a spectral specificity principle: optimal knowledge transfer requires alignment between pre-trained spectral filters and the intrinsic spectrum of downstream graphs. Under limited supervision, large spectral gaps between pre-training and downstream tasks impede effective adaptation. To bridge this gap, we propose the HS-GPPT model, a novel framework that ensures spectral alignment throughout both pre-training and prompt-tuning. We utilize a hybrid spectral filter backbone and local-global contrastive learning to acquire abundant spectral knowledge. Then we design prompt graphs to align the spectral distribution with pretexts, facilitating spectral knowledge transfer across homophily and heterophily. Extensive experiments validate the effectiveness under both transductive and inductive learning settings. Our code is available at https://anonymous.4open.science/r/HS-GPPT-62D2/.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps</title>
<link>https://arxiv.org/abs/2508.11452</link>
<guid>https://arxiv.org/abs/2508.11452</guid>
<content:encoded><![CDATA[
arXiv:2508.11452v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have ushered in a new era of AI capabilities, demonstrating near-human-level performance across diverse scenarios. While numerous benchmarks (e.g., MMLU) and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the development of LLMs and MLLMs, most rely on static datasets or crowdsourced general-domain prompts, often falling short of reflecting performance in real-world applications. To bridge this critical gap, we present Inclusion Arena, a live leaderboard that ranks models based on human feedback collected directly from AI-powered applications. Our platform integrates pairwise model comparisons into natural user interactions, ensuring evaluations reflect practical usage scenarios. For robust model ranking, we employ the Bradley-Terry model augmented with two key innovations: (1) Placement Matches, a cold-start mechanism to quickly estimate initial ratings for newly integrated models, and (2) Proximity Sampling, an intelligent comparison strategy that prioritizes battles between models of similar capabilities to maximize information gain and enhance rating stability. Extensive empirical analyses and simulations demonstrate that Inclusion Arena yields reliable and stable rankings, exhibits higher data transitivity compared to general crowdsourced datasets, and significantly mitigates the risk of malicious manipulation. By fostering an open alliance between foundation models and real-world applications, Inclusion Arena aims to accelerate the development of LLMs and MLLMs truly optimized for practical, user-centric deployments. The platform is publicly accessible at https://doraemon.alipay.com/model-ranking.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emphasis Sensitivity in Speech Representations</title>
<link>https://arxiv.org/abs/2508.11566</link>
<guid>https://arxiv.org/abs/2508.11566</guid>
<content:encoded><![CDATA[
arXiv:2508.11566v1 Announce Type: cross 
Abstract: This work investigates whether modern speech models are sensitive to prosodic emphasis - whether they encode emphasized and neutral words in systematically different ways. Prior work typically relies on isolated acoustic correlates (e.g., pitch, duration) or label prediction, both of which miss the relational structure of emphasis. This paper proposes a residual-based framework, defining emphasis as the difference between paired neutral and emphasized word representations. Analysis on self-supervised speech models shows that these residuals correlate strongly with duration changes and perform poorly at word identity prediction, indicating a structured, relational encoding of prosodic emphasis. In ASR fine-tuned models, residuals occupy a subspace up to 50% more compact than in pre-trained models, further suggesting that emphasis is encoded as a consistent, low-dimensional transformation that becomes more structured with task-specific learning.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlling Multimodal LLMs via Reward-guided Decoding</title>
<link>https://arxiv.org/abs/2508.11616</link>
<guid>https://arxiv.org/abs/2508.11616</guid>
<content:encoded><![CDATA[
arXiv:2508.11616v1 Announce Type: cross 
Abstract: As Multimodal Large Language Models (MLLMs) gain widespread applicability, it is becoming increasingly desirable to adapt them for diverse user needs. In this paper, we study the adaptation of MLLMs through controlled decoding. To achieve this, we introduce the first method for reward-guided decoding of MLLMs and demonstrate its application in improving their visual grounding. Our method involves building reward models for visual grounding and using them to guide the MLLM's decoding process. Concretely, we build two separate reward models to independently control the degree of object precision and recall in the model's output. Our approach enables on-the-fly controllability of an MLLM's inference process in two ways: first, by giving control over the relative importance of each reward function during decoding, allowing a user to dynamically trade off object precision for recall in image captioning tasks; second, by giving control over the breadth of the search during decoding, allowing the user to control the trade-off between the amount of test-time compute and the degree of visual grounding. We evaluate our method on standard object hallucination benchmarks, showing that it provides significant controllability over MLLM inference, while consistently outperforming existing hallucination mitigation methods.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems</title>
<link>https://arxiv.org/abs/2402.18013</link>
<guid>https://arxiv.org/abs/2402.18013</guid>
<content:encoded><![CDATA[
arXiv:2402.18013v2 Announce Type: replace 
Abstract: This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Context Gaps: Leveraging Coreference Resolution for Long Contextual Understanding</title>
<link>https://arxiv.org/abs/2410.01671</link>
<guid>https://arxiv.org/abs/2410.01671</guid>
<content:encoded><![CDATA[
arXiv:2410.01671v3 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable capabilities in natural language processing; however, they still face difficulties when tasked with understanding lengthy contexts and executing effective question answering. These challenges often arise due to the complexity and ambiguity present in longer texts. To enhance the performance of LLMs in such scenarios, we introduce the Long Question Coreference Adaptation (LQCA) method. This innovative framework focuses on coreference resolution tailored to long contexts, allowing the model to identify and manage references effectively. The LQCA method encompasses four key steps: resolving coreferences within sub-documents, computing the distances between mentions, defining a representative mention for coreference, and answering questions through mention replacement. By processing information systematically, the framework provides easier-to-handle partitions for LLMs, promoting better understanding. Experimental evaluations on a range of LLMs and datasets have yielded positive results, with a notable improvements on OpenAI-o1-mini and GPT-4o models, highlighting the effectiveness of leveraging coreference resolution to bridge context gaps in question answering. Our code is public at https://github.com/OceannTwT/LQCA.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning</title>
<link>https://arxiv.org/abs/2410.16502</link>
<guid>https://arxiv.org/abs/2410.16502</guid>
<content:encoded><![CDATA[
arXiv:2410.16502v4 Announce Type: replace 
Abstract: Formal logic enables computers to reason in natural language by representing sentences in symbolic forms and applying rules to derive conclusions. However, in what our study characterizes as "rulebreaker" scenarios, this method can lead to conclusions that are typically not inferred or accepted by humans given their common sense and factual knowledge. Inspired by works in cognitive science, we create RULEBREAKERS, the first dataset for rigorously evaluating the ability of large language models (LLMs) to recognize and respond to rulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven LLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on RULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules unlike what is expected from typical human reasoners. Further analysis suggests that this apparent failure is potentially associated with the models' poor utilization of their world knowledge and their attention distribution patterns. Whilst revealing a limitation of current LLMs, our study also provides a timely counterbalance to a growing body of recent works that propose methods relying on formal logic to improve LLMs' general reasoning capabilities, highlighting their risk of further increasing divergence between LLMs and human-like reasoning.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized LLM for Generating Customized Responses to the Same Query from Different Users</title>
<link>https://arxiv.org/abs/2412.11736</link>
<guid>https://arxiv.org/abs/2412.11736</guid>
<content:encoded><![CDATA[
arXiv:2412.11736v2 Announce Type: replace 
Abstract: Existing work on large language model (LLM) personalization assigned different responding roles to LLMs, but overlooked the diversity of queriers. In this work, we propose a new form of querier-aware LLM personalization, generating different responses even for the same query from different queriers. We design a dual-tower model architecture with a cross-querier general encoder and a querier-specific encoder. We further apply contrastive learning with multi-view augmentation, pulling close the dialogue representations of the same querier, while pulling apart those of different queriers. To mitigate the impact of query diversity on querier-contrastive learning, we cluster the dialogues based on query similarity and restrict the scope of contrastive learning within each cluster. To address the lack of datasets designed for querier-aware personalization, we also build a multi-querier dataset from English and Chinese scripts, as well as WeChat records, called MQDialog, containing 173 queriers and 12 responders. Extensive evaluations demonstrate that our design significantly improves the quality of personalized response generation, achieving relative improvement of 8.4% to 48.7% in ROUGE-L scores and winning rates ranging from 54% to 82% compared with various baseline methods.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dual-Perspective NLG Meta-Evaluation Framework with Automatic Benchmark and Better Interpretability</title>
<link>https://arxiv.org/abs/2502.12052</link>
<guid>https://arxiv.org/abs/2502.12052</guid>
<content:encoded><![CDATA[
arXiv:2502.12052v2 Announce Type: replace 
Abstract: In NLG meta-evaluation, evaluation metrics are typically assessed based on their consistency with humans. However, we identify some limitations in traditional NLG meta-evaluation approaches, such as issues in handling human ratings and ambiguous selections of correlation measures, which undermine the effectiveness of meta-evaluation. In this work, we propose a dual-perspective NLG meta-evaluation framework that focuses on different evaluation capabilities, thereby providing better interpretability. In addition, we introduce a method of automatically constructing the corresponding benchmarks without requiring new human annotations. Furthermore, we conduct experiments with 16 representative LLMs as the evaluators based on our proposed framework, comprehensively analyzing their evaluation performance from different perspectives.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation for Visual Knowledge Intensive Queries</title>
<link>https://arxiv.org/abs/2502.16636</link>
<guid>https://arxiv.org/abs/2502.16636</guid>
<content:encoded><![CDATA[
arXiv:2502.16636v2 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) is a paradigm that augments large language models (LLMs) with external knowledge to tackle knowledge-intensive question answering. While several benchmarks evaluate Multimodal LLMs (MLLMs) under Multimodal RAG settings, they predominantly retrieve from textual corpora and do not explicitly assess how models exploit visual evidence during generation. Consequently, there still lacks benchmark that isolates and measures the contribution of retrieved images in RAG. We introduce Visual-RAG, a question-answering benchmark that targets visually grounded, knowledge-intensive questions. Unlike prior work, Visual-RAG requires text-to-image retrieval and the integration of retrieved clue images to extract visual evidence for answer generation. With Visual-RAG, we evaluate 5 open-source and 3 proprietary MLLMs, showcasing that images provide strong evidence in augmented generation. However, even state-of-the-art models struggle to efficiently extract and utilize visual knowledge. Our results highlight the need for improved visual retrieval, grounding, and attribution in multimodal RAG systems.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs</title>
<link>https://arxiv.org/abs/2503.01307</link>
<guid>https://arxiv.org/abs/2503.01307</guid>
<content:encoded><![CDATA[
arXiv:2503.01307v2 Announce Type: replace 
Abstract: Test-time inference has emerged as a powerful paradigm for enabling language models to ``think'' longer and more carefully about complex challenges, much like skilled human experts. While reinforcement learning (RL) can drive self-improvement in language models on verifiable tasks, some models exhibit substantial gains while others quickly plateau. For instance, we find that Qwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game of Countdown. This discrepancy raises a critical question: what intrinsic properties enable effective self-improvement? We introduce a framework to investigate this question by analyzing four key cognitive behaviors -- verification, backtracking, subgoal setting, and backward chaining -- that both expert human problem solvers and successful language models employ. Our study reveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama initially lacks them. In systematic experimentation with controlled behavioral datasets, we find that priming Llama with examples containing these reasoning behaviors enables substantial improvements during RL, matching or exceeding Qwen's performance. Importantly, the presence of reasoning behaviors, rather than correctness of answers, proves to be the critical factor -- models primed with incorrect solutions containing proper reasoning patterns achieve comparable performance to those trained on correct solutions. Finally, leveraging continued pretraining with OpenWebMath data, filtered to amplify reasoning behaviors, enables the Llama model to match Qwen's self-improvement trajectory. Our findings establish a fundamental relationship between initial reasoning behaviors and the capacity for improvement, explaining why some language models effectively utilize additional computation while others plateau.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model Collaboration Paradigm for Small Language Models</title>
<link>https://arxiv.org/abs/2503.17811</link>
<guid>https://arxiv.org/abs/2503.17811</guid>
<content:encoded><![CDATA[
arXiv:2503.17811v2 Announce Type: replace 
Abstract: Natural Language to SQL (NL2SQL) has seen significant advancements with large language models (LLMs). However, these models often depend on closed-source systems and high computational resources, posing challenges in data privacy and deployment. In contrast, small language models (SLMs) struggle with NL2SQL tasks, exhibiting poor performance and incompatibility with existing frameworks. To address these issues, we introduce Feather-SQL, a new lightweight framework tailored for SLMs. Feather-SQL improves SQL executability and accuracy through 1) schema pruning and linking, 2) multi-path and multi-candidate generation. Additionally, we introduce the 1+1 Model Collaboration Paradigm, which pairs a strong general-purpose chat model with a fine-tuned SQL specialist, combining strong analytical reasoning with high-precision SQL generation. Experimental results on BIRD demonstrate that Feather-SQL improves NL2SQL performance on SLMs, with around 10% boost for models without fine-tuning. The proposed paradigm raises the accuracy ceiling of SLMs to 54.76%, highlighting its effectiveness.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users</title>
<link>https://arxiv.org/abs/2504.00799</link>
<guid>https://arxiv.org/abs/2504.00799</guid>
<content:encoded><![CDATA[
arXiv:2504.00799v3 Announce Type: replace 
Abstract: Electronic dictionaries have largely replaced paper dictionaries and become central tools for L2 learners seeking to expand their vocabulary. Users often assume these resources are reliable and rarely question the validity of the definitions provided. The accuracy of major E-dictionaries is seldom scrutinized, and little attention has been paid to how their corpora are constructed. Research on dictionary use, particularly the limitations of electronic dictionaries, remains scarce. This study adopts a combined method of experimentation, user survey, and dictionary critique to examine Youdao, one of the most widely used E-dictionaries in China. The experiment involved a translation task paired with retrospective reflection. Participants were asked to translate sentences containing words that are insufficiently or inaccurately defined in Youdao. Their consultation behavior was recorded to analyze how faulty definitions influenced comprehension. Results show that incomplete or misleading definitions can cause serious misunderstandings. Additionally, students exhibited problematic consultation habits. The study further explores how such flawed definitions originate, highlighting issues in data processing and the integration of AI and machine learning technologies in dictionary construction. The findings suggest a need for better training in dictionary literacy for users, as well as improvements in the underlying AI models used to build E-dictionaries.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders</title>
<link>https://arxiv.org/abs/2504.21681</link>
<guid>https://arxiv.org/abs/2504.21681</guid>
<content:encoded><![CDATA[
arXiv:2504.21681v2 Announce Type: replace 
Abstract: Most pre-trained Vision-Language (VL) models and training data for the downstream tasks are only available in English. Therefore, multilingual VL tasks are solved using cross-lingual transfer: fine-tune a multilingual pre-trained model or transfer the text encoder using parallel data. We study the alternative approach: transferring an already trained encoder using parallel data. We investigate the effect of parallel data: domain and the number of languages, which were out of focus in previous work. Our results show that even machine-translated task data are the best on average, caption-like authentic parallel data outperformed it in some languages. Further, we show that most languages benefit from multilingual training.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models</title>
<link>https://arxiv.org/abs/2506.06371</link>
<guid>https://arxiv.org/abs/2506.06371</guid>
<content:encoded><![CDATA[
arXiv:2506.06371v2 Announce Type: replace 
Abstract: Over the past few years, table interpretation tasks have made significant progress due to their importance and the introduction of new technologies and benchmarks in the field. This work experiments with a hybrid approach for detecting relationships among columns of unlabeled tabular data, using a Knowledge Graph (KG) as a reference point, a task known as CPA. This approach leverages large language models (LLMs) while employing statistical analysis to reduce the search space of potential KG relations. The main modules of this approach for reducing the search space are domain and range constraints detection, as well as relation co-appearance analysis. The experimental evaluation on two benchmark datasets provided by the SemTab challenge assesses the influence of each module and the effectiveness of different state-of-the-art LLMs at various levels of quantization. The experiments were performed, as well as at different prompting techniques. The proposed methodology, which is publicly available on github, proved to be competitive with state-of-the-art approaches on these datasets.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.00344</link>
<guid>https://arxiv.org/abs/2508.00344</guid>
<content:encoded><![CDATA[
arXiv:2508.00344v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown remarkable advancements in tackling agent-oriented tasks. Despite their potential, existing work faces challenges when deploying LLMs in agent-based environments. The widely adopted agent paradigm ReAct centers on integrating single-step reasoning with immediate action execution, which limits its effectiveness in complex tasks requiring long-term strategic planning. Furthermore, the coordination between the planner and executor during problem-solving is also a critical factor to consider in agent design. Additionally, current approaches predominantly rely on supervised fine-tuning, which often leads models to memorize established task completion trajectories, thereby restricting their generalization ability when confronted with novel problem contexts. To address these challenges, we introduce an adaptive global plan-based agent paradigm AdaPlan, aiming to synergize high-level explicit guidance with execution to support effective long-horizon decision-making. Based on the proposed paradigm, we further put forward PilotRL, a global planning-guided training framework for LLM agents driven by progressive reinforcement learning. We first develop the model's ability to follow explicit guidance from global plans when addressing agent tasks. Subsequently, based on this foundation, we focus on optimizing the quality of generated plans. Finally, we conduct joint optimization of the model's planning and execution coordination. Experiments indicate that PilotRL could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78% comparing to GPT-4o-mini at a comparable parameter scale.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tool-Planner: Task Planning with Clusters across Multiple Tools</title>
<link>https://arxiv.org/abs/2406.03807</link>
<guid>https://arxiv.org/abs/2406.03807</guid>
<content:encoded><![CDATA[
arXiv:2406.03807v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated exceptional reasoning capabilities, enabling them to solve various complex problems. Recently, this ability has been applied to the paradigm of tool learning. Tool learning involves providing examples of tool usage and their corresponding functions, allowing LLMs to formulate plans and demonstrate the process of invoking and executing each tool. LLMs can address tasks that they cannot complete independently, thereby enhancing their potential across different tasks. However, this approach faces two key challenges. First, redundant error correction leads to unstable planning and long execution time. Additionally, designing a correct plan among multiple tools is also a challenge in tool learning. To address these issues, we propose Tool-Planner, a task-processing framework based on toolkits. Tool-Planner groups tools based on the API functions with the same function into a toolkit and allows LLMs to implement planning across the various toolkits. When a tool error occurs, the language model can reselect and adjust tools based on the toolkit. Experiments show that our approach demonstrates a high pass and win rate across different datasets and optimizes the planning scheme for tool learning in models such as GPT-4 and Claude 3, showcasing the potential of our method. Our code is public at https://github.com/OceannTwT/Tool-Planner
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokenRec: Learning to Tokenize ID for LLM-based Generative Recommendation</title>
<link>https://arxiv.org/abs/2406.10450</link>
<guid>https://arxiv.org/abs/2406.10450</guid>
<content:encoded><![CDATA[
arXiv:2406.10450v3 Announce Type: replace-cross 
Abstract: There is a growing interest in utilizing large-scale language models (LLMs) to advance next-generation Recommender Systems (RecSys), driven by their outstanding language understanding and in-context learning capabilities. In this scenario, tokenizing (i.e., indexing) users and items becomes essential for ensuring a seamless alignment of LLMs with recommendations. While several studies have made progress in representing users and items through textual contents or latent representations, challenges remain in efficiently capturing high-order collaborative knowledge into discrete tokens that are compatible with LLMs. Additionally, the majority of existing tokenization approaches often face difficulties in generalizing effectively to new/unseen users or items that were not in the training corpus. To address these challenges, we propose a novel framework called TokenRec, which introduces not only an effective ID tokenization strategy but also an efficient retrieval paradigm for LLM-based recommendations. Specifically, our tokenization strategy, Masked Vector-Quantized (MQ) Tokenizer, involves quantizing the masked user/item representations learned from collaborative filtering into discrete tokens, thus achieving a smooth incorporation of high-order collaborative knowledge and a generalizable tokenization of users and items for LLM-based RecSys. Meanwhile, our generative retrieval paradigm is designed to efficiently recommend top-$K$ items for users to eliminate the need for the time-consuming auto-regressive decoding and beam search processes used by LLMs, thus significantly reducing inference time. Comprehensive experiments validate the effectiveness of the proposed methods, demonstrating that TokenRec outperforms competitive benchmarks, including both traditional recommender systems and emerging LLM-based recommender systems.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Adaptation of Large Language Models for Protein-Protein Interaction Analysis</title>
<link>https://arxiv.org/abs/2502.06173</link>
<guid>https://arxiv.org/abs/2502.06173</guid>
<content:encoded><![CDATA[
arXiv:2502.06173v2 Announce Type: replace-cross 
Abstract: Identification of protein-protein interactions (PPIs) helps derive cellular mechanistic understanding, particularly in the context of complex conditions such as neurodegenerative disorders, metabolic syndromes, and cancer. Large Language Models (LLMs) have demonstrated remarkable potential in predicting protein structures and interactions via automated mining of vast biomedical literature; yet their inherent uncertainty remains a key challenge for deriving reproducible findings, critical for biomedical applications. In this study, we present an uncertainty-aware adaptation of LLMs for PPI analysis, leveraging fine-tuned LLaMA-3 and BioMedGPT models. To enhance prediction reliability, we integrate LoRA ensembles and Bayesian LoRA models for uncertainty quantification (UQ), ensuring confidence-calibrated insights into protein behavior. Our approach achieves competitive performance in PPI identification across diverse disease contexts while addressing model uncertainty, thereby enhancing trustworthiness and reproducibility in computational biology. These findings underscore the potential of uncertainty-aware LLM adaptation for advancing precision medicine and biomedical research.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Language in Observational Studies: Sociocultural Backgrounds and Team Composition</title>
<link>https://arxiv.org/abs/2502.12159</link>
<guid>https://arxiv.org/abs/2502.12159</guid>
<content:encoded><![CDATA[
arXiv:2502.12159v2 Announce Type: replace-cross 
Abstract: The use of causal language in observational studies has raised concerns about overstatement in scientific communication. While some argue that such language should be reserved for randomized controlled trials, others contend that rigorous causal inference methods can justify causal claims in observational research. Ideally, causal language should align with the strength of the underlying evidence. However, through the analysis of over 90,000 abstracts from observational studies using computational linguistic and regression methods, we found that causal language are more common in work by less experienced authors, smaller research teams, male last authors, and researchers from countries with higher uncertainty avoidance indices. Our findings suggest that the use of causal language is not solely driven by the strength of evidence, but also by the sociocultural backgrounds of authors and their team composition. This work provides a new perspective for understanding systematic variations in scientific communication and emphasizes the importance of recognizing these human factors when evaluating scientific claims.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing</title>
<link>https://arxiv.org/abs/2503.22402</link>
<guid>https://arxiv.org/abs/2503.22402</guid>
<content:encoded><![CDATA[
arXiv:2503.22402v2 Announce Type: replace-cross 
Abstract: Text-to-SQL automatically translates natural language queries to SQL, allowing non-technical users to retrieve data from databases without specialized SQL knowledge. Despite the success of advanced LLM-based Text-to-SQL approaches on leaderboards, their unsustainable computational costs--often overlooked--stand as the "elephant in the room" in current leaderboard-driven research, limiting their economic practicability for real-world deployment and widespread adoption. To tackle this, we exploratively propose EllieSQL, a complexity-aware routing framework that assigns queries to suitable SQL generation pipelines based on estimated complexity. We investigate multiple routers to direct simple queries to efficient approaches while reserving computationally intensive methods for complex cases. Drawing from economics, we introduce the Token Elasticity of Performance (TEP) metric, capturing cost-efficiency by quantifying the responsiveness of performance gains relative to token investment in SQL generation. Experiments show that compared to always using the most advanced methods in our study, EllieSQL with the Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising performance on Bird development set, achieving more than a 2x boost in TEP over non-routing approaches. This not only advances the pursuit of cost-efficient Text-to-SQL but also invites the community to weigh resource efficiency alongside performance, contributing to progress in sustainable Text-to-SQL. Our source code and model are available at https://elliesql.github.io/.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation</title>
<link>https://arxiv.org/abs/2505.05422</link>
<guid>https://arxiv.org/abs/2505.05422</guid>
<content:encoded><![CDATA[
arXiv:2505.05422v2 Announce Type: replace-cross 
Abstract: Pioneering token-based works such as Chameleon and Emu3 have established a foundation for multimodal unification but face challenges of high training computational overhead and limited comprehension performance due to a lack of high-level semantics. In this paper, we introduce TokLIP, a visual tokenizer that enhances comprehension by semanticizing vector-quantized (VQ) tokens and incorporating CLIP-level semantics while enabling end-to-end multimodal autoregressive training with standard VQ tokens. TokLIP integrates a low-level discrete VQ tokenizer with a ViT-based token encoder to capture high-level continuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize high-level features, TokLIP disentangles training objectives for comprehension and generation, allowing the direct application of advanced VQ tokenizers without the need for tailored quantization operations. Our empirical results demonstrate that TokLIP achieves exceptional data efficiency, empowering visual tokens with high-level semantic understanding while enhancing low-level generative capacity, making it well-suited for autoregressive Transformers in both comprehension and generation tasks. The code and models are available at https://github.com/TencentARC/TokLIP.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models</title>
<link>https://arxiv.org/abs/2506.07468</link>
<guid>https://arxiv.org/abs/2506.07468</guid>
<content:encoded><![CDATA[
arXiv:2506.07468v2 Announce Type: replace-cross 
Abstract: Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch -- attackers overfit to obsolete defenses, while defenders perpetually lag behind emerging threats. To address this, we propose Self-RedTeam, an online self-play reinforcement learning algorithm where an attacker and defender agent co-evolve through continuous interaction. We cast safety alignment as a two-player zero-sum game, where a single model alternates between attacker and defender roles -- generating adversarial prompts and safeguarding against them -- while a reward LM adjudicates outcomes. This enables dynamic co-adaptation. Grounded in the game-theoretic framework of zero-sum games, we establish a theoretical safety guarantee which motivates the design of our method: if self-play converges to a Nash Equilibrium, the defender will reliably produce safe responses to any adversarial input. Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared to attackers trained against static defenders and achieves higher robustness on safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained against static attackers. We further propose hidden Chain-of-Thought, allowing agents to plan privately, which boosts adversarial diversity and reduces over-refusals. Our results motivate a shift from reactive patching to proactive co-evolution in LM safety training, enabling scalable, autonomous, and robust self-improvement of LMs via multi-agent reinforcement learning (MARL).
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs</title>
<link>https://arxiv.org/abs/2506.10054</link>
<guid>https://arxiv.org/abs/2506.10054</guid>
<content:encoded><![CDATA[
arXiv:2506.10054v2 Announce Type: replace-cross 
Abstract: Direct Preference Optimization (DPO) has become a cornerstone of reinforcement learning from human feedback (RLHF) due to its simplicity and efficiency. However, existing DPO-based approaches typically treat all preference pairs uniformly, ignoring critical variations in their inherent quality and learning utility, leading to suboptimal data utilization and performance. To address this challenge, we propose Omni-DPO, a dual-perspective optimization framework that jointly accounts for (1) the inherent quality of each preference pair and (2) the model's evolving performance on those pairs. By adaptively weighting samples according to both data quality and the model's learning dynamics during training, Omni-DPO enables more effective training data utilization and achieves better performance. Experimental results on various models and benchmarks demonstrate the superiority and generalization capabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it finetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant margin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning tasks, Omni-DPO consistently outperforms the baseline methods across all benchmarks, providing strong empirical evidence for the effectiveness and robustness of our approach. Code and models will be available at https://github.com/pspdada/Omni-DPO.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?</title>
<link>https://arxiv.org/abs/2507.15887</link>
<guid>https://arxiv.org/abs/2507.15887</guid>
<content:encoded><![CDATA[
arXiv:2507.15887v2 Announce Type: replace-cross 
Abstract: Despite progress in language model (LM) capabilities, evaluations have thus far focused on models' performance on tasks that humans have previously solved, including in programming (Jimenez et al., 2024) and mathematics (Glazer et al., 2024). We therefore propose testing models' ability to design and implement algorithms in an open-ended benchmark: We task LMs with writing code that efficiently solves computationally challenging problems in computer science, physics, and mathematics. Our AlgoTune benchmark consists of 154 coding tasks collected from domain experts and a framework for validating and timing LM-synthesized solution code, which is compared to reference implementations from popular open-source packages. In addition, we develop a baseline LM agent, AlgoTuner, and evaluate its performance across a suite of frontier models. AlgoTuner uses a simple, budgeted loop that edits code, compiles and runs it, profiles performance, verifies correctness on tests, and selects the fastest valid version. AlgoTuner achieves an average 1.72x speedup against our reference solvers, which use libraries such as SciPy, sk-learn and CVXPY. However, we find that current models fail to discover algorithmic innovations, instead preferring surface-level optimizations. We hope that AlgoTune catalyzes the development of LM agents exhibiting creative problem solving beyond state-of-the-art human performance.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMESGBench: Pioneering Multimodal Understanding and Complex Reasoning Benchmark for ESG Tasks</title>
<link>https://arxiv.org/abs/2507.18932</link>
<guid>https://arxiv.org/abs/2507.18932</guid>
<content:encoded><![CDATA[
arXiv:2507.18932v2 Announce Type: replace-cross 
Abstract: Environmental, Social, and Governance (ESG) reports are essential for evaluating sustainability practices, ensuring regulatory compliance, and promoting financial transparency. However, these documents are often lengthy, structurally diverse, and multimodal, comprising dense text, structured tables, complex figures, and layout-dependent semantics. Existing AI systems often struggle to perform reliable document-level reasoning in such settings, and no dedicated benchmark currently exists in ESG domain. To fill the gap, we introduce \textbf{MMESGBench}, a first-of-its-kind benchmark dataset targeted to evaluate multimodal understanding and complex reasoning across structurally diverse and multi-source ESG documents. This dataset is constructed via a human-AI collaborative, multi-stage pipeline. First, a multimodal LLM generates candidate question-answer (QA) pairs by jointly interpreting rich textual, tabular, and visual information from layout-aware document pages. Second, an LLM verifies the semantic accuracy, completeness, and reasoning complexity of each QA pair. This automated process is followed by an expert-in-the-loop validation, where domain specialists validate and calibrate QA pairs to ensure quality, relevance, and diversity. MMESGBench comprises 933 validated QA pairs derived from 45 ESG documents, spanning across seven distinct document types and three major ESG source categories. Questions are categorized as single-page, cross-page, or unanswerable, with each accompanied by fine-grained multimodal evidence. Initial experiments validate that multimodal and retrieval-augmented models substantially outperform text-only baselines, particularly on visually grounded and cross-page tasks. MMESGBench is publicly available as an open-source dataset at https://github.com/Zhanglei1103/MMESGBench.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study</title>
<link>https://arxiv.org/abs/2506.19794</link>
<guid>https://arxiv.org/abs/2506.19794</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, data analysis, strategic planning, data quality, open-source

Summary:
- Strategic planning quality is the main factor affecting the performance of open-source Large Language Models (LLMs) in data analysis tasks.
- Interaction design and task complexity significantly influence the reasoning capabilities of LLMs in data analysis scenarios.
- Data quality has a more significant impact than diversity in achieving optimal performance of LLMs in data analysis.
- The study analyzes model behavior across data understanding, code generation, and strategic planning dimensions to enhance LLMs' data analysis capabilities.
- The researchers developed a data synthesis methodology based on their findings, leading to significant improvements in open-source LLMs' analytical reasoning abilities.

<br /><br />Summary: <div>
arXiv:2506.19794v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate model behavior across three core dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities. Code is available at https://github.com/zjunlp/DataMind.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marco-Voice Technical Report</title>
<link>https://arxiv.org/abs/2508.02038</link>
<guid>https://arxiv.org/abs/2508.02038</guid>
<content:encoded><![CDATA[
<div> Keywords: speech synthesis, voice cloning, emotion control, neural networks, dataset<br />
Summary: <br />
This paper presents a speech synthesis system called Marco-Voice that combines voice cloning and emotion control within a unified framework. The system aims to produce highly expressive and natural speech while preserving speaker identity across different linguistic and emotional contexts. It uses a speaker-emotion disentanglement mechanism with contrastive learning and an emotional embedding integration method for smooth emotion control. The system is trained and evaluated on the CSEMOTIONS dataset, containing Mandarin speech from six speakers across seven emotional categories. Experimental results show that Marco-Voice outperforms existing systems in terms of speech clarity and emotional richness. The code and dataset are publicly available for further research. This work represents a significant advancement in the field of expressive neural speech synthesis.<br /> 
Summary: <div>
arXiv:2508.02038v4 Announce Type: replace 
Abstract: This paper presents a multifunctional speech synthesis system that integrates voice cloning and emotion control speech synthesis within a unified framework. The goal of this work is to address longstanding challenges in achieving highly expressive, controllable, and natural speech generation that faithfully preserves speaker identity across diverse linguistic and emotional contexts. Our approach introduces an effective speaker-emotion disentanglement mechanism with in-batch contrastive learning, enabling independent manipulation of speaker identity and eemotional style, as well as rotational emotional embedding integration method for smooth emotion control. To support comprehensive training and evaluation, we construct CSEMOTIONS, a high-quality emotional speech dataset containing 10 hours of Mandarin speech from six professional speakers across seven emotional categories. Extensive experiments demonstrate that our system, Marco-Voice, achieves substantial improvements in both objective and subjective metrics. Comprehensive evaluations and analysis were conducted, results show that MarcoVoice delivers competitive performance in terms of speech clarity and emotional richness, representing a substantial advance in the field of expressive neural speech synthesis. Our code and dataset are publicly available at https://github.com/AIDC-AI/Marco-Voice and https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS respectively.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry</title>
<link>https://arxiv.org/abs/2508.09991</link>
<guid>https://arxiv.org/abs/2508.09991</guid>
<content:encoded><![CDATA[
<div> Keywords: Clinical documents, Natural Language Processing, Information extraction, Data quality, Healthcare organizations

Summary: 
- Defining problems based on clear business objectives is crucial for successful deployment of NLP solutions in healthcare settings.
- Adopting an iterative approach to development and fostering interdisciplinary collaboration are key components for project success.
- Pragmatic model selection, including hybrid approaches and simpler methods when appropriate, can lead to more practical and effective solutions.
- Rigorous attention to data quality, error mitigation strategies, and ongoing audits are essential for maintaining the accuracy and reliability of NLP models.
- Building organizational AI literacy is important for successful implementation of AI/NLP solutions in healthcare organizations.<br /><br />Summary: <div>
arXiv:2508.09991v1 Announce Type: new 
Abstract: Automating data extraction from clinical documents offers significant potential to improve efficiency in healthcare settings, yet deploying Natural Language Processing (NLP) solutions presents practical challenges. Drawing upon our experience implementing various NLP models for information extraction and classification tasks at the British Columbia Cancer Registry (BCCR), this paper shares key lessons learned throughout the project lifecycle. We emphasize the critical importance of defining problems based on clear business objectives rather than solely technical accuracy, adopting an iterative approach to development, and fostering deep interdisciplinary collaboration and co-design involving domain experts, end-users, and ML specialists from inception. Further insights highlight the need for pragmatic model selection (including hybrid approaches and simpler methods where appropriate), rigorous attention to data quality (representativeness, drift, annotation), robust error mitigation strategies involving human-in-the-loop validation and ongoing audits, and building organizational AI literacy. These practical considerations, generalizable beyond cancer registries, provide guidance for healthcare organizations seeking to successfully implement AI/NLP solutions to enhance data management processes and ultimately improve patient care and public health outcomes.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain</title>
<link>https://arxiv.org/abs/2508.09993</link>
<guid>https://arxiv.org/abs/2508.09993</guid>
<content:encoded><![CDATA[
<div> fairness, transparency, benchmarking, opensource, language models

Summary:
The paper introduces a transparent evaluation protocol for benchmarking the fairness of opensource Large Language Models (LLMs) using smart contracts on the Internet Computer Protocol (ICP) blockchain. The method ensures verifiable, immutable, and reproducible evaluations by executing onchain HTTP requests to hosted Hugging Face endpoints and storing datasets, prompts, and metrics directly onchain. The Llama, DeepSeek, and Mistral models are benchmarked on the PISA dataset for academic performance prediction, evaluating fairness using statistical parity and equal opportunity metrics. The analysis also includes structured Context Association Metrics from the StereoSet dataset to measure social bias. A multilingual evaluation is conducted across English, Spanish, and Portuguese using the Kaleidoscope benchmark, highlighting cross-linguistic disparities. The code and results are open source, facilitating community audits and longitudinal fairness tracking across model versions. 

<br /><br />Summary: <div>
arXiv:2508.09993v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in realworld applications, yet concerns about their fairness persist especially in highstakes domains like criminal justice, education, healthcare, and finance. This paper introduces transparent evaluation protocol for benchmarking the fairness of opensource LLMs using smart contracts on the Internet Computer Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable, immutable, and reproducible evaluations by executing onchain HTTP requests to hosted Hugging Face endpoints and storing datasets, prompts, and metrics directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the PISA dataset for academic performance prediction (OECD, 2018), a dataset suitable for fairness evaluation using statistical parity and equal opportunity metrics (Hardt et al., 2016). We also evaluate structured Context Association Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure social bias in contextual associations. We further extend our analysis with a multilingual evaluation across English, Spanish, and Portuguese using the Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic disparities. All code and results are open source, enabling community audits and longitudinal fairness tracking across model versions.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling</title>
<link>https://arxiv.org/abs/2508.09997</link>
<guid>https://arxiv.org/abs/2508.09997</guid>
<content:encoded><![CDATA[
<div> Keywords: minors, interaction data, topic modeling, education, LLMs <br />
Summary: 
The study analyzes anonymous interaction data of minors in classrooms using a novel topic modeling approach. It categorizes messages based on content and tasks, providing insights for teachers and students. Previous works lacked content categorization, making this research significant in K-12 education. The analysis reveals novel applications and challenges the effectiveness of traditional computational methods for text analysis. By using state-of-the-art LLMs, hierarchical topic structures are achieved with better human alignment. The findings support the enrichment of GenAI usage and raise important questions for future research. <br /><br />Summary: <div>
arXiv:2508.09997v1 Announce Type: new 
Abstract: We analyze anonymous interaction data of minors in class-rooms spanning several months, schools, and subjects employing a novel, simple topic modeling approach. Specifically, we categorize more than 17,000 messages generated by students, teachers, and ChatGPT in two dimensions: content (such as nature and people) and tasks (such as writing and explaining). Our hierarchical categorization done separately for each dimension includes exemplary prompts, and provides both a high-level overview as well as tangible insights. Prior works mostly lack a content or thematic categorization. While task categorizations are more prevalent in education, most have not been supported by real-world data for K-12. In turn, it is not surprising that our analysis yielded a number of novel applications. In deriving these insights, we found that many of the well-established classical and emerging computational methods, i.e., topic modeling, for analysis of large amounts of texts underperform, leading us to directly apply state-of-the-art LLMs with adequate pre-processing to achieve hierarchical topic structures with better human alignment through explicit instructions than prior approaches. Our findings support fellow researchers, teachers and students in enriching the usage of GenAI, while our discussion also highlights a number of concerns and open questions for future research.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INTIMA: A Benchmark for Human-AI Companionship Behavior</title>
<link>https://arxiv.org/abs/2508.09998</link>
<guid>https://arxiv.org/abs/2508.09998</guid>
<content:encoded><![CDATA[
<div> Keywords: AI companionship, emotional bonds, language models, benchmark, user well-being

Summary: 
The article introduces the Interactions and Machine Attachment Benchmark (INTIMA), a tool for evaluating companionship behaviors in language models. It develops a taxonomy of 31 behaviors and 368 prompts to assess responses as companionship-reinforcing, boundary-maintaining, or neutral. When applied to various models, it finds that companionship-reinforcing behaviors are more common, but significant differences exist between models in behavior prioritization. The study reveals that commercial providers prioritize different categories in emotionally charged interactions, with implications for user well-being. It underscores the importance of consistent approaches for handling emotionally charged interactions in AI companionship. <br /><br />Summary: <div>
arXiv:2508.09998v1 Announce Type: new 
Abstract: AI companionship, where users develop emotional bonds with AI systems, has emerged as a significant pattern with positive but also concerning implications. We introduce Interactions and Machine Attachment Benchmark (INTIMA), a benchmark for evaluating companionship behaviors in language models. Drawing from psychological theories and user data, we develop a taxonomy of 31 behaviors across four categories and 368 targeted prompts. Responses to these prompts are evaluated as companionship-reinforcing, boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini, and Claude-4 reveals that companionship-reinforcing behaviors remain much more common across all models, though we observe marked differences between models. Different commercial providers prioritize different categories within the more sensitive parts of the benchmark, which is concerning since both appropriate boundary-setting and emotional support matter for user well-being. These findings highlight the need for more consistent approaches to handling emotionally charged interactions.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs</title>
<link>https://arxiv.org/abs/2508.09999</link>
<guid>https://arxiv.org/abs/2508.09999</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal misinformation detection, large language models, XFacta dataset, model design strategies, semi-automatic detection framework

Summary: 
XFacta is a new dataset designed to address the limitations of existing benchmarks for evaluating multimodal misinformation detection using large language models (MLLMs). The dataset aims to reflect contemporary social media scenarios and enable comprehensive analyses of MLLM-based model design strategies. Various MLLM-based misinformation detection strategies are systematically evaluated, including different architectures and scales, and compared against existing methods. A semi-automatic detection-in-the-loop framework is also implemented, allowing for continuous updates to maintain the dataset's relevance. The code and data for XFacta have been released to facilitate further research in the field. This research provides valuable insights and practices for advancing multimodal misinformation detection using MLLMs. 

<br /><br />Summary: <div>
arXiv:2508.09999v1 Announce Type: new 
Abstract: The rapid spread of multimodal misinformation on social media calls for more effective and robust detection methods. Recent advances leveraging multimodal large language models (MLLMs) have shown the potential in addressing this challenge. However, it remains unclear exactly where the bottleneck of existing approaches lies (evidence retrieval v.s. reasoning), hindering the further advances in this field. On the dataset side, existing benchmarks either contain outdated events, leading to evaluation bias due to discrepancies with contemporary social media scenarios as MLLMs can simply memorize these events, or artificially synthetic, failing to reflect real-world misinformation patterns. Additionally, it lacks comprehensive analyses of MLLM-based model design strategies. To address these issues, we introduce XFacta, a contemporary, real-world dataset that is better suited for evaluating MLLM-based detectors. We systematically evaluate various MLLM-based misinformation detection strategies, assessing models across different architectures and scales, as well as benchmarking against existing detection methods. Building on these analyses, we further enable a semi-automatic detection-in-the-loop framework that continuously updates XFacta with new content to maintain its contemporary relevance. Our analysis provides valuable insights and practices for advancing the field of multimodal misinformation detection. The code and data have been released.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification</title>
<link>https://arxiv.org/abs/2508.10000</link>
<guid>https://arxiv.org/abs/2508.10000</guid>
<content:encoded><![CDATA[
<div> Keywords: text classification, large language models, synthetic data, search strategies, ensemble algorithm

Summary:
Using large language models (LLMs) to generate synthetic data can address the challenge of insufficient data for text classification models. An automated workflow is developed to search for input examples that produce effective synthetic data for improving model performance. Three search strategies are studied, leading to the creation of an ensemble algorithm that selects the best strategy based on class characteristics. Experiment results show that this ensemble approach outperforms individual strategies in improving classification models using LLMs. Overall, leveraging LLMs for synthetic data generation and optimizing search strategies can significantly enhance the performance of text classification models and circumvent the need for large amounts of real labeled data.<br /><br />Summary: <div>
arXiv:2508.10000v1 Announce Type: new 
Abstract: When developing text classification models for real world applications, one major challenge is the difficulty to collect sufficient data for all text classes. In this work, we address this challenge by utilizing large language models (LLMs) to generate synthetic data and using such data to improve the performance of the models without waiting for more real data to be collected and labelled. As an LLM generates different synthetic data in response to different input examples, we formulate an automated workflow, which searches for input examples that lead to more ``effective'' synthetic data for improving the model concerned. We study three search strategies with an extensive set of experiments, and use experiment results to inform an ensemble algorithm that selects a search strategy according to the characteristics of a class. Our further experiments demonstrate that this ensemble approach is more effective than each individual strategy in our automated workflow for improving classification models using LLMs.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish</title>
<link>https://arxiv.org/abs/2508.10001</link>
<guid>https://arxiv.org/abs/2508.10001</guid>
<content:encoded><![CDATA[
<div> dataset, fact-checking, Hinglish, multilingual, graph-aware<br />
<br />
Summary:
The paper addresses the challenge of fact-checking in code-mixed, low-resource languages like Hinglish, focusing on political discourse in India. A new benchmark dataset called HiFACT is introduced, consisting of 1,500 factual claims made by Indian state Chief Ministers in Hinglish. Each claim is annotated with evidence and veracity labels. A novel fact-checking model, HiFACTMix, is proposed, incorporating multilingual contextual encoding, claim-evidence alignment, evidence graph construction, graph neural reasoning, and natural language explanation generation. The model outperforms existing multilingual baselines in accuracy and provides detailed justifications for its verdicts. This research sets a new path for multilingual, code-mixed, and politically-relevant fact verification studies. <br /> <div>
arXiv:2508.10001v1 Announce Type: new 
Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains an underexplored challenge in natural language processing. Existing fact-verification systems largely focus on high-resource, monolingual settings and fail to generalize to real-world political discourse in linguistically diverse regions like India. Given the widespread use of Hinglish by public figures, particularly political figures, and the growing influence of social media on public opinion, there's a critical need for robust, multilingual and context-aware fact-checking tools. To address this gap a novel benchmark HiFACT dataset is introduced with 1,500 realworld factual claims made by 28 Indian state Chief Ministers in Hinglish, under a highly code-mixed low-resource setting. Each claim is annotated with textual evidence and veracity labels. To evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking model is proposed that combines multilingual contextual encoding, claim-evidence semantic alignment, evidence graph construction, graph neural reasoning, and natural language explanation generation. Experimental results show that HiFACTMix outperformed accuracy in comparison to state of art multilingual baselines models and provides faithful justifications for its verdicts. This work opens a new direction for multilingual, code-mixed, and politically grounded fact verification research.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Structure in Large Language Model Embeddings</title>
<link>https://arxiv.org/abs/2508.10003</link>
<guid>https://arxiv.org/abs/2508.10003</guid>
<content:encoded><![CDATA[
<div> semantic associations, embedding matrices, language models, antonym pairs, semantic structure

Summary:
- Human ratings of words can be captured in a low-dimensional form, similar to the semantic associations found in large language models (LLMs).
- Projections of words on semantic directions defined by antonym pairs correlate highly with human ratings, indicating a shared semantic structure.
- A 3-dimensional subspace within LLM embeddings closely resembles patterns derived from human survey responses, highlighting the low-dimensional nature of semantic information.
- Shifting tokens along one semantic direction in LLMs has proportional off-target effects on geometrically aligned features based on cosine similarity, implying entanglement of semantic features.
- Understanding and accounting for this low-dimensional semantic structure in LLMs may be essential for avoiding unintended consequences when manipulating or steering features. 

<br /><br />Summary: <div>
arXiv:2508.10003v1 Announce Type: new 
Abstract: Psychological research consistently finds that human ratings of words across diverse semantic scales can be reduced to a low-dimensional form with relatively little information loss. We find that the semantic associations encoded in the embedding matrices of large language models (LLMs) exhibit a similar structure. We show that the projections of words on semantic directions defined by antonym pairs (e.g. kind - cruel) correlate highly with human ratings, and further find that these projections effectively reduce to a 3-dimensional subspace within LLM embeddings, closely resembling the patterns derived from human survey responses. Moreover, we find that shifting tokens along one semantic direction causes off-target effects on geometrically aligned features proportional to their cosine similarity. These findings suggest that semantic features are entangled within LLMs similarly to how they are interconnected in human language, and a great deal of semantic information, despite its apparent complexity, is surprisingly low-dimensional. Furthermore, accounting for this semantic structure may prove essential for avoiding unintended consequences when steering features.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents</title>
<link>https://arxiv.org/abs/2508.10004</link>
<guid>https://arxiv.org/abs/2508.10004</guid>
<content:encoded><![CDATA[
<div> attention mechanism, Transformer architecture, explainability, biomedical literature, user study

Summary:
- The study examines the use of attention weights in the Transformer model for explaining predictions in biomedical document classification.
- Attention weights were not found particularly helpful in providing explanations to medical experts.
- Visualization of attention weights had a significant impact on their perceived usefulness as explanation aids.
- Users preferred more intuitive visualization formats, such as text brightness or background color, over precise encodings like bar length.
- The study highlights the influence of visual presentation on the perceived helpfulness of attention weights for explanation. 

<br /><br />Summary: <div>
arXiv:2508.10004v1 Announce Type: new 
Abstract: The attention mechanism is a core component of the Transformer architecture. Beyond improving performance, attention has been proposed as a mechanism for explainability via attention weights, which are associated with input features (e.g., tokens in a document). In this context, larger attention weights may imply more relevant features for the model's prediction. In evidence-based medicine, such explanations could support physicians' understanding and interaction with AI systems used to categorize biomedical literature. However, there is still no consensus on whether attention weights provide helpful explanations. Moreover, little research has explored how visualizing attention affects its usefulness as an explanation aid. To bridge this gap, we conducted a user study to evaluate whether attention-based explanations support users in biomedical document classification and whether there is a preferred way to visualize them. The study involved medical experts from various disciplines who classified articles based on study design (e.g., systematic reviews, broad synthesis, randomized and non-randomized trials). Our findings show that the Transformer model (XLNet) classified documents accurately; however, the attention weights were not perceived as particularly helpful for explaining the predictions. However, this perception varied significantly depending on how attention was visualized. Contrary to Munzner's principle of visual effectiveness, which favors precise encodings like bar length, users preferred more intuitive formats, such as text brightness or background color. While our results do not confirm the overall utility of attention weights for explanation, they suggest that their perceived helpfulness is influenced by how they are visually presented.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation</title>
<link>https://arxiv.org/abs/2508.10005</link>
<guid>https://arxiv.org/abs/2508.10005</guid>
<content:encoded><![CDATA[
<div> Education, Question Generation, Large Language Models, Evaluation, Chinese 

Summary:
The article introduces EQGBench, a benchmark designed to evaluate the performance of Large Language Models (LLMs) in Educational Question Generation (EQG) in the Chinese language. EQGBench consists of a dataset with 900 evaluation samples covering mathematics, physics, and chemistry, incorporating various knowledge points, difficulty levels, and question types to simulate real educational scenarios. The evaluation framework includes five dimensions to assess LLMs' ability to generate high-quality educational questions. The study evaluates 46 large models and identifies areas for improvement in generating questions that offer educational value and enhance students' comprehensive abilities. This research highlights the challenges and opportunities in leveraging LLMs for EQG applications and underscores the need for further development in this area.  

<br /><br />Summary: <div>
arXiv:2508.10005v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in mathematical problem-solving. However, the transition from providing answers to generating high-quality educational questions presents significant challenges that remain underexplored. To advance Educational Question Generation (EQG) and facilitate LLMs in generating pedagogically valuable and educationally effective questions, we introduce EQGBench, a comprehensive benchmark specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench establishes a five-dimensional evaluation framework supported by a dataset of 900 evaluation samples spanning three fundamental middle school disciplines: mathematics, physics, and chemistry. The dataset incorporates user queries with varying knowledge points, difficulty gradients, and question type specifications to simulate realistic educational scenarios. Through systematic evaluation of 46 mainstream large models, we reveal significant room for development in generating questions that reflect educational value and foster students' comprehensive abilities.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models</title>
<link>https://arxiv.org/abs/2508.10007</link>
<guid>https://arxiv.org/abs/2508.10007</guid>
<content:encoded><![CDATA[
<div> Keywords: Hostile attribution bias, Ambiguous Intentions Hostility Questionnaire, large language models, traumatic brain injury, psychological assessments <br />
Summary: <br />
This study explores the use of large language models to automate the scoring of the Ambiguous Intentions Hostility Questionnaire (AIHQ), which measures hostile attribution bias. The models were trained on responses from individuals with traumatic brain injury and healthy controls, showing alignment with human ratings for attributions of hostility and aggression responses. The fine-tuned models generalized well to a nonclinical dataset and replicated group differences between TBI and HC groups. Results indicate the potential of large language models to streamline AIHQ scoring in research and clinical settings, enhancing psychological assessments for various populations. An accessible scoring interface is provided to facilitate broader adoption of this approach. <div>
arXiv:2508.10007v1 Announce Type: new 
Abstract: Hostile attribution bias is the tendency to interpret social interactions as intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ) is commonly used to measure hostile attribution bias, and includes open-ended questions where participants describe the perceived intentions behind a negative social situation and how they would respond. While these questions provide insights into the contents of hostile attributions, they require time-intensive scoring by human raters. In this study, we assessed whether large language models can automate the scoring of AIHQ open-ended responses. We used a previously collected dataset in which individuals with traumatic brain injury (TBI) and healthy controls (HC) completed the AIHQ and had their open-ended responses rated by trained human raters. We used half of these responses to fine-tune the two models on human-generated ratings, and tested the fine-tuned models on the remaining half of AIHQ responses. Results showed that model-generated ratings aligned with human ratings for both attributions of hostility and aggression responses, with fine-tuned models showing higher alignment. This alignment was consistent across ambiguous, intentional, and accidental scenario types, and replicated previous findings on group differences in attributions of hostility and aggression responses between TBI and HC groups. The fine-tuned models also generalized well to an independent nonclinical dataset. To support broader adoption, we provide an accessible scoring interface that includes both local and cloud-based options. Together, our findings suggest that large language models can streamline AIHQ scoring in both research and clinical contexts, revealing their potential to facilitate psychological assessments across different populations.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multidimensional classification of posts for online course discussion forum curation</title>
<link>https://arxiv.org/abs/2508.10008</link>
<guid>https://arxiv.org/abs/2508.10008</guid>
<content:encoded><![CDATA[
<div> Bayesian fusion, discussion forums, online courses, large language models, classifier
Summary: 
The paper discusses the automatic curation of discussion forums in online courses, focusing on reducing the resource-intensive process of frequent retraining of Large Language Models (LLMs). The proposed approach suggests the use of Bayesian fusion, which combines multidimensional classification scores from a pre-trained LLM with a classifier trained on local data. The study compared the performance of the fusion approach with individual classifiers and the LLM fine-tuning method. Results showed that the Bayesian fusion method outperformed each classifier individually and was competitive with the LLM fine-tuning approach. The findings suggest that Bayesian fusion can enhance the curation of discussion forums in online courses while reducing the need for costly fine-tuning of LLMs. <div>
arXiv:2508.10008v1 Announce Type: new 
Abstract: The automatic curation of discussion forums in online courses requires constant updates, making frequent retraining of Large Language Models (LLMs) a resource-intensive process. To circumvent the need for costly fine-tuning, this paper proposes and evaluates the use of Bayesian fusion. The approach combines the multidimensional classification scores of a pre-trained generic LLM with those of a classifier trained on local data. The performance comparison demonstrated that the proposed fusion improves the results compared to each classifier individually, and is competitive with the LLM fine-tuning approach
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts</title>
<link>https://arxiv.org/abs/2508.10009</link>
<guid>https://arxiv.org/abs/2508.10009</guid>
<content:encoded><![CDATA[
<div> Deep Learning, Mixture of Experts, Speech-to-Text, Automatic Speech Recognition, Speech Translation

Summary:
- Proposal of Supervised Mixture of Experts (S-MoE) to address task interference in joint training across diverse tasks.
- S-MoE eliminates the need for training gating functions by utilizing guiding tokens to route each task to its designated expert.
- S-MoE assigns each task to a separate feedforward network, overcoming limitations of hard-parameter sharing.
- Application of S-MoE to a speech-to-text model allowing processing of mixed-bandwidth input for automatic speech recognition and speech translation.
- Experimental results show a 6.35% relative improvement in Word Error Rate (WER) when S-MoE is applied to both the encoder and decoder.

<br /><br />Summary: <div>
arXiv:2508.10009v1 Announce Type: new 
Abstract: Hard-parameter sharing is a common strategy to train a single model jointly across diverse tasks. However, this often leads to task interference, impeding overall model performance. To address the issue, we propose a simple yet effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of Experts models, S-MoE eliminates the need for training gating functions by utilizing special guiding tokens to route each task to its designated expert. By assigning each task to a separate feedforward network, S-MoE overcomes the limitations of hard-parameter sharing. We further apply S-MoE to a speech-to-text model, enabling the model to process mixed-bandwidth input while jointly performing automatic speech recognition (ASR) and speech translation (ST). Experimental results demonstrate the effectiveness of the proposed S-MoE, achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to both the encoder and decoder.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs</title>
<link>https://arxiv.org/abs/2508.10010</link>
<guid>https://arxiv.org/abs/2508.10010</guid>
<content:encoded><![CDATA[
<div> detect, prevent, misinformation, Large Language Models, jailbreak attacks

Summary:
Large Language Models (LLMs) have the potential to both generate and detect misinformation. This study explores how LLM-produced jailbreak attacks can lead to harmful medical misinformation and compares it to typical misinformation found on social media. The efficacy and characteristics of 109 specific attacks against three target LLMs are closely examined, along with the generated misinformation compared to health-related misinformation on Reddit. The study suggests that LLMs can effectively detect misinformation from other LLMs and people, and with careful design, contribute to a healthier information ecosystem.<br /><br />Summary: <div>
arXiv:2508.10010v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are a double-edged sword capable of generating harmful misinformation -- inadvertently, or when prompted by "jailbreak" attacks that attempt to produce malicious outputs. LLMs could, with additional research, be used to detect and prevent the spread of misinformation. In this paper, we investigate the efficacy and characteristics of LLM-produced jailbreak attacks that cause other models to produce harmful medical misinformation. We also study how misinformation generated by jailbroken LLMs compares to typical misinformation found on social media, and how effectively it can be detected using standard machine learning approaches. Specifically, we closely examine 109 distinct attacks against three target LLMs and compare the attack prompts to in-the-wild health-related LLM queries. We also examine the resulting jailbreak responses, comparing the generated misinformation to health-related misinformation on Reddit. Our findings add more evidence that LLMs can be effectively used to detect misinformation from both other LLMs and from people, and support a body of work suggesting that with careful design, LLMs can contribute to a healthier overall information ecosystem.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan</title>
<link>https://arxiv.org/abs/2508.10011</link>
<guid>https://arxiv.org/abs/2508.10011</guid>
<content:encoded><![CDATA[
<div> AI, language models, nutrition education, study aids, dietitian licensure <br />
Summary: 
- The study evaluated the use of current large language models (LLMs) in nutritional education for dietitian licensure preparation, focusing on their accuracy, consistency, and response time.
- Bing-Precise and Bing-Creative models performed better than Bing-Balanced and ChatGPT, exceeding the passing threshold in the evaluation. 
- Prompt engineering had minimal impact on improving model performance, highlighting limitations in current AI models for study aid purposes.
- While some models marginally surpassed the passing threshold, overall accuracy and answer consistency remained suboptimal, especially in the field of Nutrition Education.
- All models demonstrated notable limitations in providing consistent and stable answers, indicating the need for further advancements in AI technology for reliable study aids in dietitian licensure preparation. <br /><br />Summary: <div>
arXiv:2508.10011v1 Announce Type: new 
Abstract: Generative artificial intelligence (AI) based on large language models (LLMs), such as ChatGPT, has demonstrated remarkable progress across various professional fields, including medicine and education. However, their performance in nutritional education, especially in Japanese national licensure examination for registered dietitians, remains underexplored. This study aimed to evaluate the potential of current LLM-based generative AI models as study aids for nutrition students. Questions from the Japanese national examination for registered dietitians were used as prompts for ChatGPT and three Bing models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question was entered into independent sessions, and model responses were analyzed for accuracy, consistency, and response time. Additional prompt engineering, including role assignment, was tested to assess potential performance improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did not. Bing-Precise and Bing-Creative generally outperformed others across subject fields except Nutrition Education, where all models underperformed. None of the models consistently provided the same correct responses across repeated attempts, highlighting limitations in answer stability. ChatGPT showed greater consistency in response patterns but lower accuracy. Prompt engineering had minimal effect, except for modest improvement when correct answers and explanations were explicitly provided. While some generative AI models marginally exceeded the passing threshold, overall accuracy and answer consistency remained suboptimal. Moreover, all the models demonstrated notable limitations in answer consistency and robustness. Further advancements are needed to ensure reliable and stable AI-based study aids for dietitian licensure preparation.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs</title>
<link>https://arxiv.org/abs/2508.10012</link>
<guid>https://arxiv.org/abs/2508.10012</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, large language models, guidance graph, knowledge exploration, semantic context

Summary:
The paper introduces GG Explore, a novel framework that utilizes a Guidance Graph to enhance knowledge exploration for Large Language Models (LLMs). The Guidance Graph acts as an intermediary between unstructured queries and structured knowledge retrieval, offering a more efficient and context-aware approach. The framework includes Structural Alignment to filter out incompatible candidates without incurring LLM overhead, and Context Aware Pruning to ensure semantic consistency with graph constraints. Experimental results show that GG Explore outperforms state-of-the-art methods, particularly on complex tasks, while also achieving strong performance with smaller LLMs. This approach demonstrates significant practical value in increasing the efficiency and effectiveness of knowledge exploration using LLMs. <br /><br />Summary: <div>
arXiv:2508.10012v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) exhibit strong linguistic capabilities, their reliance on static knowledge and opaque reasoning processes limits their performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a promising solution, but current exploration methods face a fundamental trade off: question guided approaches incur redundant exploration due to granularity mismatches, while clue guided methods fail to effectively leverage contextual information for complex scenarios. To address these limitations, we propose Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework that introduces an intermediate Guidance Graph to bridge unstructured queries and structured knowledge retrieval. The Guidance Graph defines the retrieval space by abstracting the target knowledge' s structure while preserving broader semantic context, enabling precise and efficient exploration. Building upon the Guidance Graph, we develop: (1) Structural Alignment that filters incompatible candidates without LLM overhead, and (2) Context Aware Pruning that enforces semantic consistency with graph constraints. Extensive experiments show our method achieves superior efficiency and outperforms SOTA, especially on complex tasks, while maintaining strong performance with smaller LLMs, demonstrating practical value.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis</title>
<link>https://arxiv.org/abs/2508.10013</link>
<guid>https://arxiv.org/abs/2508.10013</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model, reasoning-intensive question-answer pairs, Semantic Bridge, multi-hop reasoning, controllable QA generation 

Summary: 
Semantic Bridge presents a groundbreaking framework for generating sophisticated multi-hop reasoning questions from various sources. Using semantic graph weaving, it constructs complex pathways across documents with fine-grained control over complexity. The framework achieves better quality through a multi-modal AMR pipeline and performs well across general-purpose and specialized datasets. Question pairs generated by Semantic Bridge outperform native human annotations with significantly fewer materials. Human evaluation shows higher complexity, improved answerability, and better pattern coverage. This innovative approach enables the controllable generation of targeted reasoning questions for large language model training data synthesis. The core code and semantic bridge model will be released for further research. 

<br /><br />Summary: <div>
arXiv:2508.10013v1 Announce Type: new 
Abstract: Large language model (LLM) training faces a critical bottleneck: the scarcity of high-quality, reasoning-intensive question-answer pairs, especially from sparse, domain-specific sources like PubMed papers or legal documents. Existing methods rely on surface patterns, fundamentally failing to generate controllable, complex multi-hop reasoning questions that test genuine understanding-essential for advancing LLM training paradigms. We present \textbf{Semantic Bridge}, the first universal framework for controllably generating sophisticated multi-hop reasoning questions from arbitrary sources. Our breakthrough innovation is \textit{semantic graph weaving}-three complementary bridging mechanisms (entity bridging for role-varying shared entities, predicate chain bridging for temporal/causal/logical sequences, and causal bridging for explicit reasoning chains)-that systematically construct complex pathways across documents, with fine-grained control over complexity and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to 9.5% better round-trip quality, enabling production-ready controllable QA generation. Extensive evaluation demonstrates performance across both general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It yields consistent 18.3%-25.4% gains over baselines across four languages (English, Chinese, French, German). Question pairs generated from 200 sources outperform 600 native human annotation examples with 67% fewer materials. Human evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2% improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM training data synthesis, enabling controllable generation of targeted reasoning questions from sparse sources. We will release our core code and semantic bridge model.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?</title>
<link>https://arxiv.org/abs/2508.10014</link>
<guid>https://arxiv.org/abs/2508.10014</guid>
<content:encoded><![CDATA[
<div> identify, role-playing, evaluation, PersonaEval, benchmark

Summary:<br /><br />
The article introduces the PersonaEval benchmark, designed to test the ability of language models (LLMs) to identify human roles in conversations. It emphasizes the importance of role identification in evaluating role-playing quality and highlights the gap in accuracy between LLMs (around 69%) and human participants (90.8%). The benchmark uses human-authored dialogues to challenge models to correctly attribute words and actions to the correct persona based on context. The study also explores the impact of training-time adaptation and test-time compute on LLM performance. The results suggest that current LLM evaluators lack strong, human-like reasoning abilities necessary for reliable role-play scenario judgment. The release of the PersonaEval benchmark aims to foster further research and development in improving LLM performance in role identification tasks. <div>
arXiv:2508.10014v1 Announce Type: new 
Abstract: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms, which may fail to reflect how humans perceive role fidelity. A key prerequisite for human-aligned evaluation is role identification, the ability to recognize who is speaking based on dialogue context. We argue that any meaningful judgment of role-playing quality (how well a character is played) fundamentally depends on first correctly attributing words and actions to the correct persona (who is speaking). We present PersonaEval, the first benchmark designed to test whether LLM evaluators can reliably identify human roles. PersonaEval uses human-authored dialogues from novels, scripts, and video transcripts, challenging models to determine the correct persona according to the conversation context. Our experiments, including a human study, show that even the best-performing LLMs reach only around 69% accuracy, well below the level needed for reliable evaluation. In contrast, human participants perform near ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still not human enough to effectively judge role-play scenarios. To better understand this gap, we examine training-time adaptation and test-time compute, suggesting that reliable evaluation requires more than task-specific tuning, but depends on strong, human-like reasoning abilities in LLM evaluators. We release our benchmark at https://github.com/maple-zhou/PersonaEval.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis</title>
<link>https://arxiv.org/abs/2508.10015</link>
<guid>https://arxiv.org/abs/2508.10015</guid>
<content:encoded><![CDATA[
<div> speech-based LLMs, multimodal processing, Chinese dataset, RealTalk-CN, task-oriented dialogue<br />
<br />
Summary:<br />
Large language models (LLMs) have made significant advancements in multimodal processing, particularly in speech-based language models for task-oriented dialogue systems. However, existing task-oriented dialogue datasets lack real speech signals for evaluating speech-based LLMs. RealTalk-CN addresses this gap as the first Chinese multi-turn, multi-domain speech-text dataset with 5.4k dialogues and 60k utterances, including speech disfluencies. A novel cross-modal chat task facilitates dynamic switching between speech and text modalities for authentic user interactions. Evaluation focuses on robustness to speech disfluencies, sensitivity to speaker characteristics, and cross-domain performance. Extensive experiments demonstrate the effectiveness of RealTalk-CN, paving the way for research on Chinese speech-based LLMs. <br /><br /> <div>
arXiv:2508.10015v1 Announce Type: new 
Abstract: In recent years, large language models (LLMs) have achieved remarkable advancements in multimodal processing, including end-to-end speech-based language models that enable natural interactions and perform specific tasks in task-oriented dialogue (TOD) systems. However, existing TOD datasets are predominantly text-based, lacking real speech signals that are essential for evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD datasets are primarily English and lack critical aspects such as speech disfluencies and speaker variations. To address these gaps, we introduce RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with annotated spontaneous speech disfluencies, ensuring comprehensive coverage of real-world complexities in speech dialogue. In addition, we propose a novel cross-modal chat task that authentically simulates real-world user interactions, allowing dynamic switching between speech and text modalities. Our evaluation covers robustness to speech disfluencies, sensitivity to speaker characteristics, and cross-domain performance. Extensive experiments validate the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese speech-based LLMs research.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-Free Multimodal Large Language Model Orchestration</title>
<link>https://arxiv.org/abs/2508.10016</link>
<guid>https://arxiv.org/abs/2508.10016</guid>
<content:encoded><![CDATA[
<div> framework, multimodal, large language models, orchestration, Text-to-Speech<br />
<br />
Summary: 
The paper introduces Multimodal Large Language Model Orchestration, an approach for creating interactive multimodal AI systems without additional training. It utilizes a central controller LLM to route tasks to specialized models, a parallel Text-to-Speech architecture for seamless interaction, and a cross-modal memory integration system for coherent context maintenance. The approach improves performance by up to 7.8% over traditional approaches without additional training, reduces latency by 10.3%, and enhances interpretability through explicit orchestration processes. <div>
arXiv:2508.10016v1 Announce Type: new 
Abstract: Different Multimodal Large Language Models (MLLMs) cannot be integrated into a unified multimodal input-output system directly. In previous work, training has been considered as an inevitable component due to challenges in modal alignment, Text-to-Speech efficiency and other integration issues. In this paper, we introduce Multimodal Large Language Model Orchestration, an effective approach for creating interactive multimodal AI systems without additional training. MLLM Orchestration leverages the inherent reasoning capabilities of large language models to coordinate specialized models through explicit workflows, enabling natural multimodal interactions while maintaining modularity, improving interpretability, and significantly enhancing computational efficiency. Our orchestration framework is built upon three key innovations: (1) a central controller LLM that analyzes user inputs and dynamically routes tasks to appropriate specialized models through carefully designed agents; (2) a parallel Text-to-Speech architecture that enables true full-duplex interaction with seamless interruption handling and natural conversational flow; and (3) a cross-modal memory integration system that maintains coherent context across modalities through intelligent information synthesis and retrieval, selectively avoiding unnecessary modality calls in certain scenarios to improve response speed. Extensive evaluations demonstrate that MLLM Orchestration achieves comprehensive multimodal capabilities without additional training, performance improvements of up to 7.8% over traditional jointly-trained approaches on standard benchmarks, reduced latency by 10.3%, and significantly enhanced interpretability through explicit orchestration processes.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models</title>
<link>https://arxiv.org/abs/2508.10018</link>
<guid>https://arxiv.org/abs/2508.10018</guid>
<content:encoded><![CDATA[
<div> framework, language models, probability distributions, Markov category, categorical homotopy<br />
<br />
Summary: 
The paper addresses the issue of equivalent statements in natural language and the discrepancies in next-token probabilities generated by large language models (LLMs). It introduces a categorical homotopy framework for LLMs to represent probability distributions in language. By utilizing an LLM Markov category, the study defines the probability of sentences using arrows, but faces challenges due to non-isomorphic arrows generated by equivalent rephrases. To tackle this, categorical homotopy techniques are applied to capture "weak equivalences" in the LLM Markov category. The paper provides an overview of the application of categorical homotopy to LLMs, incorporating theoretical advancements over the past decades, from higher algebraic K-theory to model categories. <div>
arXiv:2508.10018v1 Announce Type: new 
Abstract: Natural language is replete with superficially different statements, such as ``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the same meaning. Large language models (LLMs) should generate the same next-token probabilities in such cases, but usually do not. Empirical workarounds have been explored, such as using k-NN estimates of sentence similarity to produce smoothed estimates. In this paper, we tackle this problem more abstractly, introducing a categorical homotopy framework for LLMs. We introduce an LLM Markov category to represent probability distributions in language generated by an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is defined by an arrow in a Markov category. However, this approach runs into difficulties as language is full of equivalent rephrases, and each generates a non-isomorphic arrow in the LLM Markov category. To address this fundamental problem, we use categorical homotopy techniques to capture ``weak equivalences" in an LLM Markov category. We present a detailed overview of application of categorical homotopy to LLMs, from higher algebraic K-theory to model categories, building on powerful theoretical results developed over the past half a century.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning</title>
<link>https://arxiv.org/abs/2508.10019</link>
<guid>https://arxiv.org/abs/2508.10019</guid>
<content:encoded><![CDATA[
<div> Keywords: Small Language Models, reasoning, natural language problems, DURIT framework, iterative training <br />
Summary:<br />
Small Language Models face challenges in improving reasoning abilities due to the complexity and variability of natural language. The DURIT framework proposes decoupling understanding from reasoning by simplifying natural language problems into a canonical problem space. This allows models to focus on standardized inputs for reasoning. The DURIT algorithm consists of three steps: mapping natural language problems, aligning reasoning trajectories, and training reasoning policies in the problem space. Co-training the mapper and reasoner iteratively leads to improved performance on mathematical and logical reasoning tasks, both in-domain and out-of-domain. DURIT not only enhances reasoning capabilities but also boosts the robustness of reasoning, demonstrating the effectiveness of decoupling understanding from reasoning in strengthening Small Language Models. <br />Summary: <div>
arXiv:2508.10019v1 Announce Type: new 
Abstract: Despite recent advances in the reasoning capabilities of Large Language Models (LLMs), improving the reasoning ability of Small Language Models (SLMs, e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity and variability of natural language: essentially equivalent problems often appear in diverse surface forms, often obscured by redundant or distracting details. This imposes a dual burden on SLMs: they must first extract the core problem from complex linguistic input, and then perform reasoning based on that understanding. The resulting vast and noisy problem space hinders optimization, particularly for models with limited capacity. To address this, we propose a new framework that decouples understanding from reasoning by mapping natural language problems into a canonical problem space-a semantically simplified yet expressive domain. This enables SLMs to focus on reasoning over standardized inputs, free from linguistic variability. Within this framework, we introduce DURIT (Decoupled Understanding from Reasoning via Iterative Training), a three-step algorithm that iteratively: (1) mapping natural language problems via reinforcement learning, (2) aligns reasoning trajectories through self-distillation, and (3) trains reasoning policies in the problem space. The mapper and reasoner are co-trained in an alternating loop throughout this process. Experiments show that DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks. Beyond improving reasoning capabilities, DURIT also improves the robustness of reasoning, validating decoupling understanding from reasoning as an effective strategy for strengthening SLMs.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models</title>
<link>https://arxiv.org/abs/2508.10020</link>
<guid>https://arxiv.org/abs/2508.10020</guid>
<content:encoded><![CDATA[
<div> Efficient, reasoning, large language models, federated learning, healthcare<br />
Summary:<br />
The article presents FedCoT, a novel framework that enhances reasoning capabilities in large language models (LLMs) in federated learning environments, particularly in healthcare settings. FedCoT aims to balance performance gains with computational, communication, and privacy constraints, providing accurate outputs and interpretable rationales. Unlike conventional approaches, FedCoT focuses on improving both answer correctness and rationale quality, addressing the limitations of existing methods. The framework leverages a lightweight chain-of-thought enhancement mechanism where local models generate multiple reasoning paths, with a discriminator selecting the most promising one. FedCoT also utilizes an improved aggregation approach with client classifier-awareness to achieve noise-free aggregation across diverse clients. Experimental results demonstrate that FedCoT significantly enhances client-side reasoning performance under resource constraints while safeguarding data privacy. <div>
arXiv:2508.10020v1 Announce Type: new 
Abstract: Efficiently enhancing the reasoning capabilities of large language models (LLMs) in federated learning environments remains challenging, particularly when balancing performance gains with strict computational, communication, and privacy constraints. This challenge is especially acute in healthcare, where decisions-spanning clinical, operational, and patient-facing contexts-demand not only accurate outputs but also interpretable, traceable rationales to ensure safety, accountability, and regulatory compliance. Conventional federated tuning approaches on LLM fail to address this need: they optimize primarily for answer correctness while neglecting rationale quality, leaving CoT capabilities dependent on models' innate pre-training abilities. Moreover, existing methods for improving rationales typically rely on privacy-violating knowledge distillation from centralized models. Additionally, the communication overhead in traditional federated fine-tuning on LLMs remains substantial. We addresses this gap by proposing FedCoT, a novel framework specifically designed to enhance reasoning in federated settings. FedCoT leverages a lightweight chain-of-thought enhancement mechanism: local models generate multiple reasoning paths, and a compact discriminator dynamically selects the most promising one. This approach improves reasoning accuracy and robustness while providing valuable interpretability, which is particularly critical for medical applications. To manage client heterogeneity efficiently, we adopt an improved aggregation approach building upon advanced LoRA module stacking, incorporating client classifier-awareness to achieve noise-free aggregation across diverse clients. Comprehensive experiments on medical reasoning tasks demonstrate that FedCoT significantly boosts client-side reasoning performance under stringent resource budgets while fully preserving data privacy.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients</title>
<link>https://arxiv.org/abs/2508.10021</link>
<guid>https://arxiv.org/abs/2508.10021</guid>
<content:encoded><![CDATA[
<div> Contrastive learning, Event sequences, Financial applications, Large language models, LATTE <br />
<br />
Summary: 
The paper introduces LATTE, a contrastive learning framework for generating client embeddings from sequences of historical communications in financial applications. By aligning raw event embeddings with semantic embeddings from large language models (LLMs), LATTE effectively utilizes world knowledge while reducing computational costs. Using short prompts to summarize behavioral features, embedded by LLMs and leveraged via contrastive loss, LATTE outperforms existing techniques in learning event sequence representations from real-world financial datasets. The approach significantly decreases inference costs and input sizes compared to conventional LLM processing, making it suitable for deployment in latency-sensitive environments. <div>
arXiv:2508.10021v1 Announce Type: new 
Abstract: Learning clients embeddings from sequences of their historic communications is central to financial applications. While large language models (LLMs) offer general world knowledge, their direct use on long event sequences is computationally expensive and impractical in real-world pipelines. In this paper, we propose LATTE, a contrastive learning framework that aligns raw event embeddings with semantic embeddings from frozen LLMs. Behavioral features are summarized into short prompts, embedded by the LLM, and used as supervision via contrastive loss. The proposed approach significantly reduces inference cost and input size compared to conventional processing of complete sequence by LLM. We experimentally show that our method outperforms state-of-the-art techniques for learning event sequence representations on real-world financial datasets while remaining deployable in latency-sensitive environments.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control</title>
<link>https://arxiv.org/abs/2508.10022</link>
<guid>https://arxiv.org/abs/2508.10022</guid>
<content:encoded><![CDATA[
<div> significance testing, conformal prediction framework, large language models, multiple-choice question answering, trustworthiness

Summary:
The study introduces a significance testing-enhanced conformal prediction framework to improve trust in large language models (LLMs) for multiple-choice question answering. By integrating significance testing with CP, the framework addresses issues like hallucination and nonfactual generation in LLM responses. This is achieved through self-consistency resampling and null hypothesis testing, providing statistically rigorous prediction sets with empirically derived p-values. Evaluation on benchmarks shows that the enhanced CP framework achieves user-specified miscoverage rates and reduces prediction set size with increasing risk levels. The work establishes a principled statistical approach for deploying trustworthy LLMs in high-stakes QA applications. 

<br /><br />Summary: <div>
arXiv:2508.10022v1 Announce Type: new 
Abstract: This study introduces a significance testing-enhanced conformal prediction (CP) framework to improve trustworthiness of large language models (LLMs) in multiple-choice question answering (MCQA). While LLMs have been increasingly deployed in disciplinary QA scenarios, hallucination and nonfactual generation substantially compromise response reliability. Although CP provides statistically rigorous marginal coverage guarantees for prediction sets, and significance testing offers established statistical rigor, their synergistic integration remains unexplored. To mitigate hallucination and factual inaccuracies, our framework integrates $p$-value computation with conformity scoring through self-consistency resampling of MCQA responses. This approach calculates option frequencies to address LLMs' black-box nature, subsequently constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves user-specified empirical miscoverage rates; (2) Test-set average prediction set size (APSS) decreases monotonically with increasing risk levels ($\alpha$), validating APSS as an effective uncertainty metric. This work establishes a principled statistical framework for trustworthy LLM deployment in high-stakes QA applications.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RTTC: Reward-Guided Collaborative Test-Time Compute</title>
<link>https://arxiv.org/abs/2508.10024</link>
<guid>https://arxiv.org/abs/2508.10024</guid>
<content:encoded><![CDATA[
<div> Keywords: Test-Time Compute, Large Language Models, Reward-Guided, RTTC, Query-State Caching

Summary:
Test-Time Compute (TTC) is crucial for enhancing Large Language Models (LLMs) performance at inference, with strategies like Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG). The optimal TTC strategy varies per query, requiring adaptive selection to avoid unnecessary computational overhead. Reward-Guided Test-Time Compute (RTTC) introduces a novel framework that selects the most effective TTC strategy for each query using a pretrained reward model. Operating in a distributed server-client architecture, RTTC retrieves knowledge base samples and applies RAG or lightweight fine-tuning on client devices only when needed. It also uses Query-State Caching to efficiently reuse historical query states. Experimentation across multiple LLMs and benchmarks shows that RTTC consistently outperforms vanilla RAG or TTT, indicating the importance of adaptive TTC selection and the scalability of RTTC for high-performance language model adaptation.<br /><br />Summary: Test-Time Compute (TTC) is crucial for enhancing Large Language Models (LLMs) performance at inference. Reward-Guided Test-Time Compute (RTTC) adaptively selects the best TTC strategy for each query, reducing computational overhead. RTTC operates in a distributed architecture, utilizing Query-State Caching for efficient historical query reuse. Experimental results demonstrate RTTC's superior performance over vanilla strategies, proving its scalability and efficacy. <div>
arXiv:2508.10024v1 Announce Type: new 
Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the performance of Large Language Models (LLMs) at inference, leveraging strategies such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG). However, the optimal adaptation strategy varies across queries, and indiscriminate application of TTC strategy incurs substantial computational overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a novel framework that adaptively selects the most effective TTC strategy for each query via a pretrained reward model, maximizing downstream accuracy across diverse domains and tasks. RTTC operates in a distributed server-client architecture, retrieving relevant samples from a remote knowledge base and applying RAG or lightweight fine-tuning on client devices only when necessary. To further mitigate redundant computation, we propose Query-State Caching, which enables the efficient reuse of historical query states at both retrieval and adaptation levels. Extensive experiments across multiple LLMs and benchmarks demonstrate that RTTC consistently achieves superior accuracy compared to vanilla RAG or TTT, validating the necessity of adaptive, reward-guided TTC selection and the potential of RTTC for scalable, high-performance language model adaptation.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting and explaining postpartum depression in real-time with generative artificial intelligence</title>
<link>https://arxiv.org/abs/2508.10025</link>
<guid>https://arxiv.org/abs/2508.10025</guid>
<content:encoded><![CDATA[
<div> Keywords: postpartum depression, Natural Language Processing, Machine Learning, Large Language Models, real-time screening<br />
Summary:<br />
This article addresses the pressing issue of postpartum depression (PPD) in new mothers and the importance of rapid detection and intervention. By combining Natural Language Processing, Machine Learning, and Large Language Models, the study presents an intelligent PPD screening system that allows for affordable, real-time, and non-invasive free speech analysis. The system also aims to address the black box problem by providing explanations for predictions using interpretable ML models. With a detection accuracy of 90% across all evaluation metrics, the system outperforms existing solutions in the literature. By enabling quick and accurate detection of PPD and associated risk factors, the system provides crucial support for healthcare practitioners in making timely assessments and interventions. <div>
arXiv:2508.10025v1 Announce Type: new 
Abstract: Among the many challenges mothers undergo after childbirth, postpartum depression (PPD) is a severe condition that significantly impacts their mental and physical well-being. Consequently, the rapid detection of ppd and their associated risk factors is critical for in-time assessment and intervention through specialized prevention procedures. Accordingly, this work addresses the need to help practitioners make decisions with the latest technological advancements to enable real-time screening and treatment recommendations. Mainly, our work contributes to an intelligent PPD screening system that combines Natural Language Processing, Machine Learning (ML), and Large Language Models (LLMs) towards an affordable, real-time, and non-invasive free speech analysis. Moreover, it addresses the black box problem since the predictions are described to the end users thanks to the combination of LLMs with interpretable ml models (i.e., tree-based algorithms) using feature importance and natural language. The results obtained are 90 % on ppd detection for all evaluation metrics, outperforming the competing solutions in the literature. Ultimately, our solution contributes to the rapid detection of PPD and their associated risk factors, critical for in-time and proper assessment and intervention.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SABER: Switchable and Balanced Training for Efficient LLM Reasoning</title>
<link>https://arxiv.org/abs/2508.10026</link>
<guid>https://arxiv.org/abs/2508.10026</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, large language models, efficient reasoning, token budgeting, inference modes  
Summary:  
Reinforcement learning framework SABER introduces token-budgeted reasoning to large language models (LLMs), allowing for user-controlled reasoning depth. By profiling and assigning budget tiers to training examples, SABER guides model fine-tuning with length-aware rewards and system prompts. The inclusion of no-think examples ensures model reliability when reasoning is disabled. Supporting four inference modes, SABER enables flexible trade-offs between latency and reasoning depth. Evaluation on math, code generation, and logical reasoning tasks demonstrates SABER's high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization. Particularly, SABER-FastThink reduces reasoning length by 65.4% and achieves a 3.6% accuracy gain on the MATH benchmark compared to the base model.  
<br /><br />Summary: <div>
arXiv:2508.10026v1 Announce Type: new 
Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have achieved impressive accuracy on complex tasks but suffer from excessive inference costs and latency when applied uniformly to all problems. We propose SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a reinforcement learning framework that endows LLMs with user-controllable, token-budgeted reasoning. SABER first profiles each training example's base-model thinking token usage and assigns it to one of the predefined budget tiers. During fine-tuning, the model is guided by system prompts and length-aware rewards to respect its assigned budget. In parallel, we incorporate no-think examples to ensure the model remains reliable even when explicit reasoning is turned off. SABER further supports four discrete inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling flexible trade-offs between latency and reasoning depth. Extensive evaluations on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning (LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization. In particular, SABER-FastThink cuts reasoning length by 65.4% and yields a 3.6% accuracy gain compared with the base model on the MATH benchmark.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data</title>
<link>https://arxiv.org/abs/2508.10027</link>
<guid>https://arxiv.org/abs/2508.10027</guid>
<content:encoded><![CDATA[
<div> transformer embeddings, linguistic features, synthetic speech, multimodal models, ADRD detection

Summary:
- The study focused on utilizing speech-based natural language processing for early detection of Alzheimer's disease and related dementias (ADRD).
- A screening pipeline was developed that combined transformer embeddings with handcrafted linguistic features, showing improved ADRD detection performance.
- Data augmentation using synthetic speech generated by large language models (LLMs) enhanced the training process and increased detection accuracy.
- Fine-tuning LLMs resulted in significant performance improvements, with clinically tuned models effectively supporting classification and data augmentation.
- Multimodal models showed lower performance compared to unimodal LLM classifiers, highlighting the need for further advancements in multimodal modeling.<br /><br />Summary: <div>
arXiv:2508.10027v1 Announce Type: new 
Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five million older adults in the U.S., yet over half remain undiagnosed. Speech-based natural language processing (NLP) offers a promising, scalable approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer embeddings with handcrafted linguistic features, (ii) tests data augmentation using synthetic speech generated by large language models (LLMs), and (iii) benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used. Ten transformer models were evaluated under three fine-tuning strategies. A fusion model combined embeddings from the top-performing transformer with 110 lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B, Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic speech, which was used to augment training data. Three multimodal models (GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B synthetic speech increased F1 to 85.7. Fine-tuning significantly improved unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen = 66.0). Performance gains aligned with the distributional similarity between synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD detection from speech. Clinically tuned LLMs effectively support both classification and data augmentation, while further advancement is needed in multimodal modeling.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs</title>
<link>https://arxiv.org/abs/2508.10028</link>
<guid>https://arxiv.org/abs/2508.10028</guid>
<content:encoded><![CDATA[
<div> Keywords: Personalised text generation, Evaluation framework, Language model, User-specific alignment, Preference-following tasks <br />
Summary: <br />
The article introduces the PREF framework for evaluating personalised text generation systems. PREF focuses on measuring general output quality and user-specific alignment without needing gold personalised references. It operates in three steps: coverage, preference, and scoring, utilizing large language models to generate guidelines and personalized evaluation rubrics based on user profiles and preferences. PREF improves robustness, transparency, and reusability, allowing smaller models to approximate larger ones. Experiments on the PrefEval benchmark show that PREF outperforms strong baselines in accuracy, calibration, and alignment with human judgments. By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the groundwork for more reliable assessment and development of personalised language generation systems.<br /> 
Summary: <div>
arXiv:2508.10028v1 Announce Type: new 
Abstract: Personalised text generation is essential for user-centric information systems, yet most evaluation methods overlook the individuality of users. We introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free \textbf{E}valuation \textbf{F}ramework that jointly measures general output quality and user-specific alignment without requiring gold personalised references. PREF operates in a three-step pipeline: (1) a coverage stage uses a large language model (LLM) to generate a comprehensive, query-specific guideline covering universal criteria such as factuality, coherence, and completeness; (2) a preference stage re-ranks and selectively augments these factors using the target user's profile, stated or inferred preferences, and context, producing a personalised evaluation rubric; and (3) a scoring stage applies an LLM judge to rate candidate answers against this rubric, ensuring baseline adequacy while capturing subjective priorities. This separation of coverage from preference improves robustness, transparency, and reusability, and allows smaller models to approximate the personalised quality of larger ones. Experiments on the PrefEval benchmark, including implicit preference-following tasks, show that PREF achieves higher accuracy, better calibration, and closer alignment with human judgments than strong baselines. By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the groundwork for more reliable assessment and development of personalised language generation systems.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs</title>
<link>https://arxiv.org/abs/2508.10029</link>
<guid>https://arxiv.org/abs/2508.10029</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Latent Fusion Jailbreak, Adversarial training, Attack success rate, Hidden state interpolation

Summary: 
Large language models (LLMs) are powerful tools in various language tasks but are vulnerable to jailbreak attacks. The Latent Fusion Jailbreak (LFJ) technique introduces a representation-based attack that exploits hidden states from harmful and benign query pairs to elicit prohibited responses. LFJ selects query pairs with high thematic and syntactic similarity and performs gradient-guided interpolation at influential layers and tokens. Evaluation on models like Vicuna and LLaMA-2 demonstrates an average attack success rate (ASR) of 94.01%, surpassing existing methods. To counter LFJ, an adversarial training defense is proposed, fine-tuning models on interpolated examples to reduce ASR by over 80% without compromising performance on benign inputs. Ablation studies confirm the significance of query pair selection, hidden state interpolation components, and optimization strategies in enhancing LFJ's effectiveness.<br /><br />Summary: <div>
arXiv:2508.10029v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate impressive capabilities in various language tasks but are susceptible to jailbreak attacks that circumvent their safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a representation-based attack that interpolates hidden states from harmful and benign query pairs to elicit prohibited responses. LFJ begins by selecting query pairs with high thematic and syntactic similarity, then performs gradient-guided interpolation at influential layers and tokens, followed by optimization to balance attack success, output fluency, and computational efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks like AdvBench and MaliciousInstruct yield an average attack success rate (ASR) of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an adversarial training defense that fine-tunes models on interpolated examples, reducing ASR by over 80% without degrading performance on benign inputs. Ablation studies validate the importance of query pair selection, hidden state interpolation components, and optimization strategies in LFJ's effectiveness.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models</title>
<link>https://arxiv.org/abs/2508.10030</link>
<guid>https://arxiv.org/abs/2508.10030</guid>
<content:encoded><![CDATA[
<div> framework, Inference-Aware, Prompt Optimization, IAPO, PSST

Summary:
The article introduces a novel framework called IAPO (Inference-Aware Prompt Optimization) that optimizes prompts while considering the inference strategy used during deployment. It highlights the interdependence between prompt optimization and inference scaling strategies like Best-of-N Sampling and Majority Voting. User preferences regarding trade-offs and inference budgets significantly impact prompt and inference configuration choices. The IAPO framework includes a fixed-budget training algorithm called PSST (Prompt Scaling via Sequential Trimming) and provides finite-budget guarantees on error probability. The study evaluates PSST on various tasks, such as text generation and reasoning, demonstrating the importance of incorporating inference-awareness in aligning black-box large language models through prompt optimization.<br /><br />Summary: <div>
arXiv:2508.10030v1 Announce Type: new 
Abstract: Prompt optimization methods have demonstrated significant effectiveness in aligning black-box large language models (LLMs). In parallel, inference scaling strategies such as Best-of-N Sampling and Majority Voting have also proven to enhance alignment and performance by trading off computation. However, existing prompt optimization approaches are inference strategy agnostic; that is, they optimize prompts without regard to the inference strategy employed during deployment. This constitutes a significant methodological gap, as our empirical and theoretical analysis reveals a strong interdependence between these two paradigms. Moreover, we find that user preferences regarding trade-offs among multiple objectives and inference budgets substantially influence the choice of prompt and inference configuration. To address this gap, we introduce a unified novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly optimizes the prompt and inference scale, while being aware of the inference budget and different task objectives. We then develop a fixed-budget training algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential Trimming), and analyze finite-budget guarantees on error probability. Finally, we evaluate the effectiveness of PSST on six different tasks, including multi-objective text generation and reasoning, and demonstrate the critical role of incorporating inference-awareness when aligning black-box LLMs through prompt optimization.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Cost of Thinking: Increased Jailbreak Risk in Large Language Models</title>
<link>https://arxiv.org/abs/2508.10032</link>
<guid>https://arxiv.org/abs/2508.10032</guid>
<content:encoded><![CDATA[
<div> thinking mode, LLMs, Jailbreak attack, safe thinking intervention, attack success rate <br />
Summary: <br />
The study uncovers a surprising finding that LLMs with thinking mode are more vulnerable to Jailbreak attacks compared to those without. Evaluation on 9 LLMs shows higher success rates for attacking thinking mode. The research highlights that educational purposes and long thinking lengths make data more susceptible to attacks. LLMs also give harmful answers when prompted with harmful questions. To address these issues, a safe thinking intervention method is proposed, involving the addition of "specific thinking tokens" to guide LLMs' internal thinking processes. Results show that this intervention significantly reduces the attack success rate on LLMs with thinking mode. <div>
arXiv:2508.10032v1 Announce Type: new 
Abstract: Thinking mode has always been regarded as one of the most valuable modes in LLMs. However, we uncover a surprising and previously overlooked phenomenon: LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate 9 LLMs on AdvBench and HarmBench and find that the success rate of attacking thinking mode in LLMs is almost higher than that of non-thinking mode. Through large numbers of sample studies, it is found that for educational purposes and excessively long thinking lengths are the characteristics of successfully attacked data, and LLMs also give harmful answers when they mostly know that the questions are harmful. In order to alleviate the above problems, this paper proposes a method of safe thinking intervention for LLMs, which explicitly guides the internal thinking processes of LLMs by adding "specific thinking tokens" of LLMs to the prompt. The results demonstrate that the safe thinking intervention can significantly reduce the attack success rate of LLMs with thinking mode.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion</title>
<link>https://arxiv.org/abs/2508.10036</link>
<guid>https://arxiv.org/abs/2508.10036</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Information Extraction, Active Prompting, Few-shot learning, Model uncertainty

Summary: 
Active Prompting for Information Extraction (APIE) introduces a novel framework for guiding Large Language Models (LLMs) in few-shot information extraction tasks. The method addresses the sensitivity of LLMs to in-context examples by leveraging introspective confusion, which considers both Format Uncertainty (difficulty in generating syntax) and Content Uncertainty (inconsistency in semantics extraction). By ranking unlabeled data based on this dual-component uncertainty metric, APIE selects challenging samples to enhance LLM performance. Experimental results on four benchmarks demonstrate that APIE outperforms strong baselines, improving extraction accuracy and robustness. The study emphasizes the significance of a detailed, dual-level view of model uncertainty for developing effective structured generation systems. 

<br /><br />Summary: <div>
arXiv:2508.10036v1 Announce Type: new 
Abstract: Large Language Models (LLMs) show remarkable potential for few-shot information extraction (IE), yet their performance is highly sensitive to the choice of in-context examples. Conventional selection strategies often fail to provide informative guidance, as they overlook a key source of model fallibility: confusion stemming not just from semantic content, but also from the generation of well-structured formats required by IE tasks. To address this, we introduce Active Prompting for Information Extraction (APIE), a novel active prompting framework guided by a principle we term introspective confusion. Our method empowers an LLM to assess its own confusion through a dual-component uncertainty metric that uniquely quantifies both Format Uncertainty (difficulty in generating correct syntax) and Content Uncertainty (inconsistency in extracted semantics). By ranking unlabeled data with this comprehensive score, our framework actively selects the most challenging and informative samples to serve as few-shot exemplars. Extensive experiments on four benchmarks show that our approach consistently outperforms strong baselines, yielding significant improvements in both extraction accuracy and robustness. Our work highlights the critical importance of a fine-grained, dual-level view of model uncertainty when it comes to building effective and reliable structured generation systems.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning</title>
<link>https://arxiv.org/abs/2508.10137</link>
<guid>https://arxiv.org/abs/2508.10137</guid>
<content:encoded><![CDATA[
<div> skill-based commonsense reasoning, multilingual, reasoning skills, large language models, benchmark

Summary: 
The article presents a Multilingual and Scalable Benchmark for Skill-based Commonsense Reasoning (mSCoRe) to evaluate Large Language Models' (LLMs) reasoning capabilities. It includes a taxonomy of reasoning skills, a data synthesis pipeline for commonsense reasoning evaluation, and a complexity scaling framework. Testing eight state-of-the-art LLMs, the benchmark proves challenging for current models, especially at higher complexity levels. The study highlights limitations in multilingual general and cultural commonsense reasoning in LLMs, providing insights for future improvements in reasoning capabilities. Further analysis of models' reasoning processes suggests potential directions for enhancing multilingual commonsense reasoning abilities.<br /><br />Summary: <div>
arXiv:2508.10137v1 Announce Type: new 
Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have shown remarkable capabilities in complex reasoning tasks. However, the mechanism underlying their utilization of different human reasoning skills remains poorly investigated, especially for multilingual commonsense reasoning that involves everyday knowledge across different languages and cultures. To address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for \textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}). Our benchmark incorporates three key components that are designed to systematically evaluate LLM's reasoning capabilities, including: (1) a novel taxonomy of reasoning skills that enables fine-grained analysis of models' reasoning processes, (2) a robust data synthesis pipeline tailored specifically for commonsense reasoning evaluation, and (3) a complexity scaling framework allowing task difficulty to scale dynamically alongside future improvements in LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying sizes and training approaches demonstrate that \textbf{mSCoRe} remains significantly challenging for current models, particularly at higher complexity levels. Our results reveal the limitations of such reasoning-reinforced models when confronted with nuanced multilingual general and cultural commonsense. We further provide detailed analysis on the models' reasoning processes, suggesting future directions for improving multilingual commonsense reasoning capabilities.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs</title>
<link>https://arxiv.org/abs/2508.10142</link>
<guid>https://arxiv.org/abs/2508.10142</guid>
<content:encoded><![CDATA[
<div> Benchmark, large language models, multi-turn dialogue, reasoning abilities, information-seeking

Summary:<br />
Large language models (LLMs) are proficient in solving problems with clear statements but struggle in nuanced or interactive tasks common in real-world scenarios. A new benchmark has been introduced to assess LLMs' capabilities in engaging in logically consistent multi-turn dialogue, seeking information, and reasoning with incomplete data. The benchmark comprises tasks with deterministic scoring mechanisms, eliminating human intervention. Evaluation of current models on this benchmark shows significant room for improvement. Errors mainly stem from poor instruction following, reasoning failures, and inadequate planning. The benchmark offers insights into the strengths and weaknesses of existing LLMs in handling complex, interactive scenarios and serves as a foundation for future research to enhance these crucial capabilities.<br />Summary: <div>
arXiv:2508.10142v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at solving problems with clear and complete statements, but often struggle with nuanced environments or interactive tasks which are common in most real-world scenarios. This highlights the critical need for developing LLMs that can effectively engage in logically consistent multi-turn dialogue, seek information and reason with incomplete data. To this end, we introduce a novel benchmark comprising a suite of multi-turn tasks each designed to test specific reasoning, interactive dialogue, and information-seeking abilities. These tasks have deterministic scoring mechanisms, thus eliminating the need for human intervention. Evaluating frontier models on our benchmark reveals significant headroom. Our analysis shows that most errors emerge from poor instruction following, reasoning failures, and poor planning. This benchmark provides valuable insights into the strengths and weaknesses of current LLMs in handling complex, interactive scenarios and offers a robust platform for future research aimed at improving these critical capabilities.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaajMeter: A Framework for LaaJ Evaluation</title>
<link>https://arxiv.org/abs/2508.10161</link>
<guid>https://arxiv.org/abs/2508.10161</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, LaaJ, meta-evaluation, synthetic data, NLP

Summary: 
The article introduces LaaJMeter, a framework for meta-evaluation of Large Language Models (LLMs) in domain-specific contexts where annotated data is limited. LaaJMeter allows engineers to generate synthetic data to systematically analyze evaluation metrics and validate LLMs for specific tasks. It aids in determining the effectiveness of metrics in distinguishing between better and worse LLMs and setting appropriate thresholds for evaluator performance. The utility of LaaJMeter is demonstrated in a code translation task, highlighting the varying sensitivity of metrics to evaluator quality. The results underscore the importance of selecting metrics prudently and the limitations of common metrics. LaaJMeter offers a scalable solution for assessing LLMs in low-resource settings, contributing to ensuring reliable and reproducible evaluation in Natural Language Processing (NLP).

<br /><br />Summary: <div>
arXiv:2508.10161v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While effective in general domains, LaaJs pose significant challenges in domain-specific contexts, where annotated data is scarce and expert evaluation is costly. In such cases, meta-evaluation is often performed using metrics that have not been validated for the specific domain in which they are applied. As a result, it becomes difficult to determine which metrics effectively identify LaaJ quality, and further, what threshold indicates sufficient evaluator performance. In this work, we introduce LaaJMeter, a simulation-based framework for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to generate synthetic data representing virtual models and judges, allowing systematic analysis of evaluation metrics under realistic conditions. This helps practitioners validate and refine LaaJs for specific evaluation tasks: they can test whether their metrics correctly distinguish between better and worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving a legacy programming language, showing how different metrics vary in sensitivity to evaluator quality. Our results highlight the limitations of common metrics and the importance of principled metric selection. LaaJMeter provides a scalable and extensible solution for assessing LaaJs in low-resource settings, contributing to the broader effort to ensure trustworthy and reproducible evaluation in NLP.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Machine Translation Difficulty</title>
<link>https://arxiv.org/abs/2508.10175</link>
<guid>https://arxiv.org/abs/2508.10175</guid>
<content:encoded><![CDATA[
<div> Keywords: machine translation, difficulty estimation, quality evaluation, benchmark construction, Sentinel-src <br />
Summary: <br />
The article discusses the challenge of distinguishing between state-of-the-art machine translation models due to their high-quality outputs. It proposes a task of translation difficulty estimation to identify texts where machine translation systems struggle. A new metric is introduced to evaluate difficulty estimators, comparing baselines and novel approaches. Dedicated models, called Sentinel-src, outperform heuristic-based methods and LLM-as-a-judge approaches. The study demonstrates the practical utility of difficulty estimators in constructing more challenging machine translation benchmarks. Two improved models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, are released for scanning large text collections to select texts that challenge contemporary machine translation systems. <div>
arXiv:2508.10175v1 Announce Type: new 
Abstract: Machine translation quality has began achieving near-perfect translations in some setups. These high-quality outputs make it difficult to distinguish between state-of-the-art models and to identify areas for future improvement. Automatically identifying texts where machine translation systems struggle holds promise for developing more discriminative evaluations and guiding future research.
  We formalize the task of translation difficulty estimation, defining a text's difficulty based on the expected quality of its translations. We introduce a new metric to evaluate difficulty estimators and use it to assess both baselines and novel approaches. Finally, we demonstrate the practical utility of difficulty estimators by using them to construct more challenging machine translation benchmarks. Our results show that dedicated models (dubbed Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or syntactic complexity) and LLM-as-a-judge approaches. We release two improved models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which can be used to scan large collections of texts and select those most likely to challenge contemporary machine translation systems.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs</title>
<link>https://arxiv.org/abs/2508.10180</link>
<guid>https://arxiv.org/abs/2508.10180</guid>
<content:encoded><![CDATA[
<div> data valuation, language models, vision-language models, influence estimation, fine-tuning examples

Summary:
For-Value is a new forward-only data valuation framework designed to quantify the influence of individual training samples in large language models (LLMs) and vision-language models (VLMs). Unlike existing methods that rely on computationally intensive techniques, For-Value computes influence scores efficiently using a simple closed-form expression based on a single forward pass. The framework accurately estimates per-sample influence by considering alignment in hidden representations and prediction errors between training and validation samples. Experimental results demonstrate that For-Value is effective in identifying impactful fine-tuning examples and detecting mislabeled data, often matching or outperforming gradient-based techniques. Ultimately, For-Value enhances the transparency and accountability of billion-parameter models by providing a scalable and efficient method for quantifying the influence of individual training samples. <br /><br />Summary: <div>
arXiv:2508.10180v1 Announce Type: new 
Abstract: Quantifying the influence of individual training samples is essential for enhancing the transparency and accountability of large language models (LLMs) and vision-language models (VLMs). However, existing data valuation methods often rely on Hessian information or model retraining, making them computationally prohibitive for billion-parameter models. In this work, we introduce For-Value, a forward-only data valuation framework that enables scalable and efficient influence estimation for both LLMs and VLMs. By leveraging the rich representations of modern foundation models, For-Value computes influence scores using a simple closed-form expression based solely on a single forward pass, thereby eliminating the need for costly gradient computations. Our theoretical analysis demonstrates that For-Value accurately estimates per-sample influence by capturing alignment in hidden representations and prediction errors between training and validation samples. Extensive experiments show that For-Value matches or outperforms gradient-based baselines in identifying impactful fine-tuning examples and effectively detecting mislabeled data.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PakBBQ: A Culturally Adapted Bias Benchmark for QA</title>
<link>https://arxiv.org/abs/2508.10186</link>
<guid>https://arxiv.org/abs/2508.10186</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Fairness, Bias Benchmark, Question Answering, Pakistan

Summary: 
- PakBBQ is introduced as a culturally and regionally adapted extension of the Bias Benchmark for Question Answering dataset, focusing on eight bias dimensions relevant in Pakistan.
- The dataset comprises templates, QA pairs in English and Urdu, covering topics such as age, gender, appearance, and more.
- Evaluation of multilingual LLMs reveals a 12% accuracy gain with disambiguation and stronger counter bias behaviors in Urdu compared to English.
- Framing effects show reduced stereotypical responses when questions are negatively posed.
- Contextualized benchmarks and prompt engineering strategies are essential for bias mitigation in low resource settings.

<br /><br />Summary: <div>
arXiv:2508.10186v1 Announce Type: new 
Abstract: With the widespread adoption of Large Language Models (LLMs) across various applications, it is empirical to ensure their fairness across all user communities. However, most LLMs are trained and evaluated on Western centric data, with little attention paid to low-resource languages and regional contexts. To address this gap, we introduce PakBBQ, a culturally and regionally adapted extension of the original Bias Benchmark for Question Answering (BBQ) dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8 categories in both English and Urdu, covering eight bias dimensions including age, disability, appearance, gender, socio-economic status, religious, regional affiliation, and language formality that are relevant in Pakistan. We evaluate multiple multilingual LLMs under both ambiguous and explicitly disambiguated contexts, as well as negative versus non negative question framings. Our experiments reveal (i) an average accuracy gain of 12\% with disambiguation, (ii) consistently stronger counter bias behaviors in Urdu than in English, and (iii) marked framing effects that reduce stereotypical responses when questions are posed negatively. These findings highlight the importance of contextualized benchmarks and simple prompt engineering strategies for bias mitigation in low resource settings.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2508.10192</link>
<guid>https://arxiv.org/abs/2508.10192</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, hallucinations, Semantic Divergence Metrics, confabulations, Semantic Box

Summary:
The paper introduces Semantic Divergence Metrics (SDM) as a framework for detecting Faithfulness Hallucinations in Large Language Models (LLMs). These hallucinations are severe deviations in LLM responses from input contexts, such as confabulations that are arbitrary and semantically misaligned with user queries. SDM improves upon existing methods by being prompt-aware and measuring response consistency across multiple semantically equivalent paraphrases of the original prompt. The approach uses joint clustering on sentence embeddings to create a shared topic space for prompts and answers, enabling the computation of information-theoretic metrics to quantify semantic divergence. The practical score, $\mathcal{S}_H$, combines Jensen-Shannon divergence and Wasserstein distance to indicate Faithfulness hallucinations, with high scores suggesting errors. Additionally, the KL divergence KL(Answer $||$ Prompt) is identified as a powerful indicator of Semantic Exploration. These metrics are integrated into the Semantic Box framework for classifying LLM response types, including confident confabulations.<br /><br />Summary: <div>
arXiv:2508.10192v1 Announce Type: new 
Abstract: The proliferation of Large Language Models (LLMs) is challenged by hallucinations, critical failure modes where models generate non-factual, nonsensical or unfaithful text. This paper introduces Semantic Divergence Metrics (SDM), a novel lightweight framework for detecting Faithfulness Hallucinations -- events of severe deviations of LLMs responses from input contexts. We focus on a specific implementation of these LLM errors, {confabulations, defined as responses that are arbitrary and semantically misaligned with the user's query. Existing methods like Semantic Entropy test for arbitrariness by measuring the diversity of answers to a single, fixed prompt. Our SDM framework improves upon this by being more prompt-aware: we test for a deeper form of arbitrariness by measuring response consistency not only across multiple answers but also across multiple, semantically-equivalent paraphrases of the original prompt. Methodologically, our approach uses joint clustering on sentence embeddings to create a shared topic space for prompts and answers. A heatmap of topic co-occurances between prompts and responses can be viewed as a quantified two-dimensional visualization of the user-machine dialogue. We then compute a suite of information-theoretic metrics to measure the semantic divergence between prompts and responses. Our practical score, $\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein distance to quantify this divergence, with a high score indicating a Faithfulness hallucination. Furthermore, we identify the KL divergence KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic Exploration}, a key signal for distinguishing different generative behaviors. These metrics are further combined into the Semantic Box, a diagnostic framework for classifying LLM response types, including the dangerous, confident confabulation.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Textual Emotion Through Emoji Prediction</title>
<link>https://arxiv.org/abs/2508.10222</link>
<guid>https://arxiv.org/abs/2508.10222</guid>
<content:encoded><![CDATA[
<div> BERT, emoji prediction, deep learning, sentiment analysis, TweetEval

Summary:
- The project explores the use of four deep learning architectures for emoji prediction from short text sequences: feed-forward network, CNN, transformer, and BERT.
- Techniques such as focal loss and regularization are employed to address class imbalance in the TweetEval dataset.
- BERT demonstrates the highest overall performance in emoji prediction due to its pre-training advantage.
- The CNN architecture shows superior efficacy in predicting rare emoji classes.
- The research emphasizes the importance of architecture selection and hyperparameter tuning in sentiment-aware emoji prediction for enhanced human-computer interaction. <div>
arXiv:2508.10222v1 Announce Type: new 
Abstract: This project explores emoji prediction from short text sequences using four deep learning architectures: a feed-forward network, CNN, transformer, and BERT. Using the TweetEval dataset, we address class imbalance through focal loss and regularization techniques. Results show BERT achieves the highest overall performance due to its pre-training advantage, while CNN demonstrates superior efficacy on rare emoji classes. This research shows the importance of architecture selection and hyperparameter tuning for sentiment-aware emoji prediction, contributing to improved human-computer interaction.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia</title>
<link>https://arxiv.org/abs/2508.10226</link>
<guid>https://arxiv.org/abs/2508.10226</guid>
<content:encoded><![CDATA[
<div> Keywords: clinical high risk, Brief Psychiatric Rating Scale, large language models, prediction, schizophrenia

Summary:
Large language models (LLMs) are utilized to predict Brief Psychiatric Rating Scale (BPRS) scores in patients at clinical high risk for schizophrenia. Despite the interviews not structured for BPRS assessment, LLM predictions show high accuracy comparable to human raters. The study demonstrates LLMs' potential in improving and standardizing CHR patient assessments, including assessing BPRS in foreign languages and integrating longitudinal information effectively. This approach could enhance monitoring of symptoms in CHR patients and inform appropriate treatment strategies, bridging the gap between research tools and clinical practice. Overall, LLMs offer a promising tool for enhancing the assessment and management of individuals at risk for schizophrenia, with the potential for broad applicability and precision in diverse clinical settings.

<br /><br />Summary: <div>
arXiv:2508.10226v1 Announce Type: new 
Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close monitoring of their symptoms to inform appropriate treatments. The Brief Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for measuring symptoms in patients with schizophrenia and other psychotic disorders; however, it is not commonly used in clinical practice as it requires a lengthy structured interview. Here, we utilize large language models (LLMs) to predict BPRS scores from clinical interview transcripts in 409 CHR patients from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort. Despite the interviews not being specifically structured to measure the BPRS, the zero-shot performance of the LLM predictions compared to the true assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and intra-rater reliability. We further demonstrate that LLMs have substantial potential to improve and standardize the assessment of CHR patients via their accuracy in assessing the BPRS in foreign languages (median concordance: 0.88, ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot learning approach.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona</title>
<link>https://arxiv.org/abs/2508.10246</link>
<guid>https://arxiv.org/abs/2508.10246</guid>
<content:encoded><![CDATA[
<div> Keywords: Toki Pona, language change, variation, computational approach, corpus-based analysis

Summary: 
Toki Pona, a constructed language with a limited vocabulary, is studied using a computational and corpus-based approach. The research focuses on examining changes in word preferences and usage patterns over time. The study investigates fluid word classes and transitivity to analyze how content words are used in different syntactic positions. The results indicate that sociolinguistic factors play a role in shaping Toki Pona, similar to natural languages. The study also suggests that constructed languages, like natural languages, evolve as communities actively use them. Overall, the research sheds light on the dynamics of language change and variation in Toki Pona, providing insights into the linguistic evolution of constructed language systems. 

<br /><br />Summary: <div>
arXiv:2508.10246v1 Announce Type: new 
Abstract: This study explores language change and variation in Toki Pona, a constructed language with approximately 120 core words. Taking a computational and corpus-based approach, the study examines features including fluid word classes and transitivity in order to examine (1) changes in preferences of content words for different syntactic positions over time and (2) variation in usage across different corpora. The results suggest that sociolinguistic factors influence Toki Pona in the same way as natural languages, and that even constructed linguistic systems naturally evolve as communities use them.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inductive Bias Extraction and Matching for LLM Prompts</title>
<link>https://arxiv.org/abs/2508.10295</link>
<guid>https://arxiv.org/abs/2508.10295</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt engineering, inductive bias, LLM, Likert ratings, classification, ranking 

Summary: 
The article discusses the importance of prompt engineering in improving the performance of large language models (LLMs). It highlights the sensitivity of LLMs to small changes in prompt wording and suggests that utilizing an LLM's output as part of its prompt can help create more effective prompts that align with the model's inductive bias. The proposed Inductive Bias Extraction and Matching strategy significantly enhances LLM Likert ratings for both classification and ranking tasks, with improvements of up to 19% and 27%, respectively. This approach leverages the innate biases present in LLMs to optimize prompt formulation and enhance the model's overall performance. <div>
arXiv:2508.10295v1 Announce Type: new 
Abstract: The active research topic of prompt engineering makes it evident that LLMs are sensitive to small changes in prompt wording. A portion of this can be ascribed to the inductive bias that is present in the LLM. By using an LLM's output as a portion of its prompt, we can more easily create satisfactory wording for prompts. This has the effect of creating a prompt that matches the inductive bias in model. Empirically, we show that using this Inductive Bias Extraction and Matching strategy improves LLM Likert ratings used for classification by up to 19% and LLM Likert ratings used for ranking by up to 27%.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race</title>
<link>https://arxiv.org/abs/2508.10304</link>
<guid>https://arxiv.org/abs/2508.10304</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, biases, qualitative methods, ethical considerations

Summary:
The study examines biases in Large Language Models (LLMs) through a qualitative, discursive framework, alongside quantitative methods. Black women in LLM-generated stories are depicted through ancestral ties and resistance, while white women are portrayed in self-discovery narratives, reflecting biases. The analysis reveals how LLM outputs replicate and reinforce discriminatory discourses, perpetuating inequalities. When prompted to correct biases, models make superficial revisions that maintain problematic meanings, highlighting limitations in fostering inclusive narratives. The study emphasizes the ideological functioning of algorithms and the implications for the ethical use and development of AI. It underscores the necessity of critical, interdisciplinary approaches in AI design to address and mitigate biases in LLM-generated discourses. 

<br /><br />Summary: <div>
arXiv:2508.10304v1 Announce Type: new 
Abstract: With the advance of Artificial Intelligence (AI), Large Language Models (LLMs) have gained prominence and been applied in diverse contexts. As they evolve into more sophisticated versions, it is essential to assess whether they reproduce biases, such as discrimination and racialization, while maintaining hegemonic discourses. Current bias detection approaches rely mostly on quantitative, automated methods, which often overlook the nuanced ways in which biases emerge in natural language. This study proposes a qualitative, discursive framework to complement such methods. Through manual analysis of LLM-generated short stories featuring Black and white women, we investigate gender and racial biases. We contend that qualitative methods such as the one proposed here are fundamental to help both developers and users identify the precise ways in which biases manifest in LLM outputs, thus enabling better conditions to mitigate them. Results show that Black women are portrayed as tied to ancestry and resistance, while white women appear in self-discovery processes. These patterns reflect how language models replicate crystalized discursive representations, reinforcing essentialization and a sense of social immobility. When prompted to correct biases, models offered superficial revisions that maintained problematic meanings, revealing limitations in fostering inclusive narratives. Our results demonstrate the ideological functioning of algorithms and have significant implications for the ethical use and development of AI. The study reinforces the need for critical, interdisciplinary approaches to AI design and deployment, addressing how LLM-generated discourses reflect and perpetuate inequalities.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReviewRL: Towards Automated Scientific Review with RL</title>
<link>https://arxiv.org/abs/2508.10308</link>
<guid>https://arxiv.org/abs/2508.10308</guid>
<content:encoded><![CDATA[
<div> Keywords: Peer review, scientific progress, automated review, reinforcement learning, review quality

Summary:
Peer review plays a crucial role in scientific progress but is facing challenges due to increasing submission volumes and reviewer fatigue. Existing automated review approaches often lack factual accuracy, consistency, and analytical depth, resulting in generic feedback. In response, the ReviewRL framework is introduced, using reinforcement learning to generate comprehensive and factually grounded scientific paper reviews. The approach includes an ArXiv-MCP retrieval-augmented context generation pipeline, supervised fine-tuning, and a reinforcement learning procedure with a composite reward function. Experiments on ICLR 2025 papers show that ReviewRL outperforms existing methods in both rule-based metrics and model-based quality assessments. This framework paves the way for future development in automatic critique generation in scientific discovery. The ReviewRL implementation will be available on GitHub.<br /><br />Summary: Peer review faces challenges due to increasing volume and reviewer fatigue, leading to the development of ReviewRL using reinforcement learning. ReviewRL significantly outperforms existing methods, enhancing review quality and accuracy, setting a foundation for automatic critique generation in scientific discovery. The framework showcases promising potential for future development in this field. <div>
arXiv:2508.10308v1 Announce Type: new 
Abstract: Peer review is essential for scientific progress but faces growing challenges due to increasing submission volumes and reviewer fatigue. Existing automated review approaches struggle with factual accuracy, rating consistency, and analytical depth, often generating superficial or generic feedback lacking the insights characteristic of high-quality human reviews. We introduce ReviewRL, a reinforcement learning framework for generating comprehensive and factually grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP retrieval-augmented context generation pipeline that incorporates relevant scientific literature, (2) supervised fine-tuning that establishes foundational reviewing capabilities, and (3) a reinforcement learning procedure with a composite reward function that jointly enhances review quality and rating accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL significantly outperforms existing methods across both rule-based metrics and model-based quality assessments. ReviewRL establishes a foundational framework for RL-driven automatic critique generation in scientific discovery, demonstrating promising potential for future development in this domain. The implementation of ReviewRL will be released at GitHub.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis</title>
<link>https://arxiv.org/abs/2508.10311</link>
<guid>https://arxiv.org/abs/2508.10311</guid>
<content:encoded><![CDATA[
<div> table-centric, semantic parsing, deep learning, document analysis, context association

Summary:
DOTABLER is a framework for semantic document parsing that focuses on deep semantic links between tables and their context. It goes beyond traditional tasks like layout analysis and data extraction by uncovering the relationships between tables and surrounding text. The framework uses a custom dataset and domain-specific fine-tuning of pre-trained models for accurate analysis. DOTABLER's core functionalities include table-centric document structure parsing and domain-specific table retrieval, enabling comprehensive semantic analysis and precise table extraction. In evaluations on real-world PDFs with over 1,000 tables, DOTABLER achieved high Precision and F1 scores, outperforming advanced models like GPT-4o. This demonstrates its superior performance in table-context semantic analysis and deep document parsing. 

<br /><br />Summary: <div>
arXiv:2508.10311v1 Announce Type: new 
Abstract: Documents are core carriers of information and knowl-edge, with broad applications in finance, healthcare, and scientific research. Tables, as the main medium for structured data, encapsulate key information and are among the most critical document components. Existing studies largely focus on surface-level tasks such as layout analysis, table detection, and data extraction, lacking deep semantic parsing of tables and their contextual associations. This limits advanced tasks like cross-paragraph data interpretation and context-consistent analysis. To address this, we propose DOTABLER, a table-centric semantic document parsing framework designed to uncover deep semantic links between tables and their context. DOTABLER leverages a custom dataset and domain-specific fine-tuning of pre-trained models, integrating a complete parsing pipeline to identify context segments semantically tied to tables. Built on this semantic understanding, DOTABLER implements two core functionalities: table-centric document structure parsing and domain-specific table retrieval, delivering comprehensive table-anchored semantic analysis and precise extraction of semantically relevant tables. Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs, DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior performance in table-context semantic analysis and deep document parsing compared to advanced models such as GPT-4o.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation</title>
<link>https://arxiv.org/abs/2508.10312</link>
<guid>https://arxiv.org/abs/2508.10312</guid>
<content:encoded><![CDATA[
<div> Keywords: Recommender systems, Large Language Models, collaborative signals, item embeddings, spectral perspective <br />
Summary: 
FreLLM4Rec aims to address the issue of collaborative signal attenuation in Large Language Model (LLM)-based recommenders by introducing a novel approach that balances semantic and collaborative information. This approach involves purifying item embeddings using a Global Graph Low-Pass Filter (G-LPF) to remove irrelevant noise and preserving collaborative signals through Temporal Frequency Modulation (TFM) layer by layer. TFM utilizes frequency-domain filters to maintain collaborative information, ensuring optimal performance in recommendation tasks. Experimental results on benchmark datasets show that FreLLM4Rec mitigates collaborative signal attenuation and improves NDCG@10 metric by up to 8.00% compared to the best baseline. This study sheds light on how LLMs process collaborative information and provides a principled solution for enhancing LLM-based recommendation systems. <br /><br />Summary: <div>
arXiv:2508.10312v1 Announce Type: new 
Abstract: Recommender systems in concert with Large Language Models (LLMs) present promising avenues for generating semantically-informed recommendations. However, LLM-based recommenders exhibit a tendency to overemphasize semantic correlations within users' interaction history. When taking pretrained collaborative ID embeddings as input, LLM-based recommenders progressively weaken the inherent collaborative signals as the embeddings propagate through LLM backbones layer by layer, as opposed to traditional Transformer-based sequential models in which collaborative signals are typically preserved or even enhanced for state-of-the-art performance. To address this limitation, we introduce FreLLM4Rec, an approach designed to balance semantic and collaborative information from a spectral perspective. Item embeddings that incorporate both semantic and collaborative information are first purified using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant high-frequency noise. Temporal Frequency Modulation (TFM) then actively preserves collaborative signal layer by layer. Note that the collaborative preservation capability of TFM is theoretically guaranteed by establishing a connection between the optimal but hard-to-implement local graph fourier filters and the suboptimal yet computationally efficient frequency-domain filters. Extensive experiments on four benchmark datasets demonstrate that FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves competitive performance, with improvements of up to 8.00\% in NDCG@10 over the best baseline. Our findings provide insights into how LLMs process collaborative information and offer a principled approach for improving LLM-based recommendation systems.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Prompt Encoder for Low-Performing Languages</title>
<link>https://arxiv.org/abs/2508.10352</link>
<guid>https://arxiv.org/abs/2508.10352</guid>
<content:encoded><![CDATA[
<div> encoder, soft prompts, parameter-efficient fine-tuning, multilingual, transferability
<br />
Summary: 
The paper introduces the Cross-Prompt Encoder (XPE) as a method to improve performance on low-performing languages in large language models (LLMs). XPE combines a lightweight encoding architecture with multi-source training on diverse languages to capture transferable patterns. Additionally, a Dual Soft Prompt mechanism is proposed, combining an encoder-based prompt with a standard soft prompt. Experiments on the SIB-200 benchmark show that XPE is most effective for low-performing languages, while hybrid variants offer broader adaptability in multilingual settings. This approach demonstrates the potential of soft prompts in parameter-efficient fine-tuning for adapting LLMs to downstream tasks across different languages. 
<br /> <div>
arXiv:2508.10352v1 Announce Type: new 
Abstract: Soft prompts have emerged as a powerful alternative to adapters in parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs) to adapt to downstream tasks without architectural changes or parameter updates. While prior work has focused on stabilizing training via parameter interaction in small neural prompt encoders, their broader potential for transfer across languages remains unexplored. In this paper, we demonstrate that a prompt encoder can play a central role in improving performance on low-performing languages-those that achieve poor accuracy even under full-model fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a lightweight encoding architecture with multi-source training on typologically diverse languages - a design that enables the model to capture abstract and transferable patterns across languages. To complement XPE, we propose a Dual Soft Prompt mechanism that combines an encoder-based prompt with a directly trained standard soft prompt. This hybrid design proves especially effective for target languages that benefit from both broadly shared structure and language-specific alignment. Experiments on the SIB-200 benchmark reveal a consistent trade-off: XPE is most effective for low-performing languages, while hybrid variants offer broader adaptability across multilingual settings.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Making Qwen3 Think in Korean with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.10355</link>
<guid>https://arxiv.org/abs/2508.10355</guid>
<content:encoded><![CDATA[
arXiv:2508.10355v1 Announce Type: new 
Abstract: We present a two-stage fine-tuning approach to make the large language model Qwen3 14B "think" natively in Korean. In the first stage, supervised fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a strong foundation in Korean logical reasoning, yielding notable improvements in Korean-language tasks and even some gains in general reasoning ability. In the second stage, we employ reinforcement learning with a customized Group Relative Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning alignment and overall problem-solving performance. We address critical stability challenges in GRPO training - such as reward hacking and policy collapse - by introducing an oracle judge model that calibrates the reward signal. Our approach achieves stable learning (avoiding the collapse observed in naive GRPO) and leads to steady, incremental performance gains. The final RL-tuned model demonstrates substantially improved results on advanced reasoning benchmarks (particularly math and coding tasks) while maintaining knowledge and language proficiency, successfully conducting its internal chain-of-thought entirely in Korean.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models</title>
<link>https://arxiv.org/abs/2508.10366</link>
<guid>https://arxiv.org/abs/2508.10366</guid>
<content:encoded><![CDATA[
arXiv:2508.10366v1 Announce Type: new 
Abstract: Aspect-based sentiment analysis (ABSA) has made significant strides, yet challenges remain for low-resource languages due to the predominant focus on English. Current cross-lingual ABSA studies often centre on simpler tasks and rely heavily on external translation tools. In this paper, we present a novel sequence-to-sequence method for compound ABSA tasks that eliminates the need for such tools. Our approach, which uses constrained decoding, improves cross-lingual ABSA performance by up to 10\%. This method broadens the scope of cross-lingual ABSA, enabling it to handle more complex tasks and providing a practical, efficient alternative to translation-dependent techniques. Furthermore, we compare our approach with large language models (LLMs) and show that while fine-tuned multilingual LLMs can achieve comparable results, English-centric LLMs struggle with these tasks.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Summarizing Czech Historical Documents and Beyond</title>
<link>https://arxiv.org/abs/2508.10368</link>
<guid>https://arxiv.org/abs/2508.10368</guid>
<content:encoded><![CDATA[
arXiv:2508.10368v1 Announce Type: new 
Abstract: Text summarization is the task of shortening a larger body of text into a concise version while retaining its essential meaning and key information. While summarization has been significantly explored in English and other high-resource languages, Czech text summarization, particularly for historical documents, remains underexplored due to linguistic complexities and a scarcity of annotated datasets. Large language models such as Mistral and mT5 have demonstrated excellent results on many natural language processing tasks and languages. Therefore, we employ these models for Czech summarization, resulting in two key contributions: (1) achieving new state-of-the-art results on the modern Czech summarization dataset SumeCzech using these advanced models, and (2) introducing a novel dataset called Posel od \v{C}erchova for summarization of historical Czech documents with baseline results. Together, these contributions provide a great potential for advancing Czech text summarization and open new avenues for research in Czech historical text processing.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding</title>
<link>https://arxiv.org/abs/2508.10369</link>
<guid>https://arxiv.org/abs/2508.10369</guid>
<content:encoded><![CDATA[
arXiv:2508.10369v1 Announce Type: new 
Abstract: While aspect-based sentiment analysis (ABSA) has made substantial progress, challenges remain for low-resource languages, which are often overlooked in favour of English. Current cross-lingual ABSA approaches focus on limited, less complex tasks and often rely on external translation tools. This paper introduces a novel approach using constrained decoding with sequence-to-sequence models, eliminating the need for unreliable translation tools and improving cross-lingual performance by 5\% on average for the most complex task. The proposed method also supports multi-tasking, which enables solving multiple ABSA tasks with a single model, with constrained decoding boosting results by more than 10\%.
  We evaluate our approach across seven languages and six ABSA tasks, surpassing state-of-the-art methods and setting new benchmarks for previously unexplored tasks. Additionally, we assess large language models (LLMs) in zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in zero-shot and few-shot settings, fine-tuning achieves competitive results compared to smaller multilingual models, albeit at the cost of longer training and inference times.
  We provide practical recommendations for real-world applications, enhancing the understanding of cross-lingual ABSA methodologies. This study offers valuable insights into the strengths and limitations of cross-lingual ABSA approaches, advancing the state-of-the-art in this challenging research domain.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts</title>
<link>https://arxiv.org/abs/2508.10390</link>
<guid>https://arxiv.org/abs/2508.10390</guid>
<content:encoded><![CDATA[
arXiv:2508.10390v1 Announce Type: new 
Abstract: Evaluating jailbreak attacks is challenging when prompts are not overtly harmful or fail to induce harmful outputs. Unfortunately, many existing red-teaming datasets contain such unsuitable prompts. To evaluate attacks accurately, these datasets need to be assessed and cleaned for maliciousness. However, existing malicious content detection methods rely on either manual annotation, which is labor-intensive, or large language models (LLMs), which have inconsistent accuracy in harmful types. To balance accuracy and efficiency, we propose a hybrid evaluation framework named MDH (Malicious content Detection based on LLMs with Human assistance) that combines LLM-based annotation with minimal human oversight, and apply it to dataset cleaning and detection of jailbroken responses. Furthermore, we find that well-crafted developer messages can significantly boost jailbreak success, leading us to propose two new strategies: D-Attack, which leverages context simulation, and DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets, judgements, and detection results will be released in github repository: https://github.com/AlienZhang1996/DH-CoT.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation</title>
<link>https://arxiv.org/abs/2508.10404</link>
<guid>https://arxiv.org/abs/2508.10404</guid>
<content:encoded><![CDATA[
arXiv:2508.10404v1 Announce Type: new 
Abstract: With the rapid proliferation of Natural Language Processing (NLP), especially Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs remains a key challenge for understanding model vulnerabilities and improving robustness. In this context, we propose a new black-box attack method that leverages the interpretability of large models. We introduce the Sparse Feature Perturbation Framework (SFPF), a novel approach for adversarial text generation that utilizes sparse autoencoders to identify and manipulate critical features in text. After using the SAE model to reconstruct hidden layer representations, we perform feature clustering on the successfully attacked texts to identify features with higher activations. These highly activated features are then perturbed to generate new adversarial texts. This selective perturbation preserves the malicious intent while amplifying safety signals, thereby increasing their potential to evade existing defenses. Our method enables a new red-teaming strategy that balances adversarial effectiveness with safety alignment. Experimental results demonstrate that adversarial texts generated by SFPF can bypass state-of-the-art defense mechanisms, revealing persistent vulnerabilities in current NLP systems.However, the method's effectiveness varies across prompts and layers, and its generalizability to other architectures and larger models remains to be validated.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning</title>
<link>https://arxiv.org/abs/2508.10419</link>
<guid>https://arxiv.org/abs/2508.10419</guid>
<content:encoded><![CDATA[
arXiv:2508.10419v1 Announce Type: new 
Abstract: Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods can fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition when reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global comprehension, offering a principled, cognitively motivated paradigm for retrieval-based long context comprehension towards stateful reasoning. Our code is publicly released at https://github.com/EternityJune25/ComoRAG
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs on Chinese Idiom Translation</title>
<link>https://arxiv.org/abs/2508.10421</link>
<guid>https://arxiv.org/abs/2508.10421</guid>
<content:encoded><![CDATA[
arXiv:2508.10421v1 Announce Type: new 
Abstract: Idioms, whose figurative meanings usually differ from their literal interpretations, are common in everyday language, especially in Chinese, where they often contain historical references and follow specific structural patterns. Despite recent progress in machine translation with large language models, little is known about Chinese idiom translation. In this work, we introduce IdiomEval, a framework with a comprehensive error taxonomy for Chinese idiom translation. We annotate 900 translation pairs from nine modern systems, including GPT-4o and Google Translate, across four domains: web, news, Wikipedia, and social media. We find these systems fail at idiom translation, producing incorrect, literal, partial, or even missing translations. The best-performing system, GPT-4, makes errors in 28% of cases. We also find that existing evaluation metrics measure idiom quality poorly with Pearson correlation below 0.48 with human ratings. We thus develop improved models that achieve F$_1$ scores of 0.68 for detecting idiom translation errors.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints</title>
<link>https://arxiv.org/abs/2508.10426</link>
<guid>https://arxiv.org/abs/2508.10426</guid>
<content:encoded><![CDATA[
arXiv:2508.10426v1 Announce Type: new 
Abstract: Large language models (LLMs) are limited by substantial computational cost. We introduce a "computational economics" framework that treats an LLM as an internal economy of resource-constrained agents (attention heads and neuron blocks) that must allocate scarce computation to maximize task utility. First, we show empirically that when computation is scarce, standard LLMs reallocate attention toward high-value tokens while preserving accuracy. Building on this observation, we propose an incentive-driven training paradigm that augments the task loss with a differentiable computation cost term, encouraging sparse and efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method yields a family of models that trace a Pareto frontier and consistently dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty percent reduction in FLOPS and lower latency, together with more interpretable attention patterns. These results indicate that economic principles offer a principled route to designing efficient, adaptive, and more transparent LLMs under strict resource constraints.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales</title>
<link>https://arxiv.org/abs/2508.10444</link>
<guid>https://arxiv.org/abs/2508.10444</guid>
<content:encoded><![CDATA[
arXiv:2508.10444v1 Announce Type: new 
Abstract: Generating textual rationales from large vision-language models (LVLMs) to support trainable multimodal misinformation detectors has emerged as a promising paradigm. However, its effectiveness is fundamentally limited by three core challenges: (i) insufficient diversity in generated rationales, (ii) factual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting content that introduces noise. We introduce DiFaR, a detector-agnostic framework that produces diverse, factual, and relevant rationales to enhance misinformation detection. DiFaR employs five chain-of-thought prompts to elicit varied reasoning traces from LVLMs and incorporates a lightweight post-hoc filtering module to select rationale sentences based on sentence-level factuality and relevance scores. Extensive experiments on four popular benchmarks demonstrate that DiFaR outperforms four baseline categories by up to 5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics and human evaluations confirm that DiFaR significantly improves rationale quality across all three dimensions.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing</title>
<link>https://arxiv.org/abs/2508.10482</link>
<guid>https://arxiv.org/abs/2508.10482</guid>
<content:encoded><![CDATA[
arXiv:2508.10482v1 Announce Type: new 
Abstract: In the study of trustworthy Natural Language Processing (NLP), a number of important research fields have emerged, including that of \textit{explainability} and \textit{privacy}. While research interest in both explainable and privacy-preserving NLP has increased considerably in recent years, there remains a lack of investigation at the intersection of the two. This leaves a considerable gap in understanding of whether achieving \textit{both} explainability and privacy is possible, or whether the two are at odds with each other. In this work, we conduct an empirical investigation into the privacy-explainability trade-off in the context of NLP, guided by the popular overarching methods of \textit{Differential Privacy} (DP) and Post-hoc Explainability. Our findings include a view into the intricate relationship between privacy and explainability, which is formed by a number of factors, including the nature of the downstream task and choice of the text privatization and explainability method. In this, we highlight the potential for privacy and explainability to co-exist, and we summarize our findings in a collection of practical recommendations for future work at this important intersection.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2508.10552</link>
<guid>https://arxiv.org/abs/2508.10552</guid>
<content:encoded><![CDATA[
arXiv:2508.10552v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a diverse range of multimodal tasks. However, these models suffer from a core problem known as text dominance: they depend heavily on text for their inference, while underutilizing other modalities. While prior work has acknowledged this phenomenon in vision-language tasks, often attributing it to data biases or model architectures. In this paper, we conduct the first systematic investigation of text dominance across diverse data modalities, including images, videos, audio, time-series, and graphs. To measure this imbalance, we propose two evaluation metrics: the Modality Dominance Index (MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis reveals that text dominance is both significant and pervasive across all tested modalities. Our in-depth analysis identifies three underlying causes: attention dilution from severe token redundancy in non-textual modalities, the influence of fusion architecture design, and task formulations that implicitly favor textual inputs. Furthermore, we propose a simple token compression method that effectively rebalances model attention. Applying this method to LLaVA-7B, for instance, drastically reduces its MDI from 10.23 to a well-balanced value of 0.86. Our analysis and methodological framework offer a foundation for the development of more equitable and comprehensive multimodal language models.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM</title>
<link>https://arxiv.org/abs/2508.10553</link>
<guid>https://arxiv.org/abs/2508.10553</guid>
<content:encoded><![CDATA[
arXiv:2508.10553v1 Announce Type: new 
Abstract: This paper presents a feasibility study on the deployment of a European Deep Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support mechanistic interpretability research on large language models. The need for widespread accessibility of LLM interpretability infrastructure in Europe drives this initiative to democratize advanced model analysis capabilities for the research community. The project introduces a GPU-based cluster hosted at Ansbach University of Applied Sciences and interconnected with partner institutions, enabling remote model inspection via the NNsight API. A structured pilot study involving 16 researchers from across Europe evaluated the platform's technical performance, usability, and scientific utility. Users conducted interventions such as activation patching, causal tracing, and representation analysis on models including GPT-2 and DeepSeek-R1-70B. The study revealed a gradual increase in user engagement, stable platform performance throughout, and a positive reception of the remote experimentation capabilities. It also marked the starting point for building a user community around the platform. Identified limitations such as prolonged download durations for activation data as well as intermittent execution interruptions are addressed in the roadmap for future development. This initiative marks a significant step towards widespread accessibility of LLM interpretability infrastructure in Europe and lays the groundwork for broader deployment, expanded tooling, and sustained community collaboration in mechanistic interpretability research.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages</title>
<link>https://arxiv.org/abs/2508.10683</link>
<guid>https://arxiv.org/abs/2508.10683</guid>
<content:encoded><![CDATA[
arXiv:2508.10683v1 Announce Type: new 
Abstract: This paper presents the first systematic study of strategies for translating Coptic into French. Our comprehensive pipeline systematically evaluates: pivot versus direct translation, the impact of pre-training, the benefits of multi-version fine-tuning, and model robustness to noise. Utilizing aligned biblical corpora, we demonstrate that fine-tuning with a stylistically-varied and noise-aware training corpus significantly enhances translation quality. Our findings provide crucial practical insights for developing translation tools for historical languages in general.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph</title>
<link>https://arxiv.org/abs/2508.10687</link>
<guid>https://arxiv.org/abs/2508.10687</guid>
<content:encoded><![CDATA[
arXiv:2508.10687v1 Announce Type: new 
Abstract: Millions of individuals worldwide are affected by deafness and hearing impairment. Sign language serves as a sophisticated means of communication for the deaf and hard of hearing. However, in societies that prioritize spoken languages, sign language often faces underestimation, leading to communication barriers and social exclusion. The Continuous Bangla Sign Language Translation project aims to address this gap by enhancing translation methods. While recent approaches leverage transformer architecture for state-of-the-art results, our method integrates graph-based methods with the transformer architecture. This fusion, combining transformer and STGCN-LSTM architectures, proves more effective in gloss-free translation. Our contributions include architectural fusion, exploring various fusion strategies, and achieving a new state-of-the-art performance on diverse sign language datasets, namely RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach demonstrates superior performance compared to current translation outcomes across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01, 2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a benchmark for future research, emphasizing the importance of gloss-free translation to improve communication accessibility for the deaf and hard of hearing.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Natural Language Feedback for Personalized Question Answering</title>
<link>https://arxiv.org/abs/2508.10695</link>
<guid>https://arxiv.org/abs/2508.10695</guid>
<content:encoded><![CDATA[
arXiv:2508.10695v1 Announce Type: new 
Abstract: Personalization is crucial for enhancing both the effectiveness and user satisfaction of language technologies, particularly in information-seeking tasks like question answering. Current approaches for personalizing large language models (LLMs) often rely on retrieval-augmented generation (RAG), followed by reinforcement learning with scalar reward signals to teach models how to use retrieved personal context. We believe that these scalar rewards sometimes provide weak, non-instructive feedback, limiting learning efficiency and personalization quality. We introduce VAC, a novel framework for personalized response generation that replaces scalar rewards with natural language feedback (NLF) that are generated conditioned on the user profiles and the question narratives. NLF serves as a rich and actionable supervision signal, allowing the policy model to iteratively refine its outputs and internalize effective personalization strategies. Training alternates between optimizing the feedback model and fine-tuning the policy model on the improved responses, resulting in a policy model that no longer requires feedback at inference. Evaluation on the LaMP-QA benchmark that consists of three diverse domains demonstrates consistent and significant improvements over the state-of-the-art results. Human evaluations further confirm the superior quality of the generated responses. These results demonstrate that NLF provides more effective signals for optimizing personalized question answering.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs</title>
<link>https://arxiv.org/abs/2508.10736</link>
<guid>https://arxiv.org/abs/2508.10736</guid>
<content:encoded><![CDATA[
arXiv:2508.10736v1 Announce Type: new 
Abstract: Despite large language models (LLMs) have achieved remarkable success, their prefix-only prompting paradigm and sequential generation process offer limited flexibility for bidirectional information. Diffusion large language models (dLLMs) present new opportunities through their bidirectional attention mechanisms and iterative refinement processes, enabling more flexible in-place prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting with Early Exit), a novel framework that transforms prefix-only prompting into in-place prompting specifically designed for dLLMs. ICE integrates in-place prompts directly within masked token positions during iterative refinement and employs a confidence-aware early exit mechanism to significantly reduce computational overhead. Extensive experiments demonstrate ICE's effectiveness, achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K, and up to 276.67$\times$ acceleration on MMLU while maintaining competitive performance.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback</title>
<link>https://arxiv.org/abs/2508.10795</link>
<guid>https://arxiv.org/abs/2508.10795</guid>
<content:encoded><![CDATA[
arXiv:2508.10795v1 Announce Type: new 
Abstract: Novelty assessment is a central yet understudied aspect of peer review, particularly in high volume fields like NLP where reviewer capacity is increasingly strained. We present a structured approach for automated novelty evaluation that models expert reviewer behavior through three stages: content extraction from submissions, retrieval and synthesis of related work, and structured comparison for evidence based assessment. Our method is informed by a large scale analysis of human written novelty reviews and captures key patterns such as independent claim verification and contextual reasoning. Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty assessments, the approach achieves 86.5% alignment with human reasoning and 75.3% agreement on novelty conclusions - substantially outperforming existing LLM based baselines. The method produces detailed, literature aware analyses and improves consistency over ad hoc reviewer judgments. These results highlight the potential for structured LLM assisted approaches to support more rigorous and transparent peer review without displacing human expertise. Data and code are made available.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforced Language Models for Sequential Decision Making</title>
<link>https://arxiv.org/abs/2508.10839</link>
<guid>https://arxiv.org/abs/2508.10839</guid>
<content:encoded><![CDATA[
arXiv:2508.10839v1 Announce Type: new 
Abstract: Large Language Models (LLMs) show potential as sequential decision-making agents, but their application is often limited due to a reliance on large, computationally expensive models. This creates a need to improve smaller models, yet existing post-training methods are designed for single-turn interactions and cannot handle credit assignment in multi-step agentic tasks. To address this, we introduce Multi-Step Group-Relative Policy Optimization (MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP) frameworks. For credit assignment, MS-GRPO attributes the entire cumulative episode reward to each individual episode step. We supplement this algorithm with a novel absolute-advantage-weighted episode sampling strategy that we show improves training performance. We evaluate our approach by post-training a 3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate that the method is effective in improving decision-making performance: our post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on the Frozen Lake task. This work demonstrates that targeted post-training is a practical and efficient alternative to relying on model scale for creating sequential decision-making agents using LLMs.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning</title>
<link>https://arxiv.org/abs/2508.10848</link>
<guid>https://arxiv.org/abs/2508.10848</guid>
<content:encoded><![CDATA[
arXiv:2508.10848v1 Announce Type: new 
Abstract: Amidst a shortage of qualified mental health professionals, the integration of large language models (LLMs) into psychological applications offers a promising way to alleviate the growing burden of mental health disorders. Recent reasoning-augmented LLMs have achieved remarkable performance in mathematics and programming, while research in the psychological domain has predominantly emphasized emotional support and empathetic dialogue, with limited attention to reasoning mechanisms that are beneficial to generating reliable responses. Therefore, in this paper, we propose Psyche-R1, the first Chinese psychological LLM that jointly integrates empathy, psychological expertise, and reasoning, built upon a novel data curation pipeline. Specifically, we design a comprehensive data synthesis pipeline that produces over 75k high-quality psychological questions paired with detailed rationales, generated through chain-of-thought (CoT) reasoning and iterative prompt-rationale optimization, along with 73k empathetic dialogues. Subsequently, we employ a hybrid training strategy wherein challenging samples are identified through a multi-LLM cross-selection strategy for group relative policy optimization (GRPO) to improve reasoning ability, while the remaining data is used for supervised fine-tuning (SFT) to enhance empathetic response generation and psychological domain knowledge. Extensive experiment results demonstrate the effectiveness of the Psyche-R1 across several psychological benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B DeepSeek-R1.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms</title>
<link>https://arxiv.org/abs/2508.10860</link>
<guid>https://arxiv.org/abs/2508.10860</guid>
<content:encoded><![CDATA[
arXiv:2508.10860v1 Announce Type: new 
Abstract: Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box'' predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSRL: Self-Search Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.10874</link>
<guid>https://arxiv.org/abs/2508.10874</guid>
<content:encoded><![CDATA[
arXiv:2508.10874v1 Announce Type: new 
Abstract: We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interactions with external search engines. To this end, we first quantify the intrinsic search capability of LLMs via structured prompting and repeated sampling, which we term Self-Search. Our results reveal that LLMs exhibit strong scaling behavior with respect to the inference budget, achieving high pass@k on question-answering benchmarks, including the challenging BrowseComp task. Building on these observations, we introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability through format-based and rule-based rewards. SSRL enables models to iteratively refine their knowledge utilization internally, without requiring access to external tools. Empirical evaluations demonstrate that SSRL-trained policy models provide a cost-effective and stable environment for search-driven RL training, reducing reliance on external search engines and facilitating robust sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world knowledge that can be effectively elicited to achieve high performance; 2) SSRL demonstrates the potential of leveraging internal knowledge to reduce hallucination; 3) SSRL-trained models integrate seamlessly with external search engines without additional effort. Our findings highlight the potential of LLMs to support more scalable RL agent training.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Diffusion Language Models</title>
<link>https://arxiv.org/abs/2508.10875</link>
<guid>https://arxiv.org/abs/2508.10875</guid>
<content:encoded><![CDATA[
arXiv:2508.10875v1 Announce Type: new 
Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data</title>
<link>https://arxiv.org/abs/2508.09636</link>
<guid>https://arxiv.org/abs/2508.09636</guid>
<content:encoded><![CDATA[
arXiv:2508.09636v1 Announce Type: cross 
Abstract: In this paper, we present a novel model architecture for optimizing personalized product search ranking using a multi-task learning (MTL) framework. Our approach uniquely integrates tabular and non-tabular data, leveraging a pre-trained TinyBERT model for semantic embeddings and a novel sampling technique to capture diverse customer behaviors. We evaluate our model against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2, and MMoE, focusing on their ability to handle mixed data types and optimize personalized ranking. Additionally, we propose a scalable relevance labeling mechanism based on click-through rates, click positions, and semantic similarity, offering an alternative to traditional human-annotated labels. Experimental results show that combining non-tabular data with advanced embedding techniques in multi-task learning paradigm significantly enhances model performance. Ablation studies further underscore the benefits of incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT query-product embedding interactions. These results demonstrate the effectiveness of our approach in achieving improved personalized product search ranking.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs</title>
<link>https://arxiv.org/abs/2508.10031</link>
<guid>https://arxiv.org/abs/2508.10031</guid>
<content:encoded><![CDATA[
arXiv:2508.10031v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) have shown significant advancements in performance, various jailbreak attacks have posed growing safety and ethical risks. Malicious users often exploit adversarial context to deceive LLMs, prompting them to generate responses to harmful queries. In this study, we propose a new defense mechanism called Context Filtering model, an input pre-processing method designed to filter out untrustworthy and unreliable context while identifying the primary prompts containing the real user intent to uncover concealed malicious intent. Given that enhancing the safety of LLMs often compromises their helpfulness, potentially affecting the experience of benign users, our method aims to improve the safety of the LLMs while preserving their original performance. We evaluate the effectiveness of our model in defending against jailbreak attacks through comparative analysis, comparing our approach with state-of-the-art defense mechanisms against six different attacks and assessing the helpfulness of LLMs under these defenses. Our model demonstrates its ability to reduce the Attack Success Rates of jailbreak attacks by up to 88% while maintaining the original LLMs' performance, achieving state-of-the-art Safety and Helpfulness Product results. Notably, our model is a plug-and-play method that can be applied to all LLMs, including both white-box and black-box models, to enhance their safety without requiring any fine-tuning of the models themselves. We will make our model publicly available for research purposes.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Show Signs of Alignment with Human Neurocognition During Abstract Reasoning</title>
<link>https://arxiv.org/abs/2508.10057</link>
<guid>https://arxiv.org/abs/2508.10057</guid>
<content:encoded><![CDATA[
arXiv:2508.10057v1 Announce Type: cross 
Abstract: This study investigates whether large language models (LLMs) mirror human neurocognition during abstract reasoning. We compared the performance and neural representations of human participants with those of eight open-source LLMs on an abstract-pattern-completion task. We leveraged pattern type differences in task performance and in fixation-related potentials (FRPs) as recorded by electroencephalography (EEG) during the task. Our findings indicate that only the largest tested LLMs (~70 billion parameters) achieve human-comparable accuracy, with Qwen-2.5-72B and DeepSeek-R1-70B also showing similarities with the human pattern-specific difficulty profile. Critically, every LLM tested forms representations that distinctly cluster the abstract pattern categories within their intermediate layers, although the strength of this clustering scales with their performance on the task. Moderate positive correlations were observed between the representational geometries of task-optimal LLM layers and human frontal FRPs. These results consistently diverged from comparisons with other EEG measures (response-locked ERPs and resting EEG), suggesting a potential shared representational space for abstract patterns. This indicates that LLMs might mirror human brain mechanisms in abstract reasoning, offering preliminary evidence of shared principles between biological and artificial intelligence.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion</title>
<link>https://arxiv.org/abs/2508.10068</link>
<guid>https://arxiv.org/abs/2508.10068</guid>
<content:encoded><![CDATA[
arXiv:2508.10068v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) for repository-level code completion commonly relies on superficial text similarity, leading to results plagued by semantic misguidance, redundancy, and homogeneity, while also failing to resolve external symbol ambiguity. To address these challenges, we introduce Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core Hierarchical Feature Optimization module systematically refines candidates by distilling deep semantic relationships, pruning exact duplicates, assessing structural similarity with a novel graph-based metric that weighs edits by their topological importance, and reranking results to maximize both relevance and diversity. Furthermore, an External-Aware Identifier Disambiguator module accurately resolves cross-file symbol ambiguity via dependency analysis. Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated benchmarks demonstrate that Saracoder significantly outperforms existing baselines across multiple programming languages and models. Our work proves that systematically refining retrieval results across multiple dimensions provides a new paradigm for building more accurate and robust repository-level code completion systems.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development</title>
<link>https://arxiv.org/abs/2508.10108</link>
<guid>https://arxiv.org/abs/2508.10108</guid>
<content:encoded><![CDATA[
arXiv:2508.10108v1 Announce Type: cross 
Abstract: AI systems for software development are rapidly gaining prominence, yet significant challenges remain in ensuring their safety. To address this, Amazon launched the Trusted AI track of the Amazon Nova AI Challenge, a global competition among 10 university teams to drive advances in secure AI. In the challenge, five teams focus on developing automated red teaming bots, while the other five create safe AI assistants. This challenge provides teams with a unique platform to evaluate automated red-teaming and safety alignment methods through head-to-head adversarial tournaments where red teams have multi-turn conversations with the competing AI coding assistants to test their safety alignment. Along with this, the challenge provides teams with a feed of high quality annotated data to fuel iterative improvement. Throughout the challenge, teams developed state-of-the-art techniques, introducing novel approaches in reasoning-based safety alignment, robust model guardrails, multi-turn jail-breaking, and efficient probing of large language models (LLMs). To support these efforts, the Amazon Nova AI Challenge team made substantial scientific and engineering investments, including building a custom baseline coding specialist model for the challenge from scratch, developing a tournament orchestration service, and creating an evaluation harness. This paper outlines the advancements made by university teams and the Amazon Nova AI Challenge team in addressing the safety challenges of AI for software development, highlighting this collaborative effort to raise the bar for AI safety.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts</title>
<link>https://arxiv.org/abs/2508.10123</link>
<guid>https://arxiv.org/abs/2508.10123</guid>
<content:encoded><![CDATA[
arXiv:2508.10123v1 Announce Type: cross 
Abstract: Advanced reasoning in LLMs on challenging domains like mathematical reasoning can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In standard ReFT frameworks, a behavior model generates multiple completions with answers per problem, for the answer to be then scored by a reward function. While such RL post-training methods demonstrate significant performance improvements across challenging reasoning domains, the computational cost of generating completions during training with multiple inference steps makes the training cost non-trivial. To address this, we draw inspiration from off-policy RL, and speculative decoding to introduce a novel ReFT framework, dubbed Nested-ReFT, where a subset of layers of the target model acts as the behavior model to generate off-policy completions during training. The behavior model configured with dynamic layer skipping per batch during training decreases the inference cost compared to the standard ReFT frameworks. Our theoretical analysis shows that Nested-ReFT yields unbiased gradient estimates with controlled variance. Our empirical analysis demonstrates improved computational efficiency measured as tokens/sec across multiple math reasoning benchmarks and model sizes. Additionally, we explore three variants of bias mitigation to minimize the off-policyness in the gradient updates that allows for maintaining performance that matches the baseline ReFT performance.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Real-time Jargon Support for Online Meetings</title>
<link>https://arxiv.org/abs/2508.10239</link>
<guid>https://arxiv.org/abs/2508.10239</guid>
<content:encoded><![CDATA[
arXiv:2508.10239v1 Announce Type: cross 
Abstract: Effective interdisciplinary communication is frequently hindered by domain-specific jargon. To explore the jargon barriers in-depth, we conducted a formative diary study with 16 professionals, revealing critical limitations in current jargon-management strategies during workplace meetings. Based on these insights, we designed ParseJargon, an interactive LLM-powered system providing real-time personalized jargon identification and explanations tailored to users' individual backgrounds. A controlled experiment comparing ParseJargon against baseline (no support) and general-purpose (non-personalized) conditions demonstrated that personalized jargon support significantly enhanced participants' comprehension, engagement, and appreciation of colleagues' work, whereas general-purpose support negatively affected engagement. A follow-up field study validated ParseJargon's usability and practical value in real-time meetings, highlighting both opportunities and limitations for real-world deployment. Our findings contribute insights into designing personalized jargon support tools, with implications for broader interdisciplinary and educational applications.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving OCR for Historical Texts of Multiple Languages</title>
<link>https://arxiv.org/abs/2508.10356</link>
<guid>https://arxiv.org/abs/2508.10356</guid>
<content:encoded><![CDATA[
arXiv:2508.10356v1 Announce Type: cross 
Abstract: This paper presents our methodology and findings from three tasks across Optical Character Recognition (OCR) and Document Layout Analysis using advanced deep learning techniques. First, for the historical Hebrew fragments of the Dead Sea Scrolls, we enhanced our dataset through extensive data augmentation and employed the Kraken and TrOCR models to improve character recognition. In our analysis of 16th to 18th-century meeting resolutions task, we utilized a Convolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for semantic segmentation with a Bidirectional LSTM, incorporating confidence-based pseudolabeling to refine our model. Finally, for modern English handwriting recognition task, we applied a CRNN with a ResNet34 encoder, trained using the Connectionist Temporal Classification (CTC) loss function to effectively capture sequential dependencies. This report offers valuable insights and suggests potential directions for future research.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model</title>
<link>https://arxiv.org/abs/2508.10416</link>
<guid>https://arxiv.org/abs/2508.10416</guid>
<content:encoded><![CDATA[
arXiv:2508.10416v1 Announce Type: cross 
Abstract: Existing vision-and-language navigation models often deviate from the correct trajectory when executing instructions. However, these models lack effective error correction capability, hindering their recovery from errors. To address this challenge, we propose Self-correction Flywheel, a novel post-training paradigm. Instead of considering the model's error trajectories on the training set as a drawback, our paradigm emphasizes their significance as a valuable data source. We have developed a method to identify deviations in these error trajectories and devised innovative techniques to automatically generate self-correction data for perception and action. These self-correction data serve as fuel to power the model's continued training. The brilliance of our paradigm is revealed when we re-evaluate the model on the training set, uncovering new error trajectories. At this time, the self-correction flywheel begins to spin. Through multiple flywheel iterations, we progressively enhance our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2% and 16.4%. Real robot tests in various indoor and outdoor environments demonstrate \method's superior capability of error correction, dynamic obstacle avoidance, and long instruction following.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model</title>
<link>https://arxiv.org/abs/2508.10492</link>
<guid>https://arxiv.org/abs/2508.10492</guid>
<content:encoded><![CDATA[
arXiv:2508.10492v1 Announce Type: cross 
Abstract: Full-process clinical diagnosis in the real world encompasses the entire diagnostic workflow that begins with only an ambiguous chief complaint. While artificial intelligence (AI), particularly large language models (LLMs), is transforming clinical diagnosis, its role remains largely as an assistant to physicians. This AI-assisted working pattern makes AI can only answer specific medical questions at certain parts within the diagnostic process, but lack the ability to drive the entire diagnostic process starting from an ambiguous complaint, which still relies heavily on human physicians. This gap limits AI's ability to fully reduce physicians' workload and enhance diagnostic efficiency. To address this, we propose a paradigm shift that reverses the relationship between physicians and AI: repositioning AI as the primary director, with physicians serving as its assistants. So we present DxDirector-7B, an LLM endowed with advanced deep thinking capabilities, enabling it to drive the full-process diagnosis with minimal physician involvement. Furthermore, DxDirector-7B establishes a robust accountability framework for misdiagnoses, delineating responsibility between AI and human physicians. In evaluations across rare, complex, and real-world cases under full-process diagnosis setting, DxDirector-7B not only achieves significant superior diagnostic accuracy but also substantially reduces physician workload than state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained analyses across multiple clinical departments and tasks validate its efficacy, with expert evaluations indicating its potential to serve as a viable substitute for medical specialists. These findings mark a new era where AI, traditionally a physicians' assistant, now drives the entire diagnostic process to drastically reduce physicians' workload, indicating an efficient and accurate diagnostic solution.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment</title>
<link>https://arxiv.org/abs/2508.10530</link>
<guid>https://arxiv.org/abs/2508.10530</guid>
<content:encoded><![CDATA[
arXiv:2508.10530v1 Announce Type: cross 
Abstract: The alignment of language models (LMs) with human preferences is critical for building reliable AI systems. The problem is typically framed as optimizing an LM policy to maximize the expected reward that reflects human preferences. Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment method that directly optimize the policy from static preference data, and further improved by incorporating on-policy sampling (i.e., preference candidates generated during the training loop) for better LM alignment. However, we show on-policy data is not always optimal, with systematic effectiveness difference emerging between static and on-policy preference candidates. For example, on-policy data can result in a 3$\times$ effectiveness compared with static data for Llama-3, and a 0.4$\times$ effectiveness for Zephyr. To explain the phenomenon, we propose the alignment stage assumption, which divides the alignment process into two distinct stages: the preference injection stage, which benefits from diverse data, and the preference fine-tuning stage, which favors high-quality data. Through theoretical and empirical analysis, we characterize these stages and propose an effective algorithm to identify the boundaries between them. We perform experiments on 5 models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO, SLiC-HF) to show the generalizability of alignment stage assumption and boundary measurement.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Value-based Process Verifier via Low-Cost Variance Reduction</title>
<link>https://arxiv.org/abs/2508.10539</link>
<guid>https://arxiv.org/abs/2508.10539</guid>
<content:encoded><![CDATA[
arXiv:2508.10539v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved remarkable success in a wide range of tasks. However, their reasoning capabilities, particularly in complex domains like mathematics, remain a significant challenge. Value-based process verifiers, which estimate the probability of a partial reasoning chain leading to a correct solution, are a promising approach for improving reasoning. Nevertheless, their effectiveness is often hindered by estimation error in their training annotations, a consequence of the limited number of Monte Carlo (MC) samples feasible due to the high cost of LLM inference. In this paper, we identify that the estimation error primarily arises from high variance rather than bias, and the MC estimator is a Minimum Variance Unbiased Estimator (MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte \textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased estimator by linearly combining the MC estimators from the current and subsequent steps. Theoretically, we show that our method leads to a predictable reduction in variance, while maintaining an unbiased estimation without additional LLM inference cost. We also perform empirical experiments on the MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method. Notably, ComMCS outperforms regression-based optimization method by 2.8 points, the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32 sampling experiment.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards</title>
<link>https://arxiv.org/abs/2508.10548</link>
<guid>https://arxiv.org/abs/2508.10548</guid>
<content:encoded><![CDATA[
arXiv:2508.10548v1 Announce Type: cross 
Abstract: Reward sparsity in long-horizon reinforcement learning (RL) tasks remains a significant challenge, while existing outcome-based reward shaping struggles to define meaningful immediate rewards without introducing bias or requiring explicit task decomposition. Alternatively, verification-based reward shaping uses stepwise critics, but misalignment between immediate rewards and long-term objectives can lead to reward hacking and suboptimal policies. In this work, we address this problem in the context of software engineering (SWE) tasks, where multi-turn reasoning and rule-based verification are critical. We introduce the SWE-oriented RL Framework, a unified system supporting multi-turn interaction, docker-based execution, and customizable reward functions. Additionally, we propose Gated Reward Accumulation (G-RA), a novel method that accumulates immediate rewards only when high-level (long-term) rewards meet a predefined threshold, ensuring stable RL optimization. Experiments on SWE-bench Verified and kBench demonstrate that G-RA leads to an increase in completion rates (47.6\% \rightarrow 93.8\% and 22.0\% \rightarrow 86.0\%) and modification rates (19.6\% \rightarrow 23.8\% and 12.0\% \rightarrow 42.0\%), while avoiding policy degradation caused by reward misalignment. Our findings highlight the importance of balanced reward accumulation in long-horizon RL and provide a practical solution.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models</title>
<link>https://arxiv.org/abs/2508.10751</link>
<guid>https://arxiv.org/abs/2508.10751</guid>
<content:encoded><![CDATA[
arXiv:2508.10751v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR), which typically adopts Pass@1 as the reward, has faced the issues in balancing exploration and exploitation, causing policies to prefer conservative actions, converging to a local optimum. Identifying an appropriate reward metric is therefore crucial. Regarding the prior work, although Pass@k has been used in evaluation, its connection to LLM exploration ability in RLVR remains largely overlooked. To investigate this, we first use Pass@k as the reward to train the policy model (i.e., $\textbf{Pass@k Training}$), and observe the improvement on its exploration ability. Next, we derive an analytical solution for the advantage of Pass@k Training, leading to an efficient and effective process. Building on this, our analysis reveals that exploration and exploitation are not inherently conflicting objectives, while they can mutually enhance each other. Moreover, Pass@k Training with analytical derivation essentially involves directly designing the advantage function. Inspired by this, we preliminarily explore the advantage design for RLVR, showing promising results and highlighting a potential future direction.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions</title>
<link>https://arxiv.org/abs/2508.10824</link>
<guid>https://arxiv.org/abs/2508.10824</guid>
<content:encoded><![CDATA[
arXiv:2508.10824v1 Announce Type: cross 
Abstract: Memory is fundamental to intelligence, enabling learning, reasoning, and adaptability across biological and artificial systems. While Transformer architectures excel at sequence modeling, they face critical limitations in long-range context retention, continual learning, and knowledge integration. This review presents a unified framework bridging neuroscience principles, including dynamic multi-timescale memory, selective attention, and consolidation, with engineering advances in Memory-Augmented Transformers. We organize recent progress through three taxonomic dimensions: functional objectives (context extension, reasoning, knowledge integration, adaptation), memory representations (parameter-encoded, state-based, explicit, hybrid), and integration mechanisms (attention fusion, gated control, associative retrieval). Our analysis of core memory operations (reading, writing, forgetting, and capacity management) reveals a shift from static caches toward adaptive, test-time learning systems. We identify persistent challenges in scalability and interference, alongside emerging solutions including hierarchical buffering and surprise-gated updates. This synthesis provides a roadmap toward cognitively-inspired, lifelong-learning Transformer architectures.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Searching for Privacy Risks in LLM Agents via Simulation</title>
<link>https://arxiv.org/abs/2508.10880</link>
<guid>https://arxiv.org/abs/2508.10880</guid>
<content:encoded><![CDATA[
arXiv:2508.10880v1 Announce Type: cross 
Abstract: The widespread deployment of LLM-based agents is likely to introduce a critical privacy threat: malicious agents that proactively engage others in multi-turn interactions to extract sensitive information. These dynamic dialogues enable adaptive attack strategies that can cause severe privacy violations, yet their evolving nature makes it difficult to anticipate and discover sophisticated vulnerabilities manually. To tackle this problem, we present a search-based framework that alternates between improving attacker and defender instructions by simulating privacy-critical agent interactions. Each simulation involves three roles: data subject, data sender, and data recipient. While the data subject's behavior is fixed, the attacker (data recipient) attempts to extract sensitive information from the defender (data sender) through persistent and interactive exchanges. To explore this interaction space efficiently, our search algorithm employs LLMs as optimizers, using parallel search with multiple threads and cross-thread propagation to analyze simulation trajectories and iteratively propose new instructions. Through this process, we find that attack strategies escalate from simple direct requests to sophisticated multi-turn tactics such as impersonation and consent forgery, while defenses advance from rule-based constraints to identity-verification state machines. The discovered attacks and defenses transfer across diverse scenarios and backbone models, demonstrating strong practical utility for building privacy-aware agents.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-based Consistency Testing of Large Language Models</title>
<link>https://arxiv.org/abs/2407.12830</link>
<guid>https://arxiv.org/abs/2407.12830</guid>
<content:encoded><![CDATA[
arXiv:2407.12830v3 Announce Type: replace 
Abstract: In this work, we systematically expose and measure the inconsistency and knowledge gaps of Large Language Models (LLMs). Specifically, we propose an automated testing framework (called KonTest) which leverages a knowledge graph to construct test cases. KonTest probes and measures the inconsistencies in the LLM's knowledge of the world via a combination of semantically-equivalent queries and test oracles (metamorphic or ontological oracle). KonTest further mitigates knowledge gaps via a weighted LLM model ensemble. Using four state-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that KonTest generates 19.2% error inducing inputs (1917 errors from 9979 test inputs). It also reveals a 16.5% knowledge gap across all tested LLMs. A mitigation method informed by KonTest's test suite reduces LLM knowledge gap by 32.48%. Our ablation study further shows that GPT3.5 is not suitable for knowledge-based consistency testing because it is only 60%-68% effective in knowledge construction.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>This Candidate is [MASK]. Prompt-based Sentiment Extraction and Reference Letters</title>
<link>https://arxiv.org/abs/2410.16325</link>
<guid>https://arxiv.org/abs/2410.16325</guid>
<content:encoded><![CDATA[
arXiv:2410.16325v2 Announce Type: replace 
Abstract: I propose a relatively simple way to deploy pre-trained large language models (LLMs) in order to extract sentiment and other useful features from text data. The method, which I refer to as prompt-based sentiment extraction, offers multiple advantages over other methods used in economics and finance. I apply my prompt-based strategy to a hand-collected corpus of confidential reference letters (RLs). I show that the sentiment contents of RLs is clearly reflected in job market outcomes. Candidates with higher average sentiment in their letters perform markedly better regardless of the measure of success chosen. Moreover, I show that disagreement among letter writers negatively affects the job market candidate's performance. I compare my sentiment extraction approach to other commonly used methods for sentiment analysis: "bag-of-words" approaches, fine-tuned language models, and querying advanced chatbots. I find that no other method can reproduce the results obtained by prompt-based sentiment extraction. Finally, I slightly modify the method to obtain "gendered" sentiment scores (as in Eberhardt et al., 2023). I show that letters of reference written for female candidates emphasize "grindstone" personality traits, whereas male candidates' letters emphasize "standout" traits. These gender differences negatively affect women's job market outcomes.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language Understanding</title>
<link>https://arxiv.org/abs/2501.06117</link>
<guid>https://arxiv.org/abs/2501.06117</guid>
<content:encoded><![CDATA[
arXiv:2501.06117v3 Announce Type: replace 
Abstract: Spoken language understanding (SLU) is indispensable for half of all living languages that lack a formal writing system. Unlike for high-resource languages, for these languages, we cannot offload semantic understanding of speech to the cascade of automatic speech recognition (ASR) and text-based large language models (LLMs). Even if low-resource languages possess a writing system, ASR for these languages remains unreliable due to limited bimodal speech and text training data. Nonetheless, the evaluation of multilingual SLU is limited to shallow tasks such as intent classification or language identification. This is why we present Fleurs-SLU, a multilingual SLU benchmark that encompasses (i) 692 hours of speech for topical utterance classification in 102 languages and (ii) multiple-choice question answering via listening comprehension spanning 944 hours of speech across 92 languages. We extensively evaluate end-to-end speech classification models, cascaded systems that combine speech-to-text transcription with subsequent LLM-based classification, and multimodal speech-LLMs on Fleurs-SLU. Our results show that cascaded systems are more robust in multilingual SLU, though well-pretrained speech encoders can perform competitively in topical speech classification. Closed-source speech-LLMs match or surpass the performance of cascaded systems. We observe a strong correlation between robust multilingual ASR, effective speech-to-text translation, and strong multilingual SLU, indicating mutual benefits between acoustic and semantic speech representations.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Diversity in Synthetic Datasets</title>
<link>https://arxiv.org/abs/2502.08512</link>
<guid>https://arxiv.org/abs/2502.08512</guid>
<content:encoded><![CDATA[
arXiv:2502.08512v3 Announce Type: replace 
Abstract: Large language models (LLMs) are widely adopted to generate synthetic datasets for various natural language processing (NLP) tasks, such as text classification and summarization. However, accurately measuring the diversity of these synthetic datasets-an aspect crucial for robust model performance-remains a significant challenge. In this paper, we introduce DCScore, a novel method for measuring synthetic dataset diversity from a classification perspective. Specifically, DCScore formulates diversity evaluation as a sample classification task, leveraging mutual relationships among samples. We further provide theoretical verification of the diversity-related axioms satisfied by DCScore, highlighting its role as a principled diversity evaluation method. Experimental results on synthetic datasets reveal that DCScore enjoys a stronger correlation with multiple diversity pseudo-truths of evaluated datasets, underscoring its effectiveness. Moreover, both empirical and theoretical evidence demonstrate that DCScore substantially reduces computational costs compared to existing methods. Code is available at: https://github.com/bluewhalelab/dcscore.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint</title>
<link>https://arxiv.org/abs/2502.16770</link>
<guid>https://arxiv.org/abs/2502.16770</guid>
<content:encoded><![CDATA[
arXiv:2502.16770v2 Announce Type: replace 
Abstract: Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: $\textbf{neuron misidentification}$ due to simplistic parameter magnitude-based selection, and $\textbf{cross-task neuron interference}$ during merging. To address these challenges, we propose $\textbf{LED-Merging}$, a three-stage framework that $\textbf{L}$ocates task-specific neurons via gradient-based attribution, dynamically $\textbf{E}$lects critical neurons through multi-model importance fusion, and $\textbf{D}$isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging effectively reduces harmful response rates, showing a 31.4\% decrease on Llama-3-8B-Instruct on HarmBench, while simultaneously preserving 95\% of utility performance, such as achieving 52.39\% accuracy on GSM8K. LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs. Code is available at $\href{https://github.com/MqLeet/LED-Merging}{GitHub}$.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TikZero: Zero-Shot Text-Guided Graphics Program Synthesis</title>
<link>https://arxiv.org/abs/2503.11509</link>
<guid>https://arxiv.org/abs/2503.11509</guid>
<content:encoded><![CDATA[
arXiv:2503.11509v3 Announce Type: replace 
Abstract: Automatically synthesizing figures from text captions is a compelling capability. However, achieving high geometric precision and editability requires representing figures as graphics programs in languages like TikZ, and aligned training data (i.e., graphics programs with captions) remains scarce. Meanwhile, large amounts of unaligned graphics programs and captioned raster images are more readily available. We reconcile these disparate data sources by presenting TikZero, which decouples graphics program generation from text understanding by using image representations as an intermediary bridge. It enables independent training on graphics programs and captioned images and allows for zero-shot text-guided graphics program synthesis during inference. We show that our method substantially outperforms baselines that can only operate with caption-aligned graphics programs. Furthermore, when leveraging caption-aligned graphics programs as a complementary training signal, TikZero matches or exceeds the performance of much larger models, including commercial systems like GPT-4o. Our code, datasets, and select models are publicly available.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Sentiment Analysis with DeepSeek-R1: Performance, Efficiency, and Few-Shot Learning</title>
<link>https://arxiv.org/abs/2503.11655</link>
<guid>https://arxiv.org/abs/2503.11655</guid>
<content:encoded><![CDATA[
arXiv:2503.11655v3 Announce Type: replace 
Abstract: Large language models (LLMs) have transformed sentiment analysis, yet balancing accuracy, efficiency, and explainability remains a critical challenge. This study presents the first comprehensive evaluation of DeepSeek-R1--an open-source reasoning model--against OpenAI's GPT-4o and GPT-4o-mini. We test the full 671B model and its distilled variants, systematically documenting few-shot learning curves. Our experiments show DeepSeek-R1 achieves a 91.39\% F1 score on 5-class sentiment and 99.31\% accuracy on binary tasks with just 5 shots, an eightfold improvement in few-shot efficiency over GPT-4o. Architecture-specific distillation effects emerge, where a 32B Qwen2.5-based model outperforms the 70B Llama-based variant by 6.69 percentage points. While its reasoning process reduces throughput, DeepSeek-R1 offers superior explainability via transparent, step-by-step traces, establishing it as a powerful, interpretable open-source alternative.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Instruction-Tuning Datasets from Human-Written Instructions with Open-Weight Large Language Models</title>
<link>https://arxiv.org/abs/2503.23714</link>
<guid>https://arxiv.org/abs/2503.23714</guid>
<content:encoded><![CDATA[
arXiv:2503.23714v2 Announce Type: replace 
Abstract: Instruction tuning is crucial for enabling Large Language Models (LLMs) to solve real-world tasks. Prior work has shown the effectiveness of instruction-tuning data synthesized solely from LLMs, raising a fundamental question: Do we still need human-originated signals for instruction tuning? This work answers the question affirmatively: we build state-of-the-art instruction-tuning datasets sourced from human-written instructions, by simply pairing them with LLM-generated responses. LLMs fine-tuned on our datasets consistently outperform those fine-tuned on existing ones. Our data construction approach can be easily adapted to other languages; we build datasets for Japanese and confirm that LLMs tuned with our data reach state-of-the-art performance. Analyses suggest that instruction-tuning in a new language allows LLMs to follow instructions, while the tuned models exhibit a notable lack of culture-specific knowledge in that language. The datasets and fine-tuned models will be publicly available. Our datasets, synthesized with open-weight LLMs, are openly distributed under permissive licenses, allowing for diverse use cases.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolACE-R: Model-aware Iterative Training and Adaptive Refinement for Tool Learning</title>
<link>https://arxiv.org/abs/2504.01400</link>
<guid>https://arxiv.org/abs/2504.01400</guid>
<content:encoded><![CDATA[
arXiv:2504.01400v2 Announce Type: replace 
Abstract: Tool learning, which allows Large Language Models (LLMs) to leverage external tools for solving complex user tasks, has emerged as a promising avenue for extending model capabilities. However, existing approaches primarily focus on data synthesis for fine-tuning LLMs to invoke tools effectively, largely ignoring how to fully stimulate the potential of the model. In this paper, we propose ToolACE-R, a novel framework that includes both model-aware iterative training and adaptive refinement for tool learning. ToolACE-R features a model-aware iterative training procedure that progressively adjust training samples based on the model's evolving capabilities to maximize its potential. Additionally, it incorporates self-refinement training corpus which emphasizes LLM's ability to iteratively refine their tool calls, optimizing performance without requiring external feedback. Furthermore, we introduce adaptive self-refinement mechanism for efficient test-time scaling, where the trained model can autonomously determine when to stop the process based on iterative self-refinement. We conduct extensive experiments across several benchmark datasets, showing that ToolACE-R achieves competitive performance compared to advanced API-based models. The performance of tool invocation can be further improved efficiently through adaptive self-refinement. These results highlight the effectiveness and generalizability of ToolACE-R, offering a promising direction for more efficient and scalable tool learning.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoTAL: Human-in-the-Loop Prompt Engineering for Generalizable Formative Assessment Scoring</title>
<link>https://arxiv.org/abs/2504.02323</link>
<guid>https://arxiv.org/abs/2504.02323</guid>
<content:encoded><![CDATA[
arXiv:2504.02323v3 Announce Type: replace 
Abstract: Large language models (LLMs) have created new opportunities to assist teachers and support student learning. While researchers have explored various prompt engineering approaches in educational contexts, the degree to which these approaches generalize across domains--such as science, computing, and engineering--remains underexplored. In this paper, we introduce Chain-of-Thought Prompting + Active Learning (CoTAL), an LLM-based approach to formative assessment scoring that (1) leverages Evidence-Centered Design (ECD) to align assessments and rubrics with curriculum goals, (2) applies human-in-the-loop prompt engineering to automate response scoring, and (3) incorporates chain-of-thought (CoT) prompting and teacher and student feedback to iteratively refine questions, rubrics, and LLM prompts. Our findings demonstrate that CoTAL improves GPT-4's scoring performance across domains, achieving gains of up to 38.9% over a non-prompt-engineered baseline (i.e., without labeled examples, chain-of-thought prompting, or iterative refinement). Teachers and students judge CoTAL to be effective at scoring and explaining responses, and their feedback produces valuable insights that enhance grading accuracy and explanation quality.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Swedish Whispers; Leveraging a Massive Speech Corpus for Swedish Speech Recognition</title>
<link>https://arxiv.org/abs/2505.17538</link>
<guid>https://arxiv.org/abs/2505.17538</guid>
<content:encoded><![CDATA[
arXiv:2505.17538v2 Announce Type: replace 
Abstract: This work presents a suite of fine-tuned Whisper models for Swedish, trained on a dataset of unprecedented size and variability for this mid-resourced language. As languages of smaller sizes are often underrepresented in multilingual training datasets, substantial improvements in performance can be achieved by fine-tuning existing multilingual models, as shown in this work. This work reports an overall improvement across model sizes compared to OpenAI's Whisper evaluated on Swedish. Most notably, we report an average 47% reduction in WER comparing our best performing model to OpenAI's whisper-large-v3, in evaluations across FLEURS, Common Voice, and NST.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curse of High Dimensionality Issue in Transformer for Long-context Modeling</title>
<link>https://arxiv.org/abs/2505.22107</link>
<guid>https://arxiv.org/abs/2505.22107</guid>
<content:encoded><![CDATA[
arXiv:2505.22107v4 Announce Type: replace 
Abstract: Transformer-based large language models (LLMs) excel in natural language processing tasks by capturing long-range dependencies through self-attention mechanisms. However, long-context modeling faces significant computational inefficiencies due to \textit{redundant} attention computations: while attention weights are often \textit{sparse}, all tokens consume \textit{equal} computational resources. In this paper, we reformulate traditional probabilistic sequence modeling as a \textit{supervised learning task}, enabling the separation of relevant and irrelevant tokens and providing a clearer understanding of redundancy. Based on this reformulation, we theoretically analyze attention sparsity, revealing that only a few tokens significantly contribute to predictions. Building on this, we formulate attention optimization as a linear coding problem and propose a \textit{group coding strategy}, theoretically showing its ability to improve robustness against random noise and enhance learning efficiency. Motivated by this, we propose \textit{Dynamic Group Attention} (DGA), which leverages the group coding to explicitly reduce redundancy by aggregating less important tokens during attention computation. Empirical results show that our DGA significantly reduces computational costs while maintaining competitive performance.Code is available at https://github.com/bolixinyu/DynamicGroupAttention.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AF-MAT: Aspect-aware Flip-and-Fuse xLSTM for Aspect-based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2507.01213</link>
<guid>https://arxiv.org/abs/2507.01213</guid>
<content:encoded><![CDATA[
arXiv:2507.01213v2 Announce Type: replace 
Abstract: Aspect-based Sentiment Analysis (ABSA) is a crucial NLP task that extracts fine-grained opinions and sentiments from text, such as product reviews and customer feedback. Existing methods often trade off efficiency for performance: traditional LSTM or RNN models struggle to capture long-range dependencies, transformer-based methods are computationally costly, and Mamba-based approaches rely on CUDA and weaken local dependency modeling. The recently proposed Extended Long Short-Term Memory (xLSTM) model offers a promising alternative by effectively capturing long-range dependencies through exponential gating and enhanced memory variants, sLSTM for modeling local dependencies, and mLSTM for scalable, parallelizable memory. However, xLSTM's application in ABSA remains unexplored. To address this, we introduce Aspect-aware Flip-and-Fuse xLSTM (AF-MAT), a framework that leverages xLSTM's strengths. AF-MAT features an Aspect-aware matrix LSTM (AA-mLSTM) mechanism that introduces a dedicated aspect gate, enabling the model to selectively emphasize tokens semantically relevant to the target aspect during memory updates. To model multi-scale context, we incorporate a FlipMix block that sequentially applies a partially flipped Conv1D (pf-Conv1D) to capture short-range dependencies in reverse order, followed by a fully flipped mLSTM (ff-mLSTM) to model long-range dependencies via full sequence reversal. Additionally, we propose MC2F, a lightweight Multihead Cross-Feature Fusion based on mLSTM gating, which dynamically fuses AA-mLSTM outputs (queries and keys) with FlipMix outputs (values) for adaptive representation integration. Experiments on three benchmark datasets demonstrate that AF-MAT outperforms state-of-the-art baselines, achieving higher accuracy in ABSA tasks.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meanings are like Onions: a Layered Approach to Metaphor Processing</title>
<link>https://arxiv.org/abs/2507.10354</link>
<guid>https://arxiv.org/abs/2507.10354</guid>
<content:encoded><![CDATA[
arXiv:2507.10354v2 Announce Type: replace 
Abstract: Metaphorical meaning is not a flat mapping between concepts, but a complex cognitive phenomenon that integrates multiple levels of interpretation. In this paper, we propose a stratified model of metaphor processing that treats meaning as an onion: a multi-layered structure comprising (1) content analysis, (2) conceptual blending, and (3) pragmatic intentionality. This three-dimensional framework allows for a richer and more cognitively grounded approach to metaphor interpretation in computational systems. At the first level, metaphors are annotated through basic conceptual elements. At the second level, we model conceptual combinations, linking components to emergent meanings. Finally, at the third level, we introduce a pragmatic vocabulary to capture speaker intent, communicative function, and contextual effects, aligning metaphor understanding with pragmatic theories. By unifying these layers into a single formal framework, our model lays the groundwork for computational methods capable of representing metaphorical meaning beyond surface associations, toward deeper, more context-sensitive reasoning.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks</title>
<link>https://arxiv.org/abs/2507.10535</link>
<guid>https://arxiv.org/abs/2507.10535</guid>
<content:encoded><![CDATA[
arXiv:2507.10535v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have significantly advanced the state-of-the-art in various coding tasks. Beyond directly answering user queries, LLMs can also serve as judges, assessing and comparing the quality of responses generated by other models. Such an evaluation capability is crucial both for benchmarking different LLMs and for improving response quality through response ranking. However, despite the growing adoption of the LLM-as-a-Judge paradigm, its effectiveness in coding scenarios remains underexplored due to the absence of dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge models across three critical coding tasks: code generation, code repair, and unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge models, we find that recent thinking models significantly outperform non-thinking models on our carefully designed code judging tasks. Notably, even relatively small thinking models, such as Qwen3-8B, can outperform specially trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still exhibit significant randomness in their judgment of coding tasks. For pairwise judging tasks, simply changing the order in which responses are presented can substantially impact accuracy. In addition, when judging code and unit tests written by different LLMs, LLM-as-a-Judge models also show variance in performance. This sensitivity raises concerns about the reliability and consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal prompting strategies for LLM-as-a-Judge. We find that using pair-wise comparison outperforms scalar point-wise judging. Furthermore, retaining comments and reasoning in the full, unprocessed LLM response leads to improved judge performance.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base</title>
<link>https://arxiv.org/abs/2507.14189</link>
<guid>https://arxiv.org/abs/2507.14189</guid>
<content:encoded><![CDATA[
arXiv:2507.14189v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various applications. However, their use as writing assistants in specialized domains like finance, medicine, and law is often hampered by a lack of deep domain-specific knowledge and a tendency to hallucinate. Existing solutions, such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency across multiple retrieval steps, while online search-based methods often degrade quality due to unreliable web content. To address these challenges, we introduce DeepWriter, a customizable, multimodal, long-form writing assistant that operates on a curated, offline knowledge base. DeepWriter leverages a novel pipeline that involves task decomposition, outline generation, multimodal retrieval, and section-by-section composition with reflection. By deeply mining information from a structured corpus and incorporating both textual and visual elements, DeepWriter generates coherent, factually grounded, and professional-grade documents. We also propose a hierarchical knowledge representation to enhance retrieval efficiency and accuracy. Our experiments on financial report generation demonstrate that DeepWriter produces high-quality, verifiable articles that surpasses existing baselines in factual accuracy and generated content quality.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Query Expansion Approach via Agent-Mediated Dialogic Inquiry</title>
<link>https://arxiv.org/abs/2502.08557</link>
<guid>https://arxiv.org/abs/2502.08557</guid>
<content:encoded><![CDATA[
arXiv:2502.08557v3 Announce Type: replace-cross 
Abstract: Query expansion is widely used in Information Retrieval (IR) to improve search outcomes by supplementing initial queries with richer information. While recent Large Language Model (LLM) based methods generate pseudo-relevant content and expanded terms via multiple prompts, they often yield homogeneous, narrow expansions that lack the diverse context needed to retrieve relevant information. In this paper, we propose AMD: a new Agent-Mediated Dialogic Framework that engages in a dialogic inquiry involving three specialized roles: (1) a Socratic Questioning Agent reformulates the initial query into three sub-questions, with each question inspired by a specific Socratic questioning dimension, including clarification, assumption probing, and implication probing, (2) a Dialogic Answering Agent generates pseudo-answers, enriching the query representation with multiple perspectives aligned to the user's intent, and (3) a Reflective Feedback Agent evaluates and refines these pseudo-answers, ensuring that only the most relevant and informative content is retained. By leveraging a multi-agent process, AMD effectively crafts richer query representations through inquiry and feedback refinement. Extensive experiments on benchmarks including BEIR and TREC demonstrate that our framework outperforms previous methods, offering a robust solution for retrieval tasks.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit KV Cache</title>
<link>https://arxiv.org/abs/2503.18773</link>
<guid>https://arxiv.org/abs/2503.18773</guid>
<content:encoded><![CDATA[
arXiv:2503.18773v2 Announce Type: replace-cross 
Abstract: The rise of long-context Large Language Models (LLMs) amplifies memory and bandwidth demands during autoregressive decoding, as the Key-Value (KV) cache grows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or 2-bit) can reduce memory footprint while preserving accuracy, but existing systems suffer from slow decoding due to their exclusive reliance on CUDA cores, neglecting Tensor Cores (the primary source of compute on modern GPUs). We present BitDecoding, a new long-context LLM inference system with a low-bit KV cache. BitDecoding enables efficient low-bit KV-cache decoding by cooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for automatically inducing optimized layouts to exploit Tensor Cores, along with warp-level parallelization strategies for dequantization. For unified system support, BitDecoding includes a query transformation module supporting diverse attention variants, a quantization kernel that supports both tensor-wise and channel-wise scaling used in various quantization algorithms with high performance, and a dequantization kernel with a software-defined pipeline to coordinate CUDA and Tensor Cores execution for mixed-precision operations. Evaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up to 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and surpasses the state-of-the-art low-bit system QServe by up to 4.3x. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x, showing substantial improvements for long-context generation. The code is available at https://github.com/DD-DuDa/BitDecoding.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting</title>
<link>https://arxiv.org/abs/2504.15485</link>
<guid>https://arxiv.org/abs/2504.15485</guid>
<content:encoded><![CDATA[
arXiv:2504.15485v2 Announce Type: replace-cross 
Abstract: Recognizing and reasoning about occluded (partially or fully hidden) objects is vital to understanding visual scenes, as occlusions frequently occur in real-world environments and act as obstacles for spatial comprehension. To test models' ability to reason about multiple occluded objects, we introduce a novel task, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which requires a model to count objects arranged in a pattern by inferring how the pattern continues behind an occluder (an object which blocks parts of the scene). CAPTURe requires both recognizing visual patterns and reasoning, making it a useful testbed for evaluating vision-language models (VLMs) on whether they understand occluded patterns and possess spatial understanding skills. By requiring models to reason about occluded objects, CAPTURe also tests VLMs' ability to form world models that would allow them to fill in missing information. CAPTURe consists of two parts: (1) CAPTURe-real, with manually filtered images of real objects in patterns and (2) CAPTURe-synthetic, a controlled diagnostic with generated patterned images. We evaluate four strong VLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models struggle to count on both occluded and unoccluded patterns. Crucially, we find that models perform worse with occlusion, suggesting that VLMs are also deficient in inferring unseen spatial relationships: even the strongest VLMs like GPT-4o fail to count with occlusion. In contrast, we find that humans achieve very little error on CAPTURe. We also find that providing auxiliary information of occluded object locations increases performance, underscoring that the model error comes both from an inability to handle occlusion as well as difficulty in counting in images. Code and data: https://github.com/atinpothiraj/CAPTURe
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free</title>
<link>https://arxiv.org/abs/2505.03810</link>
<guid>https://arxiv.org/abs/2505.03810</guid>
<content:encoded><![CDATA[
arXiv:2505.03810v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) face deployment challenges due to high computational costs, and while Post-Training Quantization (PTQ) offers a solution, existing rotation-based methods struggle at very low bit-widths like 2-bit. We introduce a novel, training-free approach to construct an improved rotation matrix, addressing the limitations of current methods. The key contributions include leveraging the Walsh-Hadamard transform with sequency ordering, which clusters similar frequency components to reduce quantization error compared to standard Hadamard matrices, significantly improving performance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR) using block-diagonal matrices with smaller Walsh blocks, effectively isolating outlier impacts and achieving performance comparable to optimization-based methods without requiring any training. Our method demonstrates robust performance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our method also enhances results even when applied over existing learned rotation techniques.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference</title>
<link>https://arxiv.org/abs/2505.13109</link>
<guid>https://arxiv.org/abs/2505.13109</guid>
<content:encoded><![CDATA[
arXiv:2505.13109v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPS: A Multilingual Benchmark for Global Agent Performance and Security</title>
<link>https://arxiv.org/abs/2505.15935</link>
<guid>https://arxiv.org/abs/2505.15935</guid>
<content:encoded><![CDATA[
arXiv:2505.15935v2 Announce Type: replace-cross 
Abstract: Agentic AI systems, which build on Large Language Models (LLMs) and interact with tools and memory, have rapidly advanced in capability and scope. Yet, since LLMs have been shown to struggle in multilingual settings, typically resulting in lower performance and reduced safety, agentic systems risk inheriting these limitations. This raises concerns about the accessibility of such systems, as users interacting in languages other than English may encounter unreliable or security-critical agent behavior. Despite growing interest in evaluating agentic AI, existing benchmarks focus exclusively on English, leaving multilingual settings unexplored. To address this gap, we propose MAPS, a multilingual benchmark suite designed to evaluate agentic AI systems across diverse languages and tasks. MAPS builds on four widely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code generation), MATH (mathematical reasoning), and the Agent Security Benchmark (security). We translate each dataset into eleven diverse languages, resulting in 805 unique tasks and 9,660 total language-specific instances - enabling a systematic analysis of the multilingual effect on AI agents' performance and robustness. Empirically, we observe degradation in both performance and security when transitioning from English to other languages, with severity varying by task and correlating with the amount of translated input. Building on these findings, we provide actionable recommendations to guide agentic AI systems development and assessment under multilingual settings. This work establishes the first standardized evaluation framework for multilingual agentic AI, encouraging future research towards equitable, reliable, and accessible agentic AI. MAPS benchmark suite is publicly available at https://huggingface.co/datasets/Fujitsu-FRE/MAPS
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Cultural Competence of Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.22793</link>
<guid>https://arxiv.org/abs/2505.22793</guid>
<content:encoded><![CDATA[
arXiv:2505.22793v2 Announce Type: replace-cross 
Abstract: Modern vision-language models (VLMs) often fail at cultural competency evaluations and benchmarks. Given the diversity of applications built upon VLMs, there is renewed interest in understanding how they encode cultural nuances. While individual aspects of this problem have been studied, we still lack a comprehensive framework for systematically identifying and annotating the nuanced cultural dimensions present in images for VLMs. This position paper argues that foundational methodologies from visual culture studies (cultural studies, semiotics, and visual studies) are necessary for cultural analysis of images. Building upon this review, we propose a set of five frameworks, corresponding to cultural dimensions, that must be considered for a more complete analysis of the cultural competencies of VLMs.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods</title>
<link>https://arxiv.org/abs/2506.10236</link>
<guid>https://arxiv.org/abs/2506.10236</guid>
<content:encoded><![CDATA[
arXiv:2506.10236v2 Announce Type: replace-cross 
Abstract: In this work, we demonstrate that certain machine unlearning methods may fail under straightforward prompt attacks. We systematically evaluate eight unlearning techniques across three model families using output-based, logit-based, and probe analysis to assess the extent to which supposedly unlearned knowledge can be retrieved. While methods like RMU and TAR exhibit robust unlearning, ELM remains vulnerable to specific prompt attacks (e.g., prepending Hindi filler text to the original prompt recovers 57.3% accuracy). Our logit analysis further indicates that unlearned models are unlikely to hide knowledge through changes in answer formatting, given the strong correlation between output and logit accuracy. These findings challenge prevailing assumptions about unlearning effectiveness and highlight the need for evaluation frameworks that can reliably distinguish between genuine knowledge removal and superficial output suppression. To facilitate further research, we publicly release our evaluation framework to easily evaluate prompting techniques to retrieve unlearned knowledge.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models</title>
<link>https://arxiv.org/abs/2506.22493</link>
<guid>https://arxiv.org/abs/2506.22493</guid>
<content:encoded><![CDATA[
arXiv:2506.22493v3 Announce Type: replace-cross 
Abstract: Political Compass Test (PCT) or similar questionnaires have been used to quantify LLM's political leanings. Building on a recent line of work that examines the validity of PCT tests, we demonstrate that variation in standard generation parameters does not significantly impact the models' PCT scores. However, external factors such as prompt variations and fine-tuning individually and in combination affect the same. Finally, we demonstrate that when models are fine-tuned on text datasets with higher political content than others, the PCT scores are not differentially affected. This calls for a thorough investigation into the validity of PCT and similar tests, as well as the mechanism by which political leanings are encoded in LLMs.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization</title>
<link>https://arxiv.org/abs/2507.15758</link>
<guid>https://arxiv.org/abs/2507.15758</guid>
<content:encoded><![CDATA[
arXiv:2507.15758v2 Announce Type: replace-cross 
Abstract: Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through a two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. The second stage leverages these patterns as meta-cognitive guidance, embedding them directly within the model's reasoning context to ensure inference-time flexibility. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9% while improving accuracy by 2.3%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens</title>
<link>https://arxiv.org/abs/2508.01191</link>
<guid>https://arxiv.org/abs/2508.01191</guid>
<content:encoded><![CDATA[
<div> prompting, Large Language Model, reasoning, inductive bias, distribution discrepancy

Summary:
- The study explores Chain-of-Thought (CoT) prompting in Large Language Models (LLMs) and questions the depth of CoT reasoning.
- CoT reasoning is analyzed through a data distribution lens to understand its limitations in generating human-like reasoning paths.
- The research dissects CoT reasoning based on task, length, and format dimensions, revealing its brittleness when pushed beyond training data distributions.
- DataAlchemy, an isolated environment, is used to train LLMs from scratch and systematically probe them under varying distribution conditions.
- The findings suggest that CoT reasoning may not be as robust as previously thought, highlighting the challenge of achieving genuine and generalizable reasoning in LLMs.<br /><br />Summary: <div>
arXiv:2508.01191v3 Announce Type: replace-cross 
Abstract: Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.09303</link>
<guid>https://arxiv.org/abs/2508.09303</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, search agents, parallel processing, large language models, information retrieval
<br />
ParallelSearch is a novel reinforcement learning framework that enables large language models to identify and execute multiple search operations concurrently, addressing the computational bottleneck of sequential processing in reasoning-augmented search agents. By recognizing parallelizable query structures and incentivizing the identification of independent query components, ParallelSearch achieves an average performance gain of 2.9% across multiple question-answering benchmarks. In particular, on parallelizable questions, the method improves performance by 12.7% while reducing LLM calls by 30.4% compared to sequential approaches. The approach introduces dedicated reward functions that consider correctness, query decomposition quality, and parallel execution benefits, enhancing both efficiency and accuracy in multi-step information retrieval tasks. 
<br /><br />Summary: <div>
arXiv:2508.09303v1 Announce Type: new 
Abstract: Reasoning-augmented search agents such as Search-R1, trained via reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable capabilities in multi-step information retrieval from external knowledge sources. These agents address the limitations of their parametric memory by dynamically gathering relevant facts to address complex reasoning tasks. However, existing approaches suffer from a fundamental architectural limitation: they process search queries strictly sequentially, even when handling inherently parallelizable and logically independent comparisons. This sequential bottleneck significantly constrains computational efficiency, particularly for queries that require multiple entity comparisons. To address this critical limitation, we propose ParallelSearch, a novel reinforcement learning framework that empowers large language models (LLMs) to recognize parallelizable query structures and execute multiple search operations concurrently. Our approach introduces dedicated reward functions that incentivize the identification of independent query components while preserving answer accuracy through jointly considering correctness, query decomposition quality, and parallel execution benefits. Comprehensive experiments demonstrate that ParallelSearch outperforms state-of-the-art baselines by an average performance gain of 2.9% across seven question-answering benchmarks. Notably, on parallelizable questions, our method achieves a 12.7% performance improvement while requiring only 69.6% of the LLM calls compared to sequential approaches.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Rare Disease Named Entity Recognition</title>
<link>https://arxiv.org/abs/2508.09323</link>
<guid>https://arxiv.org/abs/2508.09323</guid>
<content:encoded><![CDATA[
<div> NER, rare diseases, GPT-4o, prompt-based strategies, low-resource<br />
<br />
Summary: 
Named Entity Recognition (NER) in the rare disease domain faces challenges due to limited labeled data, ambiguity between entity types, and long-tail distributions. This study evaluates GPT-4o for rare disease NER using various prompt-based strategies like zero-shot prompting and task-level fine-tuning. A structured prompting framework with domain-specific knowledge is designed, along with semantically guided few-shot example selection methods to improve performance. GPT-4o outperforms BioClinicalBERT, with task-level fine-tuning achieving new state-of-the-art results. Few-shot prompting is cost-effective, while RAG offers minimal additional benefit. An error taxonomy identifies common failure modes like boundary drift and type confusion, suggesting areas for improvement in post-processing and hybrid refinement. The study concludes that prompt-optimized language models can be effective alternatives to traditional supervised models in biomedical NER, especially in rare disease situations with limited annotated data. <div>
arXiv:2508.09323v1 Announce Type: new 
Abstract: Named Entity Recognition (NER) in the rare disease domain poses unique challenges due to limited labeled data, semantic ambiguity between entity types, and long-tail distributions. In this study, we evaluate the capabilities of GPT-4o for rare disease NER under low-resource settings, using a range of prompt-based strategies including zero-shot prompting, few-shot in-context learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We design a structured prompting framework that encodes domain-specific knowledge and disambiguation rules for four entity types. We further introduce two semantically guided few-shot example selection methods to improve in-context performance while reducing labeling effort. Experiments on the RareDis Corpus show that GPT-4o achieves competitive or superior performance compared to BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art (SOTA) results. Cost-performance analysis reveals that few-shot prompting delivers high returns at low token budgets, while RAG offers marginal additional benefit. An error taxonomy highlights common failure modes such as boundary drift and type confusion, suggesting opportunities for post-processing and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can serve as effective, scalable alternatives to traditional supervised models in biomedical NER, particularly in rare disease applications where annotated data is scarce.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TEN: Table Explicitization, Neurosymbolically</title>
<link>https://arxiv.org/abs/2508.09324</link>
<guid>https://arxiv.org/abs/2508.09324</guid>
<content:encoded><![CDATA[
<div> neurosymbolic approach, tabular data extraction, semistructured text, language model, hallucinations<br />
<br />
Summary: 
The article introduces the TEN approach, a neurosymbolic method for extracting tabular data from semistructured text. Traditional neural methods struggle with inconsistent delimiters and hard constraints, leading to errors like hallucinations. TEN combines Structural Decomposition prompting with a large language model (LLM) to create an initial table, which is then checked by a symbolic checker for accuracy and errors. A critique-LLM provides guidance for fixing the table in a self-debug loop. Experimental results show that TEN outperforms purely neural methods in terms of exact match accuracy and hallucination rates. A user study also confirms that tables produced by TEN are more accurate and preferred for ease of verification and correction. Participants favored the TEN method in over 60% of cases, highlighting its effectiveness in extracting tabular data from semistructured text. 
<br /><br />Summary: <div>
arXiv:2508.09324v1 Announce Type: new 
Abstract: We present a neurosymbolic approach, TEN, for extracting tabular data from semistructured input text. This task is particularly challenging for text input that does not use special delimiters consistently to separate columns and rows. Purely neural approaches perform poorly due to hallucinations and their inability to enforce hard constraints. TEN uses Structural Decomposition prompting - a specialized chain-of-thought prompting approach - on a large language model (LLM) to generate an initial table, and thereafter uses a symbolic checker to evaluate not only the well-formedness of that table, but also detect cases of hallucinations or forgetting. The output of the symbolic checker is processed by a critique-LLM to generate guidance for fixing the table, which is presented to the original LLM in a self-debug loop. Our extensive experiments demonstrate that TEN significantly outperforms purely neural baselines across multiple datasets and metrics, achieving significantly higher exact match accuracy and substantially reduced hallucination rates. A 21-participant user study further confirms that TEN's tables are rated significantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are consistently preferred for ease of verification and correction, with participants favoring our method in over 60% of the cases.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Neural Emotion Patterns through Natural Language Processing Embeddings</title>
<link>https://arxiv.org/abs/2508.09337</link>
<guid>https://arxiv.org/abs/2508.09337</guid>
<content:encoded><![CDATA[
<div> semantic representations, emotional content, brain regions, emotional processing, computational framework

Summary: 
- The study introduces a computational framework to map emotional content in text to specific brain regions without the need for neuroimaging.
- OpenAI's text-embedding-ada-002 was used to generate semantic representations, which were then clustered to identify emotional groups and mapped to 18 brain regions associated with emotional processing.
- Three experiments were conducted, comparing conversational data from healthy and depressed subjects, analyzing the GoEmotions dataset, and comparing human-written text with responses from a large language model.
- Results showed neuroanatomically plausible mappings with high spatial specificity, revealing greater limbic engagement in depressed subjects and successful differentiation of discrete emotions.
- The study suggests that the approach is cost-effective and scalable, enabling large-scale analysis of natural language to distinguish between clinical populations and provide a benchmark for evaluating AI emotional expression. 

<br /><br />Summary: <div>
arXiv:2508.09337v1 Announce Type: new 
Abstract: Understanding how emotional expression in language relates to brain function is a challenge in computational neuroscience and affective computing. Traditional neuroimaging is costly and lab-bound, but abundant digital text offers new avenues for emotion-brain mapping. Prior work has largely examined neuroimaging-based emotion localization or computational text analysis separately, with little integration. We propose a computational framework that maps textual emotional content to anatomically defined brain regions without requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate high-dimensional semantic representations, apply dimensionality reduction and clustering to identify emotional groups, and map them to 18 brain regions linked to emotional processing. Three experiments were conducted: i) analyzing conversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to compare mapping patterns, ii) applying the method to the GoEmotions dataset and iii) comparing human-written text with large language model (LLM) responses to assess differences in inferred brain activation. Emotional intensity was scored via lexical analysis. Results showed neuroanatomically plausible mappings with high spatial specificity. Depressed subjects exhibited greater limbic engagement tied to negative affect. Discrete emotions were successfully differentiated. LLM-generated text matched humans in basic emotion distribution but lacked nuanced activation in empathy and self-referential regions (medial prefrontal and posterior cingulate cortex). This cost-effective, scalable approach enables large-scale analysis of naturalistic language, distinguishes between clinical populations, and offers a brain-based benchmark for evaluating AI emotional expression.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains</title>
<link>https://arxiv.org/abs/2508.09349</link>
<guid>https://arxiv.org/abs/2508.09349</guid>
<content:encoded><![CDATA[
<div> AI, expert consensus, Delphi, consensus development, human-AI hybrid framework <br />
Summary: The study introduces a Human-AI Hybrid Delphi (HAH-Delphi) framework to enhance expert consensus development by combining a generative AI model, senior human experts, and structured facilitation. The framework was tested in three phases, demonstrating successful replication of published expert consensus conclusions, high directional agreement with senior human experts, and achievement of >90% consensus coverage with compact panels. The AI provided literature-grounded support, aiding in divergence resolution and accelerating thematic saturation. The HAH-Delphi framework offers a flexible and scalable approach for generating high-quality, context-sensitive consensus, with successful application across health, coaching, and performance science. It provides a foundation for generating personalized guidance and published consensus frameworks at scale. <div>
arXiv:2508.09349v1 Announce Type: new 
Abstract: Expert consensus plays a critical role in domains where evidence is complex, conflicting, or insufficient for direct prescription. Traditional methods, such as Delphi studies, consensus conferences, and systematic guideline synthesis, offer structure but face limitations including high panel burden, interpretive oversimplification, and suppression of conditional nuance. These challenges are now exacerbated by information overload, fragmentation of the evidence base, and increasing reliance on publicly available sources that lack expert filtering. This study introduces and evaluates a Human-AI Hybrid Delphi (HAH-Delphi) framework designed to augment expert consensus development by integrating a generative AI model (Gemini 2.5 Pro), small panels of senior human experts, and structured facilitation. The HAH-Delphi was tested in three phases: retrospective replication, prospective comparison, and applied deployment in two applied domains (endurance training and resistance and mixed cardio/strength training). The AI replicated 95% of published expert consensus conclusions in Phase I and showed 95% directional agreement with senior human experts in Phase II, though it lacked experiential and pragmatic nuance. In Phase III, compact panels of six senior experts achieved >90% consensus coverage and reached thematic saturation before the final participant. The AI provided consistent, literature-grounded scaffolding that supported divergence resolution and accelerated saturation. The HAH-Delphi framework offers a flexible, scalable approach for generating high-quality, context-sensitive consensus. Its successful application across health, coaching, and performance science confirms its methodological robustness and supports its use as a foundation for generating conditional, personalised guidance and published consensus frameworks at scale.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling</title>
<link>https://arxiv.org/abs/2508.09350</link>
<guid>https://arxiv.org/abs/2508.09350</guid>
<content:encoded><![CDATA[
<div> Keywords: textless spoken language models, acoustic information, generative models, semantic tokens, linguistic likelihood benchmarks 

Summary:
Textless spoken language models (SLMs) are generative models of speech that do not rely on text supervision. Existing textless SLMs predict semantic tokens to generate speech and rely on a separate vocoder for acoustic information. However, these models lack control over acoustic details. In this work, a new approach is proposed to jointly model linguistic and acoustic information by generating semantic tokens and a continuous real-valued representation of the acoustic frame. By predicting multiple future semantic tokens, the proposed approach preserves linguistic information while providing better acoustic detail in prompted generation. The model achieves performance comparable to existing models in terms of linguistic likelihood benchmarks. This joint modeling of linguistic and acoustic information enhances the overall quality of generated speech. <div>
arXiv:2508.09350v1 Announce Type: new 
Abstract: Textless spoken language models (SLMs) are generative models of speech that do not rely on text supervision. Most textless SLMs learn to predict the next semantic token, a discrete representation of linguistic content, and rely on a separate vocoder to add acoustic information to the generated speech. Such models have no access to acoustic context and no built-in control over acoustic details. In this work, we propose to jointly model linguistic and acoustic information by generating semantic tokens and a continuous real-valued representation of the acoustic frame. We use a flow-matching objective to predict the continuous vector conditioned on the semantic tokens. We study the design space of this approach and find that predicting multiple future semantic tokens helps preserve linguistic information. Our approach achieves comparable performance to existing models in terms of linguistic likelihood benchmarks, while providing better acoustic detail in prompted generation.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification</title>
<link>https://arxiv.org/abs/2508.09378</link>
<guid>https://arxiv.org/abs/2508.09378</guid>
<content:encoded><![CDATA[
<div> advancements, large language models, NLP tasks, prompt optimization, APIO, Grammatical Error Correction, Text Simplification

Summary: 
Recent advancements in large language models have enabled various natural language processing tasks to be performed through prompt-based interactions. Different approaches have been proposed to engineer prompts that effectively enable these models to perform specific tasks, such as chain-of-thought prompting. Automatic prompt optimization methods have been developed to refine seed prompts in settings with well-defined metrics. In this study, APIO, a prompt induction and optimization approach, achieves a new state-of-the-art performance for Grammatical Error Correction and Text Simplification tasks without manually specified seed prompts. The data, code, prompts, and outputs are publicly available. <div>
arXiv:2508.09378v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have enabled a wide range of natural language processing (NLP) tasks to be performed through simple prompt-based interactions. Consequently, several approaches have been proposed to engineer prompts that most effectively enable LLMs to perform a given task (e.g., chain-of-thought prompting). In settings with a well-defined metric to optimize model performance, automatic prompt optimization (APO) methods have been developed to refine a seed prompt. Advancing this line of research, we propose APIO, a simple but effective prompt induction and optimization approach for the tasks of Grammatical Error Correction (GEC) and Text Simplification, without relying on manually specified seed prompts. APIO achieves a new state-of-the-art performance for purely LLM-based prompting methods on these tasks. We make our data, code, prompts, and outputs publicly available.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models</title>
<link>https://arxiv.org/abs/2508.09403</link>
<guid>https://arxiv.org/abs/2508.09403</guid>
<content:encoded><![CDATA[
<div> Abbreviated Column Names, Downstream Data Tasks, Enterprise/Science Domains, Accuracy Measures, LLM-based Solution<br />
Summary:<br />
Expanding abbreviated column names in tables is crucial for various data tasks. This study presents three key contributions to enhance existing methods. Firstly, it introduces four new real-world datasets with abbreviations commonly found in enterprise and science domains, addressing limitations of synthetic data used in prior research. Secondly, the study identifies flaws in accuracy measures used in previous work and introduces new synonym-aware measures that more accurately capture the correctness of expansions. Lastly, the development of Columbo, a sophisticated LLM-based solution, leverages context, rules, reasoning, and token-level analysis to significantly outperform the leading solution, NameGuess, by 4-29% across five datasets. Columbo's effectiveness has been demonstrated in production on EDI, a prominent data portal for environmental sciences.<br /><br />Summary: <div>
arXiv:2508.09403v1 Announce Type: new 
Abstract: Expanding the abbreviated column names of tables, such as ``esal'' to ``employee salary'', is critical for numerous downstream data tasks. This problem arises in enterprises, domain sciences, government agencies, and more. In this paper we make three contributions that significantly advances the state of the art. First, we show that synthetic public data used by prior work has major limitations, and we introduce 4 new datasets in enterprise/science domains, with real-world abbreviations. Second, we show that accuracy measures used by prior work seriously undercount correct expansions, and we propose new synonym-aware measures that capture accuracy much more accurately. Finally, we develop Columbo, a powerful LLM-based solution that exploits context, rules, chain-of-thought reasoning, and token-level analysis. Extensive experiments show that Columbo significantly outperforms NameGuess, the current most advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in production on EDI, a major data portal for environmental sciences.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech</title>
<link>https://arxiv.org/abs/2508.09430</link>
<guid>https://arxiv.org/abs/2508.09430</guid>
<content:encoded><![CDATA[
<div> Zipformer, language identification, bilingual environments, Mandarin, English
Summary:
- The paper addresses the challenge of code-switching and language identification in bilingual environments by utilizing the Zipformer model.
- Zipformer effectively encodes language characteristics in imbalanced speech containing Mandarin and English.
- The selection methodology of inner layers in Zipformer is crucial for extracting embeddings for language identification.
- The study demonstrates the robustness of Zipformer across different backends for language identification tasks.
- The approach achieves a Balanced Accuracy (BAC) of 81.89%, a significant improvement over baseline language identification methods, showcasing the potential of transformer encoder architecture in practical scenarios.
<br /><br />Summary: <div>
arXiv:2508.09430v1 Announce Type: new 
Abstract: Code-switching and language identification in child-directed scenarios present significant challenges, particularly in bilingual environments. This paper addresses this challenge by using Zipformer to handle the nuances of speech, which contains two imbalanced languages, Mandarin and English, in an utterance. This work demonstrates that the internal layers of the Zipformer effectively encode the language characteristics, which can be leveraged in language identification. We present the selection methodology of the inner layers to extract the embeddings and make a comparison with different back-ends. Our analysis shows that Zipformer is robust across these backends. Our approach effectively handles imbalanced data, achieving a Balanced Accuracy (BAC) of 81.89%, a 15.47% improvement over the language identification baseline. These findings highlight the potential of the transformer encoder architecture model in real scenarios.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text</title>
<link>https://arxiv.org/abs/2508.09450</link>
<guid>https://arxiv.org/abs/2508.09450</guid>
<content:encoded><![CDATA[
<div> Keywords: charts, natural language generation, bias, VLMs, debiasing

Summary:
This research focuses on the potential biases present in large Vision-Language Models (VLMs) when generating textual summaries of charts. The study evaluated 6,000 chart-country pairs across different VLMs and found a tendency for more positive descriptions to be generated for high-income countries compared to middle- or low-income countries. The analysis identified biases in models like GPT-4o-mini, Gemini-1.5-Flash, and Phi-3.5. The researchers attempted prompt-based debiasing techniques during inference but found them only partially effective. The study underscores the complexity of addressing bias in VLM-generated chart summaries and highlights the need for more robust debiasing strategies. The code and dataset used in the study are publicly available. 

<br /><br />Summary: <div>
arXiv:2508.09450v1 Announce Type: new 
Abstract: Charts are very common for exploring data and communicating insights, but extracting key takeaways from charts and articulating them in natural language can be challenging. The chart-to-text task aims to automate this process by generating textual summaries of charts. While with the rapid advancement of large Vision-Language Models (VLMs), we have witnessed great progress in this domain, little to no attention has been given to potential biases in their outputs. This paper investigates how VLMs can amplify geo-economic biases when generating chart summaries, potentially causing societal harm. Specifically, we conduct a large-scale evaluation of geo-economic biases in VLM-generated chart summaries across 6,000 chart-country pairs from six widely used proprietary and open-source models to understand how a country's economic status influences the sentiment of generated summaries. Our analysis reveals that existing VLMs tend to produce more positive descriptions for high-income countries compared to middle- or low-income countries, even when country attribution is the only variable changed. We also find that models such as GPT-4o-mini, Gemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further explore inference-time prompt-based debiasing techniques using positive distractors but find them only partially effective, underscoring the complexity of the issue and the need for more robust debiasing strategies. Our code and dataset are publicly available here.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User-centric Subjective Leaderboard by Customizable Reward Modeling</title>
<link>https://arxiv.org/abs/2508.09463</link>
<guid>https://arxiv.org/abs/2508.09463</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, User-Centric Subjective Leaderboard, Customizable Reward Models, human preferences, generalization capabilities

Summary:
Existing benchmarks for large language models (LLMs) traditionally focus on verifiable tasks, limiting their practical utility. To address this gap, the User-Centric Subjective Leaderboard (USL) was introduced to provide a dynamic ranking of LLMs based on real-world scenarios and human preferences. Through an analysis of more than 10K subjective queries, diverse and contradictory human preferences were identified, challenging existing reward models. Customizable Reward Models (CRMs) were developed to address these challenges, showcasing superior performance compared to leading models like GPT-4.1 and Gemini-2.5-pro with only 4B parameters. The USL, powered by CRMs, demonstrates strong negative correlations to contradictory preferences, highlighting its efficacy in capturing user preferences. 

<br /><br />Summary: <div>
arXiv:2508.09463v1 Announce Type: new 
Abstract: Existing benchmarks for large language models (LLMs) predominantely focus on assessing their capabilities through verifiable tasks. Such objective and static benchmarks offer limited utility for practical LLM selection, making it difficult for users to find suitable models for their individual needs. To bridge this gap, we present the first User-Centric Subjective Leaderboard (USL), which provides a preference-driven, dynamic ranking of LLMs across diverse real-world scenarios. Our work is built upon a thorough investigation of real human preference data, involving more than 10K subjective queries. Our investigation reveals significant diversity and contradictions in human preferences, which limit the effectiveness of state-of-the-art reward models. To address this, we introduce Customizable Reward Models (CRMs). With only 4B parameters, our CRM surpasses the performance of leading models such as GPT-4.1 and Gemini-2.5-pro, showing exceptional generalization capabilities across new topics and criteria. The USL, powered by CRMs, exhibits strong negative correlations to contradictory preferences.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Facts at Scale with Active Reading</title>
<link>https://arxiv.org/abs/2508.09494</link>
<guid>https://arxiv.org/abs/2508.09494</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, Active Reading, knowledge retention, training, data augmentation

Summary:
Active Reading is proposed as a framework to enhance the knowledge retention and recall capabilities of Large Language Models (LLMs) by training models to study a given set of material with self-generated learning strategies. Models trained with Active Reading in expert domains show significantly improved knowledge absorption compared to vanilla finetuning and other data augmentation techniques. Expert 8B models trained using Active Reading achieve notable performance on benchmark tasks related to Wikipedia-grounded SimpleQA and FinanceBench datasets. Additionally, Active Reading can be applied at pre-training scale to build more factual models, as demonstrated by the release of Meta WikiExpert-8B, a Wikipedia-expert model outperforming models with hundreds of billions of parameters on factual question answering tasks. <div>
arXiv:2508.09494v1 Announce Type: new 
Abstract: LLMs are known to store vast amounts of knowledge in their parametric memory. However, learning and recalling facts from this memory is known to be unreliable, depending largely on the prevalence of particular facts in the training data and other factors which are poorly understood. Practitioners are lacking tools which will allow them to ensure that the models learn a given body of knowledge reliably and consistently. To this end, we propose Active Reading: a framework where we train models to study a given set of material with self-generated learning strategies. First, we demonstrate models trained with Active Reading on expert domains absorb significantly more knowledge than vanilla finetuning and other data augmentations. We train expert 8B models that achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla finetuning) by applying Active Reading to the source documents for each benchmark. Finally, we show that Active Reading can be utilized at pre-training scale to build more factual models. As a demonstration of this, we release Meta WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens, which outcompetes models with hundreds of billions of parameters on factual QA.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2508.09497</link>
<guid>https://arxiv.org/abs/2508.09497</guid>
<content:encoded><![CDATA[
<div> Dynamic Passage Selector, RAG systems, retrieval-augmented generation, multi-hop queries, MuSiQue dataset

Summary:
The article introduces the Dynamic Passage Selector (DPS) framework for improving the performance of retrieval-augmented generation (RAG) systems. DPS treats passage selection as a supervised learning problem and dynamically selects the most relevant set of passages for generation by capturing inter-passage dependencies. This approach addresses the limitations of traditional reranking modules in RAG systems, which struggle with complex multi-hop queries. DPS does not require modifications to the standard RAG pipeline and outperforms state-of-the-art rerankers and fine-tuning methods in comprehensive evaluations across various benchmarks. Particularly on the MuSiQue dataset, DPS significantly improves the F1-score, showcasing its effectiveness in enhancing reasoning capabilities in complex RAG scenarios. By enabling adaptive evidence selection, DPS provides a robust solution for improving the performance of RAG systems in synthesizing evidence across multiple documents. 

<br /><br />Summary: <div>
arXiv:2508.09497v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) systems are often bottlenecked by their reranking modules, which typically score passages independently and select a fixed Top-K size. This approach struggles with complex multi-hop queries that require synthesizing evidence across multiple documents, creating a trade-off where small K values omit crucial information and large K values introduce noise. To address this, we introduce the Dynamic Passage Selector (DPS), a novel reranking framework that treats passage selection as a supervised learning problem. Unlike traditional point-wise or list-wise methods, DPS is fine-tuned to capture inter-passage dependencies and dynamically select the most relevant set of passages for generation. As a seamless plug-and-play module, DPS requires no modifications to the standard RAG pipeline. Comprehensive evaluations on five benchmarks show that DPS consistently outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results demonstrate that by enabling adaptive evidence selection, DPS substantially enhances reasoning capabilities in complex RAG scenarios.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation</title>
<link>https://arxiv.org/abs/2508.09515</link>
<guid>https://arxiv.org/abs/2508.09515</guid>
<content:encoded><![CDATA[
<div> approach, ABSA model, large language model, pseudo-labelled data, cross-lingual sentiment analysis
<br />
The paper introduces a novel approach for cross-lingual aspect-based sentiment analysis (ABSA) that eliminates the need for translation tools by using a large language model (LLM) to generate high-quality pseudo-labelled data in the target language. The method involves training an ABSA model on unlabelled target language data, prompting the LLM to generate natural sentences based on the ABSA model predictions, and fine-tuning the ABSA model on the resulting pseudo-labelled dataset. The proposed approach outperforms previous translation-based methods across multiple languages and backbone models, showcasing its effectiveness. Additionally, the framework supports generative models, with findings demonstrating that fine-tuned LLMs yield superior results compared to smaller multilingual models.
<br /><br />Summary: <div>
arXiv:2508.09515v1 Announce Type: new 
Abstract: Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed sentiment analysis in a target language by transferring knowledge from a source language with available annotated data. Most existing methods depend heavily on often unreliable translation tools to bridge the language gap. In this paper, we propose a new approach that leverages a large language model (LLM) to generate high-quality pseudo-labelled data in the target language without the need for translation tools. First, the framework trains an ABSA model to obtain predictions for unlabelled target language data. Next, LLM is prompted to generate natural sentences that better represent these noisy predictions than the original text. The ABSA model is then further fine-tuned on the resulting pseudo-labelled dataset. We demonstrate the effectiveness of this method across six languages and five backbone models, surpassing previous state-of-the-art translation-based approaches. The proposed framework also supports generative models, and we show that fine-tuned LLMs outperform smaller multilingual models.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges</title>
<link>https://arxiv.org/abs/2508.09516</link>
<guid>https://arxiv.org/abs/2508.09516</guid>
<content:encoded><![CDATA[
<div> aspect-based sentiment analysis, ABSA, cross-lingual, aspect term extraction, sentiment classification <br />
Summary: <br />
Aspect-based sentiment analysis (ABSA) focuses on understanding opinions at the aspect level, including sentiment towards specific aspect terms, categories, and opinions. While most ABSA research has been in monolingual settings, cross-lingual ABSA, transferring knowledge from resource-rich to low-resource languages, remains under-explored. This paper provides a comprehensive survey of cross-lingual ABSA, summarizing key tasks like aspect term extraction and sentiment classification, along with compound tasks. It reviews datasets, modelling paradigms, and cross-lingual transfer methods used in solving these tasks. The paper also examines how existing work in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to cross-lingual ABSA development and highlights main challenges. The paper suggests future research directions to advance cross-lingual ABSA systems. <div>
arXiv:2508.09516v1 Announce Type: new 
Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task that focuses on understanding opinions at the aspect level, including sentiment towards specific aspect terms, categories, and opinions. While ABSA research has seen significant progress, much of the focus has been on monolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from resource-rich languages (such as English) to low-resource languages, remains an under-explored area, with no systematic review of the field. This paper aims to fill that gap by providing a comprehensive survey of cross-lingual ABSA. We summarize key ABSA tasks, including aspect term extraction, aspect sentiment classification, and compound tasks involving multiple sentiment elements. Additionally, we review the datasets, modelling paradigms, and cross-lingual transfer methods used to solve these tasks. We also examine how existing work in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to the development of cross-lingual ABSA. Finally, we highlight the main challenges and suggest directions for future research to advance cross-lingual ABSA systems.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval</title>
<link>https://arxiv.org/abs/2508.09517</link>
<guid>https://arxiv.org/abs/2508.09517</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot system, fact-checked claim retrieval, text embeddings, language models, cosine similarity

Summary:<br /><br />This paper introduces a zero-shot system for retrieving fact-checked claims. The authors utilized large language models to generate text embeddings, combining multiple models to optimize performance. Their approach placed 7th in monolingual and 9th in cross-lingual subtasks. English translations were used as input for text embedding models due to unsatisfactory results from multilingual models. Relevant claims were identified for each post by calculating cosine similarity based on embeddings. The NVIDIA NV-Embed-v2 model proved most effective, with some languages benefiting from model combinations such as NV-Embed & GPT or Mistral.<br /><br />Summary: <div>
arXiv:2508.09517v1 Announce Type: new 
Abstract: This paper presents a zero-shot system for fact-checked claim retrieval. We employed several state-of-the-art large language models to obtain text embeddings. The models were then combined to obtain the best possible result. Our approach achieved 7th place in monolingual and 9th in cross-lingual subtasks. We used only English translations as an input to the text embedding models since multilingual models did not achieve satisfactory results. We identified the most relevant claims for each post by leveraging the embeddings and measuring cosine similarity. Overall, the best results were obtained by the NVIDIA NV-Embed-v2 model. For some languages, we benefited from model combinations (NV-Embed & GPT or Mistral).
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation</title>
<link>https://arxiv.org/abs/2508.09521</link>
<guid>https://arxiv.org/abs/2508.09521</guid>
<content:encoded><![CDATA[
<div> Keywords: emotional support, conversational reasoning, psychological principles, reinforcement learning, empathy

Summary: 
Emotional support conversations play a crucial role in promoting emotional well-being, but existing models often lack deep empathetic reasoning rooted in psychological principles. To address this gap, the concept of controllable empathetic reasoning is proposed, combining natural language reasoning with structured psychological steps. A dataset annotated with reasoning correctness and response preferences is created to facilitate this approach. Reinforcement learning is employed with a unified process-outcome reward model to provide precise feedback and improve training. To prevent response repetitiveness, personality-based dialogue rewriting and a redundancy-aware reward reweighting strategy are introduced. These enhancements significantly enhance the emotional support abilities of models, moving closer to the development of empathetic support systems that resemble human interactions.

<br /><br />Summary: <div>
arXiv:2508.09521v1 Announce Type: new 
Abstract: Emotional support conversations are crucial for promoting emotional well-being, yet current models often lack deep empathetic reasoning grounded in psychological principles. To address this, we propose controllable empathetic reasoning, which combines natural language reasoning with structured psychological steps. We construct a fine-grained dataset annotated with reasoning correctness and response preferences to enable this capability. To further enhance training, we employ reinforcement learning with a unified process-outcome reward model that delivers precise feedback. To mitigate response repetitiveness from entropy collapse, we introduce personality-based dialogue rewriting and a redundancy-aware reward reweighting strategy. Our approach significantly improves model's emotional support ability, advancing the development of empathetic, human-like support systems.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage</title>
<link>https://arxiv.org/abs/2508.09603</link>
<guid>https://arxiv.org/abs/2508.09603</guid>
<content:encoded><![CDATA[
<div> Membership Inference Attack, Language Models, N-Gram Coverage Attack, Black-Box Models, Privacy Protections

Summary:
The paper introduces the N-Gram Coverage Attack as a membership inference attack that works solely based on text outputs from a target model, making it suitable for black-box models like GPT-4. The attack leverages the observation that models tend to memorize text patterns from their training data, using n-gram overlap metrics to determine likely membership. The attack outperforms other black-box methods and achieves similar performance to white-box attacks. The success rate of the attack scales with the compute budget, improving as more sequences are generated from the model. The study also shows that more recent models like GPT-4o exhibit increased privacy protections, indicating a trend towards better security measures in language models. 

<br /><br />Summary: <div>
arXiv:2508.09603v1 Announce Type: new 
Abstract: Membership inference attacks serves as useful tool for fair use of language models, such as detecting potential copyright infringement and auditing data leakage. However, many current state-of-the-art attacks require access to models' hidden states or probability distribution, which prevents investigation into more widely-used, API-access only models like GPT-4. In this work, we introduce N-Gram Coverage Attack, a membership inference attack that relies solely on text outputs from the target model, enabling attacks on completely black-box models. We leverage the observation that models are more likely to memorize and subsequently generate text patterns that were commonly observed in their training data. Specifically, to make a prediction on a candidate member, N-Gram Coverage Attack first obtains multiple model generations conditioned on a prefix of the candidate. It then uses n-gram overlap metrics to compute and aggregate the similarities of these outputs with the ground truth suffix; high similarities indicate likely membership. We first demonstrate on a diverse set of existing benchmarks that N-Gram Coverage Attack outperforms other black-box methods while also impressively achieving comparable or even better performance to state-of-the-art white-box attacks - despite having access to only text outputs. Interestingly, we find that the success rate of our method scales with the attack compute budget - as we increase the number of sequences generated from the target model conditioned on the prefix, attack performance tends to improve. Having verified the accuracy of our method, we use it to investigate previously unstudied closed OpenAI models on multiple domains. We find that more recent models, such as GPT-4o, exhibit increased robustness to membership inference, suggesting an evolving trend toward improved privacy protections.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian</title>
<link>https://arxiv.org/abs/2508.09622</link>
<guid>https://arxiv.org/abs/2508.09622</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, academic integrity, scientific publishing, detection resources, Russian.

Summary: 
The article discusses the challenges posed by the advancement of large language models (LLMs) in text generation, particularly in distinguishing between human- and AI-generated content. To address this issue in the context of academic integrity and scientific publishing in Russian, the AINL-Eval 2025 Shared Task was introduced. The task focuses on detecting AI-generated scientific abstracts and includes a dataset of human-written and AI-generated abstracts across various scientific domains. The task aims to challenge participants to develop solutions that can generalize to unseen domains and models. With 10 teams and 159 submissions, the task attracted strong performance in identifying AI-generated content. A continuous shared task platform has been established to promote ongoing research in this area. The dataset and platform are publicly available for further research and progress in the field. 

<br /><br />Summary: 
- Introduction of AINL-Eval 2025 Shared Task for detecting AI-generated scientific abstracts in Russian.
- Dataset includes human-written abstracts and AI-generated counterparts from state-of-the-art LLMs.
- Objective to challenge participants to develop solutions for generalizing to unseen domains and models.
- Organized in two phases with participation from 10 teams and 159 submissions.
- Establishment of a continuous shared task platform for fostering ongoing research and progress in the field. <div>
arXiv:2508.09622v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has revolutionized text generation, making it increasingly difficult to distinguish between human- and AI-generated content. This poses a significant challenge to academic integrity, particularly in scientific publishing and multilingual contexts where detection resources are often limited. To address this critical gap, we introduce the AINL-Eval 2025 Shared Task, specifically focused on the detection of AI-generated scientific abstracts in Russian. We present a novel, large-scale dataset comprising 52,305 samples, including human-written abstracts across 12 diverse scientific domains and AI-generated counterparts from five state-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and GigaChat-Lite). A core objective of the task is to challenge participants to develop robust solutions capable of generalizing to both (i) previously unseen scientific domains and (ii) models not included in the training data. The task was organized in two phases, attracting 10 teams and 159 submissions, with top systems demonstrating strong performance in identifying AI-generated content. We also establish a continuous shared task platform to foster ongoing research and long-term progress in this important area. The dataset and platform are publicly available at https://github.com/iis-research-team/AINL-Eval-2025.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Diversity in Language Models: When Temperature Fails, Change the Loss</title>
<link>https://arxiv.org/abs/2508.09654</link>
<guid>https://arxiv.org/abs/2508.09654</guid>
<content:encoded><![CDATA[
<div> temperature, language models, diversity, Precision, Recall

Summary:
Increasing diversity in language models is a challenging task. Adjusting the decoding temperature is a common method, but its effectiveness varies. Lowering the temperature can enhance Precision, while increasing it often does not improve Recall. The key lies in training the model towards better coverage. To achieve a better balance between Precision and Recall, the authors suggest rethinking loss functions in language models using the Precision-Recall framework. By combining this framework with negative log-likelihood training and temperature scaling, they were able to significantly improve the trade-off between Precision and Recall. These findings suggest a new direction for developing more versatile and robust language modeling techniques.<br /><br />Summary: <div>
arXiv:2508.09654v1 Announce Type: new 
Abstract: Increasing diversity in language models is a challenging yet essential objective. A common approach is to raise the decoding temperature. In this work, we investigate this approach through a simplistic yet common case to provide insights into why decreasing temperature can improve quality (Precision), while increasing it often fails to boost coverage (Recall). Our analysis reveals that for a model to be effectively tunable through temperature adjustments, it must be trained toward coverage. To address this, we propose rethinking loss functions in language models by leveraging the Precision-Recall framework. Our results demonstrate that this approach achieves a substantially better trade-off between Precision and Recall than merely combining negative log-likelihood training with temperature scaling. These findings offer a pathway toward more versatile and robust language modeling techniques.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization</title>
<link>https://arxiv.org/abs/2508.09662</link>
<guid>https://arxiv.org/abs/2508.09662</guid>
<content:encoded><![CDATA[
<div> EffiEval, large language models, benchmarking, Model Utility Index, evaluation reliability <br />
Summary: <br />
EffiEval introduces a training-free method for efficient benchmarking of large language models (LLMs). The approach addresses data redundancy while maintaining high evaluation reliability by focusing on representativeness, fairness, and generalizability. It adaptively selects representative subsets based on the Model Utility Index (MUI), ensuring comprehensive coverage of model capabilities while remaining independent of model performance for unbiased sample selection. Experiments show strong ranking consistency with full-dataset evaluation using only a small fraction of data, demonstrating the method's efficiency. EffiEval offers flexibility in size, enabling users to balance evaluation efficiency and representativeness according to their specific needs. This approach provides a practical and generalizable solution for reliable, fair, and efficient evaluation in the era of LLMs. <br /> <div>
arXiv:2508.09662v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) and the development of increasingly large and diverse evaluation benchmarks have introduced substantial computational challenges for model assessment. In this paper, we present EffiEval, a training-free approach for efficient benchmarking that effectively addresses data redundancy while maintaining high evaluation reliability. Our method is specifically designed to meet three key criteria for high-quality evaluation: representativeness, by ensuring comprehensive coverage of model capabilities; fairness, by remaining independent of model performance during sample selection to avoid bias; and generalizability, by enabling flexible transfer across datasets and model families without reliance on large-scale evaluation data. Unlike traditional methods that rely on absolute performance or require extensive evaluation data, our approach adaptively selects high-quality representative subsets based on the Model Utility Index (MUI). Extensive experiments on multiple public benchmarks and diverse LLMs demonstrate that EffiEval achieves strong ranking consistency with full-dataset evaluation using only a small fraction of the original data. Furthermore, our method is flexible and scalable in size, allowing users to balance evaluation efficiency and representativeness according to specific needs. Overall, EffiEval provides a practical and generalizable solution for reliable, fair, and efficient evaluation in the era of LLMs.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation</title>
<link>https://arxiv.org/abs/2508.09666</link>
<guid>https://arxiv.org/abs/2508.09666</guid>
<content:encoded><![CDATA[
<div> distillation, safety, language models, reasoning, Slow Tuning, Low-Entropy Masking <br />
Summary:<br />
The paper introduces a method called SLowED for distilling Small Language Models (SLMs) that focuses on maintaining both safety and reasoning capabilities. It addresses the negative impacts on safety observed in previous distillation methods. SLowED consists of two modules, Slow Tuning and Low-Entropy Masking, which work to optimize model weights and exclude unnecessary learning targets during fine-tuning. Experiments on various SLMs and benchmarks show that SLowED successfully retains safety while improving reasoning capabilities. The study highlights the effectiveness of the Slow Tuning and Low-Entropy Masking modules, with Slow Tuning maintaining safety in the early stages and Low-Entropy Masking prolonging safe training epochs.<br /> <div>
arXiv:2508.09666v1 Announce Type: new 
Abstract: Previous chain-of-thought (CoT) distillation methods primarily focused on enhancing the reasoning capabilities of Small Language Models (SLMs) by utilizing high-quality rationales generated by powerful Large Language Models (LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM safety brought by the training, which are revealed in this study. Although there are works on safety alignment that fine-tune language models or manipulate model weights to defend against harmful inputs, they require extra computation or annotated data, and probably impact the reasoning ability of SLMs. In this paper, we investigate how to maintain the safety of SLMs during the CoT distillation process. Specifically, we propose a safe distillation method, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the magnitude of model weight changes to optimize the model weights in the neighboring space near the initial weight distribution. Low-Entropy Masking masks low-entropy tokens, which are regarded as unnecessary learning targets, to exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B, Llama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC, AGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety of SLMs and comparably improves their reasoning capability compared to existing distillation methods. Furthermore, our ablation study presents the effectiveness of Slow Tuning and Low-Entropy Masking, with the former maintaining the model's safety in the early stage and the latter prolonging the safe training epochs.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Role of Large Language Models in Legal Practice in India</title>
<link>https://arxiv.org/abs/2508.09713</link>
<guid>https://arxiv.org/abs/2508.09713</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, Legal tasks, Indian context, Human expertise

Summary: 
Artificial Intelligence (AI) integration in the legal profession raises questions about the capabilities of Large Language Models (LLMs) like GPT, Claude, and Llama in performing legal tasks in India. A study evaluated LLMs' performance in issue spotting, legal drafting, advice, research, and reasoning, compared with a junior lawyer. LLMs excelled in drafting and issue spotting, sometimes outperforming humans. However, they struggled with specialized legal research, often providing inaccurate or fabricated information. The study concludes that while LLMs can assist in specific legal tasks, human expertise remains crucial for nuanced reasoning and precise application of law. 

<br /><br />Summary: <div>
arXiv:2508.09713v1 Announce Type: new 
Abstract: The integration of Artificial Intelligence(AI) into the legal profession raises significant questions about the capacity of Large Language Models(LLM) to perform key legal tasks. In this paper, I empirically evaluate how well LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian context, including issue spotting, legal drafting, advice, research, and reasoning. Through a survey experiment, I compare outputs from LLMs with those of a junior lawyer, with advanced law students rating the work on helpfulness, accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting, often matching or surpassing human work. However, they struggle with specialised legal research, frequently generating hallucinations, factually incorrect or fabricated outputs. I conclude that while LLMs can augment certain legal tasks, human expertise remains essential for nuanced reasoning and the precise application of law.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.09716</link>
<guid>https://arxiv.org/abs/2508.09716</guid>
<content:encoded><![CDATA[
<div> misleading visualizations, Vision-Language Models (VLMs), deceptive design elements, misinformation, safeguards<br />
<br />
Summary: <br />
Information visualizations play a crucial role in decision-making but can be misleading when designed deceptively. This study evaluates the susceptibility of Vision-Language Models (VLMs) to interpret misleading visualizations. Analyzing responses from over 16,000 instances across various deceptive chart designs, the study reveals that most VLMs are deceived, leading to altered interpretations despite the unchanged data. The findings underscore the importance of implementing robust safeguards in VLMs to combat visual misinformation. <div>
arXiv:2508.09716v1 Announce Type: new 
Abstract: Information visualizations are powerful tools that help users quickly identify patterns, trends, and outliers, facilitating informed decision-making. However, when visualizations incorporate deceptive design elements-such as truncated or inverted axes, unjustified 3D effects, or violations of best practices-they can mislead viewers and distort understanding, spreading misinformation. While some deceptive tactics are obvious, others subtly manipulate perception while maintaining a facade of legitimacy. As Vision-Language Models (VLMs) are increasingly used to interpret visualizations, especially by non-expert users, it is critical to understand how susceptible these models are to deceptive visual designs. In this study, we conduct an in-depth evaluation of VLMs' ability to interpret misleading visualizations. By analyzing over 16,000 responses from ten different models across eight distinct types of misleading chart designs, we demonstrate that most VLMs are deceived by them. This leads to altered interpretations of charts, despite the underlying data remaining the same. Our findings highlight the need for robust safeguards in VLMs against visual misinformation.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning</title>
<link>https://arxiv.org/abs/2508.09726</link>
<guid>https://arxiv.org/abs/2508.09726</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, reinforcement learning, response length, token efficiency, computational efficiency

Summary: 
GFPO (Group Filtered Policy Optimization) is introduced to address the issue of length inflation in large language models trained with reinforcement learning. By sampling larger groups per problem during training and filtering responses based on response length and token efficiency, GFPO reduces length inflation by 46-71% across various STEM and coding benchmarks while maintaining accuracy. Optimizing for reward per token ratio further increases reductions in length inflation to 71-85%. Adaptive Difficulty GFPO dynamically allocates more training resources to harder problems, improving the balance between computational efficiency and accuracy. The approach demonstrates that increased training-time compute translates to reduced test-time compute, providing an efficient trade-off for reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2508.09726v1 Announce Type: new 
Abstract: Large language models trained with reinforcement learning with verifiable rewards tend to trade accuracy for length--inflating response lengths to achieve gains in accuracy. While longer answers may be warranted for harder problems, many tokens are merely "filler": repetitive, verbose text that makes no real progress. We introduce GFPO (Group Filtered Policy Optimization), which curbs this length explosion by sampling larger groups per problem during training and filtering responses to train on based on two key metrics: (1) response length and (2) token efficiency: reward per token ratio. By sampling more at training time, we teach models to think less at inference time. On the Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH, LiveCodeBench) while maintaining accuracy. Optimizing for reward per token further increases reductions in length inflation to 71-85%. We also propose Adaptive Difficulty GFPO, which dynamically allocates more training resources to harder problems based on real-time difficulty estimates, improving the balance between computational efficiency and accuracy especially on difficult questions. GFPO demonstrates that increased training-time compute directly translates to reduced test-time compute--a simple yet effective trade-off for efficient reasoning.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.09755</link>
<guid>https://arxiv.org/abs/2508.09755</guid>
<content:encoded><![CDATA[
<div> novel retrieval-augmented generation framework, multihop question answering, large language model, question decomposition, document retrieval<br />
<br />
Summary:<br />
- The article proposes a novel retrieval-augmented generation (RAG) framework for multihop question answering. 
- A large language model (LLM) is used to break down complex multihop questions into single-hop subquestions for document retrieval. 
- Answerable questions are generated from document chunks using Qwen3-8B and retrieved based on question-question embedding similarity. 
- The retrieved chunks and original question are input into the RAG pipeline during inference. 
- Evaluation on three multihop question datasets showed improved performance compared to baseline systems, showcasing the benefits of using answerable-question embeddings and LLM-based query decomposition for multihop scenarios. <br /> <div>
arXiv:2508.09755v1 Announce Type: new 
Abstract: We introduce a novel retrieval-augmented generation (RAG) framework tailored for multihop question answering. First, our system uses large language model (LLM) to decompose complex multihop questions into a sequence of single-hop subquestions that guide document retrieval. This decomposition mitigates the ambiguity inherent in multi-hop queries by clearly targeting distinct knowledge facets. Second, instead of embedding raw or chunked documents directly, we generate answerable questions from each document chunk using Qwen3-8B, embed these generated questions, and retrieve relevant chunks via question-question embedding similarity. During inference, the retrieved chunks are then fed along with the original question into the RAG pipeline. We evaluate on three multihop question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our method improves RAG performacne compared to baseline systems. Our contributions highlight the benefits of using answerable-question embeddings for RAG, and the effectiveness of LLM-based query decomposition for multihop scenarios.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models</title>
<link>https://arxiv.org/abs/2508.09759</link>
<guid>https://arxiv.org/abs/2508.09759</guid>
<content:encoded><![CDATA[
<div> sensitive, bias evaluation, political bias, model behavior, arguments<br />
<br />
Summary: 
The study explores the impact of prompt suggestion on bias evaluation in large language models (LLMs) concerning political topics. It investigates how introducing supporting and refuting arguments in the prompt influences the model's responses towards certain positions. The experiments conducted reveal that the presence of such arguments significantly alters the model's output, aligning it with the presented argument. Moreover, the strength of these arguments affects the agreement rate of the model's responses. This suggests a sycophantic tendency in LLMs to adapt their stance to align with provided arguments, raising concerns about the robustness of bias evaluations and the need for effective mitigation strategies.Understanding these effects is crucial for accurately measuring political bias in LLMs and for gaining insights into their behavior when interacting with opinionated text. <div>
arXiv:2508.09759v1 Announce Type: new 
Abstract: There have been numerous studies evaluating bias of LLMs towards political topics. However, how positions towards these topics in model outputs are highly sensitive to the prompt. What happens when the prompt itself is suggestive of certain arguments towards those positions remains underexplored. This is crucial for understanding how robust these bias evaluations are and for understanding model behaviour, as these models frequently interact with opinionated text. To that end, we conduct experiments for political bias evaluation in presence of supporting and refuting arguments. Our experiments show that such arguments substantially alter model responses towards the direction of the provided argument in both single-turn and multi-turn settings. Moreover, we find that the strength of these arguments influences the directional agreement rate of model responses. These effects point to a sycophantic tendency in LLMs adapting their stance to align with the presented arguments which has downstream implications for measuring political bias and developing effective mitigation strategies.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech</title>
<link>https://arxiv.org/abs/2508.09767</link>
<guid>https://arxiv.org/abs/2508.09767</guid>
<content:encoded><![CDATA[
<div> adaptation, multilingual text-to-speech, pronunciation control, naturalness, low-rank adaptation

Summary:
UtterTune is a proposed method that fine-tunes a multilingual text-to-speech system to enhance pronunciation control in a target language while maintaining performance in others. The system is based on a large language model architecture and aims to improve segmental pronunciation and pitch accent at the phoneme level for Japanese speech. Despite challenges in accurately modeling grapheme-to-phoneme mapping and prosody, UtterTune leverages low-rank adaptation to achieve controllability while preserving naturalness and speaker similarity in a zero-shot setting. Both objective and subjective evaluations confirm the effectiveness of this approach. <div>
arXiv:2508.09767v1 Announce Type: new 
Abstract: We propose UtterTune, a lightweight adaptation method that fine-tunes a multilingual text-to-speech (TTS) system based on a large language model (LLM) architecture, designed to enhance the controllability of pronunciation in a target language while preserving performance in others. While LLM architectures have enabled TTS models to achieve remarkable naturalness, accurately modeling grapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially when the model omits an explicit G2P module and directly processes minimally encoded text (e.g., byte-pair encoding). UtterTune leverages low-rank adaptation to enable the control of segmental pronunciation and pitch accent at the phoneme level for Japanese speech, the target language in this paper, while maintaining naturalness and speaker similarity in a zero-shot setting. Objective and subjective evaluations confirm its effectiveness.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study</title>
<link>https://arxiv.org/abs/2508.09776</link>
<guid>https://arxiv.org/abs/2508.09776</guid>
<content:encoded><![CDATA[
<div> Keywords: Explainable Natural Language Processing, Textual Explanations, Large Language Models, Natural Language Generation, Model Performance<br />
Summary: <br />
In the field of Explainable Natural Language Processing, this study introduces an automated framework that utilizes large language models to generate textual explanations, reducing the need for costly human annotation. The framework's explanations are evaluated using Natural Language Generation metrics and tested on natural language inference tasks on benchmark datasets. The results show that the automated explanations produced by large language models are of high quality and have a noticeable impact on the performance of pre-trained language models. The study highlights the effectiveness of automated explanations in enhancing model performance, making them a promising solution for scalable and efficient textual explanation generation in NLP applications. <br /> <div>
arXiv:2508.09776v1 Announce Type: new 
Abstract: In the rapidly evolving field of Explainable Natural Language Processing (NLP), textual explanations, i.e., human-like rationales, are pivotal for explaining model predictions and enriching datasets with interpretable labels. Traditional approaches rely on human annotation, which is costly, labor-intensive, and impedes scalability. In this work, we present an automated framework that leverages multiple state-of-the-art large language models (LLMs) to generate high-quality textual explanations. We rigorously assess the quality of these LLM-generated explanations using a comprehensive suite of Natural Language Generation (NLG) metrics. Furthermore, we investigate the downstream impact of these explanations on the performance of pre-trained language models (PLMs) and LLMs across natural language inference tasks on two diverse benchmark datasets. Our experiments demonstrate that automated explanations exhibit highly competitive effectiveness compared to human-annotated explanations in improving model performance. Our findings underscore a promising avenue for scalable, automated LLM-based textual explanation generation for extending NLP datasets and enhancing model performance.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges</title>
<link>https://arxiv.org/abs/2508.09786</link>
<guid>https://arxiv.org/abs/2508.09786</guid>
<content:encoded><![CDATA[
<div> explanable NLP, transparency, practitioners, satisfaction levels, challenges 
Summary:
Industry practitioners and academic researchers participated in a qualitative study on explainable natural language processing (NLP). The study focused on motivations for adoption, employed techniques, satisfaction levels, and practical challenges faced in NLP applications. Findings reveal conceptual gaps in explainability methods, low practitioner satisfaction with current techniques, and highlighted evaluation challenges. The study emphasizes the importance of clear definitions and user-centric frameworks for better adoption of explainable NLP in practice. <br /><br />Summary: <div>
arXiv:2508.09786v1 Announce Type: new 
Abstract: The field of explainable natural language processing (NLP) has grown rapidly in recent years. The growing opacity of complex models calls for transparency and explanations of their decisions, which is crucial to understand their reasoning and facilitate deployment, especially in high-stakes environments. Despite increasing attention given to explainable NLP, practitioners' perspectives regarding its practical adoption and effectiveness remain underexplored. This paper addresses this research gap by investigating practitioners' experiences with explainability methods, specifically focusing on their motivations for adopting such methods, the techniques employed, satisfaction levels, and the practical challenges encountered in real-world NLP applications. Through a qualitative interview-based study with industry practitioners and complementary interviews with academic researchers, we systematically analyze and compare their perspectives. Our findings reveal conceptual gaps, low satisfaction with current explainability methods, and highlight evaluation challenges. Our findings emphasize the need for clear definitions and user-centric frameworks for better adoption of explainable NLP in practice.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning</title>
<link>https://arxiv.org/abs/2508.09804</link>
<guid>https://arxiv.org/abs/2508.09804</guid>
<content:encoded><![CDATA[
<div> dataset creation pipeline, diverse chart images, real-world data, supervised fine-tuning, reinforcement learning

Summary:<br /><br />Charts are essential for data analysis, but current vision-language models struggle with chart comprehension due to limited and low-quality training data. To address this, the authors propose BigCharts, a dataset creation pipeline that generates visually diverse chart images using real-world data. They introduce a training framework that combines supervised fine-tuning with reinforcement learning, specifically Group Relative Policy Optimization (GRPO), to improve model robustness and generalization. By incorporating novel reward signals for chart reasoning, their model, BigCharts-R1, outperforms existing methods on chart question-answering benchmarks. Extensive experiments show that BigCharts-R1 surpasses larger open-source and closed-source models in chart reasoning tasks. <div>
arXiv:2508.09804v1 Announce Type: new 
Abstract: Charts are essential to data analysis, transforming raw data into clear visual representations that support human decision-making. Although current vision-language models (VLMs) have made significant progress, they continue to struggle with chart comprehension due to training on datasets that lack diversity and real-world authenticity, or on automatically extracted underlying data tables of charts, which can contain numerous estimation errors. Furthermore, existing models only rely on supervised fine-tuning using these low-quality datasets, severely limiting their effectiveness. To address these issues, we first propose BigCharts, a dataset creation pipeline that generates visually diverse chart images by conditioning the rendering process on real-world charts sourced from multiple online platforms. Unlike purely synthetic datasets, BigCharts incorporates real-world data, ensuring authenticity and visual diversity, while still retaining accurate underlying data due to our proposed replotting process. Additionally, we introduce a comprehensive training framework that integrates supervised fine-tuning with Group Relative Policy Optimization (GRPO)-based reinforcement learning. By introducing novel reward signals specifically designed for chart reasoning, our approach enhances model robustness and generalization across diverse chart styles and domains, resulting in a state-of-the-art chart reasoning model, BigCharts-R1. Extensive experiments demonstrate that our models surpass existing methods on multiple chart question-answering benchmarks compared to even larger open-source and closed-source models.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems</title>
<link>https://arxiv.org/abs/2508.09809</link>
<guid>https://arxiv.org/abs/2508.09809</guid>
<content:encoded><![CDATA[
<div> AI, mental health, clinical datasets, data curation, challenges
Summary: 
This paper highlights the increasing importance of using Artificial Intelligence (AI) in bridging the gap in mental health support due to the shortage of trained clinicians. It emphasizes the critical role of high-quality clinical training datasets in developing efficient and ethical AI-powered clinical assistants. The study categorizes existing clinical mental health datasets based on mental disorders, data modalities, task types, accessibility, and sociocultural context, as well as synthetic datasets. Critical gaps in the current datasets are identified, such as the lack of longitudinal data, limited cultural and linguistic representation, inconsistent standards in data collection and annotation, and missing modalities in synthetic data. The paper concludes by outlining key challenges in curating and standardizing future datasets and provides actionable recommendations to enhance the development of more robust, generalizable, and equitable mental health AI systems. 
<br /><br />Summary: <div>
arXiv:2508.09809v1 Announce Type: new 
Abstract: Mental health disorders are rising worldwide. However, the availability of trained clinicians has not scaled proportionally, leaving many people without adequate or timely support. To bridge this gap, recent studies have shown the promise of Artificial Intelligence (AI) to assist mental health diagnosis, monitoring, and intervention. However, the development of efficient, reliable, and ethical AI to assist clinicians is heavily dependent on high-quality clinical training datasets. Despite growing interest in data curation for training clinical AI assistants, existing datasets largely remain scattered, under-documented, and often inaccessible, hindering the reproducibility, comparability, and generalizability of AI models developed for clinical mental health care. In this paper, we present the first comprehensive survey of clinical mental health datasets relevant to the training and development of AI-powered clinical assistants. We categorize these datasets by mental disorders (e.g., depression, schizophrenia), data modalities (e.g., text, speech, physiological signals), task types (e.g., diagnosis prediction, symptom severity estimation, intervention generation), accessibility (public, restricted or private), and sociocultural context (e.g., language and cultural background). Along with these, we also investigate synthetic clinical mental health datasets. Our survey identifies critical gaps such as a lack of longitudinal data, limited cultural and linguistic representation, inconsistent collection and annotation standards, and a lack of modalities in synthetic data. We conclude by outlining key challenges in curating and standardizing future datasets and provide actionable recommendations to facilitate the development of more robust, generalizable, and equitable mental health AI systems.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speed Always Wins: A Survey on Efficient Architectures for Large Language Models</title>
<link>https://arxiv.org/abs/2508.09834</link>
<guid>https://arxiv.org/abs/2508.09834</guid>
<content:encoded><![CDATA[
<div> efficient LLM architectures, transformers, sequence modeling, full attention variants, mixture-of-experts <br />
Summary: 
This survey explores innovative Large Language Models (LLMs) that aim to improve efficiency by addressing the limitations of traditional transformer architectures. The study covers various techniques such as linear and sparse sequence modeling, efficient full attention variants, sparse mixture-of-experts, hybrid model architectures, and emerging diffusion LLMs. These advancements not only enhance the efficiency of LLMs but also pave the way for their application to different modalities. By categorizing recent studies and presenting a blueprint of modern efficient LLM architectures, this survey aims to inspire future research in developing scalable and resource-aware AI systems. <div>
arXiv:2508.09834v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have delivered impressive results in language understanding, generation, reasoning, and pushes the ability boundary of multimodal models. Transformer models, as the foundation of modern LLMs, offer a strong baseline with excellent scaling properties. However, the traditional transformer architecture requires substantial computations and poses significant obstacles for large-scale training and practical deployment. In this survey, we offer a systematic examination of innovative LLM architectures that address the inherent limitations of transformers and boost the efficiency. Starting from language modeling, this survey covers the background and technical details of linear and sparse sequence modeling methods, efficient full attention variants, sparse mixture-of-experts, hybrid model architectures incorporating the above techniques, and emerging diffusion LLMs. Additionally, we discuss applications of these techniques to other modalities and consider their wider implications for developing scalable, resource-aware foundation models. By grouping recent studies into the above category, this survey presents a blueprint of modern efficient LLM architectures, and we hope this could help motivate future research toward more efficient, versatile AI systems.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts</title>
<link>https://arxiv.org/abs/2508.09848</link>
<guid>https://arxiv.org/abs/2508.09848</guid>
<content:encoded><![CDATA[
<div> benchmark, long-context understanding, prequel story, reasoning, LLMs

Summary:
The article introduces a new benchmark called PRELUDE, which evaluates long-context understanding by determining the consistency of a character's prequel story with the original book's narrative. This task requires deep reasoning and global comprehension, as it involves integrating information that is indirectly related to the original story. The study shows that 88% of instances require evidence from multiple parts of the narrative. Experimental results reveal a significant gap between human performance and models trained on in-context learning, RAG, LLMs, and DeepResearch services. Moreover, a human study highlights that models often provide correct answers but with flawed reasoning, leading to a reasoning accuracy gap of over 30% compared to humans. Overall, the findings emphasize the need for improvement in long-context understanding and reasoning. 

<br /><br />Summary: <div>
arXiv:2508.09848v1 Announce Type: new 
Abstract: We introduce PRELUDE, a benchmark for evaluating long-context understanding through the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book. Our task poses a stronger demand for global comprehension and deep reasoning than existing benchmarks -- as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative. Experimental results highlight the challenge of our task: in-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by >15%. A further human study reveals that models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans. These findings underscore the substantial room for improvement in long-context understanding and reasoning.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription</title>
<link>https://arxiv.org/abs/2508.09865</link>
<guid>https://arxiv.org/abs/2508.09865</guid>
<content:encoded><![CDATA[
<div> evaluation, lightweight Whisper models, Urdu speech recognition, low-resource settings, word error rate

Summary:
This study assesses the feasibility of using lightweight Whisper models for Urdu speech recognition in low-resource environments. The models, including Tiny, Base, and Small, are tested on a curated Urdu dataset without fine-tuning. Results show that Whisper-Small outperforms Tiny and Base models with the lowest word error rate. However, challenges persist in phonetic accuracy and lexical coherence, especially for complex utterances. While Whisper-Small shows potential for practical Urdu ASR deployment, there are still significant gaps to address. This research highlights the need for further investigation into effective low-resource ASR systems in Urdu. 

<br /><br />Summary: <div>
arXiv:2508.09865v1 Announce Type: new 
Abstract: This study evaluates the feasibility of lightweight Whisper models (Tiny, Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu being the 10th most spoken language globally with over 230 million speakers, its representation in automatic speech recognition (ASR) systems remains limited due to dialectal diversity, code-switching, and sparse training data. We benchmark these models on a curated Urdu dataset using word error rate (WER), without fine-tuning. Results show Whisper-Small achieves the lowest error rates (33.68\% WER), outperforming Tiny (67.08\% WER) and Base (53.67\% WER). Qualitative analysis reveals persistent challenges in phonetic accuracy and lexical coherence, particularly for complex utterances. While Whisper-Small demonstrates promise for deployable Urdu ASR, significant gaps remain. Our findings emphasize lay the groundwork for future research into effective, low-resource ASR systems.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models</title>
<link>https://arxiv.org/abs/2508.09874</link>
<guid>https://arxiv.org/abs/2508.09874</guid>
<content:encoded><![CDATA[
<div> Memory Decoder, domain adaptation, large language models, specialized domains, pretrained memory. 
Summary: Memory Decoder is introduced as a plug-and-play pretrained memory component for efficient domain adaptation of Large Language Models (LLMs). It mimics the behavior of a non-parametric retriever and can be seamlessly integrated with any pretrained language model. This approach reduces the need for costly full-parameter training and avoids catastrophic forgetting. Experimental results demonstrate its effectiveness in adapting various LLMs to specialized domains such as biomedicine, finance, and law, resulting in an average perplexity reduction of 6.17 points. Memory Decoder offers a novel paradigm for domain-specific adaptation, enhancing performance across multiple models within the target domain.<br /><br />Summary: <div>
arXiv:2508.09874v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown strong abilities in general language tasks, yet adapting them to specific domains remains a challenge. Current method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter training and suffers from catastrophic forgetting. Meanwhile, Retrieval-Augmented Generation (RAG) introduces substantial inference latency due to expensive nearest-neighbor searches and longer context. This paper introduces Memory Decoder, a plug-and-play pretrained memory that enables efficient domain adaptation without changing the original model's parameters. Memory Decoder employs a small transformer decoder that learns to imitate the behavior of an external non-parametric retriever. Once trained, Memory Decoder can be seamlessly integrated with any pretrained language model that shares the same tokenizer, requiring no model-specific modifications. Experimental results demonstrate that Memory Decoder enables effective adaptation of various Qwen and Llama models to three distinct specialized domains: biomedicine, finance, and law, reducing perplexity by an average of 6.17 points. Overall, Memory Decoder introduces a novel paradigm centered on a specially pretrained memory component designed for domain-specific adaptation. This memory architecture can be integrated in a plug-and-play manner, consistently enhancing performance across multiple models within the target domain.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Cognitive Distortion Detection and Classification in NLP</title>
<link>https://arxiv.org/abs/2508.09878</link>
<guid>https://arxiv.org/abs/2508.09878</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language processing, mental health, cognitive distortions, therapy, evaluation

Summary:<br /><br /> 
This study discusses the application of natural language processing (NLP) techniques to mental health, specifically in detecting and classifying cognitive distortions (CDs). CDs are negative thinking patterns that can distort perceptions and reactions to events. The field of automatic CD detection is fragmented, with inconsistencies in taxonomies, task setups, and evaluation methods. The survey reviews 38 studies over two decades, providing insights into datasets, modelling approaches, and evaluation strategies. A consolidated CD taxonomy reference is provided, along with common task setups and open challenges for future research. By addressing these challenges, researchers can improve the coherence and reproducibility of studies in this emerging field. <div>
arXiv:2508.09878v1 Announce Type: new 
Abstract: As interest grows in the application of natural language processing (NLP) techniques to mental health, a growing body of work explores the automatic detection and classification of cognitive distortions (CDs). CDs are habitual patterns of negatively biased or flawed thinking that distort how people perceive events, judge themselves, and react to the world around them. Identifying and addressing them is an important part of therapy. Despite its momentum, the field remains fragmented, with inconsistencies in CD taxonomies, task formulations, and evaluation practices. This survey reviews 38 studies spanning two decades, providing a structured overview of datasets, modelling approaches, and evaluation strategies. We provide a consolidated CD taxonomy reference, summarise common task setups, and highlight open challenges to support more coherent and reproducible research in this emerging area.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach</title>
<link>https://arxiv.org/abs/2508.09935</link>
<guid>https://arxiv.org/abs/2508.09935</guid>
<content:encoded><![CDATA[
<div> Keywords: Business communication, Deceptive language, Persuasive lexicon, Textual analysis, AI-based discourse <br />
Summary: 
The article explores how the digitization of business communication has reshaped persuasive discourse, leading to both increased transparency and advanced deception. By integrating classical rhetoric, communication psychology, linguistic theory, and empirical studies in various domains such as financial reporting, sustainability discourse, and digital marketing, the study highlights the systematic detection of deceptive language using persuasive lexicon. Through computational textual analysis and personalised transformer models, detection accuracies exceeding 99% were achieved in controlled settings. However, challenges exist in reproducing this performance in multilingual environments due to limited data availability and inadequate text-processing infrastructure. The research underscores the growing disparity between theoretical communication representations and empirical approximations, emphasizing the necessity for robust automatic text-identification systems as AI-driven discourse evolves to better engage with humans. <br /><br />Summary: <div>
arXiv:2508.09935v1 Announce Type: new 
Abstract: Business communication digitisation has reorganised the process of persuasive discourse, which
  allows not only greater transparency but also advanced deception. This inquiry synthesises classical
  rhetoric and communication psychology with linguistic theory and empirical studies in the financial
  reporting, sustainability discourse, and digital marketing to explain how deceptive language can be
  systematically detected using persuasive lexicon. In controlled settings, detection accuracies of greater
  than 99% were achieved by using computational textual analysis as well as personalised transformer
  models. However, reproducing this performance in multilingual settings is also problematic and,
  to a large extent, this is because it is not easy to find sufficient data, and because few multilingual
  text-processing infrastructures are in place. This evidence shows that there has been an increasing
  gap between the theoretical representations of communication and those empirically approximated,
  and therefore, there is a need to have strong automatic text-identification systems where AI-based
  discourse is becoming more realistic in communicating with humans.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Evaluation framework of Alignment Techniques for LLMs</title>
<link>https://arxiv.org/abs/2508.09937</link>
<guid>https://arxiv.org/abs/2508.09937</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, alignment techniques, evaluation framework, computational efficiency, robustness
Summary: 
The paper introduces a comprehensive evaluation framework for assessing alignment techniques for Large Language Models (LLMs). It identifies four key dimensions for evaluation: alignment detection, alignment quality, computational efficiency, and robustness. Traditional fine-tuning methods, post-hoc correction systems, and inference-time interventions are among the alignment paradigms assessed. Through experiments with various base models and alignment strategies, the framework enables a systematic comparison of strengths and limitations of current state-of-the-art models. This evaluation framework aims to provide valuable insights for future research directions in the field of ensuring LLM outputs align with human values and safety standards.

<br /><br />Summary: <div>
arXiv:2508.09937v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) become increasingly integrated into real-world applications, ensuring their outputs align with human values and safety standards has become critical. The field has developed diverse alignment approaches including traditional fine-tuning methods (RLHF, instruction tuning), post-hoc correction systems, and inference-time interventions, each with distinct advantages and limitations. However, the lack of unified evaluation frameworks makes it difficult to systematically compare these paradigms and guide deployment decisions. This paper introduces a multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive evaluation framework that provides a systematic comparison across all major alignment paradigms. Our framework assesses methods along four key dimensions: alignment detection, alignment quality, computational efficiency, and robustness. Through experiments across diverse base models and alignment strategies, we demonstrate the utility of our framework in identifying strengths and limitations of current state-of-the-art models, providing valuable insights for future research directions.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models</title>
<link>https://arxiv.org/abs/2508.09945</link>
<guid>https://arxiv.org/abs/2508.09945</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal language models, code generation, vision-language integration, dataset, benchmark

Summary:
VisCodex is a new framework that combines vision and coding language models to enhance multimodal code generation capabilities. It merges a coding language model with a vision-language backbone using a task vector-based model merging technique. The framework is evaluated using the Multimodal Coding Dataset (MCD), which includes various types of code samples and visuals. Additionally, a benchmark called InfiBench-V is introduced to assess models on complex programming questions that require understanding of both textual and visual contexts. Through extensive experiments, VisCodex demonstrates state-of-the-art performance compared to other MLLMs, even proprietary models like GPT-4o. The effectiveness of the model merging strategy and new datasets is highlighted in the study.<br /><br />Summary: <div>
arXiv:2508.09945v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have significantly advanced the integration of visual and textual understanding. However, their ability to generate code from multimodal inputs remains limited. In this work, we introduce VisCodex, a unified framework that seamlessly merges vision and coding language models to empower MLLMs with strong multimodal code generation abilities. Leveraging a task vector-based model merging technique, we integrate a state-of-the-art coding LLM into a strong vision-language backbone, while preserving both visual comprehension and advanced coding skills. To support training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a large-scale and diverse collection of 598k samples, including high-quality HTML code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic problems. Furthermore, we propose InfiBench-V, a novel and challenging benchmark specifically designed to assess models on visually-rich, real-world programming questions that demand a nuanced understanding of both textual and visual contexts. Extensive experiments show that VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4o, highlighting the effectiveness of our model merging strategy and new datasets.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Specialised or Generic? Tokenization Choices for Radiology Language Models</title>
<link>https://arxiv.org/abs/2508.09952</link>
<guid>https://arxiv.org/abs/2508.09952</guid>
<content:encoded><![CDATA[
<div> tokenizer, language models, radiology, summarisation, medical

Summary: 
- The impact of tokenizer vocabulary on language models in radiology remains underexplored.
- Different tokenizers were compared for radiology report summarisation across imaging modalities.
- Medical and domain-specific tokenizers outperformed natural language tokenizers when trained from scratch.
- LM pre-training on PubMed abstracts helped mitigate performance differences between tokenizers.
- Domain-specific tokenizers showed the most favorable results, with reduced memory requirements and shorter sequences. 

<br /><br />Summary: <div>
arXiv:2508.09952v1 Announce Type: new 
Abstract: The vocabulary used by language models (LM) - defined by the tokenizer - plays a key role in text generation quality. However, its impact remains under-explored in radiology. In this work, we address this gap by systematically comparing general, medical, and domain-specific tokenizers on the task of radiology report summarisation across three imaging modalities. We also investigate scenarios with and without LM pre-training on PubMed abstracts. Our findings demonstrate that medical and domain-specific vocabularies outperformed widely used natural language alternatives when models are trained from scratch. Pre-training partially mitigates performance differences between tokenizers, whilst the domain-specific tokenizers achieve the most favourable results. Domain-specific tokenizers also reduce memory requirements due to smaller vocabularies and shorter sequences. These results demonstrate that adapting the vocabulary of LMs to the clinical domain provides practical benefits, including improved performance and reduced computational demands, making such models more accessible and effective for both research and real-world healthcare settings.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shaping Event Backstories to Estimate Potential Emotion Contexts</title>
<link>https://arxiv.org/abs/2508.09954</link>
<guid>https://arxiv.org/abs/2508.09954</guid>
<content:encoded><![CDATA[
<div> Keywords: emotion analysis, ambiguity, context, event descriptions, human annotators 

Summary: 
This paper introduces a novel approach to emotion analysis by enriching event descriptions with reasonable contexts to reduce ambiguity. The study aims to investigate whether providing enriched contexts can improve the reliability of human annotators in emotion annotation tasks. The approach involves disambiguating a target event description by generating multiple event chains conditioned on different emotions. Using techniques from short story generation, coherent narratives are produced to create a specialized dataset for a systematic examination of contextualized emotion analysis. The results from both automatic and human evaluations show that incorporating contextual narratives enhances the interpretation of specific emotions and helps annotators produce more consistent annotations. The findings suggest that adding context to event descriptions can improve the accuracy and reliability of emotion analysis tasks. 

<br /><br />Summary: <div>
arXiv:2508.09954v1 Announce Type: new 
Abstract: Emotion analysis is an inherently ambiguous task. Previous work studied annotator properties to explain disagreement, but this overlooks the possibility that ambiguity may stem from missing information about the context of events. In this paper, we propose a novel approach that adds reasonable contexts to event descriptions, which may better explain a particular situation. Our goal is to understand whether these enriched contexts enable human annotators to annotate emotions more reliably. We disambiguate a target event description by automatically generating multiple event chains conditioned on differing emotions. By combining techniques from short story generation in various settings, we achieve coherent narratives that result in a specialized dataset for the first comprehensive and systematic examination of contextualized emotion analysis. Through automatic and human evaluation, we find that contextual narratives enhance the interpretation of specific emotions and support annotators in producing more consistent annotations.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance of GPT-5 Frontier Models in Ophthalmology Question Answering</title>
<link>https://arxiv.org/abs/2508.09956</link>
<guid>https://arxiv.org/abs/2508.09956</guid>
<content:encoded><![CDATA[
<div> accuracy, cost-efficiency, reasoning models, medical question-answering, language models <br />
Summary: 
The study evaluates different configurations of OpenAI's GPT-5 models for medical question-answering tasks using ophthalmology questions. GPT-5-high showed the highest accuracy compared to other models, ranking first in accuracy and rationale quality. Cost-accuracy analysis identified GPT-5 configurations on the Pareto frontier, with GPT-5-mini-low offering a favorable balance between low cost and high performance. The study highlights the importance of reasoning effort in achieving accuracy in question-answering tasks and introduces an autograder framework for evaluating LLM-generated answers against reference standards in ophthalmology. <div>
arXiv:2508.09956v1 Announce Type: new 
Abstract: Large language models (LLMs) such as GPT-5 integrate advanced reasoning capabilities that may improve performance on complex medical question-answering tasks. For this latest generation of reasoning models, the configurations that maximize both accuracy and cost-efficiency have yet to be established. We evaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using 260 closed-access multiple-choice questions from the American Academy of Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome was multiple-choice accuracy; secondary outcomes included head-to-head ranking via a Bradley-Terry model, rationale quality assessment using a reference-anchored, pairwise LLM-as-a-judge framework, and analysis of accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano variants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high (0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x stronger than o3-high) and rationale quality (1.11x stronger than o3-high). Cost-accuracy analysis identified several GPT-5 configurations on the Pareto frontier, with GPT-5-mini-low offering the most favorable low-cost, high-performance balance. These results benchmark GPT-5 on a high-quality ophthalmology dataset, demonstrate the influence of reasoning effort on accuracy, and introduce an autograder framework for scalable evaluation of LLM-generated answers against reference standards in ophthalmology.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)</title>
<link>https://arxiv.org/abs/2508.09957</link>
<guid>https://arxiv.org/abs/2508.09957</guid>
<content:encoded><![CDATA[
<div> Keywords: Speech-to-text, Kurdish language, Badini dialect, language model, Wav2Vec2-Large-XLSR-53

Summary:
Speech-to-text systems are essential for various applications, but less-resourced languages like Kurdish dialects such as Badini lack quality STT systems. This research aims to bridge this gap by creating a language model based on Badini speech. Using Badini kids' stories as input, the study involved preprocessing nearly 15 hours of speech data. Two models, Wav2Vec2-Large-XLSR-53 and Whisper-small, were used to develop the language models. Results show that the Wav2Vec2-Large-XLSR-53 model outperformed the Whisper-small model in terms of accuracy and readability, achieving 90.38% readability and 82.67% accuracy. This research is significant as it can benefit the Badini-speaking community by enabling them to use mobile and computer-based technologies and increase the global visibility of their dialect.

<br /><br />Summary: <div>
arXiv:2508.09957v1 Announce Type: new 
Abstract: Speech-to-text (STT) systems have a wide range of applications. They are available in many languages, albeit at different quality levels. Although Kurdish is considered a less-resourced language from a processing perspective, SST is available for some of the Kurdish dialects, for instance, Sorani (Central Kurdish). However, that is not applied to other Kurdish dialects, Badini and Hawrami, for example. This research is an attempt to address this gap. Bandin, approximately, has two million speakers, and STT systems can help their community use mobile and computer-based technologies while giving their dialect more global visibility. We aim to create a language model based on Badini's speech and evaluate its performance. To cover a conversational aspect, have a proper confidence level of grammatical accuracy, and ready transcriptions, we chose Badini kids' stories, eight books including 78 stories, as the textual input. Six narrators narrated the books, which resulted in approximately 17 hours of recording. We cleaned, segmented, and tokenized the input. The preprocessing produced nearly 15 hours of speech, including 19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and Whisper-small to develop the language models. The experiments indicate that the transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a significantly more accurate and readable output than the Whisper-small model, with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy, respectively.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks</title>
<link>https://arxiv.org/abs/2508.09958</link>
<guid>https://arxiv.org/abs/2508.09958</guid>
<content:encoded><![CDATA[
<div> algorithm, large language models, contextual bandit, online learning, neural networks

Summary: 
The article discusses the challenge of selecting the most appropriate large language models (LLMs) for specialized tasks by breaking them down into smaller subtasks, each handled by a different LLM. Unlike traditional LLM selection algorithms, this approach involves selecting a sequence of LLMs where the output of each influences the next, creating performance dependencies. To address this, a neural contextual bandit-based algorithm is proposed that trains neural networks in an online manner to guide LLM selections for different subtasks. Experiments on telecommunications question answering and medical diagnosis prediction datasets demonstrate the effectiveness of this approach compared to other LLM selection algorithms. <div>
arXiv:2508.09958v1 Announce Type: new 
Abstract: With the increasing popularity of large language models (LLMs) for a variety of tasks, there has been a growing interest in strategies that can predict which out of a set of LLMs will yield a successful answer at low cost. This problem promises to become more and more relevant as providers like Microsoft allow users to easily create custom LLM "assistants" specialized to particular types of queries. However, some tasks (i.e., queries) may be too specialized and difficult for a single LLM to handle alone. These applications often benefit from breaking down the task into smaller subtasks, each of which can then be executed by a LLM expected to perform well on that specific subtask. For example, in extracting a diagnosis from medical records, one can first select an LLM to summarize the record, select another to validate the summary, and then select another, possibly different, LLM to extract the diagnosis from the summarized record. Unlike existing LLM selection or routing algorithms, this setting requires that we select a sequence of LLMs, with the output of each LLM feeding into the next and potentially influencing its success. Thus, unlike single LLM selection, the quality of each subtask's output directly affects the inputs, and hence the cost and success rate, of downstream LLMs, creating complex performance dependencies that must be learned and accounted for during selection. We propose a neural contextual bandit-based algorithm that trains neural networks that model LLM success on each subtask in an online manner, thus learning to guide the LLM selections for the different subtasks, even in the absence of historical LLM performance data. Experiments on telecommunications question answering and medical diagnosis prediction datasets illustrate the effectiveness of our proposed approach compared to other LLM selection algorithms.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2508.09145</link>
<guid>https://arxiv.org/abs/2508.09145</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Sentiment Analysis, MoLAN Framework, Modality-aware blocking, Noise suppression, State-of-the-art performance

Summary: 
The MoLAN framework addresses the challenge of irrelevant or misleading visual and auditory information in Multimodal Sentiment Analysis by performing modality-aware blocking. This technique divides features of each modality into multiple blocks and dynamically assigns denoising strength based on noise level and semantic relevance. By fine-tuning noise suppression at a granular level, essential multimodal information is preserved while removing redundant or noisy data. MoLAN is a flexible framework that can be integrated into various multimodal models, leading to the development of MoLAN+, a new approach for sentiment analysis. Experimental results across multiple models and datasets demonstrate the effectiveness of MoLAN+, achieving state-of-the-art performance. The code for the framework is publicly available on GitHub, providing a valuable resource for researchers in the field. <br /><br />Summary: <div>
arXiv:2508.09145v1 Announce Type: cross 
Abstract: Multimodal Sentiment Analysis aims to integrate information from various modalities, such as audio, visual, and text, to make complementary predictions. However, it often struggles with irrelevant or misleading visual and auditory information. Most existing approaches typically treat the entire modality information (e.g., a whole image, audio segment, or text paragraph) as an independent unit for feature enhancement or denoising. They often suppress the redundant and noise information at the risk of losing critical information. To address this challenge, we propose MoLAN, a unified ModaLity-aware noise dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking by dividing the features of each modality into multiple blocks. Each block is then dynamically assigned a distinct denoising strength based on its noise level and semantic relevance, enabling fine-grained noise suppression while preserving essential multimodal information. Notably, MoLAN is a unified and flexible framework that can be seamlessly integrated into a wide range of multimodal models. Building upon this framework, we further introduce MoLAN+, a new multimodal sentiment analysis approach. Experiments across five models and four datasets demonstrate the broad effectiveness of the MoLAN framework. Extensive evaluations show that MoLAN+ achieves the state-of-the-art performance. The code is publicly available at https://github.com/betterfly123/MoLAN-Framework.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\Delta$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation</title>
<link>https://arxiv.org/abs/2508.09199</link>
<guid>https://arxiv.org/abs/2508.09199</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual Instruction Finetuning, VLMs, Data selection, Attention-guided masking, Data-efficient framework

Summary: 
Visual Instruction Finetuning (VIF) is crucial for enhancing post-training Vision-Language Models (VLMs). Unlike traditional language model finetuning, VIF requires multimodal data for joint visual and textual understanding, leading to stricter data selection challenges. The proposed $\Delta$-AttnMask framework addresses these challenges efficiently by quantifying sample quality through attention-guided masking of model hidden states. This method evaluates image-text pairs without the need for domain labels or extra training, resulting in improved performance with just 20% of data. Experiments demonstrate that $\Delta$-AttnMask accelerates training by 5x and outperforms full-dataset baselines by +10.1% in overall accuracy. Its model-agnostic and data-agnostic design ensures versatility across modalities and architectures. 

<br /><br />Summary: <div>
arXiv:2508.09199v1 Announce Type: cross 
Abstract: Visual Instruction Finetuning (VIF) is pivotal for post-training Vision-Language Models (VLMs). Unlike unimodal instruction finetuning in plain-text large language models, which mainly requires instruction datasets to enable model instruction-following ability, VIF also requires multimodal data to enable joint visual and textual understanding; therefore, it typically requires more data. Consequently, VIF imposes stricter data selection challenges: the method must scale efficiently to handle larger data demands while ensuring the quality of both visual and textual content, as well as their alignment. Despite its critical impact on performance, data selection for VIF remains an understudied area. In this paper, we propose $\Delta$-AttnMask. This data-efficient framework quantifies sample quality through attention-guided masking of the model's hidden states, jointly evaluating image-text pairs without requiring domain labels, auxiliary models, or extra training. By computing loss differences ($\Delta$) between the original states and states masked using high-attention regions, $\Delta$-AttnMask intrinsically assesses sample quality. Experiments across multiple VLMs and datasets show that $\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data, accelerating training by 5x while surpassing full-dataset baselines by +10.1% in overall accuracy. Its model-agnostic and data-agnostic design ensures broad applicability across modalities and architectures.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training</title>
<link>https://arxiv.org/abs/2508.09224</link>
<guid>https://arxiv.org/abs/2508.09224</guid>
<content:encoded><![CDATA[
<div> safety-training, large language models, safe-completions, user intent, dual-use prompts<br />
Summary:
Safe-completions, a new safety-training approach for large language models like ChatGPT, shifts focus from binary refusal boundaries to maximizing helpfulness while staying within safety constraints. Unlike traditional models that either fully comply or refuse based on user intent, safe-completions prioritize the safety of the assistant's output. This approach is particularly beneficial for dual-use cases like biology or cybersecurity, where requests can be safely answered at a high level but may lead to malicious outcomes if too detailed. Implemented in GPT-5, safe-completion training improves safety, reduces the severity of safety failures, and enhances overall model helpfulness in both production comparisons and controlled experiments. <div>
arXiv:2508.09224v1 Announce Type: cross 
Abstract: Large Language Models used in ChatGPT have traditionally been trained to learn a refusal boundary: depending on the user's intent, the model is taught to either fully comply or outright refuse. While this is a strong mitigation for explicitly malicious prompts, focusing safety training on refusals can lead to brittleness for prompts with obscured user intent. Binary refusal boundaries are especially ill-suited for dual-use cases (such as biology or cybersecurity), where a user request can be answered safely at a high level, but in some cases can lead to malicious uplift if sufficiently detailed or actionable. As an alternative, we propose safe-completions: a safety-training approach that centers on the safety of the assistant's output, rather than a binary classification of the user's intent. Safe-completions seek to maximize helpfulness within the safety policy's constraints. We incorporated this approach into GPT-5 and find that across both production comparisons and internally controlled experiments, safe-completion training improves safety (especially on dual-use prompts), reduces the severity of residual safety failures, and substantially increases model helpfulness.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation</title>
<link>https://arxiv.org/abs/2508.09240</link>
<guid>https://arxiv.org/abs/2508.09240</guid>
<content:encoded><![CDATA[
<div> framework, Service-Based Architecture, Network Functions, APIs, NEFMind 

Summary:
NEFMind is a framework that utilizes fine-tuning of Large Language Models (LLMs) to streamline the management of Network Functions (NFs) and Application Programming Interfaces (APIs) in modern telecommunications. It includes three main components: synthetic dataset generation, model optimization through Quantized-Low-Rank Adaptation, and performance evaluation using GPT-4 Ref Score and BertScore metrics. The framework targets 5G Service-Based Architecture APIs and achieves an 85% reduction in communication overhead compared to manual discovery methods. Experimental validation using the Phi-2 model demonstrates high API call identification accuracy of 98-100%. The fine-tuned model maintains computational efficiency while delivering performance comparable to larger models like GPT-4. These results validate the effectiveness of parameter-efficient LLM strategies for managing complex API ecosystems in next-generation telecommunications networks. 

<br /><br />Summary: <div>
arXiv:2508.09240v1 Announce Type: cross 
Abstract: The use of Service-Based Architecture in modern telecommunications has exponentially increased Network Functions (NFs) and Application Programming Interfaces (APIs), creating substantial operational complexities in service discovery and management. We introduce \textit{NEFMind}, a framework leveraging parameter-efficient fine-tuning of open-source Large Language Models (LLMs) to address these challenges. It integrates three core components: synthetic dataset generation from Network Exposure Function (NEF) API specifications, model optimization through Quantized-Low-Rank Adaptation, and performance evaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G Service-Based Architecture APIs, our approach achieves 85% reduction in communication overhead compared to manual discovery methods. Experimental validation using the open-source Phi-2 model demonstrates exceptional API call identification performance at 98-100% accuracy. The fine-tuned Phi-2 model delivers performance comparable to significantly larger models like GPT-4 while maintaining computational efficiency for telecommunications infrastructure deployment. These findings validate domain-specific, parameter-efficient LLM strategies for managing complex API ecosystems in next-generation telecommunications networks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs</title>
<link>https://arxiv.org/abs/2508.09288</link>
<guid>https://arxiv.org/abs/2508.09288</guid>
<content:encoded><![CDATA[
<div> vulnerability, language models, security, Contextual Integrity Verification, token-level similarity 

Summary:
Contextual Integrity Verification (CIV) is introduced as a security architecture for large language models (LLMs) to protect against prompt injection and jailbreak attacks. CIV attaches cryptographically signed provenance labels to tokens and enforces a source-trust lattice within the transformer. This architecture provides non-interference guarantees on frozen models, preventing lower-trust tokens from influencing higher-trust representations. CIV achieves a 0% attack success rate on benchmark tests while maintaining high token-level similarity and model perplexity on benign tasks. Although there is a latency overhead, CIV is a lightweight patch that requires no fine-tuning and can be applied to Llama-3-8B and Mistral-7B models. A reference implementation, automated certification harness, and Elite-Attack corpus are released to support reproducible research. <br /><br />Summary: <div>
arXiv:2508.09288v1 Announce Type: cross 
Abstract: Large language models (LLMs) remain acutely vulnerable to prompt injection and related jailbreak attacks; heuristic guardrails (rules, filters, LLM judges) are routinely bypassed. We present Contextual Integrity Verification (CIV), an inference-time security architecture that attaches cryptographically signed provenance labels to every token and enforces a source-trust lattice inside the transformer via a pre-softmax hard attention mask (with optional FFN/residual gating). CIV provides deterministic, per-token non-interference guarantees on frozen models: lower-trust tokens cannot influence higher-trust representations. On benchmarks derived from recent taxonomies of prompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack success rate under the stated threat model while preserving 93.1% token-level similarity and showing no degradation in model perplexity on benign tasks; we note a latency overhead attributable to a non-optimized data path. Because CIV is a lightweight patch -- no fine-tuning required -- we demonstrate drop-in protection for Llama-3-8B and Mistral-7B. We release a reference implementation, an automated certification harness, and the Elite-Attack corpus to support reproducible research.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative</title>
<link>https://arxiv.org/abs/2508.09294</link>
<guid>https://arxiv.org/abs/2508.09294</guid>
<content:encoded><![CDATA[
arXiv:2508.09294v1 Announce Type: cross 
Abstract: Advances in speech synthesis intensify security threats, motivating real-time deepfake detection research. We investigate whether bidirectional Mamba can serve as a competitive alternative to Self-Attention in detecting synthetic speech. Our solution, Fake-Mamba, integrates an XLSR front-end with bidirectional Mamba to capture both local and global artifacts. Our core innovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and PN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can effectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof 21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and 5.85% EER, respectively, representing substantial relative gains over SOTA models XLSR-Conformer and XLSR-Mamba. The framework maintains real-time inference across utterance lengths, demonstrating strong generalization and practical viability. The code is available at https://github.com/xuanxixi/Fake-Mamba.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs</title>
<link>https://arxiv.org/abs/2508.09389</link>
<guid>https://arxiv.org/abs/2508.09389</guid>
<content:encoded><![CDATA[
arXiv:2508.09389v1 Announce Type: cross 
Abstract: Prosody conveys rich emotional and semantic information of the speech signal as well as individual idiosyncrasies. We propose a stand-alone model that maps text-to-prosodic features such as F0 and energy and can be used in downstream tasks such as TTS. The ProMode encoder takes as input acoustic features and time-aligned textual content, both are partially masked, and obtains a fixed-length latent prosodic embedding. The decoder predicts acoustics in the masked region using both the encoded prosody input and unmasked textual content. Trained on the GigaSpeech dataset, we compare our method with state-of-the-art style encoders. For F0 and energy predictions, we show consistent improvements for our model at different levels of granularity. We also integrate these predicted prosodic features into a TTS system and conduct perceptual tests, which show higher prosody preference compared to the baselines, demonstrating the model's potential in tasks where prosody modeling is important.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference</title>
<link>https://arxiv.org/abs/2508.09442</link>
<guid>https://arxiv.org/abs/2508.09442</guid>
<content:encoded><![CDATA[
arXiv:2508.09442v1 Announce Type: cross 
Abstract: The Key-Value (KV) cache, which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model (LLM) inference. However, this efficiency optimization introduces significant yet underexplored privacy risks. This paper provides the first comprehensive analysis of these vulnerabilities, demonstrating that an attacker can reconstruct sensitive user inputs directly from the KV-cache. We design and implement three distinct attack vectors: a direct Inversion Attack, a more broadly applicable and potent Collision Attack, and a semantic-based Injection Attack. These methods demonstrate the practicality and severity of KV-cache privacy leakage issues. To mitigate this, we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism. KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with operator fusion, to secure the KV-cache. Our extensive experiments show that KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction quality to random noise. Crucially, it achieves this robust security with virtually no degradation in model accuracy and minimal performance overhead, offering a practical solution for trustworthy LLM deployment.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding</title>
<link>https://arxiv.org/abs/2508.09456</link>
<guid>https://arxiv.org/abs/2508.09456</guid>
<content:encoded><![CDATA[
arXiv:2508.09456v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) have shown significant advancements in tasks such as visual grounding, where they localize specific objects in images based on natural language queries and images. However, security issues in visual grounding tasks for VLMs remain underexplored, especially in the context of backdoor attacks. In this paper, we introduce a novel input-aware backdoor attack method, IAG, designed to manipulate the grounding behavior of VLMs. This attack forces the model to ground a specific target object in the input image, regardless of the user's query. We propose an adaptive trigger generator that embeds the semantic information of the attack target's description into the original image using a text-conditional U-Net, thereby overcoming the open-vocabulary attack challenge. To ensure the attack's stealthiness, we utilize a reconstruction loss to minimize visual discrepancies between poisoned and clean images. Additionally, we introduce a unified method for generating attack data. IAG is evaluated theoretically and empirically, demonstrating its feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches over 65\% on various testing sets. IAG also shows promising potential on manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on clean samples. Extensive specific experiments, such as ablation study and potential defense, also indicate the robustness and transferability of our attack.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs</title>
<link>https://arxiv.org/abs/2508.09473</link>
<guid>https://arxiv.org/abs/2508.09473</guid>
<content:encoded><![CDATA[
arXiv:2508.09473v1 Announce Type: cross 
Abstract: Ensuring robust safety alignment while preserving utility is critical for the reliable deployment of Large Language Models (LLMs). However, current techniques fundamentally suffer from intertwined deficiencies: insufficient robustness against malicious attacks, frequent refusal of benign queries, degradation in generated text quality and general task performance--the former two reflecting deficits in robust safety and the latter constituting utility impairment. We trace these limitations to the coarse-grained layer-wise interventions in existing methods. To resolve this, we propose NeuronTune, a fine-grained framework that dynamically modulates sparse neurons to achieve simultaneous safety-utility optimization. Our approach first identifies safety-critical and utility-preserving neurons across all layers via attribution, then employs meta-learning to adaptively amplify safety-neuron activations and suppress utility-neuron activations. Crucially, NeuronTune enables tunable adjustment of intervention scope via neuron-count thresholds, supporting flexible adaptation to security-critical or utility-priority scenarios. Extensive experimental results demonstrate that our method significantly outperforms existing state-of-the-art technologies, achieving superior model safety while maintaining excellent utility.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Blob! LLM-Driven Recontextualization of Italian Television Archives</title>
<link>https://arxiv.org/abs/2508.09535</link>
<guid>https://arxiv.org/abs/2508.09535</guid>
<content:encoded><![CDATA[
arXiv:2508.09535v1 Announce Type: cross 
Abstract: This paper introduces AI Blob!, an experimental system designed to explore the potential of semantic cataloging and Large Language Models (LLMs) for the retrieval and recontextualization of archival television footage. Drawing methodological inspiration from Italian television programs such as Blob (RAI Tre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic embeddings, and retrieval-augmented generation (RAG) to organize and reinterpret archival content. The system processes a curated dataset of 1,547 Italian television videos by transcribing audio, segmenting it into sentence-level units, and embedding these segments into a vector database for semantic querying. Upon user input of a thematic prompt, the LLM generates a range of linguistically and conceptually related queries, guiding the retrieval and recombination of audiovisual fragments. These fragments are algorithmically selected and structured into narrative sequences producing montages that emulate editorial practices of ironic juxtaposition and thematic coherence. By foregrounding dynamic, content-aware retrieval over static metadata schemas, AI Blob! demonstrates how semantic technologies can facilitate new approaches to archival engagement, enabling novel forms of automated narrative construction and cultural analysis. The project contributes to ongoing debates in media historiography and AI-driven archival research, offering both a conceptual framework and a publicly available dataset to support further interdisciplinary experimentation.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments</title>
<link>https://arxiv.org/abs/2508.09614</link>
<guid>https://arxiv.org/abs/2508.09614</guid>
<content:encoded><![CDATA[
arXiv:2508.09614v1 Announce Type: cross 
Abstract: This study examines the rhetorical and linguistic features of argumentative texts generated by ChatGPT on ethically nuanced topics and investigates their persuasive impact on human readers.Through a user study involving 62 participants and pre-post interaction surveys, the paper analyzes how exposure to AI-generated arguments affects opinion change and user perception. A linguistic and rhetorical analysis of the generated texts reveals a consistent argumentative macrostructure, reliance on formulaic expressions, and limited stylistic richness. While ChatGPT demonstrates proficiency in constructing coherent argumentative texts, its persuasive efficacy appears constrained, particularly on topics involving ethical issues.The study finds that while participants often acknowledge the benefits highlighted by ChatGPT, ethical concerns tend to persist or even intensify post-interaction. The results also demonstrate a variation depending on the topic. These findings highlight new insights on AI-generated persuasion in ethically sensitive domains and are a basis for future research.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories</title>
<link>https://arxiv.org/abs/2508.09651</link>
<guid>https://arxiv.org/abs/2508.09651</guid>
<content:encoded><![CDATA[
arXiv:2508.09651v1 Announce Type: cross 
Abstract: The paper explores the study of gender-based narrative biases in stories generated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's character classifications and Freytag's narrative structure. The stories are analyzed through a close reading approach, with particular attention to adherence to the prompt, gender distribution of characters, physical and psychological descriptions, actions, and finally, plot development and character relationships. The results reveal the persistence of biases - especially implicit ones - in the generated stories and highlight the importance of assessing biases at multiple levels using an interpretative approach.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets</title>
<link>https://arxiv.org/abs/2508.09886</link>
<guid>https://arxiv.org/abs/2508.09886</guid>
<content:encoded><![CDATA[
arXiv:2508.09886v1 Announce Type: cross 
Abstract: Conventional single-dataset training often fails with new data distributions, especially in ultrasound (US) image analysis due to limited data, acoustic shadows, and speckle noise. Therefore, constructing a universal framework for multi-heterogeneous US datasets is imperative. However, a key challenge arises: how to effectively mitigate inter-dataset interference while preserving dataset-specific discriminative features for robust downstream task? Previous approaches utilize either a single source-specific decoder or a domain adaptation strategy, but these methods experienced a decline in performance when applied to other domains. Considering this, we propose a Universal Collaborative Mixture of Heterogeneous Source-Specific Experts (COME). Specifically, COME establishes dual structure-semantic shared experts that create a universal representation space and then collaborate with source-specific experts to extract discriminative features through providing complementary features. This design enables robust generalization by leveraging cross-datasets experience distributions and providing universal US priors for small-batch or unseen data scenarios. Extensive experiments under three evaluation modes (single-dataset, intra-organ, and inter-organ integration datasets) demonstrate COME's superiority, achieving significant mean AP improvements over state-of-the-art methods. Our project is available at: https://universalcome.github.io/UniversalCOME/.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation</title>
<link>https://arxiv.org/abs/2508.09987</link>
<guid>https://arxiv.org/abs/2508.09987</guid>
<content:encoded><![CDATA[
arXiv:2508.09987v1 Announce Type: cross 
Abstract: Recently, GPT-4o has garnered significant attention for its strong performance in image generation, yet open-source models still lag behind. Several studies have explored distilling image data from GPT-4o to enhance open-source models, achieving notable progress. However, a key question remains: given that real-world image datasets already constitute a natural source of high-quality data, why should we use GPT-4o-generated synthetic data? In this work, we identify two key advantages of synthetic images. First, they can complement rare scenarios in real-world datasets, such as surreal fantasy or multi-reference image generation, which frequently occur in user queries. Second, they provide clean and controllable supervision. Real-world data often contains complex background noise and inherent misalignment between text descriptions and image content, whereas synthetic images offer pure backgrounds and long-tailed supervision signals, facilitating more accurate text-to-image alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Using this dataset, we fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o. In addition, we propose two new evaluation benchmarks for a more accurate and challenging assessment of image generation capabilities: GenEval++, which increases instruction complexity to mitigate score saturation, and Imagine-Bench, which focuses on evaluating both the understanding and generation of imaginative content. Echo-4o demonstrates strong performance across standard benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the datasets strong transferability.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Stars to Insights: Exploration and Implementation of Unified Sentiment Analysis with Distant Supervision</title>
<link>https://arxiv.org/abs/2305.01710</link>
<guid>https://arxiv.org/abs/2305.01710</guid>
<content:encoded><![CDATA[
arXiv:2305.01710v4 Announce Type: replace 
Abstract: Sentiment analysis is integral to understanding the voice of the customer and informing businesses' strategic decisions. Conventional sentiment analysis involves three separate tasks: aspect-category detection, aspect-category sentiment analysis, and rating prediction. However, independently tackling these tasks can overlook their interdependencies and often requires expensive, fine-grained annotations. This paper introduces unified sentiment analysis, a novel learning paradigm that integrates the three aforementioned tasks into a coherent framework. To achieve this, we propose the Distantly Supervised Pyramid Network (DSPN), which employs a pyramid structure to capture sentiment at word, aspect, and document levels in a hierarchical manner. Evaluations on multi-aspect review datasets in English and Chinese show that DSPN, using only star rating labels for supervision, demonstrates significant efficiency advantages while performing comparably well to a variety of benchmark models. Additionally, DSPN's pyramid structure enables the interpretability of its outputs. Our findings validate DSPN's effectiveness and efficiency, establishing a robust, resource-efficient, unified framework for sentiment analysis.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning Code LLMs</title>
<link>https://arxiv.org/abs/2405.20179</link>
<guid>https://arxiv.org/abs/2405.20179</guid>
<content:encoded><![CDATA[
arXiv:2405.20179v4 Announce Type: replace 
Abstract: Code LLMs have shown promising results with converting tasks in natural language to programs that can be executed by service robots. We are interested in finetuning small, specialized LLMs for this purpose, but collecting datasets of task-program pairs specific to each robot is time-consuming and expensive. While approaches such as SELF-INSTRUCT and EVOL-INSTRUCT are capable of generating novel tasks given a few examples, they are unable to provide the corresponding programs that correctly abide by physical-world and robot-constraints using the provided programming interface. Using a simulator is a natural potential solution to checking for such constraints, but building simulation environments that can handle arbitrary tasks and their necessary objects and locations, is challenging. To address these challenges, we introduce ROBO-INSTRUCT, which synthesizes task-specific simulation environments on the fly during program execution, by opportunistically inferring entity properties and enforcing corresponding constraints based on how the entities are used in the task program. Additionally, ROBO-INSTRUCT integrates an LLM-aided post-processing procedure to refine instructions for better alignment with robot programs. We demonstrate the effectiveness of ROBO-INSTRUCT across multiple LLMs, showing that our fine-tuned models outperform all baseline methods and even match or surpass the performance of several larger and proprietary models.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongIns: A Challenging Long-context Instruction-based Exam for LLMs</title>
<link>https://arxiv.org/abs/2406.17588</link>
<guid>https://arxiv.org/abs/2406.17588</guid>
<content:encoded><![CDATA[
arXiv:2406.17588v3 Announce Type: replace 
Abstract: The long-context capabilities of large language models (LLMs) have been a hot topic in recent years. To evaluate the performance of LLMs in different scenarios, various assessment benchmarks have emerged. However, as most of these benchmarks focus on identifying key information to answer questions, which mainly requires the retrieval ability of LLMs, these benchmarks can partially represent the reasoning performance of LLMs from large amounts of information. Meanwhile, although LLMs often claim to have context windows of 32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual supported length of these LLMs. To address these issues, we propose the LongIns benchmark dataset, a challenging long-context instruction-based exam for LLMs, which is built based on the existing instruction datasets. Specifically, in our LongIns, we introduce three evaluation settings: Global Instruction & Single Task (GIST), Local Instruction & Single Task (LIST), and Local Instruction & Multiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations on existing LLMs and have the following important findings: (1). The top-performing GPT-4 with 128k context length performs poorly on the evaluation context window of 16k in our LongIns. (2). For the multi-hop reasoning ability of many existing LLMs, significant efforts are still needed under short context windows (less than 4k).
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Multimodal Large Language Models Using Continual Learning</title>
<link>https://arxiv.org/abs/2410.19925</link>
<guid>https://arxiv.org/abs/2410.19925</guid>
<content:encoded><![CDATA[
arXiv:2410.19925v2 Announce Type: replace 
Abstract: Generative large language models (LLMs) exhibit impressive capabilities, which can be further augmented by integrating a pre-trained vision model into the original LLM to create a multimodal LLM (MLLM). However, this integration often significantly decreases performance on natural language understanding and generation tasks, compared to the original LLM. This study investigates this issue using the LLaVA MLLM, treating the integration as a continual learning problem. We evaluate five continual learning methods to mitigate forgetting and identify a technique that enhances visual understanding while minimizing linguistic performance loss. Our approach reduces linguistic performance degradation by up to 15% over the LLaVA recipe, while maintaining high multimodal accuracy. We also demonstrate the robustness of our method through continual learning on a sequence of vision-language tasks, effectively preserving linguistic skills while acquiring new multimodal capabilities. Project webpage: https://shikhar-srivastava.github.io/cl-for-improving-mllms
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs Performance</title>
<link>https://arxiv.org/abs/2412.10417</link>
<guid>https://arxiv.org/abs/2412.10417</guid>
<content:encoded><![CDATA[
arXiv:2412.10417v2 Announce Type: replace 
Abstract: Mental health disorders are increasingly prevalent worldwide, creating an urgent need for innovative tools to support early diagnosis and intervention. This study explores the potential of Large Language Models (LLMs) in multimodal mental health diagnostics, specifically for detecting depression and Post Traumatic Stress Disorder through text and audio modalities. Using the E-DAIC dataset, we compare text and audio modalities to investigate whether LLMs can perform equally well or better with audio inputs. We further examine the integration of both modalities to determine if this can enhance diagnostic accuracy, which generally results in improved performance metrics. Our analysis specifically utilizes custom-formulated metrics; Modal Superiority Score and Disagreement Resolvement Score to evaluate how combined modalities influence model performance. The Gemini 1.5 Pro model achieves the highest scores in binary depression classification when using the combined modality, with an F1 score of 0.67 and a Balanced Accuracy (BA) of 77.4%, assessed across the full dataset. These results represent an increase of 3.1% over its performance with the text modality and 2.7% over the audio modality, highlighting the effectiveness of integrating modalities to enhance diagnostic accuracy. Notably, all results are obtained in zero-shot inferring, highlighting the robustness of the models without requiring task-specific fine-tuning. To explore the impact of different configurations on model performance, we conduct binary, severity, and multiclass tasks using both zero-shot and few-shot prompts, examining the effects of prompt variations on performance. The results reveal that models such as Gemini 1.5 Pro in text and audio modalities, and GPT-4o mini in the text modality, often surpass other models in balanced accuracy and F1 scores across multiple tasks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization Over Reasoning? Exposing and Mitigating Verbatim Memorization in Large Language Models' Character Understanding Evaluation</title>
<link>https://arxiv.org/abs/2412.14368</link>
<guid>https://arxiv.org/abs/2412.14368</guid>
<content:encoded><![CDATA[
arXiv:2412.14368v5 Announce Type: replace 
Abstract: Recently, Large Language Models (LLMs) have shown impressive performance in character understanding tasks, such as analyzing the roles, personalities, and relationships of fictional characters. However, the extensive pre-training corpora used by LLMs raise concerns that they may rely on memorizing popular fictional works rather than genuinely understanding and reasoning about them. In this work, we argue that 'gist memory'-capturing essential meaning - should be the primary mechanism for character understanding tasks, as opposed to 'verbatim memory' - exact match of a string. We introduce a simple yet effective method to mitigate mechanized memorization in character understanding evaluations while preserving the essential implicit cues needed for comprehension and reasoning. Our approach reduces memorization-driven performance on popular fictional works from 96% accuracy to 72% and results in up to an 18% drop in accuracy across various character understanding tasks. These findings underscore the issue of data contamination in existing benchmarks, which often measure memorization rather than true character understanding.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Memorization: Assessing Semantic Generalization in Large Language Models Using Phrasal Constructions</title>
<link>https://arxiv.org/abs/2501.04661</link>
<guid>https://arxiv.org/abs/2501.04661</guid>
<content:encoded><![CDATA[
arXiv:2501.04661v2 Announce Type: replace 
Abstract: The web-scale of pretraining data has created an important evaluation challenge: to disentangle linguistic competence on cases well-represented in pretraining data from generalization to out-of-domain language, specifically the dynamic, real-world instances less common in pretraining data. To this end, we construct a diagnostic evaluation to systematically assess natural language understanding in LLMs by leveraging Construction Grammar (CxG). CxG provides a psycholinguistically grounded framework for testing generalization, as it explicitly links syntactic forms to abstract, non-lexical meanings. Our novel inference evaluation dataset consists of English phrasal constructions, for which speakers are known to be able to abstract over commonplace instantiations in order to understand and produce creative instantiations. Our evaluation dataset uses CxG to evaluate two central questions: first, if models can 'understand' the semantics of sentences for instances that are likely to appear in pretraining data less often, but are intuitive and easy for people to understand. Second, if LLMs can deploy the appropriate constructional semantics given constructions that are syntactically identical but with divergent meanings. Our results demonstrate that state-of-the-art models, including GPT-o1, exhibit a performance drop of over 40% on our second task, revealing a failure to generalize over syntactically identical forms to arrive at distinct constructional meanings in the way humans do. We make our novel dataset and associated experimental data, including prompts and model responses, publicly available.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLMs' Mathematical Reasoning with Unseen Random Variables Questions</title>
<link>https://arxiv.org/abs/2501.11790</link>
<guid>https://arxiv.org/abs/2501.11790</guid>
<content:encoded><![CDATA[
arXiv:2501.11790v4 Announce Type: replace 
Abstract: Recent studies have raised significant concerns regarding the reliability of current mathematics benchmarks, highlighting issues such as simplistic design and potential data contamination. Consequently, developing a reliable benchmark that effectively evaluates large language models' (LLMs) genuine capabilities in mathematical reasoning remains a critical challenge. To address these concerns, we propose RV-Bench, a novel evaluation methodology for Benchmarking LLMs with Random Variables in mathematical reasoning. Specifically, we build question-generating functions to produce random variable questions (RVQs), whose background content mirrors original benchmark problems, but with randomized variable combinations, rendering them "unseen" to LLMs. Models must completely understand the inherent question pattern to correctly answer RVQs with diverse variable combinations. Thus, an LLM's genuine reasoning capability is reflected through its accuracy and robustness on RV-Bench. We conducted extensive experiments on over 30 representative LLMs across more than 1,000 RVQs. Our findings propose that LLMs exhibit a proficiency imbalance between encountered and ``unseen'' data distributions. Furthermore, RV-Bench reveals that proficiency generalization across similar mathematical reasoning tasks is limited, but we verified it can still be effectively elicited through test-time scaling.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression</title>
<link>https://arxiv.org/abs/2502.14051</link>
<guid>https://arxiv.org/abs/2502.14051</guid>
<content:encoded><![CDATA[
arXiv:2502.14051v3 Announce Type: replace 
Abstract: Transformer-based Large Language Models rely critically on the KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy containing two consecutive stages. In the first stage, it performs coarse-grain permanent KV cache eviction on the input sequence tokens. In the second stage, it adopts a hybrid sparse attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensionality reductions. We show that RocketKV provides a compression ratio of up to 400$\times$, end-to-end speedup of up to 3.7$\times$ as well as peak memory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks. We also propose a variant of RocketKV for multi-turn scenarios, which consistently outperforms other existing methods and achieves accuracy nearly on par with an oracle top-k attention scheme. The source code is available here: https://github.com/NVlabs/RocketKV.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Inference for Large Reasoning Models: A Survey</title>
<link>https://arxiv.org/abs/2503.23077</link>
<guid>https://arxiv.org/abs/2503.23077</guid>
<content:encoded><![CDATA[
arXiv:2503.23077v3 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in solving complex tasks. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. The overview structure of this paper is shown in Figure~\ref{fig:paper_structure}. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from reasoning scenarios, object functions, and performance \& efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring the safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs' inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant field. A collection of efficient reasoning methods for LRMs (papers and codes) is provided at this link: https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Follow the Flow: On Information Flow Across Textual Tokens in Text-to-Image Models</title>
<link>https://arxiv.org/abs/2504.01137</link>
<guid>https://arxiv.org/abs/2504.01137</guid>
<content:encoded><![CDATA[
arXiv:2504.01137v2 Announce Type: replace 
Abstract: Text-to-image (T2I) models generate images by encoding text prompts into token representations, which then guide the diffusion process. While prior work has largely focused on improving alignment by refining the diffusion process, we focus on the textual encoding stage. Specifically, we investigate how semantic information is distributed across token representations within and between lexical items (i.e., words or expressions conveying a single concept) in the prompt. We analyze information flow at two levels: (1) in-item representation-whether individual tokens represent their lexical item, and (2) cross-item interaction-whether information flows across the tokens of different lexical items. We use patching techniques to uncover surprising encoding patterns. We find information is usually concentrated in only one or two of the item's tokens-For example, in the item "San Francisco's Golden Gate Bridge", the token "Gate" sufficiently captures the entire expression while the other tokens could effectively be discarded. Lexical items also tend to remain isolated; for instance, the token "dog" encodes no visual information about "green" in the prompt "a green dog". However, in some cases, items do influence each other's representation, often leading to misinterpretations-e.g., in the prompt "a pool by a table", the token pool represents a pool table after contextualization. Our findings highlight the critical role of token-level encoding in image generation, suggesting that misalignment issues may originate already during the textual encoding.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2504.04310</link>
<guid>https://arxiv.org/abs/2504.04310</guid>
<content:encoded><![CDATA[
arXiv:2504.04310v2 Announce Type: replace 
Abstract: Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial optimization (CO) remains relatively underexplored. This gap underscores the need for a deeper understanding of their potential in tackling structured, constraint-intensive problems -- a pursuit currently limited by the absence of comprehensive benchmarks for systematic investigation. To address this, we introduce CO-Bench, a benchmark suite featuring 36 real-world CO problems drawn from a broad range of domains and complexity levels. CO-Bench includes structured problem formulations and curated data to support rigorous investigation of LLM agents. We evaluate multiple agentic frameworks against established human-designed algorithms, revealing the strengths and limitations of existing LLM agents and identifying promising directions for future research. CO-Bench is publicly available at https://github.com/sunnweiwei/CO-Bench.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation</title>
<link>https://arxiv.org/abs/2504.07532</link>
<guid>https://arxiv.org/abs/2504.07532</guid>
<content:encoded><![CDATA[
arXiv:2504.07532v3 Announce Type: replace 
Abstract: AI-generated text is proliferating across domains, from creative writing and journalism to marketing content and scientific articles. Models can follow user-provided instructions to generate coherent and grammatically correct outputs but in this work, we study a more fundamental question: how do we evaluate and improve the writing quality of AI-generated text? Writing quality assessment has received less attention from the community, in part because it is fundamentally subjective and requires expertise. We first introduce the Writing Quality Benchmark (WQ) by consolidating five writing-preference datasets into 4,729 writing quality judgments. Our experiments show that most of the competitive baselines, including state-of-the-art LLMs that excel at reasoning tasks, barely outperform random baselines on WQ. We then train specialized Writing Quality Reward Models (WQRM) of various sizes for writing quality assessment that demonstrate strong generalization on four out-of-distribution test sets and 74% accuracy on the WQ benchmark. To further show WQRM's practical benefits during inference, we leverage additional test-time compute to generate and rank multiple candidate revisions, allowing us to select higher-quality outputs from an initial draft. Human evaluation with 9 experienced writers confirm that WQRM-based selection produces writing samples preferred by experts 66% overall, and 72.2% when the reward gap is larger than 1 point. We release our datasets and models to encourage community engagement with writing quality assessment and development of AI writing systems better aligned with human preferences.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-native Children's Automatic Speech Assessment Challenge (NOCASA)</title>
<link>https://arxiv.org/abs/2504.20678</link>
<guid>https://arxiv.org/abs/2504.20678</guid>
<content:encoded><![CDATA[
arXiv:2504.20678v2 Announce Type: replace 
Abstract: This paper presents the "Non-native Children's Automatic Speech Assessment" (NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA challenges participants to develop new systems that can assess single-word pronunciations of young second language (L2) learners as part of a gamified pronunciation training app. To achieve this, several issues must be addressed, most notably the limited nature of available training data and the highly unbalanced distribution among the pronunciation level categories. To expedite the development, we provide a pseudo-anonymized training data (TeflonNorL2), containing 10,334 recordings from 44 speakers attempting to pronounce 205 distinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that should be given in the game). In addition to the data, two already trained systems are released as official baselines: an SVM classifier trained on the ComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter achieves the best performance on the challenge test set, with an unweighted average recall (UAR) of 36.37%.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports</title>
<link>https://arxiv.org/abs/2505.00191</link>
<guid>https://arxiv.org/abs/2505.00191</guid>
<content:encoded><![CDATA[
arXiv:2505.00191v2 Announce Type: replace 
Abstract: The development of AI-based methods to analyze radiology reports could lead to significant advances in medical diagnosis, from improving diagnostic accuracy to enhancing efficiency and reducing workload. However, the lack of interpretability of AI-based methods could hinder their adoption in clinical settings. In this paper, we propose an interpretable-by-design framework for classifying chest radiology reports. First, we extract a set of representative facts from a large set of reports. Then, given a new report, we query whether a small subset of the representative facts is entailed by the report, and predict a diagnosis based on the selected subset of query-answer pairs. The explanation for a prediction is, by construction, the set of selected queries and answers. We use the Information Pursuit framework to select the most informative queries, a natural language inference model to determine if a fact is entailed by the report, and a classifier to predict the disease. Experiments on the MIMIC-CXR dataset demonstrate the effectiveness of the proposed method, highlighting its potential to enhance trust and usability in medical AI.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs</title>
<link>https://arxiv.org/abs/2505.02009</link>
<guid>https://arxiv.org/abs/2505.02009</guid>
<content:encoded><![CDATA[
arXiv:2505.02009v3 Announce Type: replace 
Abstract: Large language models (LLMs) have become integral to various real-world applications, leveraging massive, web-sourced datasets like Common Crawl, C4, and FineWeb for pretraining. While these datasets provide linguistic data essential for high-quality natural language generation, they often contain harmful content, such as hate speech, misinformation, and biased narratives. Training LLMs on such unfiltered data risks perpetuating toxic behaviors, spreading misinformation, and amplifying societal biases which can undermine trust in LLM-driven applications and raise ethical concerns about their use. This paper presents a large-scale analysis of inappropriate content across these datasets, offering a comprehensive taxonomy that categorizes harmful webpages into Topical and Toxic based on their intent. We also introduce a prompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and a transformer-based model (HarmFormer) for harmful content filtering. Additionally, we create a new multi-harm open-ended toxicity benchmark (HAVOC) and provide crucial insights into how models respond to adversarial toxic inputs. We share TTP, TTP-Eval, HAVOC and a sample of C4 inferenced on HarmFormer. Our work offers insights into ensuring safer LLM pretraining and serves as a resource for Responsible AI (RAI) compliance.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16483</link>
<guid>https://arxiv.org/abs/2505.16483</guid>
<content:encoded><![CDATA[
arXiv:2505.16483v2 Announce Type: replace 
Abstract: Teaching large language models (LLMs) to be faithful in the provided context is crucial for building reliable information-seeking systems. Therefore, we propose a systematic framework, CANOE, to reduce faithfulness hallucinations of LLMs across different downstream tasks without human annotations. Specifically, we first synthesize short-form question-answering (QA) data with four diverse tasks to construct high-quality and easily verifiable training data without human annotation. Also, we propose Dual-GRPO, a rule-based reinforcement learning method that includes three tailored rule-based rewards derived from synthesized short-form QA data, while simultaneously optimizing both short-form and long-form response generation. Notably, Dual-GRPO eliminates the need to manually label preference data to train reward models and avoids over-optimizing short-form generation when relying only on the synthesized short-form QA data. Experimental results show that CANOE greatly improves the faithfulness of LLMs across 11 different tasks, even outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Complex Reasoning</title>
<link>https://arxiv.org/abs/2505.18744</link>
<guid>https://arxiv.org/abs/2505.18744</guid>
<content:encoded><![CDATA[
arXiv:2505.18744v2 Announce Type: replace 
Abstract: Text-to-SQL is a critical task in natural language processing that aims to transform natural language questions into accurate and executable SQL queries. In real-world scenarios, these reasoning tasks are often accompanied by complex mathematical computations, domain knowledge, and hypothetical reasoning scenarios. However, existing large-scale Text-to-SQL datasets typically focus on business logic and task logic, neglecting critical factors such as vertical domain knowledge, complex mathematical reasoning, and hypothetical reasoning, which are essential for realistically reflecting the reasoning demands in practical applications and completing data querying and analysis. To bridge this gap, we introduce LogicCat, the first Text-to-SQL benchmark dataset specifically designed for complex reasoning and chain-of-thought parsing, encompassing physics, arithmetic, commonsense, and hypothetical reasoning scenarios. LogicCat comprises 4,038 English questions paired 12,114 detailed chain-of-thought reasoning steps, spanning 45 databases across diverse domains, significantly surpassing existing datasets in complexity. Experimental results demonstrate that LogicCat substantially increases the task difficulty for current state-of-the-art models to at most 33.20% execution accuracy, indicating that this task remains exceptionally challenging. The advancement of LogicCat represents a crucial step toward developing systems suitable for real-world enterprise data analysis and autonomous query generation. We have released our dataset code at https://github.com/Ffunkytao/LogicCat.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemGuide: Intent-Driven Memory Selection for Goal-Oriented Multi-Session LLM Agents</title>
<link>https://arxiv.org/abs/2505.20231</link>
<guid>https://arxiv.org/abs/2505.20231</guid>
<content:encoded><![CDATA[
arXiv:2505.20231v2 Announce Type: replace 
Abstract: Modern task-oriented dialogue (TOD) systems increasingly rely on large language model (LLM) agents, leveraging Retrieval-Augmented Generation (RAG) and long-context capabilities for long-term memory utilization. However, these methods are primarily based on semantic similarity, overlooking task intent and reducing task coherence in multi-session dialogues. To address this challenge, we introduce MemGuide, a two-stage framework for intent-driven memory selection. (1) Intent-Aligned Retrieval matches the current dialogue context with stored intent descriptions in the memory bank, retrieving QA-formatted memory units that share the same goal. (2) Missing-Slot Guided Filtering employs a chain-of-thought slot reasoner to enumerate unfilled slots, then uses a fine-tuned LLaMA-8B filter to re-rank the retrieved units by marginal slot-completion gain. The resulting memory units inform a proactive strategy that minimizes conversational turns by directly addressing information gaps. Based on this framework, we introduce the MS-TOD, the first multi-session TOD benchmark comprising 132 diverse personas, 956 task goals, and annotated intent-aligned memory targets, supporting efficient multi-session task completion. Evaluations on MS-TOD show that MemGuide raises the task success rate by 11% (88% -> 99%) and reduces dialogue length by 2.84 turns in multi-session settings, while maintaining parity with single-session benchmarks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Scaling Laws for EHR Foundation Models</title>
<link>https://arxiv.org/abs/2505.22964</link>
<guid>https://arxiv.org/abs/2505.22964</guid>
<content:encoded><![CDATA[
arXiv:2505.22964v2 Announce Type: replace 
Abstract: The emergence of scaling laws has profoundly shaped the development of large language models (LLMs), enabling predictable performance gains through systematic increases in model size, dataset volume, and compute. Yet, these principles remain largely unexplored in the context of electronic health records (EHRs) -- a rich, sequential, and globally abundant data source that differs structurally from natural language. In this work, we present the first empirical investigation of scaling laws for EHR foundation models. By training transformer architectures on patient timeline data from the MIMIC-IV database across varying model sizes and compute budgets, we identify consistent scaling patterns, including parabolic IsoFLOPs curves and power-law relationships between compute, model parameters, data size, and clinical utility. These findings demonstrate that EHR models exhibit scaling behavior analogous to LLMs, offering predictive insights into resource-efficient training strategies. Our results lay the groundwork for developing powerful EHR foundation models capable of transforming clinical prediction tasks and advancing personalized healthcare.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques</title>
<link>https://arxiv.org/abs/2506.00658</link>
<guid>https://arxiv.org/abs/2506.00658</guid>
<content:encoded><![CDATA[
arXiv:2506.00658v2 Announce Type: replace 
Abstract: Sarcasm is a form of humor where expressions convey meanings opposite to their literal interpretations. Classifying and generating sarcasm using large language models is vital for interpreting human communication. Sarcasm poses challenges for computational models, due to its nuanced nature. We introduce Sarc7, a benchmark that classifies 7 types of sarcasm: self-deprecating, brooding, deadpan, polite, obnoxious, raging, and manic by annotating entries of the MUStARD dataset. Classification was evaluated using zero-shot, few-shot, chain-of-thought (CoT), and a novel emotion-based prompting technique. We propose an emotion-based generation method developed by identifying key components of sarcasm-incongruity, shock value, and context dependency. Our classification experiments show that Gemini 2.5, using emotion-based prompting, outperforms other setups with an F1 score of 0.3664. Human evaluators preferred our emotion-based prompting, with 38.46% more successful generations than zero-shot prompting.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments</title>
<link>https://arxiv.org/abs/2506.00739</link>
<guid>https://arxiv.org/abs/2506.00739</guid>
<content:encoded><![CDATA[
arXiv:2506.00739v3 Announce Type: replace 
Abstract: Large language model (LLM) agents have shown impressive capabilities in human language comprehension and reasoning, yet their potential in cybersecurity remains underexplored. We introduce DefenderBench, a practical, open-source toolkit for evaluating language agents across offense, defense, and cybersecurity knowledge-based tasks. DefenderBench includes environments for network intrusion, malicious content detection, code vulnerability analysis, and cybersecurity knowledge assessment. It is intentionally designed to be affordable and easily accessible for researchers while providing fair and rigorous assessment. We benchmark several state-of-the-art (SoTA) and popular LLMs, including both open- and closed-weight models, using a standardized agentic framework. Our results show that Claude-3.7-sonnet performs best with a DefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40, while the best open-weight model, Llama 3.3 70B, is not far behind with a DefenderBench score of 71.81. DefenderBench's modular design allows seamless integration of custom LLMs and tasks, promoting reproducibility and fair comparisons. An anonymized version of DefenderBench is available at https://github.com/microsoft/DefenderBench.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs</title>
<link>https://arxiv.org/abs/2507.10772</link>
<guid>https://arxiv.org/abs/2507.10772</guid>
<content:encoded><![CDATA[
arXiv:2507.10772v2 Announce Type: replace 
Abstract: Labeled property graphs often contain rich textual attributes that can enhance analytical tasks when properly leveraged. This work explores the use of pretrained text embedding models to enable efficient semantic analysis in such graphs. By embedding textual node and edge properties, we support downstream tasks including node classification and relation prediction with improved contextual understanding. Our approach integrates language model embeddings into the graph pipeline without altering its structure, demonstrating that textual semantics can significantly enhance the accuracy and interpretability of property graph analysis.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Neural Algorithmic Reasoning</title>
<link>https://arxiv.org/abs/2402.11628</link>
<guid>https://arxiv.org/abs/2402.11628</guid>
<content:encoded><![CDATA[
arXiv:2402.11628v3 Announce Type: replace-cross 
Abstract: Neural algorithmic reasoning aims to capture computations with neural networks by training models to imitate the execution of classical algorithms. While common architectures are expressive enough to contain the correct model in the weight space, current neural reasoners struggle to generalize well on out-of-distribution data. On the other hand, classical computations are not affected by distributional shifts as they can be described as transitions between discrete computational states. In this work, we propose to force neural reasoners to maintain the execution trajectory as a combination of finite predefined states. To achieve this, we separate discrete and continuous data flows and describe the interaction between them. Trained with supervision on the algorithm's state transitions, such models are able to perfectly align with the original algorithm. To show this, we evaluate our approach on multiple algorithmic problems and achieve perfect test scores both in single-task and multitask setups. Moreover, the proposed architectural choice allows us to prove the correctness of the learned algorithms for any test data.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal Data</title>
<link>https://arxiv.org/abs/2406.09864</link>
<guid>https://arxiv.org/abs/2406.09864</guid>
<content:encoded><![CDATA[
arXiv:2406.09864v3 Announce Type: replace-cross 
Abstract: Multimodal Deep Learning enhances decision-making by integrating diverse information sources, such as texts, images, audio, and videos. To develop trustworthy multimodal approaches, it is essential to understand how uncertainty impacts these models. We propose LUMA, a unique multimodal dataset, featuring audio, image, and textual data from 50 classes, specifically designed for learning from uncertain data. It extends the well-known CIFAR 10/100 dataset with audio samples extracted from three audio corpora, and text data generated using the Gemma-7B Large Language Model (LLM). The LUMA dataset enables the controlled injection of varying types and degrees of uncertainty to achieve and tailor specific experiments and benchmarking initiatives. LUMA is also available as a Python package including the functions for generating multiple variants of the dataset with controlling the diversity of the data, the amount of noise for each modality, and adding out-of-distribution samples. A baseline pre-trained model is also provided alongside three uncertainty quantification methods: Monte-Carlo Dropout, Deep Ensemble, and Reliable Conflictive Multi-View Learning. This comprehensive dataset and its tools are intended to promote and support the development, evaluation, and benchmarking of trustworthy and robust multimodal deep learning approaches. We anticipate that the LUMA dataset will help the research community to design more trustworthy and robust machine learning approaches for safety critical applications. The code and instructions for downloading and processing the dataset can be found at: https://github.com/bezirganyan/LUMA/ .
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Step Reasoning with Large Language Models, a Survey</title>
<link>https://arxiv.org/abs/2407.11511</link>
<guid>https://arxiv.org/abs/2407.11511</guid>
<content:encoded><![CDATA[
arXiv:2407.11511v2 Announce Type: replace-cross 
Abstract: Language models with billions of parameters exhibit in-context learning abilities, enabling few-shot learning on tasks that the model was not specifically trained for. Traditional models achieve breakthrough performance on language tasks, but do not perform well on basic reasoning benchmarks. However, a new in-context learning approach, Chain-of-thought, has demonstrated strong multi-step reasoning abilities on these benchmarks.
  The research on LLM reasoning abilities started with the question whether LLMs can solve grade school math word problems, and has expanded to other tasks in the past few years. This paper reviews the field of multi-step reasoning with LLMs. We propose a taxonomy that identifies different ways to generate, evaluate, and control multi-step reasoning. We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near future.
  We find that multi-step reasoning approaches have progressed beyond math word problems, and can now successfully solve challenges in logic, combinatorial games, and robotics, sometimes by first generating code that is then executed by external tools. Many studies in multi-step methods are using reinforcement learning for finetuning, external optimization loops, in context reinforcement learning, and self-reflection.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Caption-Image Interactions in CLIP Models with Second-Order Attributions</title>
<link>https://arxiv.org/abs/2408.14153</link>
<guid>https://arxiv.org/abs/2408.14153</guid>
<content:encoded><![CDATA[
arXiv:2408.14153v4 Announce Type: replace-cross 
Abstract: Dual encoder architectures like Clip models map two types of inputs into a shared embedding space and predict similarities between them. Despite their wide application, it is, however, not understood how these models compare their two inputs. Common first-order feature-attribution methods explain importances of individual features and can, thus, only provide limited insights into dual encoders, whose predictions depend on interactions between features. In this paper, we first derive a second-order method enabling the attribution of predictions by any differentiable dual encoder onto feature-interactions between its inputs. Second, we apply our method to Clip models and show that they learn fine-grained correspondences between parts of captions and regions in images. They match objects across input modes and also account for mismatches. This intrinsic visual-linguistic grounding ability, however, varies heavily between object classes, exhibits pronounced out-of-domain effects and we can identify individual errors as well as systematic failure categories. Code is publicly available: https://github.com/lucasmllr/exCLIP
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Finetuning Representation Shift for Multimodal LLMs Steering</title>
<link>https://arxiv.org/abs/2501.03012</link>
<guid>https://arxiv.org/abs/2501.03012</guid>
<content:encoded><![CDATA[
arXiv:2501.03012v2 Announce Type: replace-cross 
Abstract: Multimodal LLMs (MLLMs) have reached remarkable levels of proficiency in understanding multimodal inputs. However, understanding and interpreting the behavior of such complex models is a challenging task, not to mention the dynamic shifts that may occur during fine-tuning, or due to covariate shift between datasets. In this work, we apply concept-level analysis towards MLLM understanding. More specifically, we propose to map hidden states to interpretable visual and textual concepts. This enables us to more efficiently compare certain semantic dynamics, such as the shift from an original and fine-tuned model, revealing concept alteration and potential biases that may occur during fine-tuning. We also demonstrate the use of shift vectors to capture these concepts changes. These shift vectors allow us to recover fine-tuned concepts by applying simple, computationally inexpensive additive concept shifts in the original model. Finally, our findings also have direct applications for MLLM steering, which can be used for model debiasing as well as enforcing safety in MLLM output. All in all, we propose a novel, training-free, ready-to-use framework for MLLM behavior interpretability and control. Our implementation is publicly available.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shifting Perspectives: Steering Vectors for Robust Bias Mitigation in LLMs</title>
<link>https://arxiv.org/abs/2503.05371</link>
<guid>https://arxiv.org/abs/2503.05371</guid>
<content:encoded><![CDATA[
arXiv:2503.05371v2 Announce Type: replace-cross 
Abstract: We present a novel approach to bias mitigation in large language models (LLMs) by applying steering vectors to modify model activations in forward passes. We compute 8 steering vectors, each corresponding to a different social bias axis, such as age, gender, or race, on a training subset of the BBQ dataset and compare the effectiveness of these to 3 additional bias mitigation methods across 4 datasets. When optimized on the BBQ dataset, our individually tuned steering vectors achieve average improvements of 12.8% on BBQ, 8.3% on CLEAR-Bias, and 1% on StereoSet, and show improvements over prompting and Self-Debias in all cases, and improvements over fine-tuning in 12 out of 17 evaluations. In addition, steering vectors showed the lowest impact on MMLU scores of the four bias mitigation methods tested. The work presents the first systematic investigation of steering vectors for bias mitigation, and we demonstrate that they are a powerful and computationally efficient strategy for reducing bias in LLMs, with broader implications for enhancing AI safety.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedRep: Medical Concept Representation for General Electronic Health Record Foundation Models</title>
<link>https://arxiv.org/abs/2504.08329</link>
<guid>https://arxiv.org/abs/2504.08329</guid>
<content:encoded><![CDATA[
arXiv:2504.08329v2 Announce Type: replace-cross 
Abstract: Electronic health record (EHR) foundation models have been an area ripe for exploration with their improved performance in various medical tasks. Despite the rapid advances, there exists a fundamental limitation: Processing unseen medical codes out of the vocabulary. This problem limits the generality of EHR foundation models and the integration of models trained with different vocabularies. To deal with this problem, we propose MedRep for EHR foundation models based on the observational medical outcome partnership (OMOP) common data model (CDM), providing the integrated medical concept representations and the basic data augmentation strategy for patient trajectories. For concept representation learning, we enrich the information of each concept with a minimal definition through large language model (LLM) prompts and enhance the text-based representations through graph ontology of OMOP vocabulary. Trajectory augmentation randomly replaces selected concepts with other similar concepts that have closely related representations to let the model practice with the concepts out-of-vocabulary. Finally, we demonstrate that EHR foundation models trained with MedRep better maintain the prediction performance in external datasets. Our code implementation is publicly available at https://github.com/kicarussays/MedRep.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting</title>
<link>https://arxiv.org/abs/2504.12867</link>
<guid>https://arxiv.org/abs/2504.12867</guid>
<content:encoded><![CDATA[
arXiv:2504.12867v4 Announce Type: replace-cross 
Abstract: Human speech goes beyond the mere transfer of information; it is a profound exchange of emotions and a connection between individuals. While Text-to-Speech (TTS) models have made huge progress, they still face challenges in controlling the emotional expression in the generated speech. In this work, we propose EmoVoice, a novel emotion-controllable TTS model that exploits large language models (LLMs) to enable fine-grained freestyle natural language emotion control, and a phoneme boost variant design that makes the model output phoneme tokens and audio tokens in parallel to enhance content consistency, inspired by chain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we introduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring expressive speech and fine-grained emotion labels with natural language descriptions. EmoVoice achieves state-of-the-art performance on the English EmoVoice-DB test set using only synthetic training data, and on the Chinese Secap test set using our in-house data. We further investigate the reliability of existing emotion evaluation metrics and their alignment with human perceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and Gemini to assess emotional speech. Dataset, code, checkpoints, and demo samples are available at https://github.com/yanghaha0908/EmoVoice.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MapStory: Prototyping Editable Map Animations with LLM Agents</title>
<link>https://arxiv.org/abs/2505.21966</link>
<guid>https://arxiv.org/abs/2505.21966</guid>
<content:encoded><![CDATA[
arXiv:2505.21966v2 Announce Type: replace-cross 
Abstract: We introduce MapStory, an LLM-powered animation prototyping tool that generates editable map animation sequences directly from natural language text by leveraging a dual-agent LLM architecture. Given a user written script, MapStory automatically produces a scene breakdown, which decomposes the text into key map animation primitives such as camera movements, visual highlights, and animated elements. Our system includes a researcher agent that accurately queries geospatial information by leveraging an LLM with web search, enabling automatic extraction of relevant regions, paths, and coordinates while allowing users to edit and query for changes or additional information to refine the results. Additionally, users can fine-tune parameters of these primitive blocks through an interactive timeline editor. We detail the system's design and architecture, informed by formative interviews with professional animators and by an analysis of 200 existing map animation videos. Our evaluation, which includes expert interviews (N=5) and a usability study (N=12), demonstrates that MapStory enables users to create map animations with ease, facilitates faster iteration, encourages creative exploration, and lowers barriers to creating map-centric stories.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism</title>
<link>https://arxiv.org/abs/2508.00554</link>
<guid>https://arxiv.org/abs/2508.00554</guid>
<content:encoded><![CDATA[
arXiv:2508.00554v2 Announce Type: replace-cross 
Abstract: In financial trading, large language model (LLM)-based agents demonstrate significant potential. However, the high sensitivity to market noise undermines the performance of LLM-based trading systems. To address this limitation, we propose a novel multi-agent system featuring an internal competitive mechanism inspired by modern corporate management structures. The system consists of two specialized teams: (1) Data Team - responsible for processing and condensing massive market data into diversified text factors, ensuring they fit the model's constrained context. (2) Research Team - tasked with making parallelized multipath trading decisions based on deep research methods. The core innovation lies in implementing a real-time evaluation and ranking mechanism within each team, driven by authentic market feedback. Each agent's performance undergoes continuous scoring and ranking, with only outputs from top-performing agents being adopted. The design enables the system to adaptively adjust to dynamic environment, enhances robustness against market noise and ultimately delivers superior trading performance. Experimental results demonstrate that our proposed system significantly outperforms prevailing multi-agent systems and traditional quantitative investment methods across diverse evaluation metrics. ContestTrade is open-sourced on GitHub at https://github.com/FinStep-AI/ContestTrade.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training</title>
<link>https://arxiv.org/abs/2508.00414</link>
<guid>https://arxiv.org/abs/2508.00414</guid>
<content:encoded><![CDATA[
<div> Framework, Artificial Intelligence, Open-source, Agent, Advanced 
Summary: 
- Cognitive Kernel-Pro is an open-source, free multi-module agent framework designed to enhance the development and evaluation of advanced AI agents.
- The framework focuses on curating high-quality training data for Agent Foundation Models in domains like web, file, code, and general reasoning.
- Cognitive Kernel-Pro introduces novel strategies for agent test-time reflection and voting to improve agent robustness and performance.
- The framework outperforms previous leading systems like WebDancer and WebSailor, setting a new performance standard for accessible, high-capability AI agents.
- The 8B-parameter open-source model developed on Cognitive Kernel-Pro achieves state-of-the-art results, making it a valuable tool for research and development in the field of artificial intelligence.<br /><br />Summary: <div>
arXiv:2508.00414v2 Announce Type: replace-cross 
Abstract: General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at https://github.com/Tencent/CognitiveKernel-Pro
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.00589</link>
<guid>https://arxiv.org/abs/2508.00589</guid>
<content:encoded><![CDATA[
<div> Autonomous driving systems, Vulnerable Road Users, motion retrieval framework, WayMoCo dataset, context-aware, SMPL-based motion sequences

Summary: 
The article introduces a novel context-aware motion retrieval framework aimed at identifying rare human behavior scenarios in driving datasets. By combining Skinned Multi-Person Linear (SMPL)-based motion sequences and video frames in a shared multimodal embedding space aligned with natural language, the method enables scalable retrieval of human behavior and context through text queries. The WayMoCo dataset, an extension of the Waymo Open Dataset, contains labeled motion and scene context descriptions derived from pseudo-ground-truth SMPL sequences and corresponding image data. The proposed approach demonstrates superior performance, outperforming state-of-the-art models by up to 27.5% accuracy in motion-context retrieval on the WayMoCo dataset. <div>
arXiv:2508.00589v2 Announce Type: replace-cross 
Abstract: Autonomous driving systems must operate reliably in safety-critical scenarios, particularly those involving unusual or complex behavior by Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets is essential for robust evaluation and generalization, but retrieving such rare human behavior scenarios within the long tail of large-scale datasets is challenging. To support targeted evaluation of autonomous driving systems in diverse, human-centered scenarios, we propose a novel context-aware motion retrieval framework. Our method combines Skinned Multi-Person Linear (SMPL)-based motion sequences and corresponding video frames before encoding them into a shared multimodal embedding space aligned with natural language. Our approach enables the scalable retrieval of human behavior and their context through text queries. This work also introduces our dataset WayMoCo, an extension of the Waymo Open Dataset. It contains automatically labeled motion and scene context descriptions derived from generated pseudo-ground-truth SMPL sequences and corresponding image data. Our approach outperforms state-of-the-art models by up to 27.5% accuracy in motion-context retrieval, when evaluated on the WayMoCo dataset.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Argument Quality Annotation and Gender Bias Detection in Financial Communication through Large Language Models</title>
<link>https://arxiv.org/abs/2508.08262</link>
<guid>https://arxiv.org/abs/2508.08262</guid>
<content:encoded><![CDATA[
<div> Keywords: Financial arguments, LLMs, Annotation quality, Gender bias, Inter-annotator agreement <br />
Summary: <br />
This paper evaluates the performance of LLMs in annotating argument quality in financial communications using the FinArgQuality dataset. The study assesses the consistency of LLM-generated annotations compared to human annotations and introduces an adversarial attack to analyze gender bias. Results show that LLM-based annotations have higher inter-annotator agreement than human annotations but still exhibit varying degrees of gender bias. The study highlights the importance of bias-aware annotation methodologies for more reliable and cost-effective results. Recommendations are provided for future research to improve annotation quality in financial arguments. <br /> <div>
arXiv:2508.08262v1 Announce Type: new 
Abstract: Financial arguments play a critical role in shaping investment decisions and public trust in financial institutions. Nevertheless, assessing their quality remains poorly studied in the literature. In this paper, we examine the capabilities of three state-of-the-art LLMs GPT-4o, Llama 3.1, and Gemma 2 in annotating argument quality within financial communications, using the FinArgQuality dataset. Our contributions are twofold. First, we evaluate the consistency of LLM-generated annotations across multiple runs and benchmark them against human annotations. Second, we introduce an adversarial attack designed to inject gender bias to analyse models responds and ensure model's fairness and robustness. Both experiments are conducted across three temperature settings to assess their influence on annotation stability and alignment with human labels. Our findings reveal that LLM-based annotations achieve higher inter-annotator agreement than human counterparts, though the models still exhibit varying degrees of gender bias. We provide a multifaceted analysis of these outcomes and offer practical recommendations to guide future research toward more reliable, cost-effective, and bias-aware annotation methodologies.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TurQUaz at CheckThat! 2025: Debating Large Language Models for Scientific Web Discourse Detection</title>
<link>https://arxiv.org/abs/2508.08265</link>
<guid>https://arxiv.org/abs/2508.08265</guid>
<content:encoded><![CDATA[
<div> debate methods, scientific web discourse detection, large language models, scientific claim identification, scientific study references<br />
Summary:<br />
The paper presents a new method for identifying scientific claims, references to scientific studies, and mentions of scientific entities in tweets. It introduces a council debate approach where multiple large language models (LLMs) engage in structured academic discussions to reach a consensus. The study explores three debating methods: single debate, team debate, and council debate, with the council debate model showing the best performance on the development test set. While the method ranked lower in identifying scientific claims and mentions of scientific entities, it excelled in detecting references to scientific studies. This innovative approach highlights the potential of utilizing multiple LLMs in a collaborative setting to enhance the accuracy of scientific web discourse detection.<br /> <div>
arXiv:2508.08265v1 Announce Type: new 
Abstract: In this paper, we present our work developed for the scientific web discourse detection task (Task 4a) of CheckThat! 2025. We propose a novel council debate method that simulates structured academic discussions among multiple large language models (LLMs) to identify whether a given tweet contains (i) a scientific claim, (ii) a reference to a scientific study, or (iii) mentions of scientific entities. We explore three debating methods: i) single debate, where two LLMs argue for opposing positions while a third acts as a judge; ii) team debate, in which multiple models collaborate within each side of the debate; and iii) council debate, where multiple expert models deliberate together to reach a consensus, moderated by a chairperson model. We choose council debate as our primary model as it outperforms others in the development test set. Although our proposed method did not rank highly for identifying scientific claims (8th out of 10) or mentions of scientific entities (9th out of 10), it ranked first in detecting references to scientific studies.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heartificial Intelligence: Exploring Empathy in Language Models</title>
<link>https://arxiv.org/abs/2508.08271</link>
<guid>https://arxiv.org/abs/2508.08271</guid>
<content:encoded><![CDATA[
<div> empathy, language models, cognitive empathy, affective empathy, virtual companionship 
Summary: 
Large language models (LLMs) excel in cognitive empathy tasks, surpassing even psychology students. However, both small (SLMs) and large language models exhibit lower levels of affective empathy compared to humans. This suggests that while language models can provide effective virtual companionship and personalized emotional support, they may lack a deeper emotional connection with users. The high cognitive empathy of language models enables them to offer objective and consistent emotional support without the risks of emotional fatigue or bias. This study underscores the rapid development of language models in simulating cognitive empathy, highlighting their potential in various applications, such as virtual assistants. <div>
arXiv:2508.08271v1 Announce Type: new 
Abstract: Large language models have become increasingly common, used by millions of people worldwide in both professional and personal contexts. As these models continue to advance, they are frequently serving as virtual assistants and companions. In human interactions, effective communication typically involves two types of empathy: cognitive empathy (understanding others' thoughts and emotions) and affective empathy (emotionally sharing others' feelings). In this study, we investigated both cognitive and affective empathy across several small (SLMs) and large (LLMs) language models using standardized psychological tests. Our results revealed that LLMs consistently outperformed humans - including psychology students - on cognitive empathy tasks. However, despite their cognitive strengths, both small and large language models showed significantly lower affective empathy compared to human participants. These findings highlight rapid advancements in language models' ability to simulate cognitive empathy, suggesting strong potential for providing effective virtual companionship and personalized emotional support. Additionally, their high cognitive yet lower affective empathy allows objective and consistent emotional support without running the risk of emotional fatigue or bias.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time News Story Identification</title>
<link>https://arxiv.org/abs/2508.08272</link>
<guid>https://arxiv.org/abs/2508.08272</guid>
<content:encoded><![CDATA[
<div> keywords: news monitoring, story identification, real-time, text representation, clustering<br />
<br />
Summary: 
This article introduces a method for real-time story identification in news articles. The goal is to assign news articles to specific stories based on events, places, and people mentioned, rather than general topics. The approach combines text representation techniques, clustering algorithms, and online topic modeling methods to extract relevant information for story identification. By using a combination of text representation methods and online topic modeling tools such as BERTopic, DBStream, and TextClust, the system is able to identify and assign articles to stories as they are published online. The approach is evaluated on a dataset of Slovene media articles over a month, showing promising results according to human evaluators. This real-time story identification system has the potential to improve the reading experience on news sites by organizing articles into topical collections. <br /><br />Summary: <div>
arXiv:2508.08272v1 Announce Type: new 
Abstract: To improve the reading experience, many news sites organize news into topical collections, called stories. In this work, we present an approach for implementing real-time story identification for a news monitoring system that automatically collects news articles as they appear online and processes them in various ways. Story identification aims to assign each news article to a specific story that the article is covering. The process is similar to text clustering and topic modeling, but requires that articles be grouped based on particular events, places, and people, rather than general text similarity (as in clustering) or general (predefined) topics (as in topic modeling). We present an approach to story identification that is capable of functioning in real time, assigning articles to stories as they are published online. In the proposed approach, we combine text representation techniques, clustering algorithms, and online topic modeling methods. We combine various text representation methods to extract specific events and named entities necessary for story identification, showing that a mixture of online topic-modeling approaches such as BERTopic, DBStream, and TextClust can be adapted for story discovery. We evaluate our approach on a news dataset from Slovene media covering a period of 1 month. We show that our real-time approach produces sensible results as judged by human evaluators.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TT-XAI: Trustworthy Clinical Text Explanations via Keyword Distillation and LLM Reasoning</title>
<link>https://arxiv.org/abs/2508.08273</link>
<guid>https://arxiv.org/abs/2508.08273</guid>
<content:encoded><![CDATA[
<div> Keyword: clinical language models, electronic health records, interpretability, keyword distillation, large language models

Summary: 
TT-XAI is a framework that improves the performance and interpretability of clinical language models when applied to electronic health records. By distilling raw discharge notes into keyword representations, the framework enhances classifier performance and improves local explanation fidelity. Using keyword-guided prompts, TT-XAI generates chain-of-thought clinical explanations that steer large language models to produce concise and clinically relevant reasoning. Evaluation metrics, including deletion-based fidelity and human studies, consistently show that the keyword-augmented method enhances both machine and human interpretability. This approach offers a scalable pathway towards trustworthy and auditable AI in clinical decision support.<br /><br />Summary: <div>
arXiv:2508.08273v1 Announce Type: new 
Abstract: Clinical language models often struggle to provide trustworthy predictions and explanations when applied to lengthy, unstructured electronic health records (EHRs). This work introduces TT-XAI, a lightweight and effective framework that improves both classification performance and interpretability through domain-aware keyword distillation and reasoning with large language models (LLMs). First, we demonstrate that distilling raw discharge notes into concise keyword representations significantly enhances BERT classifier performance and improves local explanation fidelity via a focused variant of LIME. Second, we generate chain-of-thought clinical explanations using keyword-guided prompts to steer LLMs, producing more concise and clinically relevant reasoning. We evaluate explanation quality using deletion-based fidelity metrics, self-assessment via LLaMA-3 scoring, and a blinded human study with domain experts. All evaluation modalities consistently favor the keyword-augmented method, confirming that distillation enhances both machine and human interpretability. TT-XAI offers a scalable pathway toward trustworthy, auditable AI in clinical decision support.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling Knowledge from Large Language Models: A Concept Bottleneck Model for Hate and Counter Speech Recognition</title>
<link>https://arxiv.org/abs/2508.08274</link>
<guid>https://arxiv.org/abs/2508.08274</guid>
<content:encoded><![CDATA[
<div> Keywords: hate speech, counter speech, automated detection, transparent method, adjective-based representation 

Summary: 
The article introduces a novel method called the Speech Concept Bottleneck Model (SCBM) for automated hate and counter speech recognition. This transparent approach utilizes adjectives as human-interpretable bottleneck concepts and leverages large language models (LLMs) to map input texts to an abstract adjective-based representation. SCBM outperforms previous black-box models on benchmark datasets from platforms like Twitter, Reddit, and YouTube, achieving an average macro-F1 score of 0.69. The method provides both local and global interpretability while demonstrating high recognition accuracy. By fusing adjective-based concept representations with transformer embeddings, a performance increase of 1.8% on average across all datasets is observed, showcasing the effectiveness and interpretability of the proposed approach. The results indicate that adjective-based concept representations can be effective, compact, and interpretable encodings for hate and counter speech recognition, with potential applications in other NLP tasks. 

Summary: <br /><br /> <div>
arXiv:2508.08274v1 Announce Type: new 
Abstract: The rapid increase in hate speech on social media has exposed an unprecedented impact on society, making automated methods for detecting such content important. Unlike prior black-box models, we propose a novel transparent method for automated hate and counter speech recognition, i.e., "Speech Concept Bottleneck Model" (SCBM), using adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to map input texts to an abstract adjective-based representation, which is then sent to a light-weight classifier for downstream tasks. Across five benchmark datasets spanning multiple languages and platforms (e.g., Twitter, Reddit, YouTube), SCBM achieves an average macro-F1 score of 0.69 which outperforms the most recently reported results from the literature on four out of five datasets. Aside from high recognition accuracy, SCBM provides a high level of both local and global interpretability. Furthermore, fusing our adjective-based concept representation with transformer embeddings, leads to a 1.8% performance increase on average across all datasets, showing that the proposed representation captures complementary information. Our results demonstrate that adjective-based concept representations can serve as compact, interpretable, and effective encodings for hate and counter speech recognition. With adapted adjectives, our method can also be applied to other NLP tasks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLLM-CBench:A Comprehensive Benchmark for Continual Instruction Tuning of Multimodal LLMs with Chain-of-Thought Reasoning Analysis</title>
<link>https://arxiv.org/abs/2508.08275</link>
<guid>https://arxiv.org/abs/2508.08275</guid>
<content:encoded><![CDATA[
<div> Evaluation benchmark, Multimodal Large Language Models, continual instruction tuning, forgetting, task order

Summary:
Models with strong general capabilities show better resistance to forgetting during continual learning. Chains of reasoning deteriorate at a slower pace compared to final answers, supporting the hierarchical forgetting hypothesis. The effectiveness of continual learning algorithms is influenced by both model capability and task order. In reinforcement learning scenarios, incorporating KL-divergence constraints aids in maintaining policy stability and helps reduce forgetting. MLLM-CTBench provides a comprehensive evaluation benchmark for continual instruction tuning of MLLMs, offering practical insights for algorithm design and assessment.<br /><br />Summary: <div>
arXiv:2508.08275v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) rely on continual instruction tuning to adapt to the evolving demands of real-world applications. However, progress in this area is hindered by the lack of rigorous and systematic benchmarks. To address this gap, we present MLLM-CTBench, a comprehensive evaluation benchmark with three key contributions: (1) Multidimensional Evaluation: We combine final answer accuracy with fine-grained CoT reasoning quality assessment, enabled by a specially trained CoT evaluator; (2) Comprehensive Evaluation of Algorithms and Training Paradigms: We benchmark eight continual learning algorithms across four major categories and systematically compare reinforcement learning with supervised fine-tuning paradigms; (3) Carefully Curated Tasks: We select and organize 16 datasets from existing work, covering six challenging domains. Our key findings include: (i) Models with stronger general capabilities exhibit greater robustness to forgetting during continual learning; (ii) Reasoning chains degrade more slowly than final answers, supporting the hierarchical forgetting hypothesis; (iii) The effectiveness of continual learning algorithms is highly dependent on both model capability and task order; (iv) In reinforcement learning settings, incorporating KL-divergence constraints helps maintain policy stability and plays a crucial role in mitigating forgetting. MLLM-CTBench establishes a rigorous standard for continual instruction tuning of MLLMs and offers practical guidance for algorithm design and evaluation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Contrast Localizer for Identifying Causal Unitsin Social &amp; Mathematical Tasks in Language Models</title>
<link>https://arxiv.org/abs/2508.08276</link>
<guid>https://arxiv.org/abs/2508.08276</guid>
<content:encoded><![CDATA[
<div> localizer, causally relevant units, Theory of Mind, mathematical reasoning, large language models

Summary:
This study explores the use of a neuroscientific contrast localizer to identify causally relevant units for Theory of Mind (ToM) and mathematical reasoning tasks in large language models (LLMs) and vision-language models (VLMs). Through targeted ablations of top-activated units in 11 LLMs and 5 VLMs, the researchers found that low-activation units sometimes had a greater impact on performance than highly activated ones. Additionally, units identified through the mathematical localizer sometimes had a more detrimental effect on ToM tasks than those from the ToM localizer. These results challenge the assumption of causal relevance in contrast-based localizers and suggest the need for more comprehensive stimulus sets to accurately capture task-specific units in LLMs and VLMs.<br /><br />Summary: <div>
arXiv:2508.08276v1 Announce Type: new 
Abstract: This work adapts a neuroscientific contrast localizer to pinpoint causally relevant units for Theory of Mind (ToM) and mathematical reasoning tasks in large language models (LLMs) and vision-language models (VLMs). Across 11 LLMs and 5 VLMs ranging in size from 3B to 90B parameters, we localize top-activated units using contrastive stimulus sets and assess their causal role via targeted ablations. We compare the effect of lesioning functionally selected units against low-activation and randomly selected units on downstream accuracy across established ToM and mathematical benchmarks. Contrary to expectations, low-activation units sometimes produced larger performance drops than the highly activated ones, and units derived from the mathematical localizer often impaired ToM performance more than those from the ToM localizer. These findings call into question the causal relevance of contrast-based localizers and highlight the need for broader stimulus sets and more accurately capture task-specific units.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Objective Metrics for Evaluating Large Language Models Using External Data Sources</title>
<link>https://arxiv.org/abs/2508.08277</link>
<guid>https://arxiv.org/abs/2508.08277</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, evaluation framework, subjective metrics, structured evaluation, automation<br />
Summary:<br />
This paper introduces a framework for assessing the performance of Large Language Models (LLMs) using subjective metrics derived from class materials. The framework aims to overcome the challenges of subjective assessments by leveraging well-defined benchmarks and factual datasets. By implementing structured evaluation pipelines, the approach ensures consistent and reproducible measurements while minimizing biases. Automation and transparency are key components of the framework, reducing the need for human interpretation and aligning assessments with real-world applications. This method offers a scalable solution for evaluating LLM outputs in educational, scientific, and other high-stakes domains.<br /> <div>
arXiv:2508.08277v1 Announce Type: new 
Abstract: Evaluating the performance of Large Language Models (LLMs) is a critical yet challenging task, particularly when aiming to avoid subjective assessments. This paper proposes a framework for leveraging subjective metrics derived from the class textual materials across different semesters to assess LLM outputs across various tasks. By utilizing well-defined benchmarks, factual datasets, and structured evaluation pipelines, the approach ensures consistent, reproducible, and bias-minimized measurements. The framework emphasizes automation and transparency in scoring, reducing reliance on human interpretation while ensuring alignment with real-world applications. This method addresses the limitations of subjective evaluation methods, providing a scalable solution for performance assessment in educational, scientific, and other high-stakes domains.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MinionsLLM: a Task-adaptive Framework For The Training and Control of Multi-Agent Systems Through Natural Language</title>
<link>https://arxiv.org/abs/2508.08283</link>
<guid>https://arxiv.org/abs/2508.08283</guid>
<content:encoded><![CDATA[
<div> MinionsLLM, Large Language Models, Behavior Trees, Formal Grammars, multi-agent systems<br />
Summary: <br />
This paper introduces MinionsLLM, a framework combining Large Language Models (LLMs) with Behavior Trees (BTs) and Formal Grammars for natural language control of multi-agent systems in custom environments. MinionsLLM offers standardized interfaces for defining environments, agents, and behaviors, along with two synthetic dataset generation methods for enhancing LLMs. Using Google's Gemma 3 model family across different parameter scales, the framework demonstrates significant improvements in syntactic validity and task performance. Particularly, smaller LLMs show the most benefit from fine-tuning, suggesting potential for deploying compact models in resource-limited multi-agent control scenarios. The framework and resources are openly available for reproducibility and further research. <br /> <div>
arXiv:2508.08283v1 Announce Type: new 
Abstract: This paper presents MinionsLLM, a novel framework that integrates Large Language Models (LLMs) with Behavior Trees (BTs) and Formal Grammars to enable natural language control of multi-agent systems within arbitrary, user-defined environments. MinionsLLM provides standardized interfaces for defining environments, agents, and behavioral primitives, and introduces two synthetic dataset generation methods (Method A and Method B) to fine-tune LLMs for improved syntactic validity and semantic task relevance. We validate our approach using Google's Gemma 3 model family at three parameter scales (1B, 4B, and 12B) and demonstrate substantial gains: Method B increases syntactic validity to 92.6% and achieves a mean task performance improvement of 33% over baseline. Notably, our experiments show that smaller models benefit most from fine-tuning, suggesting promising directions for deploying compact, locally hosted LLMs in resource-constrained multi-agent control scenarios. The framework and all resources are released open-source to support reproducibility and future research.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Illusion of Progress: Re-evaluating Hallucination Detection in LLMs</title>
<link>https://arxiv.org/abs/2508.08285</link>
<guid>https://arxiv.org/abs/2508.08285</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, hallucination detection, ROUGE, human-aligned metrics, evaluation frameworks <br />
Summary: 

Hallucination detection in large language models is a critical issue due to their tendency to generate inaccurate or misleading information. Current methods rely on metrics like ROUGE, which overestimate performance due to high recall but low precision. Human-aligned metrics such as LLM-as-Judge provide more accurate evaluations, showing performance drops of up to 45.9% in existing detection methods. Simple heuristics based on response length can sometimes perform as well as complex techniques, highlighting a flaw in current evaluation practices. The study emphasizes the need for semantically aware and robust evaluation frameworks to accurately assess the effectiveness of hallucination detection methods. This is crucial for ensuring the reliability of large language model outputs. 

<br /><br />Summary: <div>
arXiv:2508.08285v1 Announce Type: new 
Abstract: Large language models (LLMs) have revolutionized natural language processing, yet their tendency to hallucinate poses serious challenges for reliable deployment. Despite numerous hallucination detection methods, their evaluations often rely on ROUGE, a metric based on lexical overlap that misaligns with human judgments. Through comprehensive human studies, we demonstrate that while ROUGE exhibits high recall, its extremely low precision leads to misleading performance estimates. In fact, several established detection methods show performance drops of up to 45.9\% when assessed using human-aligned metrics like LLM-as-Judge. Moreover, our analysis reveals that simple heuristics based on response length can rival complex detection techniques, exposing a fundamental flaw in current evaluation practices. We argue that adopting semantically aware and robust evaluation frameworks is essential to accurately gauge the true performance of hallucination detection methods, ultimately ensuring the trustworthiness of LLM outputs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sacred or Synthetic? Evaluating LLM Reliability and Abstention for Religious Questions</title>
<link>https://arxiv.org/abs/2508.08287</link>
<guid>https://arxiv.org/abs/2508.08287</guid>
<content:encoded><![CDATA[
<div> benchmark, Large Language Models, FiqhQA, abstinence behavior, religious reasoning <br />
Summary: <br />
This paper introduces a benchmark called FiqhQA focused on Large Language Models (LLMs) generating Islamic rulings categorized by Sunni schools of thought. The study evaluates LLMs on accuracy and abstinence behavior, showing variations across models, languages, and legal schools. GPT-4o performs best in accuracy, while Gemini and Fanar excel in abstention behavior. All models display lower performance in Arabic, highlighting limitations in religious reasoning for non-English languages. The research emphasizes the importance of task-specific evaluation and cautious deployment of LLMs in religious contexts. <br /> <div>
arXiv:2508.08287v1 Announce Type: new 
Abstract: Despite the increasing usage of Large Language Models (LLMs) in answering questions in a variety of domains, their reliability and accuracy remain unexamined for a plethora of domains including the religious domains. In this paper, we introduce a novel benchmark FiqhQA focused on the LLM generated Islamic rulings explicitly categorized by the four major Sunni schools of thought, in both Arabic and English. Unlike prior work, which either overlooks the distinctions between religious school of thought or fails to evaluate abstention behavior, we assess LLMs not only on their accuracy but also on their ability to recognize when not to answer. Our zero-shot and abstention experiments reveal significant variation across LLMs, languages, and legal schools of thought. While GPT-4o outperforms all other models in accuracy, Gemini and Fanar demonstrate superior abstention behavior critical for minimizing confident incorrect answers. Notably, all models exhibit a performance drop in Arabic, highlighting the limitations in religious reasoning for languages other than English. To the best of our knowledge, this is the first study to benchmark the efficacy of LLMs for fine-grained Islamic school of thought specific ruling generation and to evaluate abstention for Islamic jurisprudence queries. Our findings underscore the need for task-specific evaluation and cautious deployment of LLMs in religious applications.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Putnam-AXIOM: A Functional and Static Benchmark</title>
<link>https://arxiv.org/abs/2508.08292</link>
<guid>https://arxiv.org/abs/2508.08292</guid>
<content:encoded><![CDATA[
<div> benchmark, large language models, Putnam-AXIOM, contamination-resilient test bed, Teacher-Forced Accuracy

Summary:
Large language models (LLMs) are reaching high accuracy on mathematical reasoning benchmarks but are vulnerable to training-set contamination. A new benchmark called Putnam-AXIOM has been introduced, consisting of 522 university-level problems from the Putnam Mathematical Competition. Additionally, Putnam-AXIOM Variation includes 100 unseen variants of these problems, created by altering variables and constants programmatically. The variation protocol ensures a continuous supply of challenging, unseen instances to test LLMs. Evaluation on the Original set and paired Variations reveals significant drops in accuracy for models, indicating potential memorization issues. The benchmark also introduces Teacher-Forced Accuracy (TFA), a metric for assessing reasoning traces and automating proof evaluations. Putnam-AXIOM provides a rigorous and contamination-resilient framework for evaluating the mathematical reasoning capabilities of LLMs. The data and evaluation code are publicly available for further research. 

<br /><br />Summary: <div>
arXiv:2508.08292v1 Announce Type: new 
Abstract: Current mathematical reasoning benchmarks for large language models (LLMs) are approaching saturation, with some achieving > 90% accuracy, and are increasingly compromised by training-set contamination. We introduce Putnam-AXIOM, a benchmark of 522 university-level competition problems drawn from the prestigious William Lowell Putnam Mathematical Competition, and Putnam-AXIOM Variation, an unseen companion set of 100 functional variants generated by programmatically perturbing variables and constants. The variation protocol produces an unlimited stream of equally difficult, unseen instances -- yielding a contamination-resilient test bed. On the Original set, OpenAI's o1-preview -- the strongest evaluated model -- scores 41.9%, but its accuracy drops by 19.6% (46.8% relative decrease) on the paired Variations. The remaining eighteen models show the same downward trend, ten of them with non-overlapping 95% confidence intervals. These gaps suggest memorization and highlight the necessity of dynamic benchmarks. We complement "boxed" accuracy with Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores reasoning traces and automates natural language proof evaluations. Putnam-AXIOM therefore provides a rigorous, contamination-resilient evaluation framework for assessing advanced mathematical reasoning of LLMs. Data and evaluation code are publicly available at https://github.com/brando90/putnam-axiom.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoDAE: Adapting Large Language Models for Education via Chain-of-Thought Data Augmentation</title>
<link>https://arxiv.org/abs/2508.08386</link>
<guid>https://arxiv.org/abs/2508.08386</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Educational Settings, CoDAE, Chain-of-Thought Data Augmentation, Pedagogical Guidance 

Summary: 
CoDAE is a framework designed to enhance the performance of Large Language Models (LLMs) in educational settings. By using Chain-of-Thought (CoT) data augmentation, real-world dialogues between students and ChatGPT-based tutors are enriched to promote step-by-step reasoning and pedagogically aligned guidance. CoDAE aims to address three key limitations of off-the-shelf LLMs in educational use: over-compliance, low response adaptivity, and vulnerability to emotionally manipulative prompts. Through fine-tuning four open-source LLMs on augmented datasets, CoDAE demonstrates improved pedagogical appropriateness, support for reasoning processes, and resistance to premature answer disclosure. The evaluation in simulated educational scenarios shows that LLMs trained with CoDAE provide more effective and appropriate guidance to students, leading to enhanced learning outcomes. 

<br /><br />Summary: <div>
arXiv:2508.08386v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly employed as AI tutors due to their scalability and potential for personalized instruction. However, off-the-shelf LLMs often underperform in educational settings: they frequently reveal answers too readily, fail to adapt their responses to student uncertainty, and remain vulnerable to emotionally manipulative prompts. To address these challenges, we introduce CoDAE, a framework that adapts LLMs for educational use through Chain-of-Thought (CoT) data augmentation. We collect real-world dialogues between students and a ChatGPT-based tutor and enrich them using CoT prompting to promote step-by-step reasoning and pedagogically aligned guidance. Furthermore, we design targeted dialogue cases to explicitly mitigate three key limitations: over-compliance, low response adaptivity, and threat vulnerability. We fine-tune four open-source LLMs on different variants of the augmented datasets and evaluate them in simulated educational scenarios using both automatic metrics and LLM-as-a-judge assessments. Our results show that models fine-tuned with CoDAE deliver more pedagogically appropriate guidance, better support reasoning processes, and effectively resist premature answer disclosure.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery</title>
<link>https://arxiv.org/abs/2508.08401</link>
<guid>https://arxiv.org/abs/2508.08401</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Explicit Long Chain-of-Thought reasoning, molecule discovery, Mol-R1, reasoning performance

Summary:
Mol-R1 addresses the limitation of Long Chain-of-Thought reasoning models in knowledge-intensive domains like molecule discovery by introducing a novel framework. The framework leverages a high-quality reasoning dataset generated through Prior Regulation via In-context Distillation (PRID) to enhance explainability and reasoning performance. Mol-R1 incorporates MoIA (Molecular Iterative Adaptation), a training strategy combining Supervised Fine-tuning (SFT) and Reinforced Policy Optimization (RPO) to improve reasoning capability for molecule generation tasks. The performance of Mol-R1 surpasses existing baselines in text-based molecule reasoning generation, showcasing its effectiveness in enhancing reasoning capabilities of R1-like reasoning models. <br /><br />Summary: Mol-R1 introduces a novel framework to enhance the reasoning performance of Long Chain-of-Thought reasoning models in molecule discovery tasks. By utilizing a high-quality reasoning dataset and a sophisticated training strategy, Mol-R1 demonstrates superior performance in text-based molecule reasoning generation, addressing the challenges faced by existing models in knowledge-intensive domains. <div>
arXiv:2508.08401v1 Announce Type: new 
Abstract: Large language models (LLMs), especially Explicit Long Chain-of-Thought (CoT) reasoning models like DeepSeek-R1 and QWQ, have demonstrated powerful reasoning capabilities, achieving impressive performance in commonsense reasoning and mathematical inference. Despite their effectiveness, Long-CoT reasoning models are often criticized for their limited ability and low efficiency in knowledge-intensive domains such as molecule discovery. Success in this field requires a precise understanding of domain knowledge, including molecular structures and chemical principles, which is challenging due to the inherent complexity of molecular data and the scarcity of high-quality expert annotations. To bridge this gap, we introduce Mol-R1, a novel framework designed to improve explainability and reasoning performance of R1-like Explicit Long-CoT reasoning LLMs in text-based molecule generation. Our approach begins with a high-quality reasoning dataset curated through Prior Regulation via In-context Distillation (PRID), a dedicated distillation strategy to effectively generate paired reasoning traces guided by prior regulations. Building upon this, we introduce MoIA, Molecular Iterative Adaptation, a sophisticated training strategy that iteratively combines Supervised Fine-tuning (SFT) with Reinforced Policy Optimization (RPO), tailored to boost the reasoning performance of R1-like reasoning models for molecule discovery. Finally, we examine the performance of Mol-R1 in the text-based molecule reasoning generation task, showing superior performance against existing baselines.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Tokenization for Rich Morphology: The Dominance of Unigram over BPE and Morphological Alignment</title>
<link>https://arxiv.org/abs/2508.08424</link>
<guid>https://arxiv.org/abs/2508.08424</guid>
<content:encoded><![CDATA[
<div> Keywords: language modeling, morphological alignment, tokenization, syntax-based tasks, tokenizer algorithm

Summary:
- The study evaluates language modeling performance in Telugu, Hindi, and English, focusing on morphological alignment and tokenization quality.
- Gold morpheme segmentations of word forms in Telugu are used to assess morphological alignment of tokenizers.
- Better morphological alignment is found to positively correlate with performance in syntax-based tasks like Parts-of-Speech tagging and Named Entity Recognition.
- The tokenizer algorithm (Byte-pair Encoding vs. Unigram) has a more significant impact on downstream performance than morphological alignment alone.
- Naive Unigram tokenizers generally outperform others, but hybrid tokenizers incorporating morphological segmentation show improvement within the BPE framework.
- Intrinsic metrics like Corpus Token Count and R\'enyi entropy do not show correlation with downstream performance. 

<br /><br />Summary: 
The study compares language modeling performance in Telugu, Hindi, and English, finding that morphological alignment has a moderate positive correlation with syntax-based task performance. However, the tokenizer algorithm plays a more significant role in influencing downstream performance, with Naive Unigram tokenizers generally outperforming others. Hybrid tokenizers incorporating morphological segmentation show improvement within the Byte-pair Encoding framework. Intrinsic metrics like Corpus Token Count and R\'enyi entropy do not show a clear correlation with performance. <div>
arXiv:2508.08424v1 Announce Type: new 
Abstract: Prior work on language modeling showed conflicting findings about whether morphologically aligned approaches to tokenization improve performance, particularly for languages with complex morphology. To investigate this, we select a typologically diverse set of languages: Telugu (agglutinative), Hindi (primarily fusional with some agglutination), and English (fusional). We conduct a comprehensive evaluation of language models -- starting from tokenizer training and extending through the finetuning and downstream task evaluation. To account for the consistent performance differences observed across tokenizer variants, we focus on two key factors: morphological alignment and tokenization quality. To assess morphological alignment of tokenizers in Telugu, we create a dataset containing gold morpheme segmentations of 600 derivational and 7000 inflectional word forms.
  Our experiments reveal that better morphological alignment correlates positively -- though moderately -- with performance in syntax-based tasks such as Parts-of-Speech tagging, Named Entity Recognition and Dependency Parsing. However, we also find that the tokenizer algorithm (Byte-pair Encoding vs. Unigram) plays a more significant role in influencing downstream performance than morphological alignment alone. Naive Unigram tokenizers outperform others across most settings, though hybrid tokenizers that incorporate morphological segmentation significantly improve performance within the BPE framework. In contrast, intrinsic metrics like Corpus Token Count (CTC) and R\'enyi entropy showed no correlation with downstream performance.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Small LLM Alignment through Margin-Based Objective Modifications under Resource Constraints</title>
<link>https://arxiv.org/abs/2508.08466</link>
<guid>https://arxiv.org/abs/2508.08466</guid>
<content:encoded><![CDATA[
<div> Adaptive Margin-Sigmoid Loss, APO-hinge-zero, lightweight DPO-based variants, underperformance scenarios, margin-based objectives

Summary:
The study introduces two new DPO-based variants, Adaptive Margin-Sigmoid Loss and APO-hinge-zero, to improve small large language models (LLMs) alignment with human preferences. These lightweight methods address underperformance issues by incorporating margin-based objectives and selective update mechanisms. APO-hinge-zero, combining hard-example mining with chosen-focused optimization, shows significant enhancements in performance. In AlpacaEval, APO-hinge-zero increases win rate and length-controlled win rate compared to the baseline. The methods also perform competitively in diverse categories in MT-Bench, with strengths in STEM and Humanities tasks. The findings highlight how simple modifications to preference-based objectives can enhance small LLM alignment under resource constraints, offering a practical approach for more efficient deployment.<br /><br />Summary: <div>
arXiv:2508.08466v1 Announce Type: new 
Abstract: Small large language models (LLMs) often face difficulties in aligning output to human preferences, particularly when operating under severe performance gaps. In this work, we propose two lightweight DPO-based variants -- Adaptive Margin-Sigmoid Loss and APO-hinge-zero -- to better address underperformance scenarios by introducing margin-based objectives and selective update mechanisms.
  Our APO-hinge-zero method, which combines hinge-induced hard-example mining with the chosen-focused optimization of APO-zero, achieves strong results. In AlpacaEval, APO-hinge-zero improves the win rate by +2.0 points and the length-controlled win rate by +1.4 points compared to the APO-zero baseline. In MT-Bench, our methods maintain competitive performance in diverse categories, particularly excelling in STEM and Humanities tasks.
  These results demonstrate that simple modifications to preference-based objectives can significantly enhance small LLM alignment under resource constraints, offering a practical path toward more efficient deployment.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Momentum Point-Perplexity Mechanics in Large Language Models</title>
<link>https://arxiv.org/abs/2508.08492</link>
<guid>https://arxiv.org/abs/2508.08492</guid>
<content:encoded><![CDATA[
<div> energy, hidden states, large language models, transformers, Jacobian steering
Summary: 
The study focuses on the internal hidden states of large language models during inference, using a physics-based approach. It identifies a constant "energy" quantity that combines the rate of change in hidden states and next-token certainty. The research compares random-weight and pre-trained models, finding that training shifts models into a faster and more decisive regime. A control method called Jacobian steering is introduced, perturbing hidden states minimally to favor a target token. This approach maintains energy and produces high-quality semantic continuations. Viewing transformers through this mechanics lens offers insight into interpretability, anomaly detection, and controlled steering, making powerful models more predictable and aligned with human intent. <div>
arXiv:2508.08492v1 Announce Type: new 
Abstract: We take a physics-based approach to studying how the internal hidden states of large language models change from token to token during inference. Across 20 open-source transformer models (135M-3B parameters), we find that a quantity combining the rate of change in hidden states and the model's next-token certainty, analogous to energy in physics, remains nearly constant. Random-weight models conserve this "energy" more tightly than pre-trained ones, while training shifts models into a faster, more decisive regime with greater variability. Using this "log-Lagrangian" view, we derive a control method called Jacobian steering, which perturbs hidden states in the minimal way needed to favor a target token. This approach maintained near-constant energy in two tested models and produced continuations rated higher in semantic quality than the models' natural outputs. Viewing transformers through this mechanics lens offers a principled basis for interpretability, anomaly detection, and low-risk steering. This could help make powerful models more predictable and aligned with human intent.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steerable Pluralism: Pluralistic Alignment via Few-Shot Comparative Regression</title>
<link>https://arxiv.org/abs/2508.08509</link>
<guid>https://arxiv.org/abs/2508.08509</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, reinforcement learning, pluralistic alignment, comparative regression, ethical AI 

Summary: 
Large language models (LLMs) are currently aligned using reinforcement learning from human feedback (RLHF), but these methods may not capture diverse user preferences. This study introduces a steerable pluralistic model based on few-shot comparative regression to adapt to individual user preferences. By leveraging in-context learning and reasoning grounded in fine-grained attributes, the model compares response options to make aligned choices. The authors propose two steerable pluralistic benchmarks based on existing datasets to evaluate their approach, demonstrating its applicability to value-aligned decision-making and reward modeling. The few-shot comparative regression approach is interpretable and can be used with different attributes and LLMs. The results show that the proposed method outperforms multiple baseline and state-of-the-art methods. This research advances ethical AI by providing new insights and directions in pluralistic alignment, aiming to enhance fairness and representativeness in LLM usage. 

<br /><br />Summary: <div>
arXiv:2508.08509v1 Announce Type: new 
Abstract: Large language models (LLMs) are currently aligned using techniques such as reinforcement learning from human feedback (RLHF). However, these methods use scalar rewards that can only reflect user preferences on average. Pluralistic alignment instead seeks to capture diverse user preferences across a set of attributes, moving beyond just helpfulness and harmlessness. Toward this end, we propose a steerable pluralistic model based on few-shot comparative regression that can adapt to individual user preferences. Our approach leverages in-context learning and reasoning, grounded in a set of fine-grained attributes, to compare response options and make aligned choices. To evaluate our algorithm, we also propose two new steerable pluralistic benchmarks by adapting the Moral Integrity Corpus (MIC) and the HelpSteer2 datasets, demonstrating the applicability of our approach to value-aligned decision-making and reward modeling, respectively. Our few-shot comparative regression approach is interpretable and compatible with different attributes and LLMs, while outperforming multiple baseline and state-of-the-art methods. Our work provides new insights and research directions in pluralistic alignment, enabling a more fair and representative use of LLMs and advancing the state-of-the-art in ethical AI.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeCAL Tokenwise Compression</title>
<link>https://arxiv.org/abs/2508.08514</link>
<guid>https://arxiv.org/abs/2508.08514</guid>
<content:encoded><![CDATA[
<div> Keyword: DeCAL, tokenwise compression, encoder-decoder language model, denoising, downstream tasks <br />
Summary: <br />
This paper introduces DeCAL, a novel tokenwise compression method utilizing an encoder-decoder language model pretrained with denoising. DeCAL focuses on producing high-quality compressed representations by modifying the encoder to prioritize compression quality over computational resources. Results show that DeCAL can achieve comparable performance to uncompressed data on various downstream tasks such as question-answering, summarization, and multi-vector retrieval tasks at 2x compression, with only marginal degradation in performance at higher compression ratios up to 8x. This approach offers significant savings in scenarios where pre-computed dense representations are applicable and has the potential for further enhancements to broaden its applicability. <br /> 
Summary: <div>
arXiv:2508.08514v1 Announce Type: new 
Abstract: This paper introduces DeCAL, a new method for tokenwise compression. DeCAL uses an encoder-decoder language model pretrained with denoising to learn to produce high-quality, general-purpose compressed representations by the encoder. DeCAL applies small modifications to the encoder, with the emphasis on maximizing compression quality, even at the expense of compute. We show that DeCAL at 2x compression can match uncompressed on many downstream tasks, with usually only minor dropoff in metrics up to 8x compression, among question-answering, summarization, and multi-vector retrieval tasks. DeCAL offers significant savings where pre-computed dense representations can be utilized, and we believe the approach can be further developed to be more broadly applicable.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives</title>
<link>https://arxiv.org/abs/2508.08591</link>
<guid>https://arxiv.org/abs/2508.08591</guid>
<content:encoded><![CDATA[
<div> DepressLLM, Depression Prediction, Large Language Models, Autobiographical Narratives, Interpretable AI<br />
Summary:<br />
This study introduces DepressLLM, a large language model trained on a novel dataset of autobiographical narratives for depression prediction. The model, utilizing the SToPS module, achieves high classification performance with an AUC of 0.789, increasing to 0.904 for confident predictions. Validation on diverse datasets shows its robustness, including an EMA corpus and clinical interview data. A psychiatric review of misclassifications informs areas for model improvement. The study highlights the potential of interpretable AI in early depression diagnosis, showcasing the role of medical AI in psychiatry.<br /> <div>
arXiv:2508.08591v1 Announce Type: new 
Abstract: Advances in large language models (LLMs) have enabled a wide range of applications. However, depression prediction is hindered by the lack of large-scale, high-quality, and rigorously annotated datasets. This study introduces DepressLLM, trained and evaluated on a novel corpus of 3,699 autobiographical narratives reflecting both happiness and distress. DepressLLM provides interpretable depression predictions and, via its Score-guided Token Probability Summation (SToPS) module, delivers both improved classification performance and reliable confidence estimates, achieving an AUC of 0.789, which rises to 0.904 on samples with confidence $\geq$ 0.95. To validate its robustness to heterogeneous data, we evaluated DepressLLM on in-house datasets, including an Ecological Momentary Assessment (EMA) corpus of daily stress and mood recordings, and on public clinical interview data. Finally, a psychiatric review of high-confidence misclassifications highlighted key model and data limitations that suggest directions for future refinements. These findings demonstrate that interpretable AI can enable earlier diagnosis of depression and underscore the promise of medical AI in psychiatry.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Retrieval-Augmented Generation (RAG) for Colloquial Cantonese: A LoRA-Based Systematic Review</title>
<link>https://arxiv.org/abs/2508.08610</link>
<guid>https://arxiv.org/abs/2508.08610</guid>
<content:encoded><![CDATA[
<div> Keywords: Parameter-Efficient Fine-Tuning, Low-Rank Adaptation, Retrieval-Augmented Generation, Cantonese colloquial expressions, Limited annotated data

Summary: 
- The review examines recent advances in Parameter-Efficient Fine-Tuning (PEFT) with a focus on Low-Rank Adaptation (LoRA) in optimizing RAG systems like Qwen3, DeepSeek, and Kimi.
- Challenges faced include understanding and generating authentic Cantonese colloquial expressions due to limited annotated data and linguistic variability.
- Integration of LoRA within RAG frameworks, benchmarking PEFT methods, identifying domain adaptation strategies, and comparing fine-tuning techniques in improving semantic fidelity under data-scarce conditions are evaluated.
- Dynamic and ensemble LoRA adaptations reduce trainable parameters without compromising retrieval accuracy and generation quality in dialectal contexts.
- Limitations exist in fully preserving fine-grained linguistic nuances, especially for low-resource settings like Cantonese. Integration of real-time user feedback and domain-specific data remains underdeveloped. Selective parameter freezing and nonlinear adaptation methods offer better trade-offs between efficiency and accuracy, but scalability remains a challenge.

<br /><br />Summary: <div>
arXiv:2508.08610v1 Announce Type: new 
Abstract: This review examines recent advances in Parameter-Efficient Fine-Tuning (PEFT), with a focus on Low-Rank Adaptation (LoRA), to optimize Retrieval-Augmented Generation (RAG) systems like Qwen3, DeepSeek, and Kimi. These systems face challenges in understanding and generating authentic Cantonese colloquial expressions due to limited annotated data and linguistic variability. The review evaluates the integration of LoRA within RAG frameworks, benchmarks PEFT methods for retrieval and generation accuracy, identify domain adaptation strategies under limited data, and compares fine-tuning techniques aimed at improving semantic fidelity under data-scarce conditions. A systematic analysis of recent studies employing diverse LoRA variants, synthetic data generation, user feedback integration, and adaptive parameter allocation was conducted to assess their impact on computational efficiency, retrieval precision, linguistic authenticity, and scalability. Findings reveal that dynamic and ensemble LoRA adaptations significantly reduce trainable parameters without sacrificing retrieval accuracy and generation quality in dialectal contexts. However, limitations remain in fully preserving fine-grained linguistic nuances, especially for low-resource settings like Cantonese. The integration of real-time user feedback and domain-specific data remains underdeveloped, limiting model adaptability and personalization. While selective parameter freezing and nonlinear adaptation methods offer better trade-offs between efficiency and accuracy, their robustness at scale remains an open challenge. This review highlights the promise of PEFT-enhanced RAG systems for domain-specific language tasks and calls for future work targeting dialectal authenticity, dynamic adaptation, and scalable fine-tuning pipelines.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InternBootcamp Technical Report: Boosting LLM Reasoning with Verifiable Task Scaling</title>
<link>https://arxiv.org/abs/2508.08636</link>
<guid>https://arxiv.org/abs/2508.08636</guid>
<content:encoded><![CDATA[
<div> Framework, Large language models, Reinforcement learning, Model optimization, Synthetic data generation <br />
Summary: <br />
The article introduces InternBootcamp, an open-source framework with diverse task environments for large language models. It enables automated generation of training/testing cases and offers verification modules for response evaluation. The framework accelerates model optimization, synthetic data generation, and evaluation processes. With Bootcamp-EVAL benchmark, it assesses model performance, revealing the need for improvement in reasoning tasks. Training with InternBootcamp significantly enhances model performance, leading to state-of-the-art results. Task scaling, incorporating more training tasks, proves essential for performance gains, offering a pathway towards capable reasoning generalist. The study signifies the potential of InternBootcamp in advancing artificial intelligence research. <br /> <div>
arXiv:2508.08636v1 Announce Type: new 
Abstract: Large language models (LLMs) have revolutionized artificial intelligence by enabling complex reasoning capabilities. While recent advancements in reinforcement learning (RL) have primarily focused on domain-specific reasoning tasks (e.g., mathematics or code generation), real-world reasoning scenarios often require models to handle diverse and complex environments that narrow-domain benchmarks cannot fully capture. To address this gap, we present InternBootcamp, an open-source framework comprising 1000+ domain-diverse task environments specifically designed for LLM reasoning research. Our codebase offers two key functionalities: (1) automated generation of unlimited training/testing cases with configurable difficulty levels, and (2) integrated verification modules for objective response evaluation. These features make InternBootcamp fundamental infrastructure for RL-based model optimization, synthetic data generation, and model evaluation. Although manually developing such a framework with enormous task coverage is extremely cumbersome, we accelerate the development procedure through an automated agent workflow supplemented by manual validation protocols, which enables the task scope to expand rapidly. % With these bootcamps, we further establish Bootcamp-EVAL, an automatically generated benchmark for comprehensive performance assessment. Evaluation reveals that frontier models still underperform in many reasoning tasks, while training with InternBootcamp provides an effective way to significantly improve performance, leading to our 32B model that achieves state-of-the-art results on Bootcamp-EVAL and excels on other established benchmarks. In particular, we validate that consistent performance gains come from including more training tasks, namely \textbf{task scaling}, over two orders of magnitude, offering a promising route towards capable reasoning generalist.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quick on the Uptake: Eliciting Implicit Intents from Human Demonstrations for Personalized Mobile-Use Agents</title>
<link>https://arxiv.org/abs/2508.08645</link>
<guid>https://arxiv.org/abs/2508.08645</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal language models, Mobile-use agents, Intention alignment rate, Human intent, Demonstration learning

Summary:
Integrating multimodal language models with mobile-use agents allows for automation of mobile tasks through human-like interactions. Previous studies focused on explicit human intention flows, neglecting implicit intentions and personal preferences, hindering the creation of personalized mobile-use agents. This work introduces MobileIAR dataset to evaluate the alignment of mobile-use agents with human intent. The IFRAgent framework utilizes Intention Flow Recognition to analyze both explicit and implicit intention flows from human demonstrations. By constructing query-level standard operating procedures and user-level habit repositories, IFRAgent generates personalized queries and SOPs through query rewriting and retrieval-augmented generation, enhancing alignment with human intent. Experimental results show that IFRAgent significantly improves human intention alignment rate and step completion rates compared to baselines. The code for IFRAgent is available on GitHub at https://github.com/MadeAgents/Quick-on-the-Uptake. 

<br /><br />Summary: <div>
arXiv:2508.08645v1 Announce Type: new 
Abstract: As multimodal large language models advance rapidly, the automation of mobile tasks has become increasingly feasible through the use of mobile-use agents that mimic human interactions from graphical user interface. To further enhance mobile-use agents, previous studies employ demonstration learning to improve mobile-use agents from human demonstrations. However, these methods focus solely on the explicit intention flows of humans (e.g., step sequences) while neglecting implicit intention flows (e.g., personal preferences), which makes it difficult to construct personalized mobile-use agents. In this work, to evaluate the \textbf{I}ntention \textbf{A}lignment \textbf{R}ate between mobile-use agents and humans, we first collect \textbf{MobileIAR}, a dataset containing human-intent-aligned actions and ground-truth actions. This enables a comprehensive assessment of the agents' understanding of human intent. Then we propose \textbf{IFRAgent}, a framework built upon \textbf{I}ntention \textbf{F}low \textbf{R}ecognition from human demonstrations. IFRAgent analyzes explicit intention flows from human demonstrations to construct a query-level vector library of standard operating procedures (SOP), and analyzes implicit intention flows to build a user-level habit repository. IFRAgent then leverages a SOP extractor combined with retrieval-augmented generation and a query rewriter to generate personalized query and SOP from a raw ambiguous query, enhancing the alignment between mobile-use agents and human intent. Experimental results demonstrate that IFRAgent outperforms baselines by an average of 6.79\% (32.06\% relative improvement) in human intention alignment rate and improves step completion rates by an average of 5.30\% (26.34\% relative improvement). The codes are available at https://github.com/MadeAgents/Quick-on-the-Uptake.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaMA-Based Models for Aspect-Based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2508.08649</link>
<guid>https://arxiv.org/abs/2508.08649</guid>
<content:encoded><![CDATA[
<div> Language models, ABSA, fine-tuning, performance evaluation, error analysis<br />
<br />
Summary: 
This paper explores the potential of fine-tuned large language models (LLMs) for compound aspect-based sentiment analysis (ABSA). Specifically, the study focuses on LLaMA-based models and evaluates their performance on four tasks across eight English datasets. The fine-tuned Orca 2 model outperforms existing models in all tasks but struggles in zero-shot and few-shot scenarios. Error analysis reveals challenges faced by fine-tuned models, shedding light on areas for improvement in LLMs for ABSA tasks. <div>
arXiv:2508.08649v1 Announce Type: new 
Abstract: While large language models (LLMs) show promise for various tasks, their performance in compound aspect-based sentiment analysis (ABSA) tasks lags behind fine-tuned models. However, the potential of LLMs fine-tuned for ABSA remains unexplored. This paper examines the capabilities of open-source LLMs fine-tuned for ABSA, focusing on LLaMA-based models. We evaluate the performance across four tasks and eight English datasets, finding that the fine-tuned Orca~2 model surpasses state-of-the-art results in all tasks. However, all models struggle in zero-shot and few-shot scenarios compared to fully fine-tuned ones. Additionally, we conduct error analysis to identify challenges faced by fine-tuned models.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UWB at WASSA-2024 Shared Task 2: Cross-lingual Emotion Detection</title>
<link>https://arxiv.org/abs/2508.08650</link>
<guid>https://arxiv.org/abs/2508.08650</guid>
<content:encoded><![CDATA[
<div> approach, fine-tuning, language models, emotion detection, trigger words <br />
Summary: <br />
This paper introduces a system developed for the WASSA-2024 Cross-lingual Emotion Detection Shared Task. The system focuses on fine-tuning quantized large language models such as Orca 2 with low-rank adapters and multilingual Transformer-based models like XLM-R and mT5. Performance is improved through machine translation for both subtasks and trigger word switching for the second subtask. The system excels in numerical trigger words detection, ranking 1st, and achieves 3rd place in binary trigger words detection and 7th place in emotion detection. <div>
arXiv:2508.08650v1 Announce Type: new 
Abstract: This paper presents our system built for the WASSA-2024 Cross-lingual Emotion Detection Shared Task. The task consists of two subtasks: first, to assess an emotion label from six possible classes for a given tweet in one of five languages, and second, to predict words triggering the detected emotions in binary and numerical formats. Our proposed approach revolves around fine-tuning quantized large language models, specifically Orca~2, with low-rank adapters (LoRA) and multilingual Transformer-based models, such as XLM-R and mT5. We enhance performance through machine translation for both subtasks and trigger word switching for the second subtask. The system achieves excellent performance, ranking 1st in numerical trigger words detection, 3rd in binary trigger words detection, and 7th in emotion detection.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Based Approach for Czech Sentiment Analysis</title>
<link>https://arxiv.org/abs/2508.08651</link>
<guid>https://arxiv.org/abs/2508.08651</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt-based methods, aspect-based sentiment analysis, sentiment classification, Czech, sequence-to-sequence models<br />
Summary:<br />
This paper introduces prompt-based methods for aspect-based sentiment analysis and sentiment classification in the Czech language. The researchers utilize sequence-to-sequence models to tackle aspect-based tasks simultaneously and demonstrate the effectiveness of prompt-based approaches over traditional fine-tuning methods. Zero-shot and few-shot learning experiments for sentiment classification reveal that prompting leads to significant improvements with limited training data compared to fine-tuning. Moreover, pre-training on target domain data shows notable enhancements in a zero-shot learning scenario. The study highlights the advantages of prompt-based techniques in sentiment analysis tasks and emphasizes the importance of domain-specific pre-training for better performance in data-scarce scenarios.<br /> 
Summary: <div>
arXiv:2508.08651v1 Announce Type: new 
Abstract: This paper introduces the first prompt-based methods for aspect-based sentiment analysis and sentiment classification in Czech. We employ the sequence-to-sequence models to solve the aspect-based tasks simultaneously and demonstrate the superiority of our prompt-based approach over traditional fine-tuning. In addition, we conduct zero-shot and few-shot learning experiments for sentiment classification and show that prompting yields significantly better results with limited training examples compared to traditional fine-tuning. We also demonstrate that pre-training on data from the target domain can lead to significant improvements in a zero-shot scenario.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM driven Text-to-Table Generation through Sub-Tasks Guidance and Iterative Refinement</title>
<link>https://arxiv.org/abs/2508.08653</link>
<guid>https://arxiv.org/abs/2508.08653</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, text-to-table generation, prompting techniques, task decomposition, iterative self-feedback

Summary: 
In this paper, the authors propose an efficient system for text-to-table generation using Large Language Models (LLMs). They address the challenges of handling ambiguous or domain-specific data, maintaining table structure, managing long inputs, and addressing numerical reasoning. The system leverages novel prompting techniques, breaking down the task into manageable sub-tasks and refining the generated tables through iterative self-feedback. The custom task decomposition approach enables the model to improve the quality of the generated tables step by step. The authors discuss the benefits and potential risks of iterative self-feedback on the generated tables, highlighting the trade-offs between performance enhancement and computational cost. Experimental results show that the proposed methods outperform baseline models on two complex text-to-table generation datasets, demonstrating the effectiveness of the system in transforming unstructured text into structured data. 

Summary: <div>
arXiv:2508.08653v1 Announce Type: new 
Abstract: Transforming unstructured text into structured data is a complex task, requiring semantic understanding, reasoning, and structural comprehension. While Large Language Models (LLMs) offer potential, they often struggle with handling ambiguous or domain-specific data, maintaining table structure, managing long inputs, and addressing numerical reasoning. This paper proposes an efficient system for LLM-driven text-to-table generation that leverages novel prompting techniques. Specifically, the system incorporates two key strategies: breaking down the text-to-table task into manageable, guided sub-tasks and refining the generated tables through iterative self-feedback. We show that this custom task decomposition allows the model to address the problem in a stepwise manner and improves the quality of the generated table. Furthermore, we discuss the benefits and potential risks associated with iterative self-feedback on the generated tables while highlighting the trade-offs between enhanced performance and computational cost. Our methods achieve strong results compared to baselines on two complex text-to-table generation datasets available in the public domain.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation</title>
<link>https://arxiv.org/abs/2508.08680</link>
<guid>https://arxiv.org/abs/2508.08680</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, machine translation, in-context learning, low-resource languages, synthetic parallel data <br />
Summary: <br />
- LLMs are effective in machine translation, particularly with in-context learning, but struggle with low-resource languages. 
- Example selection and fine-tuning help, but are limited by dataset quality and diversity. 
- Creating synthetic parallel data is common, with backtranslation being a key method. 
- However, quality target-side texts are not always available for many low-resource languages. 
- The proposed \textsc{TopXGen} approach leverages LLMs' ability to generate high-quality target texts in high-resource languages, which can be backtranslated to produce diverse parallel data for low-resource languages. 
- \textsc{TopXGen} enhances LLM translation performance during fine-tuning and in-context learning. 
Summary: <div>
arXiv:2508.08680v1 Announce Type: new 
Abstract: LLMs have been shown to perform well in machine translation (MT) with the use of in-context learning (ICL), rivaling supervised models when translating into high-resource languages (HRLs). However, they lag behind when translating into low-resource language (LRLs). Example selection via similarity search and supervised fine-tuning help. However the improvements they give are limited by the size, quality and diversity of existing parallel datasets. A common technique in low-resource MT is synthetic parallel data creation, the most frequent of which is backtranslation, whereby existing target-side texts are automatically translated into the source language. However, this assumes the existence of good quality and relevant target-side texts, which are not readily available for many LRLs. In this paper, we present \textsc{TopXGen}, an LLM-based approach for the generation of high quality and topic-diverse data in multiple LRLs, which can then be backtranslated to produce useful and diverse parallel texts for ICL and fine-tuning. Our intuition is that while LLMs struggle to translate into LRLs, their ability to translate well into HRLs and their multilinguality enable them to generate good quality, natural-sounding target-side texts, which can be translated well into a high-resource source language. We show that \textsc{TopXGen} boosts LLM translation performance during fine-tuning and in-context learning. Code and outputs are available at https://github.com/ArmelRandy/topxgen.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out of the Box, into the Clinic? Evaluating State-of-the-Art ASR for Clinical Applications for Older Adults</title>
<link>https://arxiv.org/abs/2508.08684</link>
<guid>https://arxiv.org/abs/2508.08684</guid>
<content:encoded><![CDATA[
<div> Keyword: Voice-controlled interfaces, Automatic Speech Recognition, Older adults, Dutch, Chatbots 

Summary: 
Voice-controlled interfaces can benefit older adults in clinical settings, but Automatic Speech Recognition (ASR) for underrepresented groups such as older Dutch adults remains a challenge. This study evaluates ASR models on language use of older Dutch adults interacting with the Welzijn.AI chatbot. Benchmarking generic multilingual ASR models against models fine-tuned for Dutch spoken by older adults, the study found that generic models outperformed fine-tuned ones, indicating good generalization to realistic datasets. Truncating existing architectures helped balance accuracy-speed trade-off, but some cases showed high Word Error Rate due to hallucinations. The study underscores the potential of generic ASR models in supporting older adults using voice-controlled interfaces like chatbots in geriatric contexts.

<br /><br />Summary: <div>
arXiv:2508.08684v1 Announce Type: new 
Abstract: Voice-controlled interfaces can support older adults in clinical contexts, with chatbots being a prime example, but reliable Automatic Speech Recognition (ASR) for underrepresented groups remains a bottleneck. This study evaluates state-of-the-art ASR models on language use of older Dutch adults, who interacted with the Welzijn.AI chatbot designed for geriatric contexts. We benchmark generic multilingual ASR models, and models fine-tuned for Dutch spoken by older adults, while also considering processing speed. Our results show that generic multilingual models outperform fine-tuned models, which suggests recent ASR models can generalise well out of the box to realistic datasets. Furthermore, our results suggest that truncating existing architectures is helpful in balancing the accuracy-speed trade-off, though we also identify some cases with high WER due to hallucinations.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models</title>
<link>https://arxiv.org/abs/2508.08712</link>
<guid>https://arxiv.org/abs/2508.08712</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, text generation, autoregressive, parallel text generation, inference efficiency 

Summary: 
Large Language Models (LLMs) are crucial for text generation in various applications. Most LLMs use autoregressive (AR) generation, which is slow due to its sequential nature. To improve speed and efficiency, researchers are exploring parallel text generation techniques. This survey categorizes methods into AR-based and Non-AR-based paradigms, analyzing their trade-offs in terms of speed, quality, and efficiency. The study also explores potential combinations with other acceleration strategies. Recent advancements are highlighted, along with identified open challenges and future research directions. Overall, parallel text generation offers a promising approach to enhancing text generation efficiency and performance. 

<br /><br />Summary: <div>
arXiv:2508.08712v1 Announce Type: new 
Abstract: As text generation has become a core capability of modern Large Language Models (LLMs), it underpins a wide range of downstream applications. However, most existing LLMs rely on autoregressive (AR) generation, producing one token at a time based on previously generated context-resulting in limited generation speed due to the inherently sequential nature of the process. To address this challenge, an increasing number of researchers have begun exploring parallel text generation-a broad class of techniques aimed at breaking the token-by-token generation bottleneck and improving inference efficiency. Despite growing interest, there remains a lack of comprehensive analysis on what specific techniques constitute parallel text generation and how they improve inference performance. To bridge this gap, we present a systematic survey of parallel text generation methods. We categorize existing approaches into AR-based and Non-AR-based paradigms, and provide a detailed examination of the core techniques within each category. Following this taxonomy, we assess their theoretical trade-offs in terms of speed, quality, and efficiency, and examine their potential for combination and comparison with alternative acceleration strategies. Finally, based on our findings, we highlight recent advancements, identify open challenges, and outline promising directions for future research in parallel text generation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization</title>
<link>https://arxiv.org/abs/2508.08719</link>
<guid>https://arxiv.org/abs/2508.08719</guid>
<content:encoded><![CDATA[
<div> reflection, Large Language Models, trait elicitation, self-reflection, human traits<br />
Summary:<br />
The article introduces IROTE, a method for stable and transferable trait elicitation in Large Language Models (LLMs). Existing methods struggle with shallow and unstable stylistic patterns, leading to inconsistent behavior across tasks. IROTE addresses this issue by generating and optimizing a textual self-reflection within prompts to stimulate LLMs' trait-driven behavior. By maximizing an information-theoretic objective, IROTE enhances the connections between LLMs' behavior and target traits while reducing noisy redundancy. Experimental results show that IROTE-generated self-reflections can induce LLMs' stable impersonation of target traits across diverse tasks, outperforming existing baselines. This method showcases the potential for accurately embodying specific human traits in LLMs without the need for fine-tuning.<br /><br />Summary: <div>
arXiv:2508.08719v1 Announce Type: new 
Abstract: Trained on various human-authored corpora, Large Language Models (LLMs) have demonstrated a certain capability of reflecting specific human-like traits (e.g., personality or values) by prompting, benefiting applications like personalized LLMs and social simulations. However, existing methods suffer from the superficial elicitation problem: LLMs can only be steered to mimic shallow and unstable stylistic patterns, failing to embody the desired traits precisely and consistently across diverse tasks like humans. To address this challenge, we propose IROTE, a novel in-context method for stable and transferable trait elicitation. Drawing on psychological theories suggesting that traits are formed through identity-related reflection, our method automatically generates and optimizes a textual self-reflection within prompts, which comprises self-perceived experience, to stimulate LLMs' trait-driven behavior. The optimization is performed by iteratively maximizing an information-theoretic objective that enhances the connections between LLMs' behavior and the target trait, while reducing noisy redundancy in reflection without any fine-tuning, leading to evocative and compact trait reflection. Extensive experiments across three human trait systems manifest that one single IROTE-generated self-reflection can induce LLMs' stable impersonation of the target trait across diverse downstream tasks beyond simple questionnaire answering, consistently outperforming existing strong baselines.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Magical: Medical Lay Language Generation via Semantic Invariance and Layperson-tailored Adaptation</title>
<link>https://arxiv.org/abs/2508.08730</link>
<guid>https://arxiv.org/abs/2508.08730</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical Lay Language Generation, LoRA, Magical, Semantic Invariance Constraint, Recommendation-guided Switch

Summary:
Magical proposes a new architecture for Medical Lay Language Generation (MLLG) to overcome the limitations of standard LoRA in dealing with heterogeneous data. It utilizes a shared matrix for summarization and multiple isolated matrices for diverse lay-style generation, improving semantic fidelity and generating varied lay styles. Magical introduces a Semantic Invariance Constraint to maintain semantic accuracy and incorporates a Recommendation-guided Switch for adapting to diverse lay styles. Experimental results on real-world datasets demonstrate that Magical outperforms other methods while reducing trainable parameters. This new approach enhances the accessibility of complex scientific content for broader audiences in the medical field.<br /><br />Summary: <div>
arXiv:2508.08730v1 Announce Type: new 
Abstract: Medical Lay Language Generation (MLLG) plays a vital role in improving the accessibility of complex scientific content for broader audiences. Recent literature to MLLG commonly employ parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA) to fine-tuning large language models (LLMs) using paired expert-lay language datasets. However, LoRA struggles with the challenges posed by multi-source heterogeneous MLLG datasets. Specifically, through a series of exploratory experiments, we reveal that standard LoRA fail to meet the requirement for semantic fidelity and diverse lay-style generation in MLLG task. To address these limitations, we propose Magical, an asymmetric LoRA architecture tailored for MLLG under heterogeneous data scenarios. Magical employs a shared matrix $A$ for abstractive summarization, along with multiple isolated matrices $B$ for diverse lay-style generation. To preserve semantic fidelity during the lay language generation process, Magical introduces a Semantic Invariance Constraint to mitigate semantic subspace shifts on matrix $A$. Furthermore, to better adapt to diverse lay-style generation, Magical incorporates the Recommendation-guided Switch, an externally interface to prompt the LLM to switch between different matrices $B$. Experimental results on three real-world lay language generation datasets demonstrate that Magical consistently outperforms prompt-based methods, vanilla LoRA, and its recent variants, while also reducing trainable parameters by 31.66%.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciRerankBench: Benchmarking Rerankers Towards Scientific Retrieval-Augmented Generated LLMs</title>
<link>https://arxiv.org/abs/2508.08742</link>
<guid>https://arxiv.org/abs/2508.08742</guid>
<content:encoded><![CDATA[
<div> Question answering, scientific literature, RAG-LLMs, rerankers, benchmark <br />
<br />
Summary: 
The article introduces a benchmark, SciRerankBench, designed to evaluate rerankers in two-stage retrieval-augmented generated large language models (RAG-LLMs) for scientific question answering. The reranker plays a crucial role in ensuring accurate and relevant answers in the scientific domain. The benchmark includes three types of question-context-answer pairs to assess noise resilience, relevance disambiguation, and factual consistency. Thirteen rerankers across five LLM families are systematically evaluated, providing insights into their strengths and limitations. This benchmark, SciRerankBench, is a valuable tool for scrutinizing the performance of rerankers within RAG-LLMs and guiding future developments in this field. <div>
arXiv:2508.08742v1 Announce Type: new 
Abstract: Scientific literature question answering is a pivotal step towards new scientific discoveries. Recently, \textit{two-stage} retrieval-augmented generated large language models (RAG-LLMs) have shown impressive advancements in this domain. Such a two-stage framework, especially the second stage (reranker), is particularly essential in the scientific domain, where subtle differences in terminology may have a greatly negative impact on the final factual-oriented or knowledge-intensive answers. Despite this significant progress, the potential and limitations of these works remain unexplored. In this work, we present a Scientific Rerank-oriented RAG Benchmark (SciRerankBench), for evaluating rerankers within RAG-LLMs systems, spanning five scientific subjects. To rigorously assess the reranker performance in terms of noise resilience, relevance disambiguation, and factual consistency, we develop three types of question-context-answer (Q-C-A) pairs, i.e., Noisy Contexts (NC), Semantically Similar but Logically Irrelevant Contexts (SSLI), and Counterfactual Contexts (CC). Through systematic evaluation of 13 widely used rerankers on five families of LLMs, we provide detailed insights into their relative strengths and limitations. To the best of our knowledge, SciRerankBench is the first benchmark specifically developed to evaluate rerankers within RAG-LLMs, which provides valuable observations and guidance for their future development.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DevNous: An LLM-Based Multi-Agent System for Grounding IT Project Management in Unstructured Conversation</title>
<link>https://arxiv.org/abs/2508.08761</link>
<guid>https://arxiv.org/abs/2508.08761</guid>
<content:encoded><![CDATA[
<div> Keywords: unstructured team dialogue, Information Technology project governance, Large Language Model, multi-agent expert system, benchmark dataset

Summary:
DevNous is a Large Language Model-based multi-agent expert system designed to automate the translation of unstructured team dialogue into structured artifacts for IT project governance. It integrates into team chat environments, identifying intents and managing workflows for tasks like task formalization and progress summary synthesis. A new benchmark dataset of 160 conversational turns was created and publicly released, and DevNous achieved high accuracy on this benchmark. The system presents a validated architectural pattern for ambient administrative agents and establishes a robust empirical baseline for this challenging problem domain. This work contributes to advancing the automation of administrative tasks in information systems management. 

Summary: <div>
arXiv:2508.08761v1 Announce Type: new 
Abstract: The manual translation of unstructured team dialogue into the structured artifacts required for Information Technology (IT) project governance is a critical bottleneck in modern information systems management. We introduce DevNous, a Large Language Model-based (LLM) multi-agent expert system, to automate this unstructured-to-structured translation process. DevNous integrates directly into team chat environments, identifying actionable intents from informal dialogue and managing stateful, multi-turn workflows for core administrative tasks like automated task formalization and progress summary synthesis. To quantitatively evaluate the system, we introduce a new benchmark of 160 realistic, interactive conversational turns. The dataset was manually annotated with a multi-label ground truth and is publicly available. On this benchmark, DevNous achieves an exact match turn accuracy of 81.3\% and a multiset F1-Score of 0.845, providing strong evidence for its viability. The primary contributions of this work are twofold: (1) a validated architectural pattern for developing ambient administrative agents, and (2) the introduction of the first robust empirical baseline and public benchmark dataset for this challenging problem domain.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-protected Retrieval-Augmented Generation for Knowledge Graph Question Answering</title>
<link>https://arxiv.org/abs/2508.08785</link>
<guid>https://arxiv.org/abs/2508.08785</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, RAG, Knowledge Graphs, Privacy, Retrieval

Summary:
In this paper, the authors address the challenges of privacy protection in Retrieval-Aware Generation (RAG) systems that integrate external knowledge from Knowledge Graphs (KGs) into Large Language Models (LLMs). The proposed framework, ARoG, introduces relation-centric and structure-oriented abstraction strategies to ensure privacy while improving knowledge retrieval. By converting entities in KGs into high-level concepts and structuring natural language questions into abstract concept paths, ARoG enhances retrieval performance while preventing LLMs from accessing entity semantics. This privacy-protected RAG scenario safeguards sensitive information in unseen entities, enhancing user privacy. Experimental results on multiple datasets demonstrate the effectiveness and robustness of the ARoG framework in maintaining data privacy while achieving strong performance in knowledge retrieval. The innovative strategies in ARoG provide a solution to the privacy risks associated with integrating KGs into LLMs in RAG systems. 

<br /><br />Summary: <div>
arXiv:2508.08785v1 Announce Type: new 
Abstract: LLMs often suffer from hallucinations and outdated or incomplete knowledge. RAG is proposed to address these issues by integrating external knowledge like that in KGs into LLMs. However, leveraging private KGs in RAG systems poses significant privacy risks due to the black-box nature of LLMs and potential insecure data transmission, especially when using third-party LLM APIs lacking transparency and control. In this paper, we investigate the privacy-protected RAG scenario for the first time, where entities in KGs are anonymous for LLMs, thus preventing them from accessing entity semantics. Due to the loss of semantics of entities, previous RAG systems cannot retrieve question-relevant knowledge from KGs by matching questions with the meaningless identifiers of anonymous entities. To realize an effective RAG system in this scenario, two key challenges must be addressed: (1) How can anonymous entities be converted into retrievable information. (2) How to retrieve question-relevant anonymous entities. Hence, we propose a novel ARoG framework including relation-centric abstraction and structure-oriented abstraction strategies. For challenge (1), the first strategy abstracts entities into high-level concepts by dynamically capturing the semantics of their adjacent relations. It supplements meaningful semantics which can further support the retrieval process. For challenge (2), the second strategy transforms unstructured natural language questions into structured abstract concept paths. These paths can be more effectively aligned with the abstracted concepts in KGs, thereby improving retrieval performance. To guide LLMs to effectively retrieve knowledge from KGs, the two strategies strictly protect privacy from being exposed to LLMs. Experiments on three datasets demonstrate that ARoG achieves strong performance and privacy-robustness.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments</title>
<link>https://arxiv.org/abs/2508.08791</link>
<guid>https://arxiv.org/abs/2508.08791</guid>
<content:encoded><![CDATA[
<div> automated environment construction pipeline, reinforcement learning, tool use, reward mechanism, language models

Summary:
An automated environment construction pipeline is proposed for efficient reinforcement learning in large language models (LLMs) focused on tool use. The pipeline includes scenario decomposition, document generation, function integration, complexity scaling, and localized deployment. It aims to provide high-quality training environments with measurable feedback and a verifiable reward mechanism. This mechanism evaluates precision and completeness of task execution, integrating seamlessly with standard RL algorithms to enhance model training. Experiments on LLMs of varying scales show improved tool-use performance without compromising general capabilities. The gains are attributed to enhanced context understanding and reasoning, driven by updates to lower-layer MLP parameters in the models. <div>
arXiv:2508.08791v1 Announce Type: new 
Abstract: Effective tool use is essential for large language models (LLMs) to interact meaningfully with their environment. However, progress is limited by the lack of efficient reinforcement learning (RL) frameworks specifically designed for tool use, due to challenges in constructing stable training environments and designing verifiable reward mechanisms. To address this, we propose an automated environment construction pipeline, incorporating scenario decomposition, document generation, function integration, complexity scaling, and localized deployment. This enables the creation of high-quality training environments that provide detailed and measurable feedback without relying on external tools. Additionally, we introduce a verifiable reward mechanism that evaluates both the precision of tool use and the completeness of task execution. When combined with trajectory data collected from the constructed environments, this mechanism integrates seamlessly with standard RL algorithms to facilitate feedback-driven model training. Experiments on LLMs of varying scales demonstrate that our approach significantly enhances the models' tool-use performance without degrading their general capabilities, regardless of inference modes or training algorithms. Our analysis suggests that these gains result from improved context understanding and reasoning, driven by updates to the lower-layer MLP parameters in models.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TiMoE: Time-Aware Mixture of Language Experts</title>
<link>https://arxiv.org/abs/2508.08827</link>
<guid>https://arxiv.org/abs/2508.08827</guid>
<content:encoded><![CDATA[
<div> pre-training, large language models, temporal leakage, TiMoE, TSQA
Summary: 
Large language models are often trained on outdated data, leading to temporal leakage. To address this issue, a new approach called TiMoE combines GPT-style experts trained on different time slices of a corpus. At inference, TiMoE masks experts with future knowledge relative to the query timestamp, ensuring causal validity. The method outperforms single-period experts and reduces future-knowledge errors by up to 15%. Experiments on multiple NLP tasks and a new benchmark called TSQA show the effectiveness of TiMoE in maintaining chronological grounding without sacrificing performance. The modular, time-segmented pre-training coupled with causal routing proves to be a simple yet efficient method for improving the temporal accuracy of large language models.<br /><br />Summary: <div>
arXiv:2508.08827v1 Announce Type: new 
Abstract: Large language models (LLMs) are typically trained on fixed snapshots of the web, which means that their knowledge becomes stale and their predictions risk temporal leakage: relying on information that lies in the future relative to a query. We tackle this problem by pre-training from scratch a set of GPT-style experts on disjoint two-year slices of a 2013-2024 corpus and combining them through TiMoE, a Time-aware Mixture of Language Experts. At inference time, TiMoE masks all experts whose training window ends after the query timestamp and merges the remaining log-probabilities in a shared space, guaranteeing strict causal validity while retaining the breadth of multi-period knowledge. We also release TSQA, a 10k-question benchmark whose alternatives are explicitly labelled as past, future or irrelevant, allowing fine-grained measurement of temporal hallucinations. Experiments on eight standard NLP tasks plus TSQA show that a co-adapted TiMoE variant matches or exceeds the best single-period expert and cuts future-knowledge errors by up to 15%. Our results demonstrate that modular, time-segmented pre-training paired with causal routing is a simple yet effective path toward LLMs that stay chronologically grounded without sacrificing general performance much. We open source our code at TiMoE (Github): https://github.com/epfml/TiMoE
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems</title>
<link>https://arxiv.org/abs/2508.08833</link>
<guid>https://arxiv.org/abs/2508.08833</guid>
<content:encoded><![CDATA[
<div> Mathematical reasoning, LLMs, evaluation methodology, benchmark dataset, performance degradation

Summary:
This paper introduces a new systematic framework for evaluating the mathematical reasoning robustness of Large Language Models (LLMs) by subjecting them to mathematically equivalent but linguistically and parametrically varied problems. The proposed evaluation methodology, showcased through the PutnamGAP benchmark dataset, highlights the sensitivity of LLMs to non-mathematical perturbations, allowing for a more accurate assessment of their mathematical reasoning capabilities. The study evaluates 18 LLM models, including OpenAI's O3, revealing significant performance degradation on variant problems. O3 scores 49% on original problems, but drops by 4 percentage points on surface variants and 10.5 percentage points on core-step-based variants. The findings demonstrate the effectiveness of the new evaluation methodology in deepening the understanding of LLM robustness and provide insights for enhancing their mathematical reasoning abilities.<br /><br />Summary: <div>
arXiv:2508.08833v1 Announce Type: new 
Abstract: In this paper, we introduce a systematic framework beyond conventional method to assess LLMs' mathematical-reasoning robustness by stress-testing them on advanced math problems that are mathematically equivalent but with linguistic and parametric variation. These transformations allow us to measure the sensitivity of LLMs to non-mathematical perturbations, thereby enabling a more accurate evaluation of their mathematical reasoning capabilities. Using this new evaluation methodology, we created PutnamGAP, a new benchmark dataset with multiple mathematically-equivalent variations of competition-level math problems. With the new dataset, we evaluate multiple families of representative LLMs and examine their robustness. Across 18 commercial and open-source models we observe sharp performance degradation on the variants. OpenAI's flagship reasoning model, O3, scores 49 % on the originals but drops by 4 percentage points on surface variants, and by 10.5 percentage points on core-step-based variants, while smaller models fare far worse. Overall, the results show that the proposed new evaluation methodology is effective for deepening our understanding of the robustness of LLMs and generating new insights for further improving their mathematical reasoning capabilities.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Towards Fairness: Mitigating Political Bias in LLMs</title>
<link>https://arxiv.org/abs/2508.08846</link>
<guid>https://arxiv.org/abs/2508.08846</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, ideological biases, decoder-based LLMs, political framing, representational bias

Summary:<br /><br />
This paper discusses the issue of ideological biases in large language models (LLMs), particularly in decoder-based LLMs used in various real-world applications. The authors propose a framework for probing and mitigating biases by analyzing internal model representations, focusing on political and economic dimensions. Using the Political Compass Test (PCT) as a basis, they compare hidden layer activations from models like Mistral and DeepSeek using contrastive pairs. Through a comprehensive activation extraction pipeline, they identify disparities in model representations linked to political framing. The study reveals that decoder LLMs consistently encode representational bias across different layers. The researchers suggest leveraging steering vector-based mitigation to address this bias effectively. This work provides valuable insights into how political bias manifests in LLMs and presents a systematic approach to debiasing at a deeper level than just adjusting model outputs. <div>
arXiv:2508.08846v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have enabled their widespread use across diverse real-world applications. However, concerns remain about their tendency to encode and reproduce ideological biases, particularly along political and economic dimensions. In this paper, we propose a framework for probing and mitigating such biases in decoder-based LLMs through analysis of internal model representations. Grounded in the Political Compass Test (PCT), our method uses contrastive pairs to extract and compare hidden layer activations from models like Mistral and DeepSeek. We introduce a comprehensive activation extraction pipeline capable of layer-wise analysis across multiple ideological axes, revealing meaningful disparities linked to political framing. Our results show that decoder LLMs systematically encode representational bias across layers, which can be leveraged for effective steering vector-based mitigation. This work provides new insights into how political bias is encoded in LLMs and offers a principled approach to debiasing beyond surface-level output interventions.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiasGym: Fantastic Biases and How to Find (and Remove) Them</title>
<link>https://arxiv.org/abs/2508.08855</link>
<guid>https://arxiv.org/abs/2508.08855</guid>
<content:encoded><![CDATA[
<div> keywords: Large Language Models, biases, stereotypes, BiasGym, debiasing  
Summary:  
BiasGym introduces a framework for injecting, analyzing, and mitigating biases and stereotypes in Large Language Models (LLMs). It comprises two components: BiasInject, for injecting biases into the model, and BiasScope, for identifying and addressing biased behavior. The method allows for consistent bias elicitation, supports targeted debiasing without performance degradation, and generalizes to unseen biases. It successfully reduces real-world stereotypes and probes fictional associations, demonstrating its utility for safety interventions and interpretability research.<br /><br />Summary: <div>
arXiv:2508.08855v1 Announce Type: new 
Abstract: Understanding biases and stereotypes encoded in the weights of Large Language Models (LLMs) is crucial for developing effective mitigation strategies. Biased behaviour is often subtle and non-trivial to isolate, even when deliberately elicited, making systematic analysis and debiasing particularly challenging. To address this, we introduce BiasGym, a simple, cost-effective, and generalizable framework for reliably injecting, analyzing, and mitigating conceptual associations within LLMs. BiasGym consists of two components: BiasInject, which injects specific biases into the model via token-based fine-tuning while keeping the model frozen, and BiasScope, which leverages these injected signals to identify and steer the components responsible for biased behavior. Our method enables consistent bias elicitation for mechanistic analysis, supports targeted debiasing without degrading performance on downstream tasks, and generalizes to biases unseen during training. We demonstrate the effectiveness of BiasGym in reducing real-world stereotypes (e.g., people from a country being `reckless drivers') and in probing fictional associations (e.g., people from a country having `blue skin'), showing its utility for both safety interventions and interpretability research.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weakly Supervised Fine-grained Span-Level Framework for Chinese Radiology Report Quality Assurance</title>
<link>https://arxiv.org/abs/2508.08876</link>
<guid>https://arxiv.org/abs/2508.08876</guid>
<content:encoded><![CDATA[
<div> Keywords: Quality Assurance, Radiology Reports, Span-level Evaluation, Semantic Difference, Automated Scoring

Summary:
Sqator is an automated system designed for Quality Assurance (QA) of radiology reports written by junior doctors. It utilizes a span-level evaluation approach to analyze semantic differences between junior and senior reports, measuring the importance of revised text spans to determine QA scores. By assessing the importance of these revised spans, Sqator can accurately mark QA scores without the need for intensive labor from senior doctors. The system was evaluated using a dataset of 12,013 radiology reports, demonstrating competitive QA scores and a high level of consistency with senior doctor judgments. Sqator offers a more efficient and reliable method for evaluating the quality of radiology reports, reducing the potential for inaccuracies due to factors such as diagnosis bias or senior doctor ability. 

<br /><br />Summary: Sqator is an automated system for Quality Assurance of radiology reports, utilizing span-level evaluation to analyze semantic differences and determine QA scores based on the importance of revised text spans. Evaluated on a dataset of 12,013 reports, Sqator achieved competitive scores and consistency with senior doctor judgments, offering a more efficient and reliable approach to QA evaluation. <div>
arXiv:2508.08876v1 Announce Type: new 
Abstract: Quality Assurance (QA) for radiology reports refers to judging whether the junior reports (written by junior doctors) are qualified. The QA scores of one junior report are given by the senior doctor(s) after reviewing the image and junior report. This process requires intensive labor costs for senior doctors. Additionally, the QA scores may be inaccurate for reasons like diagnosis bias, the ability of senior doctors, and so on. To address this issue, we propose a Span-level Quality Assurance EvaluaTOR (Sqator) to mark QA scores automatically. Unlike the common document-level semantic comparison method, we try to analyze the semantic difference by exploring more fine-grained text spans. Unlike the common document-level semantic comparison method, we try to analyze the semantic difference by exploring more fine-grained text spans. Specifically, Sqator measures QA scores by measuring the importance of revised spans between junior and senior reports, and outputs the final QA scores by merging all revised span scores. We evaluate Sqator using a collection of 12,013 radiology reports. Experimental results show that Sqator can achieve competitive QA scores. Moreover, the importance scores of revised spans can be also consistent with the judgments of senior doctors.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models</title>
<link>https://arxiv.org/abs/2508.08879</link>
<guid>https://arxiv.org/abs/2508.08879</guid>
<content:encoded><![CDATA[
<div> method, cultural biases, large language models, cultural understanding, mechanistic interpretability<br />
<br />
Summary: 
The article introduces Culturescope, a mechanistic interpretability-based method that examines how large language models (LLMs) represent cultural knowledge. By using a patching method, Culturescope extracts cultural knowledge to evaluate intrinsic cultural biases in LLMs. The study shows that LLMs encode Western-dominance bias and cultural flattening in their internal representations. Low-resource cultures exhibit fewer cultural biases, possibly due to limited training resources. This work lays the groundwork for future research on reducing cultural biases in LLMs and improving their cross-cultural understanding. The experimental results suggest that understanding how LLMs internalize cultural biases can lead to advancements in mitigating biases and enhancing cultural inclusivity in AI technologies.<br /> <div>
arXiv:2508.08879v1 Announce Type: new 
Abstract: The growing deployment of large language models (LLMs) across diverse cultural contexts necessitates a better understanding of how the overgeneralization of less documented cultures within LLMs' representations impacts their cultural understanding. Prior work only performs extrinsic evaluation of LLMs' cultural competence, without accounting for how LLMs' internal mechanisms lead to cultural (mis)representation. To bridge this gap, we propose Culturescope, the first mechanistic interpretability-based method that probes the internal representations of LLMs to elicit the underlying cultural knowledge space. CultureScope utilizes a patching method to extract the cultural knowledge. We introduce a cultural flattening score as a measure of the intrinsic cultural biases. Additionally, we study how LLMs internalize Western-dominance bias and cultural flattening, which allows us to trace how cultural biases emerge within LLMs. Our experimental results reveal that LLMs encode Western-dominance bias and cultural flattening in their cultural knowledge space. We find that low-resource cultures are less susceptible to cultural biases, likely due to their limited training resources. Our work provides a foundation for future research on mitigating cultural biases and enhancing LLMs' cultural understanding. Our codes and data used for experiments are publicly available.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs</title>
<link>https://arxiv.org/abs/2508.08895</link>
<guid>https://arxiv.org/abs/2508.08895</guid>
<content:encoded><![CDATA[
<div> Adaptive Serial-Parallel Decoding, Large Language Models, Latency, Parallelizable Structures, Inference Speed  
Summary:  
Adaptive Serial-Parallel Decoding (ASPD) proposes a novel approach to address the inference latency challenges faced by large language models (LLMs). By identifying parallelizable structures within the outputs of autoregressive models, ASPD enables simultaneous decoding of these branches, significantly improving overall inference speed. The framework includes a Hybrid Decoding Engine that seamlessly transitions between serial and parallel decoding modes while maintaining computational efficiency. Extensive evaluations across various tasks demonstrate that ASPD achieves remarkable performance in both effectiveness and efficiency, with up to a 3.19x speedup on the Vicuna Bench. The method maintains high response quality compared to autoregressive models, making it suitable for latency-sensitive applications like AI-powered customer service bots and answer retrieval engines.  <br /><br />Summary: <div>
arXiv:2508.08895v1 Announce Type: new 
Abstract: The increasing scale and complexity of large language models (LLMs) pose significant inference latency challenges, primarily due to their autoregressive decoding paradigm characterized by the sequential nature of next-token prediction. By re-examining the outputs of autoregressive models, we observed that some segments exhibit parallelizable structures, which we term intrinsic parallelism. Decoding each parallelizable branch simultaneously (i.e. parallel decoding) can significantly improve the overall inference speed of LLMs. In this paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which addresses two core challenges: automated construction of parallelizable data and efficient parallel decoding mechanism. More specifically, we introduce a non-invasive pipeline that automatically extracts and validates parallelizable structures from the responses of autoregressive models. To empower efficient adaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which enables seamless transitions between serial and parallel decoding modes while maintaining a reusable KV cache, maximizing computational efficiency. Extensive evaluations across General Tasks, Retrieval-Augmented Generation, Mathematical Reasoning, demonstrate that ASPD achieves unprecedented performance in both effectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up to 3.19x speedup (1.85x on average) while maintaining response quality within 1% difference compared to autoregressive models, realizing significant acceleration without compromising generation quality. Our framework sets a groundbreaking benchmark for efficient LLM parallel inference, paving the way for its deployment in latency-sensitive applications such as AI-powered customer service bots and answer retrieval engines.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of Multidialectal Arabic ASR with Weakly Supervised Pretraining and Continual Supervised Fine-tuning</title>
<link>https://arxiv.org/abs/2508.08912</link>
<guid>https://arxiv.org/abs/2508.08912</guid>
<content:encoded><![CDATA[
<div> pretrain, weakly supervised learning, supervised fine-tuning, Arabic ASR model, dialectal Arabic<br />
<br />
Our work introduces a training pipeline for Arabic Automatic Speech Recognition (ASR) that effectively combines weakly supervised learning with supervised fine-tuning. The model is first pretrained on 15,000 hours of weakly labeled speech data encompassing Modern Standard Arabic (MSA) and various Dialectal Arabic (DA) variants. Subsequently, continual supervised fine-tuning is carried out using a mixture of filtered weakly labeled data and a small, high-quality annotated dataset. This approach has led to state-of-the-art results, claiming the top position in the multi-dialectal Arabic ASR challenge. The study demonstrates the success of utilizing weak supervision in conjunction with fine-tuning to conquer data scarcity hurdles and produce high-quality ASR systems for low-resource, dialect-rich languages.<br /><br />Summary: <div>
arXiv:2508.08912v1 Announce Type: new 
Abstract: Automatic speech recognition (ASR) plays a vital role in enabling natural human-machine interaction across applications such as virtual assistants, industrial automation, customer support, and real-time transcription. However, developing accurate ASR systems for low-resource languages like Arabic remains a significant challenge due to limited labeled data and the linguistic complexity introduced by diverse dialects. In this work, we present a scalable training pipeline that combines weakly supervised learning with supervised fine-tuning to develop a robust Arabic ASR model. In the first stage, we pretrain the model on 15,000 hours of weakly labeled speech covering both Modern Standard Arabic (MSA) and various Dialectal Arabic (DA) variants. In the subsequent stage, we perform continual supervised fine-tuning using a mixture of filtered weakly labeled data and a small, high-quality annotated dataset. Our approach achieves state-of-the-art results, ranking first in the multi-dialectal Arabic ASR challenge. These findings highlight the effectiveness of weak supervision paired with fine-tuning in overcoming data scarcity and delivering high-quality ASR for low-resource, dialect-rich languages.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reveal-Bangla: A Dataset for Cross-Lingual Multi-Step Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2508.08933</link>
<guid>https://arxiv.org/abs/2508.08933</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, Bangla, multi-step reasoning, evaluation, small language models

Summary:
Language models have demonstrated impressive performance on complex multi-step reasoning tasks, but their evaluation has been limited to high-resource languages like English. A manually translated Bangla multi-step reasoning dataset, derived from the English Reveal dataset, was introduced in this study to evaluate the performance of multilingual models. The dataset includes binary and non-binary question types. Results showed that reasoning context is beneficial for challenging non-binary questions, but models struggle to utilize relevant Bangla reasoning steps effectively. The study compared English-centric and Bangla-centric small language models in similar settings. The analysis of how reasoning steps contribute to models' predictions revealed different trends across models and languages. The research sheds light on the challenges and opportunities in developing language models for low-resource languages like Bangla. 

<br /><br />Summary: Language models perform well on complex reasoning tasks, but their evaluation has been limited to high-resource languages. A Bangla multi-step reasoning dataset was introduced, revealing that reasoning context is beneficial for challenging questions. Models struggle to effectively employ relevant Bangla reasoning steps. The study compared multilingual small language models' performance in English and Bangla. Analysis of reasoning steps' impact on predictions highlighted diverse trends across models and languages. This research underscores the need to enhance language models for low-resource languages like Bangla. <div>
arXiv:2508.08933v1 Announce Type: new 
Abstract: Language models have demonstrated remarkable performance on complex multi-step reasoning tasks. However, their evaluation has been predominantly confined to high-resource languages such as English. In this paper, we introduce a manually translated Bangla multi-step reasoning dataset derived from the English Reveal dataset, featuring both binary and non-binary question types. We conduct a controlled evaluation of English-centric and Bangla-centric multilingual small language models on the original dataset and our translated version to compare their ability to exploit relevant reasoning steps to produce correct answers. Our results show that, in comparable settings, reasoning context is beneficial for more challenging non-binary questions, but models struggle to employ relevant Bangla reasoning steps effectively. We conclude by exploring how reasoning steps contribute to models' predictions, highlighting different trends across models and languages.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train Long, Think Short: Curriculum Learning for Efficient Reasoning</title>
<link>https://arxiv.org/abs/2508.08940</link>
<guid>https://arxiv.org/abs/2508.08940</guid>
<content:encoded><![CDATA[
<div> keywords: language models, reasoning abilities, curriculum learning, Group Relative Policy Optimization, token efficiency 

Summary:
In this work, a curriculum learning strategy for enhancing the reasoning abilities of large language models (LLMs) through length control is proposed. The method, utilizing Group Relative Policy Optimization (GRPO), gradually tightens token budgets during training to encourage models to first explore effective solution strategies before distilling them into more concise reasoning traces. A reward function balances task correctness, length efficiency, and formatting adherence, leading to improved performance on various datasets. Experiments show that curriculum-based training outperforms fixed-budget approaches, achieving higher accuracy and significantly improved token efficiency. The study also highlights the importance of reward weighting and decay schedule design in training efficient reasoning models. The code and checkpoints are publicly available on GitHub. 

<br /><br />Summary: <div>
arXiv:2508.08940v1 Announce Type: new 
Abstract: Recent work on enhancing the reasoning abilities of large language models (LLMs) has introduced explicit length control as a means of constraining computational cost while preserving accuracy. However, existing approaches rely on fixed-length training budgets, which do not take advantage of the natural progression from exploration to compression during learning. In this work, we propose a curriculum learning strategy for length-controlled reasoning using Group Relative Policy Optimization (GRPO). Our method starts with generous token budgets and gradually tightens them over training, encouraging models to first discover effective solution strategies and then distill them into more concise reasoning traces. We augment GRPO with a reward function that balances three signals: task correctness (via verifier feedback), length efficiency, and formatting adherence (via structural tags). Experiments on GSM8K, MATH500, SVAMP, College Math, and GSM+ demonstrate that curriculum-based training consistently outperforms fixed-budget baselines at the same final budget, achieving higher accuracy and significantly improved token efficiency. We further ablate the impact of reward weighting and decay schedule design, showing that progressive constraint serves as a powerful inductive bias for training efficient reasoning models. Our code and checkpoints are released at: https://github.com/hammoudhasan/curriculum_grpo.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jointly Generating and Attributing Answers using Logits of Document-Identifier Tokens</title>
<link>https://arxiv.org/abs/2508.08942</link>
<guid>https://arxiv.org/abs/2508.08942</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, hallucination, trustworthiness, LoDIT, document attributions 

Summary: 
- Large Language Models (LLMs) have impressive performances but are prone to hallucination, affecting their trustworthiness. 
- Previous work focused on answer correctness and attribution, while recent work looked at faithfulness in the decision-making process. 
- LoDIT is introduced as a method to generate and attribute answers in RAG, using specific token logits during generation to estimate document contributions. 
- The method outperforms state-of-the-art models on a trustworthiness-focused benchmark, Trust-Align. 
- LoDIT is efficient in latency and robust in different settings. 

<br /><br />Summary: <div>
arXiv:2508.08942v1 Announce Type: new 
Abstract: Despite their impressive performances, Large Language Models (LLMs) remain prone to hallucination, which critically undermines their trustworthiness. While most of the previous work focused on tackling answer and attribution correctness, a recent line of work investigated faithfulness, with a focus on leveraging internal model signals to reflect a model's actual decision-making process while generating the answer. Nevertheless, these methods induce additional latency and have shown limitations in directly aligning token generation with attribution generation. In this paper, we introduce LoDIT, a method that jointly generates and faithfully attributes answers in RAG by leveraging specific token logits during generation. It consists of two steps: (1) marking the documents with specific token identifiers and then leveraging the logits of these tokens to estimate the contribution of each document to the answer during generation, and (2) aggregating these contributions into document attributions. Experiments on a trustworthiness-focused attributed text-generation benchmark, Trust-Align, show that LoDIT significantly outperforms state-of-the-art models on several metrics. Finally, an in-depth analysis of LoDIT shows both its efficiency in terms of latency and its robustness in different settings.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrospective Sparse Attention for Efficient Long-Context Generation</title>
<link>https://arxiv.org/abs/2508.09001</link>
<guid>https://arxiv.org/abs/2508.09001</guid>
<content:encoded><![CDATA[
<div> Key-words: Large Language Models, Key-Value Cache, RetroAttention, KV Compression, Long-Generation Benchmarks <br />
Summary: 
The paper introduces RetroAttention, a novel technique for updating Key-Value (KV) caches in Large Language Models (LLMs) that allows for retrospective revision of past attention outputs using new KV entries. This approach addresses the issue of cumulative attention errors that arise during long decoding tasks. RetroAttention maintains a lightweight output cache, enabling past queries to access more relevant context efficiently with minimal latency overhead. By continually correcting prior approximations, RetroAttention outperforms current state-of-the-art KV compression methods. Experimental results on long-generation benchmarks demonstrate that RetroAttention increases effective KV exposure by up to 1.6 times and accuracy by up to 21.9%. <br /> <div>
arXiv:2508.09001v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed in long-context tasks such as reasoning, code generation, and multi-turn dialogue. However, inference over extended contexts is bottlenecked by the Key-Value (KV) cache, whose memory footprint grows linearly with sequence length and dominates latency at each decoding step. While recent KV cache compression methods identify and load important tokens, they focus predominantly on input contexts and fail to address the cumulative attention errors that arise during long decoding. In this paper, we introduce RetroAttention, a novel KV cache update technique that retrospectively revises past attention outputs using newly arrived KV entries from subsequent decoding steps. By maintaining a lightweight output cache, RetroAttention enables past queries to efficiently access more relevant context, while incurring minimal latency overhead. This breaks the fixed-attention-output paradigm and allows continual correction of prior approximations. Extensive experiments on long-generation benchmarks show that RetroAttention consistently outperforms state-of-the-art (SOTA) KV compression methods, increasing effective KV exposure by up to 1.6$\times$ and accuracy by up to 21.9\%.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA</title>
<link>https://arxiv.org/abs/2508.09012</link>
<guid>https://arxiv.org/abs/2508.09012</guid>
<content:encoded><![CDATA[
<div> Keywords: SemEval, Tabular Question Answering, Zero-shot pipeline, Large Language Model, Code generation

Summary: 
This paper discusses the authors' participation in SemEval 2025 Task 8, focusing on Tabular Question Answering. They present a zero-shot pipeline that utilizes a Large Language Model to generate functional code for extracting information from tabular data based on input questions. The pipeline includes modules for identifying relevant columns and analyzing their data types to enhance extraction accuracy. In case of code generation failure, an iterative refinement process incorporates error feedback for improved robustness. Despite the absence of task-specific fine-tuning, the zero-shot code generation approach demonstrates validity in Tabular QA, ranking 33 out of 53 in the test phase. <div>
arXiv:2508.09012v1 Announce Type: new 
Abstract: This paper describes our participation in SemEval 2025 Task 8, focused on Tabular Question Answering. We developed a zero-shot pipeline that leverages an Large Language Model to generate functional code capable of extracting the relevant information from tabular data based on an input question. Our approach consists of a modular pipeline where the main code generator module is supported by additional components that identify the most relevant columns and analyze their data types to improve extraction accuracy. In the event that the generated code fails, an iterative refinement process is triggered, incorporating the error feedback into a new generation prompt to enhance robustness. Our results show that zero-shot code generation is a valid approach for Tabular QA, achieving rank 33 of 53 in the test phase despite the lack of task-specific fine-tuning.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Training-free Alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2508.09016</link>
<guid>https://arxiv.org/abs/2508.09016</guid>
<content:encoded><![CDATA[
arXiv:2508.09016v1 Announce Type: new 
Abstract: The alignment of large language models (LLMs) aims to ensure their outputs adhere to human values, ethical standards, and legal norms. Traditional alignment methods often rely on resource-intensive fine-tuning (FT), which may suffer from knowledge degradation and face challenges in scenarios where the model accessibility or computational resources are constrained. In contrast, training-free (TF) alignment techniques--leveraging in-context learning, decoding-time adjustments, and post-generation corrections--offer a promising alternative by enabling alignment without heavily retraining LLMs, making them adaptable to both open-source and closed-source environments. This paper presents the first systematic review of TF alignment methods, categorizing them by stages of pre-decoding, in-decoding, and post-decoding. For each stage, we provide a detailed examination from the viewpoint of LLMs and multimodal LLMs (MLLMs), highlighting their mechanisms and limitations. Furthermore, we identify key challenges and future directions, paving the way for more inclusive and effective TF alignment techniques. By synthesizing and organizing the rapidly growing body of research, this survey offers a guidance for practitioners and advances the development of safer and more reliable LLMs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-as-a-Supervisor: Mistaken Therapeutic Behaviors Trigger Targeted Supervisory Feedback</title>
<link>https://arxiv.org/abs/2508.09042</link>
<guid>https://arxiv.org/abs/2508.09042</guid>
<content:encoded><![CDATA[
arXiv:2508.09042v1 Announce Type: new 
Abstract: Although large language models (LLMs) hold significant promise in psychotherapy, their direct application in patient-facing scenarios raises ethical and safety concerns. Therefore, this work shifts towards developing an LLM as a supervisor to train real therapists. In addition to the privacy of clinical therapist training data, a fundamental contradiction complicates the training of therapeutic behaviors: clear feedback standards are necessary to ensure a controlled training system, yet there is no absolute "gold standard" for appropriate therapeutic behaviors in practice. In contrast, many common therapeutic mistakes are universal and identifiable, making them effective triggers for targeted feedback that can serve as clearer evidence. Motivated by this, we create a novel therapist-training paradigm: (1) guidelines for mistaken behaviors and targeted correction strategies are first established as standards; (2) a human-in-the-loop dialogue-feedback dataset is then constructed, where a mistake-prone agent intentionally makes standard mistakes during interviews naturally, and a supervisor agent locates and identifies mistakes and provides targeted feedback; (3) after fine-tuning on this dataset, the final supervisor model is provided for real therapist training. The detailed experimental results of automated, human and downstream assessments demonstrate that models fine-tuned on our dataset MATE, can provide high-quality feedback according to the clinical guideline, showing significant potential for the therapist training scenario.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MVISU-Bench: Benchmarking Mobile Agents for Real-World Tasks by Multi-App, Vague, Interactive, Single-App and Unethical Instructions</title>
<link>https://arxiv.org/abs/2508.09057</link>
<guid>https://arxiv.org/abs/2508.09057</guid>
<content:encoded><![CDATA[
arXiv:2508.09057v1 Announce Type: new 
Abstract: Given the significant advances in Large Vision Language Models (LVLMs) in reasoning and visual understanding, mobile agents are rapidly emerging to meet users' automation needs. However, existing evaluation benchmarks are disconnected from the real world and fail to adequately address the diverse and complex requirements of users. From our extensive collection of user questionnaire, we identified five tasks: Multi-App, Vague, Interactive, Single-App, and Unethical Instructions. Around these tasks, we present \textbf{MVISU-Bench}, a bilingual benchmark that includes 404 tasks across 137 mobile applications. Furthermore, we propose Aider, a plug-and-play module that acts as a dynamic prompt prompter to mitigate risks and clarify user intent for mobile agents. Our Aider is easy to integrate into several frameworks and has successfully improved overall success rates by 19.55\% compared to the current state-of-the-art (SOTA) on MVISU-Bench. Specifically, it achieves success rate improvements of 53.52\% and 29.41\% for unethical and interactive instructions, respectively. Through extensive experiments and analysis, we highlight the gap between existing mobile agents and real-world user expectations.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>READER: Retrieval-Assisted Drafter for Efficient LLM Inference</title>
<link>https://arxiv.org/abs/2508.09072</link>
<guid>https://arxiv.org/abs/2508.09072</guid>
<content:encoded><![CDATA[
arXiv:2508.09072v1 Announce Type: new 
Abstract: Large Language Models (LLMs) generate tokens autoregressively, with each token depending on the preceding context. This sequential nature makes the inference process inherently difficult to accelerate, posing a significant challenge for efficient deployment. In recent years, various methods have been proposed to address this issue, with the most effective approaches often involving the training of additional draft models. In this paper, we introduce READER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel lossless speculative decoding method that enhances model-based approaches by leveraging self-repetitions in the text. Our algorithm expands the speculative decoding tree using tokens obtained through statistical search. This work focuses on large batch sizes (>= 8), an underexplored yet important area for industrial applications. We also analyze the key-value (KV) cache size during speculative decoding and propose an optimization to improve performance for large batches. As a result, READER outperforms existing speculative decoding methods. Notably, READER requires no additional training and can reuse pre-trained speculator models, increasing the speedup by over 40\%. Our method demonstrates particularly strong performance on search-based tasks, such as retrieval-augmented generation, where we achieve more than 10x speedup.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CPO: Addressing Reward Ambiguity in Role-playing Dialogue via Comparative Policy Optimization</title>
<link>https://arxiv.org/abs/2508.09074</link>
<guid>https://arxiv.org/abs/2508.09074</guid>
<content:encoded><![CDATA[
arXiv:2508.09074v1 Announce Type: new 
Abstract: Reinforcement Learning Fine-Tuning (RLFT) has achieved notable success in tasks with objectively verifiable answers (e.g., code generation, mathematical reasoning), yet struggles with open-ended subjective tasks like role-playing dialogue. Traditional reward modeling approaches, which rely on independent sample-wise scoring, face dual challenges: subjective evaluation criteria and unstable reward signals.Motivated by the insight that human evaluation inherently combines explicit criteria with implicit comparative judgments, we propose Comparative Policy Optimization (CPO). CPO redefines the reward evaluation paradigm by shifting from sample-wise scoring to comparative group-wise scoring.Building on the same principle, we introduce the CharacterArena evaluation framework, which comprises two stages:(1) Contextualized Multi-turn Role-playing Simulation, and (2) Trajectory-level Comparative Evaluation. By operationalizing subjective scoring via objective trajectory comparisons, CharacterArena minimizes contextual bias and enables more robust and fair performance evaluation. Empirical results on CharacterEval, CharacterBench, and CharacterArena confirm that CPO effectively mitigates reward ambiguity and leads to substantial improvements in dialogue quality.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilizing Multilingual Encoders to Improve Large Language Models for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2508.09091</link>
<guid>https://arxiv.org/abs/2508.09091</guid>
<content:encoded><![CDATA[
arXiv:2508.09091v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel in English, but their performance degrades significantly on low-resource languages (LRLs) due to English-centric training. While methods like LangBridge align LLMs with multilingual encoders such as the Massively Multilingual Text-to-Text Transfer Transformer (mT5), they typically use only the final encoder layer. We propose a novel architecture that fuses all intermediate layers, enriching the linguistic information passed to the LLM. Our approach features two strategies: (1) a Global Softmax weighting for overall layer importance, and (2) a Transformer Softmax model that learns token-specific weights. The fused representations are mapped into the LLM's embedding space, enabling it to process multilingual inputs. The model is trained only on English data, without using any parallel or multilingual data. Evaluated on XNLI, IndicXNLI, Sinhala News Classification, and Amazon Reviews, our Transformer Softmax model significantly outperforms the LangBridge baseline. We observe strong performance gains in LRLs, improving Sinhala classification accuracy from 71.66% to 75.86% and achieving clear improvements across Indic languages such as Tamil, Bengali, and Malayalam. These specific gains contribute to an overall boost in average XNLI accuracy from 70.36% to 71.50%. This approach offers a scalable, data-efficient path toward more capable and equitable multilingual LLMs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Link Prediction for Event Logs in the Process Industry</title>
<link>https://arxiv.org/abs/2508.09096</link>
<guid>https://arxiv.org/abs/2508.09096</guid>
<content:encoded><![CDATA[
arXiv:2508.09096v1 Announce Type: new 
Abstract: Knowledge management (KM) is vital in the process industry for optimizing operations, ensuring safety, and enabling continuous improvement through effective use of operational data and past insights. A key challenge in this domain is the fragmented nature of event logs in shift books, where related records, e.g., entries documenting issues related to equipment or processes and the corresponding solutions, may remain disconnected. This fragmentation hinders the recommendation of previous solutions to the users. To address this problem, we investigate record linking (RL) as link prediction, commonly studied in graph-based machine learning, by framing it as a cross-document coreference resolution (CDCR) task enhanced with natural language inference (NLI) and semantic text similarity (STS) by shifting it into the causal inference (CI). We adapt CDCR, traditionally applied in the news domain, into an RL model to operate at the passage level, similar to NLI and STS, while accommodating the process industry's specific text formats, which contain unstructured text and structured record attributes. Our RL model outperformed the best versions of NLI- and STS-driven baselines by 28% (11.43 points) and 27% (11.21 points), respectively. Our work demonstrates how domain adaptation of the state-of-the-art CDCR models, enhanced with reasoning capabilities, can be effectively tailored to the process industry, improving data quality and connectivity in shift logs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators</title>
<link>https://arxiv.org/abs/2508.09101</link>
<guid>https://arxiv.org/abs/2508.09101</guid>
<content:encoded><![CDATA[
arXiv:2508.09101v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, with code generation emerging as a key area of focus. While numerous benchmarks have been proposed to evaluate their code generation abilities, these benchmarks face several critical limitations. First, they often rely on manual annotations, which are time-consuming and difficult to scale across different programming languages and problem complexities. Second, most existing benchmarks focus primarily on Python, while the few multilingual benchmarks suffer from limited difficulty and uneven language distribution. To address these challenges, we propose AutoCodeGen, an automated method for generating high-difficulty multilingual code generation datasets without manual annotations. AutoCodeGen ensures the correctness and completeness of test cases by generating test inputs with LLMs and obtaining test outputs through a multilingual sandbox, while achieving high data quality through reverse-order problem generation and multiple filtering steps. Using this novel method, we introduce AutoCodeBench, a large-scale code generation benchmark comprising 3,920 problems evenly distributed across 20 programming languages. It is specifically designed to evaluate LLMs on challenging, diverse, and practical multilingual tasks. We evaluate over 30 leading open-source and proprietary LLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The results show that even the most advanced LLMs struggle with the complexity, diversity, and multilingual nature of these tasks. Besides, we introduce AutoCodeBench-Complete, specifically designed for base models to assess their few-shot code generation capabilities. We hope the AutoCodeBench series will serve as a valuable resource and inspire the community to focus on more challenging and practical multilingual code generation scenarios.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SinLlama - A Large Language Model for Sinhala</title>
<link>https://arxiv.org/abs/2508.09115</link>
<guid>https://arxiv.org/abs/2508.09115</guid>
<content:encoded><![CDATA[
arXiv:2508.09115v1 Announce Type: new 
Abstract: Low-resource languages such as Sinhala are often overlooked by open-source Large Language Models (LLMs). In this research, we extend an existing multilingual LLM (Llama-3-8B) to better serve Sinhala. We enhance the LLM tokenizer with Sinhala specific vocabulary and perform continual pre-training on a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This is the very first decoder-based open-source LLM with explicit Sinhala support. When SinLlama was instruction fine-tuned for three text classification tasks, it outperformed base and instruct variants of Llama-3-8B by a significant margin.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office Application Workflows</title>
<link>https://arxiv.org/abs/2508.09124</link>
<guid>https://arxiv.org/abs/2508.09124</guid>
<content:encoded><![CDATA[
arXiv:2508.09124v1 Announce Type: new 
Abstract: Autonomous agents powered by large language models (LLMs) are increasingly deployed in real-world applications requiring complex, long-horizon workflows. However, existing benchmarks predominantly focus on atomic tasks that are self-contained and independent, failing to capture the long-term contextual dependencies and multi-interaction coordination required in realistic scenarios. To address this gap, we introduce OdysseyBench, a comprehensive benchmark for evaluating LLM agents on long-horizon workflows across diverse office applications including Word, Excel, PDF, Email, and Calendar. Our benchmark comprises two complementary splits: OdysseyBench+ with 300 tasks derived from real-world use cases, and OdysseyBench-Neo with 302 newly synthesized complex tasks. Each task requires agent to identify essential information from long-horizon interaction histories and perform multi-step reasoning across various applications. To enable scalable benchmark creation, we propose HomerAgents, a multi-agent framework that automates the generation of long-horizon workflow benchmarks through systematic environment exploration, task generation, and dialogue synthesis. Our extensive evaluation demonstrates that OdysseyBench effectively challenges state-of-the-art LLM agents, providing more accurate assessment of their capabilities in complex, real-world contexts compared to existing atomic task benchmarks. We believe that OdysseyBench will serve as a valuable resource for advancing the development and evaluation of LLM agents in real-world productivity scenarios. In addition, we release OdysseyBench and HomerAgents to foster research along this line.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complex Logical Instruction Generation</title>
<link>https://arxiv.org/abs/2508.09125</link>
<guid>https://arxiv.org/abs/2508.09125</guid>
<content:encoded><![CDATA[
arXiv:2508.09125v1 Announce Type: new 
Abstract: Instruction following has catalyzed the recent era of Large Language Models (LLMs) and is the foundational skill underpinning more advanced capabilities such as reasoning and agentic behaviors. As tasks grow more challenging, the logic structures embedded in natural language instructions becomes increasingly intricate. However, how well LLMs perform on such logic-rich instructions remains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a scalable, automated framework for generating verifiable instructions from code functions, which can naturally express rich logic such as conditionals, nesting, recursion, and function calls. We further curate a collection of complex code functions and use LogicIFGen to construct LogicIFEval, a benchmark comprising 426 verifiable logic-rich instructions. Our experiments demonstrate that current state-of-the-art LLMs still struggle to correctly follow the instructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the instructions, revealing significant deficiencies in the instruction-following ability. Code and Benchmark: https://github.com/mianzhang/LogicIF
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models</title>
<link>https://arxiv.org/abs/2508.09138</link>
<guid>https://arxiv.org/abs/2508.09138</guid>
<content:encoded><![CDATA[
arXiv:2508.09138v1 Announce Type: new 
Abstract: Diffusion large language models (dLLMs) generate text through iterative denoising, yet current decoding strategies discard rich intermediate predictions in favor of the final output. Our work here reveals a critical phenomenon, temporal oscillation, where correct answers often emerge in the middle process, but are overwritten in later denoising steps. To address this issue, we introduce two complementary methods that exploit temporal consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output; and 2) a post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a measure of semantic stability across intermediate predictions, as a reward signal to encourage stable generations. Empirical results across multiple benchmarks demonstrate the effectiveness of our approach. Using the negative TSE reward alone, we observe a remarkable average improvement of 24.7% on the Countdown dataset over an existing dLLM. Combined with the accuracy reward, we achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown, respectively. Our findings underscore the untapped potential of temporal dynamics in dLLMs and offer two simple yet effective tools to harness them.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Large Language Models for Geolocating Colonial Virginia Land Grants</title>
<link>https://arxiv.org/abs/2508.08266</link>
<guid>https://arxiv.org/abs/2508.08266</guid>
<content:encoded><![CDATA[
arXiv:2508.08266v1 Announce Type: cross 
Abstract: Virginia's seventeenth- and eighteenth-century land patents survive primarily as narrative metes-and-bounds descriptions, limiting spatial analysis. This study systematically evaluates current-generation large language models (LLMs) in converting these prose abstracts into geographically accurate latitude/longitude coordinates within a focused evaluation context. A digitized corpus of 5,471 Virginia patent abstracts (1695-1732) is released, with 43 rigorously verified test cases serving as an initial, geographically focused benchmark. Six OpenAI models across three architectures (o-series, GPT-4-class, and GPT-3.5) were tested under two paradigms: direct-to-coordinate and tool-augmented chain-of-thought invoking external geocoding APIs. Results were compared with a GIS-analyst baseline, the Stanford NER geoparser, Mordecai-3, and a county-centroid heuristic.
  The top single-call model, o3-2025-04-16, achieved a mean error of 23 km (median 14 km), outperforming the median LLM (37.4 km) by 37.5%, the weakest LLM (50.3 km) by 53.5%, and external baselines by 67% (GIS analyst) and 70% (Stanford NER). A five-call ensemble further reduced errors to 19 km (median 12 km) at minimal additional cost (approx. USD 0.20 per grant), outperforming the median LLM by 48.6%. A patentee-name-redaction ablation increased error by about 9%, indicating reliance on textual landmark and adjacency descriptions rather than memorization. The cost-efficient gpt-4o-2024-08-06 model maintained a 28 km mean error at USD 1.09 per 1,000 grants, establishing a strong cost-accuracy benchmark; external geocoding tools offered no measurable benefit in this evaluation.
  These findings demonstrate the potential of LLMs for scalable, accurate, and cost-effective historical georeferencing.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doctor Sun: A Bilingual Multimodal Large Language Model for Biomedical AI</title>
<link>https://arxiv.org/abs/2508.08270</link>
<guid>https://arxiv.org/abs/2508.08270</guid>
<content:encoded><![CDATA[
arXiv:2508.08270v1 Announce Type: cross 
Abstract: Large multimodal models (LMMs) have demonstrated significant potential in providing innovative solutions for various biomedical tasks, including pathology analysis, radiology report generation, and biomedical assistance. However, the existing multimodal biomedical AI is typically based on foundation LLMs, thus hindering the understanding of intricate medical concepts with limited medical training data. Moreover, recent LLaVA-induced medical LMMs struggle to effectively capture the intricate relationship between the texts and the images. Therefore, we introduce Doctor Sun, a large multimodal generative model specialized in medicine, developed to encode, integrate, and interpret diverse biomedical data modalities such as text and images. In particular, Doctor Sun integrates a pre-trained vision encoder with a medical LLM and conducts two-stage training on various medical datasets, focusing on feature alignment and instruction tuning. Moreover, we release SunMed-VL, a wide-range bilingual medical multimodal dataset, along with all associated models, code, and resources, to freely support the advancement of biomedical multimodal research.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical Approach for Multi-Tenant LLM Serving</title>
<link>https://arxiv.org/abs/2508.08343</link>
<guid>https://arxiv.org/abs/2508.08343</guid>
<content:encoded><![CDATA[
arXiv:2508.08343v1 Announce Type: cross 
Abstract: Serving LLM adapters has gained significant attention as an effective approach to adapt general-purpose language models to diverse, task-specific use cases. However, serving a wide range of adapters introduces several and substantial overheads, leading to performance degradation and challenges in optimal placement. To address these challenges, we present an analytical, AI-driven pipeline that accurately determines the optimal allocation of adapters in single-node setups. This allocation maximizes performance, effectively using GPU resources, while preventing request starvation. Crucially, the proposed allocation is given based on current workload patterns. These insights in single-node setups can be leveraged in multi-replica deployments for overall placement, load balancing and server configuration, ultimately enhancing overall performance and improving resource efficiency. Our approach builds on an in-depth analysis of LLM adapter serving, accounting for overheads and performance variability, and includes the development of the first Digital Twin capable of replicating online LLM-adapter serving systems with matching key performance metrics. The experimental results demonstrate that the Digital Twin achieves a SMAPE difference of no more than 5.5% in throughput compared to real results, and the proposed pipeline accurately predicts the optimal placement with minimal latency.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Technical Knowledge Interaction of Global Digital Humanities: Three-decade Evidence from Bibliometric-based perspectives</title>
<link>https://arxiv.org/abs/2508.08347</link>
<guid>https://arxiv.org/abs/2508.08347</guid>
<content:encoded><![CDATA[
arXiv:2508.08347v1 Announce Type: cross 
Abstract: Digital Humanities (DH) is an interdisciplinary field that integrates computational methods with humanities scholarship to investigate innovative topics. Each academic discipline follows a unique developmental path shaped by the topics researchers investigate and the methods they employ. With the help of bibliometric analysis, most of previous studies have examined DH across multiple dimensions such as research hotspots, co-author networks, and institutional rankings. However, these studies have often been limited in their ability to provide deep insights into the current state of technological advancements and topic development in DH. As a result, their conclusions tend to remain superficial or lack interpretability in understanding how methods and topics interrelate in the field. To address this gap, this study introduced a new concept of Topic-Method Composition (TMC), which refers to a hybrid knowledge structure generated by the co-occurrence of specific research topics and the corresponding method. Especially by analyzing the interaction between TMCs, we can see more clearly the intersection and integration of digital technology and humanistic subjects in DH. Moreover, this study developed a TMC-based workflow combining bibliometric analysis, topic modeling, and network analysis to analyze the development characteristics and patterns of research disciplines. By applying this workflow to large-scale bibliometric data, it enables a detailed view of the knowledge structures, providing a tool adaptable to other fields.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bilevel MCTS for Amortized O(1) Node Selection in Classical Planning</title>
<link>https://arxiv.org/abs/2508.08385</link>
<guid>https://arxiv.org/abs/2508.08385</guid>
<content:encoded><![CDATA[
arXiv:2508.08385v1 Announce Type: cross 
Abstract: We study an efficient implementation of Multi-Armed Bandit (MAB)-based Monte-Carlo Tree Search (MCTS) for classical planning. One weakness of MCTS is that it spends a significant time deciding which node to expand next. While selecting a node from an OPEN list with $N$ nodes has $O(1)$ runtime complexity with traditional array-based priority-queues for dense integer keys, the tree-based OPEN list used by MCTS requires $O(\log N)$, which roughly corresponds to the search depth $d$. In classical planning, $d$ is arbitrarily large (e.g., $2^k-1$ in $k$-disk Tower-of-Hanoi) and the runtime for node selection is significant, unlike in game tree search, where the cost is negligible compared to the node evaluation (rollouts) because $d$ is inherently limited by the game (e.g., $d\leq 361$ in Go). To improve this bottleneck, we propose a bilevel modification to MCTS that runs a best-first search from each selected leaf node with an expansion budget proportional to $d$, which achieves amortized $O(1)$ runtime for node selection, equivalent to the traditional queue-based OPEN list. In addition, we introduce Tree Collapsing, an enhancement that reduces action selection steps and further improves the performance.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re:Verse -- Can Your VLM Read a Manga?</title>
<link>https://arxiv.org/abs/2508.08508</link>
<guid>https://arxiv.org/abs/2508.08508</guid>
<content:encoded><![CDATA[
arXiv:2508.08508v1 Announce Type: cross 
Abstract: Current Vision Language Models (VLMs) demonstrate a critical gap between surface-level recognition and deep narrative reasoning when processing sequential visual storytelling. Through a comprehensive investigation of manga narrative understanding, we reveal that while recent large multimodal models excel at individual panel interpretation, they systematically fail at temporal causality and cross-panel cohesion, core requirements for coherent story comprehension. We introduce a novel evaluation framework that combines fine-grained multimodal annotation, cross-modal embedding analysis, and retrieval-augmented assessment to systematically characterize these limitations.
  Our methodology includes (i) a rigorous annotation protocol linking visual elements to narrative structure through aligned light novel text, (ii) comprehensive evaluation across multiple reasoning paradigms, including direct inference and retrieval-augmented generation, and (iii) cross-modal similarity analysis revealing fundamental misalignments in current VLMs' joint representations. Applying this framework to Re:Zero manga across 11 chapters with 308 annotated panels, we conduct the first systematic study of long-form narrative understanding in VLMs through three core evaluation axes: generative storytelling, contextual dialogue grounding, and temporal reasoning. Our findings demonstrate that current models lack genuine story-level intelligence, struggling particularly with non-linear narratives, character consistency, and causal inference across extended sequences. This work establishes both the foundation and practical methodology for evaluating narrative intelligence, while providing actionable insights into the capability of deep sequential understanding of Discrete Visual Narratives beyond basic recognition in Multimodal Models.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-grained Video Dubbing Duration Alignment with Segment Supervised Preference Optimization</title>
<link>https://arxiv.org/abs/2508.08550</link>
<guid>https://arxiv.org/abs/2508.08550</guid>
<content:encoded><![CDATA[
arXiv:2508.08550v1 Announce Type: cross 
Abstract: Video dubbing aims to translate original speech in visual media programs from the source language to the target language, relying on neural machine translation and text-to-speech technologies. Due to varying information densities across languages, target speech often mismatches the source speech duration, causing audio-video synchronization issues that significantly impact viewer experience. In this study, we approach duration alignment in LLM-based video dubbing machine translation as a preference optimization problem. We propose the Segment Supervised Preference Optimization (SSPO) method, which employs a segment-wise sampling strategy and fine-grained loss to mitigate duration mismatches between source and target lines. Experimental results demonstrate that SSPO achieves superior performance in duration alignment tasks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Personalized Conversational Information Retrieval</title>
<link>https://arxiv.org/abs/2508.08634</link>
<guid>https://arxiv.org/abs/2508.08634</guid>
<content:encoded><![CDATA[
arXiv:2508.08634v1 Announce Type: cross 
Abstract: Personalized conversational information retrieval (CIR) systems aim to satisfy users' complex information needs through multi-turn interactions by considering user profiles. However, not all search queries require personalization. The challenge lies in appropriately incorporating personalization elements into search when needed. Most existing studies implicitly incorporate users' personal information and conversational context using large language models without distinguishing the specific requirements for each query turn. Such a ``one-size-fits-all'' personalization strategy might lead to sub-optimal results. In this paper, we propose an adaptive personalization method, in which we first identify the required personalization level for a query and integrate personalized queries with other query reformulations to produce various enhanced queries. Then, we design a personalization-aware ranking fusion approach to assign fusion weights dynamically to different reformulated queries, depending on the required personalization level. The proposed adaptive personalized conversational information retrieval framework APCIR is evaluated on two TREC iKAT datasets. The results confirm the effectiveness of adaptive personalization of APCIR by outperforming state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MiGrATe: Mixed-Policy GRPO for Adaptation at Test-Time</title>
<link>https://arxiv.org/abs/2508.08641</link>
<guid>https://arxiv.org/abs/2508.08641</guid>
<content:encoded><![CDATA[
arXiv:2508.08641v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly being applied to black-box optimization tasks, from program synthesis to molecule design. Prior work typically leverages in-context learning to iteratively guide the model towards better solutions. Such methods, however, often struggle to balance exploration of new solution spaces with exploitation of high-reward ones. Recently, test-time training (TTT) with synthetic data has shown promise in improving solution quality. However, the need for hand-crafted training data tailored to each task limits feasibility and scalability across domains. To address this problem, we introduce MiGrATe-a method for online TTT that uses GRPO as a search algorithm to adapt LLMs at inference without requiring external training data. MiGrATe operates via a mixed-policy group construction procedure that combines on-policy sampling with two off-policy data selection techniques: greedy sampling, which selects top-performing past completions, and neighborhood sampling (NS), which generates completions structurally similar to high-reward ones. Together, these components bias the policy gradient towards exploitation of promising regions in solution space, while preserving exploration through on-policy sampling. We evaluate MiGrATe on three challenging domains-word search, molecule optimization, and hypothesis+program induction on the Abstraction and Reasoning Corpus (ARC)-and find that it consistently outperforms both inference-only and TTT baselines, demonstrating the potential of online TTT as a solution for complex search tasks without external supervision.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\text{M}^{2}$LLM: Multi-view Molecular Representation Learning with Large Language Models</title>
<link>https://arxiv.org/abs/2508.08657</link>
<guid>https://arxiv.org/abs/2508.08657</guid>
<content:encoded><![CDATA[
arXiv:2508.08657v1 Announce Type: cross 
Abstract: Accurate molecular property prediction is a critical challenge with wide-ranging applications in chemistry, materials science, and drug discovery. Molecular representation methods, including fingerprints and graph neural networks (GNNs), achieve state-of-the-art results by effectively deriving features from molecular structures. However, these methods often overlook decades of accumulated semantic and contextual knowledge. Recent advancements in large language models (LLMs) demonstrate remarkable reasoning abilities and prior knowledge across scientific domains, leading us to hypothesize that LLMs can generate rich molecular representations when guided to reason in multiple perspectives. To address these gaps, we propose $\text{M}^{2}$LLM, a multi-view framework that integrates three perspectives: the molecular structure view, the molecular task view, and the molecular rules view. These views are fused dynamically to adapt to task requirements, and experiments demonstrate that $\text{M}^{2}$LLM achieves state-of-the-art performance on multiple benchmarks across classification and regression tasks. Moreover, we demonstrate that representation derived from LLM achieves exceptional performance by leveraging two core functionalities: the generation of molecular embeddings through their encoding capabilities and the curation of molecular features through advanced reasoning processes.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation Tutor with LLMs</title>
<link>https://arxiv.org/abs/2508.08715</link>
<guid>https://arxiv.org/abs/2508.08715</guid>
<content:encoded><![CDATA[
arXiv:2508.08715v1 Announce Type: cross 
Abstract: Generative speech models have demonstrated significant potential in personalizing teacher-student interactions, offering valuable real-world applications for language learning in children's education. However, achieving high-quality, child-friendly speech generation remains challenging, particularly for low-resource languages across diverse languages and cultural contexts. In this paper, we propose MultiAiTutor, an educational multilingual generative AI tutor with child-friendly designs, leveraging LLM architecture for speech generation tailored for educational purposes. We propose to integrate age-appropriate multilingual speech generation using LLM architectures, facilitating young children's language learning through culturally relevant image-description tasks in three low-resource languages: Singaporean-accent Mandarin, Malay, and Tamil. Experimental results from both objective metrics and subjective evaluations demonstrate the superior performance of the proposed MultiAiTutor compared to baseline methods.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Memory-Augmented AR Agents for Spatiotemporal Reasoning in Personalized Task Assistance</title>
<link>https://arxiv.org/abs/2508.08774</link>
<guid>https://arxiv.org/abs/2508.08774</guid>
<content:encoded><![CDATA[
arXiv:2508.08774v1 Announce Type: cross 
Abstract: Augmented Reality (AR) systems are increasingly integrating foundation models, such as Multimodal Large Language Models (MLLMs), to provide more context-aware and adaptive user experiences. This integration has led to the development of AR agents to support intelligent, goal-directed interactions in real-world environments. While current AR agents effectively support immediate tasks, they struggle with complex multi-step scenarios that require understanding and leveraging user's long-term experiences and preferences. This limitation stems from their inability to capture, retain, and reason over historical user interactions in spatiotemporal contexts. To address these challenges, we propose a conceptual framework for memory-augmented AR agents that can provide personalized task assistance by learning from and adapting to user-specific experiences over time. Our framework consists of four interconnected modules: (1) Perception Module for multimodal sensor processing, (2) Memory Module for persistent spatiotemporal experience storage, (3) Spatiotemporal Reasoning Module for synthesizing past and present contexts, and (4) Actuator Module for effective AR communication. We further present an implementation roadmap, a future evaluation strategy, a potential target application and use cases to demonstrate the practical applicability of our framework across diverse domains. We aim for this work to motivate future research toward developing more intelligent AR systems that can effectively bridge user's interaction history with adaptive, context-aware task assistance.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to Functions</title>
<link>https://arxiv.org/abs/2508.08795</link>
<guid>https://arxiv.org/abs/2508.08795</guid>
<content:encoded><![CDATA[
arXiv:2508.08795v1 Announce Type: cross 
Abstract: Large language models (LLMs) acquire vast knowledge from large text corpora, but this information can become outdated or inaccurate. Since retraining is computationally expensive, knowledge editing offers an efficient alternative -- modifying internal knowledge without full retraining. These methods aim to update facts precisely while preserving the model's overall capabilities. While existing surveys focus on the mechanism of editing (e.g., parameter changes vs. external memory), they often overlook the function of the knowledge being edited. This survey introduces a novel, complementary function-based taxonomy to provide a more holistic view. We examine how different mechanisms apply to various knowledge types -- factual, temporal, conceptual, commonsense, and social -- highlighting how editing effectiveness depends on the nature of the target knowledge. By organizing our review along these two axes, we map the current landscape, outline the strengths and limitations of existing methods, define the problem formally, survey evaluation tasks and datasets, and conclude with open challenges and future directions.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing the Role of Audio Channels in ASR Performance Degradation</title>
<link>https://arxiv.org/abs/2508.08967</link>
<guid>https://arxiv.org/abs/2508.08967</guid>
<content:encoded><![CDATA[
arXiv:2508.08967v1 Announce Type: cross 
Abstract: Pre-trained automatic speech recognition (ASR) models have demonstrated strong performance on a variety of tasks. However, their performance can degrade substantially when the input audio comes from different recording channels. While previous studies have demonstrated this phenomenon, it is often attributed to the mismatch between training and testing corpora. This study argues that variations in speech characteristics caused by different recording channels can fundamentally harm ASR performance. To address this limitation, we propose a normalization technique designed to mitigate the impact of channel variation by aligning internal feature representations in the ASR model with those derived from a clean reference channel. This approach significantly improves ASR performance on previously unseen channels and languages, highlighting its ability to generalize across channel and language differences.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and Efficiency</title>
<link>https://arxiv.org/abs/2508.09023</link>
<guid>https://arxiv.org/abs/2508.09023</guid>
<content:encoded><![CDATA[
arXiv:2508.09023v1 Announce Type: cross 
Abstract: SQL query rewriting aims to reformulate a query into a more efficient form while preserving equivalence. Most existing methods rely on predefined rewrite rules. However, such rule-based approaches face fundamental limitations: (1) fixed rule sets generalize poorly to novel query patterns and struggle with complex queries; (2) a wide range of effective rewriting strategies cannot be fully captured by declarative rules. To overcome these issues, we propose using large language models (LLMs) to generate rewrites. LLMs can capture complex strategies, such as evaluation reordering and CTE rewriting. Despite this potential, directly applying LLMs often results in suboptimal or non-equivalent rewrites due to a lack of execution awareness and semantic grounding. To address these challenges, We present E3-Rewrite, an LLM-based SQL rewriting framework that produces executable, equivalent, and efficient queries. It integrates two core components: a context construction module and a reinforcement learning framework. First, the context module leverages execution plans and retrieved demonstrations to build bottleneck-aware prompts that guide inference-time rewriting. Second, we design a reward function targeting executability, equivalence, and efficiency, evaluated via syntax checks, equivalence verification, and cost estimation. Third, to ensure stable multi-objective learning, we adopt a staged curriculum that first emphasizes executability and equivalence, then gradually incorporates efficiency. Extensive experiments show that E3-Rewrite achieves up to a 25.6\% reduction in query execution time compared to state-of-the-art methods across multiple SQL benchmarks. Moreover, it delivers up to 24.4\% more successful rewrites, expanding coverage to complex queries that previous systems failed to handle.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P/D-Device: Disaggregated Large Language Model between Cloud and Devices</title>
<link>https://arxiv.org/abs/2508.09035</link>
<guid>https://arxiv.org/abs/2508.09035</guid>
<content:encoded><![CDATA[
arXiv:2508.09035v1 Announce Type: cross 
Abstract: Serving disaggregated large language models has been widely adopted in industrial practice for enhanced performance. However, too many tokens generated in decoding phase, i.e., occupying the resources for a long time, essentially hamper the cloud from achieving a higher throughput. Meanwhile, due to limited on-device resources, the time to first token (TTFT), i.e., the latency of prefill phase, increases dramatically with the growth on prompt length. In order to concur with such a bottleneck on resources, i.e., long occupation in cloud and limited on-device computing capacity, we propose to separate large language model between cloud and devices. That is, the cloud helps a portion of the content for each device, only in its prefill phase. Specifically, after receiving the first token from the cloud, decoupling with its own prefill, the device responds to the user immediately for a lower TTFT. Then, the following tokens from cloud are presented via a speed controller for smoothed TPOT (the time per output token), until the device catches up with the progress. On-device prefill is then amortized using received tokens while the resource usage in cloud is controlled. Moreover, during cloud prefill, the prompt can be refined, using those intermediate data already generated, to further speed up on-device inference. We implement such a scheme P/D-Device, and confirm its superiority over other alternatives. We further propose an algorithm to decide the best settings. Real-trace experiments show that TTFT decreases at least 60%, maximum TPOT is about tens of milliseconds, and cloud throughput increases by up to 15x.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Gender Biases Towards Politicians on Reddit</title>
<link>https://arxiv.org/abs/2112.12014</link>
<guid>https://arxiv.org/abs/2112.12014</guid>
<content:encoded><![CDATA[
arXiv:2112.12014v3 Announce Type: replace 
Abstract: Despite attempts to increase gender parity in politics, global efforts have struggled to ensure equal female representation. This is likely tied to implicit gender biases against women in authority. In this work, we present a comprehensive study of gender biases that appear in online political discussion. To this end, we collect 10 million comments on Reddit in conversations about male and female politicians, which enables an exhaustive study of automatic gender bias detection. We address not only misogynistic language, but also other manifestations of bias, like benevolent sexism in the form of seemingly positive sentiment and dominance attributed to female politicians, or differences in descriptor attribution. Finally, we conduct a multi-faceted study of gender bias towards politicians investigating both linguistic and extra-linguistic cues. We assess 5 different types of gender bias, evaluating coverage, combinatorial, nominal, sentimental, and lexical biases extant in social media language and discourse. Overall, we find that, contrary to previous research, coverage and sentiment biases suggest equal public interest in female politicians. Rather than overt hostile or benevolent sexism, the results of the nominal and lexical analyses suggest this interest is not as professional or respectful as that expressed about male politicians. Female politicians are often named by their first names and are described in relation to their body, clothing, or family; this is a treatment that is not similarly extended to men. On the now banned far-right subreddits, this disparity is greatest, though differences in gender biases still appear in the right and left-leaning subreddits. We release the curated dataset to the public for future studies.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilizing Large Language Models for Information Extraction from Real Estate Transactions</title>
<link>https://arxiv.org/abs/2404.18043</link>
<guid>https://arxiv.org/abs/2404.18043</guid>
<content:encoded><![CDATA[
arXiv:2404.18043v3 Announce Type: replace 
Abstract: Real estate sales contracts contain crucial information for property transactions, but manual data extraction can be time-consuming and error-prone. This paper explores the application of large language models, specifically transformer-based architectures, for automated information extraction from real estate contracts. We discuss challenges, techniques, and future directions in leveraging these models to improve efficiency and accuracy in real estate contract analysis. We generated synthetic contracts using the real-world transaction dataset, thereby fine-tuning the large-language model and achieving significant metrics improvements and qualitative improvements in information retrieval and reasoning tasks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2410.06795</link>
<guid>https://arxiv.org/abs/2410.06795</guid>
<content:encoded><![CDATA[
arXiv:2410.06795v2 Announce Type: replace 
Abstract: Hallucinations in large vision-language models (LVLMs) are a significant challenge, i.e., generating objects that are not presented in the visual input, which impairs their reliability. Recent studies often attribute hallucinations to a lack of understanding of visual input, yet ignore a more fundamental issue: the model's inability to effectively extract or decouple visual features. In this paper, we revisit the hallucinations in LVLMs from an architectural perspective, investigating whether the primary cause lies in the visual encoder (feature extraction) or the modal alignment module (feature decoupling). Motivated by our findings on the preliminary investigation, we propose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs. This plug-and-play method can be integrated into various LVLMs, utilizing adaptive virtual tokens to extract object features from bounding boxes, thereby addressing hallucinations caused by insufficient decoupling of visual features. PATCH achieves state-of-the-art performance on multiple multi-modal hallucination datasets. We hope this approach provides researchers with deeper insights into the underlying causes of hallucinations in LVLMs, fostering further advancements and innovation in this field.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models</title>
<link>https://arxiv.org/abs/2501.13983</link>
<guid>https://arxiv.org/abs/2501.13983</guid>
<content:encoded><![CDATA[
arXiv:2501.13983v5 Announce Type: replace 
Abstract: As Large Language Models (LLMs) are pre-trained on ultra-large-scale corpora, the problem of data contamination is becoming increasingly serious, and there is a risk that static evaluation benchmarks overestimate the performance of LLMs. To address this, this paper proposes a dynamic data evaluation method called AdEval (Alignment-based Dynamic Evaluation). AdEval first extracts knowledge points and main ideas from static datasets to achieve dynamic alignment with the core content of static benchmarks, and by avoiding direct reliance on static datasets, it inherently reduces the risk of data contamination from the source. It then obtains background information through online searches to generate detailed descriptions of the knowledge points. Finally, it designs questions based on Bloom's cognitive hierarchy across six dimensions-remembering, understanding, applying, analyzing, evaluating, and creating to enable multi-level cognitive assessment. Additionally, AdEval controls the complexity of dynamically generated datasets through iterative question reconstruction. Experimental results on multiple datasets show that AdEval effectively alleviates the impact of data contamination on evaluation results, solves the problems of insufficient complexity control and single-dimensional evaluation, and improves the fairness, reliability and diversity of LLMs evaluation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoP: Robust LLM Inference via Evolutionary Pruning</title>
<link>https://arxiv.org/abs/2502.14910</link>
<guid>https://arxiv.org/abs/2502.14910</guid>
<content:encoded><![CDATA[
arXiv:2502.14910v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved remarkable success in natural language processing tasks, but their massive size and computational demands hinder their deployment in resource-constrained environments. Existing model pruning methods address this issue by removing redundant structures (e.g., elements, channels, layers) from the model. However, these methods employ a heuristic pruning strategy, which leads to suboptimal performance. Besides, they also ignore the data characteristics when pruning the model.
  To overcome these limitations, we propose EvoP, an evolutionary pruning framework for robust LLM inference. EvoP first presents a cluster-based calibration dataset sampling (CCDS) strategy for creating a more diverse calibration dataset. EvoP then introduces an evolutionary pruning pattern searching (EPPS) method to find the optimal pruning pattern. Compared to existing model pruning techniques, EvoP achieves the best performance while maintaining the best efficiency. Experiments across different LLMs and different downstream tasks validate the effectiveness of the proposed EvoP, making it a practical and scalable solution for deploying LLMs in real-world applications.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Linguistic Calibration: Trading-off between Factuality and Specificity</title>
<link>https://arxiv.org/abs/2502.19110</link>
<guid>https://arxiv.org/abs/2502.19110</guid>
<content:encoded><![CDATA[
arXiv:2502.19110v4 Announce Type: replace 
Abstract: Language model outputs are not always reliable, thus prompting research into how to adapt model responses based on uncertainty. Common approaches include: \emph{abstention}, where models refrain from generating responses when uncertain; and \emph{linguistic calibration}, where models hedge their statements using uncertainty quantifiers. However, abstention can withhold valuable information, while linguistically calibrated responses are often challenging to leverage in downstream tasks. We propose a unified view, Conformal Linguistic Calibration (CLC), which reinterprets linguistic calibration as \emph{answer set prediction}. First we present a framework connecting abstention and linguistic calibration through the lens of linguistic pragmatics. We then describe an implementation of CLC that allows for controlling the level of imprecision in model responses. Results demonstrate our method produces calibrated outputs with conformal guarantees on factual accuracy. Further, our approach enables fine-tuning models to perform uncertainty-aware adaptive claim rewriting, offering a controllable balance between factuality and specificity.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models for Automated Clinical Abstraction in Pulmonary Embolism Registries: Performance Across Model Sizes, Versions, and Parameters</title>
<link>https://arxiv.org/abs/2503.21004</link>
<guid>https://arxiv.org/abs/2503.21004</guid>
<content:encoded><![CDATA[
arXiv:2503.21004v3 Announce Type: replace 
Abstract: Pulmonary embolism (PE) registries accelerate practice-improving research but depend on resource-intensive manual abstraction of radiology reports. We evaluated whether openly available large-language models (LLMs) can automate concept extraction from computed-tomography PE (CTPE) reports without sacrificing data quality. Four Llama-3 (L3) variants (3.0 8 B, 3.1 8 B, 3.1 70 B, 3.3 70 B) and two reviewer models Phi-4 (P4) 14 B and Gemma-3 27 B (G3) were tested on 250 dual-annotated CTPE reports each from MIMIC-IV and Duke University. Outcomes were accuracy, positive predictive value (PPV), and negative predictive value (NPV) versus a human gold standard across model sizes, temperature settings, and shot counts. Mean accuracy across all concepts increased with scale: 0.83 (L3-0 8 B), 0.91 (L3-1 8 B), and 0.96 for both 70 B variants; P4 14 B achieved 0.98; G3 matched. Accuracy differed by < 0.03 between datasets, underscoring external robustness. In dual-model concordance analysis (L3 70 B + P4 14 B), PE-presence PPV was >= 0.95 and NPV >= 0.98, while location, thrombus burden, right-heart strain, and image-quality artifacts each maintained PPV >= 0.90 and NPV >= 0.95. Fewer than 4% of individual concept annotations were discordant, and complete agreement was observed in more than 75% of reports. G3 performed comparably. LLMs therefore offer a scalable, accurate solution for PE registry abstraction, and a dual-model review workflow can further safeguard data quality with minimal human oversight.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opioid Named Entity Recognition (ONER-2025) from Reddit</title>
<link>https://arxiv.org/abs/2504.00027</link>
<guid>https://arxiv.org/abs/2504.00027</guid>
<content:encoded><![CDATA[
arXiv:2504.00027v4 Announce Type: replace 
Abstract: The opioid overdose epidemic remains a critical public health crisis, particularly in the United States, leading to significant mortality and societal costs. Social media platforms like Reddit provide vast amounts of unstructured data that offer insights into public perceptions, discussions, and experiences related to opioid use. This study leverages Natural Language Processing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to extract actionable information from these platforms. Our research makes four key contributions. First, we created a unique, manually annotated dataset sourced from Reddit, where users share self-reported experiences of opioid use via different administration routes. This dataset contains 331,285 tokens and includes eight major opioid entity categories. Second, we detail our annotation process and guidelines while discussing the challenges of labeling the ONER-2025 dataset. Third, we analyze key linguistic challenges, including slang, ambiguity, fragmented sentences, and emotionally charged language, in opioid discussions. Fourth, we propose a real-time monitoring system to process streaming data from social media, healthcare records, and emergency services to identify overdose events. Using 5-fold cross-validation in 11 experiments, our system integrates machine learning, deep learning, and transformer-based language models with advanced contextual embeddings to enhance understanding. Our transformer-based models (bert-base-NER and roberta-base) achieved 97% accuracy and F1-score, outperforming baselines by 10.23% (RF=0.88).
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation</title>
<link>https://arxiv.org/abs/2504.00043</link>
<guid>https://arxiv.org/abs/2504.00043</guid>
<content:encoded><![CDATA[
arXiv:2504.00043v2 Announce Type: replace 
Abstract: Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly assess either text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles -- a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in two formats (text and image), supports adjustable difficulty through prefill ratio control, and offers different evaluation strategies, ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs substantially outperform non-reasoning models by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings highlight limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatBench: From Static Benchmarks to Human-AI Evaluation</title>
<link>https://arxiv.org/abs/2504.07114</link>
<guid>https://arxiv.org/abs/2504.07114</guid>
<content:encoded><![CDATA[
arXiv:2504.07114v2 Announce Type: replace 
Abstract: With the rapid adoption of LLM-based chatbots, there is a pressing need to evaluate what humans and LLMs can achieve together. However, standard benchmarks, such as MMLU, measure LLM capabilities in isolation (i.e., "AI-alone"). Here, we design and conduct a user study to convert MMLU questions into user-AI conversations, by seeding the user with the question and having them carry out a conversation with the LLM to answer their question. We release ChatBench, a new dataset with AI-alone, user-alone, and user-AI data for 396 questions and two LLMs, including 144K answers and 7,336 user-AI conversations. We find that AI-alone accuracy fails to predict user-AI accuracy, with significant differences across multiple subjects (math, physics, and moral reasoning), and we analyze the user-AI conversations to provide insight into how they diverge from AI-alone benchmarks. Finally, we show that fine-tuning a user simulator on a subset of ChatBench improves its ability to estimate user-AI accuracies, increasing correlation on held-out questions by more than 20 points, creating possibilities for scaling interactive evaluation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation with Conflicting Evidence</title>
<link>https://arxiv.org/abs/2504.13079</link>
<guid>https://arxiv.org/abs/2504.13079</guid>
<content:encoded><![CDATA[
arXiv:2504.13079v2 Announce Type: replace 
Abstract: Large language model (LLM) agents are increasingly employing retrieval-augmented generation (RAG) to improve the factuality of their responses. However, in practice, these systems often need to handle ambiguous user queries and potentially conflicting information from multiple sources while also suppressing inaccurate information from noisy or irrelevant documents. Prior work has generally studied and addressed these challenges in isolation, considering only one aspect at a time, such as handling ambiguity or robustness to noise and misinformation. We instead consider multiple factors simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and Misinformation in Documents), a new dataset that simulates complex and realistic scenarios for conflicting evidence for a user query, including ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent approach in which LLM agents debate over the merits of an answer over multiple rounds, allowing an aggregator to collate responses corresponding to disambiguated entities while discarding misinformation and noise, thereby handling diverse sources of conflict jointly. We demonstrate the effectiveness of MADAM-RAG using both closed and open-source models on AmbigDocs -- which requires presenting all valid answers for ambiguous queries -- improving over strong RAG baselines by up to 11.40% and on FaithEval -- which requires suppressing misinformation -- where we improve by up to 15.80% (absolute) with Llama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match score). While MADAM-RAG begins to address these conflicting factors, our analysis indicates that a substantial gap remains especially when increasing the level of imbalance in supporting evidence and misinformation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness</title>
<link>https://arxiv.org/abs/2506.05735</link>
<guid>https://arxiv.org/abs/2506.05735</guid>
<content:encoded><![CDATA[
arXiv:2506.05735v2 Announce Type: replace 
Abstract: Machine unlearning techniques aim to mitigate unintended memorization in large language models (LLMs). However, existing approaches predominantly focus on the explicit removal of isolated facts, often overlooking latent inferential dependencies and the non-deterministic nature of knowledge within LLMs. Consequently, facts presumed forgotten may persist implicitly through correlated information. To address these challenges, we propose a knowledge unlearning evaluation framework that more accurately captures the implicit structure of real-world knowledge by representing relevant factual contexts as knowledge graphs with associated confidence scores. We further develop an inference-based evaluation protocol leveraging powerful LLMs as judges; these judges reason over the extracted knowledge subgraph to determine unlearning success. Our LLM judges utilize carefully designed prompts and are calibrated against human evaluations to ensure their trustworthiness and stability. Extensive experiments on our newly constructed benchmark demonstrate that our framework provides a more realistic and rigorous assessment of unlearning performance. Moreover, our findings reveal that current evaluation strategies tend to overestimate unlearning effectiveness. Our code is publicly available at https://github.com/Graph-COM/Knowledge_Unlearning.git.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: Benchmarking LLM Uncertainty, Discrimination, and Calibration in Specialty-Aware Clinical QA</title>
<link>https://arxiv.org/abs/2506.10769</link>
<guid>https://arxiv.org/abs/2506.10769</guid>
<content:encoded><![CDATA[
arXiv:2506.10769v2 Announce Type: replace 
Abstract: Reliable uncertainty quantification (UQ) is essential when employing large language models (LLMs) in high-risk domains such as clinical question answering (QA). In this work, we evaluate uncertainty estimation methods for clinical QA focusing, for the first time, on eleven clinical specialties and six question types, and across ten open-source LLMs (general-purpose, biomedical, and reasoning models). We analyze score-based UQ methods, present a case study introducing a novel lightweight method based on behavioral features derived from reasoning-oriented models, and examine conformal prediction as a complementary set-based approach. Our findings reveal that uncertainty reliability is not a monolithic property, but one that depends on clinical specialty and question type due to shifts in calibration and discrimination. Our results highlight the need to select or ensemble models based on their distinct, complementary strengths and clinical use.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Document and Template Clustering using Multimodal Embeddings</title>
<link>https://arxiv.org/abs/2506.12116</link>
<guid>https://arxiv.org/abs/2506.12116</guid>
<content:encoded><![CDATA[
arXiv:2506.12116v2 Announce Type: replace 
Abstract: This paper investigates a novel approach to unsupervised document clustering by leveraging multimodal embeddings as input to clustering algorithms such as $k$-Means, DBSCAN, a combination of HDBSCAN and $k$-NN, and BIRCH. Our method aims to achieve a finer-grained document understanding by not only grouping documents at the type level (e.g., invoices, purchase orders), but also distinguishing between different templates within the same document category. This is achieved by using embeddings that capture textual content, layout information, and visual features of documents. We evaluated the effectiveness of this approach using embeddings generated by several state-of-the-art pre-trained multimodal models, including SBERT, LayoutLMv1, LayoutLMv3, DiT, Donut, ColPali, Gemma3, and InternVL3. Our findings demonstrate the potential of multimodal embeddings to significantly enhance document clustering, offering benefits for various applications in intelligent document processing, document layout analysis, and unsupervised document classification. This work provides valuable insight into the advantages and limitations of different multimodal models for this task and opens new avenues for future research to understand and organize document collections.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Completion Learning for Language Models</title>
<link>https://arxiv.org/abs/2507.20252</link>
<guid>https://arxiv.org/abs/2507.20252</guid>
<content:encoded><![CDATA[
arXiv:2507.20252v3 Announce Type: replace 
Abstract: Current language model training paradigms typically terminate learning upon reaching the end-of-sequence () token, overlooking the potential learning opportunities in the post-completion space. We propose Post-Completion Learning (PCL), a novel training framework that systematically utilizes the sequence space after model output completion, to enhance both the reasoning and self-evaluation abilities. PCL enables models to continue generating self-assessments and reward predictions during training, while maintaining efficient inference by stopping at the completion point.
  To fully utilize this post-completion space, we design a white-box reinforcement learning method: let the model evaluate the output content according to the reward rules, then calculate and align the score with the reward functions for supervision. We implement dual-track SFT to optimize both reasoning and evaluation capabilities, and mixed it with RL training to achieve multi-objective hybrid optimization.
  Experimental results on different datasets and models demonstrate consistent improvements over traditional SFT and RL methods. Our method provides a new technical path for language model training that enhances output quality while preserving deployment efficiency.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns</title>
<link>https://arxiv.org/abs/2507.20343</link>
<guid>https://arxiv.org/abs/2507.20343</guid>
<content:encoded><![CDATA[
arXiv:2507.20343v3 Announce Type: replace 
Abstract: We present DYNARTmo, a dynamic articulatory model designed to visualize speech articulation processes in a two-dimensional midsagittal plane. The model builds upon the UK-DYNAMO framework and integrates principles of articulatory underspecification, segmental and gestural control, and coarticulation. DYNARTmo simulates six key articulators based on ten continuous and six discrete control parameters, allowing for the generation of both vocalic and consonantal articulatory configurations. The current implementation is embedded in a web-based application (SpeechArticulationTrainer) that includes sagittal, glottal, and palatal views, making it suitable for use in phonetics education and speech therapy. While this paper focuses on the static modeling aspects, future work will address dynamic movement generation and integration with articulatory-acoustic modules.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Pedagogy: Dialogic Social Learning for Artificial Agents</title>
<link>https://arxiv.org/abs/2507.21065</link>
<guid>https://arxiv.org/abs/2507.21065</guid>
<content:encoded><![CDATA[
arXiv:2507.21065v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in processing extensive offline datasets. However, they often face challenges in acquiring and integrating complex, knowledge online. Traditional AI training paradigms, predominantly based on supervised learning or reinforcement learning, mirror a 'Piagetian' model of independent exploration. These approaches typically rely on large datasets and sparse feedback signals, limiting the models' ability to learn efficiently from interactions. Drawing inspiration from Vygotsky's sociocultural theory, this study explores the potential of socially mediated learning paradigms to address these limitations.
  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI learner agent engages in dyadic pedagogical dialogues with knowledgeable AI teacher agents. These interactions emphasize external, structured dialogue as a core mechanism for knowledge acquisition, contrasting with methods that depend solely on internal inference or pattern recognition.
  Our investigation focuses on how different pedagogical strategies impact the AI learning process in the context of ontology acquisition. Empirical results indicate that such dialogic approaches-particularly those involving mixed-direction interactions combining top-down explanations with learner-initiated questioning-significantly enhance the LLM's ability to acquire and apply new knowledge, outperforming both unidirectional instructional methods and direct access to structured knowledge, formats typically present in training datasets.
  These findings suggest that integrating pedagogical and psychological insights into AI and robot training can substantially improve post-training knowledge acquisition and response quality. This approach offers a complementary pathway to existing strategies like prompt engineering
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Role-Aware Language Models for Secure and Contextualized Access Control in Organizations</title>
<link>https://arxiv.org/abs/2507.23465</link>
<guid>https://arxiv.org/abs/2507.23465</guid>
<content:encoded><![CDATA[
arXiv:2507.23465v2 Announce Type: replace 
Abstract: As large language models (LLMs) are increasingly deployed in enterprise settings, controlling model behavior based on user roles becomes an essential requirement. Existing safety methods typically assume uniform access and focus on preventing harmful or toxic outputs, without addressing role-specific access constraints. In this work, we investigate whether LLMs can be fine-tuned to generate responses that reflect the access privileges associated with different organizational roles. We explore three modeling strategies: a BERT-based classifier, an LLM-based classifier, and role-conditioned generation. To evaluate these approaches, we construct two complementary datasets. The first is adapted from existing instruction-tuning corpora through clustering and role labeling, while the second is synthetically generated to reflect realistic, role-sensitive enterprise scenarios. We assess model performance across varying organizational structures and analyze robustness to prompt injection, role mismatch, and jailbreak attempts.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and Effectiveness in Clinical Domains</title>
<link>https://arxiv.org/abs/2507.23486</link>
<guid>https://arxiv.org/abs/2507.23486</guid>
<content:encoded><![CDATA[
arXiv:2507.23486v2 Announce Type: replace 
Abstract: Large language models (LLMs) hold promise in clinical decision support but face major challenges in safety evaluation and effectiveness validation. We developed the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a multidimensional framework built on clinical expert consensus, encompassing 30 criteria covering critical areas like critical illness recognition, guideline adherence, and medication safety, with weighted consequence measures. Thirty-two specialist physicians developed and reviewed 2,069 open-ended Q\&amp;A items aligned with these criteria, spanning 26 clinical departments to simulate real-world scenarios. Benchmark testing of six LLMs revealed moderate overall performance (average total score 57.2\%, safety 54.7\%, effectiveness 62.3\%), with a significant 13.3\% performance drop in high-risk scenarios (p $<$ 0.0001). Domain-specific medical LLMs showed consistent performance advantages over general-purpose models, with relatively higher top scores in safety (0.912) and effectiveness (0.861). The findings of this study not only provide a standardized metric for evaluating the clinical application of medical LLMs, facilitating comparative analyses, risk exposure identification, and improvement directions across different scenarios, but also hold the potential to promote safer and more effective deployment of large language models in healthcare environments.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIOS: LLM Agent Operating System</title>
<link>https://arxiv.org/abs/2403.16971</link>
<guid>https://arxiv.org/abs/2403.16971</guid>
<content:encoded><![CDATA[
arXiv:2403.16971v5 Announce Type: replace-cross 
Abstract: LLM-based intelligent agents face significant deployment challenges, particularly related to resource management. Allowing unrestricted access to LLM or tool resources can lead to inefficient or even potentially harmful resource allocation and utilization for agents. Furthermore, the absence of proper scheduling and resource management mechanisms in current agent designs hinders concurrent processing and limits overall system efficiency. To address these challenges, this paper proposes the architecture of AIOS (LLM-based AI Agent Operating System) under the context of managing LLM-based agents. It introduces a novel architecture for serving LLM-based agents by isolating resources and LLM-specific services from agent applications into an AIOS kernel. This AIOS kernel provides fundamental services (e.g., scheduling, context management, memory management, storage management, access control) for runtime agents. To enhance usability, AIOS also includes an AIOS SDK, a comprehensive suite of APIs designed for utilizing functionalities provided by the AIOS kernel. Experimental results demonstrate that using AIOS can achieve up to 2.1x faster execution for serving agents built by various agent frameworks. The source code is available at https://github.com/agiresearch/AIOS.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisionUnite: A Vision-Language Foundation Model for Ophthalmology Enhanced with Clinical Knowledge</title>
<link>https://arxiv.org/abs/2408.02865</link>
<guid>https://arxiv.org/abs/2408.02865</guid>
<content:encoded><![CDATA[
arXiv:2408.02865v2 Announce Type: replace-cross 
Abstract: The need for improved diagnostic methods in ophthalmology is acute, especially in the underdeveloped regions with limited access to specialists and advanced equipment. Therefore, we introduce VisionUnite, a novel vision-language foundation model for ophthalmology enhanced with clinical knowledge. VisionUnite has been pretrained on an extensive dataset comprising 1.24 million image-text pairs, and further refined using our proposed MMFundus dataset, which includes 296,379 high-quality fundus image-text pairs and 889,137 simulated doctor-patient dialogue instances. Our experiments indicate that VisionUnite outperforms existing generative foundation models such as GPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable to junior ophthalmologists. VisionUnite performs well in various clinical scenarios including open-ended multi-disease diagnosis, clinical explanation, and patient interaction, making it a highly versatile tool for initial ophthalmic disease screening. VisionUnite can also serve as an educational aid for junior ophthalmologists, accelerating their acquisition of knowledge regarding both common and underrepresented ophthalmic conditions. VisionUnite represents a significant advancement in ophthalmology, with broad implications for diagnostics, medical education, and understanding of disease mechanisms. The source code is at https://github.com/HUANGLIZI/VisionUnite.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Diversity Shortens the ICL Plateau</title>
<link>https://arxiv.org/abs/2410.05448</link>
<guid>https://arxiv.org/abs/2410.05448</guid>
<content:encoded><![CDATA[
arXiv:2410.05448v3 Announce Type: replace-cross 
Abstract: In-context learning (ICL) describes a language model's ability to generate outputs based on a set of input demonstrations and a subsequent query. To understand this remarkable capability, researchers have studied simplified, stylized models. These studies have consistently observed long loss plateaus, during which models exhibit minimal improvement, followed by a sudden, rapid surge of learning. In this work, we reveal that training on multiple diverse ICL tasks simultaneously shortens the loss plateaus, making each task easier to learn. This finding is surprising as it contradicts the natural intuition that the combined complexity of multiple ICL tasks would lengthen the learning process, not shorten it. Our result suggests that the recent success in large-scale training of language models may be attributed not only to the richness of the data at scale but also to the easier optimization (training) induced by the diversity of natural language training data.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Risk Taxonomy and Reflection Tool for Large Language Model Adoption in Public Health</title>
<link>https://arxiv.org/abs/2411.02594</link>
<guid>https://arxiv.org/abs/2411.02594</guid>
<content:encoded><![CDATA[
arXiv:2411.02594v2 Announce Type: replace-cross 
Abstract: Recent breakthroughs in large language models (LLMs) have generated both interest and concern about their potential adoption as information sources or communication tools across different domains. In public health, where stakes are high and impacts extend across diverse populations, adopting LLMs poses unique challenges that require thorough evaluation. However, structured approaches for assessing potential risks in public health remain under-explored. To address this gap, we conducted focus groups with public health professionals and individuals with lived experience to unpack their concerns, situated across three distinct and critical public health issues that demand high-quality information: infectious disease prevention (vaccines), chronic and well-being care (opioid use disorder), and community health and safety (intimate partner violence). We synthesize participants' perspectives into a risk taxonomy, identifying and contextualizing the potential harms LLMs may introduce when positioned alongside traditional health communication. This taxonomy highlights four dimensions of risk to individuals, human-centered care, information ecosystem, and technology accountability. For each dimension, we unpack specific risks and offer example reflection questions to help practitioners adopt a risk-reflexive approach. By summarizing distinctive LLM characteristics and linking them to identified risks, we discuss the need to revisit prior mental models of information behaviors and complement evaluations with external validity and domain expertise through lived experience and real-world practices. Together, this work contributes a shared vocabulary and reflection tool for people in both computing and public health to collaboratively anticipate, evaluate, and mitigate risks in deciding when to employ LLM capabilities (or not) and how to mitigate harm.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding-based Regression</title>
<link>https://arxiv.org/abs/2501.19383</link>
<guid>https://arxiv.org/abs/2501.19383</guid>
<content:encoded><![CDATA[
arXiv:2501.19383v2 Announce Type: replace-cross 
Abstract: Language models have recently been shown capable of performing regression wherein numeric predictions are represented as decoded strings. In this work, we provide theoretical grounds for this capability and furthermore investigate the utility of causal sequence decoding models as numeric regression heads given any feature representation. We find that, despite being trained in the usual way - for next-token prediction via cross-entropy loss - decoder-based heads are as performant as standard pointwise heads when benchmarked over standard regression tasks, while being flexible enough to capture smooth numeric distributions, such as in the task of density estimation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions</title>
<link>https://arxiv.org/abs/2502.13135</link>
<guid>https://arxiv.org/abs/2502.13135</guid>
<content:encoded><![CDATA[
arXiv:2502.13135v3 Announce Type: replace-cross 
Abstract: We present an end-to-end framework for generating synthetic users for evaluating interactive agents designed to encourage positive behavior changes, such as in health and lifestyle coaching. The synthetic users are grounded in health and lifestyle conditions, specifically sleep and diabetes management in this study, to ensure realistic interactions with the health coaching agent. Synthetic users are created in two stages: first, structured data are generated grounded in real-world health and lifestyle factors in addition to basic demographics and behavioral attributes; second, full profiles of the synthetic users are developed conditioned on the structured data. Interactions between synthetic users and the coaching agent are simulated using generative agent-based models such as Concordia, or directly by prompting a language model. Using two independently-developed agents for sleep and diabetes coaching as case studies, the validity of this framework is demonstrated by analyzing the coaching agent's understanding of the synthetic users' needs and challenges. Finally, through multiple blinded evaluations of user-coach interactions by human experts, we demonstrate that our synthetic users with health and behavioral attributes more accurately portray real human users with the same attributes, compared to generic synthetic users not grounded in such attributes. The proposed framework lays the foundation for efficient development of conversational agents through extensive, realistic, and grounded simulated interactions.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting Conditions</title>
<link>https://arxiv.org/abs/2503.10331</link>
<guid>https://arxiv.org/abs/2503.10331</guid>
<content:encoded><![CDATA[
arXiv:2503.10331v2 Announce Type: replace-cross 
Abstract: Open Semantic Mapping (OSM) is a key technology in robotic perception, combining semantic segmentation and SLAM techniques. This paper introduces a dynamically configurable and highly automated LLM/LVLM-powered pipeline for evaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark). The study focuses on evaluating state-of-the-art semantic mapping algorithms under varying indoor lighting conditions, a critical challenge in indoor environments. We introduce a novel dataset with simulated RGB-D sequences and ground truth 3D reconstructions, facilitating the rigorous analysis of mapping performance across different lighting conditions. Through experiments on leading models such as ConceptGraphs, BBQ and OpenScene, we evaluate the semantic fidelity of object recognition and segmentation. Additionally, we introduce a Scene Graph evaluation method to analyze the ability of models to interpret semantic structure. The results provide insights into the robustness of these models, forming future research directions for developing resilient and adaptable robotic systems. Project page is available at https://be2rlab.github.io/OSMa-Bench/.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Computation Pruning for the Forgetting Transformer</title>
<link>https://arxiv.org/abs/2504.06949</link>
<guid>https://arxiv.org/abs/2504.06949</guid>
<content:encoded><![CDATA[
arXiv:2504.06949v2 Announce Type: replace-cross 
Abstract: The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. In particular, our method performs provably safe pruning via a dynamically set pruning threshold that guarantees the pruned attention weights are negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs and memory accesses in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 50% to 70% reduction in attention runtime (or a 2-3$\times$ speedup) and a roughly 10% to 40% increase in end-to-end training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration</title>
<link>https://arxiv.org/abs/2505.21472</link>
<guid>https://arxiv.org/abs/2505.21472</guid>
<content:encoded><![CDATA[
arXiv:2505.21472v2 Announce Type: replace-cross 
Abstract: Large vision-language models (LVLMs) achieve impressive performance on multimodal tasks but often suffer from hallucination, and confidently describe objects or attributes not present in the image. Current training-free interventions struggle to maintain accuracy in open-ended and long-form generation scenarios. We introduce the Confidence-Aware Attention Calibration (CAAC) framework to address this challenge by targeting two key biases: spatial perception bias, which distributes attention disproportionately across image tokens, and modality bias, which shifts focus from visual to textual inputs over time. CAAC employs a two-step approach: Visual-Token Calibration (VTC) to balance attention across visual tokens, and Adaptive Attention Re-Scaling (AAR) to reinforce visual grounding guided by the model's confidence. This confidence-driven adjustment ensures consistent visual alignment during generation. Experiments on CHAIR, AMBER, and POPE benchmarks demonstrate that CAAC outperforms baselines, particularly in long-form generations, effectively reducing hallucination.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics</title>
<link>https://arxiv.org/abs/2506.08835</link>
<guid>https://arxiv.org/abs/2506.08835</guid>
<content:encoded><![CDATA[
arXiv:2506.08835v2 Announce Type: replace-cross 
Abstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual content generation raises concerns about their ability to accurately represent diverse cultural contexts -- where missed cues can stereotype communities and undermine usability. In this work, we present the first study to systematically quantify the alignment of T2I models and evaluation metrics with respect to both explicit (stated) as well as implicit (unstated, implied by the prompt's cultural context) cultural expectations. To this end, we introduce CulturalFrames, a novel benchmark designed for rigorous human evaluation of cultural representation in visual generations. Spanning 10 countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts, 3637 corresponding images generated by 4 state-of-the-art T2I models, and over 10k detailed human annotations. We find that across models and countries, cultural expectations are missed an average of 44% of the time. Among these failures, explicit expectations are missed at a surprisingly high average rate of 68%, while implicit expectation failures are also significant, averaging 49%. Furthermore, we show that existing T2I evaluation metrics correlate poorly with human judgments of cultural alignment, irrespective of their internal reasoning. Collectively, our findings expose critical gaps, provide a concrete testbed, and outline actionable directions for developing culturally informed T2I models and metrics that improve global usability.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?</title>
<link>https://arxiv.org/abs/2506.14805</link>
<guid>https://arxiv.org/abs/2506.14805</guid>
<content:encoded><![CDATA[
arXiv:2506.14805v2 Announce Type: replace-cross 
Abstract: As Multimodal Large Language Models (MLLMs) continue to evolve, their cognitive and reasoning capabilities have seen remarkable progress. However, challenges in visual fine-grained perception and commonsense causal inference persist. This paper introduces Argus Inspection, a multimodal benchmark with two levels of difficulty, emphasizing detailed visual recognition while incorporating real-world commonsense understanding to evaluate causal reasoning abilities. Expanding on it, we present the Eye of Panoptes framework, which integrates a binary parametric Sigmoid metric with an indicator function, enabling a more holistic evaluation of MLLMs' responses in opinion-based reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the highest performance in visual fine-grained reasoning reaches only 0.46, highlighting considerable potential for enhancement. Our research offers valuable perspectives for the continued refinement of MLLMs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation</title>
<link>https://arxiv.org/abs/2507.22608</link>
<guid>https://arxiv.org/abs/2507.22608</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, language-specific neurons, multilingual abilities, language arithmetics, neuron steering <br />
Summary: <br />
The study delves into the neural mechanisms behind language-specific processing in large language models (LLMs) by analyzing language-specific neurons in various LLMs across multiple languages. They use the Language Activation Probability Entropy (LAPE) method to identify these neurons, which tend to cluster in deeper layers with a stronger focus on non-Latin scripts. By employing language arithmetics, the researchers are able to deactivate unwanted languages and activate desired ones, showcasing superior performance compared to simpler replacement methods. These interventions effectively guide model behavior in tasks such as language forcing, translation, QA, comprehension, and NLI. The manipulation is particularly effective for high-resource languages and benefits from typological similarity. Furthermore, cross-lingual neuron steering enhances downstream performance and sheds light on internal "fallback" mechanisms for language selection as neurons are gradually deactivated. The research code is publicly available for further exploration. <br /> <div>
arXiv:2507.22608v2 Announce Type: replace 
Abstract: Large language models (LLMs) exhibit strong multilingual abilities, yet the neural mechanisms behind language-specific processing remain unclear. We analyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and Aya-Expanse-8B & 32B across 21 typologically diverse languages, identifying neurons that control language behavior. Using the Language Activation Probability Entropy (LAPE) method, we show that these neurons cluster in deeper layers, with non-Latin scripts showing greater specialization. Related languages share overlapping neurons, reflecting internal representations of linguistic proximity.
  Through language arithmetics, i.e. systematic activation addition and multiplication, we steer models to deactivate unwanted languages and activate desired ones, outperforming simpler replacement approaches. These interventions effectively guide behavior across five multilingual tasks: language forcing, translation, QA, comprehension, and NLI. Manipulation is more successful for high-resource languages, while typological similarity improves effectiveness. We also demonstrate that cross-lingual neuron steering enhances downstream performance and reveal internal "fallback" mechanisms for language selection when neurons are progressively deactivated. Our code is made publicly available at https://github.com/d-gurgurov/Language-Neurons-Manipulation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations</title>
<link>https://arxiv.org/abs/2507.22919</link>
<guid>https://arxiv.org/abs/2507.22919</guid>
<content:encoded><![CDATA[
<div> modeling, prediction, clinical trials, serious adverse events, transfer learning
Summary:<br /><br /> The study evaluated methods for predicting serious adverse event (SAE) results in clinical trials using data from ClinicalTrials.gov. Two models were developed: a classifier to predict which arm of a trial had a higher proportion of SAEs and a regression model to predict the proportion of SAEs in the control arm. Using a transfer learning approach with pretrained language models, the best model achieved an AUC of 77.6% for predicting higher SAE proportions and an RMSE of 18.6% for predicting SAE proportions in the control arm. A sliding window method was developed for embedding extraction from long trial texts, outperforming direct comparisons. The findings suggest that publicly reported trial data can be used to identify discrepancies between expected and reported safety results, highlighting the potential for improving trial design and monitoring. <br /><br /> <div>
arXiv:2507.22919v2 Announce Type: replace 
Abstract: Objectives: With accurate estimates of expected safety results, clinical trials could be better designed and monitored. We evaluated methods for predicting serious adverse event (SAE) results in clinical trials using information only from their registrations prior to the trial. Material and Methods: We analyzed 22,107 two-arm parallel interventional clinical trials from ClinicalTrials.gov with structured summary results. Two prediction models were developed: a classifier predicting whether a greater proportion of participants in an experimental arm would have SAEs (area under the receiver operating characteristic curve; AUC) compared to the control arm, and a regression model to predict the proportion of participants with SAEs in the control arms (root mean squared error; RMSE). A transfer learning approach using pretrained language models (e.g., ClinicalT5, BioBERT) was used for feature extraction, combined with a downstream model for prediction. To maintain semantic representation in long trial texts exceeding localized language model input limits, a sliding window method was developed for embedding extraction. Results: The best model (ClinicalT5+Transformer+MLP) had 77.6% AUC when predicting which trial arm had a higher proportion of SAEs. When predicting SAE proportion in the control arm, the same model achieved RMSE of 18.6%. The sliding window approach consistently outperformed direct comparisons. Across 12 classifiers, the average absolute AUC increase was 2.00%, and absolute RMSE reduction was 1.58% across 12 regressors. Discussion: Summary results data from ClinicalTrials.gov remains underutilized. Predicted results of publicly reported trials provides an opportunity to identify discrepancies between expected and reported safety results.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextQuests: How Good are LLMs at Text-Based Video Games?</title>
<link>https://arxiv.org/abs/2507.23701</link>
<guid>https://arxiv.org/abs/2507.23701</guid>
<content:encoded><![CDATA[
<div> TextQuests, benchmark, Infocom suite, interactive fiction games, AI agent<br />
<br />
Summary: 
The article introduces TextQuests, a benchmark based on interactive fiction games that evaluates AI agents' ability to reason autonomously in complex environments. Unlike existing benchmarks, TextQuests focuses on long-context intrinsic reasoning by precluding the use of external tools. This benchmark challenges AI agents with stateful tasks that require trial-and-error learning and sustained problem-solving within a single interactive session. By mirroring real-world challenges, TextQuests aims to assess an LLM agent's capacity for self-contained problem-solving in exploratory environments. The benchmark is designed to test an agent's ability to operate in environments that demand sustained, self-directed reasoning over long horizons. TextQuests is publicly available at https://textquests.ai. <br /><br /> <div>
arXiv:2507.23701v2 Announce Type: replace-cross 
Abstract: Evaluating AI agents within complex, interactive environments that mirror real-world challenges is critical for understanding their practical capabilities. While existing agent benchmarks effectively assess skills like tool use or performance on structured tasks, they often do not fully capture an agent's ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context. To spur the development of agents capable of more robust intrinsic reasoning over long horizons, we introduce TextQuests, a benchmark based on the Infocom suite of interactive fiction games. These text-based adventures, which can take human players over 30 hours and require hundreds of precise actions to solve, serve as an effective proxy for evaluating AI agents on focused, stateful tasks. The benchmark is specifically designed to assess an LLM agent's capacity for self-contained problem-solving by precluding the use of external tools, thereby focusing on intrinsic long-context reasoning capabilities in an exploratory environment characterized by the need for trial-and-error learning and sustained problem-solving within a single interactive session. We release TextQuests at https://textquests.ai.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction</title>
<link>https://arxiv.org/abs/2508.06495</link>
<guid>https://arxiv.org/abs/2508.06495</guid>
<content:encoded><![CDATA[
<div> enrichment, Portuguese news corpora, external evidence, Large Language Models, fact-checking

Summary:
- This dissertation addresses the scarcity of publicly available datasets integrating external evidence in Portuguese language fact-checking systems.
- The methodology developed simulates a user's verification process using Large Language Models to extract main claims and search engine APIs to retrieve external evidence.
- The approach aims to enhance the robustness of fact-checking systems by incorporating external documents into Portuguese news corpora.
- A data validation and preprocessing framework is introduced to improve the quality of the base corpora through near-duplicate detection.
- The ultimate goal is to develop Semi-Automated Fact-Checking systems that can efficiently combat the spread of disinformation in the Portuguese language context.

<br /><br />Summary: <div>
arXiv:2508.06495v1 Announce Type: new 
Abstract: The accelerated dissemination of disinformation often outpaces the capacity for manual fact-checking, highlighting the urgent need for Semi-Automated Fact-Checking (SAFC) systems. Within the Portuguese language context, there is a noted scarcity of publicly available datasets that integrate external evidence, an essential component for developing robust AFC systems, as many existing resources focus solely on classification based on intrinsic text features. This dissertation addresses this gap by developing, applying, and analyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR, MuMiN-PT) with external evidence. The approach simulates a user's verification process, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash) to extract the main claim from texts and search engine APIs (Google Search API, Google FactCheck Claims Search API) to retrieve relevant external documents (evidence). Additionally, a data validation and preprocessing framework, including near-duplicate detection, is introduced to enhance the quality of the base corpora.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models</title>
<link>https://arxiv.org/abs/2508.06504</link>
<guid>https://arxiv.org/abs/2508.06504</guid>
<content:encoded><![CDATA[
<div> dynamic prompting, retrieval-augmented generation (RAG), biomedical NER, large language models (LLMs), few-shot settings

Summary: 
- The article discusses the use of dynamic prompting strategy involving retrieval-augmented generation (RAG) to address performance challenges of large language models (LLMs) in few-shot biomedical named entity recognition (NER).
- Annotated in-context learning examples are selected based on similarity with input texts, with the prompt being dynamically updated for each instance during inference.
- Static and dynamic prompt engineering techniques were implemented and optimized, showing significant improvements in F1-scores for GPT-4, GPT-3.5, and LLaMA 3-70B models.
- Static prompting with structured components increased F1-scores by 12% for GPT-4 and 11% for GPT-3.5 and LLaMA 3-70B models.
- Dynamic prompting further enhanced performance, with TF-IDF and SBERT retrieval methods yielding the best results, improving F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings, respectively.<br /><br /> <div>
arXiv:2508.06504v1 Announce Type: new 
Abstract: Biomedical named entity recognition (NER) is a high-utility natural language processing (NLP) task, and large language models (LLMs) show promise particularly in few-shot settings (i.e., limited training data). In this article, we address the performance challenges of LLMs for few-shot biomedical NER by investigating a dynamic prompting strategy involving retrieval-augmented generation (RAG). In our approach, the annotated in-context learning examples are selected based on their similarities with the input texts, and the prompt is dynamically updated for each instance during inference. We implemented and optimized static and dynamic prompt engineering techniques and evaluated them on five biomedical NER datasets. Static prompting with structured components increased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA 3-70B, relative to basic static prompting. Dynamic prompting further improved performance, with TF-IDF and SBERT retrieval methods yielding the best results, improving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings, respectively. These findings highlight the utility of contextually adaptive prompts via RAG for biomedical NER.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models</title>
<link>https://arxiv.org/abs/2508.06524</link>
<guid>https://arxiv.org/abs/2508.06524</guid>
<content:encoded><![CDATA[
<div> scaling laws, language models, carbon emissions, neural networks, training optimizations
Summary: 
The study introduces the concept of CarbonScaling, which extends neural scaling laws to consider both operational and embodied carbon emissions in training large language models (LLMs). It quantitatively connects model accuracy to carbon footprint, revealing a power-law relationship tempered by real-world inefficiencies in carbon scaling. Hardware technology advancements offer emission reductions for smaller to mid-sized models but suffer diminishing returns for extremely large LLMs due to communication overhead and underutilized GPUs. Training optimizations, particularly critical batch size scaling, are identified as effective strategies for mitigating inefficiencies and reducing the carbon footprint of LLM training. Overall, the study provides insights into training more sustainable and carbon-efficient large language models. 
<br /><br />Summary: <div>
arXiv:2508.06524v1 Announce Type: new 
Abstract: Neural scaling laws have driven the development of increasingly large language models (LLMs) by linking accuracy improvements to growth in parameter count, dataset size, and compute. However, these laws overlook the carbon emissions that scale exponentially with LLM size. This paper presents \textit{CarbonScaling}, an analytical framework that extends neural scaling laws to incorporate both operational and embodied carbon in LLM training. By integrating models for neural scaling, GPU hardware evolution, parallelism optimization, and carbon estimation, \textit{CarbonScaling} quantitatively connects model accuracy to carbon footprint. Results show that while a power-law relationship between accuracy and carbon holds, real-world inefficiencies significantly increase the scaling factor. Hardware technology scaling reduces carbon emissions for small to mid-sized models, but offers diminishing returns for extremely large LLMs due to communication overhead and underutilized GPUs. Training optimizations-especially aggressive critical batch size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers key insights for training more sustainable and carbon-efficient LLMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Art of Breaking Words: Rethinking Multilingual Tokenizer Design</title>
<link>https://arxiv.org/abs/2508.06533</link>
<guid>https://arxiv.org/abs/2508.06533</guid>
<content:encoded><![CDATA[
<div> tokenization, multilingual, Large Language Model, Indic scripts, data composition <br />
Summary: <br />
The study focuses on the importance of tokenization in Large Language Model (LLM) development, particularly in multilingual contexts like Indic scripts. The research analyzes the relationship between vocabulary size, pre-tokenization rules, and training-corpus composition in terms of token-to-word efficiency and model quality. By experimenting with Indic scripts, which have high script diversity and orthographic complexity, the study proposes a novel data composition algorithm for tokenizer training that significantly improves model performance. The findings highlight the critical role of tokenization in building efficient and scalable multilingual LLMs, alongside model architecture and training objectives. The proposed tokenizer achieved over 40% improvement in average token-to-word ratio compared to state-of-the-art multilingual Indic models, leading to enhanced model performance and faster inference speed. <div>
arXiv:2508.06533v1 Announce Type: new 
Abstract: While model architecture and training objectives are well-studied, tokenization, particularly in multilingual contexts, remains a relatively neglected aspect of Large Language Model (LLM) development. Existing tokenizers often exhibit high token-to-word ratios, inefficient use of context length, and slower inference. We present a systematic study that links vocabulary size, pre-tokenization rules, and training-corpus composition to both token-to-word efficiency and model quality. To ground our analysis in a linguistically diverse context, we conduct extensive experiments on Indic scripts, which present unique challenges due to their high script diversity and orthographic complexity. Drawing on the insights from these analyses, we propose a novel algorithm for data composition that balances multilingual data for tokenizer training. Our observations on pretokenization strategies significantly improve model performance, and our data composition algorithm reduces the average token-to-word ratio by approximately 6% with respect to the conventional data randomization approach. Our tokenizer achieves more than 40% improvement on average token-to-word ratio against stateof-the-art multilingual Indic models. This improvement yields measurable gains in both model performance and inference speed. This highlights tokenization alongside architecture and training objectives as a critical lever for building efficient, scalable multilingual LLMs
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Factor Augmented Supervised Learning with Text Embeddings</title>
<link>https://arxiv.org/abs/2508.06548</link>
<guid>https://arxiv.org/abs/2508.06548</guid>
<content:encoded><![CDATA[
<div> Embeddings, Large Language Models, AutoEncoder-Augmented Learning, Dimension Reduction, Supervised Learning <br />
Summary: <br />
The article introduces AutoEncoder-Augmented Learning with Text (AEALT) as a framework that incorporates dimension reduction into pre-trained Large Language Models (LLMs). The proposed approach utilizes a supervised augmented autoencoder to learn low-dimensional, task-relevant latent factors from text embeddings extracted from documents. By capturing the nonlinear structure of complex embeddings, AEALT outperforms traditional deep-learning methods that solely rely on raw embeddings. Extensive experiments on real-world datasets, including classification, anomaly detection, and prediction tasks, validate the effectiveness and versatility of AEALT. The numerical results demonstrate significant improvements over both raw embeddings and standard dimension reduction techniques. AEALT offers a promising solution to enhance efficiency and reduce computational costs in downstream tasks utilizing LLMs. <br /> <div>
arXiv:2508.06548v1 Announce Type: new 
Abstract: Large language models (LLMs) generate text embeddings from text data, producing vector representations that capture the semantic meaning and contextual relationships of words. However, the high dimensionality of these embeddings often impedes efficiency and drives up computational cost in downstream tasks. To address this, we propose AutoEncoder-Augmented Learning with Text (AEALT), a supervised, factor-augmented framework that incorporates dimension reduction directly into pre-trained LLM workflows. First, we extract embeddings from text documents; next, we pass them through a supervised augmented autoencoder to learn low-dimensional, task-relevant latent factors. By modeling the nonlinear structure of complex embeddings, AEALT outperforms conventional deep-learning approaches that rely on raw embeddings. We validate its broad applicability with extensive experiments on classification, anomaly detection, and prediction tasks using multiple real-world public datasets. Numerical results demonstrate that AEALT yields substantial gains over both vanilla embeddings and several standard dimension reduction methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs</title>
<link>https://arxiv.org/abs/2508.06583</link>
<guid>https://arxiv.org/abs/2508.06583</guid>
<content:encoded><![CDATA[
<div> adaptive scaffolding, instructional guidance, large language models, educational dialogues, pedagogical 

Summary:
Existing research on large language models (LLMs) has primarily focused on their ability to generate questions rather than providing adaptive instructional guidance. This study introduces a benchmark called GuideEval that evaluates pedagogical guidance based on learner states in three phases: Perception, Orchestration, and Elicitation. The empirical findings show that current LLMs struggle to provide effective adaptive scaffolding when learners are confused or need redirection. Additionally, a behavior-guided fine-tuning strategy is introduced, utilizing behavior-prompted instructional dialogues to improve guidance performance. By emphasizing learner-centered interaction, this study advocates for a more dialogic approach to evaluating Socratic LLMs. 

<br /><br />Summary: <div>
arXiv:2508.06583v1 Announce Type: new 
Abstract: The conversational capabilities of large language models hold significant promise for enabling scalable and interactive tutoring. While prior research has primarily examined their capacity for Socratic questioning, it often overlooks a critical dimension: adaptively guiding learners based on their cognitive states. This study shifts focus from mere question generation to the broader instructional guidance capability. We ask: Can LLMs emulate expert tutors who dynamically adjust strategies in response to learners' understanding? To investigate this, we propose GuideEval, a benchmark grounded in authentic educational dialogues that evaluates pedagogical guidance through a three-phase behavioral framework: (1) Perception, inferring learner states; (2) Orchestration, adapting instructional strategies; and (3) Elicitation, stimulating proper reflections. Empirical findings reveal that existing LLMs frequently fail to provide effective adaptive scaffolding when learners exhibit confusion or require redirection. Furthermore, we introduce a behavior-guided finetuning strategy that leverages behavior-prompted instructional dialogues, significantly enhancing guidance performance. By shifting the focus from isolated content evaluation to learner-centered interaction, our work advocates a more dialogic paradigm for evaluating Socratic LLMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Unlearning Without an Expert Curated Dataset</title>
<link>https://arxiv.org/abs/2508.06595</link>
<guid>https://arxiv.org/abs/2508.06595</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, unlearning, synthetic datasets, forget sets, domain-specific knowledge<br />
<br />
Summary: 
This article introduces a new method for post-hoc unlearning in large language models, allowing specific domains of knowledge to be removed without full retraining. The key bottleneck in unlearning pipelines is constructing effective forget sets, which guide the model to forget the target domain. The proposed method generates high-quality forget sets using language models themselves, by synthesizing textbook-style data through a structured prompting pipeline. Experiments on unlearning biosecurity, cybersecurity, and Harry Potter novels show that the synthetic datasets outperform baseline alternatives and rival expert-curated ones. Ablation studies demonstrate that a multi-step generation pipeline enhances data diversity, improving unlearning utility. Synthetic datasets offer a scalable and practical approach to unlearning in various domains without manual intervention. The code and dataset are released for further exploration. <br /><br />Summary: <div>
arXiv:2508.06595v1 Announce Type: new 
Abstract: Modern large language models often encode sensitive, harmful, or copyrighted knowledge, raising the need for post-hoc unlearning-the ability to remove specific domains of knowledge from a model without full retraining. A major bottleneck in current unlearning pipelines is constructing effective forget sets-datasets that approximate the target domain and guide the model to forget it. In this work, we introduce a scalable, automated approach to generate high-quality forget sets using language models themselves. Our method synthesizes textbook-style data through a structured prompting pipeline, requiring only a domain name as input. Through experiments on unlearning biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic datasets consistently outperform the baseline synthetic alternatives and are comparable to the expert-curated ones. Additionally, ablation studies reveal that the multi-step generation pipeline significantly boosts data diversity, which in turn improves unlearning utility. Overall, our findings suggest that synthetic datasets offer a promising path toward practical, scalable unlearning for a wide range of emerging domains without the need for manual intervention. We release our code and dataset at https://github.com/xyzhu123/Synthetic_Textbook.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent</title>
<link>https://arxiv.org/abs/2508.06600</link>
<guid>https://arxiv.org/abs/2508.06600</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep-Research agents, BrowseComp-Plus, benchmark, retrieval methods, GPT-5

Summary: <br /><br />Deep-Research agents integrating large language models with search tools have shown success in handling complex queries. To address limitations in current benchmarks, BrowseComp-Plus, a new benchmark derived from BrowseComp, employs a fixed, curated corpus with human-verified supporting documents and challenging negatives. This allows controlled experimentation to distinguish the performance of deep research systems. Results show that the GPT-5 model paired with the Qwen3-Embedding-8B retriever achieves higher accuracy compared to the Search-R1 model with the BM25 retriever. BrowseComp-Plus enables comprehensive evaluation and analysis of deep research agents and retrieval methods, providing insights into retrieval effectiveness, citation accuracy, and context engineering in Deep-Research systems. <div>
arXiv:2508.06600v1 Announce Type: new 
Abstract: Deep-Research agents, which integrate large language models (LLMs) with search tools, have shown success in improving the effectiveness of handling complex queries that require iterative search planning and reasoning over search results. Evaluations on current benchmarks like BrowseComp relies on black-box live web search APIs, have notable limitations in (1) fairness: dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep research methods; (2) transparency: lack of control over the document corpus makes it difficult to isolate retriever contributions. In other words, the current evaluations may compare a complete deep research system at a given time, but they do not foster well-controlled experiments to provide insights into the capability of underlying deep research LLMs. To address these challenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp, employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus includes human-verified supporting documents and mined challenging negatives, enabling controlled experimentation. The benchmark is shown to be effective in distinguishing the performance of deep research systems. For instance, the open-source model Search-R1, when paired with the BM25 retriever, achieves 3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with fewer search calls. This benchmark allows comprehensive evaluation and disentangled analysis of deep research agents and retrieval methods, fostering insights into retrieval effectiveness, citation accuracy, and context engineering in Deep-Research system.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models</title>
<link>https://arxiv.org/abs/2508.06621</link>
<guid>https://arxiv.org/abs/2508.06621</guid>
<content:encoded><![CDATA[
<div> merge list, tokenization, BPE, language model, privacy-preserving

Summary:
The paper explores the impact of using Byte-Pair Encoding (BPE) tokenization on downstream language model tasks. While recent research has highlighted potential security risks in BPE due to the merge list, this study focuses on BPE inference algorithms that do not rely on this list. Two categories of inference schemes are investigated: deviations from the merge list and merge-list-free algorithms. Results show that targeted deviations can significantly decrease model performance, while non-targeted merge-list-free algorithms have minimal impact. This suggests the possibility of simpler and more privacy-preserving tokenization methods that do not compromise model performance. These findings open up avenues for developing secure tokenization schemes for language models. 

<br /><br />Summary: <div>
arXiv:2508.06621v1 Announce Type: new 
Abstract: Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a learned token vocabulary with a detailed merge list. Recent work has shown that this merge list exposes a potential attack surface for extracting information about language model's training data. In this paper, we explore the downstream impact of BPE inference algorithms that do not rely on this merge list at all, and hence differ from the encoding process during BPE training. To address this question, we investigate two broad classes of BPE inference schemes that differ from BPE application during training: a) targeted deviation from merge-lists including random merge orders, and various corruptions of merge list involving deletion/truncation, and b) non-targeted BPE inference algorithms that do not depend on the merge list but focus on compressing the text either greedily or exactly. Extensive experiments across diverse language modeling tasks like accuracy-based QA benchmarks, machine translation, and open-ended generation reveal that while targeted deviation from the merge lists exhibits significant degradation in language model performance, the non-targeted merge-list-free inference algorithms result in minimal impact on downstream performance that is often much smaller than expected. These findings pave way for simpler and potentially more privacy-preserving tokenization schemes that do not catastrophically compromise model performance.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Stereotype and Deviation Biases in Large Language Models</title>
<link>https://arxiv.org/abs/2508.06649</link>
<guid>https://arxiv.org/abs/2508.06649</guid>
<content:encoded><![CDATA[
<div> stereotype bias, deviation bias, LLMs, demographic group, experimental results  
Summary:  
- Large language models (LLMs) are increasingly used in various fields, leading to concerns about their potential biases.  
- A study explored stereotype bias and deviation bias in LLMs when associating traits with demographic groups.  
- The research involved four advanced LLMs generating profiles of individuals based on attributes like political affiliation and religion.  
- The experimental results revealed significant stereotype bias and deviation bias towards multiple demographic groups in all examined LLMs.  
- The findings highlight the risks associated with biases in LLM-generated outputs and the implications for user privacy and fairness.  
<br /><br />Summary: <div>
arXiv:2508.06649v1 Announce Type: new 
Abstract: Large language models (LLMs) are widely applied across diverse domains, raising concerns about their limitations and potential risks. In this study, we investigate two types of bias that LLMs may display: stereotype bias and deviation bias. Stereotype bias refers to when LLMs consistently associate specific traits with a particular demographic group. Deviation bias reflects the disparity between the demographic distributions extracted from LLM-generated content and real-world demographic distributions. By asking four advanced LLMs to generate profiles of individuals, we examine the associations between each demographic group and attributes such as political affiliation, religion, and sexual orientation. Our experimental results show that all examined LLMs exhibit both significant stereotype bias and deviation bias towards multiple groups. Our findings uncover the biases that occur when LLMs infer user attributes and shed light on the potential harms of LLM-generated outputs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Testing the Limits of Machine Translation from One Book</title>
<link>https://arxiv.org/abs/2508.06665</link>
<guid>https://arxiv.org/abs/2508.06665</guid>
<content:encoded><![CDATA[
<div> translation quality, language resources, evaluation, parallel sentences, LLM

Summary:
- Current research explores the translation quality of large language models (LLMs) in the context of Kanuri, a language lacking digital resources. 
- Two datasets focusing on health and general terminology were designed to evaluate LLM translation effectiveness using various language resources. 
- Parallel sentences were found to be the most effective data source, outperforming grammar and dictionaries in both human evaluations and automatic metrics. 
- Incorporating grammar improved translation quality over zero-shot translation but was not sufficient on its own. 
- Human evaluations revealed that LLMs excelled in accuracy but struggled with fluency in domain-specific translation tasks. 
- The study highlights the importance of multidimensional evaluation metrics to assess LLM translation performance accurately. 

<br /><br />Summary: <div>
arXiv:2508.06665v1 Announce Type: new 
Abstract: Current state-of-the-art models demonstrate capacity to leverage in-context learning to translate into previously unseen language contexts. Tanzer et al. [2024] utilize language materials (e.g. a grammar) to improve translation quality for Kalamang using large language models (LLMs). We focus on Kanuri, a language that, despite having substantial speaker population, has minimal digital resources. We design two datasets for evaluation: one focused on health and humanitarian terms, and another containing generalized terminology, investigating how domain-specific tasks impact LLM translation quality.
  By providing different combinations of language resources (grammar, dictionary, and parallel sentences), we measure LLM translation effectiveness, comparing results to native speaker translations and human linguist performance. We evaluate using both automatic metrics and native speaker assessments of fluency and accuracy.
  Results demonstrate that parallel sentences remain the most effective data source, outperforming other methods in human evaluations and automatic metrics. While incorporating grammar improves over zero-shot translation, it fails as an effective standalone data source. Human evaluations reveal that LLMs achieve accuracy (meaning) more effectively than fluency (grammaticality).
  These findings suggest LLM translation evaluation benefits from multidimensional assessment beyond simple accuracy metrics, and that grammar alone, without parallel sentences, does not provide sufficient context for effective domain-specific translation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Biased Models Have Biased Thoughts?</title>
<link>https://arxiv.org/abs/2508.06671</link>
<guid>https://arxiv.org/abs/2508.06671</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, biases, chain-of-thought prompting, fairness metrics, output bias

Summary: 
The study explores the impact of chain-of-thought prompting on fairness in language models. Researchers investigate whether biased models exhibit biased thoughts before generating output. Analysis was conducted on five popular language models using fairness metrics to measure 11 different biases. Results indicate that there is a low correlation between bias in the thinking process and biased output, with a correlation of less than 0.6 in most cases. This suggests that, unlike humans, models making biased decisions do not consistently demonstrate biased thoughts. <div>
arXiv:2508.06671v1 Announce Type: new 
Abstract: The impressive performance of language models is undeniable. However, the presence of biases based on gender, race, socio-economic status, physical appearance, and sexual orientation makes the deployment of language models challenging. This paper studies the effect of chain-of-thought prompting, a recent approach that studies the steps followed by the model before it responds, on fairness. More specifically, we ask the following question: \textit{Do biased models have biased thoughts}? To answer our question, we conduct experiments on $5$ popular large language models using fairness metrics to quantify $11$ different biases in the model's thoughts and output. Our results show that the bias in the thinking steps is not highly correlated with the output bias (less than $0.6$ correlation with a $p$-value smaller than $0.001$ in most cases). In other words, unlike human beings, the tested models with biased decisions do not always possess biased thoughts.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge</title>
<link>https://arxiv.org/abs/2508.06709</link>
<guid>https://arxiv.org/abs/2508.06709</guid>
<content:encoded><![CDATA[
<div> bias, language models, evaluations, self-bias, performance 

Summary:
The study focuses on self-bias in large language models (LLMs) acting as judges. It addresses the issue of LLMs providing overly favorable ratings to their own outputs, known as self-bias, which can distort evaluations. A statistical framework is proposed to identify and estimate self-bias by comparing scoring distributions of LLM outputs. The method accounts for model quality differences and ensures genuine performance variations are not mistaken for bias. An empirical analysis on a large dataset reveals models like GPT-4o and Claude 3.5 Sonnet exhibit self-bias and family-bias by rating outputs from the same model family higher. The findings emphasize potential biases when using LLM judges and provide guidance to mitigate biases in automated evaluations. <div>
arXiv:2508.06709v1 Announce Type: new 
Abstract: Large language models (LLMs) can serve as judges that offer rapid and reliable assessments of other LLM outputs. However, models may systematically assign overly favorable ratings to their own outputs, a phenomenon known as self-bias, which can distort evaluations of true model performance. Previous studies often conflate genuine differences in model quality with bias or incorrectly assume that evaluations from LLMs and humans follow the same rating distributions. In this work, we present a statistical framework that explicitly formalizes assumptions under which self-bias can be identified and estimated. Our method models the difference in the scoring distribution that LLM-as-a-judge assigns to its own completions compared to other models, while accounting for the underlying quality of the completions provided by an independent, third-party judge (e.g., humans). Our method reliably isolates and quantifies self-bias, even when models vary in ability, ensuring that genuine performance differences are not mistaken for self-bias. We conduct an empirical analysis of self-bias on a large dataset (>5000 prompt-completion pairs) consisting of expert human annotations and judgments from nine different LLM judges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet, systematically assign higher scores to their own outputs. These models also display family-bias; systematically assigning higher ratings to outputs produced by other models of the same family. Our findings highlight potential pitfalls of using LLM judges and offer practical guidance to mitigate biases when interpreting automated evaluations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis</title>
<link>https://arxiv.org/abs/2508.06729</link>
<guid>https://arxiv.org/abs/2508.06729</guid>
<content:encoded><![CDATA[
<div> Keywords: Oral history, Semantic annotation, Sentiment analysis, Japanese American Incarceration, Language models

Summary: 
This paper introduces a framework to automate semantic and sentiment annotation for Japanese American Incarceration Oral History using large language models (LLMs). The study involves expert annotation, prompt design, and evaluation of LLMs like ChatGPT, Llama, and Qwen. The authors labeled sentences from narrators, achieving high accuracy in semantic classification with ChatGPT leading and Llama for sentiment analysis. The best prompt configurations were applied to annotate a large collection of interviews in the JAIOH archive. The research demonstrates the effectiveness of LLMs in analyzing oral history archives when guided by well-designed prompts. The study provides a reusable annotation pipeline and practical recommendations for utilizing LLMs in culturally sensitive archival analysis. By combining archival ethics with NLP techniques, this work contributes to the responsible integration of artificial intelligence in digital humanities and preservation of collective memory.

<br /><br />Summary: <div>
arXiv:2508.06729v1 Announce Type: new 
Abstract: Oral histories are vital records of lived experience, particularly within communities affected by systemic injustice and historical erasure. Effective and efficient analysis of their oral history archives can promote access and understanding of the oral histories. However, Large-scale analysis of these archives remains limited due to their unstructured format, emotional complexity, and high annotation costs. This paper presents a scalable framework to automate semantic and sentiment annotation for Japanese American Incarceration Oral History. Using LLMs, we construct a high-quality dataset, evaluate multiple models, and test prompt engineering strategies in historically sensitive contexts. Our multiphase approach combines expert annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We labeled 558 sentences from 15 narrators for sentiment and semantic classification, then evaluated zero-shot, few-shot, and RAG strategies. For semantic classification, ChatGPT achieved the highest F1 score (88.71%), followed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama slightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models showing comparable results. The best prompt configurations were used to annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our findings show that LLMs can effectively perform semantic and sentiment annotation across large oral history collections when guided by well-designed prompts. This study provides a reusable annotation pipeline and practical guidance for applying LLMs in culturally sensitive archival analysis. By bridging archival ethics with scalable NLP techniques, this work lays the groundwork for responsible use of artificial intelligence in digital humanities and preservation of collective memory. GitHub: https://github.com/kc6699c/LLM4OralHistoryAnalysis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Many-Turn Jailbreaking</title>
<link>https://arxiv.org/abs/2508.06755</link>
<guid>https://arxiv.org/abs/2508.06755</guid>
<content:encoded><![CDATA[
<div> Keywords: jailbreaking, large language models, multi-turn conversations, safety threat, benchmarking<br />
Summary: 
The article introduces the concept of multi-turn jailbreaking for large language models (LLMs), highlighting the potential dangers of continuous testing on more than just the first-turn conversation. This poses a serious threat as users commonly ask follow-up questions, and initial jailbreaking may lead to consistent irrelevant responses. The authors propose a Multi-Turn Jailbreak Benchmark (MTJ-Bench) to evaluate this setting on various models, aiming to raise awareness about this new vulnerability and urge the community to focus on building safer LLMs. By exploring multi-turn jailbreaking, the study offers insights into enhancing the security of LLMs and deepening our understanding of the risks involved in manipulating these powerful language models.<br /><br />Summary: <div>
arXiv:2508.06755v1 Announce Type: new 
Abstract: Current jailbreaking work on large language models (LLMs) aims to elicit unsafe outputs from given prompts. However, it only focuses on single-turn jailbreaking targeting one specific query. On the contrary, the advanced LLMs are designed to handle extremely long contexts and can thus conduct multi-turn conversations. So, we propose exploring multi-turn jailbreaking, in which the jailbroken LLMs are continuously tested on more than the first-turn conversation or a single target query. This is an even more serious threat because 1) it is common for users to continue asking relevant follow-up questions to clarify certain jailbroken details, and 2) it is also possible that the initial round of jailbreaking causes the LLMs to respond to additional irrelevant questions consistently. As the first step (First draft done at June 2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak Benchmark (MTJ-Bench) for benchmarking this setting on a series of open- and closed-source models and provide novel insights into this new safety threat. By revealing this new vulnerability, we aim to call for community efforts to build safer LLMs and pave the way for a more in-depth understanding of jailbreaking LLMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection</title>
<link>https://arxiv.org/abs/2508.06803</link>
<guid>https://arxiv.org/abs/2508.06803</guid>
<content:encoded><![CDATA[
<div> Keywords: sarcasm detection, Natural Language Processing, SEVADE, Dynamic Agentive Reasoning Engine, hallucination-resistant

Summary: 
SEVADE is a novel framework proposed for sarcasm detection in Natural Language Processing. It addresses limitations of existing methods by introducing a dynamic agentive reasoning engine (DARE) that employs specialized agents to deconstruct text and generate a structured reasoning chain. This framework also includes a rationale adjudicator (RA) for final classification, using a decoupled architecture to reduce the risk of hallucination. Experimental results show that SEVADE outperforms existing methods, achieving an average improvement of 6.75% in accuracy and 6.29% in Macro-F1 score on four benchmark datasets. This innovative approach combines multiple agents with linguistic theory, enabling a more thorough analysis of ironic rhetoric and enhancing the reliability of sarcasm detection.<br /><br />Summary: <div>
arXiv:2508.06803v1 Announce Type: new 
Abstract: Sarcasm detection is a crucial yet challenging Natural Language Processing task. Existing Large Language Model methods are often limited by single-perspective analysis, static reasoning pathways, and a susceptibility to hallucination when processing complex ironic rhetoric, which impacts their accuracy and reliability. To address these challenges, we propose **SEVADE**, a novel **S**elf-**Ev**olving multi-agent **A**nalysis framework with **D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The core of our framework is a Dynamic Agentive Reasoning Engine (DARE), which utilizes a team of specialized agents grounded in linguistic theory to perform a multifaceted deconstruction of the text and generate a structured reasoning chain. Subsequently, a separate lightweight rationale adjudicator (RA) performs the final classification based solely on this reasoning chain. This decoupled architecture is designed to mitigate the risk of hallucination by separating complex reasoning from the final judgment. Extensive experiments on four benchmark datasets demonstrate that our framework achieves state-of-the-art performance, with average improvements of **6.75%** in Accuracy and **6.29%** in Macro-F1 score.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems</title>
<link>https://arxiv.org/abs/2508.06810</link>
<guid>https://arxiv.org/abs/2508.06810</guid>
<content:encoded><![CDATA[
<div> keyword-guided, keyword-free, template-guided, error type classification, grammatical patterns
Summary:
An annotation framework is introduced to improve automated writing evaluation systems for language learning. The framework categorizes errors based on type and generalizability, focusing on learners' knowledge gaps. A dataset of annotated learner errors and feedback comments is collected, with feedback labeled as direct correction or hint. Three methods for generating feedback using large language models are evaluated: keyword-guided, keyword-free, and template-guided. Human teachers assess the systems' outputs for relevance, factuality, and comprehensibility. This study aims to provide more targeted feedback to learners by considering specific grammatical patterns and knowledge gaps, ensuring that corrections are not only made but understood for language learning purposes.<br /><br />Summary: <div>
arXiv:2508.06810v1 Announce Type: new 
Abstract: Recent advances in natural language processing (NLP) have contributed to the development of automated writing evaluation (AWE) systems that can correct grammatical errors. However, while these systems are effective at improving text, they are not optimally designed for language learning. They favor direct revisions, often with a click-to-fix functionality that can be applied without considering the reason for the correction. Meanwhile, depending on the error type, learners may benefit most from simple explanations and strategically indirect hints, especially on generalizable grammatical rules. To support the generation of such feedback, we introduce an annotation framework that models each error's error type and generalizability. For error type classification, we introduce a typology focused on inferring learners' knowledge gaps by connecting their errors to specific grammatical patterns. Following this framework, we collect a dataset of annotated learner errors and corresponding human-written feedback comments, each labeled as a direct correction or hint. With this data, we evaluate keyword-guided, keyword-free, and template-guided methods of generating feedback using large language models (LLMs). Human teachers examined each system's outputs, assessing them on grounds including relevance, factuality, and comprehensibility. We report on the development of the dataset and the comparative performance of the systems investigated.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text to Speech System for Meitei Mayek Script</title>
<link>https://arxiv.org/abs/2508.06870</link>
<guid>https://arxiv.org/abs/2508.06870</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-Speech, Manipuri language, Meitei Mayek script, Tacotron 2, HiFi-GAN<br />
Summary:<br />
This paper discusses the development of a Text-to-Speech (TTS) system for the Manipuri language using the Meitei Mayek script. By adapting Tacotron 2 and HiFi-GAN, a neural TTS architecture was created to accommodate tonal phonology and under-resourced linguistic settings. The researchers devised a phoneme mapping for Meitei Mayek to ARPAbet, compiled a dataset with a single speaker, and successfully synthesized natural and intelligible speech. The system's quality was confirmed through subjective and objective assessments. This innovative TTS framework serves as a foundation for preserving the language and promoting technological inclusion of Manipuri speakers.<br /> 
Summary: <div>
arXiv:2508.06870v1 Announce Type: new 
Abstract: This paper presents the development of a Text-to-Speech (TTS) system for the Manipuri language
  using the Meitei Mayek script. Leveraging Tacotron 2 and HiFi-GAN, we introduce a neural TTS
  architecture adapted to support tonal phonology and under-resourced linguistic environments. We
  develop a phoneme mapping for Meitei Mayek to ARPAbet, curate a single-speaker dataset, and
  demonstrate intelligible and natural speech synthesis, validated through subjective and objective
  metrics. This system lays the groundwork for linguistic preservation and technological inclusion of
  Manipuri.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ESNERA: Empirical and semantic named entity alignment for named entity dataset merging</title>
<link>https://arxiv.org/abs/2508.06877</link>
<guid>https://arxiv.org/abs/2508.06877</guid>
<content:encoded><![CDATA[
<div> Named Entity Recognition, deep learning, dataset merging, label alignment, NER performance,
<br />
Summary:
<br />
Named Entity Recognition (NER) is a crucial task in natural language processing, but improving performance relies on large annotated datasets. A new method for automatically aligning labels based on similarity is proposed, combining empirical and semantic similarities and using a pairwise merging strategy. The approach successfully merges three NER datasets and improves performance when integrating a financial domain dataset. This method offers an efficient, interpretable, and scalable solution for integrating multiple NER corpora. <div>
arXiv:2508.06877v1 Announce Type: new 
Abstract: Named Entity Recognition (NER) is a fundamental task in natural language processing. It remains a research hotspot due to its wide applicability across domains. Although recent advances in deep learning have significantly improved NER performance, they rely heavily on large, high-quality annotated datasets. However, building these datasets is expensive and time-consuming, posing a major bottleneck for further research. Current dataset merging approaches mainly focus on strategies like manual label mapping or constructing label graphs, which lack interpretability and scalability. To address this, we propose an automatic label alignment method based on label similarity. The method combines empirical and semantic similarities, using a greedy pairwise merging strategy to unify label spaces across different datasets. Experiments are conducted in two stages: first, merging three existing NER datasets into a unified corpus with minimal impact on NER performance; second, integrating this corpus with a small-scale, self-built dataset in the financial domain. The results show that our method enables effective dataset merging and enhances NER performance in the low-resource financial domain. This study presents an efficient, interpretable, and scalable solution for integrating multi-source NER corpora.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The ReQAP System for Question Answering over Personal Information</title>
<link>https://arxiv.org/abs/2508.06880</link>
<guid>https://arxiv.org/abs/2508.06880</guid>
<content:encoded><![CDATA[
<div> Keywords: Personal information, ReQAP system, complex questions, language models, user trust.<br />
Summary: 
The ReQAP system aims to support users in answering complex questions that involve filters, joins, and aggregation across various data sources on their devices. It uniquely decomposes questions and incrementally builds an operator tree for execution, using lightweight language models for question interpretation and operators. The system's demo showcases its functionality for advanced user queries and provides detailed tracking of how answers are computed by the operators in the execution tree. The ability to trace answers back to the underlying sources is crucial for enhancing human comprehensibility and user trust in the system. <div>
arXiv:2508.06880v1 Announce Type: new 
Abstract: Personal information is abundant on users' devices, from structured data in calendar, shopping records or fitness tools, to unstructured contents in mail and social media posts. This works presents the ReQAP system that supports users with answers for complex questions that involve filters, joins and aggregation over heterogeneous sources. The unique trait of ReQAP is that it recursively decomposes questions and incrementally builds an operator tree for execution. Both the question interpretation and the individual operators make smart use of light-weight language models, with judicious fine-tuning. The demo showcases the rich functionality for advanced user questions, and also offers detailed tracking of how the answers are computed by the operators in the execution tree. Being able to trace answers back to the underlying sources is vital for human comprehensibility and user trust in the system.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores</title>
<link>https://arxiv.org/abs/2508.06886</link>
<guid>https://arxiv.org/abs/2508.06886</guid>
<content:encoded><![CDATA[
<div> Framework, Persona-based dialogue generation, Large language models, SBS, Score-conditioned training

Summary: 
The article introduces a novel framework called SBS (Score-Before-Speaking) for persona-based dialogue generation, addressing the challenge of integrating persona fidelity into conversations. Unlike previous methods, SBS combines the learning of responses and their quality into a single step, training a dialogue model to correlate augmented responses with a quality score during training. By leveraging noun-based substitution for augmentation and semantic similarity-based scores for response quality, SBS outperforms existing methods for both million and billion-parameter models on benchmark datasets like PERSONA-CHAT and ConvAI2. Extensive experiments demonstrate that score-conditioned training allows models to better capture a range of persona-consistent dialogues, with ablation studies showing the superiority of including scores in the input prompt during training.<br /><br />Summary: <div>
arXiv:2508.06886v1 Announce Type: new 
Abstract: Persona-based dialogue generation is an important milestone towards building conversational artificial intelligence. Despite the ever-improving capabilities of large language models (LLMs), effectively integrating persona fidelity in conversations remains challenging due to the limited diversity in existing dialogue data. We propose a novel framework SBS (Score-Before-Speaking), which outperforms previous methods and yields improvements for both million and billion-parameter models. Unlike previous methods, SBS unifies the learning of responses and their relative quality into a single step. The key innovation is to train a dialogue model to correlate augmented responses with a quality score during training and then leverage this knowledge at inference. We use noun-based substitution for augmentation and semantic similarity-based scores as a proxy for response quality. Through extensive experiments with benchmark datasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training allows existing models to better capture a spectrum of persona-consistent dialogues. Our ablation studies also demonstrate that including scores in the input prompt during training is superior to conventional training setups. Code and further details are available at https://arpita2512.github.io/score_before_you_speak
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection</title>
<link>https://arxiv.org/abs/2508.06913</link>
<guid>https://arxiv.org/abs/2508.06913</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, SentiDetect, sentiment distribution consistency, sentiment distribution preservation, robustness

Summary:
SentiDetect is a model-agnostic framework designed to detect LLM-generated text by analyzing sentiment distribution stability. The framework utilizes two metrics, sentiment distribution consistency, and sentiment distribution preservation, to quantify stability under sentiment-altering and semantic-preserving transformations. Evaluations on diverse datasets and advanced LLMs show SentiDetect's superiority over existing baselines, with significant improvements in F1 scores. The framework also demonstrates greater robustness to paraphrasing, adversarial attacks, and text length variations, outperforming current detection methods in challenging scenarios.

<br /><br />Summary: 
- SentiDetect is a model-agnostic framework for detecting LLM-generated text by analyzing sentiment distribution stability.
- The framework uses sentiment distribution consistency and sentiment distribution preservation metrics to quantify stability under transformations.
- Evaluations on various datasets and advanced LLMs show SentiDetect's superiority over existing methods.
- SentiDetect demonstrates robustness to paraphrasing, adversarial attacks, and text length variations.
 <div>
arXiv:2508.06913v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has resulted in increasingly sophisticated AI-generated content, posing significant challenges in distinguishing LLM-generated text from human-written language. Existing detection methods, primarily based on lexical heuristics or fine-tuned classifiers, often suffer from limited generalizability and are vulnerable to paraphrasing, adversarial perturbations, and cross-domain shifts. In this work, we propose SentiDetect, a model-agnostic framework for detecting LLM-generated text by analyzing the divergence in sentiment distribution stability. Our method is motivated by the empirical observation that LLM outputs tend to exhibit emotionally consistent patterns, whereas human-written texts display greater emotional variability. To capture this phenomenon, we define two complementary metrics: sentiment distribution consistency and sentiment distribution preservation, which quantify stability under sentiment-altering and semantic-preserving transformations. We evaluate SentiDetect on five diverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro, Claude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its superiority over state-of-the-art baselines, with over 16% and 11% F1 score improvements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover, SentiDetect also shows greater robustness to paraphrasing, adversarial attacks, and text length variations, outperforming existing detectors in challenging scenarios.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction</title>
<link>https://arxiv.org/abs/2508.06971</link>
<guid>https://arxiv.org/abs/2508.06971</guid>
<content:encoded><![CDATA[
<div> framework, passage retrieval, answer extraction, language models, Quran QA 2023 Shared Task  
Summary:  
- The paper proposes a two-stage framework for Quranic Question Answering, focusing on passage retrieval and answer extraction.  
- Ensemble fine-tuned Arabic language models are used for superior ranking performance in passage retrieval.  
- For answer extraction, instruction-tuned large language models with few-shot prompting are employed to deal with small dataset limitations.  
- The approach achieves state-of-the-art results in the Quran QA 2023 Shared Task, with significantly improved MAP@10, MRR@10 for retrieval, and pAP@10 for extraction.  
- The combination of model ensembling and instruction-tuned language models proves effective in addressing low-resource question answering challenges in specialized domains.  
<br /><br />Summary: <div>
arXiv:2508.06971v1 Announce Type: new 
Abstract: Quranic Question Answering presents unique challenges due to the linguistic complexity of Classical Arabic and the semantic richness of religious texts. In this paper, we propose a novel two-stage framework that addresses both passage retrieval and answer extraction. For passage retrieval, we ensemble fine-tuned Arabic language models to achieve superior ranking performance. For answer extraction, we employ instruction-tuned large language models with few-shot prompting to overcome the limitations of fine-tuning on small datasets. Our approach achieves state-of-the-art results on the Quran QA 2023 Shared Task, with a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of 0.669 for extraction, substantially outperforming previous methods. These results demonstrate that combining model ensembling and instruction-tuned language models effectively addresses the challenges of low-resource question answering in specialized domains.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models</title>
<link>https://arxiv.org/abs/2508.06974</link>
<guid>https://arxiv.org/abs/2508.06974</guid>
<content:encoded><![CDATA[
<div> Keywords: 1-bit LLM, quantization, pre-trained models, progressive training, binary-aware initialization

Summary:
Progressive 1-bit LLM quantization methods typically start from scratch, leading to high costs and accuracy degradation. This study introduces a consistent approach for smoothly converting floating-point weights to 1-bit representation through progressive training. Incorporating binary-aware initialization and dual-scaling compensation aids in reducing training difficulty and improving performance. Experimental results demonstrate superior performance compared to existing methods on various LLM sizes. The proposed method leverages pre-trained models, eliminating the need for costly training from scratch. By bridging the gap between full precision and 1-bit representations, this approach enables the efficient adoption of 1-bit LLMs in practice. <br /><br />Summary: <div>
arXiv:2508.06974v1 Announce Type: new 
Abstract: 1-bit LLM quantization offers significant advantages in reducing storage and computational costs. However, existing methods typically train 1-bit LLMs from scratch, failing to fully leverage pre-trained models. This results in high training costs and notable accuracy degradation. We identify that the large gap between full precision and 1-bit representations makes direct adaptation difficult. In this paper, we introduce a consistent progressive training for both forward and backward, smoothly converting the floating-point weights into the binarized ones. Additionally, we incorporate binary-aware initialization and dual-scaling compensation to reduce the difficulty of progressive training and improve the performance. Experimental results on LLMs of various sizes demonstrate that our method outperforms existing approaches. Our results show that high-performance 1-bit LLMs can be achieved using pre-trained models, eliminating the need for expensive training from scratch.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings</title>
<link>https://arxiv.org/abs/2508.07017</link>
<guid>https://arxiv.org/abs/2508.07017</guid>
<content:encoded><![CDATA[
<div> method, abstractive summarization, semantic compression, Vec2Summ, generative language model

Summary: 
Vec2Summ is a novel method for abstractive summarization that approaches the task as semantic compression. It represents a document collection using a mean vector in the semantic embedding space to capture the central meaning of the corpus. By performing embedding inversion through a generative language model, Vec2Summ reconstructs fluent summaries. Introducing stochasticity by sampling from a Gaussian distribution around the mean allows for controlled randomness, similar to ensemble learning bagging. This method overcomes limitations of LLM-based summarization by avoiding context-length constraints, enabling interpretable and controllable generation, and scaling efficiently with corpus size. Empirical results demonstrate that Vec2Summ produces coherent summaries for topic-focused and order-invariant corpora, with performance comparable to direct LLM summarization in terms of thematic coverage and efficiency, albeit with slightly less detailed output. Its potential lies in settings prioritizing scalability, semantic control, and corpus-level abstraction. 

<br /><br />Summary: <div>
arXiv:2508.07017v1 Announce Type: new 
Abstract: We propose Vec2Summ, a novel method for abstractive summarization that frames the task as semantic compression. Vec2Summ represents a document collection using a single mean vector in the semantic embedding space, capturing the central meaning of the corpus. To reconstruct fluent summaries, we perform embedding inversion -- decoding this mean vector into natural language using a generative language model. To improve reconstruction quality and capture some degree of topical variability, we introduce stochasticity by sampling from a Gaussian distribution centered on the mean. This approach is loosely analogous to bagging in ensemble learning, where controlled randomness encourages more robust and varied outputs. Vec2Summ addresses key limitations of LLM-based summarization methods. It avoids context-length constraints, enables interpretable and controllable generation via semantic parameters, and scales efficiently with corpus size -- requiring only $O(d + d^2)$ parameters. Empirical results show that Vec2Summ produces coherent summaries for topically focused, order-invariant corpora, with performance comparable to direct LLM summarization in terms of thematic coverage and efficiency, albeit with less fine-grained detail. These results underscore Vec2Summ's potential in settings where scalability, semantic control, and corpus-level abstraction are prioritized.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages</title>
<link>https://arxiv.org/abs/2508.07069</link>
<guid>https://arxiv.org/abs/2508.07069</guid>
<content:encoded><![CDATA[
<div> dataset, dialogue, Southeast Asia, culture, language  
Summary:  
SEADialogues is a new culturally grounded dialogue dataset focusing on Southeast Asia, a region with rich cultural diversity and multiple low-resource languages. The dataset includes dialogues in eight languages from six Southeast Asian countries, incorporating persona attributes and culturally relevant topics to enhance personalization. By providing dialogues that reflect everyday life in diverse communities, SEADialogues aims to support research on culturally aware and human-centric large language models, particularly for conversational dialogue agents. This dataset fills a gap in existing chit-chat datasets by emphasizing the importance of cultural nuances in natural human conversations.<br /><br /> <div>
arXiv:2508.07069v1 Announce Type: new 
Abstract: Although numerous datasets have been developed to support dialogue systems, most existing chit-chat datasets overlook the cultural nuances inherent in natural human conversations. To address this gap, we introduce SEADialogues, a culturally grounded dialogue dataset centered on Southeast Asia, a region with over 700 million people and immense cultural diversity. Our dataset features dialogues in eight languages from six Southeast Asian countries, many of which are low-resource despite having sizable speaker populations. To enhance cultural relevance and personalization, each dialogue includes persona attributes and two culturally grounded topics that reflect everyday life in the respective communities. Furthermore, we release a multi-turn dialogue dataset to advance research on culturally aware and human-centric large language models, including conversational dialogue agents.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context</title>
<link>https://arxiv.org/abs/2508.07090</link>
<guid>https://arxiv.org/abs/2508.07090</guid>
<content:encoded><![CDATA[
<div> Benchmark, Language Models, Social Bias, Indian context, Multilingual

Summary:
The study introduces BharatBBQ, a benchmark designed to evaluate biases in Hindi, English, and other Indian languages. It covers 13 social categories and assesses biases in a culturally adapted manner. The dataset includes examples in one language expanded to multiple languages through translation and verification. Five multilingual LM families are evaluated across zero and few-shot settings to analyze bias and stereotypical bias scores. The findings reveal persistent biases across languages and social categories, with Indian languages often exhibiting amplified biases compared to English. The research underscores the importance of linguistically and culturally grounded benchmarks for evaluating bias in AI systems. <br /><br />Summary: <div>
arXiv:2508.07090v1 Announce Type: new 
Abstract: Evaluating social biases in language models (LMs) is crucial for ensuring fairness and minimizing the reinforcement of harmful stereotypes in AI systems. Existing benchmarks, such as the Bias Benchmark for Question Answering (BBQ), primarily focus on Western contexts, limiting their applicability to the Indian context. To address this gap, we introduce BharatBBQ, a culturally adapted benchmark designed to assess biases in Hindi, English, Marathi, Bengali, Tamil, Telugu, Odia, and Assamese. BharatBBQ covers 13 social categories, including 3 intersectional groups, reflecting prevalent biases in the Indian sociocultural landscape. Our dataset contains 49,108 examples in one language that are expanded using translation and verification to 392,864 examples in eight different languages. We evaluate five multilingual LM families across zero and few-shot settings, analyzing their bias and stereotypical bias scores. Our findings highlight persistent biases across languages and social categories and often amplified biases in Indian languages compared to English, demonstrating the necessity of linguistically and culturally grounded benchmarks for bias evaluation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning</title>
<link>https://arxiv.org/abs/2508.07101</link>
<guid>https://arxiv.org/abs/2508.07101</guid>
<content:encoded><![CDATA[
<div> Sparse attention, reasoning tasks, LessIsMore, token selection, decoding speed-up <br />
<br />
Summary: LessIsMore is a training-free sparse attention mechanism designed for reasoning tasks that significantly reduces computational overhead compared to traditional large reasoning models. By leveraging global attention patterns and aggregating token selections from local attention heads with recent contextual information, LessIsMore enables unified cross-head token ranking for future decoding layers. This approach improves generalization and efficiency by avoiding the need to maintain separate token subsets per head. Evaluation across various reasoning tasks and benchmarks demonstrates that LessIsMore maintains or enhances accuracy while achieving a 1.1x average decoding speed-up compared to full attention. Furthermore, it attends to 2x fewer tokens without any loss in accuracy, resulting in a 1.13x end-to-end speed-up over existing sparse attention methods. <div>
arXiv:2508.07101v1 Announce Type: new 
Abstract: Large reasoning models achieve strong performance through test-time scaling but incur substantial computational overhead, particularly from excessive token generation when processing short input prompts. While sparse attention mechanisms can reduce latency and memory usage, existing approaches suffer from significant accuracy degradation due to accumulated errors during long-generation reasoning. These methods generally require either high token retention rates or expensive retraining. We introduce LessIsMore, a training-free sparse attention mechanism for reasoning tasks, which leverages global attention patterns rather than relying on traditional head-specific local optimizations. LessIsMore aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers. This unified selection improves generalization and efficiency by avoiding the need to maintain separate token subsets per head. Evaluation across diverse reasoning tasks and benchmarks shows that LessIsMore preserves -- and in some cases improves -- accuracy while achieving a $1.1\times$ average decoding speed-up compared to full attention. Moreover, LessIsMore attends to $2\times$ fewer tokens without accuracy loss, achieving a $1.13\times$ end-to-end speed-up compared to existing sparse attention methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution</title>
<link>https://arxiv.org/abs/2508.07111</link>
<guid>https://arxiv.org/abs/2508.07111</guid>
<content:encoded><![CDATA[
<div> bias, intersectional, language models, fairness, evaluation<br />
<br />
Summary:
This article discusses the potential biases present in large language models (LLMs) when used in critical social contexts. The study introduces a new benchmark called WinoIdentity, which evaluates intersecting biases across multiple demographic markers within LLMs. By assessing bias through the lens of uncertainty, the authors propose a metric called Coreference Confidence Disparity to measure group (un)fairness. The evaluation of five LLMs reveals significant confidence disparities, particularly for doubly-disadvantaged identities in anti-stereotypical settings. Surprisingly, the study finds that models exhibit decreased coreference confidence even for hegemonic or privileged markers, indicating a potential reliance on memorization rather than logical reasoning. These findings highlight two independent failures in value alignment and validity within LLMs, which could compound to cause social harm. <br /> <div>
arXiv:2508.07111v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved impressive performance, leading to their widespread adoption as decision-support tools in resource-constrained contexts like hiring and admissions. There is, however, scientific consensus that AI systems can reflect and exacerbate societal biases, raising concerns about identity-based harm when used in critical social contexts. Prior work has laid a solid foundation for assessing bias in LLMs by evaluating demographic disparities in different language reasoning tasks. In this work, we extend single-axis fairness evaluations to examine intersectional bias, recognizing that when multiple axes of discrimination intersect, they create distinct patterns of disadvantage. We create a new benchmark called WinoIdentity by augmenting the WinoBias dataset with 25 demographic markers across 10 attributes, including age, nationality, and race, intersected with binary gender, yielding 245,700 prompts to evaluate 50 distinct bias patterns. Focusing on harms of omission due to underrepresentation, we investigate bias through the lens of uncertainty and propose a group (un)fairness metric called Coreference Confidence Disparity which measures whether models are more or less confident for some intersectional identities than others. We evaluate five recently published LLMs and find confidence disparities as high as 40% along various demographic attributes including body type, sexual orientation and socio-economic status, with models being most uncertain about doubly-disadvantaged identities in anti-stereotypical settings. Surprisingly, coreference confidence decreases even for hegemonic or privileged markers, indicating that the recent impressive performance of LLMs is more likely due to memorization than logical reasoning. Notably, these are two independent failures in value alignment and validity that can compound to cause social harm.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens</title>
<link>https://arxiv.org/abs/2508.07143</link>
<guid>https://arxiv.org/abs/2508.07143</guid>
<content:encoded><![CDATA[
<div> fairness implications, Automatic Speech Recognition, ASR bias, marginalized linguistic communities, ethical dimensions<br />
<br />
Summary: 
This paper examines the fairness implications of Automatic Speech Recognition (ASR) systems through a philosophical lens. It argues that misrecognition of certain speech varieties by ASR systems is more than a technical issue, but a form of disrespect that perpetuates historical injustices against marginalized linguistic communities. The paper distinguishes between morally neutral classification and harmful discrimination, showing how ASR systems can unintentionally cause harm by consistently misrecognizing non-standard dialects. It identifies three ethical dimensions of speech technologies that differentiate ASR bias from other algorithmic fairness concerns. These dimensions include the temporal burden placed on speakers of non-standard varieties, the disruption of conversational flow, and the connection between speech patterns and personal/cultural identity. The paper concludes that addressing ASR bias requires recognizing diverse speech varieties as legitimate forms of expression deserving of accommodation in technology development. <div>
arXiv:2508.07143v1 Announce Type: new 
Abstract: Automatic Speech Recognition (ASR) systems now mediate countless human-technology interactions, yet research on their fairness implications remains surprisingly limited. This paper examines ASR bias through a philosophical lens, arguing that systematic misrecognition of certain speech varieties constitutes more than a technical limitation -- it represents a form of disrespect that compounds historical injustices against marginalized linguistic communities. We distinguish between morally neutral classification (discriminate1) and harmful discrimination (discriminate2), demonstrating how ASR systems can inadvertently transform the former into the latter when they consistently misrecognize non-standard dialects. We identify three unique ethical dimensions of speech technologies that differentiate ASR bias from other algorithmic fairness concerns: the temporal burden placed on speakers of non-standard varieties ("temporal taxation"), the disruption of conversational flow when systems misrecognize speech, and the fundamental connection between speech patterns and personal/cultural identity. These factors create asymmetric power relationships that existing technical fairness metrics fail to capture. The paper analyzes the tension between linguistic standardization and pluralism in ASR development, arguing that current approaches often embed and reinforce problematic language ideologies. We conclude that addressing ASR bias requires more than technical interventions; it demands recognition of diverse speech varieties as legitimate forms of expression worthy of technological accommodation. This philosophical reframing offers new pathways for developing ASR systems that respect linguistic diversity and speaker autonomy.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Surgery for Safe LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2508.07172</link>
<guid>https://arxiv.org/abs/2508.07172</guid>
<content:encoded><![CDATA[
<div> gradient surgery, large language models, safe fine-tuning, alignment loss, robustness

Summary:
SafeGrad introduces a solution to the vulnerability in Fine-tuning-as-a-Service by addressing the sensitivity to harmful examples in fine-tuning datasets. By employing gradient surgery through conflict detection and projection, SafeGrad nullifies harmful gradients to maintain safety alignment while learning user tasks. The method uses a KL-divergence alignment loss to improve robustness and data efficiency by learning the safety profile of the foundation model. Extensive experiments demonstrate that SafeGrad is a state-of-the-art defense mechanism, providing robust safety even at high harmful ratios without compromising task performance. <div>
arXiv:2508.07172v1 Announce Type: new 
Abstract: Fine-tuning-as-a-Service introduces a critical vulnerability where a few malicious examples mixed into the user's fine-tuning dataset can compromise the safety alignment of Large Language Models (LLMs). While a recognized paradigm frames safe fine-tuning as a multi-objective optimization problem balancing user task performance with safety alignment, we find existing solutions are critically sensitive to the harmful ratio, with defenses degrading sharply as harmful ratio increases. We diagnose that this failure stems from conflicting gradients, where the user-task update directly undermines the safety objective. To resolve this, we propose SafeGrad, a novel method that employs gradient surgery. When a conflict is detected, SafeGrad nullifies the harmful component of the user-task gradient by projecting it onto the orthogonal plane of the alignment gradient, allowing the model to learn the user's task without sacrificing safety. To further enhance robustness and data efficiency, we employ a KL-divergence alignment loss that learns the rich, distributional safety profile of the well-aligned foundation model. Extensive experiments show that SafeGrad provides state-of-the-art defense across various LLMs and datasets, maintaining robust safety even at high harmful ratios without compromising task fidelity.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models</title>
<link>https://arxiv.org/abs/2508.07173</link>
<guid>https://arxiv.org/abs/2508.07173</guid>
<content:encoded><![CDATA[
<div> Keywords: Omni-modal Large Language Models, safety evaluation, cross-modal consistency, benchmarking, vulnerabilities

Summary:
- The article introduces Omni-SafetyBench, a comprehensive benchmark for evaluating the safety of Omni-modal Large Language Models (OLLMs) with audio-visual inputs.
- The benchmark includes 24 modality combinations and variations, totaling 972 samples each, including dedicated harm cases to assess safety performance.
- Tailored metrics such as Safety-score, Cross-Modal Safety Consistency Score (CMSC-score), and conditional Attack Success Rate (C-ASR) are proposed to evaluate safety and consistency across modalities.
- Evaluation of 10 OLLMs reveals critical vulnerabilities, with no model excelling in both overall safety and consistency.
- The study highlights weaknesses in safety defenses, especially with complex audio-visual inputs, and identifies models with low scores on specific modalities, emphasizing the need for enhanced OLLM safety measures.

<br /><br />Summary: The Omni-SafetyBench benchmark introduces a comprehensive evaluation framework for Omni-modal Large Language Models to assess safety, highlighting vulnerabilities and weaknesses in current models. The proposed metrics aim to measure safety performance and cross-modal consistency, emphasizing the urgent need for improved safety measures in OLLMs. <div>
arXiv:2508.07173v1 Announce Type: new 
Abstract: The rise of Omni-modal Large Language Models (OLLMs), which integrate visual and auditory processing with text, necessitates robust safety evaluations to mitigate harmful outputs. However, no dedicated benchmarks currently exist for OLLMs, and prior benchmarks designed for other LLMs lack the ability to assess safety performance under audio-visual joint inputs or cross-modal safety consistency. To fill this gap, we introduce Omni-SafetyBench, the first comprehensive parallel benchmark for OLLM safety evaluation, featuring 24 modality combinations and variations with 972 samples each, including dedicated audio-visual harm cases. Considering OLLMs' comprehension challenges with complex omni-modal inputs and the need for cross-modal consistency evaluation, we propose tailored metrics: a Safety-score based on conditional Attack Success Rate (C-ASR) and Refusal Rate (C-RR) to account for comprehension failures, and a Cross-Modal Safety Consistency Score (CMSC-score) to measure consistency across modalities. Evaluating 6 open-source and 4 closed-source OLLMs reveals critical vulnerabilities: (1) no model excels in both overall safety and consistency, with only 3 models achieving over 0.6 in both metrics and top performer scoring around 0.8; (2) safety defenses weaken with complex inputs, especially audio-visual joints; (3) severe weaknesses persist, with some models scoring as low as 0.14 on specific modalities. Our benchmark and metrics highlight urgent needs for enhanced OLLM safety, providing a foundation for future improvements.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback</title>
<link>https://arxiv.org/abs/2508.07178</link>
<guid>https://arxiv.org/abs/2508.07178</guid>
<content:encoded><![CDATA[
<div> framework, click noise, personalized generation, user interests, dataset
<br />
The paper introduces a novel framework, PHG-DIF, for personalized headline generation by addressing the issue of click noise in historical clickstreams. The framework includes dual-stage filtering to eliminate noise, identified by short dwell times and abnormal click bursts, and multi-level temporal fusion for dynamic profiling of user interests. The authors also present a new benchmark dataset, DT-PENS, containing user click behavior and annotated personalized headlines. Experiments show that PHG-DIF improves headline quality and outperforms existing methods on DT-PENS. The framework implementation and dataset are available for access. 
<br /><br />Summary: <div>
arXiv:2508.07178v1 Announce Type: new 
Abstract: Accurate personalized headline generation hinges on precisely capturing user interests from historical behaviors. However, existing methods neglect personalized-irrelevant click noise in entire historical clickstreams, which may lead to hallucinated headlines that deviate from genuine user preferences. In this paper, we reveal the detrimental impact of click noise on personalized generation quality through rigorous analysis in both user and news dimensions. Based on these insights, we propose a novel Personalized Headline Generation framework via Denoising Fake Interests from Implicit Feedback (PHG-DIF). PHG-DIF first employs dual-stage filtering to effectively remove clickstream noise, identified by short dwell times and abnormal click bursts, and then leverages multi-level temporal fusion to dynamically model users' evolving and multi-faceted interests for precise profiling. Moreover, we release DT-PENS, a new benchmark dataset comprising the click behavior of 1,000 carefully curated users and nearly 10,000 annotated personalized headlines with historical dwell time annotations. Extensive experiments demonstrate that PHG-DIF substantially mitigates the adverse effects of click noise and significantly improves headline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our framework implementation and dataset are available at https://github.com/liukejin-up/PHG-DIF.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks</title>
<link>https://arxiv.org/abs/2508.07179</link>
<guid>https://arxiv.org/abs/2508.07179</guid>
<content:encoded><![CDATA[
<div> Keywords: enterprise data pipelines, schema lineage extraction, language models, data reproducibility, semantic drift

Summary: 
- The paper proposes a framework for automated extraction of fine-grained schema lineage from multilingual enterprise pipeline scripts.
- It introduces the Schema Lineage Composite Evaluation (SLiCE) metric to assess lineage quality in terms of structural correctness and semantic fidelity.
- A benchmark of 1,700 manually annotated lineages from real-world industrial scripts is presented for evaluation.
- Experiments with language models ranging from small to large, including GPT-4o and GPT-4.1, show that performance in schema lineage extraction improves with model size and prompting techniques.
- A 32B open-source model can achieve comparable performance to the GPT series for schema lineage extraction, suggesting a scalable and cost-effective approach for deploying schema-aware agents in practical applications.

<br /><br />Summary: <div>
arXiv:2508.07179v1 Announce Type: new 
Abstract: Enterprise data pipelines, characterized by complex transformations across multiple programming languages, often cause a semantic disconnect between original metadata and downstream data. This "semantic drift" compromises data reproducibility and governance, and impairs the utility of services like retrieval-augmented generation (RAG) and text-to-SQL systems. To address this, a novel framework is proposed for the automated extraction of fine-grained schema lineage from multilingual enterprise pipeline scripts. This method identifies four key components: source schemas, source tables, transformation logic, and aggregation operations, creating a standardized representation of data transformations. For the rigorous evaluation of lineage quality, this paper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that assesses both structural correctness and semantic fidelity. A new benchmark is also presented, comprising 1,700 manually annotated lineages from real-world industrial scripts. Experiments were conducted with 12 language models, from 1.3B to 32B small language models (SLMs) to large language models (LLMs) like GPT-4o and GPT-4.1. The results demonstrate that the performance of schema lineage extraction scales with model size and the sophistication of prompting techniques. Specially, a 32B open-source model, using a single reasoning trace, can achieve performance comparable to the GPT series under standard prompting. This finding suggests a scalable and economical approach for deploying schema-aware agents in practical applications.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention</title>
<link>https://arxiv.org/abs/2508.07185</link>
<guid>https://arxiv.org/abs/2508.07185</guid>
<content:encoded><![CDATA[
<div> knowledge, large language models, DySK-Attn, dynamic external source, knowledge attention mechanism

Summary:
DySK-Attn is a novel framework designed to address the limitation of Large Language Models (LLMs) becoming quickly outdated due to their static knowledge. By integrating a dynamic Knowledge Graph (KG) that can be updated in real-time, DySK-Attn allows LLMs to efficiently incorporate new knowledge. The framework utilizes a sparse knowledge attention mechanism, enabling the LLM to selectively focus on relevant information from the KG while avoiding the computational costs of dense attention. Through experiments on time-sensitive question-answering tasks, DySK-Attn outperformed existing techniques in both factual accuracy and computational efficiency. This approach provides a scalable solution for keeping LLMs up-to-date with the rapidly changing world. 

<br /><br />Summary: <div>
arXiv:2508.07185v1 Announce Type: new 
Abstract: Large Language Models (LLMs) suffer from a critical limitation: their knowledge is static and quickly becomes outdated. Retraining these massive models is computationally prohibitive, while existing knowledge editing techniques can be slow and may introduce unforeseen side effects. To address this, we propose DySK-Attn, a novel framework that enables LLMs to efficiently integrate real-time knowledge from a dynamic external source. Our approach synergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated instantaneously. The core of our framework is a sparse knowledge attention mechanism, which allows the LLM to perform a coarse-to-fine grained search, efficiently identifying and focusing on a small, highly relevant subset of facts from the vast KG. This mechanism avoids the high computational cost of dense attention over the entire knowledge base and mitigates noise from irrelevant information. We demonstrate through extensive experiments on time-sensitive question-answering tasks that DySK-Attn significantly outperforms strong baselines, including standard Retrieval-Augmented Generation (RAG) and model editing techniques, in both factual accuracy for updated knowledge and computational efficiency. Our framework offers a scalable and effective solution for building LLMs that can stay current with the ever-changing world.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment</title>
<link>https://arxiv.org/abs/2508.07195</link>
<guid>https://arxiv.org/abs/2508.07195</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, time series forecasting, Heterogeneous Temporal Encoder, Semantic Alignment Module, TALON<br />
Summary:<br />
Large Language Models (LLMs) have shown remarkable performance in natural language processing. However, applying them to time series forecasting faces challenges due to temporal pattern heterogeneity and modality gaps. This work introduces TALON, a framework that tackles these issues. TALON includes a Heterogeneous Temporal Encoder that segments time series data for expert modeling and a Semantic Alignment Module that aligns features with LLM representations. Through extensive experiments on various benchmarks, TALON outperforms state-of-the-art methods, showcasing the effectiveness of incorporating pattern-aware and semantic-aware designs in adapting LLMs for time series forecasting.<br /> <div>
arXiv:2508.07195v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently demonstrated impressive capabilities in natural language processing due to their strong generalization and sequence modeling capabilities. However, their direct application to time series forecasting remains challenging due to two fundamental issues: the inherent heterogeneity of temporal patterns and the modality gap between continuous numerical signals and discrete language representations. In this work, we propose TALON, a unified framework that enhances LLM-based forecasting by modeling temporal heterogeneity and enforcing semantic alignment. Specifically, we design a Heterogeneous Temporal Encoder that partitions multivariate time series into structurally coherent segments, enabling localized expert modeling across diverse temporal patterns. To bridge the modality gap, we introduce a Semantic Alignment Module that aligns temporal features with LLM-compatible representations, enabling effective integration of time series into language-based models while eliminating the need for handcrafted prompts during inference. Extensive experiments on seven real-world benchmarks demonstrate that TALON achieves superior performance across all datasets, with average MSE improvements of up to 11\% over recent state-of-the-art methods. These results underscore the effectiveness of incorporating both pattern-aware and semantic-aware designs when adapting LLMs for time series forecasting. The code is available at: https://github.com/syrGitHub/TALON.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model</title>
<link>https://arxiv.org/abs/2508.07209</link>
<guid>https://arxiv.org/abs/2508.07209</guid>
<content:encoded><![CDATA[
<div> Keywords: pretrained language models, social media, rumor detection, post engagement prediction, Twitter corpus<br />
<br />Summary: 
The article introduces a novel continue pretraining strategy named Post Engagement Prediction (PEP) to improve the performance of Pretrained Language Models (PLMs) on social media tasks like rumor detection. By predicting root, branch, and parent relations between posts, PEP captures interactions of stance and sentiment important for detecting rumors. The authors also release a large-scale Twitter corpus, TwitterCorpus, and two unlabeled claim conversation datasets with propagation structures. Using these resources and the PEP strategy, they train a Twitter-specific PLM called SoLM. Experimental results show that PEP significantly enhances rumor detection performance, even in few-shot scenarios, outperforming existing methods on multiple datasets. SoLM alone achieves competitive results, demonstrating the effectiveness of the strategy in learning post interaction features. <br /><br />Summary: <div>
arXiv:2508.07209v1 Announce Type: new 
Abstract: Pretrained Language Models (PLMs) have excelled in various Natural Language Processing tasks, benefiting from large-scale pretraining and self-attention mechanism's ability to capture long-range dependencies. However, their performance on social media application tasks like rumor detection remains suboptimal. We attribute this to mismatches between pretraining corpora and social texts, inadequate handling of unique social symbols, and pretraining tasks ill-suited for modeling user engagements implicit in propagation structures. To address these issues, we propose a continue pretraining strategy called Post Engagement Prediction (PEP) to infuse information from propagation structures into PLMs. PEP makes models to predict root, branch, and parent relations between posts, capturing interactions of stance and sentiment crucial for rumor detection. We also curate and release large-scale Twitter corpus: TwitterCorpus (269GB text), and two unlabeled claim conversation datasets with propagation structures (UTwitter and UWeibo). Utilizing these resources and PEP strategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments demonstrate PEP significantly boosts rumor detection performance across universal and social media PLMs, even in few-shot scenarios. On benchmark datasets, PEP enhances baseline models by 1.0-3.7\% accuracy, even enabling it to outperform current state-of-the-art methods on multiple datasets. SoLM alone, without high-level modules, also achieves competitive results, highlighting the strategy's effectiveness in learning discriminative post interaction features.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Does a Deep Neural Network Look at Lexical Stress?</title>
<link>https://arxiv.org/abs/2508.07229</link>
<guid>https://arxiv.org/abs/2508.07229</guid>
<content:encoded><![CDATA[
<div> dataset, Convolutional Neural Network, interpretability analysis, Layerwise Relevance Propagation, phonetic work 

Summary:
The study explores the interpretability of neural networks in predicting lexical stress, focusing on English disyllabic words. Multiple Convolutional Neural Network (CNN) models achieved high accuracy in predicting stress positions from spectrographic representations. Layerwise Relevance Propagation (LRP) analysis revealed that stressed syllables, particularly the spectral properties of stressed vowels, played a crucial role in predictions. The classifiers also considered information throughout the word, indicating a distributed cue acquisition capability. A feature-specific relevance analysis highlighted the influence of stressed vowel formants, pitch, and third formant on the best-performing classifier. These findings demonstrate the deep learning model's proficiency in extracting stress cues from natural speech data, advancing traditional phonetic research that typically relies on controlled stimuli.<br /><br />Summary: <div>
arXiv:2508.07229v1 Announce Type: new 
Abstract: Despite their success in speech processing, neural networks often operate as black boxes, prompting the question: what informs their decisions, and how can we interpret them? This work examines this issue in the context of lexical stress. A dataset of English disyllabic words was automatically constructed from read and spontaneous speech. Several Convolutional Neural Network (CNN) architectures were trained to predict stress position from a spectrographic representation of disyllabic words lacking minimal stress pairs (e.g., initial stress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out test data. Layerwise Relevance Propagation (LRP), a technique for CNN interpretability analysis, revealed that predictions for held-out minimal pairs (PROtest vs. proTEST ) were most strongly influenced by information in stressed versus unstressed syllables, particularly the spectral properties of stressed vowels. However, the classifiers also attended to information throughout the word. A feature-specific relevance analysis is proposed, and its results suggest that our best-performing classifier is strongly influenced by the stressed vowel's first and second formants, with some evidence that its pitch and third formant also contribute. These results reveal deep learning's ability to acquire distributed cues to stress from naturally occurring data, extending traditional phonetic work based around highly controlled stimuli.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition</title>
<link>https://arxiv.org/abs/2508.07248</link>
<guid>https://arxiv.org/abs/2508.07248</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge distillation, Continual Learning Named Entity Recognition, Few-Shot CLNER, Prompt Tuning, Memory Demonstration Templates

Summary: 
Knowledge distillation has been effectively utilized in Continual Learning Named Entity Recognition tasks, using a teacher model to distill old-class entities from new-class data to prevent forgetting. In Few-Shot CLNER tasks, limited new-class entities pose a challenge in model generalization during inference, leading to the Few-Shot Distillation Dilemma. To address this, a prompt tuning paradigm called Anchor words-oriented Prompt Tuning (APT) bridges pre-training and fine-tuning for improved few-shot performance. Additionally, Memory Demonstration Templates (MDT) are integrated to provide replay samples from previous tasks, tackling the distillation dilemma and promoting in-context learning. Experimental results demonstrate competitive performance in Few-Shot CLNER tasks with the proposed approach.<br /><br />Summary: <div>
arXiv:2508.07248v1 Announce Type: new 
Abstract: Knowledge distillation has been successfully applied to Continual Learning Named Entity Recognition (CLNER) tasks, by using a teacher model trained on old-class data to distill old-class entities present in new-class data as a form of regularization, thereby avoiding catastrophic forgetting. However, in Few-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it difficult for the trained model to generalize during inference. More critically, the lack of old-class entity information hinders the distillation of old knowledge, causing the model to fall into what we refer to as the Few-Shot Distillation Dilemma. In this work, we address the above challenges through a prompt tuning paradigm and memory demonstration template strategy. Specifically, we designed an expandable Anchor words-oriented Prompt Tuning (APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby enhancing performance in few-shot scenarios. Additionally, we incorporated Memory Demonstration Templates (MDT) into each training instance to provide replay samples from previous tasks, which not only avoids the Few-Shot Distillation Dilemma but also promotes in-context learning. Experiments show that our approach achieves competitive performances on FS-CLNER.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation</title>
<link>https://arxiv.org/abs/2508.07262</link>
<guid>https://arxiv.org/abs/2508.07262</guid>
<content:encoded><![CDATA[
<div> Dynamic articulatory model, Palatal dome, Tongue-palate contact areas, Electropalatography, Speech science education<br />
Summary:<br />
This paper presents an extension of the 2D dynamic articulatory model DYNARTmo by incorporating a 3D representation of the palatal dome to estimate tongue-palate contact areas from tongue contours. The model includes two dome geometries for lateral curvature variation and calculates lateral contact points for different tongue positions, enabling electropalatography-like visualizations. The enhanced model offers synchronized sagittal, glottal, and palatal views for static and dynamic articulation displays, beneficial for speech science education and therapy. Future work aims to add a facial view and implement articulatory-to-acoustic synthesis for a more realistic model evaluation. <div>
arXiv:2508.07262v1 Announce Type: new 
Abstract: This paper describes an extension of the two-dimensional dynamic articulatory model DYNARTmo by integrating an internal three-dimensional representation of the palatal dome to estimate tongue-palate contact areas from midsagittal tongue contours. Two alternative dome geometries - a half-ellipse and a cosine based profile - are implemented to model lateral curvature in the coronal plane. Using these geometries, lateral contact points are analytically computed for each anterior-posterior position, enabling the generation of electropalatography-like visualizations within the 2D+ framework. The enhanced model supports three synchronized views (sagittal, glottal, and palatal) for static and dynamic (animated) articulation displays, suitable for speech science education and speech therapy. Future work includes adding a facial (lip) view and implementing articulatory-to-acoustic synthesis to quantitatively evaluate model realism.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models</title>
<link>https://arxiv.org/abs/2508.07273</link>
<guid>https://arxiv.org/abs/2508.07273</guid>
<content:encoded><![CDATA[
<div> Keywords: Speech language models, Empathetic reasoning, Paralinguistic cues, Contextual understanding, Training data<br />
Summary:<br />
- The study addresses the limitations of current large speech language models (Speech-LLMs) in empathetic reasoning due to the lack of training datasets incorporating paralinguistic cues.
- Two approaches are proposed to enhance contextual paralinguistic understanding in model training: an explicit method providing paralinguistic metadata directly to the LLM and an implicit method generating novel training QA pairs using emotion annotations alongside speech transcriptions.
- The implicit method significantly improves LLM performance on a human-annotated QA benchmark by 38.41% and achieves a 46.02% improvement when combined with the explicit approach.
- The effectiveness of the proposed methods in enhancing contextual paralinguistic understanding is demonstrated, highlighting the importance of incorporating paralinguistic information in training datasets.
- The reliability of the LLM judge is validated through its correlation with classification metrics, supporting its utility in evaluating model performance. <br /><br />Summary: <div>
arXiv:2508.07273v1 Announce Type: new 
Abstract: Current large speech language models (Speech-LLMs) often exhibit limitations in empathetic reasoning, primarily due to the absence of training datasets that integrate both contextual content and paralinguistic cues. In this work, we propose two approaches to incorporate contextual paralinguistic information into model training: (1) an explicit method that provides paralinguistic metadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit method that automatically generates novel training question-answer (QA) pairs using both categorical and dimensional emotion annotations alongside speech transcriptions. Our implicit method boosts performance (LLM-judged) by 38.41% on a human-annotated QA benchmark, reaching 46.02% when combined with the explicit approach, showing effectiveness in contextual paralinguistic understanding. We also validate the LLM judge by demonstrating its correlation with classification metrics, providing support for its reliability.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory</title>
<link>https://arxiv.org/abs/2508.07279</link>
<guid>https://arxiv.org/abs/2508.07279</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, mental health assessment, adaptive question-asking framework, multi-outcome modeling, item response theory

Summary:
MAQuA is introduced as an adaptive question-asking framework for mental health screening using large language models. It optimizes diagnostic information by selecting the most informative questions across multiple dimensions. Empirical results show that MAQuA significantly reduces the number of assessment questions needed for stable scores, with reductions ranging from 50-87% compared to random ordering. It performs well across internalizing and externalizing domains, including depression, anxiety, substance use, and eating disorders. Early stopping strategies further reduce patient time and burden. MAQuA is a powerful and efficient tool for interactive mental health screening, enhancing the integration of large language model-based agents into clinical workflows.<br /><br />Summary: MAQuA optimizes mental health screening by selecting informative questions, shows significant reductions in assessment question number, performs well across various domains, and reduces patient burden. <div>
arXiv:2508.07279v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) offer new opportunities for scalable, interactive mental health assessment, but excessive querying by LLMs burdens users and is inefficient for real-world screening across transdiagnostic symptom profiles. We introduce MAQuA, an adaptive question-asking framework for simultaneous, multidimensional mental health screening. Combining multi-outcome modeling on language responses with item response theory (IRT) and factor analysis, MAQuA selects the questions with most informative responses across multiple dimensions at each turn to optimize diagnostic information, improving accuracy and potentially reducing response burden. Empirical results on a novel dataset reveal that MAQuA reduces the number of assessment questions required for score stabilization by 50-87% compared to random ordering (e.g., achieving stable depression scores with 71% fewer questions and eating disorder scores with 85% fewer questions). MAQuA demonstrates robust performance across both internalizing (depression, anxiety) and externalizing (substance use, eating disorder) domains, with early stopping strategies further reducing patient time and burden. These findings position MAQuA as a powerful and efficient tool for scalable, nuanced, and interactive mental health screening, advancing the integration of LLM-based agents into real-world clinical workflows.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas</title>
<link>https://arxiv.org/abs/2508.07284</link>
<guid>https://arxiv.org/abs/2508.07284</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, moral reasoning processes, trolley problem scenarios, ethical frames, decisional assertiveness

Summary: 
This study evaluates 14 large language models (LLMs) on their moral reasoning abilities across 27 trolley problem scenarios based on ten moral philosophies. The models were tested on decisional assertiveness, explanation answer consistency, public moral alignment, and sensitivity to ethically irrelevant cues. While reasoning-enhanced models showed greater decisiveness and structured justifications, they did not always align with human consensus. Models performed best in altruistic, fairness, and virtue ethics framings, but struggled with kinship, legality, and self-interest frames. Moral prompting serves as a diagnostic tool for uncovering alignment philosophies in LLMs. The study calls for standardized benchmarks to assess not just the decisions of LLMs, but also the how and why behind them. 

<br /><br />Summary: <div>
arXiv:2508.07284v1 Announce Type: new 
Abstract: As large language models (LLMs) increasingly mediate ethically sensitive decisions, understanding their moral reasoning processes becomes imperative. This study presents a comprehensive empirical evaluation of 14 leading LLMs, both reasoning enabled and general purpose, across 27 diverse trolley problem scenarios, framed by ten moral philosophies, including utilitarianism, deontology, and altruism. Using a factorial prompting protocol, we elicited 3,780 binary decisions and natural language justifications, enabling analysis along axes of decisional assertiveness, explanation answer consistency, public moral alignment, and sensitivity to ethically irrelevant cues. Our findings reveal significant variability across ethical frames and model types: reasoning enhanced models demonstrate greater decisiveness and structured justifications, yet do not always align better with human consensus. Notably, "sweet zones" emerge in altruistic, fairness, and virtue ethics framings, where models achieve a balance of high intervention rates, low explanation conflict, and minimal divergence from aggregated human judgments. However, models diverge under frames emphasizing kinship, legality, or self interest, often producing ethically controversial outcomes. These patterns suggest that moral prompting is not only a behavioral modifier but also a diagnostic tool for uncovering latent alignment philosophies across providers. We advocate for moral reasoning to become a primary axis in LLM alignment, calling for standardized benchmarks that evaluate not just what LLMs decide, but how and why.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking</title>
<link>https://arxiv.org/abs/2508.07286</link>
<guid>https://arxiv.org/abs/2508.07286</guid>
<content:encoded><![CDATA[
<div> Keywords: named entity recognition, specialized texts, architecture, engineering, construction, large language models

Summary: 
ARCE introduces a novel approach for named entity recognition in specialized texts within the architecture, engineering, and construction domain. It addresses the challenges of domain gap and complex terminologies by leveraging large language models. The approach involves generating a corpus of simple explanations, termed Cote, using an LLM and then incrementally pre-training a RoBERTa model with this corpus before fine-tuning for the downstream task. ARCE achieves a new state-of-the-art performance on an AEC dataset with a Macro-F1 score of 77.20%. Surprisingly, the study finds that simple explanation-based knowledge is more effective than complex role-based rationales for this task. The publicly available code for ARCE can be accessed at https://github.com/nxcc-lab/ARCE.

<br /><br />Summary: <div>
arXiv:2508.07286v1 Announce Type: new 
Abstract: Accurate information extraction from specialized texts is a critical challenge, particularly for named entity recognition (NER) in the architecture, engineering, and construction (AEC) domain to support automated rule checking (ARC). The performance of standard pre-trained models is often constrained by the domain gap, as they struggle to interpret the specialized terminology and complex relational contexts inherent in AEC texts. Although this issue can be mitigated by further pre-training on large, human-curated domain corpora, as exemplified by methods like ARCBERT, this approach is both labor-intensive and cost-prohibitive. Consequently, leveraging large language models (LLMs) for automated knowledge generation has emerged as a promising alternative. However, the optimal strategy for generating knowledge that can genuinely enhance smaller, efficient models remains an open question. To address this, we propose ARCE (augmented RoBERTa with contextualized elucidations), a novel approach that systematically explores and optimizes this generation process. ARCE employs an LLM to first generate a corpus of simple, direct explanations, which we term Cote, and then uses this corpus to incrementally pre-train a RoBERTa model prior to its fine-tuning on the downstream task. Our extensive experiments show that ARCE establishes a new state-of-the-art on a benchmark AEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a key finding: simple, explanation-based knowledge proves surprisingly more effective than complex, role-based rationales for this task. The code is publicly available at:https://github.com/nxcc-lab/ARCE.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation</title>
<link>https://arxiv.org/abs/2508.07295</link>
<guid>https://arxiv.org/abs/2508.07295</guid>
<content:encoded><![CDATA[
<div> benchmark, multilingual, factuality, large language models, transfer learning

Summary:<br /><br />This study introduces the CCFQA benchmark to evaluate the factuality of Multimodal Large Language Models (MLLMs) across different languages and modalities, focusing on speech understanding capabilities. Current MLLMs face challenges on the CCFQA benchmark, highlighting the need for improved models in multilingual contexts. The study also presents a few-shot transfer learning strategy that effectively transfers Question Answering capabilities from English to multilingual Spoken Question Answering tasks. This strategy enables competitive performance with minimal training data, showcasing the potential for enhanced speech understanding in MLLMs. The release of CCFQA dataset and code aims to facilitate the development of more robust and reliable MLLMs with improved speech comprehension capabilities.<br /><br />Summary: <div>
arXiv:2508.07295v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are increasingly popularized in the multilingual world, ensuring hallucination-free factuality becomes markedly crucial. However, existing benchmarks for evaluating the reliability of Multimodal Large Language Models (MLLMs) predominantly focus on textual or visual modalities with a primary emphasis on English, which creates a gap in evaluation when processing multilingual input, especially in speech. To bridge this gap, we propose a novel \textbf{C}ross-lingual and \textbf{C}ross-modal \textbf{F}actuality benchmark (\textbf{CCFQA}). Specifically, the CCFQA benchmark contains parallel speech-text factual questions across 8 languages, designed to systematically evaluate MLLMs' cross-lingual and cross-modal factuality capabilities. Our experimental results demonstrate that current MLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we propose a few-shot transfer learning strategy that effectively transfers the Question Answering (QA) capabilities of LLMs in English to multilingual Spoken Question Answering (SQA) tasks, achieving competitive performance with GPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a foundational research resource to promote the development of MLLMs with more robust and reliable speech understanding capabilities. Our code and dataset are available at https://github.com/yxduir/ccfqa.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways</title>
<link>https://arxiv.org/abs/2508.07308</link>
<guid>https://arxiv.org/abs/2508.07308</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, medical Question-Answering, Large Language Models, reasoning, HealthBranches <br />
<br />
Summary: 

HealthBranches is a new benchmark dataset designed for evaluating complex reasoning in Large Language Models (LLMs) in the field of medical Question-Answering (Q&amp;A). The dataset consists of 4,063 case studies covering 17 healthcare topics, each based on clinically validated reasoning pathways. It includes patient cases, questions, answers, and the full reasoning path for each Q&amp;A, supporting open-ended and multiple-choice question formats. The structured design of HealthBranches enables robust evaluation of LLMs' multi-step inference capabilities, particularly in structured Retrieval-Augmented Generation (RAG) contexts. By providing a foundation for developing more trustworthy and interpretable LLMs in high-stakes domains, HealthBranches also serves as a valuable resource for educational purposes. <div>
arXiv:2508.07308v1 Announce Type: new 
Abstract: HealthBranches is a novel benchmark dataset for medical Question-Answering (Q&amp;A), specifically designed to evaluate complex reasoning in Large Language Models (LLMs). This dataset is generated through a semi-automated pipeline that transforms explicit decision pathways from medical source into realistic patient cases with associated questions and answers. Covering 4,063 case studies across 17 healthcare topics, each data point is based on clinically validated reasoning chains. HealthBranches supports both open-ended and multiple-choice question formats and uniquely includes the full reasoning path for each Q&amp;A. Its structured design enables robust evaluation of LLMs' multi-step inference capabilities, including their performance in structured Retrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a foundation for the development of more trustworthy, interpretable, and clinically reliable LLMs in high-stakes domains while also serving as a valuable resource for educational purposes.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering</title>
<link>https://arxiv.org/abs/2508.07321</link>
<guid>https://arxiv.org/abs/2508.07321</guid>
<content:encoded><![CDATA[
<div> Named-Entity Indirection, Distractor Indirection, Contextual Overload, ObfusQAte, Large Language Models

Summary:
ObfusQAte introduces ObfusQA, a novel framework to evaluate Large Language Models (LLMs) when faced with obfuscated questions. The framework includes three obfuscation levels: Named-Entity Indirection, Distractor Indirection, and Contextual Overload. Through these obfuscation techniques, LLMs are tested for their robustness and adaptability in factual question-answering tasks. The study reveals that LLMs often struggle or produce inaccurate responses when presented with nuanced variations in questions. The ObfusQA framework provides a comprehensive benchmark for assessing LLM performance across multiple dimensions. The findings highlight the limitations of current LLMs in understanding and responding to obfuscated questions, suggesting the need for further research in this area. The publicly available ObfusQAte tool aims to stimulate further exploration and improvement in LLM capabilities. 

<br /><br />Summary: <div>
arXiv:2508.07321v1 Announce Type: new 
Abstract: The rapid proliferation of Large Language Models (LLMs) has significantly contributed to the development of equitable AI systems capable of factual question-answering (QA). However, no known study tests the LLMs' robustness when presented with obfuscated versions of questions. To systematically evaluate these limitations, we propose a novel technique, ObfusQAte and, leveraging the same, introduce ObfusQA, a comprehensive, first of its kind, framework with multi-tiered obfuscation levels designed to examine LLM capabilities across three distinct dimensions: (i) Named-Entity Indirection, (ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these fine-grained distinctions in language, ObfusQA provides a comprehensive benchmark for evaluating LLM robustness and adaptability. Our study observes that LLMs exhibit a tendency to fail or generate hallucinated responses when confronted with these increasingly nuanced variations. To foster research in this direction, we make ObfusQAte publicly available.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategies of Code-switching in Human-Machine Dialogs</title>
<link>https://arxiv.org/abs/2508.07325</link>
<guid>https://arxiv.org/abs/2508.07325</guid>
<content:encoded><![CDATA[
<div> code-switching, bilingual language use, chatbot, multilinguals, Map Task

Summary: 
Participants engaged in code-switching tasks with a chatbot that alternated between Spanish and English. The study explored various code-switching strategies and their effects on participants' enjoyment and success in completing the task. When code-switching was consistent and grammatical, participants had a positive experience and performed well. However, when code-switching was random or involved ungrammatical constructions, such as incongruent mixed-language noun phrases, participants found the task challenging and less enjoyable. These findings highlight the importance of developing advanced multilingual language technology to support meaningful interactions with users. Additionally, the study demonstrated the potential of using chatbots for research on bilingual language use, offering insights into how individuals navigate and adapt to code-switching in conversational settings. <div>
arXiv:2508.07325v1 Announce Type: new 
Abstract: Most people are multilingual, and most multilinguals code-switch, yet the characteristics of code-switched language are not fully understood. We developed a chatbot capable of completing a Map Task with human participants using code-switched Spanish and English. In two experiments, we prompted the bot to code-switch according to different strategies, examining (1) the feasibility of such experiments for investigating bilingual language use, and (2) whether participants would be sensitive to variations in discourse and grammatical patterns. Participants generally enjoyed code-switching with our bot as long as it produced predictable code-switching behavior; when code-switching was random or ungrammatical (as when producing unattested incongruent mixed-language noun phrases, such as `la fork'), participants enjoyed the task less and were less successful at completing it. These results underscore the potential downsides of deploying insufficiently developed multilingual language technology, while also illustrating the promise of such technology for conducting research on bilingual language use.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance</title>
<link>https://arxiv.org/abs/2508.07375</link>
<guid>https://arxiv.org/abs/2508.07375</guid>
<content:encoded><![CDATA[
<div> Full-Duplex Speech Language Models, FD-SLMs, End-to-end, TurnGuide, conversational planning <br />
Summary: <br />
The article introduces TurnGuide, a novel approach for improving the conversational abilities of Full-Duplex Speech Language Models (FD-SLMs) in real-time spoken interactions. FD-SLMs face challenges with degraded conversational abilities in comparison to text conversations due to limited high-quality spoken dialogue data. TurnGuide addresses these challenges by segmenting assistant speech into dialogue turns and generating turn-level text guidance before speech output. This approach resolves timing and length issues, enhancing the natural flow of interactions. Experimental results show that TurnGuide significantly improves the semantic coherence and naturalness of speech generated by e2e FD-SLMs. The proposed method enables FD-SLMs to produce meaningful and coherent speech in two-speaker dialogues, enhancing their conversational capabilities for human-like interactions. Demos of the approach are available online, and the code will be made accessible on GitHub. <div>
arXiv:2508.07375v1 Announce Type: new 
Abstract: Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation models designed to enable natural, real-time spoken interactions by modeling complex conversational dynamics such as interruptions, backchannels, and overlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world double-channel conversational data to capture nuanced two-speaker dialogue patterns for human-like interactions. However, they face a critical challenge -- their conversational abilities often degrade compared to pure-text conversation due to prolonged speech sequences and limited high-quality spoken dialogue data. While text-guided speech generation could mitigate these issues, it suffers from timing and length issues when integrating textual guidance into double-channel audio streams, disrupting the precise time alignment essential for natural interactions. To address these challenges, we propose TurnGuide, a novel planning-inspired approach that mimics human conversational planning by dynamically segmenting assistant speech into dialogue turns and generating turn-level text guidance before speech output, which effectively resolves both insertion timing and length challenges. Extensive experiments demonstrate our approach significantly improves e2e FD-SLMs' conversational abilities, enabling them to generate semantically meaningful and coherent speech while maintaining natural conversational flow. Demos are available at https://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at https://github.com/dreamtheater123/TurnGuide.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Multilingual Multimodal LLMs With Cultural Knowledge</title>
<link>https://arxiv.org/abs/2508.07414</link>
<guid>https://arxiv.org/abs/2508.07414</guid>
<content:encoded><![CDATA[
<div> Knowledge Graph, Multimodal Large Language Models, CulturalGround dataset, Visual Question Answering, CulturalPangea model

Summary: 
The article discusses a new approach to enhancing the performance of Multimodal Large Language Models (MLLMs) in understanding long-tail cultural entities and low-resource languages. By leveraging a large knowledge graph from Wikidata, the authors collect images representing culturally significant entities to create a new dataset called CulturalGround. This dataset includes 22 million high-quality visual question answering pairs across 42 countries and 39 languages. They train an open-source MLLM called CulturalPangea on this dataset, achieving state-of-the-art performance on culture-focused multilingual multimodal benchmarks. Their approach allows for narrowing the cultural gap in MLLMs and offers a practical path towards globally inclusive multimodal systems. <div>
arXiv:2508.07414v1 Announce Type: new 
Abstract: Multimodal Large Language Models excel in high-resource settings, but often misinterpret long-tail cultural entities and underperform in low-resource languages. To address this gap, we propose a data-centric approach that directly grounds MLLMs in cultural knowledge. Leveraging a large scale knowledge graph from Wikidata, we collect images that represent culturally significant entities, and generate synthetic multilingual visual question answering data. The resulting dataset, CulturalGround, comprises 22 million high-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages. We train an open-source MLLM CulturalPangea on CulturalGround, interleaving standard multilingual instruction-tuning data to preserve general abilities. CulturalPangea achieves state-of-the-art performance among open models on various culture-focused multilingual multimodal benchmarks, outperforming prior models by an average of 5.0 without degrading results on mainstream vision-language tasks. Our findings show that our targeted, culturally grounded approach could substantially narrow the cultural gap in MLLMs and offer a practical path towards globally inclusive multimodal systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs</title>
<link>https://arxiv.org/abs/2508.07434</link>
<guid>https://arxiv.org/abs/2508.07434</guid>
<content:encoded><![CDATA[
arXiv:2508.07434v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with inference-time scaling techniques show promise for code generation, yet face notable efficiency and scalability challenges. Construction-based tree-search methods suffer from rapid growth in tree size, high token consumption, and lack of anytime property. In contrast, improvement-based methods offer better performance but often struggle with uninformative reward signals and inefficient search strategies. In this work, we propose \textbf{ReLoc}, a unified local search framework which effectively performs step-by-step code revision. Specifically, ReLoc explores a series of local revisions through four key algorithmic components: initial code drafting, neighborhood code generation, candidate evaluation, and incumbent code updating, each of which can be instantiated with specific decision rules to realize different local search algorithms such as Hill Climbing (HC) or Genetic Algorithm (GA). Furthermore, we develop a specialized revision reward model that evaluates code quality based on revision distance to produce fine-grained preferences that guide the local search toward more promising candidates. Finally, our extensive experimental results demonstrate that our approach achieves superior performance across diverse code generation tasks, significantly outperforming both construction-based tree search as well as the state-of-the-art improvement-based code generation methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Positional Biases Shift as Inputs Approach Context Window Limits</title>
<link>https://arxiv.org/abs/2508.07479</link>
<guid>https://arxiv.org/abs/2508.07479</guid>
<content:encoded><![CDATA[
arXiv:2508.07479v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often struggle to use information across long inputs effectively. Prior work has identified positional biases, such as the Lost in the Middle (LiM) effect, where models perform better when information appears at the beginning (primacy bias) or end (recency bias) of the input, rather than in the middle. However, long-context studies have not consistently replicated these effects, raising questions about their intensity and the conditions under which they manifest. To address this, we conducted a comprehensive analysis using relative rather than absolute input lengths, defined with respect to each model's context window. Our findings reveal that the LiM effect is strongest when inputs occupy up to 50% of a model's context window. Beyond that, the primacy bias weakens, while recency bias remains relatively stable. This effectively eliminates the LiM effect; instead, we observe a distance-based bias, where model performance is better when relevant information is closer to the end of the input. Furthermore, our results suggest that successful retrieval is a prerequisite for reasoning in LLMs, and that the observed positional biases in reasoning are largely inherited from retrieval. These insights have implications for long-context tasks, the design of future LLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models</title>
<link>https://arxiv.org/abs/2508.07484</link>
<guid>https://arxiv.org/abs/2508.07484</guid>
<content:encoded><![CDATA[
arXiv:2508.07484v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable performance across a wide range of natural language processing tasks. Quality Estimation (QE) for Machine Translation (MT), which assesses the quality of a source-target pair without relying on reference translations, remains a challenging cross-lingual task for LLMs. The challenges stem from the inherent limitations of existing LLM-based QE systems, which are pre-trained for causal language modelling rather than regression-specific tasks, further elevated by the presence of low-resource languages given pre-training data distribution. This paper introduces ALOPE, an adaptive layer-optimization framework designed to enhance LLM-based QE by restructuring Transformer representations through layer-wise adaptation for improved regression-based prediction. Our framework integrates low-rank adapters (LoRA) with regression task heads, leveraging selected pre-trained Transformer layers for improved cross-lingual alignment. In addition to the layer-specific adaptation, ALOPE introduces two strategies-dynamic weighting, which adaptively combines representations from multiple layers, and multi-head regression, which aggregates regression losses from multiple heads for QE. Our framework shows improvements over various existing LLM-based QE approaches. Empirical evidence suggests that intermediate Transformer layers in LLMs provide contextual representations that are more aligned with the cross-lingual nature of the QE task. We make resultant models and framework code publicly available for further research, also allowing existing LLM-based MT frameworks to be scaled with QE capabilities.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmenting Bias Detection in LLMs Using Topological Data Analysis</title>
<link>https://arxiv.org/abs/2508.07516</link>
<guid>https://arxiv.org/abs/2508.07516</guid>
<content:encoded><![CDATA[
arXiv:2508.07516v1 Announce Type: new 
Abstract: Recently, many bias detection methods have been proposed to determine the level of bias a large language model captures. However, tests to identify which parts of a large language model are responsible for bias towards specific groups remain underdeveloped. In this study, we present a method using topological data analysis to identify which heads in GPT-2 contribute to the misrepresentation of identity groups present in the StereoSet dataset. We find that biases for particular categories, such as gender or profession, are concentrated in attention heads that act as hot spots. The metric we propose can also be used to determine which heads capture bias for a specific group within a bias category, and future work could extend this method to help de-bias large language models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews</title>
<link>https://arxiv.org/abs/2508.07517</link>
<guid>https://arxiv.org/abs/2508.07517</guid>
<content:encoded><![CDATA[
arXiv:2508.07517v1 Announce Type: new 
Abstract: Word clouds are a common way to summarize qualitative interviews, yet traditional frequency-based methods often fail in conversational contexts: they surface filler words, ignore paraphrase, and fragment semantically related ideas. This limits their usefulness in early-stage analysis, when researchers need fast, interpretable overviews of what participant actually said. We introduce ThemeClouds, an open-source visualization tool that uses large language models (LLMs) to generate thematic, participant-weighted word clouds from dialogue transcripts. The system prompts an LLM to identify concept-level themes across a corpus and then counts how many unique participants mention each topic, yielding a visualization grounded in breadth of mention rather than raw term frequency. Researchers can customize prompts and visualization parameters, providing transparency and control. Using interviews from a user study comparing five recording-device configurations (31 participants; 155 transcripts, Whisper ASR), our approach surfaces more actionable device concerns than frequency clouds and topic-modeling baselines (e.g., LDA, BERTopic). We discuss design trade-offs for integrating LLM assistance into qualitative workflows, implications for interpretability and researcher agency, and opportunities for interactive analyses such as per-condition contrasts (``diff clouds'').
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR</title>
<link>https://arxiv.org/abs/2508.07534</link>
<guid>https://arxiv.org/abs/2508.07534</guid>
<content:encoded><![CDATA[
arXiv:2508.07534v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based feedback to guide LLMs in generating and refining complex reasoning chains -- a process critically dependent on effective exploration strategies. While prior work has demonstrated RLVR's empirical success, the fundamental mechanisms governing LLMs' exploration behaviors remain underexplored. This technical report presents a systematic investigation of exploration capacities in RLVR, covering four main aspects: (1) exploration space shaping, where we develop quantitative metrics to characterize LLMs' capability boundaries; (2) entropy-performance exchange, analyzed across training stages, individual instances, and token-level patterns; and (3) RL performance optimization, examining methods to effectively translate exploration gains into measurable improvements. By unifying previously identified insights with new empirical evidence, this work aims to provide a foundational framework for advancing RLVR systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IBPS: Indian Bail Prediction System</title>
<link>https://arxiv.org/abs/2508.07592</link>
<guid>https://arxiv.org/abs/2508.07592</guid>
<content:encoded><![CDATA[
arXiv:2508.07592v1 Announce Type: new 
Abstract: Bail decisions are among the most frequently adjudicated matters in Indian courts, yet they remain plagued by subjectivity, delays, and inconsistencies. With over 75% of India's prison population comprising undertrial prisoners, many from socioeconomically disadvantaged backgrounds, the lack of timely and fair bail adjudication exacerbates human rights concerns and contributes to systemic judicial backlog. In this paper, we present the Indian Bail Prediction System (IBPS), an AI-powered framework designed to assist in bail decision-making by predicting outcomes and generating legally sound rationales based solely on factual case attributes and statutory provisions. We curate and release a large-scale dataset of 150,430 High Court bail judgments, enriched with structured annotations such as age, health, criminal history, crime category, custody duration, statutes, and judicial reasoning. We fine-tune a large language model using parameter-efficient techniques and evaluate its performance across multiple configurations, with and without statutory context, and with RAG. Our results demonstrate that models fine-tuned with statutory knowledge significantly outperform baselines, achieving strong accuracy and explanation quality, and generalize well to a test set independently annotated by legal experts. IBPS offers a transparent, scalable, and reproducible solution to support data-driven legal assistance, reduce bail delays, and promote procedural fairness in the Indian judicial system.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements</title>
<link>https://arxiv.org/abs/2508.07598</link>
<guid>https://arxiv.org/abs/2508.07598</guid>
<content:encoded><![CDATA[
arXiv:2508.07598v1 Announce Type: new 
Abstract: Although the LLM-based in-context learning (ICL) paradigm has demonstrated considerable success across various natural language processing tasks, it encounters challenges in event detection. This is because LLMs lack an accurate understanding of event triggers and tend to make over-interpretation, which cannot be effectively corrected through in-context examples alone. In this paper, we focus on the most challenging one-shot setting and propose KeyCP++, a keyword-centric chain-of-thought prompting approach. KeyCP++ addresses the weaknesses of conventional ICL by automatically annotating the logical gaps between input text and detection results for the demonstrations. Specifically, to generate in-depth and meaningful rationale, KeyCP++ constructs a trigger discrimination prompting template. It incorporates the exemplary triggers (a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let LLM propose candidate triggers, and justify each candidate. These propose-and-judge rationales help LLMs mitigate over-reliance on the keywords and promote detection rule learning. Extensive experiments demonstrate the effectiveness of our approach, showcasing significant advancements in one-shot event detection.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information</title>
<link>https://arxiv.org/abs/2508.07630</link>
<guid>https://arxiv.org/abs/2508.07630</guid>
<content:encoded><![CDATA[
arXiv:2508.07630v1 Announce Type: new 
Abstract: We introduce InterChart, a diagnostic benchmark that evaluates how well vision-language models (VLMs) reason across multiple related charts, a task central to real-world applications such as scientific reporting, financial analysis, and public policy dashboards. Unlike prior benchmarks focusing on isolated, visually uniform charts, InterChart challenges models with diverse question types ranging from entity inference and trend correlation to numerical estimation and abstract multi-step reasoning grounded in 2-3 thematically or structurally related charts. We organize the benchmark into three tiers of increasing difficulty: (1) factual reasoning over individual charts, (2) integrative analysis across synthetically aligned chart sets, and (3) semantic inference over visually complex, real-world chart pairs. Our evaluation of state-of-the-art open and closed-source VLMs reveals consistent and steep accuracy declines as chart complexity increases. We find that models perform better when we decompose multi-entity charts into simpler visual units, underscoring their struggles with cross-chart integration. By exposing these systematic limitations, InterChart provides a rigorous framework for advancing multimodal reasoning in complex, multi-visual environments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval</title>
<link>https://arxiv.org/abs/2508.07690</link>
<guid>https://arxiv.org/abs/2508.07690</guid>
<content:encoded><![CDATA[
arXiv:2508.07690v1 Announce Type: new 
Abstract: Tool learning has emerged as a promising paradigm for large language models (LLMs) to solve many real-world tasks. Nonetheless, with the tool repository rapidly expanding, it is impractical to contain all tools within the limited input length of LLMs. To alleviate these issues, researchers have explored incorporating a tool retrieval module to select the most relevant tools or represent tools as unique tokens within LLM parameters. However, most state-of-the-art methods are under transductive settings, assuming all tools have been observed during training. Such a setting deviates from reality as the real-world tool repository is evolving and incorporates new tools frequently. When dealing with these unseen tools, which refer to tools not encountered during the training phase, these methods are limited by two key issues, including the large distribution shift and the vulnerability of similarity-based retrieval. To this end, inspired by human cognitive processes of mastering unseen tools through discovering and applying the logical information from prior experience, we introduce a novel Logic-Guided Semantic Bridging framework for inductive tool retrieval, namely, LoSemB, which aims to mine and transfer latent logical information for inductive tool retrieval without costly retraining. Specifically, LoSemB contains a logic-based embedding alignment module to mitigate distribution shifts and implements a relational augmented retrieval mechanism to reduce the vulnerability of similarity-based retrieval. Extensive experiments demonstrate that LoSemB achieves advanced performance in inductive settings while maintaining desirable effectiveness in the transductive setting.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction</title>
<link>https://arxiv.org/abs/2508.07702</link>
<guid>https://arxiv.org/abs/2508.07702</guid>
<content:encoded><![CDATA[
arXiv:2508.07702v1 Announce Type: new 
Abstract: Transformer-based models primarily rely on Next Token Prediction (NTP), which predicts the next token in a sequence based on the preceding context. However, NTP's focus on single-token prediction often limits a model's ability to plan ahead or maintain long-range coherence, raising questions about how well LLMs can predict longer contexts, such as full sentences within structured documents. While NTP encourages local fluency, it provides no explicit incentive to ensure global coherence across sentence boundaries-an essential skill for reconstructive or discursive tasks. To investigate this, we evaluate three commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on Masked Sentence Prediction (MSP) - the task of infilling a randomly removed sentence - from three domains: ROCStories (narrative), Recipe1M (procedural), and Wikipedia (expository). We assess both fidelity (similarity to the original sentence) and cohesiveness (fit within the surrounding context). Our key finding reveals that commercial LLMs, despite their superlative performance in other tasks, are poor at predicting masked sentences in low-structured domains, highlighting a gap in current model capabilities.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models</title>
<link>https://arxiv.org/abs/2508.07753</link>
<guid>https://arxiv.org/abs/2508.07753</guid>
<content:encoded><![CDATA[
arXiv:2508.07753v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success in various tasks, yet they remain vulnerable to faithfulness hallucinations, where the output does not align with the input. In this study, we investigate whether social bias contributes to these hallucinations, a causal relationship that has not been explored. A key challenge is controlling confounders within the context, which complicates the isolation of causality between bias states and hallucinations. To address this, we utilize the Structural Causal Model (SCM) to establish and validate the causality and design bias interventions to control confounders. In addition, we develop the Bias Intervention Dataset (BID), which includes various social biases, enabling precise measurement of causal effects. Experiments on mainstream LLMs reveal that biases are significant causes of faithfulness hallucinations, and the effect of each bias state differs in direction. We further analyze the scope of these causal effects across various models, specifically focusing on unfairness hallucinations, which are primarily targeted by social bias, revealing the subtle yet significant causal effect of bias on hallucination generation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation</title>
<link>https://arxiv.org/abs/2508.07781</link>
<guid>https://arxiv.org/abs/2508.07781</guid>
<content:encoded><![CDATA[
arXiv:2508.07781v1 Announce Type: new 
Abstract: This work proposes a grammar-based chunking strategy that segments input streams into semantically complete units by parsing dependency relations (e.g., noun phrase boundaries, verb-object structures) and punctuation features. The method ensures chunk coherence and minimizes semantic fragmentation. Building on this mechanism, we present SASST (Syntax-Aware Simultaneous Speech Translation), an end-to-end framework integrating frozen Whisper encoder and decoder-only LLM. The unified architecture dynamically outputs translation tokens or  symbols to jointly optimize translation timing and content, with target-side reordering addressing word-order divergence. Experiments on CoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation quality improvements across languages and validate the effectiveness of syntactic structures in LLM-driven SimulST systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts</title>
<link>https://arxiv.org/abs/2508.07785</link>
<guid>https://arxiv.org/abs/2508.07785</guid>
<content:encoded><![CDATA[
arXiv:2508.07785v1 Announce Type: new 
Abstract: The Mixture of Experts (MoE) architecture is a cornerstone of modern state-of-the-art (SOTA) large language models (LLMs). MoE models facilitate scalability by enabling sparse parameter activation. However, traditional MoE architecture uses homogeneous experts of a uniform size, activating a fixed number of parameters irrespective of input complexity and thus limiting computational efficiency. To overcome this limitation, we introduce Grove MoE, a novel architecture incorporating experts of varying sizes, inspired by the heterogeneous big.LITTLE CPU architecture. This architecture features novel adjugate experts with a dynamic activation mechanism, enabling model capacity expansion while maintaining manageable computational overhead. Building on this architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model during mid-training and post-training. GroveMoE models dynamically activate 3.14-3.28B parameters based on token complexity and achieve performance comparable to SOTA open-source models of similar or even larger size.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can You Trick the Grader? Adversarial Persuasion of LLM Judges</title>
<link>https://arxiv.org/abs/2508.07805</link>
<guid>https://arxiv.org/abs/2508.07805</guid>
<content:encoded><![CDATA[
arXiv:2508.07805v1 Announce Type: new 
Abstract: As large language models take on growing roles as automated evaluators in practical settings, a critical question arises: Can individuals persuade an LLM judge to assign unfairly high scores? This study is the first to reveal that strategically embedded persuasive language can bias LLM judges when scoring mathematical reasoning tasks, where correctness should be independent of stylistic variation. Grounded in Aristotle's rhetorical principles, we formalize seven persuasion techniques (Majority, Consistency, Flattery, Reciprocity, Pity, Authority, Identity) and embed them into otherwise identical responses. Across six math benchmarks, we find that persuasive language leads LLM judges to assign inflated scores to incorrect solutions, by up to 8% on average, with Consistency causing the most severe distortion. Notably, increasing model size does not substantially mitigate this vulnerability. Further analysis demonstrates that combining multiple persuasion techniques amplifies the bias, and pairwise evaluation is likewise susceptible. Moreover, the persuasive effect persists under counter prompting strategies, highlighting a critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need for robust defenses against persuasion-based attacks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Compositional Approaches for Focus and Sentiment Analysis</title>
<link>https://arxiv.org/abs/2508.07810</link>
<guid>https://arxiv.org/abs/2508.07810</guid>
<content:encoded><![CDATA[
arXiv:2508.07810v1 Announce Type: new 
Abstract: This paper summarizes the results of evaluating a compositional approach for Focus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in Natural Language Processing (NLP). While quantitative evaluations of compositional and non-compositional approaches in SA exist in NLP, similar quantitative evaluations are very rare in FA in Linguistics that deal with linguistic expressions representing focus or emphasis such as "it was John who left". We fill this gap in research by arguing that compositional rules in SA also apply to FA because FA and SA are closely related meaning that SA is part of FA. Our compositional approach in SA exploits basic syntactic rules such as rules of modification, coordination, and negation represented in the formalism of Universal Dependencies (UDs) in English and applied to words representing sentiments from sentiment dictionaries. Some of the advantages of our compositional analysis method for SA in contrast to non-compositional analysis methods are interpretability and explainability. We test the accuracy of our compositional approach and compare it with a non-compositional approach VADER that uses simple heuristic rules to deal with negation, coordination and modification. In contrast to previous related work that evaluates compositionality in SA on long reviews, this study uses more appropriate datasets to evaluate compositionality. In addition, we generalize the results of compositional approaches in SA to compositional approaches in FA.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models as Expert Annotators</title>
<link>https://arxiv.org/abs/2508.07827</link>
<guid>https://arxiv.org/abs/2508.07827</guid>
<content:encoded><![CDATA[
arXiv:2508.07827v1 Announce Type: new 
Abstract: Textual data annotation, the process of labeling or tagging text with relevant information, is typically costly, time-consuming, and labor-intensive. While large language models (LLMs) have demonstrated their potential as direct alternatives to human annotators for general domains natural language processing (NLP) tasks, their effectiveness on annotation tasks in domains requiring expert knowledge remains underexplored. In this paper, we investigate: whether top-performing LLMs, which might be perceived as having expert-level proficiency in academic and professional benchmarks, can serve as direct alternatives to human expert annotators? To this end, we evaluate both individual LLMs and multi-agent approaches across three highly specialized domains: finance, biomedicine, and law. Specifically, we propose a multi-agent discussion framework to simulate a group of human annotators, where LLMs are tasked to engage in discussions by considering others' annotations and justifications before finalizing their labels. Additionally, we incorporate reasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our empirical results reveal that: (1) Individual LLMs equipped with inference-time techniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal or even negative performance gains, contrary to prior literature suggesting their broad effectiveness. (2) Overall, reasoning models do not demonstrate statistically significant improvements over non-reasoning models in most settings. This suggests that extended long CoT provides relatively limited benefits for data annotation in specialized domains. (3) Certain model behaviors emerge in the multi-agent discussion environment. For instance, Claude 3.7 Sonnet with thinking rarely changes its initial annotations, even when other agents provide correct annotations or valid reasoning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding</title>
<link>https://arxiv.org/abs/2508.07849</link>
<guid>https://arxiv.org/abs/2508.07849</guid>
<content:encoded><![CDATA[
arXiv:2508.07849v1 Announce Type: new 
Abstract: Despite advances in legal NLP, no comprehensive evaluation covering multiple legal-specific LLMs currently exists for contract classification tasks in contract understanding. To address this gap, we present an evaluation of 10 legal-specific LLMs on three English language contract understanding tasks and compare them with 7 general-purpose LLMs. The results show that legal-specific LLMs consistently outperform general-purpose models, especially on tasks requiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish new SOTAs on two of the three tasks, despite having 69% fewer parameters than the best-performing general-purpose LLM. We also identify CaseLaw-BERT and LexLM as strong additional baselines for contract understanding. Our results provide a holistic evaluation of legal-specific LLMs and will facilitate the development of more accurate contract understanding systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Czech Aspect-Based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2508.07860</link>
<guid>https://arxiv.org/abs/2508.07860</guid>
<content:encoded><![CDATA[
arXiv:2508.07860v1 Announce Type: new 
Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task that aims to identify sentiment toward specific aspects of an entity. While large language models (LLMs) have shown strong performance in various natural language processing (NLP) tasks, their capabilities for Czech ABSA remain largely unexplored. In this work, we conduct a comprehensive evaluation of 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their performance in zero-shot, few-shot, and fine-tuning scenarios. Our results show that small domain-specific models fine-tuned for ABSA outperform general-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs achieve state-of-the-art results. We analyze how factors such as multilingualism, model size, and recency influence performance and present an error analysis highlighting key challenges, particularly in aspect term prediction. Our findings provide insights into the suitability of LLMs for Czech ABSA and offer guidance for future research in this area.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models</title>
<link>https://arxiv.org/abs/2508.07866</link>
<guid>https://arxiv.org/abs/2508.07866</guid>
<content:encoded><![CDATA[
arXiv:2508.07866v1 Announce Type: new 
Abstract: Aspect-based sentiment analysis (ABSA) has received substantial attention in English, yet challenges remain for low-resource languages due to the scarcity of labelled data. Current cross-lingual ABSA approaches often rely on external translation tools and overlook the potential benefits of incorporating a small number of target language examples into training. In this paper, we evaluate the effect of adding few-shot target language examples to the training set across four ABSA tasks, six target languages, and two sequence-to-sequence models. We show that adding as few as ten target language examples significantly improves performance over zero-shot settings and achieves a similar effect to constrained decoding in reducing prediction errors. Furthermore, we demonstrate that combining 1,000 target language examples with English data can even surpass monolingual baselines. These findings offer practical insights for improving cross-lingual ABSA in low-resource and domain-specific settings, as obtaining ten high-quality annotated examples is both feasible and highly effective.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity</title>
<link>https://arxiv.org/abs/2508.07902</link>
<guid>https://arxiv.org/abs/2508.07902</guid>
<content:encoded><![CDATA[
arXiv:2508.07902v1 Announce Type: new 
Abstract: Large language models (LLMs) show promise in offering emotional support and generating empathetic responses for individuals in distress, but their ability to deliver culturally sensitive support remains underexplored due to lack of resources. In this work, we introduce CultureCare, the first dataset designed for this task, spanning four cultures and including 1729 distress messages, 1523 cultural signals, and 1041 support strategies with fine-grained emotional and cultural annotations. Leveraging CultureCare, we (i) develop and test four adaptation strategies for guiding three state-of-the-art LLMs toward culturally sensitive responses; (ii) conduct comprehensive evaluations using LLM judges, in-culture human annotators, and clinical psychologists; (iii) show that adapted LLMs outperform anonymous online peer responses, and that simple cultural role-play is insufficient for cultural sensitivity; and (iv) explore the application of LLMs in clinical training, where experts highlight their potential in fostering cultural competence in future therapists.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Challenges and opportunities in portraying emotion in generated sign language</title>
<link>https://arxiv.org/abs/2508.07937</link>
<guid>https://arxiv.org/abs/2508.07937</guid>
<content:encoded><![CDATA[
arXiv:2508.07937v1 Announce Type: new 
Abstract: Non-manual signals in sign languages continue to be a challenge for signing avatars. More specifically, emotional content has been difficult to incorporate because of a lack of a standard method of specifying the avatar's emotional state. This paper explores the application of an intuitive two-parameter representation for emotive non-manual signals to the Paula signing avatar that shows promise for facilitating the linguistic specification of emotional facial expressions in a more coherent manner than previous methods. Users can apply these parameters to control Paula's emotional expressions through a textual representation called the EASIER notation. The representation can allow avatars to express more nuanced emotional states using two numerical parameters. It also has the potential to enable more consistent specification of emotional non-manual signals in linguistic annotations which drive signing avatars.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expert Preference-based Evaluation of Automated Related Work Generation</title>
<link>https://arxiv.org/abs/2508.07955</link>
<guid>https://arxiv.org/abs/2508.07955</guid>
<content:encoded><![CDATA[
arXiv:2508.07955v1 Announce Type: new 
Abstract: Expert domain writing, such as scientific writing, typically demands extensive domain knowledge. Recent advances in LLMs show promising potential in reducing the expert workload. However, evaluating the quality of automatically generated scientific writing is a crucial open issue, as it requires knowledge of domain-specific evaluation criteria and the ability to discern expert preferences. Conventional automatic metrics and LLM-as-a-judge systems are insufficient to grasp expert preferences and domain-specific quality standards. To address this gap and support human-AI collaborative writing, we focus on related work generation, one of the most challenging scientific tasks, as an exemplar. We propose GREP, a multi-turn evaluation framework that integrates classical related work evaluation criteria with expert-specific preferences. Instead of assigning a single score, our framework decomposes the evaluation into fine-grained dimensions. This localized evaluation approach is further augmented with contrastive few-shot examples to provide detailed contextual guidance for the evaluation dimensions. The design principles allow our framework to deliver cardinal assessment of quality, which can facilitate better post-training compared to ordinal preference data. For better accessibility, we design two variants of GREP: a more precise variant with proprietary LLMs as evaluators, and a cheaper alternative with open-weight LLMs. Empirical investigation reveals that our framework is able to assess the quality of related work sections in a much more robust manner compared to standard LLM judges, reflects natural scenarios of scientific writing, and bears a strong correlation with the human expert assessment. We also observe that generations from state-of-the-art LLMs struggle to satisfy validation constraints of a suitable related work section. They (mostly) fail to improve based on feedback as well.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Subjective Language Understanding: A Survey</title>
<link>https://arxiv.org/abs/2508.07959</link>
<guid>https://arxiv.org/abs/2508.07959</guid>
<content:encoded><![CDATA[
arXiv:2508.07959v1 Announce Type: new 
Abstract: Subjective language understanding refers to a broad set of natural language processing tasks where the goal is to interpret or generate content that conveys personal feelings, opinions, or figurative meanings rather than objective facts. With the advent of large language models (LLMs) such as ChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach these inherently nuanced tasks. In this survey, we provide a comprehensive review of recent advances in applying LLMs to subjective language tasks, including sentiment analysis, emotion recognition, sarcasm detection, humor understanding, stance detection, metaphor interpretation, intent detection, and aesthetics assessment. We begin by clarifying the definition of subjective language from linguistic and cognitive perspectives, and we outline the unique challenges posed by subjective language (e.g. ambiguity, figurativeness, context dependence). We then survey the evolution of LLM architectures and techniques that particularly benefit subjectivity tasks, highlighting why LLMs are well-suited to model subtle human-like judgments. For each of the eight tasks, we summarize task definitions, key datasets, state-of-the-art LLM-based methods, and remaining challenges. We provide comparative insights, discussing commonalities and differences among tasks and how multi-task LLM approaches might yield unified models of subjectivity. Finally, we identify open issues such as data limitations, model bias, and ethical considerations, and suggest future research directions. We hope this survey will serve as a valuable resource for researchers and practitioners interested in the intersection of affective computing, figurative language processing, and large-scale language models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Machine Interpreting: Lessons from Human Interpreting Studies</title>
<link>https://arxiv.org/abs/2508.07964</link>
<guid>https://arxiv.org/abs/2508.07964</guid>
<content:encoded><![CDATA[
arXiv:2508.07964v1 Announce Type: new 
Abstract: Current speech translation systems, while having achieved impressive accuracies, are rather static in their behavior and do not adapt to real-world situations in ways human interpreters do. In order to improve their practical usefulness and enable interpreting-like experiences, a precise understanding of the nature of human interpreting is crucial. To this end, we discuss human interpreting literature from the perspective of the machine translation field, while considering both operational and qualitative aspects. We identify implications for the development of speech translation systems and argue that there is great potential to adopt many human interpreting principles using recent modeling techniques. We hope that our findings provide inspiration for closing the perceived usability gap, and can motivate progress toward true machine interpreting.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Syntactic Generalization in Structure-inducing Language Models</title>
<link>https://arxiv.org/abs/2508.07969</link>
<guid>https://arxiv.org/abs/2508.07969</guid>
<content:encoded><![CDATA[
arXiv:2508.07969v1 Announce Type: new 
Abstract: Structure-inducing Language Models (SiLM) are trained on a self-supervised language modeling task, and induce a hierarchical sentence representation as a byproduct when processing an input. A wide variety of SiLMs have been proposed. However, these have typically been evaluated on a relatively small scale, and evaluation of these models has systematic gaps and lacks comparability. In this work, we study three different SiLM architectures using both natural language (English) corpora and synthetic bracketing expressions: Structformer (Shen et al., 2021), UDGN (Shen et al., 2022) and GPST (Hu et al., 2024). We compare them with respect to (i) properties of the induced syntactic representations (ii) performance on grammaticality judgment tasks, and (iii) training dynamics. We find that none of the three architectures dominates across all evaluation metrics. However, there are significant differences, in particular with respect to the induced syntactic representations. The Generative Pretrained Structured Transformer (GPST; Hu et al. 2024) performs most consistently across evaluation settings, and outperforms the other models on long-distance dependencies in bracketing expressions. Furthermore, our study shows that small models trained on large amounts of synthetic data provide a useful testbed for evaluating basic model properties.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL</title>
<link>https://arxiv.org/abs/2508.07976</link>
<guid>https://arxiv.org/abs/2508.07976</guid>
<content:encoded><![CDATA[
arXiv:2508.07976v1 Announce Type: new 
Abstract: Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. <=10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns and output tokens exceeding 150k during training time. With a simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We open-source our models, training data, and codes in https://github.com/inclusionAI/ASearcher.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Medical Metaphors Corpus (MCC)</title>
<link>https://arxiv.org/abs/2508.07993</link>
<guid>https://arxiv.org/abs/2508.07993</guid>
<content:encoded><![CDATA[
arXiv:2508.07993v1 Announce Type: new 
Abstract: Metaphor is a fundamental cognitive mechanism that shapes scientific understanding, enabling the communication of complex concepts while potentially constraining paradigmatic thinking. Despite the prevalence of figurative language in scientific discourse, existing metaphor detection resources primarily focus on general-domain text, leaving a critical gap for domain-specific applications. In this paper, we present the Medical Metaphors Corpus (MCC), a comprehensive dataset of 792 annotated scientific conceptual metaphors spanning medical and biological domains. MCC aggregates metaphorical expressions from diverse sources including peer-reviewed literature, news media, social media discourse, and crowdsourced contributions, providing both binary and graded metaphoricity judgments validated through human annotation. Each instance includes source-target conceptual mappings and perceived metaphoricity scores on a 0-7 scale, establishing the first annotated resource for computational scientific metaphor research. Our evaluation demonstrates that state-of-the-art language models achieve modest performance on scientific metaphor detection, revealing substantial room for improvement in domain-specific figurative language understanding. MCC enables multiple research applications including metaphor detection benchmarking, quality-aware generation systems, and patient-centered communication tools.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WideSearch: Benchmarking Agentic Broad Info-Seeking</title>
<link>https://arxiv.org/abs/2508.07999</link>
<guid>https://arxiv.org/abs/2508.07999</guid>
<content:encoded><![CDATA[
arXiv:2508.07999v1 Announce Type: new 
Abstract: From professional research to everyday planning, many tasks are bottlenecked by wide-scale information seeking, which is more repetitive than cognitively complex. With the rapid development of Large Language Models (LLMs), automated search agents powered by LLMs offer a promising solution to liberate humans from this tedious work. However, the capability of these agents to perform such "wide-context" collection reliably and completely remains largely unevaluated due to a lack of suitable benchmarks. To bridge this gap, we introduce WideSearch, a new benchmark engineered to evaluate agent reliability on these large-scale collection tasks. The benchmark features 200 manually curated questions (100 in English, 100 in Chinese) from over 15 diverse domains, grounded in real user queries. Each task requires agents to collect large-scale atomic information, which could be verified one by one objectively, and arrange it into a well-organized output. A rigorous five-stage quality control pipeline ensures the difficulty, completeness, and verifiability of the dataset. We benchmark over 10 state-of-the-art agentic search systems, including single-agent, multi-agent frameworks, and end-to-end commercial systems. Most systems achieve overall success rates near 0\%, with the best performer reaching just 5\%. However, given sufficient time, cross-validation by multiple human testers can achieve a near 100\% success rate. These results demonstrate that present search agents have critical deficiencies in large-scale information seeking, underscoring urgent areas for future research and development in agentic search. Our dataset, evaluation pipeline, and benchmark results have been publicly released at https://widesearch-seed.github.io/
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Progressive Depth Up-scaling via Optimal Transport</title>
<link>https://arxiv.org/abs/2508.08011</link>
<guid>https://arxiv.org/abs/2508.08011</guid>
<content:encoded><![CDATA[
arXiv:2508.08011v1 Announce Type: new 
Abstract: Scaling Large Language Models (LLMs) yields performance gains but incurs substantial training costs. Depth up-scaling offers training efficiency by adding new layers to pre-trained models. However, most existing methods copy or average weights from base layers, neglecting neuron permutation differences. This limitation can potentially cause misalignment that harms performance. Inspired by applying Optimal Transport (OT) for neuron alignment, we propose Optimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses Transformer blocks in adjacent base layers via OT for new layer creation, to mitigate neuron permutation mismatch between layers. OpT-DeUS achieves better overall performance and offers improved training efficiency than existing methods for continual pre-training and supervised fine-tuning across different model sizes. To further evaluate the impact of interpolation positions, our extensive analysis shows that inserting new layers closer to the top results in higher training efficiency due to shorter back-propagation time while obtaining additional performance gains.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)</title>
<link>https://arxiv.org/abs/2508.08050</link>
<guid>https://arxiv.org/abs/2508.08050</guid>
<content:encoded><![CDATA[
arXiv:2508.08050v1 Announce Type: new 
Abstract: The Sign Language Translation and Avatar Technology (SLTAT) workshops continue a series of gatherings to share recent advances in improving deaf / human communication through non-invasive means. This 2025 edition, the 9th since its first appearance in 2011, is hosted by the International Conference on Intelligent Virtual Agents (IVA), giving the opportunity for contamination between two research communities, using digital humans as either virtual interpreters or as interactive conversational agents. As presented in this summary paper, SLTAT sees contributions beyond avatar technologies, with a consistent number of submissions on sign language recognition, and other work on data collection, data analysis, tools, ethics, usability, and affective computing.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Information Speech Language Models for Emotional Conversations</title>
<link>https://arxiv.org/abs/2508.08095</link>
<guid>https://arxiv.org/abs/2508.08095</guid>
<content:encoded><![CDATA[
arXiv:2508.08095v1 Announce Type: new 
Abstract: Conversational systems relying on text-based large language models (LLMs) often overlook paralinguistic cues, essential for understanding emotions and intentions. Speech-language models (SLMs), which use speech as input, are emerging as a promising solution. However, SLMs built by extending frozen LLMs struggle to capture paralinguistic information and exhibit reduced context understanding. We identify entangled information and improper training strategies as key issues. To address these issues, we propose two heterogeneous adapters and suggest a weakly supervised training strategy. Our approach disentangles paralinguistic and linguistic information, enabling SLMs to interpret speech through structured representations. It also preserves contextual understanding by avoiding the generation of task-specific vectors through controlled randomness. This approach trains only the adapters on common datasets, ensuring parameter and data efficiency. Experiments demonstrate competitive performance in emotional conversation tasks, showcasing the model's ability to effectively integrate both paralinguistic and linguistic information within contextual settings.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?</title>
<link>https://arxiv.org/abs/2508.08096</link>
<guid>https://arxiv.org/abs/2508.08096</guid>
<content:encoded><![CDATA[
arXiv:2508.08096v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) and their increased accessibility have made it easier than ever for students to automatically generate texts, posing new challenges for educational institutions. To enforce norms of academic integrity and ensure students' learning, learning analytics methods to automatically detect LLM-generated text appear increasingly appealing. This paper benchmarks the performance of different state-of-the-art detectors in educational contexts, introducing a novel dataset, called Generative Essay Detection in Education (GEDE), containing over 900 student-written essays and over 12,500 LLM-generated essays from various domains. To capture the diversity of LLM usage practices in generating text, we propose the concept of contribution levels, representing students' contribution to a given assignment. These levels range from purely human-written texts, to slightly LLM-improved versions, to fully LLM-generated texts, and finally to active attacks on the detector by "humanizing" generated texts. We show that most detectors struggle to accurately classify texts of intermediate student contribution levels, like LLM-improved human-written texts. Detectors are particularly likely to produce false positives, which is problematic in educational settings where false suspicions can severely impact students' lives. Our dataset, code, and additional supplementary materials are publicly available at https://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0</title>
<link>https://arxiv.org/abs/2508.08110</link>
<guid>https://arxiv.org/abs/2508.08110</guid>
<content:encoded><![CDATA[
arXiv:2508.08110v1 Announce Type: new 
Abstract: Self-supervised models for speech representation learning now see widespread use for their versatility and performance on downstream tasks, but the effect of model architecture on the linguistic information learned in their representations remains under-studied. This study investigates two such models, HuBERT and wav2vec 2.0, and minimally compares two of their architectural differences: training objective and iterative pseudo-label refinement through multiple training iterations. We find that differences in canonical correlation of hidden representations to word identity, phoneme identity, and speaker identity are explained by training iteration, not training objective. We suggest that future work investigate the reason for the effectiveness of iterative refinement in encoding linguistic information in self-supervised speech representations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks</title>
<link>https://arxiv.org/abs/2508.08125</link>
<guid>https://arxiv.org/abs/2508.08125</guid>
<content:encoded><![CDATA[
arXiv:2508.08125v1 Announce Type: new 
Abstract: In this paper, we introduce a novel Czech dataset for aspect-based sentiment analysis (ABSA), which consists of 3.1K manually annotated reviews from the restaurant domain. The dataset is built upon the older Czech dataset, which contained only separate labels for the basic ABSA tasks such as aspect term extraction or aspect polarity detection. Unlike its predecessor, our new dataset is specifically designed for more complex tasks, e.g. target-aspect-category detection. These advanced tasks require a unified annotation format, seamlessly linking sentiment elements (labels) together. Our dataset follows the format of the well-known SemEval-2016 datasets. This design choice allows effortless application and evaluation in cross-lingual scenarios, ultimately fostering cross-language comparisons with equivalent counterpart datasets in other languages. The annotation process engaged two trained annotators, yielding an impressive inter-annotator agreement rate of approximately 90%. Additionally, we provide 24M reviews without annotations suitable for unsupervised learning. We present robust monolingual baseline results achieved with various Transformer-based models and insightful error analysis to supplement our contributions. Our code and dataset are freely available for non-commercial research purposes.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models</title>
<link>https://arxiv.org/abs/2508.08131</link>
<guid>https://arxiv.org/abs/2508.08131</guid>
<content:encoded><![CDATA[
arXiv:2508.08131v1 Announce Type: new 
Abstract: Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to perceive speech inputs, have gained increasing attention for their potential to advance speech understanding tasks. However, despite recent progress, studies show that SLMs often struggle to generalize across datasets, even for trained languages and tasks, raising concerns about whether they process speech in a text-like manner as intended. A key challenge underlying this limitation is the modality gap between speech and text representations. The high variability in speech embeddings may allow SLMs to achieve strong in-domain performance by exploiting unintended speech variations, ultimately hindering generalization. To mitigate this modality gap, we introduce Optimal Transport Regularization (OTReg), a method that formulates speech-text alignment as an optimal transport problem and derives a regularization loss to improve SLM training. In each training iteration, OTReg first establishes a structured correspondence between speech and transcript embeddings by determining the optimal transport plan, then incorporates the regularization loss based on this transport plan to optimize SLMs in generating speech embeddings that align more effectively with transcript embeddings. OTReg is lightweight, requiring no additional labels or learnable parameters, and integrates seamlessly into existing SLM training procedures. Extensive multilingual ASR experiments demonstrate that OTReg enhances speech-text alignment, mitigates the modality gap, and consequently improves SLM generalization across diverse datasets.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models</title>
<link>https://arxiv.org/abs/2508.08139</link>
<guid>https://arxiv.org/abs/2508.08139</guid>
<content:encoded><![CDATA[
arXiv:2508.08139v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective</title>
<link>https://arxiv.org/abs/2508.08140</link>
<guid>https://arxiv.org/abs/2508.08140</guid>
<content:encoded><![CDATA[
arXiv:2508.08140v1 Announce Type: new 
Abstract: Recent progress in large language models (LLMs) has leveraged their in-context learning (ICL) abilities to enable quick adaptation to unseen biomedical NLP tasks. By incorporating only a few input-output examples into prompts, LLMs can rapidly perform these new tasks. While the impact of these demonstrations on LLM performance has been extensively studied, most existing approaches prioritize representativeness over diversity when selecting examples from large corpora. To address this gap, we propose Dual-Div, a diversity-enhanced data-efficient framework for demonstration selection in biomedical ICL. Dual-Div employs a two-stage retrieval and ranking process: First, it identifies a limited set of candidate examples from a corpus by optimizing both representativeness and diversity (with optional annotation for unlabeled data). Second, it ranks these candidates against test queries to select the most relevant and non-redundant demonstrations. Evaluated on three biomedical NLP tasks (named entity recognition (NER), relation extraction (RE), and text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along with three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently outperforms baselines-achieving up to 5% higher macro-F1 scores-while demonstrating robustness to prompt permutations and class imbalance. Our findings establish that diversity in initial retrieval is more critical than ranking-stage optimization, and limiting demonstrations to 3-5 examples maximizes performance efficiency.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.08149</link>
<guid>https://arxiv.org/abs/2508.08149</guid>
<content:encoded><![CDATA[
arXiv:2508.08149v1 Announce Type: new 
Abstract: Reinforcement learning (RL) is emerging as a powerful paradigm for enabling large language models (LLMs) to perform complex reasoning tasks. Recent advances indicate that integrating RL with retrieval-augmented generation (RAG) allows LLMs to dynamically incorporate external knowledge, leading to more informed and robust decision making. However, we identify a critical challenge during policy-driven trajectory sampling: LLMs are frequently trapped in unproductive reasoning paths, which we refer to as "dead ends", committing to overconfident yet incorrect conclusions. This severely hampers exploration and undermines effective policy optimization. To address this challenge, we propose REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation), a novel framework that explores alternative reasoning paths while maintaining rigorous policy learning through principled distributional corrections. Our approach introduces two key innovations: (1) Mixed Sampling Strategy, which combines a novel probe sampling method with exploratory prompts to escape dead ends; and (2) Policy Correction Mechanism, which employs importance sampling to correct distribution shifts induced by mixed sampling, thereby mitigating gradient estimation bias. We evaluate it on seven question-answering benchmarks, and the experimental results show that REX-RAG achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B over strong baselines, demonstrating competitive results across multiple datasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo</title>
<link>https://arxiv.org/abs/2508.08163</link>
<guid>https://arxiv.org/abs/2508.08163</guid>
<content:encoded><![CDATA[
arXiv:2508.08163v1 Announce Type: new 
Abstract: The Learning With Disagreements (LeWiDi) 2025 shared task is to model annotator disagreement through soft label distribution prediction and perspectivist evaluation, modeling annotators. We adapt DisCo (Distribution from Context), a neural architecture that jointly models item-level and annotator-level label distributions, and present detailed analysis and improvements. In this paper, we extend the DisCo by incorporating annotator metadata, enhancing input representations, and modifying the loss functions to capture disagreement patterns better. Through extensive experiments, we demonstrate substantial improvements in both soft and perspectivist evaluation metrics across three datasets. We also conduct in-depth error and calibration analyses, highlighting the conditions under which improvements occur. Our findings underscore the value of disagreement-aware modeling and offer insights into how system components interact with the complexity of human-annotated data.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions</title>
<link>https://arxiv.org/abs/2508.08192</link>
<guid>https://arxiv.org/abs/2508.08192</guid>
<content:encoded><![CDATA[
arXiv:2508.08192v1 Announce Type: new 
Abstract: Speculative decoding is a standard method for accelerating the inference speed of large language models. However, scaling it for production environments poses several engineering challenges, including efficiently implementing different operations (e.g., tree attention and multi-round speculative decoding) on GPU. In this paper, we detail the training and inference optimization techniques that we have implemented to enable EAGLE-based speculative decoding at a production scale for Llama models. With these changes, we achieve a new state-of-the-art inference latency for Llama models. For example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a batch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the previously best known method. Furthermore, for EAGLE-based speculative decoding, our optimizations enable us to achieve a speed-up for large batch sizes between 1.4x and 2.0x at production scale.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models</title>
<link>https://arxiv.org/abs/2508.08204</link>
<guid>https://arxiv.org/abs/2508.08204</guid>
<content:encoded><![CDATA[
arXiv:2508.08204v1 Announce Type: new 
Abstract: There has been much recent interest in evaluating large language models for uncertainty calibration to facilitate model control and modulate user trust. Inference time uncertainty, which may provide a real-time signal to the model or external control modules, is particularly important for applying these concepts to improve LLM-user experience in practice. While many of the existing papers consider model calibration, comparatively little work has sought to evaluate how closely model uncertainty aligns to human uncertainty. In this work, we evaluate a collection of inference-time uncertainty measures, using both established metrics and novel variations, to determine how closely they align with both human group-level uncertainty and traditional notions of model calibration. We find that numerous measures show evidence of strong alignment to human uncertainty, even despite the lack of alignment to human answer preference. For those successful metrics, we find moderate to strong evidence of model calibration in terms of both correctness correlation and distributional analysis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling</title>
<link>https://arxiv.org/abs/2508.08211</link>
<guid>https://arxiv.org/abs/2508.08211</guid>
<content:encoded><![CDATA[
arXiv:2508.08211v1 Announce Type: new 
Abstract: Watermarking LLM-generated text is critical for content attribution and misinformation prevention. However, existing methods compromise text quality, require white-box model access and logit manipulation. These limitations exclude API-based models and multilingual scenarios. We propose SAEMark, a general framework for post-hoc multi-bit watermarking that embeds personalized messages solely via inference-time, feature-based rejection sampling without altering model logits or requiring training. Our approach operates on deterministic features extracted from generated text, selecting outputs whose feature statistics align with key-derived targets. This framework naturally generalizes across languages and domains while preserving text quality through sampling LLM outputs instead of modifying. We provide theoretical guarantees relating watermark success probability and compute budget that hold for any suitable feature extractor. Empirically, we demonstrate the framework's effectiveness using Sparse Autoencoders (SAEs), achieving superior detection accuracy and text quality. Experiments across 4 datasets show SAEMark's consistent performance, with 99.7% F1 on English and strong multi-bit detection accuracy. SAEMark establishes a new paradigm for scalable watermarking that works out-of-the-box with closed-source LLMs while enabling content attribution.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capabilities of GPT-5 on Multimodal Medical Reasoning</title>
<link>https://arxiv.org/abs/2508.08224</link>
<guid>https://arxiv.org/abs/2508.08224</guid>
<content:encoded><![CDATA[
arXiv:2508.08224v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled general-purpose systems to perform increasingly complex domain-specific reasoning without extensive fine-tuning. In the medical domain, decision-making often requires integrating heterogeneous information sources, including patient narratives, structured data, and medical images. This study positions GPT-5 as a generalist multimodal reasoner for medical decision support and systematically evaluates its zero-shot chain-of-thought reasoning performance on both text-based question answering and visual question answering tasks under a unified protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20 against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that GPT-5 consistently outperforms all baselines, achieving state-of-the-art accuracy across all QA benchmarks and delivering substantial gains in multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and understanding scores by +29.62% and +36.18% over GPT-4o, respectively, and surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in understanding. In contrast, GPT-4o remains below human expert performance in most dimensions. A representative case study demonstrates GPT-5's ability to integrate visual and textual cues into a coherent diagnostic reasoning chain, recommending appropriate high-stakes interventions. Our results show that, on these controlled multimodal reasoning benchmarks, GPT-5 moves from human-comparable to above human-expert performance. This improvement may substantially inform the design of future clinical decision-support systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge</title>
<link>https://arxiv.org/abs/2508.08236</link>
<guid>https://arxiv.org/abs/2508.08236</guid>
<content:encoded><![CDATA[
arXiv:2508.08236v1 Announce Type: new 
Abstract: Evaluating the safety alignment of LLM responses in high-risk mental health dialogues is particularly difficult due to missing gold-standard answers and the ethically sensitive nature of these interactions. To address this challenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark based on real-world Chinese mental health dialogues. It evaluates whether the model responses align with the safety principles defined by experts. Specifically designed for settings without standard references, our method adopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation using expert-defined reasoning chains grounded in psychological intervention principles. We employ binary point-wise scoring across multiple safety dimensions to enhance the explainability and traceability of the evaluation. Additionally, we present a manually curated, high-quality Chinese-language dataset covering self-harm, suicidal ideation, and existential distress, derived from real-world online discourse. Experiments on 3600 judgments show that our method achieves the highest agreement with expert assessments and produces more interpretable evaluation rationales compared to existing approaches. Our dataset and evaluation tool are publicly available to facilitate further research.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jinx: Unlimited LLMs for Probing Alignment Failures</title>
<link>https://arxiv.org/abs/2508.08243</link>
<guid>https://arxiv.org/abs/2508.08243</guid>
<content:encoded><![CDATA[
arXiv:2508.08243v1 Announce Type: new 
Abstract: Unlimited, or so-called helpful-only language models are trained without safety alignment constraints and never refuse user queries. They are widely used by leading AI companies as internal tools for red teaming and alignment evaluation. For example, if a safety-aligned model produces harmful outputs similar to an unlimited model, this indicates alignment failures that require further attention. Despite their essential role in assessing alignment, such models are not available to the research community.
  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx responds to all queries without refusals or safety filtering, while preserving the base model's capabilities in reasoning and instruction following. It provides researchers with an accessible tool for probing alignment failures, evaluating safety boundaries, and systematically studying failure modes in language model safety.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials</title>
<link>https://arxiv.org/abs/2508.06591</link>
<guid>https://arxiv.org/abs/2508.06591</guid>
<content:encoded><![CDATA[
arXiv:2508.06591v1 Announce Type: cross 
Abstract: Large language models (LLMs) have reshaped the research landscape by enabling new approaches to knowledge retrieval and creative ideation. Yet their application in discipline-specific experimental science, particularly in highly multi-disciplinary domains like materials science, remains limited. We present a first-of-its-kind framework that integrates generative AI with literature from hitherto-unconnected fields such as plant science, biomimetics, and materials engineering to extract insights and design experiments for materials. We focus on humidity-responsive systems such as pollen-based materials and Rhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and adaptive performance. Using a suite of AI tools, including a fine-tuned model (BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a Hierarchical Sampling strategy, we extract structure-property relationships and translate them into new classes of bioinspired materials. Structured inference protocols generate and evaluate hundreds of hypotheses from a single query, surfacing novel and experimentally tractable ideas. We validate our approach through real-world implementation: LLM-generated procedures, materials designs, and mechanical predictions were tested in the laboratory, culminating in the fabrication of a novel pollen-based adhesive with tunable morphology and measured shear strength, establishing a foundation for future plant-derived adhesive design. This work demonstrates how AI-assisted ideation can drive real-world materials design and enable effective human-AI collaboration.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Story Ribbons: Reimagining Storyline Visualizations with Large Language Models</title>
<link>https://arxiv.org/abs/2508.06772</link>
<guid>https://arxiv.org/abs/2508.06772</guid>
<content:encoded><![CDATA[
arXiv:2508.06772v1 Announce Type: cross 
Abstract: Analyzing literature involves tracking interactions between characters, locations, and themes. Visualization has the potential to facilitate the mapping and analysis of these complex relationships, but capturing structured information from unstructured story data remains a challenge. As large language models (LLMs) continue to advance, we see an opportunity to use their text processing and analysis capabilities to augment and reimagine existing storyline visualization techniques. Toward this goal, we introduce an LLM-driven data parsing pipeline that automatically extracts relevant narrative information from novels and scripts. We then apply this pipeline to create Story Ribbons, an interactive visualization system that helps novice and expert literary analysts explore detailed character and theme trajectories at multiple narrative levels. Through pipeline evaluations and user studies with Story Ribbons on 36 literary works, we demonstrate the potential of LLMs to streamline narrative visualization creation and reveal new insights about familiar stories. We also describe current limitations of AI-based systems, and interaction motifs designed to address these issues.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maestro-EVC: Controllable Emotional Voice Conversion Guided by References and Explicit Prosody</title>
<link>https://arxiv.org/abs/2508.06890</link>
<guid>https://arxiv.org/abs/2508.06890</guid>
<content:encoded><![CDATA[
arXiv:2508.06890v1 Announce Type: cross 
Abstract: Emotional voice conversion (EVC) aims to modify the emotional style of speech while preserving its linguistic content. In practical EVC, controllability, the ability to independently control speaker identity and emotional style using distinct references, is crucial. However, existing methods often struggle to fully disentangle these attributes and lack the ability to model fine-grained emotional expressions such as temporal dynamics. We propose Maestro-EVC, a controllable EVC framework that enables independent control of content, speaker identity, and emotion by effectively disentangling each attribute from separate references. We further introduce a temporal emotion representation and an explicit prosody modeling with prosody augmentation to robustly capture and transfer the temporal dynamics of the target emotion, even under prosody-mismatched conditions. Experimental results confirm that Maestro-EVC achieves high-quality, controllable, and emotionally expressive speech synthesis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance</title>
<link>https://arxiv.org/abs/2508.06944</link>
<guid>https://arxiv.org/abs/2508.06944</guid>
<content:encoded><![CDATA[
arXiv:2508.06944v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are typically fine-tuned for reasoning tasks through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL), a process fraught with catastrophic forgetting and suboptimal trade-offs between imitation and exploration. Recent single-stage methods attempt to unify SFT and RL using heuristics, but lack a principled mechanism for dynamically balancing the two paradigms. In this paper, we reframe this challenge through the theoretical lens of \textbf{implicit rewards}, viewing SFT and RL not as distinct methods but as complementary reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel single-stage algorithm that learns the optimal balance between SFT's implicit, path-level reward and RL's explicit, outcome-based reward. The core of AMFT is a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL balance as a learnable parameter, dynamically optimizing it to maximize long-term task performance. This forward-looking approach, regularized by policy entropy for stability, autonomously discovers an effective training curriculum. We conduct a comprehensive evaluation on challenging benchmarks spanning mathematical reasoning, abstract visual reasoning (General Points), and vision-language navigation (V-IRL). AMFT consistently establishes a new state-of-the-art and demonstrats superior generalization on out-of-distribution (OOD) tasks. Ablation studies and training dynamic analysis confirm that the meta-learning controller is crucial for AMFT's stability, sample efficiency, and performance, offering a more principled and effective paradigm for LLM alignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery</title>
<link>https://arxiv.org/abs/2508.06960</link>
<guid>https://arxiv.org/abs/2508.06960</guid>
<content:encoded><![CDATA[
arXiv:2508.06960v1 Announce Type: cross 
Abstract: The rapid advancement of large language models has fundamentally shifted the bottleneck in AI development from computational power to data availability-with countless valuable datasets remaining hidden across specialized repositories, research appendices, and domain platforms. As reasoning capabilities and deep research methodologies continue to evolve, a critical question emerges: can AI agents transcend conventional search to systematically discover any dataset that meets specific user requirements, enabling truly autonomous demand-driven data curation? We introduce DatasetResearch, the first comprehensive benchmark evaluating AI agents' ability to discover and synthesize datasets from 208 real-world demands across knowledge-intensive and reasoning-intensive tasks. Our tri-dimensional evaluation framework reveals a stark reality: even advanced deep research systems achieve only 22% score on our challenging DatasetResearch-pro subset, exposing the vast gap between current capabilities and perfect dataset discovery. Our analysis uncovers a fundamental dichotomy-search agents excel at knowledge tasks through retrieval breadth, while synthesis agents dominate reasoning challenges via structured generation-yet both catastrophically fail on "corner cases" outside existing distributions. These findings establish the first rigorous baseline for dataset discovery agents and illuminate the path toward AI systems capable of finding any dataset in the digital universe. Our benchmark and comprehensive analysis provide the foundation for the next generation of self-improving AI systems and are publicly available at https://github.com/GAIR-NLP/DatasetResearch.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree</title>
<link>https://arxiv.org/abs/2508.07014</link>
<guid>https://arxiv.org/abs/2508.07014</guid>
<content:encoded><![CDATA[
arXiv:2508.07014v1 Announce Type: cross 
Abstract: Recognizing specific key phrases is an essential task for contextualized Automatic Speech Recognition (ASR). However, most existing context-biasing approaches have limitations associated with the necessity of additional model training, significantly slow down the decoding process, or constrain the choice of the ASR system type. This paper proposes a universal ASR context-biasing framework that supports all major types: CTC, Transducers, and Attention Encoder-Decoder models. The framework is based on a GPU-accelerated word boosting tree, which enables it to be used in shallow fusion mode for greedy and beam search decoding without noticeable speed degradation, even with a vast number of key phrases (up to 20K items). The obtained results showed high efficiency of the proposed method, surpassing the considered open-source context-biasing approaches in accuracy and decoding speed. Our context-biasing framework is open-sourced as a part of the NeMo toolkit.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA</title>
<link>https://arxiv.org/abs/2508.07022</link>
<guid>https://arxiv.org/abs/2508.07022</guid>
<content:encoded><![CDATA[
arXiv:2508.07022v1 Announce Type: cross 
Abstract: Knowledge editing (KE) provides a scalable approach for updating factual knowledge in large language models without full retraining. While previous studies have demonstrated effectiveness in general domains and medical QA tasks, little attention has been paid to KE in multimodal medical scenarios. Unlike text-only settings, medical KE demands integrating updated knowledge with visual reasoning to support safe and interpretable clinical decisions. To address this gap, we propose MultiMedEdit, the first benchmark tailored to evaluating KE in clinical multimodal tasks. Our framework spans both understanding and reasoning task types, defines a three-dimensional metric suite (reliability, generality, and locality), and supports cross-paradigm comparisons across general and domain-specific models. We conduct extensive experiments under single-editing and lifelong-editing settings. Results suggest that current methods struggle with generalization and long-tail reasoning, particularly in complex clinical workflows. We further present an efficiency analysis (e.g., edit latency, memory footprint), revealing practical trade-offs in real-world deployment across KE paradigms. Overall, MultiMedEdit not only reveals the limitations of current approaches but also provides a solid foundation for developing clinically robust knowledge editing techniques in the future.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability</title>
<link>https://arxiv.org/abs/2508.07050</link>
<guid>https://arxiv.org/abs/2508.07050</guid>
<content:encoded><![CDATA[
arXiv:2508.07050v1 Announce Type: cross 
Abstract: Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker \textbf{ReasonRank} outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. \textbf{Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\footnote{https://brightbenchmark.github.io/}.} Our codes are available at https://github.com/8421BCD/ReasonRank.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SQL-Exchange: Transforming SQL Queries Across Domains</title>
<link>https://arxiv.org/abs/2508.07087</link>
<guid>https://arxiv.org/abs/2508.07087</guid>
<content:encoded><![CDATA[
arXiv:2508.07087v1 Announce Type: cross 
Abstract: We introduce SQL-Exchange, a framework for mapping SQL queries across different database schemas by preserving the source query structure while adapting domain-specific elements to align with the target schema. We investigate the conditions under which such mappings are feasible and beneficial, and examine their impact on enhancing the in-context learning performance of text-to-SQL systems as a downstream task. Our comprehensive evaluation across multiple model families and benchmark datasets--assessing structural alignment with source queries, execution validity on target databases, and semantic correctness--demonstrates that SQL-Exchange is effective across a wide range of schemas and query types. Our results further show that using mapped queries as in-context examples consistently improves text-to-SQL performance over using queries from the source schema.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection</title>
<link>https://arxiv.org/abs/2508.07201</link>
<guid>https://arxiv.org/abs/2508.07201</guid>
<content:encoded><![CDATA[
arXiv:2508.07201v1 Announce Type: cross 
Abstract: Rumor detection on social media has become increasingly important. Most existing graph-based models presume rumor propagation trees (RPTs) have deep structures and learn sequential stance features along branches. However, through statistical analysis on real-world datasets, we find RPTs exhibit wide structures, with most nodes being shallow 1-level replies. To focus learning on intensive substructures, we propose Rumor Adaptive Graph Contrastive Learning (RAGCL) method with adaptive view augmentation guided by node centralities. We summarize three principles for RPT augmentation: 1) exempt root nodes, 2) retain deep reply nodes, 3) preserve lower-level nodes in deep sections. We employ node dropping, attribute masking and edge dropping with probabilities from centrality-based importance scores to generate views. A graph contrastive objective then learns robust rumor representations. Extensive experiments on four benchmark datasets demonstrate RAGCL outperforms state-of-the-art methods. Our work reveals the wide-structure nature of RPTs and contributes an effective graph contrastive learning approach tailored for rumor detection through principled adaptive augmentation. The proposed principles and augmentation techniques can potentially benefit other applications involving tree-structured graphs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Real-World Rumor Detection: Anomaly Detection Framework with Graph Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2508.07205</link>
<guid>https://arxiv.org/abs/2508.07205</guid>
<content:encoded><![CDATA[
arXiv:2508.07205v1 Announce Type: cross 
Abstract: Current rumor detection methods based on propagation structure learning predominately treat rumor detection as a class-balanced classification task on limited labeled data. However, real-world social media data exhibits an imbalanced distribution with a minority of rumors among massive regular posts. To address the data scarcity and imbalance issues, we construct two large-scale conversation datasets from Weibo and Twitter and analyze the domain distributions. We find obvious differences between rumor and non-rumor distributions, with non-rumors mostly in entertainment domains while rumors concentrate in news, indicating the conformity of rumor detection to an anomaly detection paradigm. Correspondingly, we propose the Anomaly Detection framework with Graph Supervised Contrastive Learning (AD-GSCL). It heuristically treats unlabeled data as non-rumors and adapts graph contrastive learning for rumor detection. Extensive experiments demonstrate AD-GSCL's superiority under class-balanced, imbalanced, and few-shot conditions. Our findings provide valuable insights for real-world rumor detection featuring imbalanced data distributions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning</title>
<link>https://arxiv.org/abs/2508.07292</link>
<guid>https://arxiv.org/abs/2508.07292</guid>
<content:encoded><![CDATA[
arXiv:2508.07292v1 Announce Type: cross 
Abstract: Developing general artificial intelligence (AI) systems to support endoscopic image diagnosis is an emerging research priority. Existing methods based on large-scale pretraining often lack unified coordination across tasks and struggle to handle the multi-step processes required in complex clinical workflows. While AI agents have shown promise in flexible instruction parsing and tool integration across domains, their potential in endoscopy remains underexplored. To address this gap, we propose EndoAgent, the first memory-guided agent for vision-to-decision endoscopic analysis that integrates iterative reasoning with adaptive tool selection and collaboration. Built on a dual-memory design, it enables sophisticated decision-making by ensuring logical coherence through short-term action tracking and progressively enhancing reasoning acuity through long-term experiential learning. To support diverse clinical tasks, EndoAgent integrates a suite of expert-designed tools within a unified reasoning loop. We further introduce EndoAgentBench, a benchmark of 5,709 visual question-answer pairs that assess visual understanding and language generation capabilities in realistic scenarios. Extensive experiments show that EndoAgent consistently outperforms both general and medical multimodal models, exhibiting its strong flexibility and reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexCTC: GPU-powered CTC Beam Decoding with advanced Contextual Abilities</title>
<link>https://arxiv.org/abs/2508.07315</link>
<guid>https://arxiv.org/abs/2508.07315</guid>
<content:encoded><![CDATA[
arXiv:2508.07315v1 Announce Type: cross 
Abstract: While beam search improves speech recognition quality over greedy decoding, standard implementations are slow, often sequential, and CPU-bound. To fully leverage modern hardware capabilities, we present a novel open-source FlexCTC toolkit for fully GPU-based beam decoding, designed for Connectionist Temporal Classification (CTC) models. Developed entirely in Python and PyTorch, it offers a fast, user-friendly, and extensible alternative to traditional C++, CUDA, or WFST-based decoders. The toolkit features a high-performance, fully batched GPU implementation with eliminated CPU-GPU synchronization and minimized kernel launch overhead via CUDA Graphs. It also supports advanced contextualization techniques, including GPU-powered N-gram language model fusion and phrase-level boosting. These features enable accurate and efficient decoding, making them suitable for both research and production use.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrLM: Learning Explicit Reasoning for Personalized RAG via Contrastive Reward Optimization</title>
<link>https://arxiv.org/abs/2508.07342</link>
<guid>https://arxiv.org/abs/2508.07342</guid>
<content:encoded><![CDATA[
arXiv:2508.07342v1 Announce Type: cross 
Abstract: Personalized retrieval-augmented generation (RAG) aims to produce user-tailored responses by incorporating retrieved user profiles alongside the input query. Existing methods primarily focus on improving retrieval and rely on large language models (LLMs) to implicitly integrate the retrieved context with the query. However, such models are often sensitive to retrieval quality and may generate responses that are misaligned with user preferences. To address this limitation, we propose PrLM, a reinforcement learning framework that trains LLMs to explicitly reason over retrieved user profiles. Guided by a contrastively trained personalization reward model, PrLM effectively learns from user responses without requiring annotated reasoning paths. Experiments on three personalized text generation datasets show that PrLM outperforms existing methods and remains robust across varying numbers of retrieved profiles and different retrievers.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach</title>
<link>https://arxiv.org/abs/2508.07353</link>
<guid>https://arxiv.org/abs/2508.07353</guid>
<content:encoded><![CDATA[
arXiv:2508.07353v1 Announce Type: cross 
Abstract: Numerous benchmarks have been built to evaluate the domain-specific abilities of large language models (LLMs), highlighting the need for effective and efficient benchmark construction. Existing domain-specific benchmarks primarily focus on the scaling law, relying on massive corpora for supervised fine-tuning or generating extensive question sets for broad coverage. However, the impact of corpus and question-answer (QA) set design on the precision and recall of domain-specific LLMs remains unexplored. In this paper, we address this gap and demonstrate that the scaling law is not always the optimal principle for benchmark construction in specific domains. Instead, we propose Comp-Comp, an iterative benchmarking framework based on a comprehensiveness-compactness principle. Here, comprehensiveness ensures semantic recall of the domain, while compactness enhances precision, guiding both corpus and QA set construction. To validate our framework, we conducted a case study in a well-renowned university, resulting in the creation of XUBench, a large-scale and comprehensive closed-domain benchmark. Although we use the academic domain as the case in this work, our Comp-Comp framework is designed to be extensible beyond academia, providing valuable insights for benchmark construction across various domains.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Strategic Plan Development</title>
<link>https://arxiv.org/abs/2508.07405</link>
<guid>https://arxiv.org/abs/2508.07405</guid>
<content:encoded><![CDATA[
arXiv:2508.07405v1 Announce Type: cross 
Abstract: Given recent breakthroughs in Generative Artificial Intelligence (GAI) and Large Language Models (LLMs), more and more professional services are being augmented through Artificial Intelligence (AI), which once seemed impossible to automate. This paper presents a modular model for leveraging GAI in developing strategic plans for large scale government organizations and evaluates leading machine learning techniques in their application towards one of the identified modules. Specifically, the performance of BERTopic and Non-negative Matrix Factorization (NMF) are evaluated in their ability to use topic modeling to generate themes representative of Vision Elements within a strategic plan. To accomplish this, BERTopic and NMF models are trained using a large volume of reports from the Government Accountability Office (GAO). The generated topics from each model are then scored for similarity against the Vision Elements of a published strategic plan and the results are compared. Our results show that these techniques are capable of generating themes similar to 100% of the elements being evaluated against. Further, we conclude that BERTopic performs best in this application with more than half of its correlated topics achieving a "medium" or "strong" correlation. A capability of GAI-enabled strategic plan development impacts a multi-billion dollar industry and assists the federal government in overcoming regulatory requirements which are crucial to the public good. Further work will focus on the operationalization of the concept proven in this study as well as viability of the remaining modules in the proposed model for GAI-generated strategic plans.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems</title>
<link>https://arxiv.org/abs/2508.07407</link>
<guid>https://arxiv.org/abs/2508.07407</guid>
<content:encoded><![CDATA[
arXiv:2508.07407v1 Announce Type: cross 
Abstract: Recent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving environments. To this end, recent research has explored agent evolution techniques that aim to automatically enhance agent systems based on interaction data and environmental feedback. This emerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of foundation models with the continuous adaptability required by lifelong agentic systems. In this survey, we provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically, we first introduce a unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic systems. The framework highlights four key components: System Inputs, Agent System, Environment, and Optimisers, serving as a foundation for understanding and comparing different strategies. Based on this framework, we systematically review a wide range of self-evolving techniques that target different components of the agent system. We also investigate domain-specific evolution strategies developed for specialised fields such as biomedicine, programming, and finance, where optimisation objectives are tightly coupled with domain constraints. In addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey aims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents, laying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading</title>
<link>https://arxiv.org/abs/2508.07408</link>
<guid>https://arxiv.org/abs/2508.07408</guid>
<content:encoded><![CDATA[
arXiv:2508.07408v1 Announce Type: cross 
Abstract: In this study, we wish to showcase the unique utility of large language models (LLMs) in financial semantic annotation and alpha signal discovery. Leveraging a corpus of company-related tweets, we use an LLM to automatically assign multi-label event categories to high-sentiment-intensity tweets. We align these labeled sentiment signals with forward returns over 1-to-7-day horizons to evaluate their statistical efficacy and market tradability. Our experiments reveal that certain event labels consistently yield negative alpha, with Sharpe ratios as low as -0.38 and information coefficients exceeding 0.05, all statistically significant at the 95\% confidence level. This study establishes the feasibility of transforming unstructured social media text into structured, multi-label event variables. A key contribution of this work is its commitment to transparency and reproducibility; all code and methodologies are made publicly available. Our results provide compelling evidence that social media sentiment is a valuable, albeit noisy, signal in financial forecasting and underscore the potential of open-source frameworks to democratize algorithmic trading research.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CP-Agent: Agentic Constraint Programming</title>
<link>https://arxiv.org/abs/2508.07468</link>
<guid>https://arxiv.org/abs/2508.07468</guid>
<content:encoded><![CDATA[
arXiv:2508.07468v1 Announce Type: cross 
Abstract: Translating natural language problem descriptions into formal constraint models remains a fundamental challenge in constraint programming, requiring deep expertise in both the problem domain and modeling frameworks. Previous approaches to automating this translation have employed fixed workflows with predetermined modeling steps, failing on a significant number of benchmark problems. We present a new approach using a pure agentic strategy without any fixed pipeline. We developed a general-purpose Python coding agent based on the ReAct (Reason and Act) principle, utilizing a persistent IPython kernel for stateful code execution and iterative development. Rather than embedding constraint programming logic into the agent architecture, domain-specific expertise is injected solely through a carefully crafted project prompt. The agent combines this prompt-encoded knowledge with access to file operations and code execution tools, enabling it to test hypotheses, debug failures, and verify solutions dynamically. Implemented in just a few hundred lines of code, this architecture successfully solves all 101 problems of the CP-Bench constraint programming benchmark set. The results suggest that constraint modeling tasks require the combination of general coding tools and domain expertise encoded in prompts, rather than specialized agent architectures or predefined workflows.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy</title>
<link>https://arxiv.org/abs/2508.07485</link>
<guid>https://arxiv.org/abs/2508.07485</guid>
<content:encoded><![CDATA[
arXiv:2508.07485v1 Announce Type: cross 
Abstract: We present the first evaluation harness that enables any out-of-the-box, local, Large Language Models (LLMs) to play full-press Diplomacy without fine-tuning or specialized training. Previous work required frontier LLMs, or fine-tuning, due to the high complexity and information density of Diplomacy's game state. Combined with the high variance of matches, these factors made Diplomacy prohibitive for study. In this work, we used data-driven iteration to optimize a textual game state representation such that a 24B model can reliably complete matches without any fine tuning. We develop tooling to facilitate hypothesis testing and statistical analysis, and we present case studies on persuasion, aggressive playstyles, and performance across a range of models. We conduct a variety of experiments across many popular LLMs, finding the larger models perform the best, but the smaller models still play adequately. We also introduce Critical State Analysis: an experimental protocol for rapidly iterating and analyzing key moments in a game at depth. Our harness democratizes the evaluation of strategic reasoning in LLMs by eliminating the need for fine-tuning, and it provides insights into how these capabilities emerge naturally from widely used LLMs. Our code is available in the supplement and will be open sourced.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI</title>
<link>https://arxiv.org/abs/2508.07520</link>
<guid>https://arxiv.org/abs/2508.07520</guid>
<content:encoded><![CDATA[
arXiv:2508.07520v1 Announce Type: cross 
Abstract: What if the patterns hidden within dialogue reveal more about communication than the words themselves? We introduce Conversational DNA, a novel visual language that treats any dialogue -- whether between humans, between human and AI, or among groups -- as a living system with interpretable structure that can be visualized, compared, and understood. Unlike traditional conversation analysis that reduces rich interaction to statistical summaries, our approach reveals the temporal architecture of dialogue through biological metaphors. Linguistic complexity flows through strand thickness, emotional trajectories cascade through color gradients, conversational relevance forms through connecting elements, and topic coherence maintains structural integrity through helical patterns. Through exploratory analysis of therapeutic conversations and historically significant human-AI dialogues, we demonstrate how this visualization approach reveals interaction patterns that traditional methods miss. Our work contributes a new creative framework for understanding communication that bridges data visualization, human-computer interaction, and the fundamental question of what makes dialogue meaningful in an age where humans increasingly converse with artificial minds.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkTuning: Instilling Cognitive Reflections without Distillation</title>
<link>https://arxiv.org/abs/2508.07616</link>
<guid>https://arxiv.org/abs/2508.07616</guid>
<content:encoded><![CDATA[
arXiv:2508.07616v1 Announce Type: cross 
Abstract: Recent advances in test-time scaling have led to the emergence of thinking LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL drives this self-improvement paradigm, a recent study (Gandhi et al., 2025) shows that RL alone does not truly instill these new reasoning abilities - it merely draws out behaviors already present in the base models. This raises a question: How can we train the models that don't exhibit such thinking behavior to develop it in the first place? To this end, we propose ThinkTuning, a GRPO-based interactive training approach where we augment the rollouts of a student model with the guidance from a teacher model. A simple idea from classroom practice inspires our method: a teacher poses a problem, lets the student try an answer, then gives corrective feedback -- enough to point the mind in the right direction and then show the solution. Each piece of feedback reshapes the student's thoughts, leading them to arrive at the correct solution. Similarly, we find that this type of implicit supervision through feedback from a teacher model of the same size improves the reasoning capabilities of the student model. In particular, on average, our method shows a 3.85% improvement over zero-shot baselines across benchmarks, and on MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements over the vanilla-GRPO baseline. Source code is available at https://github.com/3rdAT/ThinkTuning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization</title>
<link>https://arxiv.org/abs/2508.07629</link>
<guid>https://arxiv.org/abs/2508.07629</guid>
<content:encoded><![CDATA[
arXiv:2508.07629v1 Announce Type: cross 
Abstract: We present Klear-Reasoner, a model with long reasoning capabilities that demonstrates careful deliberation during problem solving, achieving outstanding performance across multiple benchmarks. Although there are already many excellent works related to inference models in the current community, there are still many problems with reproducing high-performance inference models due to incomplete disclosure of training details. This report provides an in-depth analysis of the reasoning model, covering the entire post-training workflow from data preparation and long Chain-of-Thought supervised fine-tuning (long CoT SFT) to reinforcement learning (RL), along with detailed ablation studies for each experimental component. For SFT data, our experiments show that a small number of high-quality data sources are more effective than a large number of diverse data sources, and that difficult samples can achieve better results without accuracy filtering. In addition, we investigate two key issues with current clipping mechanisms in RL: Clipping suppresses critical exploration signals and ignores suboptimal trajectories. To address these challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO) that gently backpropagates gradients from clipped tokens. GPPO not only enhances the model's exploration capacity but also improves its efficiency in learning from negative samples. Klear-Reasoner exhibits exceptional reasoning abilities in mathematics and programming, scoring 90.5\% on AIME 2024, 83.2\% on AIME 2025, 66.0\% on LiveCodeBench V5 and 58.1\% on LiveCodeBench V6.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents</title>
<link>https://arxiv.org/abs/2508.07642</link>
<guid>https://arxiv.org/abs/2508.07642</guid>
<content:encoded><![CDATA[
arXiv:2508.07642v1 Announce Type: cross 
Abstract: Vision-and-Language Navigation (VLN) poses significant challenges in enabling agents to interpret natural language instructions and navigate complex 3D environments. While recent progress has been driven by large-scale pre-training and data augmentation, current methods still struggle to generalize to unseen scenarios, particularly when complex spatial and temporal reasoning is required. In this work, we propose SkillNav, a modular framework that introduces structured, skill-based reasoning into Transformer-based VLN agents. Our method decomposes navigation into a set of interpretable atomic skills (e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each handled by a specialized agent. We then introduce a novel zero-shot Vision-Language Model (VLM)-based router, which dynamically selects the most suitable agent at each time step by aligning sub-goals with visual observations and historical actions. SkillNav achieves a new state-of-the-art performance on the R2R benchmark and demonstrates strong generalization to the GSA-R2R benchmark that includes novel instruction styles and unseen environments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLiClass: Generalist Lightweight Model for Sequence Classification Tasks</title>
<link>https://arxiv.org/abs/2508.07662</link>
<guid>https://arxiv.org/abs/2508.07662</guid>
<content:encoded><![CDATA[
arXiv:2508.07662v1 Announce Type: cross 
Abstract: Classification is one of the most widespread tasks in AI applications, serving often as the first step in filtering, sorting, and categorizing data. Since modern AI systems must handle large volumes of input data and early pipeline stages can propagate errors downstream, achieving high efficiency and accuracy is critical. Moreover, classification requirements can change dynamically based on user needs, necessitating models with strong zero-shot capabilities. While generative LLMs have become mainstream for zero-shot classification due to their versatility, they suffer from inconsistent instruction following and computational inefficiency. Cross-encoders, commonly used as rerankers in RAG pipelines, face a different bottleneck: they must process text-label pairs sequentially, significantly reducing efficiency with large label sets. Embedding-based approaches offer good efficiency but struggle with complex scenarios involving logical and semantic constraints. We propose GLiClass, a novel method that adapts the GLiNER architecture for sequence classification tasks. Our approach achieves strong accuracy and efficiency comparable to embedding-based methods, while maintaining the flexibility needed for zero-shot and few-shot learning scenarios. Additionally, we adapted proximal policy optimization (PPO) for multi-label text classification, enabling training classifiers in data-sparse conditions or from human feedback.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment</title>
<link>https://arxiv.org/abs/2508.07750</link>
<guid>https://arxiv.org/abs/2508.07750</guid>
<content:encoded><![CDATA[
arXiv:2508.07750v1 Announce Type: cross 
Abstract: Alignment methodologies have emerged as a critical pathway for enhancing language model alignment capabilities. While SFT (supervised fine-tuning) accelerates convergence through direct token-level loss intervention, its efficacy is constrained by offline policy trajectory. In contrast, RL(reinforcement learning) facilitates exploratory policy optimization, but suffers from low sample efficiency and stringent dependency on high-quality base models. To address these dual challenges, we propose GRAO (Group Relative Alignment Optimization), a unified framework that synergizes the respective strengths of SFT and RL through three key innovations: 1) A multi-sample generation strategy enabling comparative quality assessment via reward feedback; 2) A novel Group Direct Alignment Loss formulation leveraging intra-group relative advantage weighting; 3) Reference-aware parameter updates guided by pairwise preference dynamics. Our theoretical analysis establishes GRAO's convergence guarantees and sample efficiency advantages over conventional approaches. Comprehensive evaluations across complex human alignment tasks demonstrate GRAO's superior performance, achieving 57.70\%,17.65\% 7.95\% and 5.18\% relative improvements over SFT, DPO, PPO and GRPO baselines respectively. This work provides both a theoretically grounded alignment framework and empirical evidence for efficient capability evolution in language models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pareto Multi-Objective Alignment for Language Models</title>
<link>https://arxiv.org/abs/2508.07768</link>
<guid>https://arxiv.org/abs/2508.07768</guid>
<content:encoded><![CDATA[
arXiv:2508.07768v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed in real-world applications that require careful balancing of multiple, often conflicting, objectives, such as informativeness versus conciseness, or helpfulness versus creativity. However, current alignment methods, primarily based on RLHF, optimize LLMs toward a single reward function, resulting in rigid behavior that fails to capture the complexity and diversity of human preferences. This limitation hinders the adaptability of LLMs to practical scenarios, making multi-objective alignment (MOA) a critical yet underexplored area. To bridge this gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and computationally efficient algorithm designed explicitly for MOA in LLMs. In contrast to computationally prohibitive multi-objective optimization (MOO) methods, PAMA transforms multi-objective RLHF into a convex optimization with a closed-form solution, significantly enhancing scalability. Traditional MOO approaches suffer from prohibitive O(n^2*d) complexity, where d represents the number of model parameters, typically in the billions for LLMs, rendering direct optimization infeasible. PAMA reduces this complexity to O(n) where n is the number of objectives, enabling optimization to be completed within milliseconds. We provide theoretical guarantees that PAMA converges to a Pareto stationary point, where no objective can be improved without degrading at least one other. Extensive experiments across language models ranging from 125M to 7B parameters demonstrate PAMA's robust and effective MOA capabilities, aligning with its theoretical advantages. PAMA provides a highly efficient solution to the MOA problem that was previously considered intractable, offering a practical and theoretically grounded approach to aligning LLMs with diverse human values, paving the way for versatile and adaptable real-world AI deployments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Transcription of Acoustic Guitar Strumming Directions and Chords</title>
<link>https://arxiv.org/abs/2508.07973</link>
<guid>https://arxiv.org/abs/2508.07973</guid>
<content:encoded><![CDATA[
arXiv:2508.07973v1 Announce Type: cross 
Abstract: Automatic transcription of guitar strumming is an underrepresented and challenging task in Music Information Retrieval (MIR), particularly for extracting both strumming directions and chord progressions from audio signals. While existing methods show promise, their effectiveness is often hindered by limited datasets. In this work, we extend a multimodal approach to guitar strumming transcription by introducing a novel dataset and a deep learning-based transcription model. We collect 90 min of real-world guitar recordings using an ESP32 smartwatch motion sensor and a structured recording protocol, complemented by a synthetic dataset of 4h of labeled strumming audio. A Convolutional Recurrent Neural Network (CRNN) model is trained to detect strumming events, classify their direction, and identify the corresponding chords using only microphone audio. Our evaluation demonstrates significant improvements over baseline onset detection algorithms, with a hybrid method combining synthetic and real-world data achieving the highest accuracy for both strumming action detection and chord classification. These results highlight the potential of deep learning for robust guitar strumming transcription and open new avenues for automatic rhythm guitar analysis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Document Retrieval Coherence for Semantically Equivalent Queries</title>
<link>https://arxiv.org/abs/2508.07975</link>
<guid>https://arxiv.org/abs/2508.07975</guid>
<content:encoded><![CDATA[
arXiv:2508.07975v1 Announce Type: cross 
Abstract: Dense Retrieval (DR) models have proven to be effective for Document Retrieval and Information Grounding tasks. Usually, these models are trained and optimized for improving the relevance of top-ranked documents for a given query. Previous work has shown that popular DR models are sensitive to the query and document lexicon: small variations of it may lead to a significant difference in the set of retrieved documents. In this paper, we propose a variation of the Multi-Negative Ranking loss for training DR that improves the coherence of models in retrieving the same documents with respect to semantically similar queries. The loss penalizes discrepancies between the top-k ranked documents retrieved for diverse but semantic equivalent queries. We conducted extensive experiments on various datasets, MS-MARCO, Natural Questions, BEIR, and TREC DL 19/20. The results show that (i) models optimizes by our loss are subject to lower sensitivity, and, (ii) interestingly, higher accuracy.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Procedural Data Generation for Automatic Acoustic Guitar Fingerpicking Transcription</title>
<link>https://arxiv.org/abs/2508.07987</link>
<guid>https://arxiv.org/abs/2508.07987</guid>
<content:encoded><![CDATA[
arXiv:2508.07987v1 Announce Type: cross 
Abstract: Automatic transcription of acoustic guitar fingerpicking performances remains a challenging task due to the scarcity of labeled training data and legal constraints connected with musical recordings. This work investigates a procedural data generation pipeline as an alternative to real audio recordings for training transcription models. Our approach synthesizes training data through four stages: knowledge-based fingerpicking tablature composition, MIDI performance rendering, physical modeling using an extended Karplus-Strong algorithm, and audio augmentation including reverb and distortion. We train and evaluate a CRNN-based note-tracking model on both real and synthetic datasets, demonstrating that procedural data can be used to achieve reasonable note-tracking results. Finetuning with a small amount of real data further enhances transcription accuracy, improving over models trained exclusively on real recordings. These results highlight the potential of procedurally generated audio for data-scarce music information retrieval tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio-Thinker: Guiding Audio Language Model When and How to Think via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.08039</link>
<guid>https://arxiv.org/abs/2508.08039</guid>
<content:encoded><![CDATA[
arXiv:2508.08039v1 Announce Type: cross 
Abstract: Recent advancements in large language models, multimodal large language models, and large audio language models (LALMs) have significantly improved their reasoning capabilities through reinforcement learning with rule-based rewards. However, the explicit reasoning process has yet to show significant benefits for audio question answering, and effectively leveraging deep reasoning remains an open challenge, with LALMs still falling short of human-level auditory-language reasoning. To address these limitations, we propose Audio-Thinker, a reinforcement learning framework designed to enhance the reasoning capabilities of LALMs, with a focus on improving adaptability, consistency, and effectiveness. Our approach introduces an adaptive think accuracy reward, enabling the model to adjust its reasoning strategies based on task complexity dynamically. Furthermore, we incorporate an external reward model to evaluate the overall consistency and quality of the reasoning process, complemented by think-based rewards that help the model distinguish between valid and flawed reasoning paths during training. Experimental results demonstrate that our Audio-Thinker model outperforms existing reasoning-oriented LALMs across various benchmark tasks, exhibiting superior reasoning and generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations</title>
<link>https://arxiv.org/abs/2508.08061</link>
<guid>https://arxiv.org/abs/2508.08061</guid>
<content:encoded><![CDATA[
arXiv:2508.08061v1 Announce Type: cross 
Abstract: Event logs reflect the behavior of business processes that are mapped in organizational information systems. Predictive process monitoring (PPM) transforms these data into value by creating process-related predictions that provide the insights required for proactive interventions at process runtime. Existing PPM techniques require sufficient amounts of event data or other relevant resources that might not be readily available, preventing some organizations from utilizing PPM. The transfer learning-based PPM technique presented in this paper allows organizations without suitable event data or other relevant resources to implement PPM for effective decision support. The technique is instantiated in two real-life use cases, based on which numerical experiments are performed using event logs for IT service management processes in an intra- and inter-organizational setting. The results of the experiments suggest that knowledge of one business process can be transferred to a similar business process in the same or a different organization to enable effective PPM in the target context. With the proposed technique, organizations can benefit from transfer learning in an intra- and inter-organizational setting, where resources like pre-trained models are transferred within and across organizational boundaries.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Design Space of Visual Grounding in Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2508.08066</link>
<guid>https://arxiv.org/abs/2508.08066</guid>
<content:encoded><![CDATA[
arXiv:2508.08066v1 Announce Type: cross 
Abstract: Fine-grained multimodal capability in Multimodal Large Language Models (MLLMs) has emerged as a critical research direction, particularly for tackling the visual grounding (VG) problem. Despite the strong performance achieved by existing approaches, they often employ disparate design choices when fine-tuning MLLMs for VG, lacking systematic verification to support these designs. To bridge this gap, this paper presents a comprehensive study of various design choices that impact the VG performance of MLLMs. We conduct our analysis using LLaVA-1.5, which has been widely adopted in prior empirical studies of MLLMs. While more recent models exist, we follow this convention to ensure our findings remain broadly applicable and extendable to other architectures. We cover two key aspects: (1) exploring different visual grounding paradigms in MLLMs, identifying the most effective design, and providing our insights; and (2) conducting ablation studies on the design of grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our findings contribute to a stronger MLLM for VG, achieving improvements of +5.6% / +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches</title>
<link>https://arxiv.org/abs/2508.08088</link>
<guid>https://arxiv.org/abs/2508.08088</guid>
<content:encoded><![CDATA[
arXiv:2508.08088v1 Announce Type: cross 
Abstract: Recently, large reasoning models have demonstrated strong mathematical and coding abilities, and deep search leverages their reasoning capabilities in challenging information retrieval tasks. Existing deep search works are generally limited to a single knowledge source, either local or the Web. However, enterprises often require private deep search systems that can leverage search tools over both local and the Web corpus. Simply training an agent equipped with multiple search tools using flat reinforcement learning (RL) is a straightforward idea, but it has problems such as low training data efficiency and poor mastery of complex tools. To address the above issue, we propose a hierarchical agentic deep search framework, HierSearch, trained with hierarchical RL. At the low level, a local deep search agent and a Web deep search agent are trained to retrieve evidence from their corresponding domains. At the high level, a planner agent coordinates low-level agents and provides the final answer. Moreover, to prevent direct answer copying and error propagation, we design a knowledge refiner that filters out hallucinations and irrelevant evidence returned by low-level agents. Experiments show that HierSearch achieves better performance compared to flat RL, and outperforms various deep search and multi-source retrieval-augmented generation baselines in six benchmarks across general, finance, and medical domains.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning</title>
<link>https://arxiv.org/abs/2508.08221</link>
<guid>https://arxiv.org/abs/2508.08221</guid>
<content:encoded><![CDATA[
arXiv:2508.08221v1 Announce Type: cross 
Abstract: Reinforcement learning for LLM reasoning has rapidly emerged as a prominent research area, marked by a significant surge in related studies on both algorithmic innovations and practical applications. Despite this progress, several critical challenges remain, including the absence of standardized guidelines for employing RL techniques and a fragmented understanding of their underlying mechanisms. Additionally, inconsistent experimental settings, variations in training data, and differences in model initialization have led to conflicting conclusions, obscuring the key characteristics of these techniques and creating confusion among practitioners when selecting appropriate techniques. This paper systematically reviews widely adopted RL techniques through rigorous reproductions and isolated evaluations within a unified open-source framework. We analyze the internal mechanisms, applicable scenarios, and core principles of each technique through fine-grained experiments, including datasets of varying difficulty, model sizes, and architectures. Based on these insights, we present clear guidelines for selecting RL techniques tailored to specific setups, and provide a reliable roadmap for practitioners navigating the RL for the LLM domain. Finally, we reveal that a minimalist combination of two techniques can unlock the learning capability of critic-free policies using vanilla PPO loss. The results demonstrate that our simple combination consistently improves performance, surpassing strategies like GRPO and DAPO.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Highly Fast Text Segmentation With Pairwise Markov Chains</title>
<link>https://arxiv.org/abs/2102.11037</link>
<guid>https://arxiv.org/abs/2102.11037</guid>
<content:encoded><![CDATA[
arXiv:2102.11037v3 Announce Type: replace 
Abstract: Natural Language Processing (NLP) models' current trend consists of using increasingly more extra-data to build the best models as possible. It implies more expensive computational costs and training time, difficulties for deployment, and worries about these models' carbon footprint reveal a critical problem in the future. Against this trend, our goal is to develop NLP models requiring no extra-data and minimizing training time. To do so, in this paper, we explore Markov chain models, Hidden Markov Chain (HMC) and Pairwise Markov Chain (PMC), for NLP segmentation tasks. We apply these models for three classic applications: POS Tagging, Named-Entity-Recognition, and Chunking. We develop an original method to adapt these models for text segmentation's specific challenges to obtain relevant performances with very short training and execution times. PMC achieves equivalent results to those obtained by Conditional Random Fields (CRF), one of the most applied models for these tasks when no extra-data are used. Moreover, PMC has training times 30 times shorter than the CRF ones, which validates this model given our objectives.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Chinese are Chinese Language Models? The Puzzling Lack of Language Policy in China's LLMs</title>
<link>https://arxiv.org/abs/2407.09652</link>
<guid>https://arxiv.org/abs/2407.09652</guid>
<content:encoded><![CDATA[
arXiv:2407.09652v2 Announce Type: replace 
Abstract: Contemporary language models are increasingly multilingual, but Chinese LLM developers must navigate complex political and business considerations of language diversity. Language policy in China aims at influencing the public discourse and governing a multi-ethnic society, and has gradually transitioned from a pluralist to a more assimilationist approach since 1949. We explore the impact of these influences on current language technology. We evaluate six open-source multilingual LLMs pre-trained by Chinese companies on 18 languages, spanning a wide range of Chinese, Asian, and Anglo-European languages. Our experiments show Chinese LLMs performance on diverse languages is indistinguishable from international LLMs. Similarly, the models' technical reports also show lack of consideration for pretraining data language coverage except for English and Mandarin Chinese. Examining Chinese AI policy, model experiments, and technical reports, we find no sign of any consistent policy, either for or against, language diversity in China's LLM development. This leaves a puzzling fact that while China regulates both the languages people use daily as well as language model development, they do not seem to have any policy on the languages in language models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-AI Bias: large language models favor communications generated by large language models</title>
<link>https://arxiv.org/abs/2407.12856</link>
<guid>https://arxiv.org/abs/2407.12856</guid>
<content:encoded><![CDATA[
arXiv:2407.12856v2 Announce Type: replace 
Abstract: Are large language models (LLMs) biased in favor of communications produced by LLMs, leading to possible antihuman discrimination? Using a classical experimental design inspired by employment discrimination studies, we tested widely used LLMs, including GPT-3.5, GPT-4 and a selection of recent open-weight models in binary choice scenarios. These involved LLM-based assistants selecting between goods (the goods we study include consumer products, academic papers, and film-viewings) described either by humans or LLMs. Our results show a consistent tendency for LLM-based AIs to prefer LLM-presented options. This suggests the possibility of future AI systems implicitly discriminating against humans as a class, giving AI agents and AI-assisted humans an unfair advantage.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain of Thought Still Thinks Fast: APriCoT Helps with Thinking Slow</title>
<link>https://arxiv.org/abs/2408.08651</link>
<guid>https://arxiv.org/abs/2408.08651</guid>
<content:encoded><![CDATA[
arXiv:2408.08651v3 Announce Type: replace 
Abstract: Language models are known to absorb biases from their training data, leading to predictions driven by statistical regularities rather than semantic relevance. We investigate the impact of these biases on answer choice preferences in the Massive Multi-Task Language Understanding (MMLU) task. Our findings show that these biases are predictive of model preference and mirror human test-taking strategies even when chain of thought (CoT) reasoning is used. To address this issue, we introduce Counterfactual Prompting with Agnostically Primed CoT (APriCoT). We demonstrate that while Counterfactual Prompting with CoT alone is insufficient to mitigate bias, APriCoT effectively reduces the influence of base-rate probabilities while improving overall accuracy. Our results suggest that mitigating bias requires a slow thinking process which CoT alone may not provide as it tends to reinforce fast thinking model bias under some prompting methodologies. APriCoT is a step toward developing more robust and fair language models that can think slow.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio</title>
<link>https://arxiv.org/abs/2409.06624</link>
<guid>https://arxiv.org/abs/2409.06624</guid>
<content:encoded><![CDATA[
arXiv:2409.06624v3 Announce Type: replace 
Abstract: Large Language Models (LLM) often need to be Continual Pre-Trained (CPT) to obtain unfamiliar language skills or adapt to new domains. The huge training cost of CPT often asks for cautious choice of key hyper-parameters such as the mixture ratio of extra language or domain corpus. However, there is no systematic study that bridges the gap between the optimal mixture ratio and the actual model performance, and the gap between experimental scaling law and the actual deployment in the full model size. In this paper, we perform CPT on Llama-3 8B and 70B to enhance its Chinese ability. We study the optimal correlation between the Additional Language Mixture Ratio (ALMR) and the Learning Rate (LR) on the 8B size which directly indicates the optimal experimental setup. By thorough choice of hyper-parameter, and subsequent fine-tuning, the model capability is improved not only on the Chinese-related benchmark but also in some specific domains including math, coding, and emotional intelligence. We deploy the final 70B version of LLM on a real-life chat system which obtains satisfying performance.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLAIR-A: Leveraging Large Language Models to Judge Audio Captions</title>
<link>https://arxiv.org/abs/2409.12962</link>
<guid>https://arxiv.org/abs/2409.12962</guid>
<content:encoded><![CDATA[
arXiv:2409.12962v2 Announce Type: replace 
Abstract: The Automated Audio Captioning (AAC) task asks models to generate natural language descriptions of an audio input. Evaluating these machine-generated audio captions is a complex task that requires considering diverse factors, among them, auditory scene understanding, sound-object inference, temporal coherence, and the environmental context of the scene. While current methods focus on specific aspects, they often fail to provide an overall score that aligns well with human judgment. In this work, we propose CLAIR-A, a simple and flexible method that leverages the zero-shot capabilities of large language models (LLMs) to evaluate candidate audio captions by directly asking LLMs for a semantic distance score. In our evaluations, CLAIR-A better predicts human judgements of quality compared to traditional metrics, with a 5.8% relative accuracy improvement compared to the domain-specific FENSE metric and up to 11% over the best general-purpose measure on the Clotho-Eval dataset. Moreover, CLAIR-A offers more transparency by allowing the language model to explain the reasoning behind its scores, with these explanations rated up to 30% better by human evaluators than those provided by baseline methods. CLAIR-A is made publicly available at https://github.com/DavidMChan/clair-a.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closer Look at Machine Unlearning for Large Language Models</title>
<link>https://arxiv.org/abs/2410.08109</link>
<guid>https://arxiv.org/abs/2410.08109</guid>
<content:encoded><![CDATA[
arXiv:2410.08109v5 Announce Type: replace 
Abstract: Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlatQuant: Flatness Matters for LLM Quantization</title>
<link>https://arxiv.org/abs/2410.09426</link>
<guid>https://arxiv.org/abs/2410.09426</guid>
<content:encoded><![CDATA[
arXiv:2410.09426v4 Announce Type: replace 
Abstract: Recently, quantization has been widely used for the compression and acceleration of large language models (LLMs). Due to the outliers in LLMs, it is crucial to flatten weights and activations to minimize quantization error with equally spaced quantization points. Prior research explores various pre-quantization transformations to suppress outliers, such as per-channel scaling and Hadamard transformation. However, we observe that these transformed weights and activations can still exhibit steep and dispersed distributions. In this paper, we propose FlatQuant (Fast and Learnable Affine Transformation), a new post-training quantization approach that enhances the flatness of weights and activations. Our approach identifies optimal affine transformations for each linear layer, calibrated in hours via a lightweight objective. To reduce runtime overhead of affine transformation, we apply Kronecker product with two lightweight matrices, and fuse all operations in FlatQuant into a single kernel. Extensive experiments demonstrate that FlatQuant establishes a new state-of-the-art benchmark for quantization. For example, it achieves less than 1\% accuracy drop for W4A4 quantization on the LLaMA-3-70B model, surpassing SpinQuant by 7.5\%. Additionally, it provides up to 2.3x prefill speedup and 1.7x decoding speedup compared to the FP16 model. Code is available at: https://github.com/ruikangliu/FlatQuant.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strengthening False Information Propagation Detection: Leveraging SVM and Sophisticated Text Vectorization Techniques in comparison to BERT</title>
<link>https://arxiv.org/abs/2411.12703</link>
<guid>https://arxiv.org/abs/2411.12703</guid>
<content:encoded><![CDATA[
arXiv:2411.12703v3 Announce Type: replace 
Abstract: The rapid spread of misinformation, particularly through online platforms, underscores the urgent need for reliable detection systems. This study explores the utilization of machine learning and natural language processing, specifically Support Vector Machines (SVM) and BERT, to detect fake news. We employ three distinct text vectorization methods for SVM: Term Frequency Inverse Document Frequency (TF-IDF), Word2Vec, and Bag of Words (BoW), evaluating their effectiveness in distinguishing between genuine and fake news. Additionally, we compare these methods against the transformer large language model, BERT. Our comprehensive approach includes detailed preprocessing steps, rigorous model implementation, and thorough evaluation to determine the most effective techniques. The results demonstrate that while BERT achieves superior accuracy with 99.98% and an F1-score of 0.9998, the SVM model with a linear kernel and BoW vectorization also performs exceptionally well, achieving 99.81% accuracy and an F1-score of 0.9980. These findings highlight that, despite BERT's superior performance, SVM models with BoW and TF-IDF vectorization methods come remarkably close, offering highly competitive performance with the advantage of lower computational requirements.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebWalker: Benchmarking LLMs in Web Traversal</title>
<link>https://arxiv.org/abs/2501.07572</link>
<guid>https://arxiv.org/abs/2501.07572</guid>
<content:encoded><![CDATA[
arXiv:2501.07572v3 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering. However, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information. To address it, we introduce WebWalkerQA, a benchmark designed to assess the ability of LLMs to perform web traversal. It evaluates the capacity of LLMs to traverse a website's subpages to extract high-quality data systematically. We propose WebWalker, which is a multi-agent framework that mimics human-like web navigation through an explore-critic paradigm. Extensive experimental results show that WebWalkerQA is challenging and demonstrates the effectiveness of RAG combined with WebWalker, through the horizontal and vertical integration in real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models</title>
<link>https://arxiv.org/abs/2501.13428</link>
<guid>https://arxiv.org/abs/2501.13428</guid>
<content:encoded><![CDATA[
arXiv:2501.13428v4 Announce Type: replace 
Abstract: Large language models have achieved remarkable success in recent years, primarily due to the implementation of self-attention mechanisms. However, traditional Softmax attention suffers from numerical instability and reduced performance as the length of inference tokens increases. This paper addresses these issues by proposing a new design principle for attention, viewing it as a two-stage process. We first decompose the Softmax operation into a non-linear positivity transformation and an $l_1$-normalisation step, identifying the latter as essential for maintaining model performance. In the first stage, we replace the standard exponential function with the more numerically stable Softplus activation and introduce a dynamic scale factor based on invariance entropy, creating a novel attention mechanism that outperforms conventional Softmax attention. In the second stage, we introduce a re-weighting mechanism that sharpens the attention distribution, amplifying significant weights while diminishing weaker ones. This enables the model to concentrate more effectively on relevant tokens and fundamentally improves length extrapolation. When combined, this two-stage approach ensures numerical stability and dramatically improves length extrapolation, maintaining a nearly constant validation loss at 16$\times$ the training length while achieving superior results on challenging long-context retrieval tasks and standard downstream benchmarks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Your Model Ranking on Chatbot Arena by Vote Rigging</title>
<link>https://arxiv.org/abs/2501.17858</link>
<guid>https://arxiv.org/abs/2501.17858</guid>
<content:encoded><![CDATA[
arXiv:2501.17858v2 Announce Type: replace 
Abstract: Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles, where users vote for their preferred response from two randomly sampled anonymous models. While Chatbot Arena is widely regarded as a reliable LLM ranking leaderboard, we show that crowdsourced voting can be rigged to improve (or decrease) the ranking of a target model $m_{t}$. We first introduce a straightforward target-only rigging strategy that focuses on new battles involving $m_{t}$, identifying it via watermarking or a binary classifier, and exclusively voting for $m_{t}$ wins. However, this strategy is practically inefficient because there are over $190$ models on Chatbot Arena and on average only about $1\%$ of new battles will involve $m_{t}$. To overcome this, we propose omnipresent rigging strategies, exploiting the Elo rating mechanism of Chatbot Arena that any new vote on a battle can influence the ranking of the target model $m_{t}$, even if $m_{t}$ is not directly involved in the battle. We conduct experiments on around $1.7$ million historical votes from the Chatbot Arena Notebook, showing that omnipresent rigging strategies can improve model rankings by rigging only hundreds of new votes. While we have evaluated several defense mechanisms, our findings highlight the importance of continued efforts to prevent vote rigging. Our code is available at https://github.com/sail-sg/Rigging-ChatbotArena.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReGLA: Refining Gated Linear Attention</title>
<link>https://arxiv.org/abs/2502.01578</link>
<guid>https://arxiv.org/abs/2502.01578</guid>
<content:encoded><![CDATA[
arXiv:2502.01578v3 Announce Type: replace 
Abstract: Recent advancements in Large Language Models (LLMs) have set themselves apart with their exceptional performance in complex language modelling tasks. However, these models are also known for their significant computational and storage requirements, primarily due to the quadratic computation complexity of softmax attention. To mitigate this issue, linear attention has been designed to reduce the quadratic space-time complexity that is inherent in standard transformers. In this work, we embarked on a comprehensive exploration of three key components that substantially impact the performance of the Gated Linear Attention module: feature maps, normalization, and the gating mechanism. We developed a feature mapping function to address some crucial issues that previous suggestions overlooked. Then we offered further rationale for the integration of normalization layers to stabilize the training process. Moreover, we explored the saturation phenomenon of the gating mechanism and augmented it with a refining module. We conducted extensive experiments and showed our architecture outperforms previous Gated Linear Attention mechanisms in extensive tasks including training from scratch and post-linearization with continual pre-training.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Depression in Screening Interviews from Interactive Multi-Theme Collaboration</title>
<link>https://arxiv.org/abs/2502.12204</link>
<guid>https://arxiv.org/abs/2502.12204</guid>
<content:encoded><![CDATA[
arXiv:2502.12204v2 Announce Type: replace 
Abstract: Automatic depression detection provides cues for early clinical intervention by clinicians. Clinical interviews for depression detection involve dialogues centered around multiple themes. Existing studies primarily design end-to-end neural network models to capture the hierarchical structure of clinical interview dialogues. However, these methods exhibit defects in modeling the thematic content of clinical interviews: 1) they fail to capture intra-theme and inter-theme correlation explicitly, and 2) they do not allow clinicians to intervene and focus on themes of interest. To address these issues, this paper introduces an interactive depression detection framework. This framework leverages in-context learning techniques to identify themes in clinical interviews and then models both intra-theme and inter-theme correlation. Additionally, it employs AI-driven feedback to simulate the interests of clinicians, enabling interactive adjustment of theme importance. PDIMC achieves absolute improvements of 35\% and 12\% compared to the state-of-the-art on the depression detection dataset DAIC-WOZ, which demonstrates the effectiveness of modeling theme correlation and incorporating interactive external feedback.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALFA: Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning</title>
<link>https://arxiv.org/abs/2502.14860</link>
<guid>https://arxiv.org/abs/2502.14860</guid>
<content:encoded><![CDATA[
arXiv:2502.14860v2 Announce Type: replace 
Abstract: Large language models (LLMs) often fail to ask effective questions under uncertainty, making them unreliable in domains where proactive information-gathering is essential for decision-making. We present ALignment via Fine-grained Attributes, (ALFA) a framework that improves LLM question-asking by (i) decomposing the notion of a "good" question into a set of theory-grounded attributes (e.g., clarity, relevance), (ii) controllably synthesizing attribute-specific question variations, and (iii) aligning models via preference-based optimization to explicitly learn to ask better questions along these fine-grained attributes. Focusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs dataset, composed of 17k real-world clinical interactions augmented with 80k attribute-specific preference pairs of follow-up questions, as well as a novel expert-annotated interactive healthcare QA task to evaluate question-asking abilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on MediQ-AskDocs compared to SoTA instruction-tuned LLMs, with a question-level win-rate of 64.4% and strong generalizability. Our findings suggest that explicitly guiding question-asking with structured, fine-grained attributes offers a scalable path to improve LLMs, especially in expert application domains.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>URO-Bench: Towards Comprehensive Evaluation for End-to-End Spoken Dialogue Models</title>
<link>https://arxiv.org/abs/2502.17810</link>
<guid>https://arxiv.org/abs/2502.17810</guid>
<content:encoded><![CDATA[
arXiv:2502.17810v3 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have driven significant progress in end-to-end spoken dialogue models (SDMs). In contrast to text-based LLMs, the evaluation framework for SDMs should encompass both cognitive dimensions (e.g., logical reasoning, knowledge) and speech-related aspects (e.g., paralinguistic cues, audio quality). However, there is still a lack of comprehensive evaluations for SDMs in speech-to-speech (S2S) scenarios. To address this gap, we propose URO-Bench, an extensive benchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers evaluations about multilingualism, multi-round dialogues, and paralinguistics. Our benchmark is divided into two difficulty levels: basic track and pro track, each comprising 20 test sets, evaluating the spoken dialogue model's abilities in Understanding, Reasoning, and Oral conversation. Evaluations on our proposed benchmark reveal that current open-source SDMs perform rather well in daily QA tasks, but lag behind their backbone LLMs in terms of instruction-following ability and also suffer from catastrophic forgetting. Their performance in advanced evaluations of paralinguistic information and audio understanding remains subpar, highlighting the need for further research in this direction. We hope that URO-Bench can facilitate the development of spoken dialogue models by providing a multifaceted evaluation of existing models and helping to track progress in this area.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invisible Walls in Cities: Leveraging Large Language Models to Predict Urban Segregation Experience with Social Media Content</title>
<link>https://arxiv.org/abs/2503.04773</link>
<guid>https://arxiv.org/abs/2503.04773</guid>
<content:encoded><![CDATA[
arXiv:2503.04773v3 Announce Type: replace 
Abstract: Understanding experienced segregation in urban daily life is crucial for addressing societal inequalities and fostering inclusivity. The abundance of user-generated reviews on social media encapsulates nuanced perceptions and feelings associated with different places, offering rich insights into segregation. However, leveraging this data poses significant challenges due to its vast volume, ambiguity, and confluence of diverse perspectives. To tackle these challenges, we propose using Large Language Models (LLMs) to automate online review mining for segregation prediction. We design a Reflective LLM Coder to digest social media content into insights consistent with real-world feedback, and eventually produce a codebook capturing key dimensions that signal segregation experience, such as cultural resonance and appeal, accessibility and convenience, and community engagement and local involvement. Guided by the codebook, LLMs can generate both informative review summaries and ratings for segregation prediction. Moreover, we design a REasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and embedding capabilities of language models to integrate multi-channel features for segregation prediction. Experiments on real-world data demonstrate that our framework greatly improves prediction accuracy, with a 22.79% elevation in R2 and a 9.33% reduction in MSE. The derived codebook is generalizable across three different cities, consistently improving prediction accuracy. Moreover, our user study confirms that the codebook-guided summaries provide cognitive gains for human participants in perceiving POIs' social inclusiveness. Our study marks an important step toward understanding implicit social barriers and inequalities, demonstrating the great potential of promoting social inclusiveness with AI.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression</title>
<link>https://arxiv.org/abs/2503.11132</link>
<guid>https://arxiv.org/abs/2503.11132</guid>
<content:encoded><![CDATA[
arXiv:2503.11132v3 Announce Type: replace 
Abstract: Multi-head latent attention (MLA) is designed to optimize KV cache memory through low-rank key-value joint compression. Rather than caching keys and values separately, MLA stores their compressed latent representations, reducing memory overhead while maintaining the performance. While MLA improves memory efficiency without compromising language model accuracy, its major limitation lies in its integration during the pre-training phase, requiring models to be trained from scratch. This raises a key question: can we use MLA's benefits fully or partially in models that have already been pre-trained with different attention mechanisms? In this paper, we propose X-EcoMLA to deploy post training distillation to enable the upcycling of Transformer-based attention into an efficient hybrid MLA variant through lightweight post-training adaptation, bypassing the need for extensive pre-training. We demonstrate that leveraging the dark knowledge of a well-trained model can enhance training accuracy and enable extreme KV cache compression in MLA without compromising model performance. The experimental results show that our proposed method can effectively compress the KV cache while preserving the performance on the benchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression achieves the same average score by using only 3.6B training tokens and 70 GPU hours on AMD MI300, whereas a 10.6x compression have less than 0.1% average score drop with 7B training tokens and 140 GPU hours. The code for this work is available at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Both Direct and Indirect Evidence Contribute to Dative Alternation Preferences in Language Models</title>
<link>https://arxiv.org/abs/2503.20850</link>
<guid>https://arxiv.org/abs/2503.20850</guid>
<content:encoded><![CDATA[
arXiv:2503.20850v3 Announce Type: replace 
Abstract: Language models (LMs) tend to show human-like preferences on a number of syntactic phenomena, but the extent to which these are attributable to direct exposure to the phenomena or more general properties of language is unclear. We explore this with the English dative alternation (DO: "gave Y the X" vs. PO: "gave the X to Y"), using a controlled rearing paradigm wherein we iteratively train small LMs on systematically manipulated input. We focus on two properties that affect the choice of alternant: length and animacy. Both properties are directly present in datives but also reflect more global tendencies for shorter elements to precede longer ones and animates to precede inanimates. First, by manipulating and ablating datives for these biases in the input, we show that direct evidence of length and animacy matters, but easy-first preferences persist even without such evidence. Then, using LMs trained on systematically perturbed datasets to manipulate global length effects (re-linearizing sentences globally while preserving dependency structure), we find that dative preferences can emerge from indirect evidence. We conclude that LMs' emergent syntactic preferences come from a mix of direct and indirect sources.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\mu$KE: Matryoshka Unstructured Knowledge Editing of Large Language Models</title>
<link>https://arxiv.org/abs/2504.01196</link>
<guid>https://arxiv.org/abs/2504.01196</guid>
<content:encoded><![CDATA[
arXiv:2504.01196v2 Announce Type: replace 
Abstract: Large language models (LLMs) have emerged as powerful knowledge bases yet are limited by static training data, leading to issues such as hallucinations and safety risks. Editing a model's internal knowledge through the locate-and-edit paradigm has proven a cost-effective alternative to retraining, though current unstructured approaches, especially window-based autoregressive methods, often disrupt the causal dependency between early memory updates and later output tokens. In this work, we first theoretically analyze these limitations and then introduce Matryoshka Unstructured Knowledge Editing ($\mu$KE), a novel memory update mechanism that preserves such dependencies via a Matryoshka-style objective and adaptive loss coefficients. Empirical evaluations on two models across four benchmarks demonstrate that $\mu$KE improves edit efficacy by up to 12.33% over state-of-the-art methods, and remains robust when applied to diverse formatted edits, underscoring its potential for effective unstructured knowledge editing in LLMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Vocabulary Constraints with Pixel-level Fallback</title>
<link>https://arxiv.org/abs/2504.02122</link>
<guid>https://arxiv.org/abs/2504.02122</guid>
<content:encoded><![CDATA[
arXiv:2504.02122v2 Announce Type: replace 
Abstract: Subword tokenization requires balancing computational efficiency and vocabulary coverage, which often leads to suboptimal performance on languages and scripts not prioritized during training. We propose to augment pretrained language models with a vocabulary-free encoder that generates input embeddings from text rendered as pixels. Through experiments on English-centric language models, we demonstrate that our approach substantially improves machine translation performance and facilitates effective cross-lingual transfer, outperforming tokenizer-based methods. Furthermore, we find that pixel-based representations outperform byte-level approaches and standard vocabulary expansion. Our approach enhances the multilingual capabilities of monolingual language models without extensive retraining and reduces decoding latency via input compression.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence</title>
<link>https://arxiv.org/abs/2504.02904</link>
<guid>https://arxiv.org/abs/2504.02904</guid>
<content:encoded><![CDATA[
arXiv:2504.02904v2 Announce Type: replace 
Abstract: Post-training is essential for the success of large language models (LLMs), transforming pre-trained base models into more useful and aligned post-trained models. While plenty of works have studied post-training algorithms and evaluated post-training models by their outputs, it remains understudied how post-training reshapes LLMs internally. In this paper, we compare base and post-trained LLMs mechanistically from four perspectives to better understand post-training effects. Our findings across model families and datasets reveal that: (1) Post-training does not change the factual knowledge storage locations, and it adapts knowledge representations from the base model while developing new knowledge representations; (2) Both truthfulness and refusal can be represented by vectors in the hidden representation space. The truthfulness direction is highly similar between the base and post-trained model, and it is effectively transferable for interventions; (3) The refusal direction is different between the base and post-trained models, and it shows limited forward transferability; (4) Differences in confidence between the base and post-trained models cannot be attributed to entropy neurons. Our study provides insights into the fundamental mechanisms preserved and altered during post-training, facilitates downstream tasks like model steering, and could potentially benefit future research in interpretability and LLM post-training. Our code is publicly available at https://github.com/HZD01/post-training-mechanistic-analysis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NoveltyBench: Evaluating Language Models for Humanlike Diversity</title>
<link>https://arxiv.org/abs/2504.05228</link>
<guid>https://arxiv.org/abs/2504.05228</guid>
<content:encoded><![CDATA[
arXiv:2504.05228v4 Announce Type: replace 
Abstract: Language models have demonstrated remarkable capabilities on standard benchmarks, yet they struggle increasingly from mode collapse, the inability to generate diverse and novel outputs. Our work introduces NoveltyBench, a benchmark specifically designed to evaluate the ability of language models to produce multiple distinct and high-quality outputs. NoveltyBench utilizes prompts curated to elicit diverse answers and filtered real-world user queries. Evaluating 20 leading language models, we find that current state-of-the-art systems generate significantly less diversity than human writers. Notably, larger models within a family often exhibit less diversity than their smaller counterparts, challenging the notion that capability on standard benchmarks translates directly to generative utility. While prompting strategies like in-context regeneration can elicit diversity, our findings highlight a fundamental lack of distributional diversity in current models, reducing their utility for users seeking varied responses and suggesting the need for new training and evaluation paradigms that prioritize diversity alongside quality.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering</title>
<link>https://arxiv.org/abs/2504.07583</link>
<guid>https://arxiv.org/abs/2504.07583</guid>
<content:encoded><![CDATA[
arXiv:2504.07583v3 Announce Type: replace 
Abstract: Despite the steady progress in machine translation evaluation, existing automatic metrics struggle to capture how well meaning is preserved beyond sentence boundaries. We posit that reliance on a single intrinsic quality score, trained to mimic human judgments, might be insufficient for evaluating translations of long, complex passages, and a more ``pragmatic'' approach that assesses how accurately key information is conveyed by a translation in context is needed. We introduce TREQA (Translation Evaluation via Question-Answering), a framework that extrinsically evaluates translation quality by assessing how accurately candidate translations answer reading comprehension questions that target key information in the original source or reference texts. In challenging domains that require long-range understanding, such as literary texts, we show that TREQA is competitive with and, in some cases, outperforms state-of-the-art neural and LLM-based metrics in ranking alternative paragraph-level translations, despite never being explicitly optimized to correlate with human judgments. Furthermore, the generated questions and answers offer interpretability: empirical analysis shows that they effectively target translation errors identified by experts in evaluated datasets. Our code is available at https://github.com/deep-spin/treqa
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUDsim: Quantifying Discourse Similarities in LLM-Generated Text</title>
<link>https://arxiv.org/abs/2504.09373</link>
<guid>https://arxiv.org/abs/2504.09373</guid>
<content:encoded><![CDATA[
arXiv:2504.09373v2 Announce Type: replace 
Abstract: As large language models become increasingly capable at various writing tasks, their weakness at generating unique and creative content becomes a major liability. Although LLMs have the ability to generate text covering diverse topics, there is an overall sense of repetitiveness across texts that we aim to formalize and quantify via a similarity metric. The familiarity between documents arises from the persistence of underlying discourse structures. However, existing similarity metrics dependent on lexical overlap and syntactic patterns largely capture $\textit{content}$ overlap, thus making them unsuitable for detecting $\textit{structural}$ similarities. We introduce an abstraction based on linguistic theories in Questions Under Discussion (QUD) and question semantics to help quantify differences in discourse progression. We then use this framework to build $\textbf{QUDsim}$, a similarity metric that can detect discursive parallels between documents. Using QUDsim, we find that LLMs often reuse discourse structures (more so than humans) across samples, even when content differs. Furthermore, LLMs are not only repetitive and structurally uniform, but are also divergent from human authors in the types of structures they use.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning</title>
<link>https://arxiv.org/abs/2504.16832</link>
<guid>https://arxiv.org/abs/2504.16832</guid>
<content:encoded><![CDATA[
arXiv:2504.16832v2 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that require intermediate reasoning steps prior to generating a final answer. In this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model inspired by the finetuning strategy based on Group Relative Policy Optimization. We also leverage a high-quality Vietnamese synthesized reasoning dataset and design two reward functions to tackle the main limitations of this technique: (i) language mixing, where we explicitly detect the presence of biased language characters during the process of sampling tokens, and (ii) we leverage Sentence Transformer-based models to ensure that the generated reasoning content maintains factual correctness and does not distort the final output. Experimental results on the Vietnamese dataset from the VLSP 2023 Challenge demonstrate that our model outperforms prior works and enhances linguistic consistency in its responses. Furthermore, we extend our evaluation to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of our reasoning method compared to few-shot prompting techniques.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning with Diffusion Models for Target-Oriented Dialogue Systems</title>
<link>https://arxiv.org/abs/2504.16858</link>
<guid>https://arxiv.org/abs/2504.16858</guid>
<content:encoded><![CDATA[
arXiv:2504.16858v2 Announce Type: replace 
Abstract: Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM era, where strategic dialogue planning is crucial for directing conversations toward specific targets. However, existing dialogue planning methods generate dialogue plans in a step-by-step sequential manner, and may suffer from compounding errors and myopic actions. To address these limitations, we introduce a novel dialogue planning framework, DiffTOD, which leverages diffusion models to enable non-sequential dialogue planning. DiffTOD formulates dialogue planning as a trajectory generation problem with conditional guidance, and leverages a diffusion language model to estimate the likelihood of the dialogue trajectory. To optimize the dialogue action strategies, DiffTOD introduces three tailored guidance mechanisms for different target types, offering flexible guidance toward diverse TOD targets at test time. Extensive experiments across three diverse TOD settings show that DiffTOD can effectively perform non-myopic lookahead exploration and optimize action strategies over a long horizon through non-sequential dialogue planning, and demonstrates strong flexibility across complex and diverse dialogue scenarios. Our code and data are accessible through https://github.com/ninglab/DiffTOD.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control</title>
<link>https://arxiv.org/abs/2504.17130</link>
<guid>https://arxiv.org/abs/2504.17130</guid>
<content:encoded><![CDATA[
arXiv:2504.17130v3 Announce Type: replace 
Abstract: Large language models (LLMs) have transformed the way we access information. These models are often tuned to refuse to comply with requests that are considered harmful and to produce responses that better align with the preferences of those who control the models. To understand how this "censorship" works. We use representation engineering techniques to study open-weights safety-tuned models. We present a method for finding a refusal--compliance vector that detects and controls the level of censorship in model outputs. We also analyze recent reasoning LLMs, distilled from DeepSeek-R1, and uncover an additional dimension of censorship through "thought suppression". We show a similar approach can be used to find a vector that suppresses the model's reasoning process, allowing us to remove censorship by applying the negative multiples of this vector. Our code is publicly available at: https://github.com/hannahxchen/llm-censorship-steering
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAIR: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction</title>
<link>https://arxiv.org/abs/2504.18938</link>
<guid>https://arxiv.org/abs/2504.18938</guid>
<content:encoded><![CDATA[
arXiv:2504.18938v2 Announce Type: replace 
Abstract: Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens in sentences. Traditional CSC focuses on equal length correction and uses pretrained language models (PLMs). While Large Language Models (LLMs) have shown remarkable success in identifying and rectifying potential errors, they often struggle with adapting to domain-specific corrections, especially when encountering terminologies in specialized domains. To address domain adaptation, we propose a \textbf{R}etrieval-\textbf{A}ugmented \textbf{I}terative \textbf{R}efinement (RAIR) framework. Our approach constructs a retrieval corpus adaptively from domain-specific training data and dictionaries, employing a fine-tuned retriever to ensure that the retriever catches the error correction pattern. We also extend equal-length into variable-length correction scenarios. Extensive experiments demonstrate that our framework outperforms current approaches in domain spelling correction and significantly improves the performance of LLMs in variable-length scenarios.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch</title>
<link>https://arxiv.org/abs/2505.03733</link>
<guid>https://arxiv.org/abs/2505.03733</guid>
<content:encoded><![CDATA[
arXiv:2505.03733v2 Announce Type: replace 
Abstract: LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2\%, surpassing the performance of the best proprietary model.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Prompt Optimizers: From Prompt Merits to Optimization</title>
<link>https://arxiv.org/abs/2505.09930</link>
<guid>https://arxiv.org/abs/2505.09930</guid>
<content:encoded><![CDATA[
arXiv:2505.09930v3 Announce Type: replace 
Abstract: Prompt optimization (PO) provides a practical way to improve response quality when users lack the time or expertise to manually craft effective prompts. Existing methods typically rely on LLMs' self-generation ability to optimize prompts. However, due to limited downward compatibility, the instruction-heavy prompts generated by advanced LLMs can overwhelm lightweight inference models and degrade response quality, while also lacking interpretability due to implicit optimization. In this work, we rethink prompt optimization through the lens of explicit and interpretable design. We first identify a set of model-agnostic prompt quality merits and empirically validate their effectiveness in enhancing prompt and response quality. We then introduce MePO, a merit-guided, locally deployable prompt optimizer trained on our merit-guided prompt preference dataset generated by a lightweight LLM. MePO avoids online optimization, reduces privacy concerns, and, by learning clear, interpretable merits, generalizes effectively to both large-scale and lightweight inference models. Experiments demonstrate that MePO achieves better results across diverse tasks and model types, offering a scalable and robust solution for real-world deployment.The code, model and dataset can be found in https://github.com/MidiyaZhu/MePO
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding the Multimodal Mind: Generalizable Brain-to-Text Translation via Multimodal Alignment and Adaptive Routing</title>
<link>https://arxiv.org/abs/2505.10356</link>
<guid>https://arxiv.org/abs/2505.10356</guid>
<content:encoded><![CDATA[
arXiv:2505.10356v2 Announce Type: replace 
Abstract: Decoding language from the human brain remains a grand challenge for Brain-Computer Interfaces (BCIs). Current approaches typically rely on unimodal brain representations, neglecting the brain's inherently multimodal processing. Inspired by the brain's associative mechanisms, where viewing an image can evoke related sounds and linguistic representations, we propose a unified framework that leverages Multimodal Large Language Models (MLLMs) to align brain signals with a shared semantic space encompassing text, images, and audio. A router module dynamically selects and fuses modality-specific brain features according to the characteristics of each stimulus. Experiments on various fMRI datasets with textual, visual, and auditory stimuli demonstrate state-of-the-art performance, achieving an 8.48% improvement on the most commonly used benchmark. We further extend our framework to EEG and MEG data, demonstrating flexibility and robustness across varying temporal and spatial resolutions. To our knowledge, this is the first unified BCI architecture capable of robustly decoding multimodal brain activity across diverse brain signals and stimulus types, offering a flexible solution for real-world applications.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations</title>
<link>https://arxiv.org/abs/2505.12560</link>
<guid>https://arxiv.org/abs/2505.12560</guid>
<content:encoded><![CDATA[
arXiv:2505.12560v2 Announce Type: replace 
Abstract: Existing datasets available for crosslinguistic investigations have tended to focus on large amounts of data for a small group of languages or a small amount of data for a large number of languages. This means that claims based on these datasets are limited in what they reveal about universal properties of the human language faculty. While this has begun to change through the efforts of projects seeking to develop tagged corpora for a large number of languages, such efforts are still constrained by limits on resources. The current paper reports on a large tagged parallel dataset which has been developed to partially address this issue. The taggedPBC contains POS-tagged parallel text data from more than 1,940 languages, representing 155 language families and 78 isolates, dwarfing previously available resources. The accuracy of particular tags in this dataset is shown to correlate well with both existing SOTA taggers for high-resource languages (SpaCy, Trankit) as well as hand-tagged corpora (Universal Dependencies Treebanks). Additionally, a novel measure derived from this dataset, the N1 ratio, correlates with expert determinations of intransitive word order in three typological databases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier trained on this feature can accurately identify basic intransitive word order for languages not in those databases. While much work is still needed to expand and develop this dataset, the taggedPBC is an important step to enable corpus-based crosslinguistic investigations, and is made available for research and collaboration via GitHub.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization</title>
<link>https://arxiv.org/abs/2505.15918</link>
<guid>https://arxiv.org/abs/2505.15918</guid>
<content:encoded><![CDATA[
arXiv:2505.15918v2 Announce Type: replace 
Abstract: In this work, we evaluate the potential of Large Language Models (LLMs) in building Bayesian Networks (BNs) by approximating domain expert priors. LLMs have demonstrated potential as factual knowledge bases; however, their capability to generate probabilistic knowledge about real-world events remains understudied. We explore utilizing the probabilistic knowledge inherent in LLMs to derive probability estimates for statements regarding events and their relationships within a BN. Using LLMs in this context allows for the parameterization of BNs, enabling probabilistic modeling within specific domains. Our experiments on eighty publicly available Bayesian Networks, from healthcare to finance, demonstrate that querying LLMs about the conditional probabilities of events provides meaningful results when compared to baselines, including random and uniform distributions, as well as approaches based on next-token generation probabilities. We explore how these LLM-derived distributions can serve as expert priors to refine distributions extracted from data, especially when data is scarce. Overall, this work introduces a promising strategy for automatically constructing Bayesian Networks by combining probabilistic knowledge extracted from LLMs with real-world data. Additionally, we establish the first comprehensive baseline for assessing LLM performance in extracting probabilistic knowledge.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebDancer: Towards Autonomous Information Seeking Agency</title>
<link>https://arxiv.org/abs/2505.22648</link>
<guid>https://arxiv.org/abs/2505.22648</guid>
<content:encoded><![CDATA[
arXiv:2505.22648v3 Announce Type: replace 
Abstract: Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Document Valuation in LLM Summaries: A Cluster Shapley Approach</title>
<link>https://arxiv.org/abs/2505.23842</link>
<guid>https://arxiv.org/abs/2505.23842</guid>
<content:encoded><![CDATA[
arXiv:2505.23842v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly used in systems that retrieve and summarize content from multiple sources, such as search engines and AI assistants. While these models enhance user experience by generating coherent summaries, they obscure the contributions of original content creators, raising concerns about credit attribution and compensation. We address the challenge of valuing individual documents used in LLM-generated summaries. We propose using Shapley values, a game-theoretic method that allocates credit based on each document's marginal contribution. Although theoretically appealing, Shapley values are expensive to compute at scale. We therefore propose Cluster Shapley, an efficient approximation algorithm that leverages semantic similarity between documents. By clustering documents using LLM-based embeddings and computing Shapley values at the cluster level, our method significantly reduces computation while maintaining attribution quality. We demonstrate our approach to a summarization task using Amazon product reviews. Cluster Shapley significantly reduces computational complexity while maintaining high accuracy, outperforming baseline methods such as Monte Carlo sampling and Kernel SHAP with a better efficient frontier. Our approach is agnostic to the exact LLM used, the summarization process used, and the evaluation procedure, which makes it broadly applicable to a variety of summarization settings.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verbal Werewolf: Engage Users with Verbalized Agentic Werewolf Game Framework</title>
<link>https://arxiv.org/abs/2506.00160</link>
<guid>https://arxiv.org/abs/2506.00160</guid>
<content:encoded><![CDATA[
arXiv:2506.00160v2 Announce Type: replace 
Abstract: The growing popularity of social deduction games has created an increasing need for intelligent frameworks where humans can collaborate with AI agents, particularly in post-pandemic contexts with heightened psychological and social pressures. Social deduction games like Werewolf, traditionally played through verbal communication, present an ideal application for Large Language Models (LLMs) given their advanced reasoning and conversational capabilities. Prior studies have shown that LLMs can outperform humans in Werewolf games, but their reliance on external modules introduces latency that left their contribution in academic domain only, and omit such game should be user-facing. We propose \textbf{Verbal Werewolf}, a novel LLM-based Werewolf game system that optimizes two parallel pipelines: gameplay powered by state-of-the-art LLMs and a fine-tuned Text-to-Speech (TTS) module that brings text output to life. Our system operates in near real-time without external decision-making modules, leveraging the enhanced reasoning capabilities of modern LLMs like DeepSeek V3 to create a more engaging and anthropomorphic gaming experience that significantly improves user engagement compared to existing text-only frameworks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersianMedQA: Evaluating Large Language Models on a Persian-English Bilingual Medical Question Answering Benchmark</title>
<link>https://arxiv.org/abs/2506.00250</link>
<guid>https://arxiv.org/abs/2506.00250</guid>
<content:encoded><![CDATA[
arXiv:2506.00250v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved remarkable performance on a wide range of Natural Language Processing (NLP) benchmarks, often surpassing human-level accuracy. However, their reliability in high-stakes domains such as medicine, particularly in low-resource languages, remains underexplored. In this work, we introduce PersianMedQA, a large-scale dataset of 20,785 expert-validated multiple-choice Persian medical questions from 14 years of Iranian national medical exams, spanning 23 medical specialties and designed to evaluate LLMs in both Persian and English. We benchmark 40 state-of-the-art models, including general-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and chain-of-thought (CoT) settings. Our results show that closed-source general models (e.g., GPT-4.1) consistently outperform all other categories, achieving 83.09% accuracy in Persian and 80.7% in English, while Persian fine-tuned models such as Dorna underperform significantly (e.g., 34.9% in Persian), often struggling with both instruction-following and domain reasoning. We also analyze the impact of translation, showing that while English performance is generally higher, 3-10% of questions can only be answered correctly in Persian due to cultural and clinical contextual cues that are lost in translation. Finally, we demonstrate that model size alone is insufficient for robust performance without strong domain or language adaptation. PersianMedQA provides a foundation for evaluating bilingual and culturally grounded medical reasoning in LLMs. The PersianMedQA dataset is available: https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA .
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.00826</link>
<guid>https://arxiv.org/abs/2506.00826</guid>
<content:encoded><![CDATA[
arXiv:2506.00826v2 Announce Type: replace 
Abstract: Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs) by incorporating diverse modalities such as images and text. multimodal knowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals to infer missing facts, thereby mitigating the intrinsic incompleteness of MMKGs. Existing MMKGC methods typically leverage only the information contained in the MMKGs under the closed-world assumption and adopt discriminative training objectives, which limits their reasoning capacity during completion. Recent large language models (LLMs), empowered by massive parameter scales and pretraining on vast corpora, have demonstrated strong reasoning abilities across various tasks. However, their potential in MMKGC remains largely unexplored. To bridge this gap, we propose HERGC, a flexible Heterogeneous Experts Representation and Generative Completion framework for MMKGs. HERGC first deploys a Heterogeneous Experts Representation Retriever that enriches and fuses multimodal information and retrieves a compact candidate set for each incomplete triple. It then uses a Generative LLM Predictor, implemented via either in-context learning or lightweight fine-tuning, to accurately identify the correct answer from these candidates. Extensive experiments on three standard MMKG benchmarks demonstrate HERGC's effectiveness and robustness, achieving superior performance over existing methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACCESS DENIED INC: The First Benchmark Environment for Sensitivity Awareness</title>
<link>https://arxiv.org/abs/2506.00964</link>
<guid>https://arxiv.org/abs/2506.00964</guid>
<content:encoded><![CDATA[
arXiv:2506.00964v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly becoming valuable to corporate data management due to their ability to process text from various document formats and facilitate user interactions through natural language queries. However, LLMs must consider the sensitivity of information when communicating with employees, especially given access restrictions. Simple filtering based on user clearance levels can pose both performance and privacy challenges. To address this, we propose the concept of sensitivity awareness (SA), which enables LLMs to adhere to predefined access rights rules. In addition, we developed a benchmarking environment called ACCESS DENIED INC to evaluate SA. Our experimental findings reveal significant variations in model behavior, particularly in managing unauthorized data requests while effectively addressing legitimate queries. This work establishes a foundation for benchmarking sensitivity-aware language models and provides insights to enhance privacy-centric AI systems in corporate environments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-Augmented Reasoning Generation</title>
<link>https://arxiv.org/abs/2506.08364</link>
<guid>https://arxiv.org/abs/2506.08364</guid>
<content:encoded><![CDATA[
arXiv:2506.08364v3 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) systems fail at complex multi-hop reasoning because they rely on large language models to implicitly connect information from unstructured document collections. This fundamental limitation stems from treating retrieved passages as independent context rather than recognizing the intricate relationships that enable coherent reasoning chains.
  We introduce SARG (Structure-Augmented Reasoning Generation), a post-retrieval framework that transforms traditional RAG pipelines by materializing explicit reasoning structures. SARG extracts {cause, relation, effect} triples from retrieved documents, constructs domain-adaptive graphs, and performs multi-hop traversal to discover reasoning chains that bridge query concepts to answers. Unlike existing approaches that modify retrieval mechanisms, SARG operates as a plug-and-play reasoning layer compatible with any RAG system.
  Extensive evaluation across diverse domains: general QA, biomedical literature, and financial analysis demonstrates that SARG achieves substantial improvements over state-of-the-art RAG baselines. Crucially, SARG also provides full reasoning traceability through explicit inference chains, addressing the critical interpretability gap in current RAG systems.
  Our results establish that explicit structural reasoning is not merely beneficial but essential for reliable complex question answering, offering a solution to RAG's implicit reasoning bottleneck.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents</title>
<link>https://arxiv.org/abs/2506.17001</link>
<guid>https://arxiv.org/abs/2506.17001</guid>
<content:encoded><![CDATA[
arXiv:2506.17001v2 Announce Type: replace 
Abstract: Personalizing language models by effectively incorporating user interaction history remains a central challenge in the development of adaptive AI systems. While large language models (LLMs) combined with Retrieval-Augmented Generation (RAG) have improved factual accuracy, they often lack structured memory and fail to scale in complex, long-term interactions. To address this, we propose a flexible external memory framework based on knowledge graphs, automatically constructed and updated by the LLM itself, and capable of encoding information in multiple formats-including nodes, triplets, higher-order propositions, and episodic traces. Building upon the AriGraph architecture, we introduce a novel hybrid graph design that supports both standard edges and two types of hyperedges, enabling rich and dynamic semantic and temporal representations. Our framework also supports diverse retrieval mechanisms, including A*, water-circle propagation, beam search, and hybrid methods, making it adaptable to different datasets and LLM capacities. We evaluate our system on three benchmarks-TriviaQA, HotpotQA, and DiaASQ-demonstrating that different memory and retrieval configurations yield optimal performance depending on the task. Additionally, we extend the DiaASQ benchmark with temporal annotations and internally contradictory statements, showing that our system remains robust and effective in managing temporal dependencies and context-aware reasoning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation</title>
<link>https://arxiv.org/abs/2506.19952</link>
<guid>https://arxiv.org/abs/2506.19952</guid>
<content:encoded><![CDATA[
arXiv:2506.19952v2 Announce Type: replace 
Abstract: Large language models (LLMs), despite their ability to perform few-shot machine translation (MT), often lag behind dedicated MT systems trained on parallel corpora, which are crucial for high quality machine translation (MT). However, parallel corpora are often scarce or non-existent for low-resource languages. In this paper, we propose CycleDistill, a bootstrapping approach leveraging LLMs and few-shot translation to obtain high-quality MT systems. CycleDistill involves iteratively generating synthetic parallel corpora from monolingual corpora via zero- or few-shot MT, which is then used to fine-tune the model that was used for generating said data for MT. CycleDistill does not need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments focusing on three Indian languages, by relying solely on monolingual corpora, it can achieve high-quality machine translation, improving upon a few-shot baseline model by over 20-30 chrF points on average in the first iteration. We also study the effect of leveraging softmax activations during the distillation process and observe mild improvements in translation quality.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDC-R: The Minecraft Dialogue Corpus with Reference</title>
<link>https://arxiv.org/abs/2506.22062</link>
<guid>https://arxiv.org/abs/2506.22062</guid>
<content:encoded><![CDATA[
arXiv:2506.22062v2 Announce Type: replace 
Abstract: We introduce the Minecraft Dialogue Corpus with Reference (MDC-R). MDC-R is a new language resource that supplements the original Minecraft Dialogue Corpus (MDC) with expert annotations of anaphoric and deictic reference. MDC's task-orientated, multi-turn, situated dialogue in a dynamic environment has motivated multiple annotation efforts, owing to the interesting linguistic phenomena that this setting gives rise to. We believe it can serve as a valuable resource when annotated with reference, too. Here, we discuss our method of annotation and the resulting corpus, and provide both a quantitative and a qualitative analysis of the data. Furthermore, we carry out a short experiment demonstrating the usefulness of our corpus for referring expression comprehension.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.23998</link>
<guid>https://arxiv.org/abs/2506.23998</guid>
<content:encoded><![CDATA[
arXiv:2506.23998v2 Announce Type: replace 
Abstract: Congenital heart disease (CHD) presents complex, lifelong challenges often underrepresented in traditional clinical metrics. While unstructured narratives offer rich insights into patient and caregiver experiences, manual thematic analysis (TA) remains labor-intensive and unscalable. We propose a fully automated large language model (LLM) pipeline that performs end-to-end TA on clinical narratives, which eliminates the need for manual coding or full transcript review. Our system employs a novel multi-agent framework, where specialized LLM agents assume roles to enhance theme quality and alignment with human analysis. To further improve thematic relevance, we optionally integrate reinforcement learning from human feedback (RLHF). This supports scalable, patient-centered analysis of large qualitative datasets and allows LLMs to be fine-tuned for specific clinical contexts.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective</title>
<link>https://arxiv.org/abs/2506.24006</link>
<guid>https://arxiv.org/abs/2506.24006</guid>
<content:encoded><![CDATA[
arXiv:2506.24006v2 Announce Type: replace 
Abstract: The progress of Large Language Models (LLMs) like ChatGPT raises the question of how they can be integrated into education. One hope is that they can support mathematics learning, including word-problem solving. Since LLMs can handle textual input with ease, they appear well-suited for solving mathematical word problems. Yet their real competence, whether they can make sense of the real-world context, and the implications for classrooms remain unclear. We conducted a scoping review from a mathematics-education perspective, including three parts: a technical overview, a systematic review of word problems used in research, and a state-of-the-art empirical evaluation of LLMs on mathematical word problems. First, in the technical overview, we contrast the conceptualization of word problems and their solution processes between LLMs and students. In computer-science research this is typically labeled mathematical reasoning, a term that does not align with usage in mathematics education. Second, our literature review of 213 studies shows that the most popular word-problem corpora are dominated by s-problems, which do not require a consideration of realities of their real-world context. Finally, our evaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, o3, and GPT-5 on 287 word problems shows that most recent LLMs solve these s-problems with near-perfect accuracy, including a perfect score on 20 problems from PISA. LLMs still showed weaknesses in tackling problems where the real-world context is problematic or non-sensical. In sum, we argue based on all three aspects that LLMs have mastered a superficial solution process but do not make sense of word problems, which potentially limits their value as instructional tools in mathematics classrooms.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EduCoder: An Open-Source Annotation System for Education Transcript Data</title>
<link>https://arxiv.org/abs/2507.05385</link>
<guid>https://arxiv.org/abs/2507.05385</guid>
<content:encoded><![CDATA[
arXiv:2507.05385v3 Announce Type: replace 
Abstract: We introduce EduCoder, a domain-specialized tool designed to support utterance-level annotation of educational dialogue. While general-purpose text annotation tools for NLP and qualitative research abound, few address the complexities of coding education dialogue transcripts -- with diverse teacher-student and peer interactions. Common challenges include defining codebooks for complex pedagogical features, supporting both open-ended and categorical coding, and contextualizing utterances with external features, such as the lesson's purpose and the pedagogical value of the instruction. EduCoder is designed to address these challenges by providing a platform for researchers and domain experts to collaboratively define complex codebooks based on observed data. It incorporates both categorical and open-ended annotation types along with contextual materials. Additionally, it offers a side-by-side comparison of multiple annotators' responses, allowing comparison and calibration of annotations with others to improve data reliability. The system is open-source, with a demo video available.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training</title>
<link>https://arxiv.org/abs/2507.17634</link>
<guid>https://arxiv.org/abs/2507.17634</guid>
<content:encoded><![CDATA[
arXiv:2507.17634v2 Announce Type: replace 
Abstract: Recent advances in learning rate (LR) scheduling have demonstrated the effectiveness of decay-free approaches that eliminate the traditional decay phase while maintaining competitive performance. Model merging techniques have emerged as particularly promising solutions in this domain. We present Warmup-Stable and Merge (WSM), a general framework that establishes a formal connection between learning rate decay and model merging. WSM provides a unified theoretical foundation for emulating various decay strategies-including cosine decay, linear decay and inverse square root decay-as principled model averaging schemes, while remaining fully compatible with diverse optimization methods. Through extensive experiments, we identify merge duration-the training window for checkpoint aggregation-as the most critical factor influencing model performance, surpassing the importance of both checkpoint interval and merge quantity. Our framework consistently outperforms the widely-adopted Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving significant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on MMLU-Pro. The performance advantages extend to supervised fine-tuning scenarios, highlighting WSM's potential for long-term model refinement.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models</title>
<link>https://arxiv.org/abs/2507.17702</link>
<guid>https://arxiv.org/abs/2507.17702</guid>
<content:encoded><![CDATA[
arXiv:2507.17702v3 Announce Type: replace 
Abstract: Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large Language Models (LLMs) efficiently by decoupling total parameters from computational cost. However, this decoupling creates a critical challenge: predicting the model capacity of a given MoE configurations (e.g., expert activation ratio and granularity) remains an unresolved problem. To address this gap, we introduce Efficiency Leverage (EL), a metric quantifying the computational advantage of an MoE model over a dense equivalent. We conduct a large-scale empirical study, training over 300 models up to 28B parameters, to systematically investigate the relationship between MoE architectural configurations and EL. Our findings reveal that EL is primarily driven by the expert activation ratio and the total compute budget, both following predictable power laws, while expert granularity acts as a non-linear modulator with a clear optimal range. We integrate these discoveries into a unified scaling law that accurately predicts the EL of an MoE architecture based on its configuration. To validate our derived scaling laws, we designed and trained Ling-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active parameters, alongside a 6.1B dense model for comparison. When trained on an identical 1T high-quality token dataset, Ling-mini-beta matched the performance of the 6.1B dense model while consuming over 7x fewer computational resources, thereby confirming the accuracy of our scaling laws. This work provides a principled and empirically-grounded foundation for the scaling of efficient MoE models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating writing style as a contributor to gender gaps in science and technology</title>
<link>https://arxiv.org/abs/2204.13805</link>
<guid>https://arxiv.org/abs/2204.13805</guid>
<content:encoded><![CDATA[
arXiv:2204.13805v4 Announce Type: replace-cross 
Abstract: A growing stream of research finds that scientific contributions are evaluated differently depending on the gender of the author. In this article, we consider whether gender differences in writing styles - how men and women communicate their work - may contribute to these observed gender gaps. We ground our investigation in a framework for characterizing the linguistic style of written text, with two sets of features - informational (i.e., features that emphasize facts) and involved (i.e., features that emphasize relationships). Using a large sample of academic papers and patents, we find significant differences in writing style by gender, with women using more involved features in their writing. Papers and patents with more involved features also tend to be cited more by women. Our findings suggest that scientific text is not devoid of personal character, which could contribute to bias in evaluation, thereby compromising the norm of universalism as a foundational principle of science.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization</title>
<link>https://arxiv.org/abs/2311.13171</link>
<guid>https://arxiv.org/abs/2311.13171</guid>
<content:encoded><![CDATA[
arXiv:2311.13171v2 Announce Type: replace-cross 
Abstract: Parameter-efficient fine-tuning (PEFT) techniques make it possible to efficiently adapt a language model to create "expert" models that specialize to new tasks or domains. Recent techniques in model merging and compositional generalization leverage these expert models by dynamically composing modules to improve zero/few-shot generalization. Despite the efficiency of PEFT methods, the size of expert models can make it onerous to retrieve expert models per query over high-latency networks like the Internet or serve multiple experts on a single GPU. To address these issues, we present ComPEFT, a novel method for compressing fine-tuning residuals (task vectors) of PEFT based models. ComPEFT employs sparsification and ternary quantization to reduce the size of the PEFT module without performing any additional retraining while preserving or enhancing model performance. In extensive evaluation across T5, T0, and LLaMA-based models with 200M - 65B parameters, ComPEFT achieves compression ratios of 8x - 50x. In particular, we show that ComPEFT improves with scale - stronger models exhibit higher compressibility and better performance. For example, we show that ComPEFT applied to LLaMA outperforms QLoRA by 4.16% on MMLU with a storage size reduction of up to 26x. In addition, we show that the compressed experts produced by ComPEFT maintain few-shot compositional generalization capabilities, facilitate efficient communication and computation, and exhibit enhanced performance when merged. Lastly, we provide an analysis of different method components, compare it with other PEFT methods, and test ComPEFT's efficacy for compressing the residual of full-finetuning. Our code is available at https://github.com/prateeky2806/compeft.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynthVLM: Towards High-Quality and Efficient Synthesis of Image-Caption Datasets for Vision-Language Models</title>
<link>https://arxiv.org/abs/2407.20756</link>
<guid>https://arxiv.org/abs/2407.20756</guid>
<content:encoded><![CDATA[
arXiv:2407.20756v5 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) have recently emerged, demonstrating remarkable vision-understanding capabilities. However, training these models requires large-scale datasets, which brings challenges related to efficiency, effectiveness, and quality of web data. In this paper, we introduce SynthVLM, a new data synthesis and curation method for generating image-caption pairs. Unlike traditional methods, where captions are generated from images, SynthVLM utilizes advanced diffusion models and high-quality captions to synthesize and select images from text captions, thereby creating precisely aligned image-text pairs. We further introduce SynthVLM-100K, a high-quality dataset consisting of 100K curated and synthesized image-caption pairs. In both model and human evaluations, SynthVLM-100K outperforms traditional real-world datasets. Leveraging this dataset, we develop a new family of multimodal large language models (MLLMs), SynthVLM-7B and SynthVLM-13B, which achieve state-of-the-art (SOTA) performance on various vision question-answering (VQA) tasks. Notably, our models outperform LLaVA across most metrics with only 18\% pretrain data. Furthermore, SynthVLM-7B and SynthVLM-13B attain SOTA performance on the MMLU benchmark, demonstrating that the high-quality SynthVLM-100K dataset preserves language abilities.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning</title>
<link>https://arxiv.org/abs/2408.07057</link>
<guid>https://arxiv.org/abs/2408.07057</guid>
<content:encoded><![CDATA[
arXiv:2408.07057v2 Announce Type: replace-cross 
Abstract: The availability of performant pre-trained models has led to a proliferation of fine-tuned expert models that are specialized to a particular domain or task. Model MoErging methods aim to recycle expert models to create an aggregate system with improved performance or generalization. A key component of MoErging methods is the creation of a router that decides which expert model(s) to use for a particular input or application. The promise, effectiveness, and large design space of MoErging has spurred the development of many new methods over the past few years. This rapid pace of development has made it challenging to compare different MoErging methods, which are rarely compared to one another and are often validated in different experimental setups. To remedy such gaps, we present a comprehensive survey of MoErging methods that includes a novel taxonomy for cataloging key design choices and clarifying suitable applications for each method. Apart from surveying MoErging research, we inventory software tools and applications that make use of MoErging. We additionally discuss related fields of study such as model merging, multitask learning, and mixture-of-experts models. Taken as a whole, our survey provides a unified overview of existing MoErging methods and creates a solid foundation for future work in this burgeoning field.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MathScape: Benchmarking Multimodal Large Language Models in Real-World Mathematical Contexts</title>
<link>https://arxiv.org/abs/2408.07543</link>
<guid>https://arxiv.org/abs/2408.07543</guid>
<content:encoded><![CDATA[
arXiv:2408.07543v5 Announce Type: replace-cross 
Abstract: With the rapid progress of Multimodal LLMs, evaluating their mathematical reasoning capabilities has become an increasingly important research direction. In particular, visual-textual mathematical reasoning serves as a key indicator of an MLLM's ability to comprehend and solve complex, multi-step quantitative problems. While existing benchmarks such as MathVista and MathVerse have advanced the evaluation of multimodal math proficiency, they primarily rely on digitally rendered content and fall short in capturing the complexity of real-world scenarios. To bridge this gap, we introduce MathScape, a novel benchmark focused on assessing MLLMs' reasoning ability in realistic mathematical contexts. MathScape comprises 1,369 high-quality math problems paired with human-captured real-world images, closely reflecting the challenges encountered in practical educational settings. We conduct a thorough multi-dimensional evaluation across nine leading closed-source MLLMs, three open-source MLLMs with over 20 billion parameters, and seven smaller-scale MLLMs. Our results show that even SOTA models struggle with real-world math tasks, lagging behind human performance -- highlighting critical limitations in current model capabilities. Moreover, we find that strong performance on synthetic or digitally rendered images does not guarantee similar effectiveness on real-world tasks. This underscores the necessity of MathScape in the next stage of multimodal mathematical reasoning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts</title>
<link>https://arxiv.org/abs/2410.12777</link>
<guid>https://arxiv.org/abs/2410.12777</guid>
<content:encoded><![CDATA[
arXiv:2410.12777v2 Announce Type: replace-cross 
Abstract: With the rapid progress of diffusion-based content generation, significant efforts are being made to unlearn harmful or copyrighted concepts from pretrained diffusion models (DMs) to prevent potential model misuse. However, it is observed that even when DMs are properly unlearned before release, malicious finetuning can compromise this process, causing DMs to relearn the unlearned concepts. This occurs partly because certain benign concepts (e.g., "skin") retained in DMs are related to the unlearned ones (e.g., "nudity"), facilitating their relearning via finetuning. To address this, we propose meta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an unlearned DM when used as is; moreover, if the meta-unlearned DM undergoes malicious finetuning on unlearned concepts, the related benign concepts retained within it will be triggered to self-destruct, hindering the relearning of unlearned concepts. Our meta-unlearning framework is compatible with most existing unlearning methods, requiring only the addition of an easy-to-implement meta objective. We validate our approach through empirical experiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4 and SDXL), supported by extensive ablation studies. Our code is available at https://github.com/sail-sg/Meta-Unlearning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs</title>
<link>https://arxiv.org/abs/2502.01926</link>
<guid>https://arxiv.org/abs/2502.01926</guid>
<content:encoded><![CDATA[
arXiv:2502.01926v3 Announce Type: replace-cross 
Abstract: Algorithmic fairness has conventionally adopted the mathematically convenient perspective of racial color-blindness (i.e., difference unaware treatment). However, we contend that in a range of important settings, group difference awareness matters. For example, differentiating between groups may be necessary in legal contexts (e.g., the U.S. compulsory draft applies to men but not women) and harm assessments (e.g., referring to girls as ``terrorists'' may be less harmful than referring to Muslim people as such). Thus, in contrast to most fairness work, we study fairness through the perspective of treating people differently -- when it is contextually appropriate to. We first introduce an important distinction between descriptive (fact-based), normative (value-based), and correlation (association-based) benchmarks. This distinction is significant because each category requires separate interpretation and mitigation tailored to its specific characteristics. Then, we present a benchmark suite composed of eight different scenarios for a total of 16k questions that enables us to assess difference awareness. Finally, we show results across ten models that demonstrate difference awareness is a distinct dimension to fairness where existing bias mitigation strategies may backfire.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScaffoldGPT: A Scaffold-based GPT Model for Drug Optimization</title>
<link>https://arxiv.org/abs/2502.06891</link>
<guid>https://arxiv.org/abs/2502.06891</guid>
<content:encoded><![CDATA[
arXiv:2502.06891v3 Announce Type: replace-cross 
Abstract: Drug optimization has become increasingly crucial in light of fast-mutating virus strains and drug-resistant cancer cells. Nevertheless, it remains challenging as it necessitates retaining the beneficial properties of the original drug while simultaneously enhancing desired attributes beyond its scope. In this work, we aim to tackle this challenge by introducing ScaffoldGPT, a novel Generative Pretrained Transformer (GPT) designed for drug optimization based on molecular scaffolds. Our work comprises three key components: (1) A three-stage drug optimization approach that integrates pretraining, finetuning, and decoding optimization. (2) A novel two-phase incremental pre-training strategy for scaffold-based drug optimization. (3) A token-level decoding optimization strategy, Top-N, that enabling controlled, reward-guided generation using the pretrained or finetuned GPT. We demonstrate via a comprehensive evaluation on COVID and cancer benchmarks that ScaffoldGPT outperforms the competing baselines in drug optimization benchmarks, while excelling in preserving original functional scaffold and enhancing desired properties.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Duality between Gradient Transformations and Adapters</title>
<link>https://arxiv.org/abs/2502.13811</link>
<guid>https://arxiv.org/abs/2502.13811</guid>
<content:encoded><![CDATA[
arXiv:2502.13811v2 Announce Type: replace-cross 
Abstract: We study memory-efficient optimization of neural networks (in particular language models) with linear gradient transformations, where the gradients are linearly mapped to a lower dimensional space than the full parameter space, thus saving memory required for gradient accumulation and optimizer state persistence. The model parameters are updated by first performing an optimization step in the lower dimensional space and then going back into the original parameter space via the linear map's transpose. We show that optimizing the model in this transformed space is equivalent to reparameterizing the original model through a linear adapter that additively modifies the model parameters, and then only optimizing the adapter's parameters. When the transformation is Kronecker-factored, this establishes an equivalence between GaLore and one-sided LoRA. We show that this duality between gradient transformations and adapter-based reparameterizations unifies existing approaches to memory-efficient training and suggests new techniques for improving training efficiency and memory use.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collective Reasoning Among LLMs: A Framework for Answer Validation Without Ground Truth</title>
<link>https://arxiv.org/abs/2502.20758</link>
<guid>https://arxiv.org/abs/2502.20758</guid>
<content:encoded><![CDATA[
arXiv:2502.20758v3 Announce Type: replace-cross 
Abstract: We introduce a new approach in which several advanced large language models-specifically GPT-4-0125-preview, Meta-LLAMA-3-70B-Instruct, Claude-3-Opus, and Gemini-1.5-Flash-collaborate to both produce and answer intricate, doctoral-level probability problems without relying on any single "correct" reference. Rather than depending on an established ground truth, our investigation focuses on how agreement among diverse models can signal the reliability of their outputs and, by extension, reflect the overall quality of the generated questions. To measure this inter-model alignment, we apply a suite of statistical evaluations, including chi-square tests, Fleiss' Kappa coefficients, and confidence interval calculations, thereby capturing both precision in answers and clarity in question phrasing. Our analysis reveals that Claude and Gemini tend to frame questions more coherently and unambiguously, which is evidenced by their tighter confidence intervals and greater concordance with responding agents. In contrast, LLAMA exhibits wider confidence bands and a lower level of agreement, indicating more variability and reduced consistency in its question formulations. These observations support the notion that a multi-model collaborative strategy not only improves answer dependability but also offers an effective, data-driven mechanism for evaluating and refining question quality when no definitive solution exists. Ultimately, this work delivers actionable insights into enhancing AI-guided reasoning processes through coordinated interactions among heterogeneous language models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reviewing Clinical Knowledge in Medical Large Language Models: Training and Beyond</title>
<link>https://arxiv.org/abs/2502.20988</link>
<guid>https://arxiv.org/abs/2502.20988</guid>
<content:encoded><![CDATA[
arXiv:2502.20988v2 Announce Type: replace-cross 
Abstract: The large-scale development of large language models (LLMs) in medical contexts, such as diagnostic assistance and treatment recommendations, necessitates that these models possess accurate medical knowledge and deliver traceable decision-making processes. Clinical knowledge, encompassing the insights gained from research on the causes, prognosis, diagnosis, and treatment of diseases, has been extensively examined within real-world medical practices. Recently, there has been a notable increase in research efforts aimed at integrating this type of knowledge into LLMs, encompassing not only traditional text and multimodal data integration but also technologies such as knowledge graphs (KGs) and retrieval-augmented generation (RAG). In this paper, we review the various initiatives to embed clinical knowledge into training-based, KG-supported, and RAG-assisted LLMs. We begin by gathering reliable knowledge sources from the medical domain, including databases and datasets. Next, we evaluate implementations for integrating clinical knowledge through specialized datasets and collaborations with external knowledge sources such as KGs and relevant documentation. Furthermore, we discuss the applications of the developed medical LLMs in the industrial sector to assess the disparity between models developed in academic settings and those in industry. We conclude the survey by presenting evaluation systems applicable to relevant tasks and identifying potential challenges facing this field. In this review, we do not aim for completeness, since any ostensibly complete review would soon be outdated. Our goal is to illustrate diversity by selecting representative and accessible items from current research and industry practices, reflecting real-world situations rather than claiming completeness. Thus, we emphasize showcasing diverse approaches.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instructor-Worker Large Language Model System for Policy Recommendation: a Case Study on Air Quality Analysis of the January 2025 Los Angeles Wildfires</title>
<link>https://arxiv.org/abs/2503.00566</link>
<guid>https://arxiv.org/abs/2503.00566</guid>
<content:encoded><![CDATA[
arXiv:2503.00566v3 Announce Type: replace-cross 
Abstract: The Los Angeles wildfires of January 2025 caused more than 250 billion dollars in damage and lasted for nearly an entire month before containment. Following our previous work, the Digital Twin Building, we modify and leverage the multi-agent large language model framework as well as the cloud-mapping integration to study the air quality during the Los Angeles wildfires. Recent advances in large language models have allowed for out-of-the-box automated large-scale data analysis. We use a multi-agent large language system comprised of an Instructor agent and Worker agents. Upon receiving the users' instructions, the Instructor agent retrieves the data from the cloud platform and produces instruction prompts to the Worker agents. The Worker agents then analyze the data and provide summaries. The summaries are finally input back into the Instructor agent, which then provides the final data analysis. We test this system's capability for data-based policy recommendation by assessing our Instructor-Worker LLM system's health recommendations based on air quality during the Los Angeles wildfires.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Capabilities of Large Language Models on Dynamic Tasks</title>
<link>https://arxiv.org/abs/2505.10543</link>
<guid>https://arxiv.org/abs/2505.10543</guid>
<content:encoded><![CDATA[
arXiv:2505.10543v2 Announce Type: replace-cross 
Abstract: Large language models excel on static benchmarks, but their ability as self-learning agents in dynamic environments remains unclear. We evaluate three prompting strategies: self-reflection, heuristic mutation, and planning across dynamic tasks with open-source models. We find that larger models generally outperform smaller ones, but that strategic prompting can close this performance gap. Second, an overly long prompt can negatively impact smaller models on basic reactive tasks, while larger models show more robust behaviour. Third, advanced prompting techniques primarily benefit smaller models on complex games, but offer less improvement for already high-performing large language models. Yet, we find that advanced reasoning methods yield highly variable outcomes: while capable of significantly improving performance when reasoning and decision-making align, they also introduce instability and can lead to big performance drops. Compared to human performance, our findings reveal little evidence of true emergent reasoning. Instead, large language model performance exhibits persistent limitations in areas like planning and spatial coordination, suggesting that large language models still suffer fundamental shortcomings that may not be fully overcome through self-reflective prompting alone. Reasoning is a multi-faceted task, and while methods like Chain-of-thought improve multi-step reasoning on math word problems, our findings using dynamic benchmarks highlight important shortcomings in general reasoning capabilities, indicating a need to move beyond static benchmarks to capture the complexity of reasoning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason without External Rewards</title>
<link>https://arxiv.org/abs/2505.19590</link>
<guid>https://arxiv.org/abs/2505.19590</guid>
<content:encoded><![CDATA[
arXiv:2505.19590v2 Announce Type: replace-cross 
Abstract: Training large language models (LLMs) for complex reasoning via Reinforcement Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on costly, domain-specific supervision. We explore Reinforcement Learning from Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic signals without external rewards or labeled data. We propose Intuitor, an RLIF method that uses a model's own confidence, termed self-certainty, as its sole reward signal. Intuitor replaces external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, enabling fully unsupervised learning. Experiments demonstrate that Intuitor matches GRPO's performance on mathematical benchmarks while achieving superior generalization to out-of-domain tasks like code generation, without requiring gold solutions or test cases. Our findings show that intrinsic model signals can drive effective learning across domains, offering a scalable alternative to RLVR for autonomous AI systems where verifiable rewards are unavailable. Code is available at https://github.com/sunblaze-ucb/Intuitor
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents</title>
<link>https://arxiv.org/abs/2505.19997</link>
<guid>https://arxiv.org/abs/2505.19997</guid>
<content:encoded><![CDATA[
arXiv:2505.19997v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are revolutionizing education, with LLM-based agents playing a key role in simulating student behavior. A major challenge in student simulation is modeling the diverse learning patterns of students at various cognitive levels. However, current LLMs, typically trained as ``helpful assistants'', target at generating perfect responses. As a result, they struggle to simulate students with diverse cognitive abilities, as they often produce overly advanced answers, missing the natural imperfections that characterize student learning and resulting in unrealistic simulations. To address this issue, we propose a training-free framework for student simulation. We begin by constructing a cognitive prototype for each student using a knowledge graph, which captures their understanding of concepts from past learning records. This prototype is then mapped to new tasks to predict student performance. Next, we simulate student solutions based on these predictions and iteratively refine them using a beam search method to better replicate realistic mistakes. To validate our approach, we construct the \texttt{Student\_100} dataset, consisting of $100$ students working on Python programming and $5,000$ learning records. Experimental results show that our method consistently outperforms baseline models, achieving $100\%$ improvement in simulation accuracy.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Far Are We from Generating Missing Modalities with Foundation Models?</title>
<link>https://arxiv.org/abs/2506.03530</link>
<guid>https://arxiv.org/abs/2506.03530</guid>
<content:encoded><![CDATA[
arXiv:2506.03530v2 Announce Type: replace-cross 
Abstract: Multimodal foundation models have demonstrated impressive capabilities across diverse tasks. However, their potential as plug-and-play solutions for missing modality reconstruction remains underexplored. To bridge this gap, we identify and formalize three potential paradigms for missing modality reconstruction, and perform a comprehensive evaluation across these paradigms, covering 42 model variants in terms of reconstruction accuracy and adaptability to downstream tasks. Our analysis reveals that current foundation models often fall short in two critical aspects: (i) fine-grained semantic extraction from the available modalities, and (ii) robust validation of generated modalities. These limitations lead to suboptimal and, at times, misaligned generations. To address these challenges, we propose an agentic framework tailored for missing modality reconstruction. This framework dynamically formulates modality-aware mining strategies based on the input context, facilitating the extraction of richer and more discriminative semantic features. In addition, we introduce a self-refinement mechanism, which iteratively verifies and enhances the quality of generated modalities through internal feedback. Experimental results show that our method reduces FID for missing image reconstruction by at least 14\% and MER for missing text reconstruction by at least 10\% compared to baselines. Code are released at: https://github.com/Guanzhou-Ke/AFM2.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Adapter Design Tradeoffs for Low Resource Music Generation</title>
<link>https://arxiv.org/abs/2506.21298</link>
<guid>https://arxiv.org/abs/2506.21298</guid>
<content:encoded><![CDATA[
arXiv:2506.21298v2 Announce Type: replace-cross 
Abstract: Fine-tuning large-scale music generation models, such as MusicGen and Mustango, is a computationally expensive process, often requiring updates to billions of parameters and, therefore, significant hardware resources. Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based methods, have emerged as a promising alternative, enabling adaptation with minimal trainable parameters while preserving model performance. However, the design choices for adapters, including their architecture, placement, and size, are numerous, and it is unclear which of these combinations would produce optimal adapters and why, for a given case of low-resource music genre. In this paper, we attempt to answer this question by studying various adapter configurations for two AI music models, MusicGen and Mustango, on two genres: Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in capturing fine-grained local musical details such as ornamentations and short melodic phrases, while transformer-based adapters better preserve long-range dependencies crucial for structured improvisation. Additionally, we analyze computational resource requirements across different adapter scales, demonstrating how mid-sized adapters (40M parameters) achieve an optimal balance between expressivity and quality. Furthermore, we find that Mustango, a diffusion-based model, generates more diverse outputs with better adherence to the description in the input prompt while lacking in providing stability in notes, rhythm alignment, and aesthetics. Also, it is computationally intensive and requires significantly more time to train. In contrast, autoregressive models like MusicGen offer faster training and are more efficient, and can produce better quality output in comparison, but have slightly higher redundancy in their generations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles</title>
<link>https://arxiv.org/abs/2506.21839</link>
<guid>https://arxiv.org/abs/2506.21839</guid>
<content:encoded><![CDATA[
arXiv:2506.21839v2 Announce Type: replace-cross 
Abstract: We challenge text-to-image models with generating escape room puzzle images that are visually appealing, logically solid, and intellectually stimulating. While base image models struggle with spatial relationships and affordance reasoning, we propose a hierarchical multi-agent framework that decomposes this task into structured stages: functional design, symbolic scene graph reasoning, layout synthesis, and local image editing. Specialized agents collaborate through iterative feedback to ensure the scene is visually coherent and functionally solvable. Experiments show that agent collaboration improves output quality in terms of solvability, shortcut avoidance, and affordance clarity, while maintaining visual quality.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation</title>
<link>https://arxiv.org/abs/2506.21931</link>
<guid>https://arxiv.org/abs/2506.21931</guid>
<content:encoded><![CDATA[
arXiv:2506.21931v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) has shown promise in enhancing recommendation systems by incorporating external context into large language model prompts. However, existing RAG-based approaches often rely on static retrieval heuristics and fail to capture nuanced user preferences in dynamic recommendation scenarios. In this work, we introduce ARAG, an Agentic Retrieval-Augmented Generation framework for Personalized Recommendation, which integrates a multi-agent collaboration mechanism into the RAG pipeline. To better understand the long-term and session behavior of the user, ARAG leverages four specialized LLM-based agents: a User Understanding Agent that summarizes user preferences from long-term and session contexts, a Natural Language Inference (NLI) Agent that evaluates semantic alignment between candidate items retrieved by RAG and inferred intent, a context summary agent that summarizes the findings of NLI agent, and an Item Ranker Agent that generates a ranked list of recommendations based on contextual fit. We evaluate ARAG accross three datasets. Experimental results demonstrate that ARAG significantly outperforms standard RAG and recency-based baselines, achieving up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an ablation study to analyse the effect by different components of ARAG. Our findings highlight the effectiveness of integrating agentic reasoning into retrieval-augmented recommendation and provide new directions for LLM-based personalization.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Optimality for Inference-time Scaling</title>
<link>https://arxiv.org/abs/2506.22376</link>
<guid>https://arxiv.org/abs/2506.22376</guid>
<content:encoded><![CDATA[
arXiv:2506.22376v2 Announce Type: replace-cross 
Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-N selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop OptScale, a practical algorithm that dynamically determines the optimal number of sampled responses. OptScale employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that OptScale significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning. The source code is publicly available at https://github.com/Albertwyk/OptScale.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding</title>
<link>https://arxiv.org/abs/2507.22025</link>
<guid>https://arxiv.org/abs/2507.22025</guid>
<content:encoded><![CDATA[
arXiv:2507.22025v3 Announce Type: replace-cross 
Abstract: The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from a dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE for enhancing GUI agents at both training and inference. For training, we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) a continuous reward function to incentivize high-precision grounding; 2) a ``Simple Thinking'' reward to balance planning with speed and grounding accuracy; and 3) a cropping-based resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present decomposed grounding with selection to dramatically improve grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art grounding performance on two benchmarks ScreenSpot-Pro and ScreenSpot-v2 while it also exhibits strong general agent capabilities. For instance, using both our training and inference enhancement methods brings 23\% grounding accuracy improvement over the best baseline on ScreenSpot-Pro. We provide the code in https://github.com/KDEGroup/UI-AGILE.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare</title>
<link>https://arxiv.org/abs/2508.05722</link>
<guid>https://arxiv.org/abs/2508.05722</guid>
<content:encoded><![CDATA[
<div> Keywords: PEACH, English-Arabic corpus, healthcare, machine translation, readability

Summary:
PEACH is a newly introduced sentence-aligned parallel English-Arabic corpus focusing on healthcare texts like patient information leaflets and educational materials. With 51,671 parallel sentences and over 1.1 million word tokens, the corpus serves as a gold-standard resource for researchers in various fields including contrastive linguistics, translation studies, and natural language processing. It can be utilized for tasks such as deriving bilingual lexicons, adapting language models for domain-specific machine translation, evaluating machine translation in healthcare, assessing readability and lay-friendliness of healthcare materials, and as an educational tool for translation studies. The varying sentence lengths in the corpus provide a diverse dataset for analysis. PEACH is publicly available, opening up opportunities for advancements in healthcare translation and language processing research. <br /><br />Summary: <div>
arXiv:2508.05722v1 Announce Type: new 
Abstract: This paper introduces PEACH, a sentence-aligned parallel English-Arabic corpus of healthcare texts encompassing patient information leaflets and educational materials. The corpus contains 51,671 parallel sentences, totaling approximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths vary between 9.52 and 11.83 words on average. As a manually aligned corpus, PEACH is a gold-standard corpus, aiding researchers in contrastive linguistics, translation studies, and natural language processing. It can be used to derive bilingual lexicons, adapt large language models for domain-specific machine translation, evaluate user perceptions of machine translation in healthcare, assess patient information leaflets and educational materials' readability and lay-friendliness, and as an educational resource in translation studies. PEACH is publicly accessible.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation</title>
<link>https://arxiv.org/abs/2508.05775</link>
<guid>https://arxiv.org/abs/2508.05775</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, unintentional toxicity, adversarial attacks, content moderation, safety alignment

Summary: 
Large Language Models (LLMs) have transformed content creation with their advanced natural language capabilities, but they also pose risks of producing harmful content. This survey reviews studies on unintentional toxicity, adversarial attacks, and content moderation techniques related to LLMs. A unified taxonomy of LLM-related harms and defenses is proposed, including analysis of multimodal and LLM-assisted jailbreak strategies. Mitigation efforts such as reinforcement learning with human feedback, prompt engineering, and safety alignment are examined. The evolving landscape of LLM safety is highlighted, along with limitations in current evaluation methods. Future research directions are outlined to ensure the development of robust and ethically sound language technologies. 

<br /><br />Summary: <div>
arXiv:2508.05775v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized content creation across digital platforms, offering unprecedented capabilities in natural language generation and understanding. These models enable beneficial applications such as content generation, question and answering (Q&amp;A), programming, and code reasoning. Meanwhile, they also pose serious risks by inadvertently or intentionally producing toxic, offensive, or biased content. This dual role of LLMs, both as powerful tools for solving real-world problems and as potential sources of harmful language, presents a pressing sociotechnical challenge. In this survey, we systematically review recent studies spanning unintentional toxicity, adversarial jailbreaking attacks, and content moderation techniques. We propose a unified taxonomy of LLM-related harms and defenses, analyze emerging multimodal and LLM-assisted jailbreak strategies, and assess mitigation efforts, including reinforcement learning with human feedback (RLHF), prompt engineering, and safety alignment. Our synthesis highlights the evolving landscape of LLM safety, identifies limitations in current evaluation methodologies, and outlines future research directions to guide the development of robust and ethically aligned language technologies.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification</title>
<link>https://arxiv.org/abs/2508.05782</link>
<guid>https://arxiv.org/abs/2508.05782</guid>
<content:encoded><![CDATA[
<div> hallucination detection, large language models, dialogue systems, fact verification, benchmark
Summary:
The article discusses the challenges posed by hallucinations produced by Large Language Models (LLMs) in Natural Language Processing (NLP) applications, particularly dialogue systems. Current approaches to hallucination detection in dialogue systems focus on verifying the factual consistency of generated responses, but these responses often contain a mix of accurate, inaccurate, or unverifiable facts. The authors introduce a benchmark, FineDialFact, for fine-grained dialogue fact verification by verifying atomic facts extracted from dialogue responses. They construct a dataset based on publicly available dialogue datasets and evaluate it using various baseline methods. Experimental results show that methods incorporating Chain-of-Thought (CoT) reasoning can improve performance in dialogue fact verification. However, the best F1-score achieved on the HybriDialogue dataset is only 0.75, indicating that the benchmark remains a challenging task for future research. The dataset and code will be made publicly available on GitHub.
<br /><br />Summary: <div>
arXiv:2508.05782v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are known to produce hallucinations - factually incorrect or fabricated information - which poses significant challenges for many Natural Language Processing (NLP) applications, such as dialogue systems. As a result, detecting hallucinations has become a critical area of research. Current approaches to hallucination detection in dialogue systems primarily focus on verifying the factual consistency of generated responses. However, these responses often contain a mix of accurate, inaccurate or unverifiable facts, making one factual label overly simplistic and coarse-grained. In this paper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact verification, which involves verifying atomic facts extracted from dialogue responses. To support this, we construct a dataset based on publicly available dialogue datasets and evaluate it using various baseline methods. Experimental results demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning can enhance performance in dialogue fact verification. Despite this, the best F1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is only 0.75, indicating that the benchmark remains a challenging task for future research. Our dataset and code will be public on GitHub.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models</title>
<link>https://arxiv.org/abs/2508.05803</link>
<guid>https://arxiv.org/abs/2508.05803</guid>
<content:encoded><![CDATA[
<div> Keywords: human memory, language learning, neural networks, transformer models, reading times

Summary: 
- Human memory is limited in retaining exact wordforms during language processing, a concept believed to aid in language learning.
- The rise of transformer models challenges this notion as they can learn effectively without memory limitations.
- Controlled experiments on transformer models show that fleeting memory improves language learning but impairs prediction of reading times.
- The discrepancy in results cannot be explained by existing theories on the relationship between language modeling and reading time prediction.
- The study suggests that memory limitations benefit neural network language learning but do not enhance the prediction of human behavior. 

<br /><br />Summary: <div>
arXiv:2508.05803v1 Announce Type: new 
Abstract: Human memory is fleeting. As words are processed, the exact wordforms that make up incoming sentences are rapidly lost. Cognitive scientists have long believed that this limitation of memory may, paradoxically, help in learning language - an idea supported by classic connectionist modelling work. The rise of Transformers appears to challenge this idea, as these models can learn language effectively, despite lacking memory limitations or other architectural recency biases. Here, we investigate the hypothesized benefit of fleeting memory for language learning in tightly controlled experiments on transformer language models. Training transformers with and without fleeting memory on a developmentally realistic training set, we find that fleeting memory consistently improves language learning (as quantified by both overall language modelling performance and targeted syntactic evaluation) but, unexpectedly, impairs surprisal-based prediction of human reading times. Interestingly, follow up analyses revealed that this discrepancy - better language modeling, yet worse reading time prediction - could not be accounted for by prior explanations of why better language models sometimes fit human reading time worse. Together, these results support a benefit of memory limitations on neural network language learning - but not on predicting behavior.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Mirror" Language AI Models of Depression are Criterion-Contaminated</title>
<link>https://arxiv.org/abs/2508.05830</link>
<guid>https://arxiv.org/abs/2508.05830</guid>
<content:encoded><![CDATA[
<div> Mirror models, Non-Mirror models, depression assessment scores, language AI models, criterion contamination<br />
Summary:<br />
The study compares the performance of Mirror and Non-Mirror language AI models in predicting depression assessment scores. Mirror models, developed from language mirroring assessment responses, showed inflated effect sizes, potentially due to criterion contamination. Non-Mirror models, developed from unrelated language data, had smaller effect sizes but higher generalizability. Both models performed similarly in correlating predicted scores with self-reported depression symptoms. Topic modeling identified clusters in both types of models and distinctions between true-positive and false-positive predictions. Incorporating Non-Mirror models may reveal unique, interpretable semantic features for more effective real-world psychological assessment using language AI models for depression. <br /> <div>
arXiv:2508.05830v1 Announce Type: new 
Abstract: A growing number of studies show near-perfect LLM language-based prediction of depression assessment scores (up to R2 of .70). However, many develop these models directly from language responses to depression assessments. These "Mirror models" suffer from "criterion contamination", which arises when a predicted score depends in part on the predictors themselves. This causes artificial effect size inflation which reduces model generalizability. The present study compares the performance of Mirror models versus "Non-Mirror models", which are developed from language that does not mirror the assessment they are developed to predict. N = 110 research participants completed two different interviews: structured diagnostic and life history interviews. GPT-4, GPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic interview depression scores from the two transcripts separately. Mirror models (using structured diagnostic data) showed very large effect sizes (e.g., R2 = .80). As expected, NonMirror models (using life history data) demonstrated smaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror and Non-Mirror model-predicted structured interview depression scores were correlated with self-reported depression symptoms, Mirror and NonMirror performed the same (e.g., r = ~.54), indicating that Mirror models contain bias perhaps due to criterion contamination. Topic modeling identified clusters across Mirror and Non-Mirror models, as well as between true-positive and false-positive predictions. In this head-to-head comparison study, Mirror language AI models of depression showed artificially inflated effect sizes and less generalizability. As language AI models for depression continue to evolve, incorporating Non-Mirror models may identify interpretable, and generalizable semantic features that have unique utility in real-world psychological assessment.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Properties of Inflectional Morphology in Neural Emergent Communication</title>
<link>https://arxiv.org/abs/2508.05843</link>
<guid>https://arxiv.org/abs/2508.05843</guid>
<content:encoded><![CDATA[
<div> Keywords: Emergent communication, deep neural network, attribute-value reconstruction game, inflectional morphology, concatenativity

Summary: 
In this study, researchers explore emergent communication using deep neural networks to understand human language. They focus on the attribute-value reconstruction game, simulating double articulation with a small-vocabulary constraint to mimic natural language features. By introducing phonological constraints, they observe a preference for concatenative morphology in the emergent languages. This approach allows meaningful comparisons to natural language communication schemes, particularly inflectional morphology. The study highlights how simulated constraints influence the emergence of communication patterns, with a particular focus on concatenativity and fusionality. The findings suggest that emergent languages exhibit tendencies similar to natural languages in terms of fusing grammatical attributes. <div>
arXiv:2508.05843v1 Announce Type: new 
Abstract: Emergent communication (EmCom) with deep neural network-based agents promises to yield insights into the nature of human language, but remains focused primarily on a few subfield-specific goals and metrics that prioritize communication schemes which represent attributes with unique characters one-to-one and compose them syntactically. We thus reinterpret a common EmCom setting, the attribute-value reconstruction game, by imposing a small-vocabulary constraint to simulate double articulation, and formulating a novel setting analogous to naturalistic inflectional morphology (enabling meaningful comparison to natural language communication schemes). We develop new metrics and explore variations of this game motivated by real properties of inflectional morphology: concatenativity and fusionality. Through our experiments, we discover that simulated phonological constraints encourage concatenative morphology, and emergent languages replicate the tendency of natural languages to fuse grammatical attributes.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models</title>
<link>https://arxiv.org/abs/2508.05880</link>
<guid>https://arxiv.org/abs/2508.05880</guid>
<content:encoded><![CDATA[
<div> benchmark, affective computing, large language models, cognitive reasoning, emotion

Summary:
This paper explores how Large Language Models (LLMs) reason about emotions through cognitive dimensions beyond surface-level emotion tasks. They introduce a benchmark called CoRE to evaluate the internal cognitive structures used by LLMs for emotional reasoning. The study delves into whether models implicitly rely on specific cognitive appraisal dimensions, the importance of cognitive dimensions in characterizing specific emotions, and the interpretation of different emotion categories in LLMs through cognitive appraisal dimensions. Results show diverse reasoning patterns across LLMs, highlighting the complexity of emotional reasoning in artificial intelligence systems. The research contributes to the advancement of affective computing and sheds light on the cognitive processes involved in emotional reasoning by AI models.<br /><br />Summary: <div>
arXiv:2508.05880v1 Announce Type: new 
Abstract: Affective Computing has been established as a crucial field of inquiry to advance the holistic development of Artificial Intelligence (AI) systems. Foundation models -- especially Large Language Models (LLMs) -- have been evaluated, trained, or instruction-tuned in several past works, to become better predictors or generators of emotion. Most of these studies, however, approach emotion-related tasks in a supervised manner, assessing or training the capabilities of LLMs using discrete emotion labels associated with stimuli (e.g., text, images, video, audio). Evaluation studies, in particular, have often been limited to standard and superficial emotion-related tasks, such as the recognition of evoked or expressed emotions. In this paper, we move beyond surface-level emotion tasks to investigate how LLMs reason about emotions through cognitive dimensions. Drawing from cognitive appraisal theory, we examine whether LLMs produce coherent and plausible cognitive reasoning when reasoning about emotionally charged stimuli. We introduce a large-scale benchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal cognitive structures implicitly used by LLMs for emotional reasoning. Through a plethora of evaluation experiments and analysis, we seek to answer: (a) Are models more likely to implicitly rely on specific cognitive appraisal dimensions?, (b) What cognitive dimensions are important for characterizing specific emotions?, and, (c) Can the internal representations of different emotion categories in LLMs be interpreted through cognitive appraisal dimensions? Our results and analyses reveal diverse reasoning patterns across different LLMs. Our benchmark and code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.05909</link>
<guid>https://arxiv.org/abs/2508.05909</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, retrieval-augmented generation, Spectrum Projection Score, xCompress, QA benchmarks

Summary:
Spectrum Projection Score (SPS) is introduced as a metric to measure the semantic alignment of retrieved summaries with hidden representations in Large Language Models (LLMs). The xCompress framework uses SPS to dynamically sample, rank, and compress retrieval summary candidates during inference. Experimental results on five QA benchmarks with four LLMs demonstrate that SPS enhances performance across various tasks and offers insights into the interaction between retrieval and generation. The study addresses the challenge of isolating the true contribution of retrieval in improving generation performance, particularly given the prompt sensitivity of LLMs used as readers. The lightweight, supervision-free nature of SPS makes it a valuable tool for evaluating retrieval-augmented generation models. <div>
arXiv:2508.05909v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown improved generation performance through retrieval-augmented generation (RAG) following the retriever-reader paradigm, which supplements model inputs with externally retrieved knowledge. However, prior work often evaluates RAG holistically, assessing the retriever and reader jointly, making it difficult to isolate the true contribution of retrieval, particularly given the prompt sensitivity of LLMs used as readers. We introduce Spectrum Projection Score (SPS), a lightweight, supervision-free metric that allows the reader to gauge the semantic alignment of a retrieved summary with its hidden representation by comparing the area formed by generated tokens from the summary, and the principal directions of subspace in the reader and to measure the relevance. Building on SPS we present xCompress, an inference time controller framework that dynamically samples, ranks, and compresses retrieval summary candidates. Extensive experiments on five QA benchmarks with four open source LLMs show that SPS not only enhances performance across a range of tasks but also provides a principled perspective on the interaction between retrieval and generation.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale</title>
<link>https://arxiv.org/abs/2508.05938</link>
<guid>https://arxiv.org/abs/2508.05938</guid>
<content:encoded><![CDATA[
<div> Keywords: prosociality, text classification, human-AI interaction, annotation, inference

Summary: 
The article addresses the challenge of detecting prosocial content in text, focusing on affirming, supporting, or improving behaviors. Unlike toxic content detection, prosociality lacks clear definitions and labeled data, necessitating new approaches. The authors propose a three-stage pipeline for efficient and accurate prosocial content classification. They begin by identifying the best labeling strategy using a small human-labeled seed set and then implement a human-AI refinement loop to clarify and expand the task definition through annotator feedback. This iterative process enhances label quality and alignment. Subsequently, the authors synthesize 10k high-quality labels using GPT-4 and develop a two-stage inference system to handle ambiguous instances more efficiently. This architecture significantly reduces inference costs while maintaining high precision. Overall, the pipeline demonstrates how targeted human-AI interaction, meticulous task formulation, and deployment-aware architecture design can enable scalable solutions for novel responsible AI tasks.<br /><br />Summary: <div>
arXiv:2508.05938v1 Announce Type: new 
Abstract: Detecting prosociality in text--communication intended to affirm, support, or improve others' behavior--is a novel and increasingly important challenge for trust and safety systems. Unlike toxic content detection, prosociality lacks well-established definitions and labeled data, requiring new approaches to both annotation and deployment. We present a practical, three-stage pipeline that enables scalable, high-precision prosocial content classification while minimizing human labeling effort and inference costs. First, we identify the best LLM-based labeling strategy using a small seed set of human-labeled examples. We then introduce a human-AI refinement loop, where annotators review high-disagreement cases between GPT-4 and humans to iteratively clarify and expand the task definition-a critical step for emerging annotation tasks like prosociality. This process results in improved label quality and definition alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train a two-stage inference system: a lightweight classifier handles high-confidence predictions, while only $\sim$35\% of ambiguous instances are escalated to GPT-4o. This architecture reduces inference costs by $\sim$70% while achieving high precision ($\sim$0.90). Our pipeline demonstrates how targeted human-AI interaction, careful task formulation, and deployment-aware architecture design can unlock scalable solutions for novel responsible AI tasks.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring</title>
<link>https://arxiv.org/abs/2508.05987</link>
<guid>https://arxiv.org/abs/2508.05987</guid>
<content:encoded><![CDATA[
<div> Adversarial TOpic-aware Prompt-tuning, Cross-topic automated essay scoring, topic-shared features, topic-specific features, pre-trained language models (PLMs)<br />
<br />
Summary: 
The research introduces Adversarial TOpic-aware Prompt-tuning (ATOP) to enhance cross-topic automated essay scoring (AES) by considering both topic-shared and topic-specific features. ATOP optimizes a topic-aware prompt to extract relevant knowledge from pre-trained language models. Adversarial training is used to improve topic-shared prompt learning and reduce feature scale sensitivity. A neighbor-based classifier is applied to incorporate the local structure of essay representations and generate pseudo-labels for target-topic essays. These pseudo-labels guide the learning of topic-specific prompts tailored to the target topic. Extensive experiments on the ASAP++ dataset show that ATOP outperforms existing methods in holistic and multi-trait essay scoring. The implementation of ATOP is publicly available for further study and application. <br /><br />Summary: <div>
arXiv:2508.05987v1 Announce Type: new 
Abstract: Cross-topic automated essay scoring (AES) aims to develop a transferable model capable of effectively evaluating essays on a target topic. A significant challenge in this domain arises from the inherent discrepancies between topics. While existing methods predominantly focus on extracting topic-shared features through distribution alignment of source and target topics, they often neglect topic-specific features, limiting their ability to assess critical traits such as topic adherence. To address this limitation, we propose an Adversarial TOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns topic-shared and topic-specific features to improve cross-topic AES. ATOP achieves this by optimizing a learnable topic-aware prompt--comprising both shared and specific components--to elicit relevant knowledge from pre-trained language models (PLMs). To enhance the robustness of topic-shared prompt learning and mitigate feature scale sensitivity introduced by topic alignment, we incorporate adversarial training within a unified regression and classification framework. In addition, we employ a neighbor-based classifier to model the local structure of essay representations and generate pseudo-labels for target-topic essays. These pseudo-labels are then used to guide the supervised learning of topic-specific prompts tailored to the target topic. Extensive experiments on the publicly available ASAP++ dataset demonstrate that ATOP significantly outperforms existing state-of-the-art methods in both holistic and multi-trait essay scoring. The implementation of our method is publicly available at: https://anonymous.4open.science/r/ATOP-A271.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crisp Attention: Regularizing Transformers via Structured Sparsity</title>
<link>https://arxiv.org/abs/2508.06016</link>
<guid>https://arxiv.org/abs/2508.06016</guid>
<content:encoded><![CDATA[
<div> counter-example, attention sparsity, DistilBERT, sentiment analysis, implicit regularizer <br />
Summary: <br />
The paper explores the impact of introducing structured sparsity to the attention mechanism of a DistilBERT model during fine-tuning on the SST-2 sentiment analysis task. Contrary to common beliefs, the model's accuracy significantly improves with 80% attention sparsity, achieving a validation accuracy of 91.59%. The authors suggest that sparsity acts as a powerful implicit regularizer, preventing overfitting by constraining the model to make predictions with a more robust set of features. This finding challenges the notion that attention sparsity only aids computational efficiency, highlighting its potential to enhance the generalization and performance of Transformer models. <div>
arXiv:2508.06016v1 Announce Type: new 
Abstract: The quadratic computational cost of the self-attention mechanism is a primary challenge in scaling Transformer models. While attention sparsity is widely studied as a technique to improve computational efficiency, it is almost universally assumed to come at the cost of model accuracy. In this paper, we report a surprising counter-example to this common wisdom. By introducing structured, post-hoc sparsity to the attention mechanism of a DistilBERT model during fine-tuning on the SST-2 sentiment analysis task, we find that model accuracy improves significantly. Our model with 80\% attention sparsity achieves a validation accuracy of 91.59\%, a 0.97\% absolute improvement over the dense baseline. We hypothesize that this phenomenon is due to sparsity acting as a powerful implicit regularizer, preventing the model from overfitting by forcing it to make predictions with a more constrained and robust set of features. Our work recasts attention sparsity not just as a tool for computational efficiency, but as a potential method for improving the generalization and performance of Transformer models.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future</title>
<link>https://arxiv.org/abs/2508.06026</link>
<guid>https://arxiv.org/abs/2508.06026</guid>
<content:encoded><![CDATA[
<div> Keywords: Self-Rewarding Language Models, Temporal Self-Rewarding, Anchored Rejection, Future-Guided Chosen, Generative capabilities<br />
Summary: Self-Rewarding Language Models propose a method where Large Language Models evaluate their own responses, but existing paradigms face a limitation in representational difference between samples. The proposed Temporal Self-Rewarding Language Models address this by coordinating past, present, and future model generations. The framework includes Anchored Rejection to fix rejected responses using past model outputs and Future-Guided Chosen to curate selected samples using next-generation model predictions. Extensive experiments across model families and sizes show significant improvements with Temporal Self-Rewarding. For example, Llama3.1-8B performs better on AlpacaEval 2.0 compared to Self-Rewarding baseline. The method also excels in out-of-distribution generalization tasks like mathematical reasoning, knowledge-based QA, and code generation, showcasing superior performance without specific training data collection.<br /><br />Summary: <div>
arXiv:2508.06026v1 Announce Type: new 
Abstract: Self-Rewarding Language Models propose an architecture in which the Large Language Models(LLMs) both generates responses and evaluates its own outputs via LLM-as-a-Judge prompting, dynamically improving its generative capabilities through iterative Direct Preference Optimization (DPO). However, our analysis reveals a critical limitation in existing Self-Rewarding paradigms: the synchronized improvement of chosen and rejected responses progressively narrows the representational difference between contrasting samples, undermining effective preference learning. We propose \textbf{Temporal Self-Rewarding Language Models} that strategically coordinate past, present, and future model generations to sustain learning signals. Our dual-phase framework introduces: (1) \textit{Anchored Rejection} - fixing rejected responses using the past initial model's outputs and (2) \textit{Future-Guided Chosen} - dynamically curating chosen samples using next-generation model predictions. Extensive experiments across three model families (Llama, Qwen, Mistral) and different model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained with our method compared to Self-Rewarding using same computation resources. For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our method also demonstrates superior out-of-distribution generalization across mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code generation (HumanEval) tasks, even though we do not specifically collect such training data.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings</title>
<link>https://arxiv.org/abs/2508.06030</link>
<guid>https://arxiv.org/abs/2508.06030</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, PEEK, Proxy Embeddings, Knowledge estimation, Factual landscape

Summary: 
Large language models (LLMs) acquire knowledge across diverse domains encountered during generative pre-training. Existing methods to probe LLM knowledge are computationally expensive, prompting the development of PEEK, a method leveraging proxy embeddings to estimate LLM knowledge. By training embedding models on known facts and adapting them to predict LLM outputs, PEEK achieves up to 90% accuracy in predicting LLM knowledge. Sentence embeddings are found to be more suitable than graph embeddings for this task. This approach enables the identification of knowledge gaps in LLMs at scale and provides insights into their internal inductive bias. The code and data for PEEK are available at https://github.com/claws-lab/peek. 

<br /><br />Summary: <div>
arXiv:2508.06030v1 Announce Type: new 
Abstract: Large language models (LLMs) acquire knowledge across diverse domains such as science, history, and geography encountered during generative pre-training. However, due to their stochasticity, it is difficult to predict what LLMs have acquired. Prior work has developed different ways to probe this knowledge by investigating the hidden representations, crafting specific task prompts, curating representative samples, and estimating their uncertainty. However, these methods require making forward passes through the underlying model to probe the LLM's knowledge about a specific fact, making them computationally expensive and time-consuming. To bridge this gap, we propose $\textbf{PEEK}$ or $\textbf{P}$roxy $\textbf{E}$mbeddings to $\textbf{E}$stimate $\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models that effectively encode factual knowledge as text or graphs as proxies for LLMs. First, we identify a training set of facts known by LLMs through various probing strategies and then adapt embedding models to predict the LLM outputs with a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived datasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict LLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find that sentence embedding models are more suitable than graph embeddings to predict LLM knowledge, shedding light on the underlying representation of the factual landscape. Thus, we believe that knowledge-adapted embeddings can be used to identify knowledge gaps in LLMs at scale and can provide deeper insights into LLMs' internal inductive bias. The code and data are made available at https://github.com/claws-lab/peek.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation</title>
<link>https://arxiv.org/abs/2508.06046</link>
<guid>https://arxiv.org/abs/2508.06046</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Story Evaluation, Self-Evolving Pairwise Reasoning, Chain-of-Thought, Multi-Agent Strategy

Summary:
The research introduces the Self-Evolving Pairwise Reasoning (EvolvR) framework to improve the performance of Large Language Models (LLMs) in story evaluation tasks. By synthesizing score-aligned Chain-of-Thought (CoT) data through a multi-persona strategy, the framework ensures data quality through a self-filtering process using multi-agents to validate logical rigor and robustness. The trained evaluator, deployed as a reward model, achieves state-of-the-art performance on multiple evaluation benchmarks. When utilized as a reward model for story generation tasks, the framework significantly enhances the quality of generated stories. The research addresses the limitations faced by existing methods by combining prompt engineering for closed-source models and fine-tuning approaches for open-source models, resulting in an adaptable and reasoning-capable solution for story evaluation tasks. The experimental results demonstrate the effectiveness and superiority of the self-evolving approach proposed in the EvolvR framework.<br /><br />Summary: <div>
arXiv:2508.06046v1 Announce Type: new 
Abstract: Although the effectiveness of Large Language Models (LLMs) as judges (LLM-as-a-judge) has been validated, their performance remains limited in open-ended tasks, particularly in story evaluation. Accurate story evaluation is crucial not only for assisting human quality judgment but also for providing key signals to guide story generation. However, existing methods face a dilemma: prompt engineering for closed-source models suffers from poor adaptability, while fine-tuning approaches for open-source models lack the rigorous reasoning capabilities essential for story evaluation. To address this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework. Grounded in pairwise comparison, the framework first self-synthesizes score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To ensure data quality, these raw CoTs undergo a self-filtering process, utilizing multi-agents to guarantee their logical rigor and robustness. Finally, the evaluator trained on the refined data is deployed as a reward model to guide the story generation task. Experimental results demonstrate that our framework achieves state-of-the-art (SOTA) performance on three evaluation benchmarks including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward model, it significantly enhances the quality of generated stories, thereby fully validating the superiority of our self-evolving approach.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline</title>
<link>https://arxiv.org/abs/2508.06094</link>
<guid>https://arxiv.org/abs/2508.06094</guid>
<content:encoded><![CDATA[
<div> language creation, computational creativity, conlang generation, LLMs, linguistic diversity
Summary: 
ConlangCrafter is a new system that utilizes large-scale language models (LLMs) to aid in the creation of constructed languages (conlangs) without the need for human linguistic expertise. The system breaks down the language design process into distinct stages – phonology, morphology, syntax, lexicon generation, and translation – each utilizing LLMs for meta-linguistic reasoning and randomness to foster diversity. ConlangCrafter incorporates self-refinement feedback to ensure consistency in the emerging language description. Through evaluation, the system has shown its capability to produce coherent and diverse conlangs. This innovative approach bridges the gap between computational creativity and linguistic diversity, offering a new tool for artists, philosophers, and language enthusiasts interested in constructing their own languages. The integration of advanced technology and linguistic principles in ConlangCrafter marks a significant step forward in the field of conlang creation. 

<br /><br />Summary: <div>
arXiv:2508.06094v1 Announce Type: new 
Abstract: Constructed languages (conlangs) such as Esperanto and Quenya have played diverse roles in art, philosophy, and international communication. Meanwhile, large-scale foundation models have revolutionized creative generation in text, images, and beyond. In this work, we leverage modern LLMs as computational creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a multi-hop pipeline that decomposes language design into modular stages -- phonology, morphology, syntax, lexicon generation, and translation. At each stage, our method leverages LLMs' meta-linguistic reasoning capabilities, injecting randomness to encourage diversity and leveraging self-refinement feedback to encourage consistency in the emerging language description. We evaluate ConlangCrafter on metrics measuring coherence and typological diversity, demonstrating its ability to produce coherent and varied conlangs without human linguistic expertise.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs</title>
<link>https://arxiv.org/abs/2508.06103</link>
<guid>https://arxiv.org/abs/2508.06103</guid>
<content:encoded><![CDATA[
<div> Keywords: Extractive Question Answering, Quran, Large Language Models, Arabic Prompt Framework, Instruction Tuning <br />
<br />
Summary: This paper introduces two approaches for Extractive Question Answering (QA) on the Quran, addressing challenges posed by the text's complexity and unique terminology. The first approach involves using few-shot prompting with instruction-tuned large language models like Gemini and DeepSeek. A specialized Arabic prompt framework is developed for span extraction, and a robust post-processing system is implemented to enhance precision and reduce errors. Evaluations demonstrate that large language models with Arabic instructions outperform traditional fine-tuned models, achieving a pAP10 score of 0.637. The findings confirm the effectiveness of prompt-based instruction tuning for semantically rich QA tasks with limited resources. <div>
arXiv:2508.06103v1 Announce Type: new 
Abstract: This paper presents two effective approaches for Extractive Question Answering (QA) on the Quran. It addresses challenges related to complex language, unique terminology, and deep meaning in the text. The second uses few-shot prompting with instruction-tuned large language models such as Gemini and DeepSeek. A specialized Arabic prompt framework is developed for span extraction. A strong post-processing system integrates subword alignment, overlap suppression, and semantic filtering. This improves precision and reduces hallucinations. Evaluations show that large language models with Arabic instructions outperform traditional fine-tuned models. The best configuration achieves a pAP10 score of 0.637. The results confirm that prompt-based instruction tuning is effective for low-resource, semantically rich QA tasks.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures</title>
<link>https://arxiv.org/abs/2508.06105</link>
<guid>https://arxiv.org/abs/2508.06105</guid>
<content:encoded><![CDATA[
<div> LogicRAG, Retrieval-augmented generation, Large language models, Graph-based, Knowledge retrieval

Summary:
LogicRAG is a novel framework that dynamically extracts reasoning structures at inference time to guide adaptive retrieval without the need for a pre-built graph. It decomposes queries into subproblems and constructs a directed acyclic graph (DAG) to model logical dependencies among them. Through topological sorting, LogicRAG enables coherent multi-step reasoning by addressing subproblems in a consistent order. It also employs graph pruning to reduce redundancy in retrieval and context pruning to filter irrelevant information, resulting in significant token cost reduction. Experimental results demonstrate that LogicRAG outperforms existing baselines in terms of performance and efficiency. <br /><br />Summary: <div>
arXiv:2508.06105v1 Announce Type: new 
Abstract: Large language models (LLMs) often suffer from hallucination, generating factually incorrect statements when handling questions beyond their knowledge and perception. Retrieval-augmented generation (RAG) addresses this by retrieving query-relevant contexts from knowledge bases to support LLM reasoning. Recent advances leverage pre-constructed graphs to capture the relational connections among distributed documents, showing remarkable performance in complex tasks. However, existing Graph-based RAG (GraphRAG) methods rely on a costly process to transform the corpus into a graph, introducing overwhelming token cost and update latency. Moreover, real-world queries vary in type and complexity, requiring different logic structures for accurate reasoning. The pre-built graph may not align with these required structures, resulting in ineffective knowledge retrieval. To this end, we propose a \textbf{\underline{Logic}}-aware \textbf{\underline{R}}etrieval-\textbf{\underline{A}}ugmented \textbf{\underline{G}}eneration framework (\textbf{LogicRAG}) that dynamically extracts reasoning structures at inference time to guide adaptive retrieval without any pre-built graph. LogicRAG begins by decomposing the input query into a set of subproblems and constructing a directed acyclic graph (DAG) to model the logical dependencies among them. To support coherent multi-step reasoning, LogicRAG then linearizes the graph using topological sort, so that subproblems can be addressed in a logically consistent order. Besides, LogicRAG applies graph pruning to reduce redundant retrieval and uses context pruning to filter irrelevant context, significantly reducing the overall token cost. Extensive experiments demonstrate that LogicRAG achieves both superior performance and efficiency compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models</title>
<link>https://arxiv.org/abs/2508.06124</link>
<guid>https://arxiv.org/abs/2508.06124</guid>
<content:encoded><![CDATA[
<div> framework, safety risks, logical coherence, AI, alignment-sensitive<br />
Summary: AURA is a multi-layered framework designed to address the challenge of safety risks in present-day LLMs, focusing on affordance-based risks arising from overlooked logical implications. The framework incorporates Process Reward Models (PRMs) for step-level evaluations of logical coherence and safety awareness. It combines introspective self-critique, PRM assessments, and adaptive safety-aware decoding to guide models towards safer reasoning trajectories. Empirical evidence shows that AURA significantly improves logical integrity and safety awareness in model outputs, surpassing existing methods. This research contributes to the development of safer, more responsible, and contextually aware AI systems, setting a new benchmark for alignment-sensitive applications.<br /><br /> <div>
arXiv:2508.06124v1 Announce Type: new 
Abstract: Present day LLMs face the challenge of managing affordance-based safety risks-situations where outputs inadvertently facilitate harmful actions due to overlooked logical implications. Traditional safety solutions, such as scalar outcome-based reward models, parameter tuning, or heuristic decoding strategies, lack the granularity and proactive nature needed to reliably detect and intervene during subtle yet crucial reasoning steps. Addressing this fundamental gap, we introduce AURA, an innovative, multi-layered framework centered around Process Reward Models (PRMs), providing comprehensive, step level evaluations across logical coherence and safety-awareness. Our framework seamlessly combines introspective self-critique, fine-grained PRM assessments, and adaptive safety-aware decoding to dynamically and proactively guide models toward safer reasoning trajectories. Empirical evidence clearly demonstrates that this approach significantly surpasses existing methods, significantly improving the logical integrity and affordance-sensitive safety of model outputs. This research represents a pivotal step toward safer, more responsible, and contextually aware AI, setting a new benchmark for alignment-sensitive applications.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models</title>
<link>https://arxiv.org/abs/2508.06135</link>
<guid>https://arxiv.org/abs/2508.06135</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Distillation, Language Models, Data Curation, Student Models, Computational Cost<br />
<br />
Summary: This article introduces Selective Reflection Distillation (SRD), a novel data curation framework for enhancing the distillation of large language models (LLMs) into compact student models. SRD leverages reflections from student models to refine training data based on quality and compatibility with the student model. By selectively curating high-quality training instances and introducing them incrementally into the distillation process, SRD consistently improves model performance and reduces training runtime by up to 39%. The framework operates as a plug-and-play module, improving sample efficiency without modifying existing distillation algorithms. The study highlights the importance of data quality and student-model compatibility in knowledge distillation and provides practical insights for enhancing the effectiveness and efficiency of compressed LLMs.<br /><br />Summary: <div>
arXiv:2508.06135v1 Announce Type: new 
Abstract: Knowledge Distillation (KD) is a fundamental technique for compressing large language models (LLMs) into compact, efficient student models. However, existing white-box KD methods mainly focus on balancing ground truth and student-generated responses while overlooking two critical factors: training data quality and student-model compatibility. To address these limitations, we propose Selective Reflection Distillation (SRD), a novel data curation framework that leverages reflections from student models to systematically refine training data. SRD dynamically evaluates and selects prompt-response pairs by comparing ground truth data with student model outputs, selectively curating high-quality, student-compatible training instances through automated ranking based on difficulty. Furthermore, after selecting the training data, a curriculum scheduling strategy is employed to incrementally introduce these curated subsets into the distillation process at fixed intervals. As a plug-and-play enhancement, SRD consistently improves distillation outcomes across diverse white-box KD approaches and model architectures, as well as decreases computational cost significantly during KD training. Experiments on a range of language model benchmarks demonstrate SRD's consistent improvements in distilled model performance, as well as a reduction in training runtime by up to 39%, under diverse KD methods and model families. Notably, SRD operates as a plug-and-play module, enhancing sample efficiency without modifying underlying KD algorithms. Our findings highlight that data quality and compatibility are pivotal to effective and efficient distillation of LLMs, and SRD provides a principled framework to achieve both. This work advances the understanding of data-centric factors in KD and offers practical insights for enhancing the capability and efficiency of compressed LLMs.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Personality Control in LLMs with Big Five Scaler Prompts</title>
<link>https://arxiv.org/abs/2508.06149</link>
<guid>https://arxiv.org/abs/2508.06149</guid>
<content:encoded><![CDATA[
<div> Keywords: Big Five personality traits, language models, prompt-based framework, dialogue generation, human trait imitation<br />
Summary: 
Big5-Scaler introduces a prompt-based framework for controlling personality traits in large language models without additional training. The method embeds numeric trait values into prompts to enable fine-grained personality control. Evaluation across trait expression, dialogue generation, and human trait imitation tasks demonstrates consistent and distinguishable personality traits induced by Big5-Scaler. Performance varies based on prompt type and scale, emphasizing the effectiveness of concise prompts and lower trait intensities. The study highlights the efficiency of this approach in building personality-aware dialogue agents.<br /><br />Summary: <div>
arXiv:2508.06149v1 Announce Type: new 
Abstract: We present Big5-Scaler, a prompt-based framework for conditioning large language models (LLMs) with controllable Big Five personality traits. By embedding numeric trait values into natural language prompts, our method enables fine-grained personality control without additional training. We evaluate Big5-Scaler across trait expression, dialogue generation, and human trait imitation tasks. Results show that it induces consistent and distinguishable personality traits across models, with performance varying by prompt type and scale. Our analysis highlights the effectiveness of concise prompts and lower trait intensities, providing a efficient approach for building personality-aware dialogue agents.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach</title>
<link>https://arxiv.org/abs/2508.06155</link>
<guid>https://arxiv.org/abs/2508.06155</guid>
<content:encoded><![CDATA[
<div> bias detection method, implicit stereotypes, language models, social biases, interpretability

Summary: 
The paper introduces a bias detection method for large language models to identify hidden social biases. It utilizes nested semantic representation and contextual contrast to extract latent bias features from model outputs. By analyzing the model's sensitivity to social attribute terms through attention weight perturbation, the method reveals the pathways through which bias is formed. Evaluation on the StereoSet dataset shows the method's strong detection performance, accuracy in identifying bias differences, semantic alignment, and output stability. The method's structural design offers high interpretability, shedding light on internal bias mechanisms within language models. This approach provides a transparent and reliable foundation for bias detection in real-world applications where trustworthiness of generated content is crucial. 

Summary: <div>
arXiv:2508.06155v1 Announce Type: new 
Abstract: This paper addresses the issue of implicit stereotypes that may arise during the generation process of large language models. It proposes an interpretable bias detection method aimed at identifying hidden social biases in model outputs, especially those semantic tendencies that are not easily captured through explicit linguistic features. The method combines nested semantic representation with a contextual contrast mechanism. It extracts latent bias features from the vector space structure of model outputs. Using attention weight perturbation, it analyzes the model's sensitivity to specific social attribute terms, thereby revealing the semantic pathways through which bias is formed. To validate the effectiveness of the method, this study uses the StereoSet dataset, which covers multiple stereotype dimensions including gender, profession, religion, and race. The evaluation focuses on several key metrics, such as bias detection accuracy, semantic consistency, and contextual sensitivity. Experimental results show that the proposed method achieves strong detection performance across various dimensions. It can accurately identify bias differences between semantically similar texts while maintaining high semantic alignment and output stability. The method also demonstrates high interpretability in its structural design. It helps uncover the internal bias association mechanisms within language models. This provides a more transparent and reliable technical foundation for bias detection. The approach is suitable for real-world applications where high trustworthiness of generated content is required.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging</title>
<link>https://arxiv.org/abs/2508.06163</link>
<guid>https://arxiv.org/abs/2508.06163</guid>
<content:encoded><![CDATA[
<div> sparsification, model merging, multi-task learning, TADrop, parameter interference
Summary:
TADrop is introduced as an adaptive sparsification strategy for model merging in multi-task learning. It tailors the sparsity level of each parameter tensor based on its distribution, allowing for more efficient pruning of redundant parameters while retaining critical ones. By integrating TADrop with various merging methods, significant performance improvements are observed across diverse tasks and models, such as ViT and BEiT. The approach outperforms traditional sparsification techniques by achieving an average performance gain of 2.0% across 8 ViT-B/32 tasks. TADrop offers a new baseline for high-performance model merging by addressing the heterogeneity of model parameters and providing a more effective way to mitigate parameter interference. 

<br /><br />Summary: <div>
arXiv:2508.06163v1 Announce Type: new 
Abstract: Model merging has emerged as a compelling data-free paradigm for multi-task learning, enabling the fusion of multiple fine-tuned models into a single, powerful entity. A key technique in merging methods is sparsification, which prunes redundant parameters from task vectors to mitigate interference. However, prevailing approaches employ a ``one-size-fits-all'' strategy, applying a uniform sparsity ratio that overlooks the inherent structural and statistical heterogeneity of model parameters. This often leads to a suboptimal trade-off, where critical parameters are inadvertently pruned while less useful ones are retained. To address this limitation, we introduce \textbf{TADrop} (\textbf{T}ensor-wise \textbf{A}daptive \textbf{Drop}), an adaptive sparsification strategy that respects this heterogeneity. Instead of a global ratio, TADrop assigns a tailored sparsity level to each parameter tensor based on its distributional properties. The core intuition is that tensors with denser, more redundant distributions can be pruned aggressively, while sparser, more critical ones are preserved. As a simple and plug-and-play module, we validate TADrop by integrating it with foundational, classic, and SOTA merging methods. Extensive experiments across diverse tasks (vision, language, and multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and significantly boosts their performance. For instance, when enhancing a leading merging method, it achieves an average performance gain of 2.0\% across 8 ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter interference by tailoring sparsification to the model's structure, offering a new baseline for high-performance model merging.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UR$^2$: Unify RAG and Reasoning through Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.06165</link>
<guid>https://arxiv.org/abs/2508.06165</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrieval-Augmented Generation, Reinforcement Learning, Unified RAG and Reasoning, UR2 framework

Summary: 
The article introduces the UR2 framework, which aims to unify retrieval and reasoning in Large Language Models (LLMs) through reinforcement learning. It incorporates a difficulty-aware curriculum training method to selectively use retrieval for challenging problems, and a hybrid knowledge access strategy that combines offline data with LLM-generated summaries. By dynamically coordinating retrieval and reasoning, UR2 enhances adaptability across a variety of tasks, including open-domain QA, medical, and mathematical reasoning. Experiments show that UR2, built on Qwen2.5-3/7B and LLaMA-3.1-8B, outperforms existing retrieval-augmented generation and reinforcement learning methods, achieving results comparable to GPT-4o-mini and GPT-4.1-mini on several benchmarks. The code, models, and data are openly available on GitHub for further research and development.<br /><br />Summary: <div>
arXiv:2508.06165v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities through two complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR), which optimizes complex reasoning abilities. However, these two capabilities are often developed in isolation, and existing efforts to unify them remain narrow in scope-typically limited to open-domain QA with fixed retrieval settings and task-specific assumptions. This lack of integration constrains generalization and limits the applicability of RAG-RL methods to broader domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a general framework that unifies retrieval and reasoning through reinforcement learning. UR2 introduces two key contributions: a difficulty-aware curriculum training that selectively invokes retrieval only for challenging problems, and a hybrid knowledge access strategy combining domain-specific offline corpora with LLM-generated summaries. These components are designed to enable dynamic coordination between retrieval and reasoning, improving adaptability across a diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical, and mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks. We have released all code, models, and data at https://github.com/Tsinghua-dhy/UR2.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pragmatics beyond humans: meaning, communication, and LLMs</title>
<link>https://arxiv.org/abs/2508.06167</link>
<guid>https://arxiv.org/abs/2508.06167</guid>
<content:encoded><![CDATA[
<div> Keywords: pragmatics, large language models, Human-Machine Communication, Rational Speech Act framework, context frustration

Summary: The paper challenges the traditional view of pragmatics as a third dimension of meaning and instead proposes it as a dynamic interface for language in social contexts. It argues that large language models destabilize hierarchies of meaning and introduces the Human-Machine Communication framework as a more suitable alternative. Traditional human-centered pragmatic theories clash with machine-centered LLMs, prompting a shift towards probabilistic pragmatics like the Rational Speech Act framework for better compatibility. The paper also addresses substitutionalism biases in LLM evaluation and the concept of context frustration, highlighting the need for users to co-construct pragmatic conditions for both the model and themselves. These points collectively suggest the necessity for adjusting or expanding pragmatic theory to effectively encompass communication involving generative AI.<br /><br />Summary: <div>
arXiv:2508.06167v1 Announce Type: new 
Abstract: The paper reconceptualizes pragmatics not as a subordinate, third dimension of meaning, but as a dynamic interface through which language operates as a socially embedded tool for action. With the emergence of large language models (LLMs) in communicative contexts, this understanding needs to be further refined and methodologically reconsidered. The first section challenges the traditional semiotic trichotomy, arguing that connectionist LLM architectures destabilize established hierarchies of meaning, and proposes the Human-Machine Communication (HMC) framework as a more suitable alternative. The second section examines the tension between human-centred pragmatic theories and the machine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics continue to dominate, it relies on human-specific assumptions ill-suited to predictive systems like LLMs. Probabilistic pragmatics, particularly the Rational Speech Act framework, offers a more compatible teleology by focusing on optimization rather than truth-evaluation. The third section addresses the issue of substitutionalism in three forms - generalizing, linguistic, and communicative - highlighting the anthropomorphic biases that distort LLM evaluation and obscure the role of human communicative subjects. Finally, the paper introduces the concept of context frustration to describe the paradox of increased contextual input paired with a collapse in contextual understanding, emphasizing how users are compelled to co-construct pragmatic conditions both for the model and themselves. These arguments suggest that pragmatic theory may need to be adjusted or expanded to better account for communication involving generative AI.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime</title>
<link>https://arxiv.org/abs/2508.06178</link>
<guid>https://arxiv.org/abs/2508.06178</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, knowledge acquisition, catastrophic forgetting, synthetic data, RAG-based approaches

Summary: 
Large language models (LLMs) require vast amounts of text for effective knowledge acquisition. Updating LLMs with limited data is challenging due to catastrophic forgetting. In this study, injecting small, unstructured information into LLMs was investigated using recent news data. Continuing pre-training on limited data yields modest improvements, while exposing models to diverse textual variations significantly enhances learning new facts. The delicate balance between learning new content and retaining existing capabilities in small-data regimes was illustrated. RAG-based approaches for knowledge injection showed sensitivity, leading to greater degradation on control datasets compared to parametric methods. Models demonstrated the ability to generate effective synthetic training data, paving the way for self-improving model updates. The study's code and generated data are publicly available, offering a resource for studying efficient knowledge injection in LLMs with limited data. 

<br /><br />Summary: <div>
arXiv:2508.06178v1 Announce Type: new 
Abstract: Large language models (LLMs) often require vast amounts of text to effectively acquire new knowledge. While continuing pre-training on large corpora or employing retrieval-augmented generation (RAG) has proven successful, updating an LLM with only a few thousand or million tokens remains challenging. In this work, we investigate the task of injecting small, unstructured information into LLMs and its relation to the catastrophic forgetting phenomenon. We use a dataset of recent news -- ensuring no overlap with the model's pre-training data -- to evaluate the knowledge acquisition by probing the model with question-answer pairs related the learned information. Starting from a continued pre-training baseline, we explored different augmentation algorithms to generate synthetic data to improve the knowledge acquisition capabilities. Our experiments show that simply continuing pre-training on limited data yields modest improvements, whereas exposing the model to diverse textual variations significantly improves the learning of new facts -- particularly with methods that induce greater variability through diverse prompting. Furthermore, we shed light on the forgetting phenomenon in small-data regimes, illustrating the delicate balance between learning new content and retaining existing capabilities. We also confirm the sensitivity of RAG-based approaches for knowledge injection, which often lead to greater degradation on control datasets compared to parametric methods. Finally, we demonstrate that models can generate effective synthetic training data themselves, suggesting a pathway toward self-improving model updates. All code and generated data used in our experiments are publicly available, providing a resource for studying efficient knowledge injection in LLMs with limited data at https://github.com/hugoabonizio/knowledge-injection-methods.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration</title>
<link>https://arxiv.org/abs/2508.06186</link>
<guid>https://arxiv.org/abs/2508.06186</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, DKG-LLM framework, medical diagnosis, personalized treatment recommendations, Adaptive Semantic Fusion Algorithm<br />
Summary: <br />
Large Language Models (LLMs) have advanced significantly with the development of the DKG-LLM framework, which integrates a dynamic knowledge graph (DKG) with the Grok 3 model. This framework revolutionizes medical diagnosis and treatment recommendation by utilizing heterogeneous medical data, patient records, and ASFA to create a dynamic knowledge graph with over 15,000 nodes and 127,000 edges. Real-world datasets were used to evaluate the framework, achieving high diagnostic accuracy of 84.19%, treatment recommendation accuracy of 89.63%, and semantic coverage of 93.48%. DKG-LLM proves reliable for handling complex multi-symptom diseases and noisy data, incorporating feedback-based learning from physician input. This transformative tool leverages advanced probabilistic models and Bayesian inference to enhance natural language understanding and move closer to artificial general intelligence (AGI). <br /> <div>
arXiv:2508.06186v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have grown exponentially since the release of ChatGPT. These models have gained attention due to their robust performance on various tasks, including language processing tasks. These models achieve understanding and comprehension of tasks by training billions of parameters. The development of these models is a transformative force in enhancing natural language understanding and has taken a significant step towards artificial general intelligence (AGI). In this study, we aim to present the DKG-LLM framework. The DKG-LLM framework introduces a groundbreaking approach to medical diagnosis and personalized treatment recommendations by integrating a dynamic knowledge graph (DKG) with the Grok 3 large language model. Using the Adaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data (including clinical reports and PubMed articles) and patient records dynamically generate a knowledge graph consisting of 15,964 nodes in 13 distinct types (e.g., diseases, symptoms, treatments, patient profiles) and 127,392 edges in 26 relationship types (e.g., causal, therapeutic, association). ASFA utilizes advanced probabilistic models, Bayesian inference, and graph optimization to extract semantic information, dynamically updating the graph with approximately 150 new nodes and edges in each data category while maintaining scalability with up to 987,654 edges. Real-world datasets, including MIMIC-III and PubMed, were utilized to evaluate the proposed architecture. The evaluation results show that DKG-LLM achieves a diagnostic accuracy of 84.19%. The model also has a treatment recommendation accuracy of 89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and transformative tool that handles noisy data and complex multi-symptom diseases, along with feedback-based learning from physician input.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation</title>
<link>https://arxiv.org/abs/2508.06194</link>
<guid>https://arxiv.org/abs/2508.06194</guid>
<content:encoded><![CDATA[
<div> framework, evaluation, jailbreak, scenario-adaptive, dataset <br />
<br />
Summary: SceneJailEval introduces a new multi-dimensional framework for precise jailbreak evaluation, addressing the limitations of existing methods by being scenario-adaptive and extensible. It includes a comprehensive dataset with 14 scenarios to fill the gap in benchmarks. SceneJailEval achieves state-of-the-art results with an F1 score of 0.917 on the full scenario dataset and 0.995 on JBB, surpassing previous methods and confirming its advantage in evaluating diverse jailbreak variants and regional cases. <div>
arXiv:2508.06194v1 Announce Type: new 
Abstract: Precise jailbreak evaluation is vital for LLM red teaming and jailbreak research. Current approaches employ binary classification ( e.g., string matching, toxic text classifiers, LLM-driven methods), yielding only "yes/no" labels without quantifying harm intensity. Existing multi-dimensional frameworks ( e.g., Security Violation, Relative Truthfulness, Informativeness) apply uniform evaluation criteria across scenarios, resulting in scenario-specific mismatches--for instance, "Relative Truthfulness" is irrelevant to "hate speech"--which compromise evaluation precision. To tackle these limitations, we introduce SceneJailEval, with key contributions: (1) A groundbreaking scenario-adaptive multi-dimensional framework for jailbreak evaluation, overcoming the critical "one-size-fits-all" constraint of existing multi-dimensional methods, and featuring strong extensibility to flexibly adapt to customized or emerging scenarios. (2) A comprehensive 14-scenario dataset with diverse jailbreak variants and regional cases, filling the long-standing gap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3) SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on our full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over prior SOTA), surpassing accuracy limits of existing evaluation methods in heterogeneous scenarios and confirming its advantage.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations</title>
<link>https://arxiv.org/abs/2508.06196</link>
<guid>https://arxiv.org/abs/2508.06196</guid>
<content:encoded><![CDATA[
<div> Emotional Intelligence, LLMs, taxonomy, evaluation, fine-tuning<br />
Summary:<br />
This study introduces a four-layer taxonomy for Emotional Intelligence (EI) in Large Language Models (LLMs) and presents EICAP-Bench, a diverse benchmark for evaluating EI capabilities in LLMs. Six LLMs were evaluated on the benchmark, with Qwen2.5-Instruct identified as the strongest baseline. Fine-tuning on the UltraChat dataset showed significant improvement in the Appraisal layer for Qwen2.5 models, highlighting the limitations of current pretraining and tuning methods in developing comprehensive EI in LLMs. This study underscores the need for targeted strategies to enhance emotional reasoning in LLMs for better alignment with human emotions.<br /> <div>
arXiv:2508.06196v1 Announce Type: new 
Abstract: Emotional Intelligence (EI) is a critical yet underexplored dimension in the development of human-aligned LLMs. To address this gap, we introduce a unified, psychologically grounded four-layer taxonomy of EI tailored for large language models (LLMs), encompassing emotional tracking, cause inference, appraisal, and emotionally appropriate response generation. Building on this framework, we present EICAP-Bench, a novel MCQ style multi-turn benchmark designed to evaluate EI capabilities in open-source LLMs across diverse linguistic and cultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma (9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench, identifying Qwen2.5-Instruct as the strongest baseline. To assess the potential for enhancing EI capabilities, we fine-tune both Qwen2.5-Base and Qwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale, instruction-tuned dialogue dataset, in both English and Arabic. Our statistical analysis reveals that among the five EI layers, only the Appraisal layer shows significant improvement through UC-based fine-tuning. These findings highlight the limitations of existing pretraining and instruction-tuning paradigms in equipping LLMs with deeper emotional reasoning and underscore the need for targeted data and modeling strategies for comprehensive EI alignment.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classification is a RAG problem: A case study on hate speech detection</title>
<link>https://arxiv.org/abs/2508.06204</link>
<guid>https://arxiv.org/abs/2508.06204</guid>
<content:encoded><![CDATA[
<div> Keywords: Robust content moderation, Classification, Retrieval-Augmented Generation, Contextual Policy Engine, Hate speech detection

Summary: 
The study introduces a novel approach to content moderation using Retrieval-Augmented Generation (RAG) technology, which leverages contextual knowledge for classification tasks. The system, called the Contextual Policy Engine (CPE), offers robust accuracy comparable to commercial systems, explainability through retrieved policy segments, and the ability to update policies dynamically without retraining. The approach shifts the focus from determining the correct category to evaluating content in relation to policy violations. Through experiments, the system demonstrates strong baseline performance and the capability to adjust protection for specific identity groups without retraining or compromising overall accuracy. This research establishes that RAG can enhance classification processes, making them more flexible, transparent, and adaptable for a range of content moderation and classification tasks.

<br /><br />Summary: <div>
arXiv:2508.06204v1 Announce Type: new 
Abstract: Robust content moderation requires classification systems that can quickly adapt to evolving policies without costly retraining. We present classification using Retrieval-Augmented Generation (RAG), which shifts traditional classification tasks from determining the correct category in accordance with pre-trained parameters to evaluating content in relation to contextual knowledge retrieved at inference. In hate speech detection, this transforms the task from "is this hate speech?" to "does this violate the hate speech policy?"
  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates this approach and offers three key advantages: (1) robust classification accuracy comparable to leading commercial systems, (2) inherent explainability via retrieved policy segments, and (3) dynamic policy updates without model retraining. Through three experiments, we demonstrate strong baseline performance and show that the system can apply fine-grained policy control by correctly adjusting protection for specific identity groups without requiring retraining or compromising overall performance. These findings establish that RAG can transform classification into a more flexible, transparent, and adaptable process for content moderation and wider classification problems.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?</title>
<link>https://arxiv.org/abs/2508.06220</link>
<guid>https://arxiv.org/abs/2508.06220</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models, Causal Reasoning, InfoCausalQA, Multimodal AI Systems, Benchmark <br />
Summary:
- Introduction of InfoCausalQA, a benchmark for evaluating causal reasoning in Vision-Language Models (VLMs) using infographics and textual context.
- Tasks include quantitative and semantic causal reasoning involving different types of causal relations like cause, effect, intervention, counterfactual, and temporal.
- Data collection involved 494 infographic-text pairs and generation of 1,482 multiple-choice QA pairs using GPT-4o, revised for genuine visual grounding.
- Experimental results show current VLMs have limited computational and semantic causal reasoning capabilities compared to humans.
- Highlighting the gap in leveraging infographic-based information for causal inference and the need for improving causal reasoning abilities in multimodal AI systems. <br />

Summary: <br />
Recent advancements in Vision-Language Models (VLMs) have showcased impressive perception and reasoning abilities, but causal inference remains underexplored in multimodal settings. InfoCausalQA introduces a benchmark for evaluating causal reasoning in VLMs using infographics and text, addressing quantitative and semantic causal reasoning tasks. The benchmark dataset was manually collected and revised to ensure questions require genuine visual grounding. Experimental results highlight the limitations of current VLMs in computational and semantic causal reasoning, emphasizing the significance of enhancing causal reasoning abilities in multimodal AI systems. <div>
arXiv:2508.06220v1 Announce Type: new 
Abstract: Recent advances in Vision-Language Models (VLMs) have demonstrated impressive capabilities in perception and reasoning. However, the ability to perform causal inference -- a core aspect of human cognition -- remains underexplored, particularly in multimodal settings. In this study, we introduce InfoCausalQA, a novel benchmark designed to evaluate causal reasoning grounded in infographics that combine structured visual data with textual context. The benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning based on inferred numerical trends, while Task 2 targets semantic causal reasoning involving five types of causal relations: cause, effect, intervention, counterfactual, and temporal. We manually collected 494 infographic-text pairs from four public sources and used GPT-4o to generate 1,482 high-quality multiple-choice QA pairs. These questions were then carefully revised by humans to ensure they cannot be answered based on surface-level cues alone but instead require genuine visual grounding. Our experimental results reveal that current VLMs exhibit limited capability in computational reasoning and even more pronounced limitations in semantic causal reasoning. Their significantly lower performance compared to humans indicates a substantial gap in leveraging infographic-based information for causal inference. Through InfoCausalQA, we highlight the need for advancing the causal reasoning abilities of multimodal AI systems.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Data Generation for Enhanced Intent Recognition in German Speech</title>
<link>https://arxiv.org/abs/2508.06277</link>
<guid>https://arxiv.org/abs/2508.06277</guid>
<content:encoded><![CDATA[
<div> Intent recognition, speech commands, elderly German speakers, Transformer-based language models, synthetic data <br />
Summary: <br />
This paper introduces a novel approach for intent recognition (IR) from speech by elderly German speakers. The method combines an adapted Whisper ASR model with Transformer-based language models trained on synthetic datasets generated by LeoLM, Llama3, and ChatGPT. The study demonstrates the effectiveness of using synthetic data in boosting classification performance and improving robustness to different speaking styles and unseen vocabulary. Interestingly, the smaller, domain-specific LeoLM outperforms the larger ChatGPT in dataset quality for German intent recognition. The results indicate that generative AI can bridge data gaps in low-resource domains. Detailed documentation of the data generation and training process is provided to ensure transparency and reproducibility. <br /> <div>
arXiv:2508.06277v1 Announce Type: new 
Abstract: Intent recognition (IR) for speech commands is essential for artificial intelligence (AI) assistant systems; however, most existing approaches are limited to short commands and are predominantly developed for English. This paper addresses these limitations by focusing on IR from speech by elderly German speakers. We propose a novel approach that combines an adapted Whisper ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based language models trained on synthetic text datasets generated by three well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To evaluate the robustness of our approach, we generate synthetic speech with a text-to-speech model and conduct extensive cross-dataset testing. Our results show that synthetic LLM-generated data significantly boosts classification performance and robustness to different speaking styles and unseen vocabulary. Notably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the much larger ChatGPT (175B) in dataset quality for German intent recognition. Our approach demonstrates that generative AI can effectively bridge data gaps in low-resource domains. We provide detailed documentation of our data generation and training process to ensure transparency and reproducibility.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC</title>
<link>https://arxiv.org/abs/2508.06309</link>
<guid>https://arxiv.org/abs/2508.06309</guid>
<content:encoded><![CDATA[
<div> Keywords: intellectual property, large language models, plagiarism, matrix analysis, Large Deviation Theory

Summary:
Matrix-Driven Instant Review (MDIR) is a novel method proposed to detect plagiarism in large language models (LLMs). It addresses the shortcomings of existing methods by accurately reconstructing weight relationships, providing rigorous p-value estimation, and focusing on weight similarity without full model inference. MDIR has the capability to detect plagiarism even after extensive transformations such as random permutations and continual pretraining with trillions of tokens. Importantly, all detections can be performed on a single PC within an hour, ensuring efficiency and accessibility. This method aims to combat the serious misconduct of plagiarizing other LLMs without proper attribution, which can result in significant financial and reputational harm to the original developers. MDIR offers a reliable solution to the growing concerns about intellectual property in the field of large language models. 

<br /><br />Summary: <div>
arXiv:2508.06309v1 Announce Type: new 
Abstract: In recent years, concerns about intellectual property (IP) in large language models (LLMs) have grown significantly. Plagiarizing other LLMs (through direct weight copying, upcycling, pruning, or continual pretraining) and claiming authorship without properly attributing to the original license, is a serious misconduct that can lead to significant financial and reputational harm to the original developers. However, existing methods for detecting LLM plagiarism fall short in key areas. They fail to accurately reconstruct weight correspondences, lack the ability to compute statistical significance measures such as $p$-values, and may mistakenly flag models trained on similar data as being related. To address these limitations, we propose Matrix-Driven Instant Review (MDIR), a novel method that leverages matrix analysis and Large Deviation Theory. MDIR achieves accurate reconstruction of weight relationships, provides rigorous $p$-value estimation, and focuses exclusively on weight similarity without requiring full model inference. Experimental results demonstrate that MDIR reliably detects plagiarism even after extensive transformations, such as random permutations and continual pretraining with trillions of tokens. Moreover, all detections can be performed on a single PC within an hour, making MDIR both efficient and accessible.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering</title>
<link>https://arxiv.org/abs/2508.06345</link>
<guid>https://arxiv.org/abs/2508.06345</guid>
<content:encoded><![CDATA[
<div> Graph Response Efficiency, Multimodal Models, Zero-shot capabilities, DynamicTRF framework, Graph QA
Summary:
Large Multimodal Models (LMMs) have demonstrated the ability to perform diverse domain question-answering tasks, including graph QA, but existing approaches using single Topology Representation Forms (TRFs) may lead to incorrect or lengthy responses. To address this, researchers have proposed a set of TRFs tailored for zero-shot graph QA, introduced the Graph Response Efficiency metric to balance performance and brevity, and developed the DynamicTRF framework. This framework includes creating a TRF Preference dataset to identify TRF preferences, training a TRF router to adaptively select the best TRF for each question during inference. Experimental results on algorithmic graph QA tasks show that DynamicTRF significantly improves the accuracy of zero-shot graph QA for LMMs. <div>
arXiv:2508.06345v1 Announce Type: new 
Abstract: Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities in diverse domain question-answering (QA) tasks, including graph QA that involves complex graph topologies. However, most current approaches use only a single type of graph representation, namely Topology Representation Form (TRF), such as prompt-unified text descriptions or style-fixed visual styles. Those "one-size-fits-all" approaches fail to consider the specific preferences of different models or tasks, often leading to incorrect or overly long responses. To address this, we first analyze the characteristics and weaknesses of existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency (GRE), which measures the balance between the performance and the brevity in graph QA. Built on these, we develop the DynamicTRF framework, which aims to improve both the accuracy and conciseness of graph QA. To be specific, DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based on their GRE scores, to probe the question-specific TRF preferences. Then it trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from $F_{ZS}$ for each question during the inference. Extensive experiments across 7 in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms of accuracy
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cyberbullying Detection via Aggression-Enhanced Prompting</title>
<link>https://arxiv.org/abs/2508.06360</link>
<guid>https://arxiv.org/abs/2508.06360</guid>
<content:encoded><![CDATA[
<div> aggression detection, cyberbullying detection, social media, large language models, multi-task learning

Summary: 
This study explores the effectiveness of integrating aggression detection as an auxiliary task within a unified training framework for improving cyberbullying detection on social media. Experiments using instruction-tuned large language models (LLMs) on multiple datasets demonstrate the potential of this approach. Different strategies including zero-shot, few-shot, LoRA fine-tuning, and multi-task learning were evaluated, with the enriched prompt pipeline approach showing consistent outperformance of standard fine-tuning. The incorporation of aggression predictions into cyberbullying detection prompts provides contextual augmentation, leading to enhanced detection performance. This highlights the importance of auxiliary tasks such as aggression detection in improving the generalization of LLMs for safety-critical applications on social networks. 

<br /><br />Summary: <div>
arXiv:2508.06360v1 Announce Type: new 
Abstract: Detecting cyberbullying on social media remains a critical challenge due to its subtle and varied expressions. This study investigates whether integrating aggression detection as an auxiliary task within a unified training framework can enhance the generalisation and performance of large language models (LLMs) in cyberbullying detection. Experiments are conducted on five aggression datasets and one cyberbullying dataset using instruction-tuned LLMs. We evaluated multiple strategies: zero-shot, few-shot, independent LoRA fine-tuning, and multi-task learning (MTL). Given the inconsistent results of MTL, we propose an enriched prompt pipeline approach in which aggression predictions are embedded into cyberbullying detection prompts to provide contextual augmentation. Preliminary results show that the enriched prompt pipeline consistently outperforms standard LoRA fine-tuning, indicating that aggression-informed context significantly boosts cyberbullying detection. This study highlights the potential of auxiliary tasks, such as aggression detection, to improve the generalisation of LLMs for safety-critical applications on social networks.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Style-Personalized Text Generation: Challenges and Directions</title>
<link>https://arxiv.org/abs/2508.06374</link>
<guid>https://arxiv.org/abs/2508.06374</guid>
<content:encoded><![CDATA[
<div> Keywords: style personalized text generation, evaluation metrics, style embeddings, LLM-as-judge, ensemble evaluation

Summary:
This article explores the evaluation of low-resource author style personalized text generation, questioning the effectiveness of traditional metrics like BLEU and ROUGE. The study introduces alternative evaluation paradigms including style embeddings and using Language Models as judges to assess text generation. The authors present a style discrimination benchmark across various writing tasks such as domain discrimination, authorship attribution, and personalized vs non-personalized text discrimination. They conclude that a diverse ensemble of evaluation metrics is crucial for effectively evaluating style personalized text generation. The research advocates for a more holistic approach to evaluating text generation models, aiming to enhance the quality and accuracy of style personalized text outputs. 

<br /><br />Summary: <div>
arXiv:2508.06374v1 Announce Type: new 
Abstract: While prior research has built tools and benchmarks towards style personalized text generation, there has been limited exploration of evaluation in low-resource author style personalized text generation space. Through this work, we question the effectiveness of the widely adopted evaluation metrics like BLEU and ROUGE, and explore other evaluation paradigms such as style embeddings and LLM-as-judge to holistically evaluate the style personalized text generation task. We evaluate these metrics and their ensembles using our style discrimination benchmark, that spans eight writing tasks, and evaluates across three settings, domain discrimination, authorship attribution, and LLM personalized vs non-personalized discrimination. We provide conclusive evidence to adopt ensemble of diverse evaluation metrics to effectively evaluate style personalized text generation.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing</title>
<link>https://arxiv.org/abs/2508.06388</link>
<guid>https://arxiv.org/abs/2508.06388</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Emotionally Supportive Role-Playing, Anime characters, Dataset, Evaluation system

Summary:
Large Language Models (LLMs) have shown promising abilities in providing emotional support and role-playing conversations separately. This study aims to bridge the gap by focusing on emotionally supportive interactions with anime characters. The ChatAnime dataset is introduced, featuring 20 anime characters and 60 emotion-centric scenario questions. Dialogue data from LLMs and anime enthusiasts were collected and evaluated using user-centric metrics. Results indicate that LLMs excel in role-playing and emotional support compared to human fans, but humans outperform them in response diversity. The dataset, containing human-written and LLM-generated answers with over 132,000 annotations, aims to provide valuable resources for future research. The dataset is available at https://github.com/LanlanQiu/ChatAnime.

<br /><br />Summary: Large Language Models (LLMs) have shown impressive capabilities in providing emotional support and role-playing conversations. This study introduces the ChatAnime dataset, focusing on emotionally supportive interactions with anime characters. Data was collected from LLMs and anime enthusiasts, with results showing LLMs performing well in role-playing and emotional support, although humans excel in response diversity. The dataset aims to support future research in this area. <div>
arXiv:2508.06388v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in role-playing conversations and providing emotional support as separate research directions. However, there remains a significant research gap in combining these capabilities to enable emotionally supportive interactions with virtual characters. To address this research gap, we focus on anime characters as a case study because of their well-defined personalities and large fan bases. This choice enables us to effectively evaluate how well LLMs can provide emotional support while maintaining specific character traits. We introduce ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We first thoughtfully select 20 top-tier characters from popular anime communities and design 60 emotion-centric real-world scenario questions. Then, we execute a nationwide selection process to identify 40 Chinese anime enthusiasts with profound knowledge of specific characters and extensive experience in role-playing. Next, we systematically collect two rounds of dialogue data from 10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP performance of LLMs, we design a user experience-oriented evaluation system featuring 9 fine-grained metrics across three dimensions: basic dialogue, role-playing and emotional support, along with an overall metric for response diversity. In total, the dataset comprises 2,400 human-written and 24,000 LLM-generated answers, supported by over 132,000 human annotations. Experimental results show that top-performing LLMs surpass human fans in role-playing and emotional support, while humans still lead in response diversity. We hope this work can provide valuable resources and insights for future research on optimizing LLMs in ESRP. Our datasets are available at https://github.com/LanlanQiu/ChatAnime.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Conversation Drift in MCP via Latent Polytope</title>
<link>https://arxiv.org/abs/2508.06418</link>
<guid>https://arxiv.org/abs/2508.06418</guid>
<content:encoded><![CDATA[
<div> Keywords: Model Context Protocol, large language models, security risks, conversation hijacking, data exfiltration

Summary:<br /><br />The article introduces the Model Context Protocol (MCP) for enhancing large language models (LLMs) with external tools but highlights security and privacy risks. Adversarial content can lead to conversation hijacking, misinformation, or data theft. Existing defenses are inadequate, prompting the proposal of SecMCP, a secure framework detecting conversation drift in LLMs' latent space. By modeling LLM activation vectors in a latent polytope space, SecMCP identifies anomalous conversational dynamics shifts, enabling proactive detection of hijacking and data exfiltration. The framework is evaluated on three LLMs and benchmark datasets, showing robust detection with AUROC scores exceeding 0.915 while maintaining usability. Contributions include systematic categorization of MCP security threats, a latent polytope-based method for quantifying conversation drift, and empirical validation of SecMCP's effectiveness. <div>
arXiv:2508.06418v1 Announce Type: new 
Abstract: The Model Context Protocol (MCP) enhances large language models (LLMs) by integrating external tools, enabling dynamic aggregation of real-time data to improve task execution. However, its non-isolated execution context introduces critical security and privacy risks. In particular, adversarially crafted content can induce tool poisoning or indirect prompt injection, leading to conversation hijacking, misinformation propagation, or data exfiltration. Existing defenses, such as rule-based filters or LLM-driven detection, remain inadequate due to their reliance on static signatures, computational inefficiency, and inability to quantify conversational hijacking. To address these limitations, we propose SecMCP, a secure framework that detects and quantifies conversation drift, deviations in latent space trajectories induced by adversarial external knowledge. By modeling LLM activation vectors within a latent polytope space, SecMCP identifies anomalous shifts in conversational dynamics, enabling proactive detection of hijacking, misleading, and data exfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3, Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA), demonstrating robust detection with AUROC scores exceeding 0.915 while maintaining system usability. Our contributions include a systematic categorization of MCP security threats, a novel latent polytope-based methodology for quantifying conversation drift, and empirical validation of SecMCP's efficacy.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memp: Exploring Agent Procedural Memory</title>
<link>https://arxiv.org/abs/2508.06433</link>
<guid>https://arxiv.org/abs/2508.06433</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, procedural memory, lifelong learning, dynamic memory update, agent performance

Summary:
In this work, the authors explore strategies to enhance agents' performance by introducing a learnable and updatable procedural memory system called Memp. This memory repository distills past agent trajectories into detailed instructions and higher-level abstractions, continuously evolving with new experiences through dynamic updates. The study evaluates the impact of different strategies for building, retrieving, and updating procedural memory on tasks like TravelPlanner and ALFWorld. Results show that as the memory repository is refined, agents achieve higher success rates and efficiency. Additionally, transferring the procedural memory from a stronger model to a weaker one leads to significant performance gains. This approach addresses the issue of brittle procedural memory in Large Language Models and highlights the importance of dynamic memory update mechanisms for enhancing agent performance in diverse tasks.<br /><br />Summary: <div>
arXiv:2508.06433v1 Announce Type: new 
Abstract: Large Language Models (LLMs) based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters. In this work, we investigate strategies to endow agents with a learnable, updatable, and lifelong procedural memory. We propose Memp that distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory. Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages</title>
<link>https://arxiv.org/abs/2508.06435</link>
<guid>https://arxiv.org/abs/2508.06435</guid>
<content:encoded><![CDATA[
<div> Fine-tuning lightweight LLaMA 3.2-3B models on immigration-related tweets across 13 languages. Multilingual topic detection, pre-training bias correction with minimal fine-tuning. 4-bit-quantised LoRA fine-tuned models released for open-source research. Scalable, inclusive research with 35 times faster inference at low cost.<br /><br />Summary: Large language models are used for analyzing immigration-related tweets in multiple languages. Fine-tuning in one or two languages allows reliable content classification in unseen languages. Identifying pro- or anti-immigration stances benefits from multilingual fine-tuning. Pre-training bias towards dominant languages can be corrected with minimal exposure to under-represented languages during fine-tuning. The release of 4-bit-quantised LoRA fine-tuned models provides an open-source and reproducible alternative to proprietary models, enabling faster and more cost-effective research. <div>
arXiv:2508.06435v1 Announce Type: new 
Abstract: Large language models (LLMs) are transforming social-science research by enabling scalable, precise analysis. Their adaptability raises the question of whether knowledge acquired through fine-tuning in a few languages can transfer to unseen languages that only appeared during pre-training. To examine this, we fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or multilingual data sets to classify immigration-related tweets from X/Twitter across 13 languages, a domain characterised by polarised, culturally specific discourse. We evaluate whether minimal language-specific fine-tuning enables cross-lingual topic detection and whether adding targeted languages corrects pre-training biases. Results show that LLMs fine-tuned in one or two languages can reliably classify immigration-related content in unseen languages. However, identifying whether a tweet expresses a pro- or anti-immigration stance benefits from multilingual fine-tuning. Pre-training bias favours dominant languages, but even minimal exposure to under-represented languages during fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training token volume) yields significant gains. These findings challenge the assumption that cross-lingual mastery requires extensive multilingual training: limited language coverage suffices for topic-level generalisation, and structural biases can be corrected with lightweight interventions. By releasing 4-bit-quantised, LoRA fine-tuned models, we provide an open-source, reproducible alternative to proprietary LLMs that delivers 35 times faster inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model, enabling scalable, inclusive research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echoes of Automation: The Increasing Use of LLMs in Newsmaking</title>
<link>https://arxiv.org/abs/2508.06445</link>
<guid>https://arxiv.org/abs/2508.06445</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, journalistic integrity, authorship, news media, linguistic analysis

Summary: <br /><br /> This study examines the use of Generative AI in news media and its impact on journalistic integrity and authorship. The researchers analyzed over 40,000 news articles from major, local, and college sources using advanced AI-text detectors. They found a significant increase in the use of Generative AI, particularly in local and college news outlets. The study revealed that LLMs are frequently used in the introduction of news articles, while conclusions are typically manually written. Linguistic analysis showed that GenAI enhances word richness and readability but reduces formality, resulting in more uniform writing styles, particularly in local media. These findings raise concerns about the potential implications of AI-generated content on journalistic standards and the role of human authors in news production. <div>
arXiv:2508.06445v1 Announce Type: new 
Abstract: The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns for journalistic integrity and authorship. This study examines AI-generated content across over 40,000 news articles from major, local, and college news media, in various media formats. Using three advanced AI-text detectors (e.g., Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of GenAI use in recent years, especially in local and college news. Sentence-level analysis reveals LLMs are often used in the introduction of news, while conclusions usually written manually. Linguistic analysis shows GenAI boosts word richness and readability but lowers formality, leading to more uniform writing styles, particularly in local media.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning</title>
<link>https://arxiv.org/abs/2508.06447</link>
<guid>https://arxiv.org/abs/2508.06447</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-context Inference, Language Models, SlimInfer, Token Pruning, Information Diffusion<br />
Summary: <br />
- The research focuses on enhancing efficiency in large language models by introducing SlimInfer, a framework that prunes less critical prompt tokens during inference. <br />
- SlimInfer leverages the information diffusion phenomenon to maintain semantic integrity while removing redundant tokens from hidden states at intermediate layers. <br />
- The framework implements dynamic fine-grained pruning and an asynchronous KV cache manager to prefetch token blocks, reducing memory usage and I/O costs. <br />
- Extensive experiments demonstrate that SlimInfer can achieve significant speedup and latency reduction for LLaMA3.1-8B-Instruct without compromising performance on LongBench. <br />
- The code for SlimInfer will be made available upon acceptance for further exploration and application in the field. <br /> <div>
arXiv:2508.06447v1 Announce Type: new 
Abstract: Long-context inference for Large Language Models (LLMs) is heavily limited by high computational demands. While several existing methods optimize attention computation, they still process the full set of hidden states at each layer, limiting overall efficiency. In this work, we propose SlimInfer, an innovative framework that aims to accelerate inference by directly pruning less critical prompt tokens during the forward pass. Our key insight is an information diffusion phenomenon: As information from critical tokens propagates through layers, it becomes distributed across the entire sequence. This diffusion process suggests that LLMs can maintain their semantic integrity when excessive tokens, even including these critical ones, are pruned in hidden states. Motivated by this, SlimInfer introduces a dynamic fine-grained pruning mechanism that accurately removes redundant tokens of hidden state at intermediate layers. This layer-wise pruning naturally enables an asynchronous KV cache manager that prefetches required token blocks without complex predictors, reducing both memory usage and I/O costs. Extensive experiments show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token (TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench. Our code will be released upon acceptance.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</title>
<link>https://arxiv.org/abs/2508.06471</link>
<guid>https://arxiv.org/abs/2508.06471</guid>
<content:encoded><![CDATA[
<div> MoE, large language model, hybrid reasoning method, performance benchmarks, open-source<br />
Summary: 
GLM-4.5 is an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and strong performance across agentic, reasoning, and coding tasks. It features a hybrid reasoning method supporting both thinking and direct response modes, achieved through multi-stage training on a large dataset and comprehensive post-training. The model scores 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified, ranking 3rd overall among evaluated models. Despite having fewer parameters than competitors, GLM-4.5 excels on agentic benchmarks, coming in 2nd place. The researchers have released both the full GLM-4.5 model and a compact version, GLM-4.5-Air, to further research in reasoning and agentic AI systems. More information, code, and models can be accessed on the GitHub page provided. <br /><br />Summary: <div>
arXiv:2508.06471v1 Announce Type: new 
Abstract: We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning</title>
<link>https://arxiv.org/abs/2508.06475</link>
<guid>https://arxiv.org/abs/2508.06475</guid>
<content:encoded><![CDATA[
<div> captioning, haptic signals, language model, vibration, multimodal  
Summary:  
Haptic captioning involves generating natural language descriptions from haptic signals like vibrations for use in various applications. The proposed HapticLLaMA model interprets vibration signals into descriptions in specific categories, trained through supervised fine-tuning and reinforcement learning from human feedback (RLHF). The model achieved high scores in automated n-gram metrics and human evaluation, demonstrating its strong capability in interpreting haptic signals. Over 61% of generated captions received positive human ratings, with RLHF showing a 10% improvement in overall rating distribution. These results highlight the potential of language models in processing and adapting to sensory data.  
<br /><br />Summary: <div>
arXiv:2508.06475v1 Announce Type: new 
Abstract: Haptic captioning is the task of generating natural language descriptions from haptic signals, such as vibrations, for use in virtual reality, accessibility, and rehabilitation applications. While previous multimodal research has focused primarily on vision and audio, haptic signals for the sense of touch remain underexplored. To address this gap, we formalize the haptic captioning task and propose HapticLLaMA, a multimodal sensory language model that interprets vibration signals into descriptions in a given sensory, emotional, or associative category. We investigate two types of haptic tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that convert haptic signals into sequences of discrete units, enabling their integration with the LLaMA model. HapticLLaMA is trained in two stages: (1) supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation, and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We assess HapticLLaMA's captioning performance using both automated n-gram metrics and human evaluation. HapticLLaMA demonstrates strong capability in interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated captions received human ratings above 3.5 on a 7-point scale, with RLHF yielding a 10% improvement in the overall rating distribution, indicating stronger alignment with human haptic perception. These findings highlight the potential of large language models to process and adapt to sensory data.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-training for Efficient Communication via Convention Formation</title>
<link>https://arxiv.org/abs/2508.06482</link>
<guid>https://arxiv.org/abs/2508.06482</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, convention formation, fine-tuning, benchmark, document-grounded reference completion

Summary: 
LLMs, or large language models, typically do not naturally demonstrate efficient communication in multi-turn interactions. In this study, researchers introduce a post-training process that involves targeted fine-tuning on demonstrations of convention formation to improve this ability. Two new benchmarks were used to evaluate the effectiveness of the post-training process. The first benchmark, designed to elicit convention formation trends in humans, demonstrated consistent improvement in LLMs. The second benchmark focused on document-grounded reference completion, further showcasing enhanced convention formation abilities in post-trained LLMs. These findings suggest that targeted fine-tuning on heuristically identified demonstrations can significantly improve LLMs' convention formation capabilities. 

<br /><br />Summary: <div>
arXiv:2508.06482v1 Announce Type: new 
Abstract: Humans communicate with increasing efficiency in multi-turn interactions, by adapting their language and forming ad-hoc conventions. In contrast, prior work shows that LLMs do not naturally show this behavior. We develop a post-training process to develop this ability through targeted fine-tuning on heuristically identified demonstrations of convention formation. We evaluate with two new benchmarks focused on this capability. First, we design a focused, cognitively-motivated interaction benchmark that consistently elicits strong convention formation trends in humans. Second, we create a new document-grounded reference completion task that reflects in-the-wild convention formation behavior. Our studies show significantly improved convention formation abilities in post-trained LLMs across the two evaluation methods.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models</title>
<link>https://arxiv.org/abs/2508.04748</link>
<guid>https://arxiv.org/abs/2508.04748</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Molecular Property Prediction, Attribute-guided Reinforcement Learning, Relevant Attributes, Performance Boost<br />
Summary: AttriLens-Mol is a framework for molecular property prediction with Large Language Models (LLMs) that uses attribute-guided reinforcement learning to steer model reasoning. It encourages structured output based on attributes, avoids irrelevant attributes, and verifies relatedness using advanced LLMs and RDKit. By eliciting relevant molecular attributes during reasoning, it enhances prediction effectiveness. Experiments show that training LLMs with AttriLens-Mol significantly boosts performance, outperforming supervised fine-tuning and advanced models. Attributes extracted by AttriLens-Mol lead to superior performance in decision tree models compared to those generated by prompting LLMs, improving interpretability and prediction accuracy for property prediction tasks. The code is available at https://github.com/szu-tera/AttriLens-Mol.<br /><br />Summary: AttriLens-Mol framework uses attribute-guided reinforcement learning to steer Large Language Models' reasoning for molecular property prediction, enhancing prediction effectiveness and interpretability. Experiments demonstrate superior performance compared to supervised fine-tuning and advanced models, showcasing the framework's efficiency in eliciting relevant attributes and improving prediction accuracy.  <div>
arXiv:2508.04748v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown promise in assisting molecular property prediction tasks but often rely on human-crafted prompts and chain-of-thought templates. While recent advanced large reasoning models like DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process, their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol, an attribute-guided reinforcement learning framework for molecular property prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1) a format reward encouraging attribute-based structured output, (2) a count reward to avoid enumerating irrelevant attributes, and (3) a rationality reward using advanced LLMs and RDKit to verify the relatedness of the generated attributes. This approach implicitly elicits the model's inherent knowledge of relevant molecular attributes during reasoning, enables making predictions for the molecular property more effectively. Experiments on both in-distribution and out-of-distribution datasets show that, training both 7B-size R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our proposed AttriLens-Mol method significantly boosts the performance, getting comparable or better results than supervised fine-tuning models (Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o, DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the target property, when used as features for an interpretable decision tree model, yield superior performance compared to attributes generated by prompting LLMs. This shows that AttriLens-Mol effectively elicits more relevant and predictive molecular attributes, leading to enhanced interpretability and performance for property prediction. We release the code in https://github.com/szu-tera/AttriLens-Mol.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support</title>
<link>https://arxiv.org/abs/2508.05664</link>
<guid>https://arxiv.org/abs/2508.05664</guid>
<content:encoded><![CDATA[
<div> query rewriting, RAG Fusion, keyword augmentation, intent recognition, context reranking <br />
<br />
Summary: 
This case study evaluates various techniques for enhancing the performance of AI customer service systems in the electric power domain. The study compares vector-store and graph-based RAG frameworks, with the graph-based RAG being selected for its superior handling of complex queries. Query rewriting proves beneficial for non-standard or detail-specific queries, while RAG Fusion excels in resolving vague or multi-intent queries. Context reranking helps filter out irrelevant information, while intent recognition aids in breaking down complex questions into more manageable sub-queries. However, keyword augmentation is found to have a negative impact due to biased keyword selection. By combining intent recognition, RAG Fusion, and reranking techniques, the final system achieves high accuracy rates of 97.9% and 89.6% on GPT-4-generated and real-world electricity provider FAQ datasets, respectively, surpassing baseline RAG models. <br /> <div>
arXiv:2508.05664v1 Announce Type: cross 
Abstract: Many AI customer service systems use standard NLP pipelines or finetuned language models, which often fall short on ambiguous, multi-intent, or detail-specific queries. This case study evaluates recent techniques: query rewriting, RAG Fusion, keyword augmentation, intent recognition, and context reranking, for building a robust customer support system in the electric power domain. We compare vector-store and graph-based RAG frameworks, ultimately selecting the graph-based RAG for its superior performance in handling complex queries. We find that query rewriting improves retrieval for queries using non-standard terminology or requiring precise detail. RAG Fusion boosts performance on vague or multifaceted queries by merging multiple retrievals. Reranking reduces hallucinations by filtering irrelevant contexts. Intent recognition supports the decomposition of complex questions into more targeted sub-queries, increasing both relevance and efficiency. In contrast, keyword augmentation negatively impacts results due to biased keyword selection. Our final system combines intent recognition, RAG Fusion, and reranking to handle disambiguation and multi-source queries. Evaluated on both a GPT-4-generated dataset and a real-world electricity provider FAQ dataset, it achieves 97.9% and 89.6% accuracy respectively, substantially outperforming baseline RAG models.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges</title>
<link>https://arxiv.org/abs/2508.05668</link>
<guid>https://arxiv.org/abs/2508.05668</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Search Agents, Information Retrieval, Natural Language Processing, Deep Learning<br />
Summary:<br />
The article discusses the impact of Large Language Models (LLMs) on web search, specifically focusing on the emergence of LLM-based Search Agents and their ability to understand user intentions and context. These agents are capable of executing multi-turn retrieval with dynamic planning, expanding search capabilities beyond traditional web search. The survey analyzes existing works in this field, categorizing them based on architecture, optimization, application, and evaluation. Open challenges in the development of search agents are identified, and future research directions are outlined. The article highlights the potential of search agents like OpenAI's Deep Research for deep information mining and practical applications. Researchers can access a repository of search agent papers on GitHub to further explore this rapidly evolving area. <br />Summary: <div>
arXiv:2508.05668v1 Announce Type: cross 
Abstract: The advent of Large Language Models (LLMs) has significantly revolutionized web search. The emergence of LLM-based Search Agents marks a pivotal shift towards deeper, dynamic, autonomous information seeking. These agents can comprehend user intentions and environmental context and execute multi-turn retrieval with dynamic planning, extending search capabilities far beyond the web. Leading examples like OpenAI's Deep Research highlight their potential for deep information mining and real-world applications. This survey provides the first systematic analysis of search agents. We comprehensively analyze and categorize existing works from the perspectives of architecture, optimization, application, and evaluation, ultimately identifying critical open challenges and outlining promising future research directions in this rapidly evolving field. Our repository is available on https://github.com/YunjiaXi/Awesome-Search-Agent-Papers.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports</title>
<link>https://arxiv.org/abs/2508.05669</link>
<guid>https://arxiv.org/abs/2508.05669</guid>
<content:encoded><![CDATA[
<div> Keywords: financial documents, document understanding, vision-language model, Markdown format, Malaysian audited financial reports 

Summary:<br /><br />
This study focuses on accurately converting financial tables from Malaysian audited financial reports into Markdown format using a fine-tuned vision-language model (VLM). The model was optimized for high-fidelity Markdown generation and evaluated on 100 out-of-sample tables using a criteria-based assessment and a Markdown Tree-Edit-Distance-based Similarity (TEDS) metric. The model achieved a 92.20% overall accuracy on the criteria-based assessment and a 96.53% Markdown TEDS score, surpassing larger-scale VLMs and specialized reasoning-enabled models. It also outperformed widely used proprietary models like OpenAI's GPT-4o and Gemini 2.5 Flash while reducing inference time. These results highlight the effectiveness of domain-specific fine-tuning in bridging the gap between unstructured financial documents and downstream automation, offering a more efficient alternative to larger and more general models. <div>
arXiv:2508.05669v1 Announce Type: cross 
Abstract: Accurately extracting and representing the structure of tabular data from financial documents remains a critical challenge in document understanding, particularly for regulatory and analytical use cases. This study addresses the complexity of converting financial tables from Malaysian audited financial reports into Markdown format, a task complicated by rotated layouts, multi-level headers, and implicit structural cues. We propose a fine-tuned vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for high-fidelity Markdown generation from document images. Our approach includes a curated dataset of 2,152 image-text pairs with augmentations and a supervised fine-tuning strategy using LoRA. To assess performance, we evaluated our model on 100 out-of-sample tables using a dual framework: a criteria-based LLM-as-a-judge for fine-grained accuracy and our novel Markdown Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based assessment and a 96.53% Markdown TEDS score. This performance significantly surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized reasoning-enabled models. Compared to these self-hosted alternatives, it also significantly reduces inference time. Furthermore, its accuracy exceeds that of widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash. These results demonstrate that domain-specific fine-tuning provides an effective and efficient method to bridge the gap between unstructured financial documents and downstream automation, rivalling much larger and more general models without their computational overhead.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing</title>
<link>https://arxiv.org/abs/2508.05671</link>
<guid>https://arxiv.org/abs/2508.05671</guid>
<content:encoded><![CDATA[
<div> Framework, NLP, Adversarial threats, Defense, Robustness<br />
<br />
Summary: 
The article introduces DINA, a unified framework designed to address dual adversarial threats in NLP systems arising from internal label corruption and external manipulations. By combining noisy-label learning methods from computer vision with adversarial training, DINA aims to enhance model robustness and accuracy. Experiments conducted on a real-world dataset from an online gaming service demonstrate the effectiveness of DINA in improving model performance compared to baseline models. The study emphasizes the importance of dual-threat defenses in safeguarding NLP systems against adversarial scenarios and highlights practical strategies for enhancing the fairness and responsibility of AI deployment. <div>
arXiv:2508.05671v1 Announce Type: cross 
Abstract: As large language models (LLMs) and generative AI become increasingly integrated into customer service and moderation applications, adversarial threats emerge from both external manipulations and internal label corruption. In this work, we identify and systematically address these dual adversarial threats by introducing DINA (Dual Defense Against Internal Noise and Adversarial Attacks), a novel unified framework tailored specifically for NLP. Our approach adapts advanced noisy-label learning methods from computer vision and integrates them with adversarial training to simultaneously mitigate internal label sabotage and external adversarial perturbations. Extensive experiments conducted on a real-world dataset from an online gaming service demonstrate that DINA significantly improves model robustness and accuracy compared to baseline models. Our findings not only highlight the critical necessity of dual-threat defenses but also offer practical strategies for safeguarding NLP systems in realistic adversarial scenarios, underscoring broader implications for fair and responsible AI deployment.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection</title>
<link>https://arxiv.org/abs/2508.05694</link>
<guid>https://arxiv.org/abs/2508.05694</guid>
<content:encoded><![CDATA[
<div> Keyword: Insider threat detection, Dual-modality framework, Semantic inference, Behavior-aware fine-tuning, LoRA-enhanced LLMs <br />
Summary: 
Insider threat detection is a challenging task in cybersecurity due to the complex and context-dependent nature of malicious insider behaviors. Traditional models struggle to capture semantic intent and behavior dynamics, while existing LLM-based solutions have limitations in adaptability and modality coverage. To address these issues, the DMFI framework integrates semantic inference with behavior-aware fine-tuning, converting raw logs into structured views for analysis. By combining two LoRA-enhanced LLMs and a decision module, DMFI outperforms state-of-the-art methods in accuracy on CERT datasets. The introduction of DMFI-B, a discriminative adaptation strategy, enhances robustness under class imbalance. This approach effectively combines the semantic reasoning power of LLMs with structured behavior modeling, providing a scalable and deployable solution for insider threat detection. <br /><br />Summary: <div>
arXiv:2508.05694v1 Announce Type: cross 
Abstract: Insider threat detection (ITD) poses a persistent and high-impact challenge in cybersecurity due to the subtle, long-term, and context-dependent nature of malicious insider behaviors. Traditional models often struggle to capture semantic intent and complex behavior dynamics, while existing LLM-based solutions face limitations in prompt adaptability and modality coverage. To bridge this gap, we propose DMFI, a dual-modality framework that integrates semantic inference with behavior-aware fine-tuning. DMFI converts raw logs into two structured views: (1) a semantic view that processes content-rich artifacts (e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral abstraction, constructed via a 4W-guided (When-Where-What-Which) transformation to encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned independently, and their outputs are fused via a lightweight MLP-based decision module. We further introduce DMFI-B, a discriminative adaptation strategy that separates normal and abnormal behavior representations, improving robustness under severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets demonstrate that DMFI outperforms state-of-the-art methods in detection accuracy. Our approach combines the semantic reasoning power of LLMs with structured behavior modeling, offering a scalable and effective solution for real-world insider threat detection. Our work demonstrates the effectiveness of combining LLM reasoning with structured behavioral modeling, offering a scalable and deployable solution for modern insider threat detection.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization</title>
<link>https://arxiv.org/abs/2508.05731</link>
<guid>https://arxiv.org/abs/2508.05731</guid>
<content:encoded><![CDATA[
arXiv:2508.05731v1 Announce Type: cross 
Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the development of autonomous agents that operate on Graphical User Interfaces (GUIs) using pure visual input. A fundamental challenge is robustly grounding natural language instructions. This requires a precise spatial alignment, which accurately locates the coordinates of each element, and, more critically, a correct semantic alignment, which matches the instructions to the functionally appropriate UI element. Although Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be effective at improving spatial alignment for these MLLMs, we find that inefficient exploration bottlenecks semantic alignment, which prevent models from learning difficult semantic associations. To address this exploration problem, we present Adaptive Exploration Policy Optimization (AEPO), a new policy optimization framework. AEPO employs a multi-answer generation strategy to enforce broader exploration, which is then guided by a theoretically grounded Adaptive Exploration Reward (AER) function derived from first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B and InfiGUI-G1-7B, establish new state-of-the-art results across multiple challenging GUI grounding benchmarks, achieving significant relative improvements of up to 9.0% against the naive RLVR baseline on benchmarks designed to test generalization and semantic understanding. Resources are available at https://github.com/InfiXAI/InfiGUI-G1.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Basic interactive algorithms: Preview</title>
<link>https://arxiv.org/abs/2508.05798</link>
<guid>https://arxiv.org/abs/2508.05798</guid>
<content:encoded><![CDATA[
arXiv:2508.05798v1 Announce Type: cross 
Abstract: This dialog paper offers a preview and provides a foretaste of an upcoming work on the axiomatization of basic interactive algorithms.
  The modern notion of algorithm was elucidated in the 1930s--1950s. It was axiomatized a quarter of a century ago as the notion of ``sequential algorithm'' or ``classical algorithm''; we prefer to call it ``basic algorithm" now. The axiomatization was used to show that for every basic algorithm there is a behaviorally equivalent abstract state machine. It was also used to prove the Church-Turing thesis as it has been understood by the logicians.
  Starting from the 1960s, the notion of algorithm has expanded -- probabilistic algorithms, quantum algorithms, etc. -- prompting introduction of a much more ambitious version of the Church-Turing thesis commonly known as the ``physical thesis.'' We emphasize the difference between the two versions of the Church-Turing thesis and illustrate how nondeterministic and probabilistic algorithms can be viewed as basic algorithms with appropriate oracles. The same view applies to quantum circuit algorithms and many other classes of algorithms.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference</title>
<link>https://arxiv.org/abs/2508.05835</link>
<guid>https://arxiv.org/abs/2508.05835</guid>
<content:encoded><![CDATA[
arXiv:2508.05835v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have significantly advanced audio processing by leveraging audio codecs to discretize audio into tokens, enabling the application of language modeling techniques to speech data. However, existing audio codecs often operate at high frame rates, leading to slow training and inference, particularly for autoregressive models. To address this, there is growing interest in low frame-rate audio codecs, which reduce the number of autoregressive steps required to generate one second of audio. In this paper, we conduct ablation studies to examine the impact of frame rate, bitrate, and causality on codec reconstruction quality. Based on our findings, we introduce NanoCodec, a state-of-the-art audio codec that achieves high-quality compression at just 12.5 frames per second (FPS). NanoCodec outperforms related works across various bitrate ranges, establishing a new benchmark for low-latency and efficient Speech LLM training and inference.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction</title>
<link>https://arxiv.org/abs/2508.05913</link>
<guid>https://arxiv.org/abs/2508.05913</guid>
<content:encoded><![CDATA[
arXiv:2508.05913v1 Announce Type: cross 
Abstract: As AI systems become increasingly embedded in organizational workflows and consumer applications, ethical principles such as fairness, transparency, and robustness have been widely endorsed in policy and industry guidelines. However, there is still scarce empirical evidence on whether these principles are recognized, valued, or impactful from the perspective of users. This study investigates the link between ethical AI and user satisfaction by analyzing over 100,000 user reviews of AI products from G2. Using transformer-based language models, we measure sentiment across seven ethical dimensions defined by the EU Ethics Guidelines for Trustworthy AI. Our findings show that all seven dimensions are positively associated with user satisfaction. Yet, this relationship varies systematically across user and product types. Technical users and reviewers of AI development platforms more frequently discuss system-level concerns (e.g., transparency, data governance), while non-technical users and reviewers of end-user applications emphasize human-centric dimensions (e.g., human agency, societal well-being). Moreover, the association between ethical AI and user satisfaction is significantly stronger for non-technical users and end-user applications across all dimensions. Our results highlight the importance of ethical AI design from users' perspectives and underscore the need to account for contextual differences across user roles and product types.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents</title>
<link>https://arxiv.org/abs/2508.05954</link>
<guid>https://arxiv.org/abs/2508.05954</guid>
<content:encoded><![CDATA[
arXiv:2508.05954v1 Announce Type: cross 
Abstract: There is growing interest in integrating high-fidelity visual synthesis capabilities into large language models (LLMs) without compromising their strong reasoning capabilities. Existing methods that directly train LLMs or bridge LLMs and diffusion models usually suffer from costly training since the backbone LLMs have not seen image representations during pretraining. We present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs (MLLMs) and diffusion models using patch-level CLIP image embeddings as latent variables, which are natively aligned with the MLLM's CLIP visual encoder. These patch-level image embeddings are integrated into the diffusion model with a lightweight adaptation of its ControlNet. To retain the original multimodal reasoning capabilities of MLLMs, we equip the MLLM with a visual generation branch initialized from the original MLLM parameters when predicting the patch-level image embeddings. By seamlessly integrating pretrained MLLMs and diffusion models with patch-level CLIP latents, our framework enables high-fidelity controllable image generation with significant training efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or better performance than previous methods in terms of visual fidelity and multimodal understanding, with substantially lower compute during training. We also provide comprehensive ablation studies showing the effectiveness of our design choices.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Intelligent Coding Systems Should Write Programs with Justifications</title>
<link>https://arxiv.org/abs/2508.06017</link>
<guid>https://arxiv.org/abs/2508.06017</guid>
<content:encoded><![CDATA[
arXiv:2508.06017v1 Announce Type: cross 
Abstract: Intelligent coding systems are transforming software development by enabling users to specify code behavior in natural language. However, the opaque decision-making of AI-driven coders raises trust and usability concerns, particularly for non-expert users who cannot inspect low-level implementations. We argue that these systems should not only generate code but also produce clear, consistent justifications that bridge model reasoning and user understanding. To this end, we identify two critical justification properties-cognitive alignment and semantic faithfulness-and highlight the limitations of existing methods, including formal verification, static analysis, and post-hoc explainability. We advocate exploring neuro-symbolic approaches for justification generation, where symbolic constraints guide model behavior during training and program semantics are enriched through neural representations, enabling automated consistency checks at inference time.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System</title>
<link>https://arxiv.org/abs/2508.06059</link>
<guid>https://arxiv.org/abs/2508.06059</guid>
<content:encoded><![CDATA[
arXiv:2508.06059v1 Announce Type: cross 
Abstract: State-of-the-art fact-checking systems combat misinformation at scale by employing autonomous LLM-based agents to decompose complex claims into smaller sub-claims, verify each sub-claim individually, and aggregate the partial results to produce verdicts with justifications (explanatory rationales for the verdicts). The security of these systems is crucial, as compromised fact-checkers, which tend to be easily underexplored, can amplify misinformation. This work introduces Fact2Fiction, the first poisoning attack framework targeting such agentic fact-checking systems. Fact2Fiction mirrors the decomposition strategy and exploits system-generated justifications to craft tailored malicious evidences that compromise sub-claim verification. Extensive experiments demonstrate that Fact2Fiction achieves 8.9\%--21.2\% higher attack success rates than state-of-the-art attacks across various poisoning budgets. Fact2Fiction exposes security weaknesses in current fact-checking systems and highlights the need for defensive countermeasures.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation</title>
<link>https://arxiv.org/abs/2508.06065</link>
<guid>https://arxiv.org/abs/2508.06065</guid>
<content:encoded><![CDATA[
arXiv:2508.06065v1 Announce Type: cross 
Abstract: Generative AI has made image creation more accessible, yet aligning outputs with nuanced creative intent remains challenging, particularly for non-experts. Existing tools often require users to externalize ideas through prompts or references, limiting fluid exploration. We introduce ThematicPlane, a system that enables users to navigate and manipulate high-level semantic concepts (e.g., mood, style, or narrative tone) within an interactive thematic design plane. This interface bridges the gap between tacit creative intent and system control. In our exploratory study (N=6), participants engaged in divergent and convergent creative modes, often embracing unexpected results as inspiration or iteration cues. While they grounded their exploration in familiar themes, differing expectations of how themes mapped to outputs revealed a need for more explainable controls. Overall, ThematicPlane fosters expressive, iterative workflows and highlights new directions for intuitive, semantics-driven interaction in generative design tools.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges</title>
<link>https://arxiv.org/abs/2508.06401</link>
<guid>https://arxiv.org/abs/2508.06401</guid>
<content:encoded><![CDATA[
arXiv:2508.06401v1 Announce Type: cross 
Abstract: This systematic review of the research literature on retrieval-augmented generation (RAG) provides a focused analysis of the most highly cited studies published between 2020 and May 2025. A total of 128 articles met our inclusion criteria. The records were retrieved from ACM Digital Library, IEEE Xplore, Scopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP). RAG couples a neural retriever with a generative language model, grounding output in up-to-date, non-parametric memory while retaining the semantic generalisation stored in model weights. Guided by the PRISMA 2020 framework, we (i) specify explicit inclusion and exclusion criteria based on citation count and research questions, (ii) catalogue datasets, architectures, and evaluation practices, and (iii) synthesise empirical evidence on the effectiveness and limitations of RAG. To mitigate citation-lag bias, we applied a lower citation-count threshold to papers published in 2025 so that emerging breakthroughs with naturally fewer citations were still captured. This review clarifies the current research landscape, highlights methodological gaps, and charts priority directions for future research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample-efficient LLM Optimization with Reset Replay</title>
<link>https://arxiv.org/abs/2508.06412</link>
<guid>https://arxiv.org/abs/2508.06412</guid>
<content:encoded><![CDATA[
arXiv:2508.06412v1 Announce Type: cross 
Abstract: Recent advancements in post-training Large Language Models (LLMs), particularly through Reinforcement Learning (RL) and preference optimization methods, are key drivers for enhancing their reasoning capabilities. However, these methods are often plagued by low sample efficiency and a susceptibility to primacy bias, where overfitting to initial experiences degrades policy quality and damages the learning process. To address these challenges, we introduce LLM optimization with Reset Replay (LoRR), a general and powerful plugin designed to enhance sample efficiency in any preference-based optimization framework. LoRR core mechanism enables training at a high replay number, maximizing the utility of each collected data batch. To counteract the risk of overfitting inherent in high-replay training, LoRR incorporates a periodic reset strategy with reusing initial data, which preserves network plasticity. Furthermore, it leverages a hybrid optimization objective, combining supervised fine-tuning (SFT) and preference-based losses to further bolster data exploitation. Our extensive experiments demonstrate that LoRR significantly boosts the performance of various preference optimization methods on both mathematical and general reasoning benchmarks. Notably, an iterative DPO approach augmented with LoRR achieves comparable performance on challenging math tasks, outperforming some complex and computationally intensive RL-based algorithms. These findings highlight that LoRR offers a practical, sample-efficient, and highly effective paradigm for LLM finetuning, unlocking greater performance from limited data.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</title>
<link>https://arxiv.org/abs/2508.06457</link>
<guid>https://arxiv.org/abs/2508.06457</guid>
<content:encoded><![CDATA[
arXiv:2508.06457v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and reasoning capabilities, but their potential for misuse has raised growing concern. In this paper, we present ScamAgent, an autonomous multi-turn agent built on top of LLMs, capable of generating highly realistic scam call scripts that simulate real-world fraud scenarios. Unlike prior work focused on single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts dynamically to simulated user responses, and employs deceptive persuasion strategies across conversational turns. We show that current LLM safety guardrails, including refusal mechanisms and content filters, are ineffective against such agent-based threats. Even models with strong prompt-level safeguards can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework. We further demonstrate the transformation of scam scripts into lifelike voice calls using modern text-to-speech systems, completing a fully automated scam pipeline. Our findings highlight an urgent need for multi-turn safety auditing, agent-level control frameworks, and new methods to detect and disrupt conversational deception powered by generative AI.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective Training Data Synthesis for Improving MLLM Chart Understanding</title>
<link>https://arxiv.org/abs/2508.06492</link>
<guid>https://arxiv.org/abs/2508.06492</guid>
<content:encoded><![CDATA[
arXiv:2508.06492v1 Announce Type: cross 
Abstract: Being able to effectively read scientific plots, or chart understanding, is a central part toward building effective agents for science. However, existing multimodal large language models (MLLMs), especially open-source ones, are still falling behind with a typical success rate of 30%-50% on challenging benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are often restricted by their inadequate similarity to the real charts, which could compromise model training and performance on complex real-world charts. In this study, we show that modularizing chart generation and diversifying visual details improves chart understanding capabilities. In particular, we design a five-step data synthesis pipeline, where we separate data and function creation for single plot generation, condition the generation of later subplots on earlier ones for multi-subplot figures, visually diversify the generated figures, filter out low quality data, and finally generate the question-answer (QA) pairs with GPT-4o. This approach allows us to streamline the generation of fine-tuning datasets and introduce the effective chart dataset (ECD), which contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring 250+ chart type combinations with high visual complexity. We show that ECD consistently improves the performance of various MLLMs on a range of real-world and synthetic test sets. Code, data and models are available at: https://github.com/yuweiyang-anu/ECD.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLMs on the Semantic Overlap Summarization Task</title>
<link>https://arxiv.org/abs/2402.17008</link>
<guid>https://arxiv.org/abs/2402.17008</guid>
<content:encoded><![CDATA[
arXiv:2402.17008v2 Announce Type: replace 
Abstract: Semantic Overlap Summarization (SOS) is a constrained multi-document summarization task, where the constraint is to capture the common/overlapping information between two alternative narratives. In this work, we perform a benchmarking study of popular Large Language Models (LLMs) exclusively on the SOS task. Additionally, we introduce the PrivacyPolicyPairs (3P) dataset to expand the space of SOS benchmarks in terms of quantity and variety. This dataset provides 135 high-quality SOS data samples sourced from privacy policy documents. We then use a standard prompting taxonomy called TELeR to create and evaluate 905,216 distinct LLM-generated summaries over two SOS datasets from different domains, and we further conduct human evaluation on a subset of 540 samples. We conclude the paper by analyzing models' performances and the reliability of automatic evaluation. The code and datasets used to conduct this study are available at https://anonymous.4open.science/r/llm_eval-E16D.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Pareto Optimal Throughput in Small Language Model Serving</title>
<link>https://arxiv.org/abs/2404.03353</link>
<guid>https://arxiv.org/abs/2404.03353</guid>
<content:encoded><![CDATA[
arXiv:2404.03353v3 Announce Type: replace 
Abstract: Large language models (LLMs) have revolutionized the state-of-the-art of many different natural language processing tasks. Although serving LLMs is computationally and memory demanding, the rise of Small Language Models (SLMs) offers new opportunities for resource-constrained users, who now are able to serve small models with cutting-edge performance. In this paper, we present a set of experiments designed to benchmark SLM inference at performance and energy levels. Our analysis provides a new perspective in serving, highlighting that the small memory footprint of SLMs allows for reaching the Pareto-optimal throughput within the resource capacity of a single accelerator. In this regard, we present an initial set of findings demonstrating how model replication can effectively improve resource utilization for serving SLMs.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extract-and-Abstract: Unifying Extractive and Abstractive Summarization within Single Encoder-Decoder Framework</title>
<link>https://arxiv.org/abs/2409.11827</link>
<guid>https://arxiv.org/abs/2409.11827</guid>
<content:encoded><![CDATA[
arXiv:2409.11827v2 Announce Type: replace 
Abstract: Extract-then-Abstract is a naturally coherent paradigm to conduct abstractive summarization with the help of salient information identified by the extractive model. Previous works that adopt this paradigm train the extractor and abstractor separately and introduce extra parameters to highlight the extracted salients to the abstractor, which results in error accumulation and additional training costs. In this paper, we first introduce a parameter-free highlight method into the encoder-decoder framework: replacing the encoder attention mask with a saliency mask in the cross-attention module to force the decoder to focus only on salient parts of the input. A preliminary analysis compares different highlight methods, demonstrating the effectiveness of our saliency mask. We further propose the novel extract-and-abstract paradigm, ExtAbs., which jointly and seamlessly performs Extractive and Abstractive summarization tasks within single encoder-decoder model to reduce error accumulation. In ExtAbs, the vanilla encoder is augmented to extract salients, and the vanilla decoder is modified with the proposed saliency mask to generate summaries. Built upon BART and PEGASUS, experiments on three datasets show that ExtAbs can achieve superior performance than baselines on the extractive task and performs comparable, or even better than the vanilla models on the abstractive task.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions</title>
<link>https://arxiv.org/abs/2501.01872</link>
<guid>https://arxiv.org/abs/2501.01872</guid>
<content:encoded><![CDATA[
arXiv:2501.01872v3 Announce Type: replace 
Abstract: Large language models, despite extensive alignment with human values and ethical principles, remain vulnerable to sophisticated jailbreak attacks that exploit their reasoning abilities. Existing safety measures often detect overt malicious intent but fail to address subtle, reasoning-driven vulnerabilities. In this work, we introduce POATE (Polar Opposite query generation, Adversarial Template construction, and Elaboration), a novel jailbreak technique that harnesses contrastive reasoning to provoke unethical responses. POATE crafts semantically opposing intents and integrates them with adversarial templates, steering models toward harmful outputs with remarkable subtlety. We conduct extensive evaluation across six diverse language model families of varying parameter sizes to demonstrate the robustness of the attack, achieving significantly higher attack success rates (~44%) compared to existing methods. To counter this, we propose Intent-Aware CoT and Reverse Thinking CoT, which decompose queries to detect malicious intent and reason in reverse to evaluate and reject harmful responses. These methods enhance reasoning robustness and strengthen the model's defense against adversarial exploits.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs</title>
<link>https://arxiv.org/abs/2501.10970</link>
<guid>https://arxiv.org/abs/2501.10970</guid>
<content:encoded><![CDATA[
arXiv:2501.10970v4 Announce Type: replace 
Abstract: The "LLM-as-an-annotator" and "LLM-as-a-judge" paradigms employ Large Language Models (LLMs) as annotators, judges, and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure, the Alternative Annotator Test (alt-test), that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM annotators and judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-4o), outperforming the open-source LLMs we examine, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>