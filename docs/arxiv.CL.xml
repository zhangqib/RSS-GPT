<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>


<item>
<title>Large Language Models in Argument Mining: A Survey</title>
<link>https://arxiv.org/abs/2506.16383</link>
<guid>https://arxiv.org/abs/2506.16383</guid>
<content:encoded><![CDATA[
<div> Keywords: Argument Mining, Natural Language Processing, Large Language Models, Annotation frameworks, Evaluation practices

Summary: 
Argument Mining (AM) in Natural Language Processing has been significantly impacted by the emergence of Large Language Models (LLMs), allowing for in-context learning, prompt-based generation, and cross-domain adaptability. This survey examines recent advancements in LLM-driven AM, covering foundational theories, annotation frameworks, datasets, and a taxonomy of AM subtasks. The review includes discussions on LLM techniques such as prompting and chain-of-thought reasoning, as well as challenges like long-context reasoning and annotation bottlenecks. Assessment of LLM architectures, methodologies, and evaluation practices is provided, along with insights into interpretability issues. The paper concludes by identifying trends and proposing a research agenda for future developments in LLM-based computational argumentation. The comprehensive overview aims to support researchers in navigating the evolving landscape of AM with LLMs.<br /><br />Summary: <div>
arXiv:2506.16383v5 Announce Type: replace 
Abstract: Argument Mining (AM), a critical subfield of Natural Language Processing (NLP), focuses on extracting argumentative structures from text. The advent of Large Language Models (LLMs) has profoundly transformed AM, enabling advanced in-context learning, prompt-based generation, and robust cross-domain adaptability. This survey systematically synthesizes recent advancements in LLM-driven AM. We provide a concise review of foundational theories and annotation frameworks, alongside a meticulously curated catalog of datasets. A key contribution is our comprehensive taxonomy of AM subtasks, elucidating how contemporary LLM techniques -- such as prompting, chain-of-thought reasoning, and retrieval augmentation -- have reconfigured their execution. We further detail current LLM architectures and methodologies, critically assess evaluation practices, and delineate pivotal challenges including long-context reasoning, interpretability, and annotation bottlenecks. Conclusively, we highlight emerging trends and propose a forward-looking research agenda for LLM-based computational argumentation, aiming to strategically guide researchers in this rapidly evolving domain.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HuggingGraph: Understanding the Supply Chain of LLM Ecosystem</title>
<link>https://arxiv.org/abs/2507.14240</link>
<guid>https://arxiv.org/abs/2507.14240</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, natural language processing, Hugging Face, vulnerabilities, supply chain

Summary:
Large language models (LLMs) are powerful tools in natural language processing, but their size and complexity pose challenges for researchers. Platforms like Hugging Face host a vast number of models and datasets, but these can inherit vulnerabilities, biases, or malicious components from previous models. To address this, a study examines the supply chain of LLMs by collecting information and creating a graph to model relationships between models and datasets. Analysis on this graph reveals insights into the development process and potential risks associated with LLMs. Understanding these relationships is crucial for improving model fairness, detecting risks, and ensuring compliance with regulatory frameworks.<br /><br />Summary: <div>
arXiv:2507.14240v2 Announce Type: replace 
Abstract: Large language models (LLMs) leverage deep learning architectures to process and predict sequences of words based on context, enabling them to perform a wide range of natural language processing tasks, such as translation, summarization, question answering, and content generation. However, the increasing size and complexity of developing, training, and deploying cutting-edge LLMs demand extensive computational resources and large-scale datasets. This creates a significant barrier for researchers and practitioners. Because of that, platforms that host models and datasets have gained widespread popularity. For example, on one of the most popular platforms, i.e., Hugging Face, there are more than 1.8 million models and more than 450K datasets by the end of June 2025, and the trend does not show any slowdown.
  As existing LLMs are often built from base models or other pretrained models and use external datasets, they can inevitably inherit vulnerabilities, biases, or malicious components that exist in previous models or datasets. Therefore, it is critical to understand these components' origin and development process to detect potential risks better, improve model fairness, and ensure compliance with regulatory frameworks. Motivated by that, this project aims to study such relationships between models and datasets, which are the central parts of the LLM supply chain. First, we design a methodology to collect LLMs' supply chain information systematically. With the collected information, we design a new graph to model the relationships between models and datasets, which is a large directed heterogeneous graph having 402,654 nodes and 462,524 edges. Then, on top of this graph, we perform different types of analysis and make multiple interesting findings.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches</title>
<link>https://arxiv.org/abs/2508.00864</link>
<guid>https://arxiv.org/abs/2508.00864</guid>
<content:encoded><![CDATA[
<div> Keywords: document classification, graph-based models, self-attention model, data-driven graph structures, statistical filtering

Summary:
Our study introduces a novel method for document classification that utilizes graph-based models to capture document structure effectively. Unlike previous approaches, our method learns data-driven graph structures, eliminating the need for manual design and reducing domain dependence. The approach constructs homogeneous weighted graphs with sentences as nodes and learns edges using a self-attention model to identify dependencies between sentence pairs. A statistical filtering strategy is employed to retain only strongly correlated sentences, enhancing graph quality and reducing size. Experimental results on three document classification datasets show that learned graphs consistently outperform heuristic-based graphs, achieving higher accuracy and $F_1$ score. The study also highlights the effectiveness of statistical filtering in improving classification robustness. These findings suggest the potential of automatic graph generation in NLP applications and pave the way for broader applications in the field. 

Summary: <div>
arXiv:2508.00864v1 Announce Type: new 
Abstract: In document classification, graph-based models effectively capture document structure, overcoming sequence length limitations and enhancing contextual understanding. However, most existing graph document representations rely on heuristics, domain-specific rules, or expert knowledge. Unlike previous approaches, we propose a method to learn data-driven graph structures, eliminating the need for manual design and reducing domain dependence. Our approach constructs homogeneous weighted graphs with sentences as nodes, while edges are learned via a self-attention model that identifies dependencies between sentence pairs. A statistical filtering strategy aims to retain only strongly correlated sentences, improving graph quality while reducing the graph size. Experiments on three document classification datasets demonstrate that learned graphs consistently outperform heuristic-based graphs, achieving higher accuracy and $F_1$ score. Furthermore, our study demonstrates the effectiveness of the statistical filtering in improving classification robustness. These results highlight the potential of automatic graph generation over traditional heuristic approaches and open new directions for broader applications in NLP.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts</title>
<link>https://arxiv.org/abs/2508.00889</link>
<guid>https://arxiv.org/abs/2508.00889</guid>
<content:encoded><![CDATA[
<div> Large language models, factuality evaluation, contact center conversations, FECT benchmark dataset, 3D paradigm
Summary: 
- The article introduces the challenges of hallucinations in large language models analyzing contact center conversations.
- A 3D paradigm (Decompose, Decouple, Detach) is introduced for factuality evaluation, grounding factuality labels in linguistically-informed criteria.
- The FECT benchmark dataset is presented for Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts, labeled under the 3D paradigm.
- Findings on aligning LLM-judges on the 3D paradigm are reported.
- The study contributes a new approach for automatically evaluating factuality in AI-generated outputs from contact center conversations.
<br /><br /> <div>
arXiv:2508.00889v1 Announce Type: new 
Abstract: Large language models (LLMs) are known to hallucinate, producing natural language outputs that are not grounded in the input, reference materials, or real-world knowledge. In enterprise applications where AI features support business decisions, such hallucinations can be particularly detrimental. LLMs that analyze and summarize contact center conversations introduce a unique set of challenges for factuality evaluation, because ground-truth labels often do not exist for analytical interpretations about sentiments captured in the conversation and root causes of the business problems. To remedy this, we first introduce a \textbf{3D} -- \textbf{Decompose, Decouple, Detach} -- paradigm in the human annotation guideline and the LLM-judges' prompt to ground the factuality labels in linguistically-informed evaluation criteria. We then introduce \textbf{FECT}, a novel benchmark dataset for \textbf{F}actuality \textbf{E}valuation of Interpretive AI-Generated \textbf{C}laims in Contact Center Conversation \textbf{T}ranscripts, labeled under our 3D paradigm. Lastly, we report our findings from aligning LLM-judges on the 3D paradigm. Overall, our findings contribute a new approach for automatically evaluating the factuality of outputs generated by an AI system for analyzing contact center conversations.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML</title>
<link>https://arxiv.org/abs/2508.00924</link>
<guid>https://arxiv.org/abs/2508.00924</guid>
<content:encoded><![CDATA[
<div> AutoML, language models, meta-learning, fine-tuning, resource-efficient <br />
Summary:<br />
XAutoLM is a new automated framework for efficient language model fine-tuning that leverages meta-learning to optimize model selection and hyperparameter optimization. By learning from past experiences, XAutoLM extracts meta-features to guide the search process towards successful configurations and away from dead ends. Results on various text classification and question-answering tasks show that XAutoLM outperforms zero-shot optimization methods, reduces evaluation time, decreases error rates, and identifies more effective pipelines. The framework is designed to enhance resource-efficient and environmentally friendly fine-tuning of language models in the NLP community. The release of XAutoLM and the associated experience store aims to facilitate Green AI practices and promote more sustainable machine learning processes. <br /> <br />Summary: <div>
arXiv:2508.00924v1 Announce Type: new 
Abstract: Experts in machine learning leverage domain knowledge to navigate decisions in model selection, hyperparameter optimisation, and resource allocation. This is particularly critical for fine-tuning language models (LMs), where repeated trials incur substantial computational overhead and environmental impact. However, no existing automated framework simultaneously tackles the entire model selection and HPO task for resource-efficient LM fine-tuning. We introduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past experiences to optimise discriminative and generative LM fine-tuning pipelines efficiently. XAutoLM learns from stored successes and failures by extracting task- and system-level meta-features to bias its sampling toward fruitful configurations and away from costly dead ends. On four text classification and two question-answering benchmarks, XAutoLM surpasses zero-shot optimiser's peak F1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error ratios by up to sevenfold, and uncovers up to 50% more pipelines above the zero-shot Pareto front. In contrast, simpler memory-based baselines suffer negative transfer. We release XAutoLM and our experience store to catalyse resource-efficient, Green AI fine-tuning in the NLP community.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.01005</link>
<guid>https://arxiv.org/abs/2508.01005</guid>
<content:encoded><![CDATA[
<div> Keywords: question-answering, retrieval-augmented generation, adaptive framework, multi-agent orchestration, reinforcement learning

Summary: 
The article introduces a new adaptive framework for question-answering systems called MAO-ARAG, which utilizes multi-agent orchestration to dynamically plan workflows for each query. The framework consists of multiple executor agents for tasks such as query reformulation, document selection, and generation. A planner agent selects and integrates these executors in a suitable workflow, guided by reinforcement learning. The framework aims to balance high-quality answers with reasonable costs by optimizing a reward based on F1 score and penalizing costs. Experimental results on various datasets show that MAO-ARAG achieves high answer quality while maintaining cost and latency within acceptable limits. The code for MAO-ARAG is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2508.01005v1 Announce Type: new 
Abstract: In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has become pivotal in enhancing response accuracy and reducing hallucination issues. The architecture of RAG systems varies significantly, encompassing single-round RAG, iterative RAG, and reasoning RAG, each tailored to address different types of queries. Due to the varying complexity of real-world queries, a fixed RAG pipeline often struggles to balance performance and cost efficiency across different queries. To address this challenge, we propose an adaptive RAG framework called MAO-ARAG, which leverages multi-agent orchestration. Our adaptive RAG is conceived as a multi-turn framework. Specifically, we define multiple executor agents, representing typical RAG modules such as query reformulation agents, document selection agent, and generation agents. A planner agent intelligently selects and integrates the appropriate agents from these executors into a suitable workflow tailored for each query, striving for high-quality answers while maintaining reasonable costs. During each turn, the planner agent is trained using reinforcement learning, guided by an outcome-based reward (F1 score) and a cost-based penalty, continuously improving answer quality while keeping costs within a reasonable range. Experiments conducted on multiple QA datasets demonstrate that our approach, which dynamically plans workflows for each query, not only achieves high answer quality but also maintains both cost and latency within acceptable limits.The code of MAO-ARAG is on https://github.com/chenyiqun/Agentic-RAG.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu</title>
<link>https://arxiv.org/abs/2508.01006</link>
<guid>https://arxiv.org/abs/2508.01006</guid>
<content:encoded><![CDATA[
<div> Benchmark, Linguistic Minimal Pairs, Urdu, Multilingual Large Language Models, Syntactic Phenomena<br />
<br />
Summary: <br />
A new benchmark called UrBLiMP assesses the linguistic knowledge of Multilingual Large Language Models (LLMs) in Urdu by using pairs of minimally different sentences. The dataset comprises 5,696 minimal pairs targeting core syntactic phenomena in Urdu, ensuring reliability through human evaluation with a 96.10% inter-annotator agreement. Evaluation of twenty multilingual LLMs on UrBLiMP reveals varying performance across linguistic phenomena, with models like LLaMA-3-70B and Gemma-3-27B-PT achieving high accuracy. This study showcases the potential and limitations of current multilingual LLMs in capturing fine-grained syntactic knowledge in low-resource languages. <br /> <div>
arXiv:2508.01006v1 Announce Type: new 
Abstract: Multilingual Large Language Models (LLMs) have shown remarkable performance across various languages; however, they often include significantly less data for low-resource languages such as Urdu compared to high-resource languages like English. To assess the linguistic knowledge of LLMs in Urdu, we present the Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) i.e. pairs of minimally different sentences that contrast in grammatical acceptability. UrBLiMP comprises 5,696 minimal pairs targeting ten core syntactic phenomena, carefully curated using the Urdu Treebank and diverse Urdu text corpora. A human evaluation of UrBLiMP annotations yielded a 96.10% inter-annotator agreement, confirming the reliability of the dataset. We evaluate twenty multilingual LLMs on UrBLiMP, revealing significant variation in performance across linguistic phenomena. While LLaMA-3-70B achieves the highest average accuracy (94.73%), its performance is statistically comparable to other top models such as Gemma-3-27B-PT. These findings highlight both the potential and the limitations of current multilingual LLMs in capturing fine-grained syntactic knowledge in low-resource languages.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Domain Web Information Extraction at Pinterest</title>
<link>https://arxiv.org/abs/2508.01096</link>
<guid>https://arxiv.org/abs/2508.01096</guid>
<content:encoded><![CDATA[
<div> structured data, Pinterest, attribute extraction, webpage representation, XGBoost 

Summary:
- Pinterest has developed a system for accurately extracting structured product data from e-commerce websites.
- The system uses a unique webpage representation that combines structural, visual, and text modalities for efficient learning.
- This representation captures information from visible HTML nodes, including text, style, and layout details.
- Simple models like eXtreme Gradient Boosting (XGBoost) outperform more complex models like Generative Pre-trained Transformer (GPT) in attribute extraction accuracy.
- The system is highly scalable, processing over 1,000 URLs per second, and is significantly more cost-effective than GPT alternatives. 

<br /><br />Summary: <div>
arXiv:2508.01096v1 Announce Type: new 
Abstract: The internet offers a massive repository of unstructured information, but it's a significant challenge to convert this into a structured format. At Pinterest, the ability to accurately extract structured product data from e-commerce websites is essential to enhance user experiences and improve content distribution. In this paper, we present Pinterest's system for attribute extraction, which achieves remarkable accuracy and scalability at a manageable cost. Our approach leverages a novel webpage representation that combines structural, visual, and text modalities into a compact form, optimizing it for small model learning. This representation captures each visible HTML node with its text, style and layout information. We show how this allows simple models such as eXtreme Gradient Boosting (XGBoost) to extract attributes more accurately than much more complex Large Language Models (LLMs) such as Generative Pre-trained Transformer (GPT). Our results demonstrate a system that is highly scalable, processing over 1,000 URLs per second, while being 1000 times more cost-effective than the cheapest GPT alternatives.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates</title>
<link>https://arxiv.org/abs/2508.01159</link>
<guid>https://arxiv.org/abs/2508.01159</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, clinical consultation templates, electronic consultation, prioritization analysis, structured clinical information exchange <br />
Summary: 
This study evaluates the capabilities of large language models (LLMs) in generating structured clinical consultation templates for electronic consultations. Models like o3, GPT-4o, Kimi K2, Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro were assessed for their ability to create clinically coherent and concise templates. While these models showed high comprehensiveness, they struggled with generating overly long templates and prioritizing the most important clinical questions within length constraints. Performance varied across different medical specialties, with narrative-driven fields like psychiatry and pain medicine showing significant deterioration. The study highlights the potential of LLMs in improving structured clinical information exchange among physicians but also underscores the necessity for more robust evaluation methods that consider a model's capacity to prioritize crucial clinical information within real-world communication time constraints. <br /><br />Summary: <div>
arXiv:2508.01159v1 Announce Type: new 
Abstract: This study evaluates the capacity of large language models (LLMs) to generate structured clinical consultation templates for electronic consultation. Using 145 expert-crafted templates developed and routinely used by Stanford's eConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2, Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to produce clinically coherent, concise, and prioritized clinical question schemas. Through a multi-agent pipeline combining prompt optimization, semantic autograding, and prioritization analysis, we show that while models like o3 achieve high comprehensiveness (up to 92.2\%), they consistently generate excessively long templates and fail to correctly prioritize the most clinically important questions under length constraints. Performance varies across specialties, with significant degradation in narrative-driven fields such as psychiatry and pain medicine. Our findings demonstrate that LLMs can enhance structured clinical information exchange between physicians, while highlighting the need for more robust evaluation methods that capture a model's ability to prioritize clinically salient information within the time constraints of real-world physician communication.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages</title>
<link>https://arxiv.org/abs/2508.01161</link>
<guid>https://arxiv.org/abs/2508.01161</guid>
<content:encoded><![CDATA[
<div> Keywords: emotion recognition, multilingual LLMs, Semeval 2025, task adaptation strategies, LoRA <br />
Summary: <br />
- The study focuses on emotion recognition across different languages, highlighting the challenges due to varied expressions and cultural nuances.
- The Semeval 2025 Task 11, "Bridging the Gap in Text-Based Emotion," aims to develop an emotion recognizer for identifying basic emotional states and their intensities in written text snippets.
- Various task-adaptation strategies for Language Models (LLMs) in emotion recognition were investigated, with a focus on fine-tuning pre-trained multilingual LLMs using LoRA setting separately for each language.
- The study found that fine-tuning a multilingual LLM with LoRA setting for each language proved to be the most effective method for the emotion recognition task.
- This research contributes to the advancement of emotion recognition technology and highlights the importance of considering language-specific nuances in emotion analysis. <br /> 
Summary: <div>
arXiv:2508.01161v1 Announce Type: new 
Abstract: Detecting emotions across different languages is challenging due to the varied and culturally nuanced ways of emotional expressions. The \textit{Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion} shared task was organised to investigate emotion recognition across different languages. The goal of the task is to implement an emotion recogniser that can identify the basic emotional states that general third-party observers would attribute to an author based on their written text snippet, along with the intensity of those emotions. We report our investigation of various task-adaptation strategies for LLMs in emotion recognition. We show that the most effective method for this task is to fine-tune a pre-trained multilingual LLM with LoRA setting separately for each language.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Content Restriction for Large Language Models via Suffix Optimization</title>
<link>https://arxiv.org/abs/2508.01198</link>
<guid>https://arxiv.org/abs/2508.01198</guid>
<content:encoded><![CDATA[
<div> Adaptive Content Restriction, Large Language Models, Supervised Fine-Tuning, Suffix Optimization, Content Restriction Benchmark  
Summary:  
- Large Language Models (LLMs) have been successful but face challenges in enforcing content restrictions.
- Supervised Fine-Tuning (SFT) is used to prevent LLMs from generating harmful content.
- The proposed task, Adaptive Content Restriction (AdaCoRe), focuses on lightweight strategies to prevent LLMs from generating specific restricted terms.
- Suffix Optimization (SOP) is introduced as a method to append a short suffix to prompts to achieve content restriction without model fine-tuning.
- The effectiveness of SOP is demonstrated on the Content Restriction Benchmark (CoReBench) and online platform POE, showcasing practical applicability in real-world scenarios.  
<br /><br />Summary:  
This article introduces Adaptive Content Restriction (AdaCoRe) as a lightweight strategy to prevent Large Language Models (LLMs) from generating specific restricted terms. The proposed method, Suffix Optimization (SOP), effectively appends optimized suffixes to prompts, achieving content restriction without the need for model fine-tuning. Evaluation on the Content Restriction Benchmark (CoReBench) and the online platform POE shows that SOP outperforms system-level baselines, highlighting its practical utility in real-world applications. <div>
arXiv:2508.01198v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated significant success across diverse applications. However, enforcing content restrictions remains a significant challenge due to their expansive output space. One aspect of content restriction is preventing LLMs from generating harmful content via model alignment approaches such as supervised fine-tuning (SFT). Yet, the need for content restriction may vary significantly across user groups, change rapidly over time, and not always align with general definitions of harmfulness. Applying SFT to each of these specific use cases is impractical due to the high computational, data, and storage demands. Motivated by this need, we propose a new task called \textit{Adaptive Content Restriction} (AdaCoRe), which focuses on lightweight strategies -- methods without model fine-tuning -- to prevent deployed LLMs from generating restricted terms for specific use cases. We propose the first method for AdaCoRe, named \textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to any prompt to a) prevent a target LLM from generating a set of restricted terms, while b) preserving the output quality. To evaluate AdaCoRe approaches, including our SOP, we create a new \textit{Content Restriction Benchmark} (CoReBench), which contains 400 prompts for 80 restricted terms across 8 carefully selected categories. We demonstrate the effectiveness of SOP on CoReBench, which outperforms the system-level baselines such as system suffix by 15\%, 17\%, 10\%, 9\%, and 6\% on average restriction rates for Gemma2-2B, Mistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also demonstrate that SOP is effective on POE, an online platform hosting various commercial LLMs, highlighting its practicality in real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Show or Tell? Modeling the evolution of request-making in Human-LLM conversations</title>
<link>https://arxiv.org/abs/2508.01213</link>
<guid>https://arxiv.org/abs/2508.01213</guid>
<content:encoded><![CDATA[
<div> keywords: chat logs, user behavior, LLM queries, diachronic analyses, model capabilities
Summary: 
Chat logs are a valuable resource for studying user behavior in Language Model (LLM) queries. A new task of segmenting chat queries into different elements highlights how request-making in LLM queries differs from human-human interactions. Diachronic analyses on user expressions reveal evolving query patterns, with early queries focusing on requests and users eventually converging with experience. The introduction of new models influences user behavior, leading to traceable changes at the community level. This research sheds light on the complexities of user interactions in chat-based systems and the impact of model capabilities on query patterns over time.<br /><br />Summary: <div>
arXiv:2508.01213v1 Announce Type: new 
Abstract: Chat logs provide a rich source of information about LLM users, but patterns of user behavior are often masked by the variability of queries. We present a new task, segmenting chat queries into contents of requests, roles, query-specific context, and additional expressions. We find that, despite the familiarity of chat-based interaction, request-making in LLM queries remains significantly different from comparable human-human interactions. With the data resource, we introduce an important perspective of diachronic analyses with user expressions. We find that query patterns vary between early ones emphasizing requests, and individual users explore patterns but tend to converge with experience. Finally, we show that model capabilities affect user behavior, particularly with the introduction of new models, which are traceable at the community level.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebDS: An End-to-End Benchmark for Web-based Data Science</title>
<link>https://arxiv.org/abs/2508.01222</link>
<guid>https://arxiv.org/abs/2508.01222</guid>
<content:encoded><![CDATA[
<div> benchmark, web-based data science, multi-hop interactions, end-to-end workflows, performance gaps <br />
Summary: <br />
The article introduces WebDS, the first end-to-end web-based data science benchmark consisting of 870 tasks from diverse websites. It addresses the complexity of real-world data science tasks that involve multi-hop interactions and diverse tool-using capabilities. Existing benchmarks focus on simplistic interactions or static datasets, lacking the realism of modern data analytics tasks. Evaluation of state-of-the-art language model agents on WebDS reveals significant performance gaps, indicating the need for improvement in handling complex web-based data science tasks. Browser Use, which performs well on other benchmarks, struggles on WebDS due to new failure modes like poor information grounding and repetitive behavior. By providing a more realistic testing ground, WebDS aims to advance the development of language model-based data science applications. <br /> <div>
arXiv:2508.01222v1 Announce Type: new 
Abstract: A large portion of real-world data science tasks are complex and require multi-hop web-based interactions: finding appropriate data available on the internet, synthesizing real-time data of various modalities from different locations, and producing summarized analyses. Existing web benchmarks often focus on simplistic interactions, such as form submissions or e-commerce transactions, and often do not require diverse tool-using capabilities required for web based data science. Conversely, traditional data science benchmarks typically concentrate on static, often textually bound datasets and do not assess end-to-end workflows that encompass data acquisition, cleaning, analysis, and insight generation. In response, we introduce WebDS, the first end-to-end web-based data science benchmark. It comprises 870 web-based data science tasks across 29 diverse websites from structured government data portals to unstructured news media, challenging agents to perform complex, multi-step operations requiring the use of tools and heterogeneous data formats that better reflect the realities of modern data analytics. Evaluations of current SOTA LLM agents indicate significant performance gaps in accomplishing these tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web Voyager, successfully completes only 15% of tasks in WebDS, which our analysis suggests is due to new failure modes like poor information grounding, repetitive behavior and shortcut-taking that agents performing WebDS' tasks display. By providing a more robust and realistic testing ground, WebDS sets the stage for significant advances in the development of practically useful LLM-based data science.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework</title>
<link>https://arxiv.org/abs/2508.01245</link>
<guid>https://arxiv.org/abs/2508.01245</guid>
<content:encoded><![CDATA[
<div> framework, mathematics, data synthesis, progressive training, LLMs
Summary:
WarriorMath, a defect-aware framework for mathematical problem solving, addresses limitations in Large Language Models (LLMs) by integrating targeted data synthesis and progressive training. In the synthesis stage, multiple expert LLMs collaborate to generate, refine, and improve problems that base models struggle with. This results in high-quality, defect-aware training data. The training stage employs a progressive learning framework that fine-tunes the model using increasingly challenging data tailored to its weaknesses. Experimental results on six mathematical benchmarks show that WarriorMath outperforms strong baselines by 12.57% on average, setting a new state-of-the-art. This approach demonstrates the effectiveness of a defect-aware, multi-expert framework in enhancing mathematical ability. <br /><br />Summary: <div>
arXiv:2508.01245v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel in solving mathematical problems, yet their performance is often limited by the availability of high-quality, diverse training data. Existing methods focus on augmenting datasets through rephrasing or difficulty progression but overlook the specific failure modes of LLMs. This results in synthetic questions that the model can already solve, providing minimal performance gains. To address this, we propose WarriorMath, a defect-aware framework for mathematical problem solving that integrates both targeted data synthesis and progressive training. In the synthesis stage, we employ multiple expert LLMs in a collaborative process to generate, critique, and refine problems. Questions that base LLMs fail to solve are identified and iteratively improved through expert-level feedback, producing high-quality, defect-aware training data. In the training stage, we introduce a progressive learning framework that iteratively fine-tunes the model using increasingly challenging data tailored to its weaknesses. Experiments on six mathematical benchmarks show that WarriorMath outperforms strong baselines by 12.57% on average, setting a new state-of-the-art. Our results demonstrate the effectiveness of a defect-aware, multi-expert framework for improving mathematical ability.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025</title>
<link>https://arxiv.org/abs/2508.01263</link>
<guid>https://arxiv.org/abs/2508.01263</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Education, XAI, Hackathon, Trustworthiness

Summary:
The article discusses the XAI Challenge 2025, a hackathon-style competition focused on eXplainable AI in education, specifically building QA systems for student queries with logic-based explanations. Organized by HCMUT and TRNS-AI at IJCNN 2025, the challenge aimed to promote transparency using lightweight Large Language Models (LLMs). The dataset used logic-based templates validated with Z3 and refined through expert review to align with real-world academic scenarios. The paper outlines the challenge's motivation, structure, dataset construction, and evaluation protocol, highlighting the novel effort to blend LLMs and symbolic reasoning for explainability. The findings provide valuable insights for future XAI-centered educational systems and competitive research initiatives.

<br /><br />Summary: The XAI Challenge 2025 focused on developing QA systems for student queries in education using lightweight Large Language Models (LLMs) and logic-based explanations. The competition, held at IJCNN 2025, aimed to promote transparency and trustworthiness by bridging LLMs and symbolic reasoning. The dataset construction involved logic templates validated with Z3 and refined through expert review. The challenge's structure, motivation, and evaluation protocol aimed to advance XAI in educational contexts, offering actionable insights for future research initiatives. <div>
arXiv:2508.01263v1 Announce Type: new 
Abstract: The growing integration of Artificial Intelligence (AI) into education has intensified the need for transparency and interpretability. While hackathons have long served as agile environments for rapid AI prototyping, few have directly addressed eXplainable AI (XAI) in real-world educational contexts. This paper presents a comprehensive analysis of the XAI Challenge 2025, a hackathon-style competition jointly organized by Ho Chi Minh City University of Technology (HCMUT) and the International Workshop on Trustworthiness and Reliability in Neurosymbolic AI (TRNS-AI), held as part of the International Joint Conference on Neural Networks (IJCNN 2025). The challenge tasked participants with building Question-Answering (QA) systems capable of answering student queries about university policies while generating clear, logic-based natural language explanations. To promote transparency and trustworthiness, solutions were required to use lightweight Large Language Models (LLMs) or hybrid LLM-symbolic systems. A high-quality dataset was provided, constructed via logic-based templates with Z3 validation and refined through expert student review to ensure alignment with real-world academic scenarios. We describe the challenge's motivation, structure, dataset construction, and evaluation protocol. Situating the competition within the broader evolution of AI hackathons, we argue that it represents a novel effort to bridge LLMs and symbolic reasoning in service of explainability. Our findings offer actionable insights for future XAI-centered educational systems and competitive research initiatives.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities</title>
<link>https://arxiv.org/abs/2508.01290</link>
<guid>https://arxiv.org/abs/2508.01290</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Large Language Models, Knowledge Graphs, Question Answering, Unseen Entity KGQA

Summary:
Retrieval-Augmented Generation (RAG) utilizes explicit answer evidence, implicit answer clues, and partially relevant information to enhance Large Language Models (LLMs). This study explores the concept of awakening LLMs using partially relevant knowledge already embedded in them, rather than solely relying on external retrieval sources. The research investigates the impact of awakening on LLMs using triplets from gold reasoning paths with removed answer-containing paths. Theoretical analysis and experiments on Knowledge Graphs (KGs) Question Answering (QA) datasets support the effectiveness of this awakening-based approach. Additionally, a new task, Unseen Entity KGQA, is introduced to address challenges arising from incomplete knowledge base retrieval. The awakening-based method outperforms traditional embedding-based similarity approaches, particularly in real-world scenarios where entity linking fails due to incomplete KGs. 

<br /><br />Summary: 
1. RAG leverages various types of knowledge to enhance LLMs, including explicit evidence, implicit clues, and partially relevant information.
2. The awakening concept suggests utilizing partially relevant knowledge already embedded in LLMs for improved performance.
3. Experimental results and theoretical analysis support the effectiveness of awakening LLMs.
4. Introduction of the Unseen Entity KGQA task addresses challenges from incomplete knowledge base retrieval.
5. The awakening-based approach shows superiority over traditional methods in scenarios where entity linking fails due to incomplete KGs. <div>
arXiv:2508.01290v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) shows impressive performance by supplementing and substituting parametric knowledge in Large Language Models (LLMs). Retrieved knowledge can be divided into three types: explicit answer evidence, implicit answer clue, and insufficient answer context which can be further categorized into totally irrelevant and partially relevant information. Effectively utilizing partially relevant knowledge remains a key challenge for RAG systems, especially in incomplete knowledge base retrieval. Contrary to the conventional view, we propose a new perspective: LLMs can be awakened via partially relevant knowledge already embedded in LLMs. To comprehensively investigate this phenomenon, the triplets located in the gold reasoning path and their variants are used to construct partially relevant knowledge by removing the path that contains the answer. We provide theoretical analysis of the awakening effect in LLMs and support our hypothesis with experiments on two Knowledge Graphs (KGs) Question Answering (QA) datasets. Furthermore, we present a new task, Unseen Entity KGQA, simulating real-world challenges where entity linking fails due to KG incompleteness. Our awakening-based approach demonstrates greater efficacy in practical applications, outperforms traditional methods that rely on embedding-based similarity which are prone to returning noisy information.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference</title>
<link>https://arxiv.org/abs/2508.01302</link>
<guid>https://arxiv.org/abs/2508.01302</guid>
<content:encoded><![CDATA[
<div> alignment, knowledge editing, language models, augmentation, inference<br />
Summary:<br />
The paper introduces a new method, KEDAS, for knowledge editing in large language models (LLMs). KEDAS aligns LLMs with edited knowledge by utilizing low-rank adaptation in the alignment phase. It incorporates a diverse edit augmentation technique to enhance the recall of edits. A self-adaptive post-alignment inference mechanism is proposed, leveraging a smart retriever for dynamic selection of inference routing. KEDAS outperforms existing methods in 35 out of 36 cases across multiple datasets and LLM settings, showing superior performance in edit success, locality, and portability. It proves more efficient and robust while maintaining high performance levels in general tasks. KEDAS presents a promising approach to efficient knowledge editing alignment in language models. <br />Summary: <div>
arXiv:2508.01302v1 Announce Type: new 
Abstract: Knowledge editing aims to modify outdated knowledge in large language models (LLMs) efficiently while retaining their powerful capabilities. Most existing methods rely on either parameter-level editing or retrieval-based approaches. In this work, we propose Knowledge Editing alignment with Diverse Augmentation and Self-adaptive inference (KEDAS) to better align LLMs with knowledge editing. In the alignment phase, LLMs learn to apply in-context edited knowledge via low-rank adaptation. During editing, we design a diverse edit augmentation technique to improve the recall of edits. After that, a self-adaptive post-alignment inference mechanism is proposed, in which a filter-based smart retriever is employed to perform a dynamic selection of inference routing. Specifically, irrelevant queries will go through the original pre-alignment model directly, while relevant ones, together with their related edits, go through the model with aligned adapters activated. In experiments, KEDAS secures the highest overall performance scores in 35 out of 36 cases across four datasets with three LLMs on three settings, surpassing its strong knowledge editing alignment counterpart by about 19.8 harmonic mean scores of edit success, locality and portability and outperforming both parameter editing and retrieval-based baselines significantly. Analysis of computational cost and performance on general tasks further validates the robustness and efficiency of KEDAS, indicating that it presents an ideal paradigm of knowledge editing alignment.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation</title>
<link>https://arxiv.org/abs/2508.01309</link>
<guid>https://arxiv.org/abs/2508.01309</guid>
<content:encoded><![CDATA[
<div> Prompt engineering, large language models, question-answering datasets, domain-specific fine-tuning, D-SCoRE <br />
Summary: D-SCoRE introduces a training-free pipeline that leverages large language models (LLMs) and prompt engineering to create high-quality question-answering datasets for domain-specific fine-tuning. This pipeline, integrating Document-centric processing, Segmentation, CoT Reasoning, and structured Export, generates QA datasets suited for supervised fine-tuning. Various control mechanisms enhance diversity and relevance by incorporating semantic role transformation, question type balancing, and counterfactual materials. Evaluations show that LLMs fine-tuned on D-SCoRE-generated datasets outperform those trained on human-annotated datasets across different domains. D-SCoRE efficiently generates QA-CoT pairs with counterfactual materials within a short time frame, showcasing simplicity and scalability in QA generation and fine-tuning processes. This approach allows for high-performance fine-tuning of LLMs in various domains, addressing the scarcity and cost issues of high-quality QA datasets. <br /><br />Summary: <div>
arXiv:2508.01309v1 Announce Type: new 
Abstract: The scarcity and high cost of high-quality question-answering (QA) datasets hinder supervised fine-tuning (SFT) for domain-specific large language models (LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that utilizes LLMs and prompt engineering to produce diverse, high-quality QA datasets from arbitrary textual sources. D-SCoRE integrates $\textbf{D}$ocument-centric processing, $\textbf{S}$egmentation, $\textbf{Co}$T $\textbf{R}$easoning, and structured $\textbf{E}$xport to generate QA-COT datasets tailored for domain-aware SFT. Multi-dimensional control mechanisms, such as semantic role transformation, question type balancing, and counterfactual materials, enhance diversity and relevance, overcoming limitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA datasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on SQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most domains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual materials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade hardware. Its simplicity and scalability enable efficient QA generation and high-performance fine-tuning across domains.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points</title>
<link>https://arxiv.org/abs/2508.01317</link>
<guid>https://arxiv.org/abs/2508.01317</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, knowledge point graph, QA data, synthesis framework, LinkQA

Summary:
The article introduces LinkSyn, a framework for synthesizing diverse question-answering (QA) data to enhance training for large language models (LLMs). LinkSyn utilizes a knowledge point graph to extract KPs from QA seed data and synthesize QA data from multiple sources while balancing coverage and popularity of KPs. By incorporating a knowledge distribution value function, diffusion-based synthesis, and difficulty adjustment mechanisms, LinkSyn creates LinkQA, a multi-disciplinary QA dataset with 50 billion tokens. Experiment results on the Llama-3 8B model show that pre-training with LinkQA leads to significant improvements in MMLU and CMMLU metrics. This approach consistently boosts model performance across different scales and demonstrates state-of-the-art results in large language model advancement.

<br /><br />Summary: <div>
arXiv:2508.01317v1 Announce Type: new 
Abstract: The advancement of large language models (LLMs) struggles with the scarcity of high-quality, diverse training data. To address this limitation, we propose LinkSyn, a novel knowledge point (KP) graph-based synthesis framework that enables flexible control over discipline and difficulty distributions while balancing KP coverage and popularity. LinkSyn extracts KPs from question-answering (QA) seed data and constructs a KP graph to synthesize diverse QA data from multiple seeds strongly linked by KPs and sampled from graph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution value function to guide the adjustment of path sampling probability and balance KP coverage and popularity during graph walks; (2) diffusion-based synthesis via DeepSeek-R1 by leveraging multiple seeds with dense logical associations along each path; and (3) high-difficulty QA enhancement within given disciplines by flexible difficulty adjustments. By executing LinkSyn, we synthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens. Extensive experiments on Llama-3 8B demonstrate that continual pre-training with LinkQA yields an average improvement of $\mathbf{11.51\%}$ on MMLU and CMMLU, establishing new SOTA results. LinkQA consistently enhances performance across model size and initial FLOPs scales.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Diverse Synthesis for Mid-Training</title>
<link>https://arxiv.org/abs/2508.01326</link>
<guid>https://arxiv.org/abs/2508.01326</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, QA dataset, knowledge diversity, STEM disciplines, data synthesis

Summary:
BoostQA is a novel pipeline for synthesizing a large-scale QA dataset by curating seed data from various sources and implementing STEM-focused multi-grade synthesis to enhance data diversity and high-difficulty synthesis to mitigate difficulty degradation. The dataset is refined using DeepSeek-V3 to improve output quality. BoostQA is utilized in mid-training to optimize domain-specific knowledge acquisition and enhance data quality. The methodology enables mid-trained Llama-3 8B to achieve a significant average improvement of 12.74% on MMLU and CMMLU and establishes state-of-the-art performance across 12 benchmarks. BoostQA demonstrates robust scalability, with performance consistently improving with increased model size, data volume, and initial FLOPs scaling.<br /><br />Summary: BoostQA is a pipeline for synthesizing a large-scale QA dataset with diverse knowledge and high difficulty data synthesis, refined using DeepSeek-V3. It enhances mid-training for Llama-3 8B, resulting in significant performance improvements and demonstrating scalability across various metrics. <div>
arXiv:2508.01326v1 Announce Type: new 
Abstract: The scarcity of high-quality, knowledge-intensive training data hinders the development of large language models (LLMs), as traditional corpora provide limited information. Previous studies have synthesized and integrated corpora-dependent question-answering (QA) data to improve model performance but face challenges in QA data scalability and knowledge diversity, particularly in cross-domain contexts. Furthermore, leveraging our designed discipline and difficulty annotation system, we probe model deficiencies in STEM disciplines and high-difficulty data. To overcome these limitations, we propose a novel diversified pipeline to synthesize BoostQA, a 100B-token large-scale QA dataset. Our synthesis framework: (1) curates seed data from heterogeneous sources; (2) utilizes DeepSeek-R1 to implement STEM-focused multi-grade synthesis to boost data diversity and high-difficulty synthesis to mitigate difficulty degradation; (3) refines answers via DeepSeek-V3 to improve output quality. We utilize BoostQA in mid-training, a mid-stage between pre-training and post-training, to optimize domain-specific knowledge acquisition and enhance data quality. Our method enables Llama-3 8B, mid-trained on a 40B-token dataset, to achieve an average improvement of $\mathbf{12.74\%}$ on MMLU and CMMLU and establish SOTA average performance across 12 benchmarks. BoostQA also demonstrates robust scalability, with performance consistently improving as model size, data volume, and initial FLOPs scale.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis</title>
<link>https://arxiv.org/abs/2508.01370</link>
<guid>https://arxiv.org/abs/2508.01370</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Business Analysis, Market Report Generation, Autonomous Framework, Automated Review Cycles

Summary:
An autonomous framework utilizing Large Language Models (LLMs) has been developed to automate end-to-end business analysis and market report generation. The system consists of specialized agents - Researcher, Reviewer, Writer, and Retriever - working together to analyze data and generate comprehensive reports. By learning from real professional consultants' materials, the framework replicates professional analytical methodologies. A multi-step process including querying databases, analyzing data, generating insights, creating visualizations, and composing reports is executed. A novel LLM-based evaluation system assesses report quality, showing alignment with expert evaluations. An iterative improvement mechanism optimizes report quality through automated review cycles and consultants' knowledge. Experimental results demonstrate the framework can generate detailed reports in minutes at a low cost. This work represents a significant advancement in automatically creating affordable market insights.<br /><br />Summary: <div>
arXiv:2508.01370v1 Announce Type: new 
Abstract: We present an autonomous framework that leverages Large Language Models (LLMs) to automate end-to-end business analysis and market report generation. At its core, the system employs specialized agents - Researcher, Reviewer, Writer, and Retriever - that collaborate to analyze data and produce comprehensive reports. These agents learn from real professional consultants' presentation materials at Amazon through in-context learning to replicate professional analytical methodologies. The framework executes a multi-step process: querying databases, analyzing data, generating insights, creating visualizations, and composing market reports. We also introduce a novel LLM-based evaluation system for assessing report quality, which shows alignment with expert human evaluations. Building on these evaluations, we implement an iterative improvement mechanism that optimizes report quality through automated review cycles. Experimental results show that report quality can be improved by both automated review cycles and consultants' unstructured knowledge. In experimental validation, our framework generates detailed 6-page reports in 7 minutes at a cost of approximately \$1. Our work could be an important step to automatically create affordable market insights.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs</title>
<link>https://arxiv.org/abs/2508.01401</link>
<guid>https://arxiv.org/abs/2508.01401</guid>
<content:encoded><![CDATA[
<div> Keywords: Physicians, medical documentation, automation tools, dataset, Dialog-to-Note task

Summary:
Physicians face challenges in documenting clinical encounters leading to professional burnout. The MedSynth dataset introduces synthetic medical dialogues and notes to improve the Dialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks. With over 10,000 dialogue-note pairs covering 2000 ICD-10 codes, this dataset enhances models' performance in generating medical notes from dialogues and vice versa. By analyzing disease distributions, the dataset provides diverse and privacy-compliant training data. The availability of code and dataset on GitHub and Hugging Face platforms offers open-access resources in a field lacking such resources.<br /><br />Summary: <div>
arXiv:2508.01401v1 Announce Type: new 
Abstract: Physicians spend significant time documenting clinical encounters, a burden that contributes to professional burnout. To address this, robust automation tools for medical documentation are crucial. We introduce MedSynth -- a novel dataset of synthetic medical dialogues and notes designed to advance the Dialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks. Informed by an extensive analysis of disease distributions, this dataset includes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We demonstrate that our dataset markedly enhances the performance of models in generating medical notes from dialogues, and dialogues from medical notes. The dataset provides a valuable resource in a field where open-access, privacy-compliant, and diverse training data are scarce. Code is available at https://github.com/ahmadrezarm/MedSynth/tree/main and the dataset is available at https://huggingface.co/datasets/Ahmad0067/MedSynth.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations</title>
<link>https://arxiv.org/abs/2508.01411</link>
<guid>https://arxiv.org/abs/2508.01411</guid>
<content:encoded><![CDATA[
<div> Keywords: Egyptian Arabic, parallel dataset, machine translation, translation studies, pedagogical purposes

Summary:
The arXiv article introduces the ArzEn-MultiGenre dataset, which consists of parallel Egyptian Arabic song lyrics, novels, and TV show subtitles with their English translations. With 25,557 segment pairs, the dataset can be used for benchmarking new machine translation models, fine-tuning language models, and improving commercial translation applications like Google Translate. The dataset's value extends to research in translation studies, cross-linguistic analysis, and lexical semantics. Additionally, it can benefit translation students and professional translators as a training and memory resource. The dataset stands out for its inclusion of diverse textual genres not found in existing datasets and for its high-quality human expert translations and alignments.

<br /><br />Summary: <div>
arXiv:2508.01411v1 Announce Type: new 
Abstract: ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics, novels, and TV show subtitles that are manually translated and aligned with their English counterparts. The dataset contains 25,557 segment pairs that can be used to benchmark new machine translation models, fine-tune large language models in few-shot settings, and adapt commercial machine translation applications such as Google Translate. Additionally, the dataset is a valuable resource for research in various disciplines, including translation studies, cross-linguistic analysis, and lexical semantics. The dataset can also serve pedagogical purposes by training translation students and aid professional translators as a translation memory. The contributions are twofold: first, the dataset features textual genres not found in existing parallel Egyptian Arabic and English datasets, and second, it is a gold-standard dataset that has been translated and aligned by human experts.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Bias Associations through Open-Ended LLM Generations</title>
<link>https://arxiv.org/abs/2508.01412</link>
<guid>https://arxiv.org/abs/2508.01412</guid>
<content:encoded><![CDATA[
<div> Framework, Bias Association, Large Language Models, Social Biases, Evaluation Methods

Summary: 
The Bias Association Discovery Framework (BADF) is introduced to tackle social biases embedded in Large Language Models (LLMs). These biases can lead to unfair or distorted representations of demographic groups, impacting the generated language. Existing evaluation methods have limitations in identifying new or unexpected biases. BADF systematically extracts known and previously unnoticed associations between demographic identities and descriptive concepts from open-ended LLM outputs. Through extensive experiments across various models and real-world scenarios, BADF allows for robust mapping and analysis of concepts associated with demographic identities. This framework enhances the understanding of biases in open-ended text generation and offers a scalable tool for detecting and examining bias associations in LLMs. The data, code, and results of the study are accessible on GitHub at https://github.com/JP-25/Discover-Open-Ended-Generation. 

<br /><br />Summary: <div>
arXiv:2508.01412v1 Announce Type: new 
Abstract: Social biases embedded in Large Language Models (LLMs) raise critical concerns, resulting in representational harms -- unfair or distorted portrayals of demographic groups -- that may be expressed in subtle ways through generated language. Existing evaluation methods often depend on predefined identity-concept associations, limiting their ability to surface new or unexpected forms of bias. In this work, we present the Bias Association Discovery Framework (BADF), a systematic approach for extracting both known and previously unrecognized associations between demographic identities and descriptive concepts from open-ended LLM outputs. Through comprehensive experiments spanning multiple models and diverse real-world contexts, BADF enables robust mapping and analysis of the varied concepts that characterize demographic identities. Our findings advance the understanding of biases in open-ended generation and provide a scalable tool for identifying and analyzing bias associations in LLMs. Data, code, and results are available at https://github.com/JP-25/Discover-Open-Ended-Generation
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2508.01424</link>
<guid>https://arxiv.org/abs/2508.01424</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, complex multi-hop question answering, ontology-driven reasoning, knowledge graphs, logical reasoning.

Summary:
ORACLE (Ontology-driven Reasoning And Chain for Logical Elucidation) is a training-free framework that addresses the limitations of Large Language Models (LLMs) in complex multi-hop question answering tasks by combining generative capabilities with knowledge graphs. The framework dynamically constructs question-specific knowledge ontologies using LLMs, transforms them into First-Order Logic reasoning chains, and decomposes the query into logically coherent sub-questions. Experimental results on standard benchmarks show competitive performance comparable to state-of-the-art models like DeepSeek-R1. Detailed analyses confirm the effectiveness of each component and demonstrate that ORACLE generates more logical and interpretable reasoning chains than existing approaches.<br /><br />Summary: ORACLE is a framework that enhances the reasoning capabilities of LLMs by combining them with knowledge graphs. It constructs ontologies, transforms them into logical chains, and decomposes queries for improved multi-hop question answering performance. Experimental results show ORACLE's competitiveness and effectiveness in generating logical and interpretable reasoning chains. <div>
arXiv:2508.01424v1 Announce Type: new 
Abstract: Large Language Models (LLMs), despite their success in question answering, exhibit limitations in complex multi-hop question answering (MQA) tasks that necessitate non-linear, structured reasoning. This limitation stems from their inability to adequately capture deep conceptual relationships between entities. To overcome this challenge, we present **ORACLE** (**O**ntology-driven **R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a training-free framework that combines LLMs' generative capabilities with the structural benefits of knowledge graphs. Our approach operates through three stages: (1) dynamic construction of question-specific knowledge ontologies using LLMs, (2) transformation of these ontologies into First-Order Logic reasoning chains, and (3) systematic decomposition of the original query into logically coherent sub-questions. Experimental results on several standard MQA benchmarks show that our framework achieves highly competitive performance, rivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses further confirm the effectiveness of each component, while demonstrating that our method generates more logical and interpretable reasoning chains than existing approaches.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data</title>
<link>https://arxiv.org/abs/2508.01450</link>
<guid>https://arxiv.org/abs/2508.01450</guid>
<content:encoded><![CDATA[
<div> fine-tuning, language models, data selection, medical reasoning, gradient influence<br />
Summary:<br />
Supervised Fine-Tuning (SFT) for Large Language Models (LLMs) in specialized domains like medical reasoning often suffers from using unfiltered datasets leading to computational costs and suboptimal performance. A new data selection strategy, Difficulty-Influence Quadrant (DIQ), prioritizes samples with high difficulty and high influence, balancing reasoning complexity with gradient impact. DIQ-selected subsets show higher data quality and generate expert-like clinical reasoning. Experiments demonstrate that models fine-tuned on only 1% of selected data match full-dataset performance, while using 10% consistently outperforms the baseline. This highlights the effectiveness of principled data selection over brute-force scaling.<br /> <div>
arXiv:2508.01450v1 Announce Type: new 
Abstract: Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language Models (LLMs) to specialized domains such as medical reasoning. However, existing SFT practices often rely on unfiltered datasets that contain redundant and low-quality samples, leading to substantial computational costs and suboptimal performance. Although existing methods attempt to alleviate this problem by selecting data based on sample difficulty, defined by knowledge and reasoning complexity, they overlook each sample's optimization utility reflected in its gradient. Interestingly, we find that gradient-based influence alone favors easy-to-optimize samples that cause large parameter shifts but lack deep reasoning chains, while difficulty alone selects noisy or overly complex cases that fail to guide stable optimization. Based on this observation, we propose a data selection strategy, Difficulty-Influence Quadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence quadrant to balance complex clinical reasoning with substantial gradient influence, enabling efficient medical reasoning with minimal fine-tuning data. Furthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected subsets demonstrate higher data quality and generate clinical reasoning that is more aligned with expert practices in differential diagnosis, safety check, and evidence citation, as DIQ emphasizes samples that foster expert-like reasoning patterns. Extensive experiments on medical reasoning benchmarks demonstrate that DIQ enables models fine-tuned on only 1% of selected data to match full-dataset performance, while using 10% consistently outperforms the baseline, highlighting the superiority of principled data selection over brute-force scaling. The code and data are available at https://github.com/mihara-bot/DIQ.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreeDiff: AST-Guided Code Generation with Diffusion LLMs</title>
<link>https://arxiv.org/abs/2508.01473</link>
<guid>https://arxiv.org/abs/2508.01473</guid>
<content:encoded><![CDATA[
<div> diffusion-based language models, sequence generation, source code, syntax-aware corruption, Abstract Syntax Trees (ASTs) 
Summary: 
Syntax-aware diffusion framework proposed for structured domains, such as source code, by incorporating structural priors from ASTs into denoising process. Standard token-level corruption techniques not suitable for code due to strict syntax and semantics. Selective corruption of syntactically meaningful code spans from AST subtrees improves reconstruction accuracy, syntactic correctness, and generalization on unseen code patterns. Model reconstructs programs respecting grammatical boundaries and capturing long-range dependencies. Syntax-guided denoising enhances diffusion-based language models for code generation tasks. <div>
arXiv:2508.01473v1 Announce Type: new 
Abstract: Recent advances in diffusion-based language models have opened new possibilities for controllable and bidirectional sequence generation. These models provide an alternative to traditional autoregressive approaches by framing text generation as an iterative denoising process. However, applying diffusion models to structured domains such as source code remains a significant challenge. Programming languages differ from natural language in that they follow strict syntactic and semantic rules, with hierarchical organization that must be preserved for correctness. Standard token-level corruption techniques used during training often ignore this structure, which may hinder the model's ability to learn meaningful representations of code. To address this limitation, we propose a syntax-aware diffusion framework that incorporates structural priors from Abstract Syntax Trees (ASTs) into the denoising process. Instead of masking individual tokens at random, we selectively corrupt syntactically meaningful code spans derived from AST subtrees. This enables the model to reconstruct programs in a way that respects grammatical boundaries and captures long-range dependencies. Experimental results demonstrate that syntax-aware corruption significantly improves syntactic correctness, reconstruction accuracy, and generalization to unseen code patterns. These findings highlight the potential of incorporating structural information into diffusion-based training and suggest that syntax-guided denoising is a promising direction for advancing diffusion-based language models in code generation tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach</title>
<link>https://arxiv.org/abs/2508.01480</link>
<guid>https://arxiv.org/abs/2508.01480</guid>
<content:encoded><![CDATA[
<div> Biomedical text mining, question-answering, BioASQ challenge, large language models, semantic question-answering <br />
<br />
Summary: 
This study focuses on biomedical question-answering in the BioASQ challenge, utilizing open-source large language models (LLMs). Multiple models are used in a majority voting system for Yes/No questions and their union for list and factoid type questions. Evaluation of 13 LLMs reveals optimal combinations for specific question types. The system achieved noteworthy results in the 2025 BioASQ challenge, securing top rankings for ideal and exact answers in the Synergy task across multiple rounds. <div>
arXiv:2508.01480v1 Announce Type: new 
Abstract: Biomedical text mining and question-answering are essential yet highly demanding tasks, particularly in the face of the exponential growth of biomedical literature. In this work, we present our participation in the 13th edition of the BioASQ challenge, which involves biomedical semantic question-answering for Task 13b and biomedical question-answering for developing topics for the Synergy task. We deploy a selection of open-source large language models (LLMs) as retrieval-augmented generators to answer biomedical questions. Various models are used to process the questions. A majority voting system combines their output to determine the final answer for Yes/No questions, while for list and factoid type questions, the union of their answers in used. We evaluated 13 state-of-the-art open source LLMs, exploring all possible model combinations to contribute to the final answer, resulting in tailored LLM pipelines for each question type. Our findings provide valuable insight into which combinations of LLMs consistently produce superior results for specific question types. In the four rounds of the 2025 BioASQ challenge, our system achieved notable results: in the Synergy task, we secured 1st place for ideal answers and 2nd place for exact answers in round 2, as well as two shared 1st places for exact answers in round 3 and 4.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu</title>
<link>https://arxiv.org/abs/2508.01486</link>
<guid>https://arxiv.org/abs/2508.01486</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, sentiment classification, explainability, fairness, Telugu

Summary:<br />
- TeSent is a benchmark dataset for sentiment classification in Telugu, addressing the underrepresentation of the language in NLP and Machine Learning.
- It consists of 26,150 sentences scraped from social media platforms and news websites, with ground truth labels and human-annotated rationales.
- The dataset includes provisions for evaluating explainability and fairness, crucial in modern machine learning tasks.
- Fine-tuning SOTA pre-trained models with and without rationales showed potential for accuracy improvement and bias reduction.
- The plausibility and faithfulness evaluation suite provides insights into the alignment of model explainers with human reasoning.<br /><br />Summary: <div>
arXiv:2508.01486v1 Announce Type: new 
Abstract: In the Indian subcontinent, Telugu, one of India's six classical languages, is the most widely spoken Dravidian Language. Despite its 96 million speaker base worldwide, Telugu remains underrepresented in the global NLP and Machine Learning landscape, mainly due to lack of high-quality annotated resources. This work introduces TeSent, a comprehensive benchmark dataset for sentiment classification, a key text classification problem, in Telugu. TeSent not only provides ground truth labels for the sentences, but also supplements with provisions for evaluating explainability and fairness, two critical requirements in modern-day machine learning tasks. We scraped Telugu texts covering multiple domains from various social media platforms, news websites and web-blogs to preprocess and generate 26,150 sentences, and developed a custom-built annotation platform and a carefully crafted annotation protocol for collecting the ground truth labels along with their human-annotated rationales. We then fine-tuned several SOTA pre-trained models in two ways: with rationales, and without rationales. Further, we provide a detailed plausibility and faithfulness evaluation suite, which exploits the rationales, for six widely used post-hoc explainers applied on the trained models. Lastly, we curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate fairness of Telugu sentiment and emotion related NLP tasks, and provide a fairness evaluation suite for the trained classifier models. Our experimental results suggest that training with rationales may improve model accuracy, reduce bias in models, and make the explainers' output more aligned to human reasoning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Homogenizing Effect of Large Language Models on Human Expression and Thought</title>
<link>https://arxiv.org/abs/2508.01491</link>
<guid>https://arxiv.org/abs/2508.01491</guid>
<content:encoded><![CDATA[
<div> Keywords: Cognitive diversity, Language models, Collective intelligence, Homogenization, Marginalization  

Summary: Large language models (LLMs) play a crucial role in shaping language and reasoning patterns in society. However, there is a concern that as LLMs become more prevalent, they may standardize language and reasoning processes, potentially marginalizing alternative voices and cognitive strategies. This review discusses how LLMs reflect and reinforce dominant styles while diminishing diversity in language and reasoning. The design and widespread use of LLMs contribute to this homogenization by replicating patterns from their training data and promoting a convergence of language and thinking across different contexts. If left unchecked, this trend could lead to the flattening of cognitive landscapes that drive collective intelligence and adaptability. It is important to consider the potential consequences of this homogenization on creativity and diversity in society. 

<br /><br />Summary: Cognitive diversity is essential for collective intelligence, but large language models (LLMs) may standardize language and reasoning, marginalizing alternative voices and strategies. The design and use of LLMs mirror dominant styles, amplifying convergence in language and thinking. This homogenization poses a risk to cognitive landscapes that foster adaptability and collective intelligence. <div>
arXiv:2508.01491v1 Announce Type: new 
Abstract: Cognitive diversity, reflected in variations of language, perspective, and reasoning, is essential to creativity and collective intelligence. This diversity is rich and grounded in culture, history, and individual experience. Yet as large language models (LLMs) become deeply embedded in people's lives, they risk standardizing language and reasoning. This Review synthesizes evidence across linguistics, cognitive, and computer science to show how LLMs reflect and reinforce dominant styles while marginalizing alternative voices and reasoning strategies. We examine how their design and widespread use contribute to this effect by mirroring patterns in their training data and amplifying convergence as all people increasingly rely on the same models across contexts. Unchecked, this homogenization risks flattening the cognitive landscapes that drive collective intelligence and adaptability.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents</title>
<link>https://arxiv.org/abs/2508.01503</link>
<guid>https://arxiv.org/abs/2508.01503</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, pedagogical agents, Evidence-Centered Design, Social Cognitive Theory, STEM+C learning

Summary:<br /><br />Large language models (LLMs) such as ChatGPT can enhance student learning through dialogue, but often lack theoretical foundations. To address this, a framework combining Evidence-Centered Design and Social Cognitive Theory is proposed for adaptive scaffolding in LLM-based agents focused on STEM+C learning. Illustrated with the Inquizzitor agent, this framework integrates human-AI hybrid intelligence to offer feedback grounded in cognitive science principles. Findings demonstrate Inquizzitor's ability to provide high-quality assessment aligned with core learning theories, aiding teachers in offering effective guidance valued by students. This research showcases the potential for theory-driven integration of LLMs in education, illustrating their capacity to deliver adaptive and principled instruction. <div>
arXiv:2508.01503v1 Announce Type: new 
Abstract: Large language models (LLMs) present new opportunities for creating pedagogical agents that engage in meaningful dialogue to support student learning. However, the current use of LLM systems like ChatGPT in classrooms often lacks the solid theoretical foundation found in earlier intelligent tutoring systems. To bridge this gap, we propose a framework that combines Evidence-Centered Design with Social Cognitive Theory for adaptive scaffolding in LLM-based agents focused on STEM+C learning. We illustrate this framework with Inquizzitor, an LLM-based formative assessment agent that integrates human-AI hybrid intelligence and provides feedback grounded in cognitive science principles. Our findings show that Inquizzitor delivers high-quality assessment and interaction aligned with core learning theories, offering teachers effective guidance that students value. This research underscores the potential for theory-driven LLM integration in education, highlighting the ability of these systems to provide adaptive and principled instruction.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization</title>
<link>https://arxiv.org/abs/2508.01541</link>
<guid>https://arxiv.org/abs/2508.01541</guid>
<content:encoded><![CDATA[
<div> Keywords: Prompt engineering, Large Language Models, Multi-objective Evolutionary Optimization, Sentiment analysis, Portuguese

Summary: 
The paper introduces MOPrompt, a Multi-objective Evolutionary Optimization framework that optimizes prompts for both accuracy and context size simultaneously. Existing automated methods often focus on a single objective, neglecting the trade-off between task performance and context size. MOPrompt maps the Pareto front of prompt solutions, providing practitioners with trade-offs between context size and performance. Evaluation on a sentiment analysis task in Portuguese showed MOPrompt outperforming baseline frameworks. For the Sabiazinho model, MOPrompt identified a prompt achieving the same peak accuracy as the best baseline solution but with a 31% reduction in token length. This framework offers a crucial tool for deploying Large Language Models in real-world applications, addressing the challenge of balancing efficiency and effectiveness in prompt optimization.<br /><br />Summary: <div>
arXiv:2508.01541v1 Announce Type: new 
Abstract: Prompt engineering is crucial for unlocking the potential of Large Language Models (LLMs). Still, since manual prompt design is often complex, non-intuitive, and time-consuming, automatic prompt optimization has emerged as a research area. However, a significant challenge in prompt optimization is managing the inherent trade-off between task performance, such as accuracy, and context size. Most existing automated methods focus on a single objective, typically performance, thereby failing to explore the critical spectrum of efficiency and effectiveness. This paper introduces the MOPrompt, a novel Multi-objective Evolutionary Optimization (EMO) framework designed to optimize prompts for both accuracy and context size (measured in tokens) simultaneously. Our framework maps the Pareto front of prompt solutions, presenting practitioners with a set of trade-offs between context size and performance, a crucial tool for deploying Large Language Models (LLMs) in real-world applications. We evaluate MOPrompt on a sentiment analysis task in Portuguese, using Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that MOPrompt substantially outperforms the baseline framework. For the Sabiazinho model, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97) as the best baseline solution, but with a 31% reduction in token length.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models</title>
<link>https://arxiv.org/abs/2508.01554</link>
<guid>https://arxiv.org/abs/2508.01554</guid>
<content:encoded><![CDATA[
<div> Framework, Adversarial attacks, Language models, PromptAnatomy, ComPerturb  
Summary:  
A new automated framework called PromptAnatomy is introduced, which dissects prompts into functional components to generate diverse and interpretable adversarial examples. The framework utilizes a method called ComPerturb to selectively perturb each component of the prompt. It also incorporates a perplexity-based filtering mechanism to ensure linguistic plausibility and mitigate distribution shifts. The study highlights the importance of understanding prompt structure and controlled perturbation for evaluating the robustness of large language models. Extensive experiments with advanced language models demonstrate that ComPerturb achieves state-of-the-art attack success rates. Annotated public instruction-tuning datasets verified through human review are provided as a complementary resource. Ablation studies confirm the benefits of prompt dissection and perplexity filtering in evaluating adversarial robustness in language models. The code and data for PromptAnatomy are available on GitHub for further exploration. 
<br /><br />Summary: <div>
arXiv:2508.01554v1 Announce Type: new 
Abstract: Prompt-based adversarial attacks have become an effective means to assess the robustness of large language models (LLMs). However, existing approaches often treat prompts as monolithic text, overlooking their structural heterogeneity-different prompt components contribute unequally to adversarial robustness. Prior works like PromptRobust assume prompts are value-neutral, but our analysis reveals that complex, domain-specific prompts with rich structures have components with differing vulnerabilities. To address this gap, we introduce PromptAnatomy, an automated framework that dissects prompts into functional components and generates diverse, interpretable adversarial examples by selectively perturbing each component using our proposed method, ComPerturb. To ensure linguistic plausibility and mitigate distribution shifts, we further incorporate a perplexity (PPL)-based filtering mechanism. As a complementary resource, we annotate four public instruction-tuning datasets using the PromptAnatomy framework, verified through human review. Extensive experiments across these datasets and five advanced LLMs demonstrate that ComPerturb achieves state-of-the-art attack success rates. Ablation studies validate the complementary benefits of prompt dissection and PPL filtering. Our results underscore the importance of prompt structure awareness and controlled perturbation for reliable adversarial robustness evaluation in LLMs. Code and data are available at https://github.com/Yujiaaaaa/PACP.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets</title>
<link>https://arxiv.org/abs/2508.01630</link>
<guid>https://arxiv.org/abs/2508.01630</guid>
<content:encoded><![CDATA[
<div> transformer models, NER, domain-adapted, LoRA, OpenMed <br />
Summary: <br />
The article introduces OpenMed NER, a suite of open-source transformer models for named-entity recognition in healthcare data. These models combine domain-adaptive pre-training with Low-Rank Adaptation for efficient performance. OpenMed NER achieves state-of-the-art results on 10 out of 12 biomedical NER benchmarks, including improvements in diverse entity types such as diseases, genes, and species. The models show significant advancements in disease and chemical benchmarks, as well as specialized gene and clinical cell line corpora. The training process is completed in under 12 hours on a single GPU with low carbon footprint, making it efficient and eco-friendly. The open-source checkpoints provided can aid practitioners in complying with data protection and AI regulations like the EU AI Act. <div>
arXiv:2508.01630v1 Announce Type: new 
Abstract: Named-entity recognition (NER) is fundamental to extracting structured information from the >80% of healthcare data that resides in unstructured clinical notes and biomedical literature. Despite recent advances with large language models, achieving state-of-the-art performance across diverse entity types while maintaining computational efficiency remains a significant challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted transformer models that combine lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced, publicly available research repositories and de-identified clinical notes (PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA backbones. This is followed by task-specific fine-tuning with LoRA, which updates less than 1.5% of model parameters. We evaluate our models on 12 established biomedical NER benchmarks spanning chemicals, diseases, genes, and species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of these 12 datasets, with substantial gains across diverse entity types. Our models advance the state-of-the-art on foundational disease and chemical benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger improvements of over 5.3 and 9.7 percentage points on more specialized gene and clinical cell line corpora. This work demonstrates that strategically adapted open-source models can surpass closed-source solutions. This performance is achieved with remarkable efficiency: training completes in under 12 hours on a single GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively licensed, open-source checkpoints designed to help practitioners facilitate compliance with emerging data protection and AI regulations, such as the EU AI Act.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Authorship Attribution in Multilingual Machine-Generated Texts</title>
<link>https://arxiv.org/abs/2508.01656</link>
<guid>https://arxiv.org/abs/2508.01656</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, authorship attribution, multilingual, text generation, cross-lingual transferability 

Summary:<br /><br />Large Language Models (LLMs) have advanced to the point where distinguishing between machine-generated text and human-written content is becoming increasingly difficult. This study introduces the concept of Multilingual Authorship Attribution, focusing on attributing texts to human or various LLM generators across 18 languages. The research examines the suitability of monolingual authorship attribution methods in a multilingual context, the transferability of these methods across languages, and the impact of different generators on attribution accuracy. Results indicate that while some monolingual methods can be adapted for multilingual use, challenges persist, especially in transferring across diverse language families. This highlights the complexity of multilingual authorship attribution and the need for more robust approaches to address real-world scenarios. <div>
arXiv:2508.01656v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) have reached human-like fluency and coherence, distinguishing machine-generated text (MGT) from human-written content becomes increasingly difficult. While early efforts in MGT detection have focused on binary classification, the growing landscape and diversity of LLMs require a more fine-grained yet challenging authorship attribution (AA), i.e., being able to identify the precise generator (LLM or human) behind a text. However, AA remains nowadays confined to a monolingual setting, with English being the most investigated one, overlooking the multilingual nature and usage of modern LLMs. In this work, we introduce the problem of Multilingual Authorship Attribution, which involves attributing texts to human or multiple LLM generators across diverse languages. Focusing on 18 languages -- covering multiple families and writing scripts -- and 8 generators (7 LLMs and the human-authored class), we investigate the multilingual suitability of monolingual AA methods, their cross-lingual transferability, and the impact of generators on attribution performance. Our results reveal that while certain monolingual AA methods can be adapted to multilingual settings, significant limitations and challenges remain, particularly in transferring across diverse language families, underscoring the complexity of multilingual AA and the need for more robust approaches to better match real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions</title>
<link>https://arxiv.org/abs/2508.01674</link>
<guid>https://arxiv.org/abs/2508.01674</guid>
<content:encoded><![CDATA[
<div> benchmark, large language models, personalization, user preferences, context

Summary:
The article introduces a new benchmark called CUPID, which consists of 756 human-curated interaction session histories between users and LLM-based chat assistants. The benchmark aims to assess the ability of LLMs to infer and apply users' dynamic preferences based on context. The study evaluated 10 LLMs and found that they struggle to accurately infer preferences from multi-turn interactions, achieving less than 50% precision and 65% recall. This highlights the need for advancements in LLM capabilities to enable more contextually personalized interactions. CUPID serves as a resource to drive these improvements and emphasizes the importance of aligning LLM responses with users' changing preferences in different contexts. <br /><br />Summary: <div>
arXiv:2508.01674v1 Announce Type: new 
Abstract: Personalization of Large Language Models (LLMs) often assumes users hold static preferences that reflect globally in all tasks. In reality, humans hold dynamic preferences that change depending on the context. As users interact with an LLM in various contexts, they naturally reveal their contextual preferences, which a model must infer and apply in future contexts to ensure alignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated interaction session histories between users and LLM-based chat assistants. In each interaction session, the user provides a request in a specific context and expresses their preference through multi-turn feedback. Given a new user request and prior interaction sessions, our benchmark assesses whether LLMs can infer the preference relevant to this request and generate a response that satisfies this preference. With CUPID, we evaluated 10 open and proprietary LLMs, revealing that state-of-the-art LLMs struggle to infer preferences from multi-turn interactions and fail to discern what previous context is relevant to a new request -- under 50% precision and 65% recall. Our work highlights the need to advance LLM capabilities for more contextually personalized interactions and proposes CUPID as a resource to drive these improvements.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Bidirectional Process Reward Model</title>
<link>https://arxiv.org/abs/2508.01682</link>
<guid>https://arxiv.org/abs/2508.01682</guid>
<content:encoded><![CDATA[
<div> Keywords: Process Reward Models, Bidirectional evaluation paradigm, mathematical reasoning benchmarks, stepwise reward evaluation, process-based reward modeling

Summary: 
Process Reward Models (PRMs) aim to enhance the reasoning quality of Large Language Models by assigning scores to intermediate steps. However, existing PRMs have limitations in leveraging global context. In response, a Bidirectional Process Reward Model (BiPRM) is proposed. BiPRM incorporates a bidirectional evaluation paradigm, allowing later reasoning steps to assess earlier ones in real time. The bidirectional evaluation is achieved through prompt modifications without additional parameters or latency. Experimental results on mathematical reasoning benchmarks demonstrate BiPRM's superiority over unidirectional baselines, with a significant improvement in stepwise reward evaluation. The findings indicate BiPRM's effectiveness, robustness, and general applicability in process-based reward modeling, offering a promising new direction for enhancing reasoning quality in language models. 

<br /><br />Summary: <div>
arXiv:2508.01682v1 Announce Type: new 
Abstract: Process Reward Models (PRMs) have emerged as a promising approach to enhance the reasoning quality of Large Language Models (LLMs) by assigning fine-grained scores to intermediate reasoning steps within a solution trajectory. However, existing PRMs predominantly adopt a unidirectional left-to-right (L2R) evaluation paradigm, which limits their ability to leverage global context, making it challenging to verify the consistency of earlier steps based on later ones. In light of these challenges, we propose a novel bidirectional evaluation paradigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly incorporates a parallel right-to-left (R2L) evaluation stream alongside the conventional L2R flow, enabling later reasoning steps to help assess earlier ones in real time. Notably, the built-in R2L evaluation is implemented solely through prompt modifications that reverse the original reasoning trajectory, without any additional parameters or inference latency introduced. This ensures BiPRM remains both efficient and broadly compatible with existing PRM studies. We conduct extensive experiments on two mathematical reasoning benchmarks using samples generated by three different policy models. Our method, BiPRM, is evaluated across three backbones and three distinct PRM objectives. Across all settings, BiPRM consistently outperforms unidirectional baselines, achieving up to a 31.9% improvement in stepwise reward evaluation. Generally, our results highlight BiPRM's effectiveness, robustness, and general applicability, offering a promising new direction for process-based reward modeling.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy</title>
<link>https://arxiv.org/abs/2508.01696</link>
<guid>https://arxiv.org/abs/2508.01696</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, Large Language Models, Knowledge-Intensive Tasks, Collaborative Chain-of-Agents, Multi-Agent Reasoning

Summary:
The paper introduces Collaborative Chain-of-Agents, a framework aimed at improving the integration of parametric and retrieved knowledge in Retrieval-Augmented Generation models. They propose CoCoA-zero, a multi-agent framework for conditional knowledge induction and reasoning. Building on this, they develop CoCoA, a training strategy to enhance the model's ability to combine parametric and retrieved knowledge effectively. Experiments show that CoCoA-zero and CoCoA outperform existing methods on open-domain and multi-hop QA tasks. The new framework enhances the synergy between the model's internal knowledge and external retrieved knowledge, improving the model's performance in knowledge-intensive tasks. 

<br /><br />Summary: <div>
arXiv:2508.01696v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising framework for enhancing the capabilities of Large Language Models (LLMs), especially in knowledge-intensive tasks. Despite its advantages, current RAG methods often struggle to *fully exploit knowledge during generation*. In particular, the synergy between the model's internal parametric knowledge and external retrieved knowledge remains limited. Retrieved contents may sometimes mislead generation, while certain generated content can guide the model toward more accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a framework designed to enhance explicitly synergy over both parametric and retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent RAG framework that first performs conditional knowledge induction and then reasons answers. Building on this, we develop CoCoA, a long-chain training strategy that synthesizes extended multi-agent reasoning trajectories from CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability to explicitly integrate and jointly leverage parametric and retrieved knowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior performance on open-domain and multi-hop QA tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption</title>
<link>https://arxiv.org/abs/2508.01708</link>
<guid>https://arxiv.org/abs/2508.01708</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, sentiment leakage, expression leakage, benchmark dataset, automatic evaluation

Summary:
Large language models (LLMs) in natural language processing have advanced skills but can incorporate irrelevant information due to expression leakage, where they generate sentimentally charged expressions unrelated to the input. A benchmark dataset and automatic evaluation pipeline were created to analyze expression leakage, showing that as model size increases, leakage decreases within the same LLM family. Mitigating expression leakage requires careful model building and is not resolved by prompting. Negative sentiment in prompts increases expression leakage more than positive sentiment. This study highlights the need for attention to expression leakage and its impact on LLM performance.<br /><br />Summary: <div>
arXiv:2508.01708v1 Announce Type: new 
Abstract: Large language models (LLMs) have advanced natural language processing (NLP) skills such as through next-token prediction and self-attention, but their ability to integrate broad context also makes them prone to incorporating irrelevant information. Prior work has focused on semantic leakage, bias introduced by semantically irrelevant context. In this paper, we introduce expression leakage, a novel phenomenon where LLMs systematically generate sentimentally charged expressions that are semantically unrelated to the input context. To analyse the expression leakage, we collect a benchmark dataset along with a scheme to automatically generate a dataset from free-form text from common-crawl. In addition, we propose an automatic evaluation pipeline that correlates well with human judgment, which accelerates the benchmarking by decoupling from the need of annotation for each analysed model. Our experiments show that, as the model scales in the parameter space, the expression leakage reduces within the same LLM family. On the other hand, we demonstrate that expression leakage mitigation requires specific care during the model building process, and cannot be mitigated by prompting. In addition, our experiments indicate that, when negative sentiment is injected in the prompt, it disrupts the generation process more than the positive sentiment, causing a higher expression leakage rate.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications</title>
<link>https://arxiv.org/abs/2508.01710</link>
<guid>https://arxiv.org/abs/2508.01710</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Multilingual, Content Safety, CultureGuard, Safety Guard Models
Summary:
CultureGuard introduces a pipeline for generating culturally aligned safety datasets in multiple languages, expanding the Nemotron-Content-Safety-Dataset-V2 to eight languages. The resulting Nemotron-Content-Safety-Dataset-Multilingual-v1 contains 386,661 samples and is used to train the Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 model, achieving state-of-the-art performance on multilingual safety benchmarks. The study shows that open LLMs are more likely to produce unsafe responses in non-English languages, highlighting the need for culturally aware safety guard models. This work addresses the safety gap in multilingual LLMs and represents a significant step towards enhancing content safety across languages.
<br /><br />Summary: <div>
arXiv:2508.01710v1 Announce Type: new 
Abstract: The increasing use of Large Language Models (LLMs) in agentic applications highlights the need for robust safety guard models. While content safety in English is well-studied, non-English languages lack similar advancements due to the high cost of collecting culturally aligned labeled datasets. We present CultureGuard, a novel solution for curating culturally aligned, high-quality safety datasets across multiple languages. Our approach introduces a four-stage synthetic data generation and filtering pipeline: cultural data segregation, cultural data adaptation, machine translation, and quality filtering. This pipeline enables the conversion and expansion of the Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese. The resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1, comprises 386,661 samples in 9 languages and facilitates the training of Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning. The final model achieves state-of-the-art performance on several multilingual content safety benchmarks. We also benchmark the latest open LLMs on multilingual safety and observe that these LLMs are more prone to give unsafe responses when prompted in non-English languages. This work represents a significant step toward closing the safety gap in multilingual LLMs by enabling the development of culturally aware safety guard models.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction</title>
<link>https://arxiv.org/abs/2508.01739</link>
<guid>https://arxiv.org/abs/2508.01739</guid>
<content:encoded><![CDATA[
<div> keywords: user preferences, dialogue systems, preference extractor, fine-tuning, multi-turn dialogue

Summary: 
- Identifying user preferences in dialogue systems is crucial for providing satisfactory services.
- Using large language models (LLMs) for fine-tuning task-specific preference extractors shows promising results in accuracy and generalization.
- Obtaining high-quality labeled multi-turn dialogue data is a challenge due to the complexity of tracking user preference transitions.
- The proposed framework, IterChat, decomposes multi-turn preference extraction into iterative one-turn processes to reduce annotation errors and improve efficiency.
- By pre-defining preference slots with GPT4 and generating diverse dialogue datasets, IterChat achieves superior performance and annotator efficiency compared to traditional multi-turn dialogues. 

<br /><br />Summary: <div>
arXiv:2508.01739v1 Announce Type: new 
Abstract: Identifying user preferences in dialogue systems is a pivotal aspect of providing satisfying services. Current research shows that using large language models (LLMs) to fine-tune a task-specific preference extractor yields excellent results in terms of accuracy and generalization. However, the primary challenge stems from the inherent difficulty in obtaining high-quality labeled multi-turn dialogue data. Accurately tracking user preference transitions across turns not only demands intensive domain expertise and contextual consistency maintenance for annotators (termed \textbf{``Annotating Disaster''}) but also complicates model training due to error propagation in sequential dependency learning. Inspired by the observation that multi-turn preference extraction can be decomposed into iterative executions of one-turn extraction processes. We propose a novel dialogue data generation framework named \textbf{IterChat}. First, we construct a new data format that categorizes the dialogue data into attributed historical preferences and one-turn dialogues. This reduces the probability of annotation errors and improves annotation efficiency. Then, to generate a high-quality and diverse dialogue dataset, we adopt GPT4 to pre-define the preference slots in the target preference extractor task and then randomly sample the subset of the slots and their corresponding schema values to create the dialogue datasets. Experimental results indicate that fine-tuning or only few-shot prompting with the new dialogue format yields superior performance compared to the original multi-turn dialogues. Additionally, the new data format improves annotator efficiency with a win rate of 28.4\% higher than the original multi-turn dialogues.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Generated Text is Non-Stationary: Detection via Temporal Tomography</title>
<link>https://arxiv.org/abs/2508.01754</link>
<guid>https://arxiv.org/abs/2508.01754</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated text detection, Temporal Discrepancy Tomography, non-stationarity, adversarial perturbations, Continuous Wavelet Transform <br />
Summary: <br />
The article discusses the limitations of current AI-generated text detection methods, which fail to account for positional information of anomalies in text. It highlights the significant non-stationarity in AI-generated text and introduces Temporal Discrepancy Tomography (TDT) as a novel detection paradigm that preserves positional information by treating discrepancies as a time-series signal. TDT uses Continuous Wavelet Transform to capture both location and linguistic scale of anomalies, leading to improved detection performance on the RAID benchmark. TDT also demonstrates robustness against adversarial attacks, showcasing a 14.1% improvement in AUROC on Level 2 paraphrasing attacks. Despite its sophisticated analysis, TDT maintains practical efficiency with minimal computational overhead. The study emphasizes the importance of preserving temporal dynamics in detecting anomalies in AI-generated text. <br /> <div>
arXiv:2508.01754v1 Announce Type: new 
Abstract: The field of AI-generated text detection has evolved from supervised classification to zero-shot statistical analysis. However, current approaches share a fundamental limitation: they aggregate token-level measurements into scalar scores, discarding positional information about where anomalies occur. Our empirical analysis reveals that AI-generated text exhibits significant non-stationarity, statistical properties vary by 73.8\% more between text segments compared to human writing. This discovery explains why existing detectors fail against localized adversarial perturbations that exploit this overlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT), a novel detection paradigm that preserves positional information by reformulating detection as a signal processing task. TDT treats token-level discrepancies as a time-series signal and applies Continuous Wavelet Transform to generate a two-dimensional time-scale representation, capturing both the location and linguistic scale of statistical anomalies. On the RAID benchmark, TDT achieves 0.855 AUROC (7.1\% improvement over the best baseline). More importantly, TDT demonstrates robust performance on adversarial tasks, with 14.1\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its sophisticated analysis, TDT maintains practical efficiency with only 13\% computational overhead. Our work establishes non-stationarity as a fundamental characteristic of AI-generated text and demonstrates that preserving temporal dynamics is essential for robust detection.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comprehensive taxonomy of hallucinations in Large Language Models</title>
<link>https://arxiv.org/abs/2508.01781</link>
<guid>https://arxiv.org/abs/2508.01781</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, hallucination, taxonomy, mitigation strategies, responsible deployment

Summary:
This report discusses the challenges posed by the propensity of Large Language Models (LLMs) to generate factually incorrect or fabricated content, known as hallucinations. It introduces a comprehensive taxonomy of LLM hallucinations, categorizing them based on intrinsic/extrinsic distinctions, factuality/faithfulness, and various manifestations such as factual errors and ethical violations. The report identifies data-related, model-related, and prompt-related factors as underlying causes of hallucinations and explores cognitive and human factors influencing perception. It highlights the need for robust detection, mitigation strategies, and continuous human oversight to ensure responsible and reliable deployment of LLMs in critical applications. The report also discusses evaluation benchmarks, metrics for detection, and introduces web-based resources for monitoring LLM releases and performance. Despite the theoretical inevitability of LLM hallucinations, the report emphasizes the importance of ongoing efforts to address and mitigate these challenges. 

<br /><br />Summary: <div>
arXiv:2508.01781v1 Announce Type: new 
Abstract: Large language models (LLMs) have revolutionized natural language processing, yet their propensity for hallucination, generating plausible but factually incorrect or fabricated content, remains a critical challenge. This report provides a comprehensive taxonomy of LLM hallucinations, beginning with a formal definition and a theoretical framework that posits its inherent inevitability in computable LLMs, irrespective of architecture or training. It explores core distinctions, differentiating between intrinsic (contradicting input context) and extrinsic (inconsistent with training data or reality), as well as factuality (absolute correctness) and faithfulness (adherence to input). The report then details specific manifestations, including factual errors, contextual and logical inconsistencies, temporal disorientation, ethical violations, and task-specific hallucinations across domains like code generation and multimodal applications. It analyzes the underlying causes, categorizing them into data-related issues, model-related factors, and prompt-related influences. Furthermore, the report examines cognitive and human factors influencing hallucination perception, surveys evaluation benchmarks and metrics for detection, and outlines architectural and systemic mitigation strategies. Finally, it introduces web-based resources for monitoring LLM releases and performance. This report underscores the complex, multifaceted nature of LLM hallucinations and emphasizes that, given their theoretical inevitability, future efforts must focus on robust detection, mitigation, and continuous human oversight for responsible and reliable deployment in critical applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark</title>
<link>https://arxiv.org/abs/2508.01812</link>
<guid>https://arxiv.org/abs/2508.01812</guid>
<content:encoded><![CDATA[
<div> semantic understanding, Hebrew, Machine Reading Comprehension, morphologically rich, evaluation metrics  
Summary:  
- The article discusses the lack of semantic benchmarks in Hebrew Natural Language Processing (NLP) and introduces a new Hebrew Machine Reading Comprehension (MRC) dataset, HeQ.  
- The complex morphology of Hebrew presents challenges in MRC tasks, leading to annotation inconsistencies and flaws in evaluation metrics.  
- The authors propose novel guidelines, a crowdsourcing protocol, and revised evaluation metrics tailored to the morphologically rich nature of Hebrew.  
- Standard evaluation metrics like F1 scores and Exact Match are found to be inappropriate for Hebrew, suggesting the need for enhancements in evaluation methods.  
- Models designed for morpho-syntactic tasks may not perform well on semantic-heavy tasks, indicating the importance of addressing both dimensions in NLP models.  
<br /><br /> <div>
arXiv:2508.01812v1 Announce Type: new 
Abstract: Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly on morpho-syntactic tasks, neglecting the semantic dimension of language understanding. To bridge this gap, we set out to deliver a Hebrew Machine Reading Comprehension (MRC) dataset, where MRC is to be realized as extractive Question Answering. The morphologically rich nature of Hebrew poses a challenge to this endeavor: the indeterminacy and non-transparency of span boundaries in morphologically complex forms lead to annotation inconsistencies, disagreements, and flaws in standard evaluation metrics.
  To remedy this, we devise a novel set of guidelines, a controlled crowdsourcing protocol, and revised evaluation metrics that are suitable for the morphologically rich nature of the language. Our resulting benchmark, HeQ (Hebrew QA), features 30,147 diverse question-answer pairs derived from both Hebrew Wikipedia articles and Israeli tech news. Our empirical investigation reveals that standard evaluation metrics such as F1 scores and Exact Match (EM) are not appropriate for Hebrew (and other MRLs), and we propose a relevant enhancement.
  In addition, our experiments show low correlation between models' performance on morpho-syntactic tasks and on MRC, which suggests that models designed for the former might underperform on semantics-heavy tasks. The development and exploration of HeQ illustrate some of the challenges MRLs pose in natural language understanding (NLU), fostering progression towards more and better NLU models for Hebrew and other MRLs.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy</title>
<link>https://arxiv.org/abs/2508.01815</link>
<guid>https://arxiv.org/abs/2508.01815</guid>
<content:encoded><![CDATA[
<div> Knowledge Graphs, Question Answering, AgenticT2S, Circular Economy, Scalable Reasoning
Summary:
AgenticT2S is a framework designed for Question Answering over heterogeneous knowledge graphs (KGQA) in domains such as the circular economy. It addresses challenges related to diverse schemas, incomplete alignments, and distributed data sources. The framework decomposes KGQA into retrieval, query generation, and verification subtasks managed by specialized agents. A scheduler assigns subgoals to different graphs using weak-to-strong alignment strategies. A two-stage verifier detects invalid and underspecified queries through symbolic validation and consistency checks. Experimental results on real-world circular economy KGs show that AgenticT2S significantly improves execution accuracy and triple level F1 over existing baselines while reducing average prompt length. The framework demonstrates the benefits of agent-based schema-aware reasoning for scalable KGQA in sustainability domains, enabling robust cross-graph reasoning for decision-making purposes.<br /><br />Summary: <div>
arXiv:2508.01815v1 Announce Type: new 
Abstract: Question answering over heterogeneous knowledge graphs (KGQA) involves reasoning across diverse schemas, incomplete alignments, and distributed data sources. Existing text-to-SPARQL approaches rely on large-scale domain-specific fine-tuning or operate within single-graph settings, limiting their generalizability in low-resource domains and their ability to handle queries spanning multiple graphs. These challenges are particularly relevant in domains such as the circular economy, where information about classifications, processes, and emissions is distributed across independently curated knowledge graphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes KGQA into subtasks managed by specialized agents responsible for retrieval, query generation, and verification. A scheduler assigns subgoals to different graphs using weak-to-strong alignment strategies. A two-stage verifier detects structurally invalid and semantically underspecified queries through symbolic validation and counterfactual consistency checks. Experiments on real-world circular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy by 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing the average prompt length by 46.4%. These results demonstrate the benefits of agent-based schema-aware reasoning for scalable KGQA and support decision-making in sustainability domains through robust cross-graph reasoning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLP Memory: Language Modeling with Retriever-pretrained External Memory</title>
<link>https://arxiv.org/abs/2508.01832</link>
<guid>https://arxiv.org/abs/2508.01832</guid>
<content:encoded><![CDATA[
<div> Keywords: decoder-only LLMs, hallucinations, retriever-augmented generation, external memory, MLP memory

Summary: 

The article introduces a new architecture to address hallucinations in decoder-only LLMs. The proposed model, combining a transformer decoder and an external MLP memory trained to mimic the behavior of a retriever, shows promising results on language modeling and retrieval tasks. The architecture demonstrates improved performance on various datasets, showcasing a significant enhancement in model scalability compared to decoder-only models. The approach effectively handles hallucinations and memory-intensive tasks, outperforming existing methods like $k$NN-LM. Moreover, the external memory leads to improved reasoning capabilities in tasks like StrategyQA. The model not only achieves impressive results but also offers faster inference times, making it a practical solution for knowledge-intensive applications. The code and models will be made available open-source, facilitating further research and applications. 

<br /><br />Summary: <div>
arXiv:2508.01832v1 Announce Type: new 
Abstract: While modern decoder-only LLMs achieve superior performance across various domains, hallucinations have risen to be a common problem in their generated text, hindering their application in knowledge-intensive tasks. Retriever-augmented generation (RAG) offers a solution, but the non-parametric nature of the retriever hinders its deep interaction with LLM. In this work, we propose to decouple memorization from the LLM decoder using a pretrained, differentiable external memory. The external memory is an MLP pretrained by imitating the behavior of a retriever on the entire pretraining dataset. Our resulting architecture, which comprises a transformer decoder and an external MLP memory pretrained on language modeling and retriever imitation respectively, demonstrates strong perplexity and performance on downstream tasks. Experiments show our architecture exhibits steeper power-law scaling with model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web datasets compared to decoder-only models while benefiting from added training without overfitting. We demonstrate superior performance on three hallucination benchmarks and nine memory-intensive tasks. Additionally, our approach delivers $80\times$ speedup over $k$NN-LM (500M tokens) and $1.3\times$ faster inference than decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP memory improves StrategyQA performance. We will open-source our code and models in the future.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents</title>
<link>https://arxiv.org/abs/2508.01858</link>
<guid>https://arxiv.org/abs/2508.01858</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large-scale models, web agents, cognitive reasoning, knowledge acquisition, cognitive processes

Summary:
The paper introduces the Web-CogKnowledge Framework, which divides a web agent's capabilities into knowledge content learning and cognitive processes. Knowledge is categorized as Factual, Conceptual, and Procedural, with learning processes of Memorizing and Understanding and cognitive processes of Exploring. The Web-CogDataset is curated from real-world websites to facilitate knowledge acquisition. A knowledge-driven Chain-of-Thought (CoT) reasoning framework is developed to train the Web-CogReasoner, which outperforms existing models in generalizing to unseen tasks. The Web-CogBench evaluation suite is introduced to assess agent performance. The paper is accompanied by open-sourced code and data. <br /><br />Summary: The paper introduces the Web-CogKnowledge Framework, Web-CogDataset, Chain-of-Thought reasoning framework, Web-CogReasoner, and Web-CogBench evaluation suite to enhance web agents' knowledge acquisition and cognitive reasoning capabilities. <div>
arXiv:2508.01858v1 Announce Type: new 
Abstract: Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent's capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent's processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the "what" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the "how" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent's conceptual grounding-the "nouns" upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models</title>
<link>https://arxiv.org/abs/2508.01862</link>
<guid>https://arxiv.org/abs/2508.01862</guid>
<content:encoded><![CDATA[
<div> hallucination, language models, counterfactual probing, detection, mitigation  
Summary:<br />
Large Language Models (LLMs) have shown impressive abilities in various tasks but often produce hallucinated outputs with factual errors. A new approach called Counterfactual Probing is proposed to detect and reduce these hallucinations in LLM outputs. By creating counterfactual statements with subtle factual errors and analyzing the model's response, the method aims to identify inaccuracies in the generated content. This approach, without requiring model retraining, detects hallucinations more effectively than existing methods. Through adaptive mitigation strategies, hallucination scores are reduced by an average of 24.5%, enhancing the reliability of LLM outputs. The technique can be seamlessly integrated into current LLM workflows as a real-time verification tool, ensuring the accuracy and consistency of generated content. <div>
arXiv:2508.01862v1 Announce Type: new 
Abstract: Large Language Models have demonstrated remarkable capabilities across diverse tasks, yet they frequently generate hallucinations outputs that are fluent but factually incorrect or unsupported. We propose Counterfactual Probing, a novel approach for detecting and mitigating hallucinations in LLM outputs. Our method dynamically generates counterfactual statements that appear plausible but contain subtle factual errors, then evaluates the model's sensitivity to these perturbations. We hypothesize that genuine knowledge exhibits robustness to counterfactual variations, while hallucinated content shows inconsistent confidence patterns when confronted with plausible alternatives. Our comprehensive evaluation on TruthfulQA, factual statement datasets, and curated hallucination examples demonstrates that counterfactual probing achieves superior detection performance compared to baseline methods, while our adaptive mitigation strategies reduce hallucination scores by an average of 24.5%. The approach requires no model retraining and can be integrated into existing LLM pipelines as a realtime verification mechanism.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language</title>
<link>https://arxiv.org/abs/2508.01918</link>
<guid>https://arxiv.org/abs/2508.01918</guid>
<content:encoded><![CDATA[
<div> Quantum-RAG, Punjabi, large language models, low-resource languages, retrieval-augmented generation

Summary:
PunGPT2 and Pun-RAG are introduced as open-source Punjabi language models trained on a diverse corpus. PunGPT2 captures unique syntactic and morphological features while Pun-RAG improves factual grounding with a dense retriever. Pun-Instruct is a parameter-efficient variant enhancing zero-shot performance. Quantum-RAG combines sparse and dense retrieval methods with quantum-inspired semantic matching for improved contextual relevance. These models outperform multilingual baselines in perplexity, factuality, and fluency. This work provides a blueprint for extending LLM capabilities to underrepresented languages and pioneers quantum-aware retrieval in low-resource NLP. 

Summary: <div>
arXiv:2508.01918v1 Announce Type: new 
Abstract: Despite the rapid advancement of large language models (LLMs), low-resource languages remain largely excluded from the NLP landscape. We present PunGPT2, the first fully open-source suite of Punjabi large language models, trained from scratch on a 35GB domain-diverse corpus encompassing literature, religious texts, news, and social discourse. Unlike prior multilingual approaches, PunGPT2 captures rich syntactic and morphological features unique to Punjabi through a tokenizer optimised with byte pair encoding and linguistically aligned pretraining objectives. To improve factual grounding and domain recall, we introduce Pun-RAG, a retrieval-augmented generation framework combining PunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We further develop Pun-Instruct, a parameter-efficient, instruction-tuned variant using QLoRA, enabling robust zero-shot and instruction-following performance with significantly reduced compute needs.
  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system that fuses sparse (BM25) and dense methods with quantum-inspired semantic matching. By encoding queries using amplitude-based embeddings and retrieving via quantum kernel similarity, Quantum-RAG achieves improved contextual relevance with minimal memory overhead marking the first practical integration of quantum representations in low-resource language generation. Our models significantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in perplexity, factuality, and fluency. This work provides a scalable, reproducible blueprint for extending LLM capabilities to underrepresented languages and pioneers quantum-aware retrieval in low-resource NLP
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback</title>
<link>https://arxiv.org/abs/2508.01930</link>
<guid>https://arxiv.org/abs/2508.01930</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Learning from Human Feedback, Lexical preferences, Lexical overuse, Alignment research

Summary:
Large Language Models (LLMs) have been observed to overuse specific terms like "delve" and "intricate," with the reasons behind these lexical choices previously unclear. This study utilizes Meta's Llama model to investigate the role of Learning from Human Feedback (LHF), encompassing techniques like Reinforcement Learning from Human Feedback and Direct Preference Optimization. A method is introduced to detect LLMs' lexical preferences influenced by LHF. Experimentally emulating the LHF process confirms a systematic preference for text variants containing certain words, highlighting a potential misalignment between LHF workers and LLM users' lexical expectations. The research contributes to the field of explainable artificial intelligence, emphasizing the significance of data transparency and procedural clarity in alignment studies. <div>
arXiv:2508.01930v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are known to overuse certain terms like "delve" and "intricate." The exact reasons for these lexical choices, however, have been unclear. Using Meta's Llama model, this study investigates the contribution of Learning from Human Feedback (LHF), under which we subsume Reinforcement Learning from Human Feedback and Direct Preference Optimization. We present a straightforward procedure for detecting the lexical preferences of LLMs that are potentially LHF-induced. Next, we more conclusively link LHF to lexical overuse by experimentally emulating the LHF procedure and demonstrating that participants systematically prefer text variants that include certain words. This lexical overuse can be seen as a sort of misalignment, though our study highlights the potential divergence between the lexical expectations of different populations -- namely LHF workers versus LLM users. Our work contributes to the growing body of research on explainable artificial intelligence and emphasizes the importance of both data and procedural transparency in alignment research.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks</title>
<link>https://arxiv.org/abs/2508.01943</link>
<guid>https://arxiv.org/abs/2508.01943</guid>
<content:encoded><![CDATA[
<div> framework, reasoning, video, trajectory, ROVER
Summary:
ROVER is a framework designed to improve the performance of vision-language models in reasoning over long video sequences in embodied settings. It decomposes video trajectories into segments corresponding to subtasks, allowing for more focused reasoning without losing global context. The framework, implemented using in-context learning, outperforms strong baselines in three video reasoning tasks: task progress estimation, frame-level natural language reasoning, and video question answering. By reducing the number of frames considered at each timestep, ROVER mitigates hallucinations and improves performance during unexpected or non-optimal moments of a trajectory. It also features a subtask-specific sliding context window, resulting in linear time complexity with video length, which is an improvement over baselines. The evaluation on diverse datasets demonstrates the effectiveness of ROVER in enhancing the capabilities of vision-language models in handling long-horizon video sequences. <div>
arXiv:2508.01943v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have exhibited impressive capabilities across diverse image understanding tasks, but still struggle in settings that require reasoning over extended sequences of camera frames from a video. This limits their utility in embodied settings, which require reasoning over long frame sequences from a continuous stream of visual input at each moment of a task attempt. To address this limitation, we propose ROVER (Reasoning Over VidEo Recursively), a framework that enables the model to recursively decompose long-horizon video trajectories into segments corresponding to shorter subtasks within the trajectory. In doing so, ROVER facilitates more focused and accurate reasoning over temporally localized frame sequences without losing global context. We evaluate ROVER, implemented using an in-context learning approach, on diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa that consists of 543 videos showing both expert and perturbed non-expert trajectories across 27 robotic manipulation tasks. ROVER outperforms strong baselines across three video reasoning tasks: task progress estimation, frame-level natural language reasoning, and video question answering. We observe that, by reducing the number of frames the model reasons over at each timestep, ROVER mitigates hallucinations, especially during unexpected or non-optimal moments of a trajectory. In addition, by enabling the implementation of a subtask-specific sliding context window, ROVER's time complexity scales linearly with video length, an asymptotic improvement over baselines. Demos, code, and data available at: https://rover-vlm.github.io
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension</title>
<link>https://arxiv.org/abs/2508.01959</link>
<guid>https://arxiv.org/abs/2508.01959</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval-augmented generation, long documents, contextual information, embedding models, SitEmb

Summary:
In the field of retrieval-augmented generation, handling long documents poses a challenge as splitting them into smaller chunks may lead to loss of contextual information. Existing approaches to encoding longer context windows have limitations in improving retrieval and downstream tasks. To address this, a new approach is introduced where short chunks are represented conditioned on a broader context window to enhance retrieval performance. The proposed SitEmb models are designed to effectively encode situated context, outperforming existing embedding models in a book-plot retrieval dataset. The SitEmb-v1 model based on BGE-M3 demonstrates superior performance with only 1B parameters compared to larger models. The SitEmb-v1.5 model further enhances performance by over 10% across different languages and downstream applications.<br /><br />Summary: The article discusses challenges in retrieval-augmented generation with long documents and proposes a novel approach to enhance retrieval performance by representing short chunks within a broader context. The SitEmb models are introduced to effectively encode situated context, outperforming existing embedding models in a curated benchmark dataset. The results demonstrate significant improvements in retrieval capabilities, even with lower parameter counts, across various languages and downstream applications. <div>
arXiv:2508.01959v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) over long documents typically involves splitting the text into smaller chunks, which serve as the basic units for retrieval. However, due to dependencies across the original document, contextual information is often essential for accurately interpreting each chunk. To address this, prior work has explored encoding longer context windows to produce embeddings for longer chunks. Despite these efforts, gains in retrieval and downstream tasks remain limited. This is because (1) longer chunks strain the capacity of embedding models due to the increased amount of information they must encode, and (2) many real-world applications still require returning localized evidence due to constraints on model or human bandwidth.
  We propose an alternative approach to this challenge by representing short chunks in a way that is conditioned on a broader context window to enhance retrieval performance -- i.e., situating a chunk's meaning within its context. We further show that existing embedding models are not well-equipped to encode such situated context effectively, and thus introduce a new training paradigm and develop the situated embedding models (SitEmb). To evaluate our method, we curate a book-plot retrieval dataset specifically designed to assess situated retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3 substantially outperforms state-of-the-art embedding models, including several with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model further improves performance by over 10% and shows strong results across different languages and several downstream applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2508.01977</link>
<guid>https://arxiv.org/abs/2508.01977</guid>
<content:encoded><![CDATA[
arXiv:2508.01977v1 Announce Type: new 
Abstract: To address the severe data scarcity in Tibetan, a low-resource language spoken by over six million people, we introduce TIBSTC-CoT, the large-scale, multi-domain Tibetan dataset automatically constructed via chain-of-thought prompting with large language models (LLMs). TIBSTC-CoT establishes a scalable and reproducible framework for dataset creation in low-resource settings, covering diverse domains and reasoning patterns essential for language understanding and generation. Building on this dataset, we develop the Sunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with chain-of-thought capabilities. Trained entirely on TIBSTC-CoT, Sunshine-thinking has demonstrated strong reasoning and generation performance, comparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a significant step toward inclusive AI by enabling high-quality Tibetan language processing through both resource creation and model innovation. All data are available: https://github.com/Vicentvankor/sun-shine.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextually Aware E-Commerce Product Question Answering using RAG</title>
<link>https://arxiv.org/abs/2508.01990</link>
<guid>https://arxiv.org/abs/2508.01990</guid>
<content:encoded><![CDATA[
arXiv:2508.01990v1 Announce Type: new 
Abstract: E-commerce product pages contain a mix of structured specifications, unstructured reviews, and contextual elements like personalized offers or regional variants. Although informative, this volume can lead to cognitive overload, making it difficult for users to quickly and accurately find the information they need. Existing Product Question Answering (PQA) systems often fail to utilize rich user context and diverse product information effectively. We propose a scalable, end-to-end framework for e-commerce PQA using Retrieval Augmented Generation (RAG) that deeply integrates contextual understanding. Our system leverages conversational history, user profiles, and product attributes to deliver relevant and personalized answers. It adeptly handles objective, subjective, and multi-intent queries across heterogeneous sources, while also identifying information gaps in the catalog to support ongoing content improvement. We also introduce novel metrics to measure the framework's performance which are broadly applicable for RAG system evaluations.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompting Large Language Models to Detect Dementia Family Caregivers</title>
<link>https://arxiv.org/abs/2508.01999</link>
<guid>https://arxiv.org/abs/2508.01999</guid>
<content:encoded><![CDATA[
arXiv:2508.01999v1 Announce Type: new 
Abstract: Social media, such as Twitter, provides opportunities for caregivers of dementia patients to share their experiences and seek support for a variety of reasons. Availability of this information online also paves the way for the development of internet-based interventions in their support. However, for this purpose, tweets written by caregivers of dementia patients must first be identified. This paper demonstrates our system for the SMM4H 2025 shared task 3, which focuses on detecting tweets posted by individuals who have a family member with dementia. The task is outlined as a binary classification problem, differentiating between tweets that mention dementia in the context of a family member and those that do not. Our solution to this problem explores large language models (LLMs) with various prompting methods. Our results show that a simple zero-shot prompt on a fine-tuned model yielded the best results. Our final system achieved a macro F1-score of 0.95 on the validation set and the test set. Our full code is available on GitHub.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents</title>
<link>https://arxiv.org/abs/2508.02013</link>
<guid>https://arxiv.org/abs/2508.02013</guid>
<content:encoded><![CDATA[
arXiv:2508.02013v1 Announce Type: new 
Abstract: Recently, role-playing agents have emerged as a promising paradigm for achieving personalized interaction and emotional resonance. Existing research primarily focuses on the textual modality, neglecting the critical dimension of speech in realistic interactive scenarios. In particular, there is a lack of systematic evaluation for Speech Role-Playing Agents (SRPAs). To address this gap, we construct SpeechRole-Data, a large-scale, high-quality dataset that comprises 98 diverse roles and 112k speech-based single-turn and multi-turn conversations. Each role demonstrates distinct vocal characteristics, including timbre and prosody, thereby enabling more sophisticated speech role-playing. Furthermore, we propose SpeechRole-Eval, a multidimensional evaluation benchmark that systematically assesses SRPAs performance in key aspects such as fundamental interaction ability, speech expressiveness, and role-playing fidelity. Experimental results reveal the advantages and challenges of both cascaded and end-to-end speech role-playing agents in maintaining vocal style consistency and role coherence. We release all data, code, and baseline models to provide a solid foundation for speech-driven multimodal role-playing research and to foster further developments in this field.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models</title>
<link>https://arxiv.org/abs/2508.02018</link>
<guid>https://arxiv.org/abs/2508.02018</guid>
<content:encoded><![CDATA[
arXiv:2508.02018v1 Announce Type: new 
Abstract: Large audio-language models (LALMs) have achieved near-human performance in sentence-level transcription and emotion recognition. However, existing evaluations focus mainly on surface-level perception, leaving the capacity of models for contextual and inference-driven reasoning in speech-based scenarios insufficiently examined. To address this gap, we introduce SpeechR, a unified benchmark for evaluating reasoning over speech in large audio-language models. SpeechR evaluates models along three key dimensions: factual retrieval, procedural inference, and normative judgment. It includes three distinct evaluation formats. The multiple-choice version measures answer selection accuracy. The generative version assesses the coherence and logical consistency of reasoning chains. The acoustic-feature version investigates whether variations in stress and emotion affect reasoning performance. Evaluations on eleven state-of-the-art LALMs reveal that high transcription accuracy does not translate into strong reasoning capabilities. SpeechR establishes a structured benchmark for evaluating reasoning in spoken language, enabling more targeted analysis of model capabilities across diverse dialogue-based tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time</title>
<link>https://arxiv.org/abs/2508.02037</link>
<guid>https://arxiv.org/abs/2508.02037</guid>
<content:encoded><![CDATA[
arXiv:2508.02037v1 Announce Type: new 
Abstract: Large Language Models (LLMs) perform well on reasoning benchmarks but often fail when inputs alter slightly, raising concerns about the extent to which their success relies on memorization. This issue is especially acute in Chain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger intermediate errors that cascade into incorrect final answers. We introduce STIM, a novel framework for Source-aware Token-level Identification of Memorization, which attributes each token in a reasoning chain to one of multiple memorization sources - local, mid-range, or long-range - based on their statistical co-occurrence with the token in the pretraining corpus. Our token-level analysis across tasks and distributional settings reveals that models rely more on memorization in complex or long-tail cases, and that local memorization is often the dominant driver of errors, leading to up to 67% of wrong tokens. We also show that memorization scores from STIM can be effective in predicting the wrong tokens in the wrong reasoning step. STIM offers a powerful tool for diagnosing and improving model reasoning and can generalize to other structured step-wise generation tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marco-Voice Technical Report</title>
<link>https://arxiv.org/abs/2508.02038</link>
<guid>https://arxiv.org/abs/2508.02038</guid>
<content:encoded><![CDATA[
arXiv:2508.02038v1 Announce Type: new 
Abstract: This paper presents a multifunctional speech synthesis system that integrates voice cloning and emotion control speech synthesis within a unified framework. The goal of this work is to address longstanding challenges in achieving highly expressive, controllable, and natural speech generation that faithfully preserves speaker identity across diverse linguistic and emotional contexts. Our approach introduces an effective speaker-emotion disentanglement mechanism with in-batch contrastive learning, enabling independent manipulation of speaker identity and eemotional style, as well as rotational emotional embedding integration method for smooth emotion control. To support comprehensive training and evaluation, we construct CSEMOTIONS, a high-quality emotional speech dataset containing 10 hours of Mandarin speech from six professional speakers across seven emotional categories. Extensive experiments demonstrate that our system, Marco-Voice, achieves substantial improvements in both objective and subjective metrics. Comprehensive evaluations and analysis were conducted, results show that MarcoVoice delivers competitive performance in terms of speech clarity and emotional richness, representing a substantial advance in the field of expressive neural speech synthesis.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models</title>
<link>https://arxiv.org/abs/2508.02045</link>
<guid>https://arxiv.org/abs/2508.02045</guid>
<content:encoded><![CDATA[
arXiv:2508.02045v1 Announce Type: new 
Abstract: Facts evolve over time, making it essential for Large Language Models (LLMs) to handle time-sensitive factual knowledge accurately and reliably. While factual Time-Sensitive Question-Answering (TSQA) tasks have been widely studied, existing benchmarks often rely on manual curation or a small, fixed set of predefined templates, which restricts scalable and comprehensive TSQA evaluation. To address these challenges, we propose TDBench, a new benchmark that systematically constructs TSQA pairs by harnessing temporal databases and database techniques such as temporal SQL and functional dependencies. We also introduce a fine-grained evaluation metric called time accuracy, which assesses the validity of time references in model explanations alongside traditional answer accuracy to enable a more reliable TSQA evaluation. Extensive experiments on contemporary LLMs show how \ours{} enables scalable and comprehensive TSQA evaluation while reducing the reliance on human labor, complementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by enabling LLM evaluation on application-specific data and seamless multi-hop question generation. Code and data are publicly available at: https://github.com/ssoy0701/tdbench.git.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProCut: LLM Prompt Compression via Attribution Estimation</title>
<link>https://arxiv.org/abs/2508.02053</link>
<guid>https://arxiv.org/abs/2508.02053</guid>
<content:encoded><![CDATA[
arXiv:2508.02053v1 Announce Type: new 
Abstract: In large-scale industrial LLM systems, prompt templates often expand to thousands of tokens as teams iteratively incorporate sections such as task instructions, few-shot examples, and heuristic rules to enhance robustness and coverage. This expansion leads to bloated prompts that are difficult to maintain and incur significant inference latency and serving costs. To address this, we introduce Prompt Compression via Attribution Estimation (ProCut), a flexible, LLM-agnostic, training-free framework that compresses prompts through attribution analysis. ProCut segments prompt templates into semantically meaningful units, quantifies their impact on task performance, and prunes low-utility components. Through extensive experiments on five public benchmark datasets and real-world industrial prompts, we show that ProCut achieves substantial prompt size reductions (78% fewer tokens in production) while maintaining or even slightly improving task performance (up to 62% better than alternative methods). We further introduce an LLM-driven attribution estimator that reduces compression latency by over 50%, and demonstrate that ProCut integrates seamlessly with existing prompt-optimization frameworks to produce concise, high-performing prompts.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The SMeL Test: A simple benchmark for media literacy in language models</title>
<link>https://arxiv.org/abs/2508.02074</link>
<guid>https://arxiv.org/abs/2508.02074</guid>
<content:encoded><![CDATA[
arXiv:2508.02074v1 Announce Type: new 
Abstract: The internet is rife with unattributed, deliberately misleading, or otherwise untrustworthy content. Though large language models (LLMs) are often tasked with autonomous web browsing, the extent to which they have learned the simple heuristics human researchers use to navigate this noisy environment is not currently known. In this paper, we introduce the Synthetic Media Literacy Test (SMeL Test), a minimal benchmark that tests the ability of language models to actively filter out untrustworthy information in context. We benchmark a variety of commonly used instruction-tuned LLMs, including reasoning models, and find that no model consistently trusts more reliable sources; while reasoning in particular is associated with higher scores, even the best API model we test hallucinates up to 70% of the time. Remarkably, larger and more capable models do not necessarily outperform their smaller counterparts. We hope our work sheds more light on this important form of hallucination and guides the development of new methods to combat it.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models</title>
<link>https://arxiv.org/abs/2508.02087</link>
<guid>https://arxiv.org/abs/2508.02087</guid>
<content:encoded><![CDATA[
arXiv:2508.02087v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing with user-stated opinions even when those contradict factual knowledge. While prior work has documented this tendency, the internal mechanisms that enable such behavior remain poorly understood. In this paper, we provide a mechanistic account of how sycophancy arises within LLMs. We first systematically study how user opinions induce sycophancy across different model families. We find that simple opinion statements reliably induce sycophancy, whereas user expertise framing has a negligible impact. Through logit-lens analysis and causal activation patching, we identify a two-stage emergence of sycophancy: (1) a late-layer output preference shift and (2) deeper representational divergence. We also verify that user authority fails to influence behavior because models do not encode it internally. In addition, we examine how grammatical perspective affects sycophantic behavior, finding that first-person prompts (``I believe...'') consistently induce higher sycophancy rates than third-person framings (``They believe...'') by creating stronger representational perturbations in deeper layers. These findings highlight that sycophancy is not a surface-level artifact but emerges from a structural override of learned knowledge in deeper layers, with implications for alignment and truthful AI systems.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Harmless to You, Hurtful to Me!": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth</title>
<link>https://arxiv.org/abs/2508.02094</link>
<guid>https://arxiv.org/abs/2508.02094</guid>
<content:encoded><![CDATA[
arXiv:2508.02094v1 Announce Type: new 
Abstract: Risk perception is subjective, and youth's understanding of toxic content differs from that of adults. Although previous research has conducted extensive studies on toxicity detection in social media, the investigation of youth's unique toxicity, i.e., languages perceived as nontoxic by adults but toxic as youth, is ignored. To address this gap, we aim to explore: 1) What are the features of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing toxicity detection techniques accurately detect these languages (RQ2). For these questions, we took Chinese youth as the research target, constructed the first Chinese ``youth-toxicity'' dataset, and then conducted extensive analysis. Our results suggest that youth's perception of these is associated with several contextual factors, like the source of an utterance and text-related features. Incorporating these meta information into current toxicity detection methods significantly improves accuracy overall. Finally, we propose several insights into future research on youth-centered toxicity detection.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Dynamics of Meta-Learning in Small Model Pretraining</title>
<link>https://arxiv.org/abs/2508.02189</link>
<guid>https://arxiv.org/abs/2508.02189</guid>
<content:encoded><![CDATA[
arXiv:2508.02189v1 Announce Type: new 
Abstract: Large language models are powerful but costly. We ask whether meta-learning can make the pretraining of small language models not only better but also more interpretable. We integrate first-order MAML with subset-masked LM pretraining, producing four LLama-style decoder-only models (11M-570M params), and evaluate it on a fundamental NLP task with many settings and real-world applications. Compared with vanilla training, our model (i) reaches the same loss up to 1.6x sooner, (ii) improves F1 on multilingual Universal NER under equal compute, and (iii) makes the training dynamics easy to read: first the network's representations fan out ("diversify") and later they collapse into a smaller, shared subspace ("compress"). This two-stage shift shows up as a rise-and-fall in both effective-rank curves and attention-head entropy. The same curves pinpoint which layers specialise earliest and which later reconverge, giving a compact, interpretable signature of meta-adaptation. Code, checkpoints and WandB logs are released.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference</title>
<link>https://arxiv.org/abs/2508.02193</link>
<guid>https://arxiv.org/abs/2508.02193</guid>
<content:encoded><![CDATA[
arXiv:2508.02193v1 Announce Type: new 
Abstract: We present Seed Diffusion Preview, a large-scale language model based on discrete-state diffusion, offering remarkably fast inference speed. Thanks to non-sequential, parallel generation, discrete diffusion models provide a notable speedup to mitigate the inherent latency of token-by-token decoding, as demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion Preview achieves an inference speed of 2,146 token/s over H20 GPUs while maintaining competitive performance across a sweep of standard code evaluation benchmarks, significantly faster than contemporary Mercury and Gemini Diffusion, establishing new state of the art on the speed-quality Pareto frontier for code models.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems</title>
<link>https://arxiv.org/abs/2508.02208</link>
<guid>https://arxiv.org/abs/2508.02208</guid>
<content:encoded><![CDATA[
arXiv:2508.02208v1 Announce Type: new 
Abstract: Evaluating the mathematical capability of Large Language Models (LLMs) is a critical yet challenging frontier. Existing benchmarks fall short, particularly for proof-centric problems, as manual creation is unscalable and costly, leaving the true mathematical abilities of LLMs largely unassessed. To overcome these barriers, we propose Proof2Hybrid, the first fully automated framework that synthesizes high-quality, proof-centric benchmarks from natural language mathematical corpora. The key novelty of our solution is Proof2X, a roadmap of converting mathematical proofs into various kinds of questions that are easy to verify. Instructed by this roadmap, we propose a new type of hybrid-formatted questions, named ``$m$-out-of-$n$ multiple judge questions'', specifically designed to enable robust, automatic evaluation while being resilient to guessing and superficial pattern matching inherent in traditional formats. As a demonstration of our framework, we introduce AlgGeoTest, a benchmark for algebraic geometry--a frontier domain of modern mathematics--comprising 456 challenging items. Our extensive evaluations on state-of-the-art LLMs using AlgGeoTest reveal profound deficits in their comprehension of algebraic geometry, providing a more precise measure of their true mathematical capabilities. Our framework and benchmark pave the way for a new wave of in-depth research into the mathematical intelligence of AI systems.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Isolating Culture Neurons in Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2508.02241</link>
<guid>https://arxiv.org/abs/2508.02241</guid>
<content:encoded><![CDATA[
arXiv:2508.02241v1 Announce Type: new 
Abstract: Language and culture are deeply intertwined, yet it is so far unclear how and where multilingual large language models encode culture. Here, we extend upon an established methodology for identifying language-specific neurons and extend it to localize and isolate culture-specific neurons, carefully disentangling their overlap and interaction with language-specific neurons. To facilitate our experiments, we introduce MUREL, a curated dataset of 85.2 million tokens spanning six different cultures. Our localization and intervention experiments show that LLMs encode different cultures in distinct neuron populations, predominantly in upper layers, and that these culture neurons can be modulated independently from language-specific neurons or those specific to other cultures. These findings suggest that cultural knowledge and propensities in multilingual language models can be selectively isolated and edited - promoting fairness, inclusivity, and alignment. Code and data is available at https://github.com/namazifard/Culture_Neurons .
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interference Matrix: Quantifying Cross-Lingual Interference in Transformer Encoders</title>
<link>https://arxiv.org/abs/2508.02256</link>
<guid>https://arxiv.org/abs/2508.02256</guid>
<content:encoded><![CDATA[
arXiv:2508.02256v1 Announce Type: new 
Abstract: In this paper, we present a comprehensive study of language interference in encoder-only Transformer models across 83 languages. We construct an interference matrix by training and evaluating small BERT-like models on all possible language pairs, providing a large-scale quantification of cross-lingual interference. Our analysis reveals that interference between languages is asymmetrical and that its patterns do not align with traditional linguistic characteristics, such as language family, nor with proxies like embedding similarity, but instead better relate to script. Finally, we demonstrate that the interference matrix effectively predicts performance on downstream tasks, serving as a tool to better design multilingual models to obtain optimal performance.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.02260</link>
<guid>https://arxiv.org/abs/2508.02260</guid>
<content:encoded><![CDATA[
arXiv:2508.02260v1 Announce Type: new 
Abstract: Recently, reinforcement learning with verifiable rewards (RLVR) has been widely used for enhancing the reasoning abilities of large language models (LLMs). A core challenge in RLVR involves managing the exchange between entropy and performance of policies. Despite the importance of this exchange, a fine-grained understanding of when and how this exchange operates most effectively remains limited. To bridge this gap, we conduct a systematic empirical analysis of the entropy-performance exchange mechanism of RLVR across different levels of granularity. Specifically, we first divide the training process into two distinct stages based on entropy dynamics, i.e., rising stage and plateau stage, and then systematically investigate how this mechanism varies across stage-level, instance-level, and token-level granularitiess. Our analysis reveals that, in the rising stage, entropy reduction in negative samples facilitates the learning of effective reasoning patterns, which in turn drives rapid performance gains. Moreover, in the plateau stage, learning efficiency strongly correlates with high-entropy tokens present in low-perplexity samples and those located at the end of sequences. Motivated by these findings, we propose two methods that dynamically adjust the reward signal using perplexity and positional information to focus RL updates on tokens that exhibit high learning potential, achieving improvements compared to the baseline methods on various LLMs.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional Machine Translation System</title>
<link>https://arxiv.org/abs/2508.02268</link>
<guid>https://arxiv.org/abs/2508.02268</guid>
<content:encoded><![CDATA[
arXiv:2508.02268v1 Announce Type: new 
Abstract: The rich linguistic landscape of the Arab world is characterized by a significant gap between Modern Standard Arabic (MSA), the language of formal communication, and the diverse regional dialects used in everyday life. This diglossia presents a formidable challenge for natural language processing, particularly machine translation. This paper introduces \textbf{SHAMI-MT}, a bidirectional machine translation system specifically engineered to bridge the communication gap between MSA and the Syrian dialect. We present two specialized models, one for MSA-to-Shami and another for Shami-to-MSA translation, both built upon the state-of-the-art AraT5v2-base-1024 architecture. The models were fine-tuned on the comprehensive Nabra dataset and rigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami model achieved an outstanding average quality score of \textbf{4.01 out of 5.0} when judged by OPENAI model GPT-4.1, demonstrating its ability to produce translations that are not only accurate but also dialectally authentic. This work provides a crucial, high-fidelity tool for a previously underserved language pair, advancing the field of dialectal Arabic translation and offering significant applications in content localization, cultural heritage, and intercultural communication.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynaword: From One-shot to Continuously Developed Datasets</title>
<link>https://arxiv.org/abs/2508.02271</link>
<guid>https://arxiv.org/abs/2508.02271</guid>
<content:encoded><![CDATA[
arXiv:2508.02271v1 Announce Type: new 
Abstract: Large-scale datasets are foundational for research and development in natural language processing. However, current approaches face three key challenges: (1) reliance on ambiguously licensed sources restricting use, sharing, and derivative works; (2) static dataset releases that prevent community contributions and diminish longevity; and (3) quality assurance processes restricted to publishing teams rather than leveraging community expertise.
  To address these limitations, we introduce two contributions: the Dynaword approach and Danish Dynaword. The Dynaword approach is a framework for creating large-scale, open datasets that can be continuously updated through community collaboration. Danish Dynaword is a concrete implementation that validates this approach and demonstrates its potential. Danish Dynaword contains over four times as many tokens as comparable releases, is exclusively openly licensed, and has received multiple contributions across industry and research. The repository includes light-weight tests to ensure data formatting, quality, and documentation, establishing a sustainable framework for ongoing community contributions and dataset evolution.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A French Version of the OLDI Seed Corpus</title>
<link>https://arxiv.org/abs/2508.02290</link>
<guid>https://arxiv.org/abs/2508.02290</guid>
<content:encoded><![CDATA[
arXiv:2508.02290v1 Announce Type: new 
Abstract: We present the first French partition of the OLDI Seed Corpus, our submission to the WMT 2025 Open Language Data Initiative (OLDI) shared task. We detail its creation process, which involved using multiple machine translation systems and a custom-built interface for post-editing by qualified native speakers. We also highlight the unique translation challenges presented by the source data, which combines highly technical, encyclopedic terminology with the stylistic irregularities characteristic of user-generated content taken from Wikipedia. This French corpus is not an end in itself, but is intended as a crucial pivot resource to facilitate the collection of parallel corpora for the under-resourced regional languages of France.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple Methods Defend RAG Systems Well Against Real-World Attacks</title>
<link>https://arxiv.org/abs/2508.02296</link>
<guid>https://arxiv.org/abs/2508.02296</guid>
<content:encoded><![CDATA[
arXiv:2508.02296v1 Announce Type: new 
Abstract: Ensuring safety and in-domain responses for Retrieval-Augmented Generation (RAG) systems is paramount in safety-critical applications, yet remains a significant challenge. To address this, we evaluate four methodologies for Out-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal Component Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG system only responds to queries confined to the system's knowledge base. Specifically, our evaluation explores two novel dimensionality reduction and feature separation strategies: \textit{PCA}, where top components are selected using explained variance or OOD separability, and an adaptation of \textit{Neural Collapse Feature Separation}. We validate our approach on standard datasets (StackExchange and MSMARCO) and real-world applications (Substance Use and COVID-19), including tests against LLM-simulated and actual attacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations of response correctness and relevance, we confirm that an external OOD detector is crucial for maintaining response relevance.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaMPE: Length-aware Multi-grained Position Encoding for Adaptive Long-context Scaling Without Training</title>
<link>https://arxiv.org/abs/2508.02308</link>
<guid>https://arxiv.org/abs/2508.02308</guid>
<content:encoded><![CDATA[
arXiv:2508.02308v1 Announce Type: new 
Abstract: Large language models (LLMs) experience significant performance degradation when the input exceeds the pretraining context window, primarily due to the out-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent studies mitigate this problem by remapping OOD positions into the in-distribution range with fixed mapping strategies, ignoring the dynamic relationship between input length and the model's effective context window. To this end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a training-free method that fully utilizes the model's effective context window for adaptive long-context scaling in LLMs. Motivated by the left-skewed frequency distribution of relative positions, LaMPE establishes a dynamic relationship between mapping length and input length through a parametric scaled sigmoid function to adaptively allocate positional capacity across varying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention mechanism that strategically allocates positional resolution across different sequence regions to capture both fine-grained locality and long-range dependencies. Our method can be seamlessly applied to a wide range of RoPE-based LLMs without training. Extensive experiments on three representative LLMs across five mainstream long-context benchmarks demonstrate that LaMPE achieves significant performance improvements compared to existing length extrapolation methods. The code will be released at https://github.com/scar-on/LaMPE.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo</title>
<link>https://arxiv.org/abs/2508.02317</link>
<guid>https://arxiv.org/abs/2508.02317</guid>
<content:encoded><![CDATA[
arXiv:2508.02317v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. %
We present \veomni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. \veomni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. \veomni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. %
Using \veomni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis</title>
<link>https://arxiv.org/abs/2508.02322</link>
<guid>https://arxiv.org/abs/2508.02322</guid>
<content:encoded><![CDATA[
arXiv:2508.02322v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are distinguished by their strong performance scaling with increasing parameters across a wide range of tasks, yet they also suffer from substantial computational and storage overheads. Notably, the performance gains of MoE models do not scale proportionally with the growth in expert parameters. While prior works attempt to reduce parameters via expert-level pruning, merging, or decomposition, they still suffer from challenges in both performance and computational efficiency. In this paper, we address these challenges by introducing micro-expert as a finer-grained compression unit that spans across matrices. We first establish a more fundamental perspective, viewing MoE layers as mixtures of micro-experts, and present CAMERA, a lightweight and training-free framework for identifying micro-expert redundancy. Our analysis uncovers significant variance in micro-expert contributions during decoding. Based on this insight, we further propose CAMERA-P, a structured micro-expert pruning framework, and CAMERA-Q, a mixed-precision quantization idea designed for micro-experts. Extensive experiments on nine downstream tasks show that CAMERA-P consistently outperforms strong baselines under pruning ratios ranging from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under aggressive 2-bit quantization, surpassing existing matrix- and channel-level ideas. Notably, our method enables complete micro-expert analysis of Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models</title>
<link>https://arxiv.org/abs/2508.02360</link>
<guid>https://arxiv.org/abs/2508.02360</guid>
<content:encoded><![CDATA[
arXiv:2508.02360v1 Announce Type: new 
Abstract: Fine-tuning Large Language Models on a political topic will significantly manipulate their political stance on various issues and unintentionally affect their stance on unrelated topics. While previous studies have proposed this issue, there is still a lack of understanding regarding the internal representations of these stances and the mechanisms that lead to unintended cross-topic generalization. In this paper, we systematically explore the internal mechanisms underlying this phenomenon from a neuron-level perspective and how to mitigate the cross-topic generalization of political fine-tuning. Firstly, we propose Political Neuron Localization through Activation Contrasting (PNLAC) to identify two distinct types of political neurons: general political neurons, which govern stance across multiple political topics, and topic-specific neurons} that affect the model's political stance on individual topics. We find the existence of these political neuron types across four models and datasets through activation patching experiments. Leveraging these insights, we introduce InhibitFT, an inhibition-based fine-tuning method, effectively mitigating the cross-topic stance generalization. Experimental results demonstrate the robustness of identified neuron types across various models and datasets, and show that InhibitFT significantly reduces the cross-topic stance generalization by 20% on average, while preserving topic-specific performance. Moreover, we demonstrate that selectively inhibiting only 5% of neurons is sufficient to effectively mitigate the cross-topic stance generalization.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation</title>
<link>https://arxiv.org/abs/2508.02401</link>
<guid>https://arxiv.org/abs/2508.02401</guid>
<content:encoded><![CDATA[
arXiv:2508.02401v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have significantly boosted long-context processing. However, the increasing key-value (KV) cache size poses critical challenges to memory and execution efficiency. Most KV cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based LLMs. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of LLMs.
  To address the issue above, instead of using all the attention heads in GQA-based LLMs to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding KV cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive KV cache allocation strategy. Experimental results demonstrate the proposed CompressKV consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding</title>
<link>https://arxiv.org/abs/2508.02426</link>
<guid>https://arxiv.org/abs/2508.02426</guid>
<content:encoded><![CDATA[
arXiv:2508.02426v1 Announce Type: new 
Abstract: Since knowledge graphs (KG) will continue to evolve in real scenarios, traditional KGE models are only suitable for static knowledge graphs. Therefore, continual knowledge graph embedding (CKGE) has attracted the attention of researchers. Currently, a key challenge facing CKGE is that the model is prone to "catastrophic forgetting", resulting in the loss of previously learned knowledge. In order to effectively alleviate this problem, we propose a new CKGE model BAKE. First, we note that the Bayesian posterior update principle provides a natural continual learning strategy that is insensitive to data order and can theoretically effectively resist the forgetting of previous knowledge during data evolution. Different from the existing CKGE method, BAKE regards each batch of new data as a Bayesian update of the model prior. Under this framework, as long as the posterior distribution of the model is maintained, the model can better preserve the knowledge of early snapshots even after evolving through multiple time snapshots. Secondly, we propose a continual clustering method for CKGE, which further directly combats knowledge forgetting by constraining the evolution difference (or change amplitude) between new and old knowledge between different snapshots. We conduct extensive experiments on BAKE on multiple datasets, and the results show that BAKE significantly outperforms existing baseline models.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Based Measurement of Innovation: Mapping Expert Insight into Large Language Model Applications</title>
<link>https://arxiv.org/abs/2508.02430</link>
<guid>https://arxiv.org/abs/2508.02430</guid>
<content:encoded><![CDATA[
arXiv:2508.02430v1 Announce Type: new 
Abstract: Measuring innovation often relies on context-specific proxies and on expert evaluation. Hence, empirical innovation research is often limited to settings where such data is available. We investigate how large language models (LLMs) can be leveraged to overcome the constraints of manual expert evaluations and assist researchers in measuring innovation. We design an LLM framework that reliably approximates domain experts' assessment of innovation from unstructured text data. We demonstrate the performance and broad applicability of this framework through two studies in different contexts: (1) the innovativeness of software application updates and (2) the originality of user-generated feedback and improvement ideas in product reviews. We compared the performance (F1-score) and reliability (consistency rate) of our LLM framework against alternative measures used in prior innovation studies, and to state-of-the-art machine learning- and deep learning-based models. The LLM framework achieved higher F1-scores than the other approaches, and its results are highly consistent (i.e., results do not change across runs). This article equips R&amp;D personnel in firms, as well as researchers, reviewers, and editors, with the knowledge and tools to effectively use LLMs for measuring innovation and evaluating the performance of LLM-based innovation measures. In doing so, we discuss, the impact of important design decisions-including model selection, prompt engineering, training data size, training data distribution, and parameter settings-on performance and reliability. Given the challenges inherent in using human expert evaluation and existing text-based measures, our framework has important implications for harnessing LLMs as reliable, increasingly accessible, and broadly applicable research tools for measuring innovation.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LatentPrompt: Optimizing Promts in Latent Space</title>
<link>https://arxiv.org/abs/2508.02452</link>
<guid>https://arxiv.org/abs/2508.02452</guid>
<content:encoded><![CDATA[
arXiv:2508.02452v1 Announce Type: new 
Abstract: Recent advances have shown that optimizing prompts for Large Language Models (LLMs) can significantly improve task performance, yet many optimization techniques rely on heuristics or manual exploration. We present LatentPrompt, a model-agnostic framework for prompt optimization that leverages latent semantic space to automatically generate, evaluate, and refine candidate prompts without requiring hand-crafted rules. Beginning with a set of seed prompts, our method embeds them in a continuous latent space and systematically explores this space to identify prompts that maximize task-specific performance. In a proof-of-concept study on the Financial PhraseBank sentiment classification benchmark, LatentPrompt increased classification accuracy by approximately 3 percent after a single optimization cycle. The framework is broadly applicable, requiring only black-box access to an LLM and an automatic evaluation metric, making it suitable for diverse domains and tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monsoon Uprising in Bangladesh: How Facebook Shaped Collective Identity</title>
<link>https://arxiv.org/abs/2508.02498</link>
<guid>https://arxiv.org/abs/2508.02498</guid>
<content:encoded><![CDATA[
arXiv:2508.02498v1 Announce Type: new 
Abstract: This study investigates how Facebook shaped collective identity during the July 2024 pro-democracy uprising in Bangladesh, known as the Monsoon Uprising. During government repression, protesters turned to Facebook as a central space for resistance, where multimodal expressions, images, memes, videos, hashtags, and satirical posts played an important role in unifying participants. Using a qualitative approach, this research analyzes visual rhetoric, verbal discourse, and digital irony to reveal how shared symbols, protest art, and slogans built a sense of solidarity. Key elements included the symbolic use of red, the ironic metaphorical use of the term "Razakar", and the widespread sharing of visuals representing courage, injustice, and resistance. The findings show that the combination of visual and verbal strategies on Facebook not only mobilized public sentiment, but also built a strong collective identity that challenged authoritarian narratives. This study tries to demonstrate how online platforms can serve as powerful tools for identity construction and political mobilization in the digital age.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Monolingual to Bilingual: Investigating Language Conditioning in Large Language Models for Psycholinguistic Tasks</title>
<link>https://arxiv.org/abs/2508.02502</link>
<guid>https://arxiv.org/abs/2508.02502</guid>
<content:encoded><![CDATA[
arXiv:2508.02502v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit strong linguistic capabilities, but little is known about how they encode psycholinguistic knowledge across languages. We investigate whether and how LLMs exhibit human-like psycholinguistic responses under different linguistic identities using two tasks: sound symbolism and word valence. We evaluate two models, Llama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and bilingual prompting in English, Dutch, and Chinese. Behaviorally, both models adjust their outputs based on prompted language identity, with Qwen showing greater sensitivity and sharper distinctions between Dutch and Chinese. Probing analysis reveals that psycholinguistic signals become more decodable in deeper layers, with Chinese prompts yielding stronger and more stable valence representations than Dutch. Our results demonstrate that language identity conditions both output behavior and internal representations in LLMs, providing new insights into their application as models of cross-linguistic cognition.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular Arithmetic: Language Models Solve Math Digit by Digit</title>
<link>https://arxiv.org/abs/2508.02513</link>
<guid>https://arxiv.org/abs/2508.02513</guid>
<content:encoded><![CDATA[
arXiv:2508.02513v1 Announce Type: new 
Abstract: While recent work has begun to uncover the internal strategies that Large Language Models (LLMs) employ for simple arithmetic tasks, a unified understanding of their underlying mechanisms is still lacking. We extend recent findings showing that LLMs represent numbers in a digit-wise manner and present evidence for the existence of digit-position-specific circuits that LLMs use to perform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that operate independently on different digit positions (units, tens, hundreds). Notably, such circuits exist independently of model size and of tokenization strategy, i.e. both for models that encode longer numbers digit-by-digit and as one token. Using Feature Importance and Causal Interventions, we identify and validate the digit-position-specific circuits, revealing a compositional and interpretable structure underlying the solving of arithmetic problems in LLMs. Our interventions selectively alter the model's prediction at targeted digit positions, demonstrating the causal role of digit-position circuits in solving arithmetic tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs</title>
<link>https://arxiv.org/abs/2508.02515</link>
<guid>https://arxiv.org/abs/2508.02515</guid>
<content:encoded><![CDATA[
arXiv:2508.02515v1 Announce Type: new 
Abstract: This paper presents a systematic investigation into the constrained generation capabilities of large language models (LLMs) in producing Songci, a classical Chinese poetry form characterized by strict structural, tonal, and rhyme constraints defined by Cipai templates. We first develop a comprehensive, multi-faceted evaluation framework that includes: (i) a formal conformity score, (ii) automated quality assessment using LLMs, (iii) human evaluation, and (iv) classification-based probing tasks. Using this framework, we evaluate the generative performance of 18 LLMs, including 3 proprietary models and 15 open-source models across four families, under five prompting strategies: zero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought. Finally, we propose a Generate-Critic architecture in which the evaluation framework functions as an automated critic. Leveraging the critic's feedback as a reward signal, we fine-tune three lightweight open-source LLMs via supervised fine-tuning (SFT), resulting in improvements of up to 5.88% in formal conformity. Our findings offer new insights into the generative strengths and limitations of LLMs in producing culturally significant and formally constrained literary texts.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic Representations in LLaMA 3.2</title>
<link>https://arxiv.org/abs/2508.02527</link>
<guid>https://arxiv.org/abs/2508.02527</guid>
<content:encoded><![CDATA[
arXiv:2508.02527v1 Announce Type: new 
Abstract: Large language models demonstrate proficiency on phonetic tasks, such as rhyming, without explicit phonetic or auditory grounding. In this work, we investigate how \verb|Llama-3.2-1B-Instruct| represents token-level phonetic information. Our results suggest that Llama uses a rich internal model of phonemes to complete phonetic tasks. We provide evidence for high-level organization of phoneme representations in its latent space. In doing so, we also identify a ``phoneme mover head" which promotes phonetic information during rhyming tasks. We visualize the output space of this head and find that, while notable differences exist, Llama learns a model of vowels similar to the standard IPA vowel chart for humans, despite receiving no direct supervision to do so.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction</title>
<link>https://arxiv.org/abs/2508.02532</link>
<guid>https://arxiv.org/abs/2508.02532</guid>
<content:encoded><![CDATA[
arXiv:2508.02532v1 Announce Type: new 
Abstract: Standard transformer-based language models, while powerful for general text, often struggle with the fine-grained syntax and entity relationships in complex technical, engineering documents. To address this, we propose the Contextual Graph Transformer (CGT), a hybrid neural architecture that combines Graph Neural Networks (GNNs) and Transformers for domain-specific question answering. CGT constructs a dynamic graph over input tokens using sequential, skip-gram, and semantic similarity edges, which is processed by GATv2Conv layers for local structure learning. These enriched embeddings are then passed to a Transformer encoder to capture global dependencies. Unlike generic large models, technical domains often require specialized language models with stronger contextualization and structure awareness. CGT offers a parameter-efficient solution for such use cases. Integrated into a Retrieval-Augmented Generation (RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7% higher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from CGTs ability to jointly model structural token interactions and long-range semantic coherence. The model is trained from scratch using a two-phase approach: pretraining on general text followed by fine-tuning on domain-specific manuals. This highlights CGTs adaptability to technical language, enabling better grounding, entity tracking, and retrieval-augmented responses in real-world applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What's in the News? Towards Identification of Bias by Commission, Omission, and Source Selection (COSS)</title>
<link>https://arxiv.org/abs/2508.02540</link>
<guid>https://arxiv.org/abs/2508.02540</guid>
<content:encoded><![CDATA[
arXiv:2508.02540v1 Announce Type: new 
Abstract: In a world overwhelmed with news, determining which information comes from reliable sources or how neutral is the reported information in the news articles poses a challenge to news readers. In this paper, we propose a methodology for automatically identifying bias by commission, omission, and source selection (COSS) as a joint three-fold objective, as opposed to the previous work separately addressing these types of bias. In a pipeline concept, we describe the goals and tasks of its steps toward bias identification and provide an example of a visualization that leverages the extracted features and patterns of text reuse.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building and Aligning Comparable Corpora</title>
<link>https://arxiv.org/abs/2508.02555</link>
<guid>https://arxiv.org/abs/2508.02555</guid>
<content:encoded><![CDATA[
arXiv:2508.02555v1 Announce Type: new 
Abstract: Comparable corpus is a set of topic aligned documents in multiple languages, which are not necessarily translations of each other. These documents are useful for multilingual natural language processing when there is no parallel text available in some domains or languages. In addition, comparable documents are informative because they can tell what is being said about a topic in different languages. In this paper, we present a method to build comparable corpora from Wikipedia encyclopedia and EURONEWS website in English, French and Arabic languages. We further experiment a method to automatically align comparable documents using cross-lingual similarity measures. We investigate two cross-lingual similarity measures to align comparable documents. The first measure is based on bilingual dictionary, and the second measure is based on Latent Semantic Indexing (LSI). Experiments on several corpora show that the Cross-Lingual LSI (CL-LSI) measure outperforms the dictionary based measure. Finally, we collect English and Arabic news documents from the British Broadcast Corporation (BBC) and from ALJAZEERA (JSC) news website respectively. Then we use the CL-LSI similarity measure to automatically align comparable documents of BBC and JSC. The evaluation of the alignment shows that CL-LSI is not only able to align cross-lingual documents at the topic level, but also it is able to do this at the event level.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks</title>
<link>https://arxiv.org/abs/2508.02556</link>
<guid>https://arxiv.org/abs/2508.02556</guid>
<content:encoded><![CDATA[
arXiv:2508.02556v1 Announce Type: new 
Abstract: Automated annotation of clinical text with standardized medical concepts is critical for enabling structured data extraction and decision support. SNOMED CT provides a rich ontology for labeling clinical entities, but manual annotation is labor-intensive and impractical at scale. This study introduces a neural sequence labeling approach for SNOMED CT concept recognition using a Bidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text with domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences into overlapping 19-token chunks enriched with contextual, syntactic, and morphological features. The Bi-GRU model assigns IOB tags to identify concept spans and achieves strong performance with a 90 percent F1-score on the validation set. These results surpass traditional rule-based systems and match or exceed existing neural models. Qualitative analysis shows effective handling of ambiguous terms and misspellings. Our findings highlight that lightweight RNN-based architectures can deliver high-quality clinical concept annotation with significantly lower computational cost than transformer-based models, making them well-suited for real-world deployment.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction</title>
<link>https://arxiv.org/abs/2508.02558</link>
<guid>https://arxiv.org/abs/2508.02558</guid>
<content:encoded><![CDATA[
arXiv:2508.02558v1 Announce Type: new 
Abstract: Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10$\times$ higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs</title>
<link>https://arxiv.org/abs/2508.02573</link>
<guid>https://arxiv.org/abs/2508.02573</guid>
<content:encoded><![CDATA[
arXiv:2508.02573v1 Announce Type: new 
Abstract: Verbatim memorization in Large Language Models (LLMs) is a multifaceted phenomenon involving distinct underlying mechanisms. We introduce a novel method to analyze the different forms of memorization described by the existing taxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the attention weights of the LLM and evaluate the alignment between this taxonomy and the attention weights involved in decoding.
  We find that the existing taxonomy performs poorly and fails to reflect distinct mechanisms within the attention blocks. We propose a new taxonomy that maximizes alignment with the attention weights, consisting of three categories: memorized samples that are guessed using language modeling abilities, memorized samples that are recalled due to high duplication in the training set, and non-memorized samples. Our results reveal that few-shot verbatim memorization does not correspond to a distinct attention mechanism. We also show that a significant proportion of extractable samples are in fact guessed by the model and should therefore be studied separately. Finally, we develop a custom visual interpretability technique to localize the regions of the attention weights involved in each form of memorization.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare</title>
<link>https://arxiv.org/abs/2508.02574</link>
<guid>https://arxiv.org/abs/2508.02574</guid>
<content:encoded><![CDATA[
arXiv:2508.02574v1 Announce Type: new 
Abstract: Arabic-language patient feedback remains under-analysed because dialect diversity and scarce aspect-level sentiment labels hinder automated assessment. To address this gap, we introduce EHSAN, a data-centric hybrid pipeline that merges ChatGPT pseudo-labelling with targeted human review to build the first explainable Arabic aspect-based sentiment dataset for healthcare. Each sentence is annotated with an aspect and sentiment label (positive, negative, or neutral), forming a pioneering Arabic dataset aligned with healthcare themes, with ChatGPT-generated rationales provided for each label to enhance transparency. To evaluate the impact of annotation quality on model performance, we created three versions of the training data: a fully supervised set with all labels reviewed by humans, a semi-supervised set with 50% human review, and an unsupervised set with only machine-generated labels. We fine-tuned two transformer models on these datasets for both aspect and sentiment classification. Experimental results show that our Arabic-specific model achieved high accuracy even with minimal human supervision, reflecting only a minor performance drop when using ChatGPT-only labels. Reducing the number of aspect classes notably improved classification metrics across the board. These findings demonstrate an effective, scalable approach to Arabic aspect-based sentiment analysis (SA) in healthcare, combining large language model annotation with human expertise to produce a robust and explainable dataset. Future directions include generalisation across hospitals, prompt refinement, and interpretable data-driven modelling.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification</title>
<link>https://arxiv.org/abs/2508.02584</link>
<guid>https://arxiv.org/abs/2508.02584</guid>
<content:encoded><![CDATA[
arXiv:2508.02584v1 Announce Type: new 
Abstract: Leveraging outputs from multiple large language models (LLMs) is emerging as a method for harnessing their power across a wide range of tasks while mitigating their capacity for making errors, e.g., hallucinations. However, current approaches to combining insights from multiple LLMs often involve unstructured interactions (e.g., free debate), resulting in model generations that are not faithfully justifiable. In this work, we introduce MArgE, a novel framework to provide formal structure to the evidence from each LLM, in the form of a tree of extracted arguments, for the task of claim verification. We use a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks and semantics from the field of computational argumentation, to construct structured argument trees for given claims. This process creates an inspectable pathway from the initial arguments to the final claim verification decisions, providing a faithful justification thereof. We show experimentally that MArgE can significantly outperform single LLMs, including three open-source models (4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior methods for unstructured multi-LLM debates. We thus demonstrate the advantages of incorporating formal, argumentative reasoning mechanisms when combining multiple LLM outputs.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CharBench: Evaluating the Role of Tokenization in Character-Level Tasks</title>
<link>https://arxiv.org/abs/2508.02591</link>
<guid>https://arxiv.org/abs/2508.02591</guid>
<content:encoded><![CDATA[
arXiv:2508.02591v1 Announce Type: new 
Abstract: Tasks that require character-level reasoning, such as counting or locating characters within words, remain challenging for contemporary language models. A common conjecture is that language models' reliance on subword units, rather than characters, contributes to their struggles with character-level tasks, yet recent studies offer conflicting conclusions about the role of tokenization, leaving its impact unclear. To address this gap, we introduce CharBench, a comprehensive benchmark of character-level tasks that is two orders of magnitude larger than existing alternatives. We evaluate a diverse range of leading open-weight and proprietary models on CharBench and find that it presents a significant challenge to modern LLMs, with an average accuracy of 43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic properties of words and their segmentations into tokens correspond to model performance. For counting tasks, we find that tokenization properties are weakly correlated with correctness, while the length of the queried word and the actual character count play a more significant part. In contrast, for tasks requiring intra-word positional understanding, performance is negatively correlated with the length of the token containing the queried character, suggesting that longer tokens obscure character position information for LLMs. We encourage future work to build on the benchmark and evaluation methodology introduced here as tools for improving model performance on such tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation</title>
<link>https://arxiv.org/abs/2508.02618</link>
<guid>https://arxiv.org/abs/2508.02618</guid>
<content:encoded><![CDATA[
arXiv:2508.02618v1 Announce Type: new 
Abstract: The reward model (RM), as the core component of reinforcement learning from human feedback (RLHF) for large language models (LLMs), responsible for providing reward signals to generated responses. However, mainstream preference modeling in RM is inadequate in terms of token-level interaction, making its judgment signals vulnerable to being hacked by misallocated attention to context. This stems from two fundamental limitations: (1) Current preference modeling employs decoder-only architectures, where the unidirectional causal attention mechanism leads to forward-decaying intra-sequence attention within the prompt-response sequence. (2) The independent Siamese-encoding paradigm induces the absence of token-level inter-sequence attention between chosen and rejected sequences. To address this "attention hacking", we propose "Interaction Distillation", a novel training framework for more adequate preference modeling through attention-level optimization. The method introduces an interaction-based natural language understanding model as the teacher to provide sophisticated token interaction patterns via comprehensive attention, and guides the preference modeling to simulate teacher model's interaction pattern through an attentional alignment objective. Through extensive experiments, interaction distillation has demonstrated its ability to provide more stable and generalizable reward signals compared to state-of-the-art RM optimization methods that target data noise, highlighting the attention hacking constitute a more fundamental limitation in RM.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pointer: Linear-Complexity Long-Range Modeling without Pre-training</title>
<link>https://arxiv.org/abs/2508.02631</link>
<guid>https://arxiv.org/abs/2508.02631</guid>
<content:encoded><![CDATA[
arXiv:2508.02631v1 Announce Type: new 
Abstract: We introduce Pointer, a novel architecture that achieves linear $O(NK)$ complexity for long-range sequence modeling while maintaining superior performance without requiring pre-training. Unlike standard attention mechanisms that compute $O(N^2)$ pairwise interactions, our approach uses layer-wise pointer chaining where each layer's pointer selection depends on previous layer's pointer positions, creating explicit long-distance connections through pointer chains. We demonstrate that this architecture achieves $2$--$10\times$ speedup on long sequences compared to standard transformers, maintains $>95\%$ accuracy on copy tasks at distances up to 2048 tokens, and learns interpretable pointer patterns that reveal structured dependency modeling. Our experiments on efficiency benchmarks, long-range dependency tasks, and interpretability analysis show that Pointer offers a compelling alternative to attention mechanisms for scenarios requiring efficient long-range modeling without pre-training dependencies.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test Set Quality in Multilingual LLM Evaluation</title>
<link>https://arxiv.org/abs/2508.02635</link>
<guid>https://arxiv.org/abs/2508.02635</guid>
<content:encoded><![CDATA[
arXiv:2508.02635v1 Announce Type: new 
Abstract: Several multilingual benchmark datasets have been developed in a semi-automatic manner in the recent past to measure progress and understand the state-of-the-art in the multilingual capabilities of Large Language Models. However, there is not a lot of attention paid to the quality of the datasets themselves, despite the existence of previous work in identifying errors in even fully human-annotated test sets. In this paper, we manually analyze recent multilingual evaluation sets in two languages - French and Telugu, identifying several errors in the process. We compare the performance difference across several LLMs with the original and revised versions of the datasets and identify large differences (almost 10% in some cases) in both languages). Based on these results, we argue that test sets should not be considered immutable and should be revisited, checked for correctness, and potentially versioned. We end with some recommendations for both the dataset creators as well as consumers on addressing the dataset quality issues.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models</title>
<link>https://arxiv.org/abs/2505.09805</link>
<guid>https://arxiv.org/abs/2505.09805</guid>
<content:encoded><![CDATA[
arXiv:2505.09805v1 Announce Type: cross 
Abstract: Clustering patient subgroups is essential for personalized care and efficient resource use. Traditional clustering methods struggle with high-dimensional, heterogeneous healthcare data and lack contextual understanding. This study evaluates Large Language Model (LLM) based clustering against classical methods using a pediatric sepsis dataset from a low-income country (LIC), containing 2,686 records with 28 numerical and 119 categorical variables. Patient records were serialized into text with and without a clustering objective. Embeddings were generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with low-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was applied to these embeddings. Classical comparisons included K-Medoids clustering on UMAP and FAMD-reduced mixed data. Silhouette scores and statistical tests evaluated cluster quality and distinctiveness. Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B with the clustering objective performed better with higher number of clusters, identifying subgroups with distinct nutritional, clinical, and socioeconomic profiles. LLM-based methods outperformed classical techniques by capturing richer context and prioritizing key features. These results highlight potential of LLMs for contextual phenotyping and informed decision-making in resource-limited settings.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Attribution Crisis in LLM Search Results</title>
<link>https://arxiv.org/abs/2508.00838</link>
<guid>https://arxiv.org/abs/2508.00838</guid>
<content:encoded><![CDATA[
arXiv:2508.00838v1 Announce Type: cross 
Abstract: Web-enabled LLMs frequently answer queries without crediting the web pages they consume, creating an "attribution gap" - the difference between relevant URLs read and those actually cited. Drawing on approximately 14,000 real-world LMArena conversation logs with search-enabled LLM systems, we document three exploitation patterns: 1) No Search: 34% of Google Gemini and 24% of OpenAI GPT-4o responses are generated without explicitly fetching any online content; 2) No citation: Gemini provides no clickable citation source in 92% of answers; 3) High-volume, low-credit: Perplexity's Sonar visits approximately 10 relevant pages per query but cites only three to four. A negative binomial hurdle model shows that the average query answered by Gemini or Sonar leaves about 3 relevant websites uncited, whereas GPT-4o's tiny uncited gap is best explained by its selective log disclosures rather than by better attribution. Citation efficiency - extra citations provided per additional relevant web page visited - varies widely across models, from 0.19 to 0.45 on identical queries, underscoring that retrieval design, not technical limits, shapes ecosystem impact. We recommend a transparent LLM search architecture based on standardized telemetry and full disclosure of search traces and citation logs.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination Detection and Mitigation with Diffusion in Multi-Variate Time-Series Foundation Models</title>
<link>https://arxiv.org/abs/2508.00881</link>
<guid>https://arxiv.org/abs/2508.00881</guid>
<content:encoded><![CDATA[
arXiv:2508.00881v1 Announce Type: cross 
Abstract: Foundation models for natural language processing have many coherent definitions of hallucination and methods for its detection and mitigation. However, analogous definitions and methods do not exist for multi-variate time-series (MVTS) foundation models. We propose new definitions for MVTS hallucination, along with new detection and mitigation methods using a diffusion model to estimate hallucination levels. We derive relational datasets from popular time-series datasets to benchmark these relational hallucination levels. Using these definitions and models, we find that open-source pre-trained MVTS imputation foundation models relationally hallucinate on average up to 59.5% as much as a weak baseline. The proposed mitigation method reduces this by up to 47.7% for these models. The definition and methods may improve adoption and safe usage of MVTS foundation models.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks</title>
<link>https://arxiv.org/abs/2508.00890</link>
<guid>https://arxiv.org/abs/2508.00890</guid>
<content:encoded><![CDATA[
arXiv:2508.00890v1 Announce Type: cross 
Abstract: Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of a sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study a novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filtering with Self-Attention and Storing with MLP: One-Layer Transformers Can Provably Acquire and Extract Knowledge</title>
<link>https://arxiv.org/abs/2508.00901</link>
<guid>https://arxiv.org/abs/2508.00901</guid>
<content:encoded><![CDATA[
arXiv:2508.00901v1 Announce Type: cross 
Abstract: Modern large language models excel in knowledge-intensive tasks, yet how transformers acquire (store) knowledge during pre-training and extract (retrieve) it during post-fine-tuning inference remains theoretically opaque. While prior theoretical work has begun to investigate these questions through the analysis of training dynamics, such studies are limited to single-layer, attention-only architectures. However, most existing studies suggest that MLPs are the most contributing components for storing knowledge in transformer-based language models. Meanwhile, our empirical investigations reveal that such simplified models, when trained using standard next-token prediction objectives, may be incapable of acquiring or extracting factual knowledge. To overcome this limitation, we introduce a tractable one-layer transformer framework that crucially incorporates both self-attention and MLP modules. By tracking its gradient dynamics, we establish convergence and generalization guarantees that illuminate the ability of knowledge acquisition and extraction. We prove that 1) Transformers can achieve near-optimal training loss during pre-training, signifying effective knowledge acquisition; 2) With a large fine-tuning dataset and specific data multiplicity conditions met, transformers can achieve low generalization error when tested on factual knowledge learned during pre-training but not reinforced during the fine-tuning, indicating successful knowledge extraction; 3) When the conditions are not satisfied, transformers exhibit high generalization loss, resulting in hallucinations. Our analysis includes both full fine-tuning and low-rank fine-tuning. Furthermore, our analysis offers theoretical insights into several pertinent empirical phenomena, such as the role of learning rate schedules. Experiments on synthetic and real-world PopQA datasets with GPT-2 and Llama-3.2-1B validate our results.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An analysis of AI Decision under Risk: Prospect theory emerges in Large Language Models</title>
<link>https://arxiv.org/abs/2508.00902</link>
<guid>https://arxiv.org/abs/2508.00902</guid>
<content:encoded><![CDATA[
arXiv:2508.00902v1 Announce Type: cross 
Abstract: Judgment of risk is key to decision-making under uncertainty. As Daniel Kahneman and Amos Tversky famously discovered, humans do so in a distinctive way that departs from mathematical rationalism. Specifically, they demonstrated experimentally that humans accept more risk when they feel themselves at risk of losing something than when they might gain. I report the first tests of Kahneman and Tversky's landmark 'prospect theory' with Large Language Models, including today's state of the art chain-of-thought 'reasoners'.
  In common with humans, I find that prospect theory often anticipates how these models approach risky decisions across a range of scenarios. I also demonstrate that context is key to explaining much of the variance in risk appetite. The 'frame' through which risk is apprehended appears to be embedded within the language of the scenarios tackled by the models. Specifically, I find that military scenarios generate far larger 'framing effects' than do civilian settings, ceteris paribus. My research suggests, therefore, that language models the world, capturing our human heuristics and biases. But also that these biases are uneven - the idea of a 'frame' is richer than simple gains and losses. Wittgenstein's notion of 'language games' explains the contingent, localised biases activated by these scenarios. Finally, I use my findings to reframe the ongoing debate about reasoning and memorisation in LLMs.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cyber-Zero: Training Cybersecurity Agents without Runtime</title>
<link>https://arxiv.org/abs/2508.00910</link>
<guid>https://arxiv.org/abs/2508.00910</guid>
<content:encoded><![CDATA[
arXiv:2508.00910v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success in software engineering tasks when trained with executable runtime environments, particularly in resolving GitHub issues. However, such runtime environments are often unavailable in other domains, especially cybersecurity, where challenge configurations and execution contexts are ephemeral or restricted. We present Cyber-Zero, the first runtime-free framework for synthesizing high-quality agent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly available CTF writeups and employs persona-driven LLM simulation to reverse-engineer runtime behaviors and generate realistic, long-horizon interaction sequences without actual environments. Using trajectories synthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1% absolute performance gains over baseline models on three prominent CTF benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model, Cyber-Zero-32B, establishes new state-of-the-art performance among open-weight models, matching the capabilities of proprietary systems like DeepSeek-V3-0324 and Claude-3.5-Sonnet while offering superior cost-effectiveness, and demonstrating that runtime-free trajectory synthesis can effectively democratize the development of state-of-the-art cybersecurity agents.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small sample-based adaptive text classification through iterative and contrastive description refinement</title>
<link>https://arxiv.org/abs/2508.00957</link>
<guid>https://arxiv.org/abs/2508.00957</guid>
<content:encoded><![CDATA[
arXiv:2508.00957v1 Announce Type: cross 
Abstract: Zero-shot text classification remains a difficult task in domains with evolving knowledge and ambiguous category boundaries, such as ticketing systems. Large language models (LLMs) often struggle to generalize in these scenarios due to limited topic separability, while few-shot methods are constrained by insufficient data diversity. We propose a classification framework that combines iterative topic refinement, contrastive prompting, and active learning. Starting with a small set of labeled samples, the model generates initial topic labels. Misclassified or ambiguous samples are then used in an iterative contrastive prompting process to refine category distinctions by explicitly teaching the model to differentiate between closely related classes. The framework features a human-in-the-loop component, allowing users to introduce or revise category definitions in natural language. This enables seamless integration of new, unseen categories without retraining, making the system well-suited for real-world, dynamic environments. The evaluations on AGNews and DBpedia demonstrate strong performance: 91% accuracy on AGNews (3 seen, 1 unseen class) and 84% on DBpedia (8 seen, 1 unseen), with minimal accuracy shift after introducing unseen classes (82% and 87%, respectively). The results highlight the effectiveness of prompt-based semantic reasoning for fine-grained classification with limited supervision.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent</title>
<link>https://arxiv.org/abs/2508.01031</link>
<guid>https://arxiv.org/abs/2508.01031</guid>
<content:encoded><![CDATA[
arXiv:2508.01031v1 Announce Type: cross 
Abstract: Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing but typically requires a high level of expertise from designers. To lower the entry barrier and improve design efficiency, we present an agent for CAD conceptual design powered by large language models (LLMs). The agent accepts both abstract textual descriptions and freehand sketches as input, engaging in interactive dialogue with users to refine and clarify design requirements through comprehensive requirement analysis. Built upon a novel Context-Independent Imperative Paradigm (CIP), the agent generates high-quality CAD modeling code. During the generation process, the agent incorporates iterative visual feedback to improve model quality. Generated design cases are stored in a structured knowledge base, enabling continuous improvement of the agent's code generation capabilities. Experimental results demonstrate that our method achieves state-of-the-art performance in CAD code generation.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs</title>
<link>https://arxiv.org/abs/2508.01136</link>
<guid>https://arxiv.org/abs/2508.01136</guid>
<content:encoded><![CDATA[
arXiv:2508.01136v1 Announce Type: cross 
Abstract: The operation and maintenance (O&amp;M) of database systems is critical to ensuring system availability and performance, typically requiring expert experience (e.g., identifying metric-to-anomaly relations) for effective diagnosis and recovery. However, existing automatic database O&amp;M methods, including commercial products, cannot effectively utilize expert experience. On the one hand, rule-based methods only support basic O&amp;M tasks (e.g., metric-based anomaly detection), which are mostly numerical equations and cannot effectively incorporate literal O&amp;M experience (e.g., troubleshooting guidance in manuals). On the other hand, LLM-based methods, which retrieve fragmented information (e.g., standard documents + RAG), often generate inaccurate or generic results. To address these limitations, we present DBAIOps, a novel hybrid database O&amp;M system that combines reasoning LLMs with knowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a heterogeneous graph model for representing the diagnosis experience, and proposes a semi-automatic graph construction algorithm to build that graph from thousands of documents. Second, DBAIOps develops a collection of (800+) reusable anomaly models that identify both directly alerted metrics and implicitly correlated experience and metrics. Third, for each anomaly, DBAIOps proposes a two-stage graph evolution mechanism to explore relevant diagnosis paths and identify missing relations automatically. It then leverages a reasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear diagnosis reports for both DBAs and common users. Our evaluation over four mainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates that DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher in root cause and human evaluation accuracy, respectively.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens</title>
<link>https://arxiv.org/abs/2508.01191</link>
<guid>https://arxiv.org/abs/2508.01191</guid>
<content:encoded><![CDATA[
arXiv:2508.01191v1 Announce Type: cross 
Abstract: Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</title>
<link>https://arxiv.org/abs/2508.01249</link>
<guid>https://arxiv.org/abs/2508.01249</guid>
<content:encoded><![CDATA[
arXiv:2508.01249v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents offer a powerful new paradigm for solving various problems by combining natural language reasoning with the execution of external tools. However, their dynamic and non-transparent behavior introduces critical security risks, particularly in the presence of prompt injection attacks. In this work, we propose a novel insight that treats the agent runtime traces as structured programs with analyzable semantics. Thus, we present AgentArmor, a program analysis framework that converts agent traces into graph intermediate representation-based structured program dependency representations (e.g., CFG, DFG, and PDG) and enforces security policies via a type system. AgentArmor consists of three key components: (1) a graph constructor that reconstructs the agent's working traces as graph-based intermediate representations with control flow and data flow described within; (2) a property registry that attaches security-relevant metadata of interacted tools & data, and (3) a type system that performs static inference and checking over the intermediate representation. By representing agent behavior as structured programs, AgentArmor enables program analysis over sensitive data flow, trust boundaries, and policy violations. We evaluate AgentArmor on the AgentDojo benchmark, the results show that AgentArmor can achieve 95.75% of TPR, with only 3.66% of FPR. Our results demonstrate AgentArmor's ability to detect prompt injection vulnerabilities and enforce fine-grained security constraints.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question Answering in Taiwan</title>
<link>https://arxiv.org/abs/2508.01274</link>
<guid>https://arxiv.org/abs/2508.01274</guid>
<content:encoded><![CDATA[
arXiv:2508.01274v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) process visual, acoustic, and textual inputs, addressing the limitations of single-modality LLMs. However, existing benchmarks often overlook tri-modal evaluation in Traditional Chinese and do not consider inference latency. To address this, we introduce Multi-TW, the first Traditional Chinese benchmark for evaluating the performance and latency of any-to-any multimodal models. Multi-TW includes 900 multiple-choice questions (image and text, audio and text pairs) sourced from official proficiency tests developed with the Steering Committee for the Test of Proficiency-Huayu (SC-TOP). We evaluated various any-to-any models and vision-language models (VLMs) with audio transcription. Our results show that closed-source models generally outperform open-source ones across modalities, although open-source models can perform well in audio tasks. End-to-end any-to-any pipelines offer clear latency advantages compared to VLMs using separate audio transcription. Multi-TW presents a comprehensive view of model capabilities and highlights the need for Traditional Chinese fine-tuning and efficient multimodal architectures.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models</title>
<link>https://arxiv.org/abs/2508.01365</link>
<guid>https://arxiv.org/abs/2508.01365</guid>
<content:encoded><![CDATA[
arXiv:2508.01365v1 Announce Type: cross 
Abstract: Backdoor attacks pose a significant threat to Large Language Models (LLMs), where adversaries can embed hidden triggers to manipulate LLM's outputs. Most existing defense methods, primarily designed for classification tasks, are ineffective against the autoregressive nature and vast output space of LLMs, thereby suffering from poor performance and high latency. To address these limitations, we investigate the behavioral discrepancies between benign and backdoored LLMs in output space. We identify a critical phenomenon which we term sequence lock: a backdoored model generates the target sequence with abnormally high and consistent confidence compared to benign generation. Building on this insight, we propose ConfGuard, a lightweight and effective detection method that monitors a sliding window of token confidences to identify sequence lock. Extensive experiments demonstrate ConfGuard achieves a near 100\% true positive rate (TPR) and a negligible false positive rate (FPR) in the vast majority of cases. Crucially, the ConfGuard enables real-time detection almost without additional latency, making it a practical backdoor defense for real-world LLM deployments.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChEmbed: Enhancing Chemical Literature Search Through Domain-Specific Text Embeddings</title>
<link>https://arxiv.org/abs/2508.01643</link>
<guid>https://arxiv.org/abs/2508.01643</guid>
<content:encoded><![CDATA[
arXiv:2508.01643v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems in chemistry heavily depend on accurate and relevant retrieval of chemical literature. However, general-purpose text embedding models frequently fail to adequately represent complex chemical terminologies, resulting in suboptimal retrieval quality. Specialized embedding models tailored to chemical literature retrieval have not yet been developed, leaving a substantial performance gap. To address this challenge, we introduce ChEmbed, a domain-adapted family of text embedding models fine-tuned on a dataset comprising chemistry-specific text from the PubChem, Semantic Scholar, and ChemRxiv corpora. To create effective training data, we employ large language models to synthetically generate queries, resulting in approximately 1.7 million high-quality query-passage pairs. Additionally, we augment the tokenizer by adding 900 chemically specialized tokens to previously unused slots, which significantly reduces the fragmentation of chemical entities, such as IUPAC names. ChEmbed also maintains a 8192-token context length, enabling the efficient retrieval of longer passages compared to many other open-source embedding models, which typically have a context length of 512 or 2048 tokens. Evaluated on our newly introduced ChemRxiv Retrieval benchmark, ChEmbed outperforms state-of-the-art general embedding models, raising nDCG@10 from 0.82 to 0.91 (+9 pp). ChEmbed represents a practical, lightweight, and reproducible embedding solution that effectively improves retrieval for chemical literature search.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DUP: Detection-guided Unlearning for Backdoor Purification in Language Models</title>
<link>https://arxiv.org/abs/2508.01647</link>
<guid>https://arxiv.org/abs/2508.01647</guid>
<content:encoded><![CDATA[
arXiv:2508.01647v1 Announce Type: cross 
Abstract: As backdoor attacks become more stealthy and robust, they reveal critical weaknesses in current defense strategies: detection methods often rely on coarse-grained feature statistics, and purification methods typically require full retraining or additional clean models. To address these challenges, we propose DUP (Detection-guided Unlearning for Purification), a unified framework that integrates backdoor detection with unlearning-based purification. The detector captures feature-level anomalies by jointly leveraging class-agnostic distances and inter-layer transitions. These deviations are integrated through a weighted scheme to identify poisoned inputs, enabling more fine-grained analysis. Based on the detection results, we purify the model through a parameter-efficient unlearning mechanism that avoids full retraining and does not require any external clean model. Specifically, we innovatively repurpose knowledge distillation to guide the student model toward increasing its output divergence from the teacher on detected poisoned samples, effectively forcing it to unlearn the backdoor behavior. Extensive experiments across diverse attack methods and language model architectures demonstrate that DUP achieves superior defense performance in detection accuracy and purification efficacy. Our code is available at https://github.com/ManHu2025/DUP.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and Regional Languages Around the Globe</title>
<link>https://arxiv.org/abs/2508.01691</link>
<guid>https://arxiv.org/abs/2508.01691</guid>
<content:encoded><![CDATA[
arXiv:2508.01691v1 Announce Type: cross 
Abstract: We present Voxlect, a novel benchmark for modeling dialects and regional languages worldwide using speech foundation models. Specifically, we report comprehensive benchmark evaluations on dialects and regional language varieties in English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai, Spanish, French, German, Brazilian Portuguese, and Italian. Our study used over 2 million training utterances from 30 publicly available speech corpora that are provided with dialectal information. We evaluate the performance of several widely used speech foundation models in classifying speech dialects. We assess the robustness of the dialectal models under noisy conditions and present an error analysis that highlights modeling results aligned with geographic continuity. In addition to benchmarking dialect classification, we demonstrate several downstream applications enabled by Voxlect. Specifically, we show that Voxlect can be applied to augment existing speech recognition datasets with dialect information, enabling a more detailed analysis of ASR performance across dialectal variations. Voxlect is also used as a tool to evaluate the performance of speech generation systems. Voxlect is publicly available with the license of the RAIL family at: https://github.com/tiantiaf0627/voxlect.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2508.01773</link>
<guid>https://arxiv.org/abs/2508.01773</guid>
<content:encoded><![CDATA[
arXiv:2508.01773v1 Announce Type: cross 
Abstract: Large language models have demonstrated remarkable capabilities in complex mathematical reasoning tasks, but they inevitably generate errors throughout multi-step solutions. Process-level Reward Models (PRMs) have shown great promise by providing supervision and evaluation at each intermediate step, thereby effectively improving the models' reasoning abilities. However, training effective PRMs requires high-quality process reward data, yet existing methods for constructing such data are often labour-intensive or inefficient. In this paper, we propose an uncertainty-driven framework for automated process reward data construction, encompassing both data generation and annotation processes for PRMs. Additionally, we identify the limitations of both majority vote and PRMs, and introduce two generic uncertainty-aware output aggregation methods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which combine the strengths of majority vote with PRMs. Extensive experiments on ProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the proposed PRM data construction framework, and demonstrate that the two output aggregation methods further improve the mathematical reasoning abilities across diverse PRMs. The code and data will be publicly available at https://github.com/Jiuzhouh/UnPRM.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?</title>
<link>https://arxiv.org/abs/2508.01780</link>
<guid>https://arxiv.org/abs/2508.01780</guid>
<content:encoded><![CDATA[
arXiv:2508.01780v1 Announce Type: cross 
Abstract: With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase</title>
<link>https://arxiv.org/abs/2508.01791</link>
<guid>https://arxiv.org/abs/2508.01791</guid>
<content:encoded><![CDATA[
arXiv:2508.01791v1 Announce Type: cross 
Abstract: The field of Continuous Sign Language Recognition (CSLR) poses substantial technical challenges, including fluid inter-sign transitions, the absence of temporal boundaries, and co-articulation effects. This paper, developed for the MSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of signer-independent recognition to advance the generalization capabilities of CSLR systems across diverse signers. A data-centric methodology is proposed, centered on systematic feature engineering, a robust preprocessing pipeline, and an optimized model architecture. Key contributions include a principled feature selection process guided by Exploratory Data Analysis (EDA) to isolate communicative keypoints, a rigorous preprocessing pipeline incorporating DBSCAN-based outlier filtering and spatial normalization, and the novel CSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer design of the Conformer model, leveraging its capacity to model local temporal dependencies and global sequence context; a characteristic uniquely suited for the spatio-temporal dynamics of sign language. The proposed methodology achieved a competitive performance, with a Word Error Rate (WER) of 5.60% on the development set and 12.01% on the test set, a result that secured a 3rd place ranking on the official competition platform. This research validates the efficacy of cross-domain architectural adaptation, demonstrating that the Conformer model, originally conceived for speech recognition, can be successfully repurposed to establish a new state-of-the-art performance in keypoint-based CSLR.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection</title>
<link>https://arxiv.org/abs/2508.01887</link>
<guid>https://arxiv.org/abs/2508.01887</guid>
<content:encoded><![CDATA[
arXiv:2508.01887v1 Announce Type: cross 
Abstract: AI-generated text detectors have become essential tools for maintaining content authenticity, yet their robustness against evasion attacks remains questionable. We present PDFuzz, a novel attack that exploits the discrepancy between visual text layout and extraction order in PDF documents. Our method preserves exact textual content while manipulating character positioning to scramble extraction sequences. We evaluate this approach against the ArguGPT detector using a dataset of human and AI-generated text. Our results demonstrate complete evasion: detector performance drops from (93.6 $\pm$ 1.4) % accuracy and 0.938 $\pm$ 0.014 F1 score to random-level performance ((50.4 $\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity. Our work reveals a vulnerability in current detection systems that is inherent to PDF document structures and underscores the need for implementing sturdy safeguards against such attacks. We make our code publicly available at https://github.com/ACMCMC/PDFuzz.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models</title>
<link>https://arxiv.org/abs/2508.01908</link>
<guid>https://arxiv.org/abs/2508.01908</guid>
<content:encoded><![CDATA[
arXiv:2508.01908v1 Announce Type: cross 
Abstract: Training large language models (LLMs) typically involves pre-training on massive corpora, only to restart the process entirely when new data becomes available. A more efficient and resource-conserving approach would be continual pre-training, where models are updated with new data rather than retraining from scratch. However, the introduction of new data often causes distribution shifts, leading to performance degradation on previously learned tasks. In this paper, we take a deeper look at two popular proposals for addressing this distribution shift within the continual learning literature: experience replay and gradient alignment. We consider continual pre-training of models within the Llama family of architectures at a large scale across languages with 100 billion tokens of training data in each language, finding that both replay and gradient alignment lead to more stable learning without forgetting. This conclusion holds both as we vary the model scale and as we vary the number and diversity of tasks. Moreover, we are the first to demonstrate the effectiveness of gradient alignment techniques in the context of LLM pre-training and propose an efficient implementation of meta-experience replay (MER) that imbues experience replay with the benefits of gradient alignment despite negligible compute and memory overhead. Our scaling analysis across model sizes and replay rates indicates that small rates of replaying old examples are definitely a more valuable use of compute than investing in model size, but that it is more compute efficient to scale the size of the model than invest in high rates of replaying old examples.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Decentralized Framework for Ethical Authorship Validation in Academic Publishing: Leveraging Self-Sovereign Identity and Blockchain Technology</title>
<link>https://arxiv.org/abs/2508.01913</link>
<guid>https://arxiv.org/abs/2508.01913</guid>
<content:encoded><![CDATA[
arXiv:2508.01913v1 Announce Type: cross 
Abstract: Academic publishing, integral to knowledge dissemination and scientific advancement, increasingly faces threats from unethical practices such as unconsented authorship, gift authorship, author ambiguity, and undisclosed conflicts of interest. While existing infrastructures like ORCID effectively disambiguate researcher identities, they fall short in enforcing explicit authorship consent, accurately verifying contributor roles, and robustly detecting conflicts of interest during peer review. To address these shortcomings, this paper introduces a decentralized framework leveraging Self-Sovereign Identity (SSI) and blockchain technology. The proposed model uses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to securely verify author identities and contributions, reducing ambiguity and ensuring accurate attribution. A blockchain-based trust registry records authorship consent and peer-review activity immutably. Privacy-preserving cryptographic techniques, especially Zero-Knowledge Proofs (ZKPs), support conflict-of-interest detection without revealing sensitive data. Verified authorship metadata and consent records are embedded in publications, increasing transparency. A stakeholder survey of researchers, editors, and reviewers suggests the framework improves ethical compliance and confidence in scholarly communication. This work represents a step toward a more transparent, accountable, and trustworthy academic publishing ecosystem.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning</title>
<link>https://arxiv.org/abs/2508.01916</link>
<guid>https://arxiv.org/abs/2508.01916</guid>
<content:encoded><![CDATA[
arXiv:2508.01916v1 Announce Type: cross 
Abstract: Understanding internal representations of neural models is a core interest of mechanistic interpretability. Due to its large dimensionality, the representation space can encode various aspects about inputs. To what extent are different aspects organized and encoded in separate subspaces? Is it possible to find these ``natural'' subspaces in a purely unsupervised way? Somewhat surprisingly, we can indeed achieve this and find interpretable subspaces by a seemingly unrelated training objective. Our method, neighbor distance minimization (NDM), learns non-basis-aligned subspaces in an unsupervised manner. Qualitative analysis shows subspaces are interpretable in many cases, and encoded information in obtained subspaces tends to share the same abstract concept across different inputs, making such subspaces similar to ``variables'' used by the model. We also conduct quantitative experiments using known circuits in GPT-2; results show a strong connection between subspaces and circuit variables. We also provide evidence showing scalability to 2B models by finding separate subspaces mediating context and parametric knowledge routing. Viewed more broadly, our findings offer a new perspective on understanding model internals and building circuits.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolReasoner: Toward Effective and Interpretable Reasoning for Molecular LLMs</title>
<link>https://arxiv.org/abs/2508.02066</link>
<guid>https://arxiv.org/abs/2508.02066</guid>
<content:encoded><![CDATA[
arXiv:2508.02066v1 Announce Type: cross 
Abstract: Large Language Models(LLMs) have demonstrated remarkable performance across various domains, yet their capabilities in molecular reasoning remain insufficiently explored. Current approaches tend to rely heavily on general-purpose prompting, which lacks domain-specific molecular semantics, while those that use fine-tuning strategies often face challenges with interpretability and reasoning depth. To address these issues, we introduce MolReasoner, a two-stage framework designed to transition LLMs from memorization towards chemical reasoning. First, we propose Mol-SFT, which initializes the model's reasoning abilities via synthetic Chain-of-Thought(CoT) samples generated by GPT-4o and verified for chemical accuracy. Subsequently, Mol-RL applies reinforcement learning with specialized reward functions designed explicitly to align chemical structures with linguistic descriptions, thereby enhancing molecular reasoning capabilities. Our approach notably enhances interpretability, improving the model 's molecular understanding and enabling better generalization. Extensive experiments demonstrate that MolReasoner outperforms existing methods, and marking a significant shift from memorization-based outputs to robust chemical reasoning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Capital Visualization using Speech Amount during Meetings</title>
<link>https://arxiv.org/abs/2508.02075</link>
<guid>https://arxiv.org/abs/2508.02075</guid>
<content:encoded><![CDATA[
arXiv:2508.02075v1 Announce Type: cross 
Abstract: In recent years, many companies have recognized the importance of human resources and are investing in human capital to revitalize their organizations and enhance internal communication, thereby fostering innovation. However, conventional quantification methods have mainly focused on readily measurable indicators without addressing the fundamental role of conversations in human capital. This study focuses on routine meetings and proposes strategies to visualize human capital by analyzing speech amount during these meetings. We employ conversation visualization technology, which operates effectively, to quantify speech. We then measure differences in speech amount by attributes such as gender and job post, changes in speech amount depending on whether certain participants are present, and correlations between speech amount and continuous attributes. To verify the effectiveness of our proposed methods, we analyzed speech amounts by departmental affiliation during weekly meetings at small to medium enterprises.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search</title>
<link>https://arxiv.org/abs/2508.02091</link>
<guid>https://arxiv.org/abs/2508.02091</guid>
<content:encoded><![CDATA[
arXiv:2508.02091v1 Announce Type: cross 
Abstract: Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trainable Dynamic Mask Sparse Attention</title>
<link>https://arxiv.org/abs/2508.02124</link>
<guid>https://arxiv.org/abs/2508.02124</guid>
<content:encoded><![CDATA[
arXiv:2508.02124v1 Announce Type: cross 
Abstract: In large language models, the demand for modeling long contexts is constantly increasing, but the quadratic complexity of the standard self-attention mechanism often becomes a bottleneck. Although existing sparse attention mechanisms have improved efficiency, they may still encounter issues such as static patterns or information loss. We introduce a trainable dynamic mask sparse attention mechanism, Dynamic Mask Attention, which effectively utilizes content-aware and position-aware sparsity. DMA achieves this through two key innovations: First, it dynamically generates content-aware sparse masks from value representations, enabling the model to identify and focus on critical information adaptively. Second, it implements position-aware sparse attention computation that effectively skips unnecessary calculation regions. This dual-sparsity design allows the model to significantly reduce the computational complexity of important information while retaining complete information, achieving an excellent balance between information fidelity and computational efficiency. We have verified the performance of DMA through comprehensive experiments. Comparative studies show that DMA outperforms multi-head attention, sliding window attention, multi-head latent attention, and native sparse attention in terms of perplexity under Chinchilla Scaling Law settings. Moreover, in challenging multi-query associative recall tasks, DMA also demonstrates superior performance and efficiency compared to these methods. Crucially, in the evaluation of a 1.7B parameter model, DMA significantly outperforms multi-head attention in both standard benchmark performance and the challenging needle-in-a-haystack task. These experimental results highlight its capability to balance model efficiency and long-context modeling ability effectively.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subject or Style: Adaptive and Training-Free Mixture of LoRAs</title>
<link>https://arxiv.org/abs/2508.02165</link>
<guid>https://arxiv.org/abs/2508.02165</guid>
<content:encoded><![CDATA[
arXiv:2508.02165v1 Announce Type: cross 
Abstract: Fine-tuning models via Low-Rank Adaptation (LoRA) demonstrates remarkable performance in subject-driven or style-driven generation tasks. Studies have explored combinations of different LoRAs to jointly generate learned styles and content. However, current methods struggle to balance the original subject and style, and often require additional training. Recently, K-LoRA proposed a training-free LoRA fusion method. But it involves multiple hyperparameters, making it difficult to adapt to all styles and subjects. In this paper, we propose EST-LoRA, a training-free adaptive LoRA fusion method. It comprehensively considers three critical factors: \underline{E}nergy of matrix, \underline{S}tyle discrepancy scores and \underline{T}ime steps. Analogous to the Mixture of Experts (MoE) architecture, the model adaptively selects between subject LoRA and style LoRA within each attention layer. This integrated selection mechanism ensures balanced contributions from both components during the generation process. Experimental results show that EST-LoRA outperforms state-of-the-art methods in both qualitative and quantitative evaluations and achieves faster generation speed compared to other efficient fusion approaches. Our code is publicly available at: https://anonymous.4open.science/r/EST-LoRA-F318.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers</title>
<link>https://arxiv.org/abs/2508.02175</link>
<guid>https://arxiv.org/abs/2508.02175</guid>
<content:encoded><![CDATA[
arXiv:2508.02175v1 Announce Type: cross 
Abstract: As Audio Large Language Models (ALLMs) emerge as powerful tools for speech processing, their safety implications demand urgent attention. While considerable research has explored textual and vision safety, audio's distinct characteristics present significant challenges. This paper first investigates: Is ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In response to this issue, we introduce Hidden in the Noise (HIN), a novel backdoor attack framework designed to exploit subtle, audio-specific features. HIN applies acoustic modifications to raw audio waveforms, such as alterations to temporal dynamics and strategic injection of spectrally tailored noise. These changes introduce consistent patterns that an ALLM's acoustic feature encoder captures, embedding robust triggers within the audio stream. To evaluate ALLM robustness against audio-feature-based triggers, we develop the AudioSafe benchmark, assessing nine distinct risk types. Extensive experiments on AudioSafe and three established safety datasets reveal critical vulnerabilities in existing ALLMs: (I) audio features like environment noise and speech rate variations achieve over 90% average attack success rate. (II) ALLMs exhibit significant sensitivity differences across acoustic features, particularly showing minimal response to volume as a trigger, and (III) poisoned sample inclusion causes only marginal loss curve fluctuations, highlighting the attack's stealth.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeanK: Learnable K Cache Channel Pruning for Efficient Decoding</title>
<link>https://arxiv.org/abs/2508.02215</link>
<guid>https://arxiv.org/abs/2508.02215</guid>
<content:encoded><![CDATA[
arXiv:2508.02215v1 Announce Type: cross 
Abstract: Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CellForge: Agentic Design of Virtual Cell Models</title>
<link>https://arxiv.org/abs/2508.02276</link>
<guid>https://arxiv.org/abs/2508.02276</guid>
<content:encoded><![CDATA[
arXiv:2508.02276v1 Announce Type: cross 
Abstract: Virtual cell modeling represents an emerging frontier at the intersection of artificial intelligence and biology, aiming to predict quantities such as responses to diverse perturbations quantitatively. However, autonomously building computational models for virtual cells is challenging due to the complexity of biological systems, the heterogeneity of data modalities, and the need for domain-specific expertise across multiple disciplines. Here, we introduce CellForge, an agentic system that leverages a multi-agent framework that transforms presented biological datasets and research objectives directly into optimized computational models for virtual cells. More specifically, given only raw single-cell multi-omics data and task descriptions as input, CellForge outputs both an optimized model architecture and executable code for training virtual cell models and inference. The framework integrates three core modules: Task Analysis for presented dataset characterization and relevant literature retrieval, Method Design, where specialized agents collaboratively develop optimized modeling strategies, and Experiment Execution for automated generation of code. The agents in the Design module are separated into experts with differing perspectives and a central moderator, and have to collaboratively exchange solutions until they achieve a reasonable consensus. We demonstrate CellForge's capabilities in single-cell perturbation prediction, using six diverse datasets that encompass gene knockouts, drug treatments, and cytokine stimulations across multiple modalities. CellForge consistently outperforms task-specific state-of-the-art methods. Overall, CellForge demonstrates how iterative interaction between LLM agents with differing perspectives provides better solutions than directly addressing a modeling challenge. Our code is publicly available at https://github.com/gersteinlab/CellForge.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dialogue Systems Engineering: A Survey and Future Directions</title>
<link>https://arxiv.org/abs/2508.02279</link>
<guid>https://arxiv.org/abs/2508.02279</guid>
<content:encoded><![CDATA[
arXiv:2508.02279v1 Announce Type: cross 
Abstract: This paper proposes to refer to the field of software engineering related to the life cycle of dialogue systems as Dialogue Systems Engineering, and surveys this field while also discussing its future directions. With the advancement of large language models, the core technologies underlying dialogue systems have significantly progressed. As a result, dialogue system technology is now expected to be applied to solving various societal issues and in business contexts. To achieve this, it is important to build, operate, and continuously improve dialogue systems correctly and efficiently. Accordingly, in addition to applying existing software engineering knowledge, it is becoming increasingly important to evolve software engineering tailored specifically to dialogue systems. In this paper, we enumerate the knowledge areas of dialogue systems engineering based on those of software engineering, as defined in the Software Engineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based on this survey, we identify unexplored topics in each area and discuss the future direction of dialogue systems engineering.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative Credit Assignment</title>
<link>https://arxiv.org/abs/2508.02298</link>
<guid>https://arxiv.org/abs/2508.02298</guid>
<content:encoded><![CDATA[
arXiv:2508.02298v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has improved the reasoning abilities of Large Language Models (LLMs) by using rule-based binary feedback, helping to mitigate reward hacking. However, current RLVR methods typically treat whole responses as single actions, assigning the same reward to every token. This coarse-grained feedback hampers precise credit assignment, making it hard for models to identify which reasoning steps lead to success or failure, and often results in suboptimal policies and inefficient learning. Methods like PPO provide credit assignment through value estimation, but often yield inaccurate and unverifiable signals due to limited sampling. On the other hand, methods using Process Reward Models can provide step-by-step judgments for each reasoning step, but they require high-quality process supervision labels and are time-consuming when applied in online reinforcement learning (RL). To overcome these limitations, we introduce a simple but efficient method Credit Assignment Policy Optimization (CAPO). Given a reasoning response rollout from the policy model, CAPO directly leverages an off-the-shelf, general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to generate all step-wise critique by one pass, thereby providing verifiable token-level rewards to refine the tokens that were originally assigned identical rule-based rewards. This enables more fine-grained credit assignment in an effective way. Furthermore, to enhance the accuracy and robustness of CAPO, we employ voting mechanisms that scale with the number of generated critiques. Extensive experiments using different backbones like Llama and Qwen models and in different sizes show that CAPO consistently outperforms supervised learning-based and RL-based fine-tuning methods across six challenging mathematical benchmarks and three out-of-domain benchmarks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding User Preferences for Interaction Styles in Conversational Recommender Systems: The Predictive Role of System Qualities, User Experience, and Traits</title>
<link>https://arxiv.org/abs/2508.02328</link>
<guid>https://arxiv.org/abs/2508.02328</guid>
<content:encoded><![CDATA[
arXiv:2508.02328v1 Announce Type: cross 
Abstract: Conversational Recommender Systems (CRSs) deliver personalised recommendations through multi-turn natural language dialogue and increasingly support both task-oriented and exploratory interactions. Yet, the factors shaping user interaction preferences remain underexplored. In this within-subjects study (\(N = 139\)), participants experienced two scripted CRS dialogues, rated their experiences, and indicated the importance of eight system qualities. Logistic regression revealed that preference for the exploratory interaction was predicted by enjoyment, usefulness, novelty, and conversational quality. Unexpectedly, perceived effectiveness was also associated with exploratory preference. Clustering uncovered five latent user profiles with distinct dialogue style preferences. Moderation analyses indicated that age, gender, and control preference significantly influenced these choices. These findings integrate affective, cognitive, and trait-level predictors into CRS user modelling and inform autonomy-sensitive, value-adaptive dialogue design. The proposed predictive and adaptive framework applies broadly to conversational AI systems seeking to align dynamically with evolving user needs.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Model Guided Reinforcement Learning in Quantitative Trading</title>
<link>https://arxiv.org/abs/2508.02366</link>
<guid>https://arxiv.org/abs/2508.02366</guid>
<content:encoded><![CDATA[
arXiv:2508.02366v1 Announce Type: cross 
Abstract: Algorithmic trading requires short-term decisions aligned with long-term financial goals. While reinforcement learning (RL) has been explored for such tactical decisions, its adoption remains limited by myopic behavior and opaque policy rationale. In contrast, large language models (LLMs) have recently demonstrated strategic reasoning and multi-modal financial signal interpretation when guided by well-designed prompts.
  We propose a hybrid system where LLMs generate high-level trading strategies to guide RL agents in their actions. We evaluate (i) the rationale of LLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and Maximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results show improved return and risk metrics over standard RL.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Six Guidelines for Trustworthy, Ethical and Responsible Automation Design</title>
<link>https://arxiv.org/abs/2508.02371</link>
<guid>https://arxiv.org/abs/2508.02371</guid>
<content:encoded><![CDATA[
arXiv:2508.02371v1 Announce Type: cross 
Abstract: Calibrated trust in automated systems (Lee and See 2004) is critical for their safe and seamless integration into society. Users should only rely on a system recommendation when it is actually correct and reject it when it is factually wrong. One requirement to achieve this goal is an accurate trustworthiness assessment, ensuring that the user's perception of the system's trustworthiness aligns with its actual trustworthiness, allowing users to make informed decisions about the extent to which they can rely on the system (Schlicker et al. 2022). We propose six design guidelines to help designers optimize for accurate trustworthiness assessments, thus fostering ethical and responsible human-automation interactions. The proposed guidelines are derived from existing literature in various fields, such as human-computer interaction, cognitive psychology, automation research, user-experience design, and ethics. We are incorporating key principles from the field of pragmatics, specifically the cultivation of common ground (H. H. Clark 1996) and Gricean communication maxims (Grice 1975). These principles are essential for the design of automated systems because the user's perception of the system's trustworthiness is shaped by both environmental contexts, such as organizational culture or societal norms, and by situational context, including the specific circumstances or scenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed guidelines provide actionable insights for designers to create automated systems that make relevant trustworthiness cues available. This would ideally foster calibrated trust and more satisfactory, productive, and safe interactions between humans and automated systems. Furthermore, the proposed heuristics might work as a tool for evaluating to what extent existing systems enable users to accurately assess a system's trustworthiness.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens</title>
<link>https://arxiv.org/abs/2508.02419</link>
<guid>https://arxiv.org/abs/2508.02419</guid>
<content:encoded><![CDATA[
arXiv:2508.02419v1 Announce Type: cross 
Abstract: Large vision-language models (LVLMs) have demonstrated remarkable multimodal comprehension and reasoning capabilities, but they still suffer from severe object hallucination. Previous studies primarily attribute the flaw to linguistic prior caused by the scale mismatch between visual encoders and large language models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon LLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs, generating descriptions inconsistent with visual cues. However, through an in-depth investigation of the hallucinated mechanisms, we empirically reveal a previously overlooked phenomenon: LVLMs may ignore not only visual information but also textual modality during hallucination, a behavior termed as modality bias, which indicates that LVLMs struggle to simultaneously attend to both visual and textual modalities, leading to fragmented understanding of user-provided instructions. Based on this observation, we propose a simple yet effective training-free method to mitigate object hallucination. Concretely, we intervene and adjust the attention weights of textual and visual tokens, balancing cross-modal compatibility for better alignment with user intentions. Furthermore, we adopt a contrastive decoding strategy to reduce the LVLM's overreliance on its parametric knowledge, synergistically enhancing our attention manipulation. Extensive experiments confirm the widespread presence of modality bias in LVLMs. Notably, our method effectively mitigates hallucination across multiple open-source LVLMs and benchmarks, highlighting its generalizability and efficacy.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2508.02470</link>
<guid>https://arxiv.org/abs/2508.02470</guid>
<content:encoded><![CDATA[
arXiv:2508.02470v1 Announce Type: cross 
Abstract: While many tools are available for designing AI, non-experts still face challenges in clearly expressing their intent and managing system complexity. We introduce AIAP, a no-code platform that integrates natural language input with visual workflows. AIAP leverages a coordinated multi-agent system to decompose ambiguous user instructions into modular, actionable steps, hidden from users behind a unified interface. A user study involving 32 participants showed that AIAP's AI-generated suggestions, modular workflows, and automatic identification of data, actions, and context significantly improved participants' ability to develop services intuitively. These findings highlight that natural language-based visual programming significantly reduces barriers and enhances user experience in AI service design.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling</title>
<link>https://arxiv.org/abs/2508.02503</link>
<guid>https://arxiv.org/abs/2508.02503</guid>
<content:encoded><![CDATA[
arXiv:2508.02503v1 Announce Type: cross 
Abstract: LLM-based solvers have emerged as a promising means of automating problem modeling and solving. However, they remain unreliable and often depend on iterative repair loops that result in significant latency. We introduce OptiHive, an LLM-based framework that produces high-quality solvers for optimization problems from natural-language descriptions without iterative self-correction. OptiHive uses a single batched LLM query to generate diverse components (solvers, problem instances, and validation tests) and filters out erroneous components to ensure fully interpretable outputs. Taking into account the imperfection of the generated components, we employ a statistical model to infer their true performance, enabling principled uncertainty quantification and solver selection. On tasks ranging from traditional optimization problems to challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive significantly outperforms baselines, increasing the optimality rate from 5\% to 92\% on the most complex problems.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-time Prompt Intervention</title>
<link>https://arxiv.org/abs/2508.02511</link>
<guid>https://arxiv.org/abs/2508.02511</guid>
<content:encoded><![CDATA[
arXiv:2508.02511v1 Announce Type: cross 
Abstract: Test-time compute has led to remarkable success in the large language model (LLM) community, particularly for complex tasks, where longer chains of thought (CoTs) are generated to enhance reasoning capabilities. However, growing evidence reveals that such reasoning models often produce CoTs plagued by excessive redundancy, including unnecessary verification steps and repetitive reasoning shifts. The root cause lies in post-training of them that overly rely on outcome reward paradigms, as the data of process reward paradigms, which regulate intermediate reasoning steps, is difficult to construct at scale. To address this, we propose PI, a novel framework for Test-time Prompt Intervention. PI provides an interface to dynamically guide and regulate reasoning paths during inference through timely (When module) and proper (How module) interventions and post-intervention sampling (Which module). This allows human problem-solving expertise and cognitive science principles to be seamlessly integrated into LLMs' reasoning processes, enhancing controllability and interpretability. Extensive experiments across multiple models and datasets demonstrate that PI significantly shortens CoTs while reducing hallucination, yielding more concise and reliable reasoning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What are you sinking? A geometric approach on attention sink</title>
<link>https://arxiv.org/abs/2508.02546</link>
<guid>https://arxiv.org/abs/2508.02546</guid>
<content:encoded><![CDATA[
arXiv:2508.02546v1 Announce Type: cross 
Abstract: Attention sink (AS) is a consistent pattern in transformer attention maps where certain tokens (often special tokens or positional anchors) disproportionately attract attention from other tokens. We show that in transformers, AS is not an architectural artifact, but it is the manifestation of a fundamental geometric principle: the establishment of reference frames that anchor representational spaces. We analyze several architectures and identify three distinct reference frame types, centralized, distributed, and bidirectional, that correlate with the attention sink phenomenon. We show that they emerge during the earliest stages of training as optimal solutions to the problem of establishing stable coordinate systems in high-dimensional spaces. We show the influence of architecture components, particularly position encoding implementations, on the specific type of reference frame. This perspective transforms our understanding of transformer attention mechanisms and provides insights for both architecture design and the relationship with AS.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-Efficient Routed Fine-Tuning: Mixture-of-Experts Demands Mixture of Adaptation Modules</title>
<link>https://arxiv.org/abs/2508.02587</link>
<guid>https://arxiv.org/abs/2508.02587</guid>
<content:encoded><![CDATA[
arXiv:2508.02587v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) benefits from a dynamic routing mechanism among their specialized experts, which existing Parameter- Efficient Fine-Tuning (PEFT) strategies fail to leverage. This motivates us to investigate whether adaptation modules themselves should incorporate routing mechanisms to align with MoE's multi-expert architecture. We analyze dynamics of core components when applying PEFT to MoE language models and examine how different routing strategies affect adaptation effectiveness. Extensive experiments adapting OLMoE-1B-7B and Mixtral-8x7B on various commonsense and math reasoning tasks validate the performance and efficiency of our routed approach. We identify the optimal configurations for different scenarios and provide empirical analyses with practical insights to facilitate better PEFT and MoE applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous Healthcare Research</title>
<link>https://arxiv.org/abs/2508.02621</link>
<guid>https://arxiv.org/abs/2508.02621</guid>
<content:encoded><![CDATA[
arXiv:2508.02621v1 Announce Type: cross 
Abstract: The efficacy of AI agents in healthcare research is hindered by their reliance on static, predefined strategies. This creates a critical limitation: agents can become better tool-users but cannot learn to become better strategic planners, a crucial skill for complex domains like healthcare. We introduce HealthFlow, a self-evolving AI agent that overcomes this limitation through a novel meta-level evolution mechanism. HealthFlow autonomously refines its own high-level problem-solving policies by distilling procedural successes and failures into a durable, strategic knowledge base. To anchor our research and facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark featuring complex, realistic health data analysis tasks derived from peer-reviewed clinical research. Our comprehensive experiments demonstrate that HealthFlow's self-evolving approach significantly outperforms state-of-the-art agent frameworks. This work marks a necessary shift from building better tool-users to designing smarter, self-evolving task-managers, paving the way for more autonomous and effective AI for scientific discovery.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noosemia: toward a Cognitive and Phenomenological Account of Intentionality Attribution in Human-Generative AI Interaction</title>
<link>https://arxiv.org/abs/2508.02622</link>
<guid>https://arxiv.org/abs/2508.02622</guid>
<content:encoded><![CDATA[
arXiv:2508.02622v1 Announce Type: cross 
Abstract: This paper introduces and formalizes Noosemia, a novel cognitive-phenomenological phenomenon emerging from human interaction with generative AI systems, particularly those enabling dialogic or multimodal exchanges. We propose a multidisciplinary framework to explain how, under certain conditions, users attribute intentionality, agency, and even interiority to these systems - a process grounded not in physical resemblance, but in linguistic performance, epistemic opacity, and emergent technological complexity. By linking an LLM declination of meaning holism to our technical notion of the LLM Contextual Cognitive Field, we clarify how LLMs construct meaning relationally and how coherence and a simulacrum of agency arise at the human-AI interface. The analysis situates noosemia alongside pareidolia, animism, the intentional stance and the uncanny valley, distinguishing its unique characteristics. We also introduce a-noosemia to describe the phenomenological withdrawal of such projections. The paper concludes with reflections on the broader philosophical, epistemological, and social implications of noosemic dynamics and directions for future research.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents</title>
<link>https://arxiv.org/abs/2508.02629</link>
<guid>https://arxiv.org/abs/2508.02629</guid>
<content:encoded><![CDATA[
arXiv:2508.02629v1 Announce Type: cross 
Abstract: Recent advances in multimodal large language models (MLLMs) have enabled richer perceptual grounding for code policy generation in embodied agents. However, most existing systems lack effective mechanisms to adaptively monitor policy execution and repair codes during task completion. In this work, we introduce HyCodePolicy, a hybrid language-based control framework that systematically integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair into a closed-loop programming cycle for embodied agents. Technically, given a natural language instruction, our system first decomposes it into subgoals and generates an initial executable program grounded in object-centric geometric primitives. The program is then executed in simulation, while a vision-language model (VLM) observes selected checkpoints to detect and localize execution failures and infer failure reasons. By fusing structured execution traces capturing program-level events with VLM-based perceptual feedback, HyCodePolicy infers failure causes and repairs programs. This hybrid dual feedback mechanism enables self-correcting program synthesis with minimal human supervision. Our results demonstrate that HyCodePolicy significantly improves the robustness and sample efficiency of robot manipulation policies, offering a scalable strategy for integrating multimodal reasoning into autonomous decision-making pipelines.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Can Generate It Again: Data-to-Text Generation with Verification and Correction Prompting</title>
<link>https://arxiv.org/abs/2306.15933</link>
<guid>https://arxiv.org/abs/2306.15933</guid>
<content:encoded><![CDATA[
arXiv:2306.15933v2 Announce Type: replace 
Abstract: Small language models like T5 excel in generating high-quality text for data-to-text tasks, offering adaptability and cost-efficiency compared to Large Language Models (LLMs). However, they frequently miss keywords, which is considered one of the most severe and common errors in this task. In this work, we explore the potential of using feedback systems to enhance semantic fidelity in smaller language models for data-to-text generation tasks, through our Verification and Correction Prompting (VCP) approach. In the inference stage, our approach involves a multi-step process, including generation, verification, and regeneration stages. During the verification stage, we implement a simple rule to check for the presence of every keyword in the prediction. Recognizing that this rule can be inaccurate, we have developed a carefully designed training procedure, which enabling the model to incorporate feedback from the error-correcting prompt effectively, despite its potential inaccuracies. The VCP approach effectively reduces the Semantic Error Rate (SER) while maintaining the text's quality.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinker-DDM: Modeling Deliberation for Machine Translation with a Drift-Diffusion Process</title>
<link>https://arxiv.org/abs/2402.10699</link>
<guid>https://arxiv.org/abs/2402.10699</guid>
<content:encoded><![CDATA[
arXiv:2402.10699v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated promising potential in various downstream tasks, including machine translation. However, prior work on LLM-based machine translation has mainly focused on better utilizing training data, demonstrations, or pre-defined and universal knowledge to improve performance, with a lack of consideration of decision-making like human translators. In this paper, we incorporate Thinker with the Drift-Diffusion Model (Thinker-DDM) to address this issue. We then redefine the Drift-Diffusion process to emulate human translators' dynamic decision-making under constrained resources. We conduct extensive experiments under the high-resource, low-resource, and commonsense translation settings using the WMT22 and CommonMT datasets, in which Thinker-DDM outperforms baselines in the first two scenarios. We also perform additional analysis and evaluation on commonsense translation to illustrate the high effectiveness and efficacy of the proposed method.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>THREAD: Thinking Deeper with Recursive Spawning</title>
<link>https://arxiv.org/abs/2405.17402</link>
<guid>https://arxiv.org/abs/2405.17402</guid>
<content:encoded><![CDATA[
arXiv:2405.17402v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown impressive capabilities across diverse settings, but still struggle as the length and complexity of the context increases. To address this challenge, we propose Thinking Recursively and Dynamically (ThReaD). THREAD frames model generation as a thread of execution that, based on the context, can run to completion or dynamically spawn new threads. By spawning, threads can offload work (e.g., thinking, retrieving information) to child threads, which only return tokens needed for the parent thread to do its work. In effect, this enables the model to adapt, as needed, the amount of intermediate work used to produce tokens. We apply THREAD in the settings of LLM task solving and question answering, where the dynamic threading allows the model to recursively decompose the given task or question into progressively simpler sub-problems that can be solved by separate child threads. We test THREAD, implemented using a few-shot learning approach, on diverse benchmarks for agent tasks and data-grounded question answering. THREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these benchmarks, including ALFWorld, TextCraft, and WebShop, along with two new benchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD outperforms existing frameworks by 10% to 50% absolute points with smaller models, including Llama-3-8b and CodeLlama-7b.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Order Template Prediction for Generative Aspect-Based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2406.11130</link>
<guid>https://arxiv.org/abs/2406.11130</guid>
<content:encoded><![CDATA[
arXiv:2406.11130v2 Announce Type: replace 
Abstract: Aspect-based sentiment analysis (ABSA) assesses sentiments towards specific aspects within texts, resulting in detailed sentiment tuples. Previous ABSA models often use static templates to predict all of the elements in the tuples, and these models often fail to accurately capture dependencies between elements. Multi-view prompting method improves the performance of ABSA by predicting tuples with various templates and then ensembling the results. However, this method suffers from inefficiencies and out-of-distribution errors. In this paper, we propose a Dynamic Order Template (DOT) method for ABSA, which dynamically generates necessary views for each instance based on instance-level entropy. Ensuring the diverse and relevant view generation, our proposed method improves F1-scores on ASQP and ACOS datasets while significantly reducing inference time.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Tool-augmented Large Language Models be Aware of Incomplete Conditions?</title>
<link>https://arxiv.org/abs/2406.12307</link>
<guid>https://arxiv.org/abs/2406.12307</guid>
<content:encoded><![CDATA[
arXiv:2406.12307v5 Announce Type: replace 
Abstract: Recent advancements in integrating large language models (LLMs) with tools have allowed the models to interact with real-world environments. However, these tool-augmented LLMs often encounter incomplete scenarios when users provide partial information or the necessary tools are unavailable. Recognizing and managing such scenarios is crucial for LLMs to ensure their reliability, but this exploration remains understudied. This study examines whether LLMs can identify incomplete conditions and appropriately determine when to refrain from using tools. To quantitatively evaluate this capability, we construct a new benchmark dataset where instances are systematically altered to simulate the ambiguous and incomplete conditions common in real-world interactions. Our experiments reveal that even state-of-the-art LLMs often struggle to identify these conditions, attempting to use tools without sufficient information or when the correct tool is unavailable. To better understand these limitations, we conduct a detailed behavioral analysis across various conditions, including implicit evaluation and scenarios where models receive feedback from previous tool invocations. Based on this analysis, we propose a novel prompting-based reasoning strategy that explicitly instructs models to assess the sufficiency of information and the availability of tools. Our proposed approach significantly enhances the models' ability to recognize incomplete conditions, resulting in more informed and contextually appropriate tool-use decisions. We believe our research contributes to advancing the reliability of LLMs, especially in real-world applications where incomplete or ambiguous information is common. Our dataset is available at https://huggingface.co/datasets/ddehun/ICT.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cascade Reward Sampling for Efficient Decoding-Time Alignment</title>
<link>https://arxiv.org/abs/2406.16306</link>
<guid>https://arxiv.org/abs/2406.16306</guid>
<content:encoded><![CDATA[
arXiv:2406.16306v3 Announce Type: replace 
Abstract: Aligning large language models (LLMs) with human preferences is essential for their applications. Recently, decoding-time alignment has emerged as an effective plug-and-play technique that avoids fine-tuning model parameters. This approach retains the general utility of pretrained LLMs but often suffers from significant inefficiencies during decoding, primarily due to wasted token generation and excessive reward evaluations. To address these challenges, we introduce Cascade Reward Sampling (CARDS) to resolve both efficiency bottlenecks in decoding-time alignment. Specifically, we develop a segment-level rejection sampling algorithm that minimizes redundant computations of both LLMs and reward models (RMs). Central to CARDS is an uncertainty-based segmentation mechanism, which ensures the accuracy of RMs evaluations on incomplete segments. Furthermore, we provide a detailed analysis of reward scores on segments to elucidate the improved alignment performance. Experimental results demonstrate that CARDS significantly improves decoding efficiency, alignment quality, and general utility compared to existing decoding-time alignment methods, achieving approximately a 70% reduction in decoding time and over 90% win-ties in utility and safety benchmarks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation</title>
<link>https://arxiv.org/abs/2408.05456</link>
<guid>https://arxiv.org/abs/2408.05456</guid>
<content:encoded><![CDATA[
arXiv:2408.05456v2 Announce Type: replace 
Abstract: Unified graph representation learning aims to generate node embeddings, which can be applied to multiple downstream applications of graph analytics. However, existing studies based on graph neural networks and language models either suffer from the limitations of numerous training needs toward specific downstream predictions, poor generalization, or shallow semantic features. In this work, we propose a novel Path-LLM model to efficiently learn unified graph representation, which leverages a powerful large language model (LLM) to incorporate our proposed path features. Our Path-LLM framework consists of four well-designed techniques. First, we develop a new mechanism of long-to-short shortest path (L2SP) selection, which can cover key connections between different dense groups. An in-depth analysis and comparison of different path selections is conducted to justify the rationale behind our designed L2SP method. Next, we design path textualization to obtain L2SP-based training texts with key phrase selection from node text attributes. We then feed the texts into a self-supervised LLM training process to align next node/edge generation in L2SP with next token generation in causal language modeling for graph representation learning and finally extract the unified graph embeddings. We theoretically analyze the algorithm complexity of our Path-LLM approach. Extensive experiments on large-scale graph benchmarks validate the superiority of Path-LLM against state-of-the-art methods WalkLM, GraphGPT, OFA, and GraphTranslator on two classical graph learning tasks (node classification and edge validation) and one NP-hard graph query processing task (keyword search). Compared with WalkLM, our approach saves more than 90% of training paths on millions-scale graphs and runs at most 35x faster.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Negative Samples in Biomedical Generative Entity Linking</title>
<link>https://arxiv.org/abs/2408.16493</link>
<guid>https://arxiv.org/abs/2408.16493</guid>
<content:encoded><![CDATA[
arXiv:2408.16493v3 Announce Type: replace 
Abstract: Generative models have become widely used in biomedical entity linking (BioEL) due to their excellent performance and efficient memory usage. However, these models are usually trained only with positive samples, i.e., entities that match the input mention's identifier, and do not explicitly learn from hard negative samples, which are entities that look similar but have different meanings. To address this limitation, we introduce ANGEL (Learning from Negative Samples in Biomedical Generative Entity Linking), the first framework that trains generative BioEL models using negative samples. Specifically, a generative model is initially trained to generate positive entity names from the knowledge base for given input entities. Subsequently, both correct and incorrect outputs are gathered from the model's top-k predictions. Finally, the model is updated to prioritize the correct predictions through preference optimization. Our models outperform the previous best baseline models by up to an average top-1 accuracy of 1.4% on five benchmarks. When incorporating our framework into pre-training, the performance improvement increases further to 1.7%, demonstrating its effectiveness in both the pre-training and fine-tuning stages. The code and model weights are available at https://github.com/dmis-lab/ANGEL.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization</title>
<link>https://arxiv.org/abs/2410.12601</link>
<guid>https://arxiv.org/abs/2410.12601</guid>
<content:encoded><![CDATA[
arXiv:2410.12601v3 Announce Type: replace 
Abstract: To broaden the dissemination of scientific knowledge to diverse audiences, it is desirable for scientific document summarization systems to simultaneously control multiple attributes such as length and empirical focus. However, existing research typically focuses on controlling single attributes, leaving the compositional control of multiple attributes underexplored. To address this gap, we introduce CCSBench, the first evaluation benchmark for compositional controllable summarization in the scientific domain. Our benchmark enables fine-grained control over both explicit attributes (e.g., length), which are objective and straightforward, and implicit attributes (e.g., conceptual or empirical focus), which are more subjective and abstract. We conduct extensive experiments using various large language models (LLMs) under various settings, including in-context learning, parameter-efficient fine-tuning, and two-stage modular methods for balancing control over different attributes. Our findings reveal significant limitations in LLMs capabilities in balancing trade-offs between control attributes, especially implicit ones that require deeper understanding and abstract reasoning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arena-Lite: Efficient and Reliable Large Language Model Evaluation via Tournament-Based Direct Comparisons</title>
<link>https://arxiv.org/abs/2411.01281</link>
<guid>https://arxiv.org/abs/2411.01281</guid>
<content:encoded><![CDATA[
arXiv:2411.01281v4 Announce Type: replace 
Abstract: As Large Language Models (LLMs) expand across domains, LLM judges have become essential for systems evaluation. Current benchmarks typically compare system outputs against baselines. This baseline-mediated approach, though convenient, yields lower reliability than direct comparison between systems. We propose Arena-Lite which integrates tournament structure on top of head-to-head comparison. The application of a tournament structure and direct comparison eliminates the need for baseline outputs, reduces the number of required comparisons, and allows higher reliability in system rankings. We conducted two experiments: (1) controlled stochastic modeling and (2) empirical validation with a real LLM judge. Those experiments collectively demonstrate that Arena-Lite consistently achieves higher reliability with fewer comparisons, even with smaller datasets or weaker judges. We release an easy-to-use web demonstration and code to foster adoption of Arena-Lite, streamlining model selection across research and industry communities. Arena-Lite demo and code are available on \href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training and Evaluating Language Models with Template-based Data Generation</title>
<link>https://arxiv.org/abs/2411.18104</link>
<guid>https://arxiv.org/abs/2411.18104</guid>
<content:encoded><![CDATA[
arXiv:2411.18104v4 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, a fundamental bottleneck persists: these models often struggle with tasks requiring complex, multi-step reasoning, particularly in mathematical problem-solving. This deficiency stems from the critical scarcity of large-scale, high-quality, domain-specific datasets necessary for cultivating sophisticated reasoning abilities. To overcome this challenge, we introduce Template-based Data Generation (TDG), a novel and scalable paradigm that harnesses frontier LLMs (GPT-4) to automatically generate parameterized meta-templates, which in turn synthesize a virtually infinite stream of high-quality problems and solutions. Using this paradigm, we create TemplateMath Part I: TemplateGSM, a foundational dataset of over 7 million synthetically generated grade school math problems. Each problem is accompanied by a programmatically verifiable solution, offering an unprecedented level of quality at scale. This resource not only resolves the data scarcity issue for supervised fine-tuning but also provides a robust mechanism for model alignment through Reinforcement Learning with Verifiable Rewards (RLVR). Our approach elevates data augmentation by employing GPT-4 for meta-template creation, guaranteeing diverse and complex problem structures. By providing a scalable solution to the data and verification bottleneck, TDG and TemplateGSM pave the way for a new generation of LLMs with powerful, reliable reasoning skills. The code and data are available at https://github.com/iiis-ai/TemplateMath.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity</title>
<link>https://arxiv.org/abs/2412.02252</link>
<guid>https://arxiv.org/abs/2412.02252</guid>
<content:encoded><![CDATA[
arXiv:2412.02252v2 Announce Type: replace 
Abstract: The rapid expansion of context window sizes in Large Language Models~(LLMs) has enabled them to tackle increasingly complex tasks involving lengthy documents. However, this progress comes at the cost of a substantial increase in memory usage during inference, primarily due to the linear growth of the key-value~(KV) cache. Existing KV cache compression methods often discard less relevant tokens, which can lead to significant performance degradation when critical information is lost. In this paper, we propose \textsc{PoD}~(Proximal tokens over Distant tokens), a novel KV cache compression framework that allocates memory according to token importance, retaining less important tokens in a more compact, shared form rather than discarding them entirely. Our approach is motivated by two key observations: (1) proximal tokens -- those at the beginning and end of the context -- are significantly more important for next-token prediction, and (2) attention scores for distant tokens are highly redundant across consecutive layers. Leveraging these insights, \textsc{PoD} preserves the full KV cache for proximal tokens, while for distant tokens, it shares key states across layers. Since attention scores are determined by both queries and keys, sharing key states enables multiple layers to reuse a single set of keys for distant tokens, substantially reducing KV cache memory without discarding essential context. We further introduce a lightweight post-training adaptation to enable the model to adjust to this new attention-sharing structure. Extensive experiments on both synthetic~(Needle in a Haystack) and real-world long-context benchmarks demonstrate that \textsc{PoD} reduces KV cache memory usage by up to 35\% without compromising performance. Our method is orthogonal to existing token-selection-based techniques and can be combined with them for further KV cache compression.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Core Context Aware Transformers for Long Context Language Modeling</title>
<link>https://arxiv.org/abs/2412.12465</link>
<guid>https://arxiv.org/abs/2412.12465</guid>
<content:encoded><![CDATA[
arXiv:2412.12465v3 Announce Type: replace 
Abstract: Transformer-based Large Language Models (LLMs) have exhibited remarkable success in extensive tasks primarily attributed to self-attention mechanism, which requires a token to consider all preceding tokens as its context to compute attention. However, when the context length L becomes very large (e.g., 128K), the amount of potentially redundant information in the context tends to increase. The redundant context not only hampers the modeling representation performance but also incurs unnecessary computational and storage overhead. In this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for efficient long-context modeling, comprising two complementary modules: 1) Globality-aware pooling module groups input tokens and dynamically compresses each group into one core token based on their significance. In this way, our method automatically focuses and strengthens core context while diminishing redundancy during the learning process, leading to effective long-term dependency modeling. 2) Locality-preserving module incorporates neighboring tokens to preserve local context for detailed representation. Notably, our CCA-Attention is able to replace the self-attention module in existing LLMs with minimal fine-tuning cost. Extensive experimental results show the superiority of our method in both long-context modeling and computational efficiency over state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities</title>
<link>https://arxiv.org/abs/2501.00571</link>
<guid>https://arxiv.org/abs/2501.00571</guid>
<content:encoded><![CDATA[
arXiv:2501.00571v5 Announce Type: replace 
Abstract: Document-level relation extraction (Doc-RE) aims to extract relations between entities across multiple sentences. Therefore, Doc-RE requires more comprehensive reasoning abilities like humans, involving complex cross-sentence interactions between entities, contexts, and external general knowledge, compared to the sentence-level RE. However, most existing Doc-RE methods focus on optimizing single reasoning ability, but lack the ability to utilize external knowledge for comprehensive reasoning on long documents. To solve these problems, a knowledge retrieval augmented method, named KnowRA, was proposed with comprehensive reasoning to autonomously determine whether to accept external knowledge to assist DocRE. Firstly, we constructed a document graph for semantic encoding and integrated the co-reference resolution model to augment the co-reference reasoning ability. Then, we expanded the document graph into a document knowledge graph by retrieving the external knowledge base for common-sense reasoning and a novel knowledge filtration method was presented to filter out irrelevant knowledge. Finally, we proposed the axis attention mechanism to build direct and indirect associations with intermediary entities for achieving cross-sentence logical reasoning. Extensive experiments conducted on two datasets verified the effectiveness of our method compared to the state-of-the-art baselines. Our code is available at https://anonymous.4open.science/r/KnowRA.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Evolving Critique Abilities in Large Language Models</title>
<link>https://arxiv.org/abs/2501.05727</link>
<guid>https://arxiv.org/abs/2501.05727</guid>
<content:encoded><![CDATA[
arXiv:2501.05727v2 Announce Type: replace 
Abstract: Despite their remarkable performance, Large Language Models (LLMs) face a critical challenge: providing feedback for tasks where human evaluation is difficult or where LLMs potentially outperform humans. In such scenarios, leveraging the critique ability of LLMs themselves - identifying and correcting flaws - shows considerable promise. This paper explores enhancing critique abilities of LLMs, noting that current approaches rely on human annotations or more powerful models, leaving the challenge of improving critique abilities without external supervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework that trains LLMs with self-generated data to evolve their critique abilities. To address the low quality of naively generated data, we propose a contrastive-critic approach that uses reference solutions during data synthesis to enhance the model's understanding of key concepts, and incorporates a self-validation scheme to ensure data quality. The final trained model operates without any reference solutions at inference time. Implemented with Qwen2.5-72B-Instruct, a leading LLM, SCRIT demonstrates consistent improvements across a wide range of benchmarks spanning both mathematical and scientific reasoning: achieving a 10.0\% relative gain in critique-correction accuracy and a 19.0\% relative improvement in error identification F1-score. Our analysis reveals that SCRIT's performance scales positively with data and model size and enables continuous improvement through multi-round iterations.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Table Instruction Tuning</title>
<link>https://arxiv.org/abs/2501.14693</link>
<guid>https://arxiv.org/abs/2501.14693</guid>
<content:encoded><![CDATA[
arXiv:2501.14693v4 Announce Type: replace 
Abstract: Recent advances in table understanding have focused on instruction-tuning large language models (LLMs) for table-related tasks. However, existing research has overlooked the impact of hyperparameter choices, and also lacks a comprehensive evaluation of the out-of-domain table understanding ability and the general capabilities of these table LLMs. In this paper, we evaluate these abilities in existing table LLMs, and find significant declines in both out-of-domain table understanding and general capabilities as compared to their base models. Through systematic analysis, we show that hyperparameters, such as learning rate, can significantly influence both table-specific and general capabilities. Contrary to the previous table instruction-tuning work, we demonstrate that smaller learning rates and fewer training instances can enhance table understanding while preserving general capabilities. Based on our findings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B Instruct, which achieves performance on par with, or surpassing GPT-3.5 and GPT-4 on table tasks, while maintaining strong out-of-domain generalization and general capabilities. Our findings highlight the potential for reduced data annotation costs and more efficient model development through careful hyperparameter selection. We open-source the project and our models.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Scaling: Measuring and Predicting the Upper Bound of Knowledge Retention in Language Model Pre-Training</title>
<link>https://arxiv.org/abs/2502.04066</link>
<guid>https://arxiv.org/abs/2502.04066</guid>
<content:encoded><![CDATA[
arXiv:2502.04066v4 Announce Type: replace 
Abstract: The GPT-4 technical report highlights the possibility of predicting model performance on downstream tasks using only pre-training signals, though detailed methodologies are absent. Such predictive capabilities are essential for resource-efficient pre-training and the construction of task-aligned datasets. In this paper, we aim to predict performance in closed-book question answering (QA), a vital downstream task that directly reflects a model's internalized knowledge without the help of external tools. We address three primary challenges: (1) limited access to and understanding of pre-training corpora, (2) limitations of current evaluation methods for pre-trained models, and (3) limitations of frequency-based metrics in predicting model performance. In response, we conduct large-scale retrieval and semantic analysis across the pre-training corpora of 21 publicly available and 3 custom-trained large language models. We then develop a multi-template QA evaluation framework incorporating paraphrased question variants. Building on these foundations, we propose Size-dependent Mutual Information (SMI), an information-theoretic metric that linearly correlates pre-training data characteristics, model size, and QA accuracy, without requiring additional training. Experimental results show that SMI outperforms co-occurrence-based baselines, achieving $R^2 > 0.75$ on models with over one billion parameters. Theoretical analysis further suggests an upper bound of around 80% QA accuracy under optimal pre-training, reflecting intrinsic memory limitations and motivating the use of retrieval or few-shot methods in later stages.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Response Planning in LLMs</title>
<link>https://arxiv.org/abs/2502.06258</link>
<guid>https://arxiv.org/abs/2502.06258</guid>
<content:encoded><![CDATA[
arXiv:2502.06258v3 Announce Type: replace 
Abstract: In this work, we argue that large language models (LLMs), though trained to predict only the next token, exhibit emergent planning behaviors: $\textbf{their hidden representations encode future outputs beyond the next token}$. Through simple probing, we demonstrate that LLM prompt representations encode global attributes of their entire responses, including $\textit{structure attributes}$ (e.g., response length, reasoning steps), $\textit{content attributes}$ (e.g., character choices in storywriting, multiple-choice answers at the end of response), and $\textit{behavior attributes}$ (e.g., answer confidence, factual consistency). In addition to identifying response planning, we explore how it scales with model size across tasks and how it evolves during generation. The findings that LLMs plan ahead for the future in their hidden representations suggest potential applications for improving transparency and generation control.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation</title>
<link>https://arxiv.org/abs/2502.13207</link>
<guid>https://arxiv.org/abs/2502.13207</guid>
<content:encoded><![CDATA[
arXiv:2502.13207v2 Announce Type: replace 
Abstract: Despite the increasing use of large language models for creative tasks, their outputs often lack diversity. Common solutions, such as sampling at higher temperatures, can compromise the quality of the results. Dealing with this trade-off is still an open challenge in designing AI systems for creativity. Drawing on information theory, we propose a context-based score to quantitatively evaluate value and originality. This score incentivizes accuracy and adherence to the request while fostering divergence from the learned distribution. We show that our score can be used as a reward in a reinforcement learning framework to fine-tune large language models for maximum performance. We validate our strategy through experiments considering a variety of creative tasks, such as poetry generation and math problem solving, demonstrating that it enhances the value and originality of the generated solutions.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Question Answering over Large Semi-structured Tables</title>
<link>https://arxiv.org/abs/2502.13422</link>
<guid>https://arxiv.org/abs/2502.13422</guid>
<content:encoded><![CDATA[
arXiv:2502.13422v2 Announce Type: replace 
Abstract: Table Question Answering (TableQA) attracts strong interests due to the prevalence of web information presented in the form of semi-structured tables. Despite many efforts, TableQA over large tables remains an open challenge. This is because large tables may overwhelm models that try to comprehend them in full to locate question answers. Recent studies reduce input table size by decomposing tables into smaller, question-relevant sub-tables via generating programs to parse the tables. However, such solutions are subject to program generation and execution errors and are difficult to ensure decomposition quality. To address this issue, we propose TaDRe, a TableQA model that incorporates both pre- and post-table decomposition refinements to ensure table decomposition quality, hence achieving highly accurate TableQA results. To evaluate TaDRe, we construct two new large-table TableQA benchmarks via LLM-driven table expansion and QA pair generation. Extensive experiments on both the new and public benchmarks show that TaDRe achieves state-of-the-art performance on large-table TableQA tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation</title>
<link>https://arxiv.org/abs/2502.14037</link>
<guid>https://arxiv.org/abs/2502.14037</guid>
<content:encoded><![CDATA[
arXiv:2502.14037v3 Announce Type: replace 
Abstract: Despite their growing capabilities, language models still frequently reproduce content from their training data, generate repetitive text, and favor common grammatical patterns and vocabulary. A possible cause is the decoding strategy: the most common strategies either consider only the most probable tokens, which reduces output diversity, or increase the likelihood of unlikely tokens, compromising output accuracy and correctness. In this paper, we propose DiffSampling, a new decoding method that leverages a mathematical analysis of the token probability distribution to ensure the generation of contextually appropriate text. In particular, the difference between consecutive, sorted probabilities can be used to truncate incorrect tokens. In addition, we also propose two variations of the proposed method that aim to correct the subtle inconsistencies of common sampling strategies. Experiments involving four different text-generation tasks demonstrate that our approach consistently performs at least on par with the existing methods it builds upon in terms of quality, while potentially improving output diversity.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Control Illusion: The Failure of Instruction Hierarchies in Large Language Models</title>
<link>https://arxiv.org/abs/2502.15851</link>
<guid>https://arxiv.org/abs/2502.15851</guid>
<content:encoded><![CDATA[
arXiv:2502.15851v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly deployed with hierarchical instruction schemes, where certain instructions (e.g., system-level directives) are expected to take precedence over others (e.g., user messages). Yet, we lack a systematic understanding of how effectively these hierarchical control mechanisms work. We introduce a systematic evaluation framework based on constraint prioritization to assess how well LLMs enforce instruction hierarchies. Our experiments across six state-of-the-art LLMs reveal that models struggle with consistent instruction prioritization, even for simple formatting conflicts. We find that the widely-adopted system/user prompt separation fails to establish a reliable instruction hierarchy, and models exhibit strong inherent biases toward certain constraint types regardless of their priority designation. We find that LLMs more reliably obey constraints framed through natural social hierarchies (e.g., authority, expertise, consensus) than system/user roles, which suggests that pretraining-derived social structures act as latent control priors, with potentially stronger influence than post-training guardrails.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What are Foundation Models Cooking in the Post-Soviet World?</title>
<link>https://arxiv.org/abs/2502.18583</link>
<guid>https://arxiv.org/abs/2502.18583</guid>
<content:encoded><![CDATA[
arXiv:2502.18583v2 Announce Type: replace 
Abstract: The culture of the Post-Soviet states is complex, shaped by a turbulent history that continues to influence current events. In this study, we investigate the Post-Soviet cultural food knowledge of foundation models by constructing BORSch, a multimodal dataset encompassing 1147 and 823 dishes in the Russian and Ukrainian languages, centered around the Post-Soviet region. We demonstrate that leading models struggle to correctly identify the origins of dishes from Post-Soviet nations in both text-only and multimodal Question Answering (QA), instead over-predicting countries linked to the language the question is asked in. Through analysis of pretraining data, we show that these results can be explained by misleading dish-origin co-occurrences, along with linguistic phenomena such as Russian-Ukrainian code mixing. Finally, to move beyond QA-based assessments, we test models' abilities to produce accurate visual descriptions of dishes. The weak correlation between this task and QA suggests that QA alone may be insufficient as an evaluation of cultural understanding. To foster further research, we will make BORSch publicly available at https://github.com/alavrouk/BORSch.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Data Selection: The Data That Predicts Is the Data That Teaches</title>
<link>https://arxiv.org/abs/2503.00808</link>
<guid>https://arxiv.org/abs/2503.00808</guid>
<content:encoded><![CDATA[
arXiv:2503.00808v4 Announce Type: replace 
Abstract: Language model pretraining involves training on extensive corpora, where data quality plays a pivotal role. In this work, we aim to directly estimate the contribution of data during pretraining and select pretraining data in an efficient manner. Specifically, we draw inspiration from recent findings showing that compression efficiency (i.e., the normalized loss) of diverse models on certain text correlates strongly with their downstream performance, when the text domain aligns with the downstream benchmarks(Huang et al., 2024). Building on this observation, we hypothesize that data on which model losses are predictive of downstream abilities also contribute effectively to learning, which shares similar intuition with Thrush et al.(2024). To leverage this insight, we introduce predictive data selection (PreSelect), a lightweight and efficient data selection method that requires training and deploying only a fastText-based scorer. Through comprehensive experiments with 1B and 3B parameter models, we demonstrate that models trained on 30B tokens selected with PreSelect surpass the performance of the vanilla baseline trained on 300B tokens, achieving a 10x reduction in compute requirements. Furthermore, PreSelect significantly outperforms other competitive data selection baselines, such as DCLM and FineWeb-Edu on a scale of 3B models trained on 100B tokens. We open-source our trained data selection scorer along with the curated datasets at https://github.com/hkust-nlp/PreSelect.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Dynamic Clustering-Based Document Compression for Retrieval-Augmented-Generation</title>
<link>https://arxiv.org/abs/2504.03165</link>
<guid>https://arxiv.org/abs/2504.03165</guid>
<content:encoded><![CDATA[
arXiv:2504.03165v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach for knowledge injection during large language model (LLM) inference in recent years. However, due to their limited ability to exploit fine-grained inter-document relationships, current RAG implementations face challenges in effectively addressing the retrieved noise and redundancy content, which may cause error in the generation results. To address these limitations, we propose an Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG) that utilizes latent inter-document relationships while simultaneously removing irrelevant information and redundant content. We validate our approach, built upon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and Hallucination-Detection datasets. Experimental results show that our method achieves consistent performance improvements across various scenarios and experimental settings, demonstrating strong robustness and applicability. Our code and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Time Series Forecasting via Multi-Level Text Alignment with LLMs</title>
<link>https://arxiv.org/abs/2504.07360</link>
<guid>https://arxiv.org/abs/2504.07360</guid>
<content:encoded><![CDATA[
arXiv:2504.07360v2 Announce Type: replace 
Abstract: The adaptation of large language models (LLMs) to time series forecasting poses unique challenges, as time series data is continuous in nature, while LLMs operate on discrete tokens. Despite the success of LLMs in natural language processing (NLP) and other structured domains, aligning time series data with language-based representations while maintaining both predictive accuracy and interpretability remains a significant hurdle. Existing methods have attempted to reprogram time series data into text-based forms, but these often fall short in delivering meaningful, interpretable results. In this paper, we propose a multi-level text alignment framework for time series forecasting using LLMs that not only improves prediction accuracy but also enhances the interpretability of time series representations. Our method decomposes time series into trend, seasonal, and residual components, which are then reprogrammed into component-specific text representations. We introduce a multi-level alignment mechanism, where component-specific embeddings are aligned with pre-trained word tokens, enabling more interpretable forecasts. Experiments on multiple datasets demonstrate that our method outperforms state-of-the-art models in accuracy while providing good interpretability.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2504.13626</link>
<guid>https://arxiv.org/abs/2504.13626</guid>
<content:encoded><![CDATA[
arXiv:2504.13626v2 Announce Type: replace 
Abstract: Recent advancements in large reasoning models (LRMs) have demonstrated the effectiveness of scaling test-time computation to enhance reasoning capabilities on various tasks. However, LRMs often suffer from an ``overthinking'' problem, where the model generates excessively redundant reasoning steps with limited performance gains. In this work, we empirically reveal an important characteristic of LRM behaviors that placing external CoTs generated by smaller models between the thinking token (\texttt{} and \texttt{}) can effectively manipulate the model to generate fewer thoughts. Building on this finding, we propose a simple yet efficient pipeline, \Method, to enable LRMs to bypass unnecessary intermediate steps, thereby significantly reducing computational costs. We conduct extensive experiments to evaluate the utility and efficiency of \Method. For instance, when applied to QwQ-32B on the LiveBench/Code dataset, \Method keeps the original performance while reducing output token counts by approximately 30\%, with minimal overhead introduced by the CoT generator. Furthermore, we identify two suboptimal modes, blindly following flawed external thoughts and unnecessary rethinking, and show that simple mitigations, such as difficulty-aware fallbacks, can further improve performance. Overall, \Method offers a practical, general, and efficient way to optimize LRM inference, making powerful reasoning models more accessible and scalable for real-world applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agree to Disagree? A Meta-Evaluation of LLM Misgendering</title>
<link>https://arxiv.org/abs/2504.17075</link>
<guid>https://arxiv.org/abs/2504.17075</guid>
<content:encoded><![CDATA[
arXiv:2504.17075v2 Announce Type: replace 
Abstract: Numerous methods have been proposed to measure LLM misgendering, including probability-based evaluations (e.g., automatically with templatic sentences) and generation-based evaluations (e.g., with automatic heuristics or human validation). However, it has gone unexamined whether these evaluation methods have convergent validity, that is, whether their results align. Therefore, we conduct a systematic meta-evaluation of these methods across three existing datasets for LLM misgendering. We propose a method to transform each dataset to enable parallel probability- and generation-based evaluation. Then, by automatically evaluating a suite of 6 models from 3 families, we find that these methods can disagree with each other at the instance, dataset, and model levels, conflicting on 20.2% of evaluation instances. Finally, with a human evaluation of 2400 LLM generations, we show that misgendering behaviour is complex and goes far beyond pronouns, which automatic evaluations are not currently designed to capture, suggesting essential disagreement with human evaluations. Based on our findings, we provide recommendations for future evaluations of LLM misgendering. Our results are also more widely relevant, as they call into question broader methodological conventions in LLM evaluation, which often assume that different evaluation methods agree.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination</title>
<link>https://arxiv.org/abs/2505.00008</link>
<guid>https://arxiv.org/abs/2505.00008</guid>
<content:encoded><![CDATA[
arXiv:2505.00008v2 Announce Type: replace 
Abstract: Objective: This review aims to explore the potential and challenges of using Natural Language Processing (NLP) to detect, correct, and mitigate medically inaccurate information, including errors, misinformation, and hallucination. By unifying these concepts, the review emphasizes their shared methodological foundations and their distinct implications for healthcare. Our goal is to advance patient safety, improve public health communication, and support the development of more reliable and transparent NLP applications in healthcare.
  Methods: A scoping review was conducted following PRISMA guidelines, analyzing studies from 2020 to 2024 across five databases. Studies were selected based on their use of NLP to address medically inaccurate information and were categorized by topic, tasks, document types, datasets, models, and evaluation metrics.
  Results: NLP has shown potential in addressing medically inaccurate information on the following tasks: (1) error detection (2) error correction (3) misinformation detection (4) misinformation correction (5) hallucination detection (6) hallucination mitigation. However, challenges remain with data privacy, context dependency, and evaluation standards.
  Conclusion: This review highlights the advancements in applying NLP to tackle medically inaccurate information while underscoring the need to address persistent challenges. Future efforts should focus on developing real-world datasets, refining contextual methods, and improving hallucination management to ensure reliable and transparent healthcare applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do MLLMs Capture How Interfaces Guide User Behavior? A Benchmark for Multimodal UI/UX Design Understanding</title>
<link>https://arxiv.org/abs/2505.05026</link>
<guid>https://arxiv.org/abs/2505.05026</guid>
<content:encoded><![CDATA[
arXiv:2505.05026v3 Announce Type: replace 
Abstract: User interface (UI) design goes beyond visuals, guiding user behavior and overall user experience (UX). Strategically crafted interfaces, for example, can boost sign-ups and drive business sales, underscoring the shift toward UI/UX as a unified design concept. While recent studies have explored UI quality evaluation using Multimodal Large Language Models (MLLMs), they largely focus on surface-level features, overlooking behavior-oriented aspects. To fill this gap, we introduce WiserUI-Bench, a novel benchmark for assessing models' multimodal understanding of UI/UX design. It includes 300 diverse real-world UI image pairs, each consisting of two design variants A/B-tested at scale by actual companies, where one was empirically validated to steer more user actions than the other. Each pair is accompanied one or more of 684 expert-curated rationales that capture key factors behind each winning design's effectiveness, spanning diverse cognitive dimensions of UX. Our benchmark supports two core tasks: (1) selecting the more effective UI/UX design by predicting the A/B test verified winner and (2) assessing how well a model, given the winner, can explain its effectiveness in alignment with expert reasoning. Experiments across several MLLMs show that current models exhibit limited nuanced reasoning about UI/UX design and its behavioral impact. We believe our work will foster research in UI/UX understanding and enable broader applications such as behavior-aware interface optimization.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction</title>
<link>https://arxiv.org/abs/2505.10939</link>
<guid>https://arxiv.org/abs/2505.10939</guid>
<content:encoded><![CDATA[
arXiv:2505.10939v2 Announce Type: replace 
Abstract: Large language models often struggle with zero-shot generalization, and several modular approaches have been proposed to address this challenge. Yet, we hypothesize that a key limitation remains: the entanglement of general knowledge and task-specific adaptations. To overcome this, we propose a modular framework that disentangles these components by constructing a library of task-specific LoRA modules alongside a general-domain LoRA. By subtracting this general knowledge component from each task-specific module, we obtain residual modules that focus more exclusively on task-relevant information, a method we call general knowledge subtraction (GenKnowSub). Leveraging the refined task-specific modules and the Arrow routing algorithm \citep{ostapenko2024towards}, we dynamically select and combine modules for new inputs without additional training. Our studies on the Phi-3 model and standard Arrow as baselines reveal that using general knowledge LoRAs derived from diverse languages, including English, French, and German, yields consistent performance gains in both monolingual and cross-lingual settings across a wide set of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub generalizes to weaker LLMs. The complete code and data are available at https://github.com/saharsamr/Modular-LLM.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XtraGPT: Context-Aware and Controllable Academic Paper Revision via Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2505.11336</link>
<guid>https://arxiv.org/abs/2505.11336</guid>
<content:encoded><![CDATA[
arXiv:2505.11336v2 Announce Type: replace 
Abstract: Despite the growing adoption of large language models (LLMs) in academic workflows, their capabilities remain limited when it comes to supporting high-quality scientific writing. Most existing systems are designed for general-purpose scientific text generation and fail to meet the sophisticated demands of research communication beyond surface-level polishing, such as conceptual coherence across sections. Furthermore, academic writing is inherently iterative and revision-driven, a process not well supported by direct prompting-based paradigms. To address these scenarios, we propose a human-AI collaboration framework for academic paper revision. We first introduce a comprehensive dataset of 7,040 research papers from top-tier venues annotated with over 140,000 instruction-response pairs that reflect realistic, section-level scientific revisions. Building on the dataset, we develop XtraGPT, the first suite of open-source LLMs, designed to provide context-aware, instruction-guided writing assistance, ranging from 1.5B to 14B parameters. Extensive experiments validate that XtraGPT significantly outperforms same-scale baselines and approaches the quality of proprietary systems. Both automated preference assessments and human evaluations confirm the effectiveness of our models in improving scientific drafts.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing</title>
<link>https://arxiv.org/abs/2505.11935</link>
<guid>https://arxiv.org/abs/2505.11935</guid>
<content:encoded><![CDATA[
arXiv:2505.11935v2 Announce Type: replace 
Abstract: Although multimodal large language models (MLLMs) show promise in generating chart rendering code, editing charts via code presents a greater challenge. This task demands MLLMs to integrate chart understanding and reasoning capacities, which are labor-intensive. While many MLLMs claim such editing capabilities, current evaluations rely on limited case studies, highlighting the urgent need for a comprehensive evaluation framework. In this work, we propose \textsc{ChartEdit}, a novel benchmark designed for chart editing tasks, featuring $1405$ diverse editing instructions applied to $233$ real-world charts, each manually annotated and validated for accuracy. Utilizing \textsc{ChartEdit}, we evaluate the performance of 10 mainstream MLLMs across two types of experiments at both the code and chart levels. The results suggest that large-scale models can generate code to produce images that partially match the reference images. However, their ability to generate accurate edits according to the instructions remains limited. The state-of-the-art (SOTA) model achieves a score of only $59.96$, highlighting significant challenges in precise modification. In contrast, small-scale models, including chart-domain models, struggle both with following editing instructions and generating overall chart images, underscoring the need for further development in this area. Code is available at https://github.com/xxlllz/ChartEdit.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Generalization vs Fidelity Paradox in Knowledge Distillation</title>
<link>https://arxiv.org/abs/2505.15442</link>
<guid>https://arxiv.org/abs/2505.15442</guid>
<content:encoded><![CDATA[
arXiv:2505.15442v2 Announce Type: replace 
Abstract: Knowledge distillation (KD) is a key technique for compressing large language models into smaller ones while preserving performance. Despite the recent traction of KD research, its effectiveness for smaller language models (LMs) and the mechanisms driving knowledge transfer remain underexplored. In this work, we present the first large-scale empirical and statistical analysis of KD across models ranging from 0.5B to 7B parameters on 14 complex reasoning tasks in a zero-shot setting. Our findings reveal that KD can improve the average performance of smaller models by up to $10\%$, with a peak task specific gain of $22\%$, while providing only marginal benefits ($\sim 1.3\%$) for larger models. Surprisingly, teacher performance has a minimal impact on student outcomes, while teacher task expertise impacts KD effectiveness. A correlation study indicates that smaller LMs benefit more from KD, whereas larger LMs show diminished gains. Additionally, we uncover a misalignment between improvements in student performance and reasoning fidelity, suggesting that while KD enhances accuracy, it does not always maintain the structured decision-making processes of the teacher. Our ablation study further highlights the importance of teacher signals and logit smoothing in influencing students' performance after distillation. Overall, our study offers a comprehensive empirical and statistical assessment of KD, highlighting both its benefits and trade-offs when distilling knowledge from larger to smaller LMs.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Tokens Are What You Need In Thinking</title>
<link>https://arxiv.org/abs/2505.17827</link>
<guid>https://arxiv.org/abs/2505.17827</guid>
<content:encoded><![CDATA[
arXiv:2505.17827v2 Announce Type: replace 
Abstract: Modern reasoning models, such as OpenAI's o1 and DeepSeek-R1, exhibit impressive problem-solving capabilities but suffer from critical inefficiencies: high inference latency, excessive computational resource consumption, and a tendency toward overthinking -- generating verbose chains of thought (CoT) laden with redundant tokens that contribute minimally to the final answer. To address these issues, we propose Conditional Token Selection (CTS), a token-level compression framework with a flexible and variable compression ratio that identifies and preserves only the most essential tokens in CoT. CTS evaluates each token's contribution to deriving correct answers using conditional importance scoring, then trains models on compressed CoT. Extensive experiments demonstrate that CTS effectively compresses long CoT while maintaining strong reasoning performance. Notably, on the GPQA benchmark, Qwen2.5-14B-Instruct trained with CTS achieves a 9.1% accuracy improvement with 13.2% fewer reasoning tokens (13% training token reduction). Further reducing training tokens by 42% incurs only a marginal 5% accuracy drop while yielding a 75.8% reduction in reasoning tokens, highlighting the prevalence of redundancy in existing CoT.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shifting AI Efficiency From Model-Centric to Data-Centric Compression</title>
<link>https://arxiv.org/abs/2505.19147</link>
<guid>https://arxiv.org/abs/2505.19147</guid>
<content:encoded><![CDATA[
arXiv:2505.19147v2 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) and multi-modal LLMs (MLLMs) has historically relied on model-centric scaling through increasing parameter counts from millions to hundreds of billions to drive performance gains. However, as we approach hardware limits on model size, the dominant computational bottleneck has fundamentally shifted to the quadratic cost of self-attention over long token sequences, now driven by ultra-long text contexts, high-resolution images, and extended videos. In this position paper, \textbf{we argue that the focus of research for efficient AI is shifting from model-centric compression to data-centric compression}. We position token compression as the new frontier, which improves AI efficiency via reducing the number of tokens during model training or inference. Through comprehensive analysis, we first examine recent developments in long-context AI across various domains and establish a unified mathematical framework for existing model efficiency strategies, demonstrating why token compression represents a crucial paradigm shift in addressing long-context overhead. Subsequently, we systematically review the research landscape of token compression, analyzing its fundamental benefits and identifying its compelling advantages across diverse scenarios. Furthermore, we provide an in-depth analysis of current challenges in token compression research and outline promising future directions. Ultimately, our work aims to offer a fresh perspective on AI efficiency, synthesize existing research, and catalyze innovative developments to address the challenges that increasing context lengths pose to the AI community's advancement.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It's High Time: A Survey of Temporal Question Answering</title>
<link>https://arxiv.org/abs/2505.20243</link>
<guid>https://arxiv.org/abs/2505.20243</guid>
<content:encoded><![CDATA[
arXiv:2505.20243v3 Announce Type: replace 
Abstract: Time plays a critical role in how information is generated, retrieved, and interpreted. In this survey, we provide a comprehensive overview of Temporal Question Answering (TQA), a research area that focuses on answering questions involving temporal constraints or context. As the amount of time-stamped content from sources like news articles, web archives, and knowledge bases increases, systems must address challenges such as detecting temporal intent, normalizing time expressions, ordering events, and reasoning over evolving or ambiguous facts. We focus on recent advances in TQA enabled by neural architectures, especially transformer-based models and Large Language Models (LLMs), highlighting progress in temporal language modeling, retrieval-augmented generation (RAG), and temporal reasoning. We also discuss benchmark datasets and evaluation strategies designed to test temporal robustness, recency awareness, and generalization.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affordance Benchmark for MLLMs</title>
<link>https://arxiv.org/abs/2506.00893</link>
<guid>https://arxiv.org/abs/2506.00893</guid>
<content:encoded><![CDATA[
arXiv:2506.00893v2 Announce Type: replace 
Abstract: Affordance theory suggests that environments inherently provide action possibilities shaping perception and behavior. While Multimodal Large Language Models (MLLMs) achieve strong performance in vision-language tasks, their ability to perceive affordance, which is crucial for intuitive and safe interactions, remains underexplored. To address this, we introduce **A4Bench**, a novel benchmark designed to evaluate the affordance perception abilities of MLLMs across two dimensions: 1) Constitutive Affordance, assessing understanding of inherent object properties through 1,282 questionanswer pairs spanning nine sub-disciplines, and 2) Transformative Affordance, probing dynamic and contextual nuances (e.g., misleading, time-dependent, cultural, or individual-specific affordance) with 718 challenging question-answer pairs. We evaluate 17 MLLMs (nine proprietary and eight open-source) and compare them to human performance. Results show that proprietary models generally outperform open-source ones, yet all models perform far below humans, especially in transformative affordance. Furthermore, even top-performing models, such as Gemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag behind human performance (best: 85.34%, worst: 81.25%). These findings highlight critical gaps in environmental understanding of MLLMs and provide a foundation for advancing AI systems toward more robust, context-aware interactions.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leaps Beyond the Seen: Reinforced Reasoning Augmented Generation for Clinical Notes</title>
<link>https://arxiv.org/abs/2506.05386</link>
<guid>https://arxiv.org/abs/2506.05386</guid>
<content:encoded><![CDATA[
arXiv:2506.05386v2 Announce Type: replace 
Abstract: Clinical note generation aims to produce free-text summaries of a patient's condition and diagnostic process, with discharge instructions being a representative long-form example. While recent LLM-based methods pre-trained on general clinical corpora show promise in clinical text generation, they fall short in producing long-form notes from limited patient information. In this paper, we propose ReinRAG, a reinforced reasoning augmented generation (RAG) for long-form discharge instructions based on pre-admission information. ReinRAG retrieves reasoning paths from a medical knowledge graph to provide explicit semantic guidance to the LLM. To bridge the information gap, we propose group-based retriever optimization (GRO) which improves retrieval quality with group-normalized rewards, encouraging reasoning leaps for deeper inference by the LLM. Comprehensive experiments on the real-world dataset show that ReinRAG outperforms baselines in both clinical efficacy and natural language generation metrics. Further analysis reveals that ReinRAG fills semantic gaps in sparse input scenarios, and retrieved reasoning paths help LLMs avoid clinical misinterpretation by focusing on key evidence and following coherent reasoning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval</title>
<link>https://arxiv.org/abs/2506.08625</link>
<guid>https://arxiv.org/abs/2506.08625</guid>
<content:encoded><![CDATA[
arXiv:2506.08625v2 Announce Type: replace 
Abstract: Scientific reasoning requires not only long-chain reasoning processes, but also knowledge of domain-specific terminologies and adaptation to updated findings. To deal with these challenges for scientific reasoning, we introduce RAISE, a step-by-step retrieval-augmented framework which retrieves logically relevant documents from in-the-wild corpus. RAISE is divided into three steps: problem decomposition, logical query generation, and logical retrieval. We observe that RAISE consistently outperforms other baselines on scientific reasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves documents that are not only similar in terms of the domain knowledge, but also documents logically more relevant.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extrapolation by Association: Length Generalization Transfer in Transformers</title>
<link>https://arxiv.org/abs/2506.09251</link>
<guid>https://arxiv.org/abs/2506.09251</guid>
<content:encoded><![CDATA[
arXiv:2506.09251v2 Announce Type: replace 
Abstract: Transformer language models have demonstrated impressive generalization capabilities in natural language domains, yet we lack a fine-grained understanding of how such generalization arises. In this paper, we investigate length generalization--the ability to extrapolate from shorter to longer inputs--through the lens of \textit{task association}. We find that length generalization can be \textit{transferred} across related tasks. That is, training a model with a longer and related auxiliary task can lead it to generalize to unseen and longer inputs from some other target task. We demonstrate this length generalization transfer across diverse algorithmic tasks, including arithmetic operations, string transformations, and maze navigation. Our results show that transformer models can inherit generalization capabilities from similar tasks when trained jointly. Moreover, we observe similar transfer effects in pretrained language models, suggesting that pretraining equips models with reusable computational scaffolding that facilitates extrapolation in downstream settings. Finally, we provide initial mechanistic evidence that length generalization transfer correlates with the re-use of the same attention heads between the tasks. Together, our findings deepen our understanding of how transformers generalize to out-of-distribution inputs and highlight the compositional reuse of inductive structure across tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning with Exploration: An Entropy Perspective on Reinforcement Learning for LLMs</title>
<link>https://arxiv.org/abs/2506.14758</link>
<guid>https://arxiv.org/abs/2506.14758</guid>
<content:encoded><![CDATA[
arXiv:2506.14758v2 Announce Type: replace 
Abstract: Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing large language model (LLM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LLMs. Through empirical analysis, we uncover positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LLMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LLM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LLM reasoning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning</title>
<link>https://arxiv.org/abs/2506.16123</link>
<guid>https://arxiv.org/abs/2506.16123</guid>
<content:encoded><![CDATA[
arXiv:2506.16123v2 Announce Type: replace 
Abstract: This paper presents FinCoT, a structured chain-of-thought (CoT) prompting framework that embeds domain-specific expert financial reasoning blueprints to guide large language models' behaviors. We identify three main prompting styles in financial NLP (FinNLP): (1) standard prompting (zero-shot), (2) unstructured CoT (free-form reasoning), and (3) structured CoT (with explicitly structured reasoning steps). Prior work has mainly focused on the first two, while structured CoT remains underexplored and lacks domain expertise incorporation. Therefore, we evaluate all three prompting approaches across ten CFA-style financial domains and introduce FinCoT as the first structured finance-specific prompting approach incorporating blueprints from domain experts. FinCoT improves the accuracy of a general-purpose model, Qwen3-8B-Base, from 63.2% to 80.5%, and boosts Fin-R1 (7B), a finance-specific model, from 65.7% to 75.7%, while reducing output length by up to 8.9x and 1.16x compared to structured CoT methods, respectively. We find that FinCoT proves most effective for models lacking financial post-training. Our findings show that FinCoT does not only improve performance and reduce inference costs but also yields more interpretable and expert-aligned reasoning traces.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning</title>
<link>https://arxiv.org/abs/2506.16792</link>
<guid>https://arxiv.org/abs/2506.16792</guid>
<content:encoded><![CDATA[
arXiv:2506.16792v2 Announce Type: replace 
Abstract: Despite efforts to align large language models (LLMs) with societal and moral values, these models remain susceptible to jailbreak attacks -- methods designed to elicit harmful responses. Jailbreaking black-box LLMs is considered challenging due to the discrete nature of token inputs, restricted access to the target LLM, and limited query budget. To address the issues above, we propose an effective method for jailbreaking black-box large language Models via Iterative Semantic Tuning, named MIST. MIST enables attackers to iteratively refine prompts that preserve the original semantic intent while inducing harmful content. Specifically, to balance semantic similarity with computational efficiency, MIST incorporates two key strategies: sequential synonym search, and its advanced version -- order-determining optimization. We conduct extensive experiments on two datasets using two open-source and four closed-source models. Results show that MIST achieves competitive attack success rate, relatively low query count, and fair transferability, outperforming or matching state-of-the-art jailbreak methods. Additionally, we conduct analysis on computational efficiency to validate the practical viability of MIST.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Reasoning Help Large Language Models Capture Human Annotator Disagreement?</title>
<link>https://arxiv.org/abs/2506.19467</link>
<guid>https://arxiv.org/abs/2506.19467</guid>
<content:encoded><![CDATA[
arXiv:2506.19467v2 Announce Type: replace 
Abstract: Variation in human annotation (i.e., disagreements) is common in NLP, often reflecting important information like task subjectivity and sample ambiguity. Modeling this variation is important for applications that are sensitive to such information. Although RLVR-style reasoning (Reinforcement Learning with Verifiable Rewards) has improved Large Language Model (LLM) performance on many tasks, it remains unclear whether such reasoning enables LLMs to capture informative variation in human annotation. In this work, we evaluate the influence of different reasoning settings on LLM disagreement modeling. We systematically evaluate each reasoning setting across model sizes, distribution expression methods, and steering methods, resulting in 60 experimental setups across 3 tasks. Surprisingly, our results show that RLVR-style reasoning degrades performance in disagreement modeling, while naive Chain-of-Thought (CoT) reasoning improves the performance of RLHF LLMs (RL from human feedback). These findings underscore the potential risk of replacing human annotators with reasoning LLMs, especially when disagreements are important.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction</title>
<link>https://arxiv.org/abs/2506.21562</link>
<guid>https://arxiv.org/abs/2506.21562</guid>
<content:encoded><![CDATA[
arXiv:2506.21562v2 Announce Type: replace 
Abstract: In the architectural design process, floor plan generation is inherently progressive and iterative. However, existing generative models for floor plans are predominantly end-to-end generation that produce an entire pixel-based layout in a single pass. This paradigm is often incompatible with the incremental workflows observed in real-world architectural practice. To address this issue, we draw inspiration from the autoregressive 'next token prediction' mechanism commonly used in large language models, and propose a novel 'next room prediction' paradigm tailored to architectural floor plan modeling. Experimental evaluation indicates that FPDS demonstrates competitive performance in comparison to diffusion models and Tell2Design in the text-to-floorplan task, indicating its potential applicability in supporting future intelligent architectural design.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinguaSynth: Heterogeneous Linguistic Signals for News Classification</title>
<link>https://arxiv.org/abs/2506.21848</link>
<guid>https://arxiv.org/abs/2506.21848</guid>
<content:encoded><![CDATA[
arXiv:2506.21848v3 Announce Type: replace 
Abstract: Deep learning has significantly advanced NLP, but its reliance on large black-box models introduces critical interpretability and computational efficiency concerns. This paper proposes LinguaSynth, a novel text classification framework that strategically integrates five complementary linguistic feature types: lexical, syntactic, entity-level, word-level semantics, and document-level semantics within a transparent logistic regression model. Unlike transformer-based architectures, LinguaSynth maintains interpretability and computational efficiency, achieving an accuracy of 84.89 percent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by 3.32 percent. Through rigorous feature interaction analysis, we show that syntactic and entity-level signals provide essential disambiguation and effectively complement distributional semantics. LinguaSynth sets a new benchmark for interpretable, resource-efficient NLP models and challenges the prevailing assumption that deep neural networks are necessary for high-performing text classification.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What to Keep and What to Drop: Adaptive Table Filtering Framework</title>
<link>https://arxiv.org/abs/2506.23463</link>
<guid>https://arxiv.org/abs/2506.23463</guid>
<content:encoded><![CDATA[
arXiv:2506.23463v3 Announce Type: replace 
Abstract: Large language models (LLMs) for table-based reasoning often struggle with large tables due to input length limits. We propose ATF (Adaptive Table Filtering Framework), a modular and question-aware filtering pipeline that prunes uninformative columns and rows using LLM-generated column descriptions, clustering, and sparse-dense alignment scores. ATF integrates seamlessly with existing models (e.g., TAPAS, TAPEX) without retraining. Experiments show that ATF reduces table cells by 70%, boosting performance on out-of-domain TableQA tasks while causing slight performance drops on Table Fact Verification, where full-table context is more critical. These results highlight ATF's ability to adaptively balance informativeness and minimalism across tasks. Our code available at: https://github.com/torijune/ATF-Adaptive-Table-Filtering-Framework
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizing Verifiable Instruction Following</title>
<link>https://arxiv.org/abs/2507.02833</link>
<guid>https://arxiv.org/abs/2507.02833</guid>
<content:encoded><![CDATA[
arXiv:2507.02833v2 Announce Type: replace 
Abstract: A crucial factor for successful human and AI interaction is the ability of language models or chatbots to follow human instructions precisely. A common feature of instructions are output constraints like ``only answer with yes or no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to craft a more useful answer. Even today's strongest models struggle with fulfilling such constraints. We find that most models strongly overfit on a small set of verifiable constraints from the benchmarks that test these abilities, a skill called precise instruction following, and are not able to generalize well to unseen output constraints. We introduce a new benchmark, IFBench, to evaluate precise instruction following generalization on 58 new, diverse, and challenging verifiable out-of-domain constraints. In addition, we perform an extensive analysis of how and on what data models can be trained to improve precise instruction following generalization. Specifically, we carefully design constraint verification modules and show that reinforcement learning with verifiable rewards (RLVR) significantly improves instruction following. In addition to IFBench, we release 29 additional new hand-annotated training constraints and verification functions, RLVR training prompts, and code.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexOlmo: Open Language Models for Flexible Data Use</title>
<link>https://arxiv.org/abs/2507.07024</link>
<guid>https://arxiv.org/abs/2507.07024</guid>
<content:encoded><![CDATA[
arXiv:2507.07024v3 Announce Type: replace 
Abstract: We introduce FlexOlmo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners, leading to an average 41% relative improvement while allowing users to opt out of certain data based on data licensing or permission requirements. Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, this research presents a solution for both data owners and researchers in regulated industries with sensitive or protected data. FlexOlmo enables benefiting from closed data while respecting data owners' preferences by keeping their data local and supporting fine-grained control of data access during inference.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distillation versus Contrastive Learning: How to Train Your Rerankers</title>
<link>https://arxiv.org/abs/2507.08336</link>
<guid>https://arxiv.org/abs/2507.08336</guid>
<content:encoded><![CDATA[
arXiv:2507.08336v2 Announce Type: replace 
Abstract: Training effective text rerankers is crucial for information retrieval. Two strategies are widely used: contrastive learning (optimizing directly on ground-truth labels) and knowledge distillation (transferring knowledge from a larger reranker). While both have been studied extensively, a clear comparison of their effectiveness for training cross-encoder rerankers under practical conditions is needed.
  This paper empirically compares these strategies by training rerankers of different sizes and architectures using both methods on the same data, with a strong contrastive learning model acting as the distillation teacher. Our results show that knowledge distillation generally yields better in-domain and out-of-domain ranking performance than contrastive learning when distilling from a larger teacher model. This finding is consistent across student model sizes and architectures. However, distilling from a teacher of the same capacity does not provide the same advantage, particularly for out-of-domain tasks. These findings offer practical guidance for choosing a training strategy based on available teacher models. We recommend using knowledge distillation to train smaller rerankers if a larger, more powerful teacher is accessible; in its absence, contrastive learning remains a robust baseline.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models</title>
<link>https://arxiv.org/abs/2507.09506</link>
<guid>https://arxiv.org/abs/2507.09506</guid>
<content:encoded><![CDATA[
arXiv:2507.09506v2 Announce Type: replace 
Abstract: Long-context language models (LCLMs) have exhibited impressive capabilities in long-context understanding tasks. Among these, long-context referencing -- a crucial task that requires LCLMs to attribute items of interest to specific parts of long-context data -- remains underexplored. To bridge this gap, this paper proposes Referencing Evaluation for Long-context Language Models (Ref-Long), a novel benchmark designed to assess the long-context referencing capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the indexes of documents that reference a specific key, emphasizing contextual relationships between the key and the documents over simple retrieval. Based on the task design, we construct three subsets ranging from synthetic to realistic scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs reveal significant shortcomings in long-context referencing, even among advanced models like GPT-4o. To further investigate these challenges, we conduct comprehensive analyses, including human evaluations, task format adjustments, fine-tuning experiments, and error analyses, leading to several key insights. Our data and code can be found in https://github. com/wujunjie1998/Ref-Long.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs on Trial: Evaluating Judicial Fairness for Large Language Models</title>
<link>https://arxiv.org/abs/2507.10852</link>
<guid>https://arxiv.org/abs/2507.10852</guid>
<content:encoded><![CDATA[
arXiv:2507.10852v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly used in high-stakes fields where their decisions impact rights and equity. However, LLMs' judicial fairness and implications for social justice remain underexplored. When LLMs act as judges, the ability to fairly resolve judicial issues is a prerequisite to ensure their trustworthiness. Based on theories of judicial fairness, we construct a comprehensive framework to measure LLM fairness, leading to a selection of 65 labels and 161 corresponding values. Applying this framework to the judicial system, we compile an extensive dataset, JudiFair, comprising 177,100 unique case facts. To achieve robust statistical inference, we develop three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and introduce a method to assess the overall fairness of multiple LLMs across various labels. Through experiments with 16 LLMs, we uncover pervasive inconsistency, bias, and imbalanced inaccuracy across models, underscoring severe LLM judicial unfairness. Particularly, LLMs display notably more pronounced biases on demographic labels, with slightly less bias on substance labels compared to procedure ones. Interestingly, increased inconsistency correlates with reduced biases, but more accurate predictions exacerbate biases. While we find that adjusting the temperature parameter can influence LLM fairness, model size, release date, and country of origin do not exhibit significant effects on judicial fairness. Accordingly, we introduce a publicly available toolkit containing all datasets and code, designed to support future research in evaluating and improving LLM fairness.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description</title>
<link>https://arxiv.org/abs/2405.18937</link>
<guid>https://arxiv.org/abs/2405.18937</guid>
<content:encoded><![CDATA[
arXiv:2405.18937v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce Part-Aware Point Grounded Description (PaPGD), a challenging task aimed at advancing 3D multimodal learning for fine-grained, part-aware segmentation grounding and detailed explanation of 3D objects. Existing 3D datasets largely focus on either vision-only part segmentation or vision-language scene segmentation, lacking the fine-grained multimodal segmentation needed for robotic navigation and interaction in real-world environments. To address this gap, we present the 3DCoMPaT Grounded Instructions (3DCoMPaT-GrIn) Dataset, a comprehensive resource that pairs rich point cloud descriptions with corresponding part-level segmentation masks. This dataset encompasses extensive samples designed for both PaPGD and fine-grained single-part grounding tasks. To tackle the inherent challenges of grounding objects and generating grounded descriptions at the part level, we propose Kestrel, a part-aware 3D multimodal large language model that integrates an advanced language model for nuanced language comprehension with multi-level point feature propagation and query refinement mechanism to enhance spatial reasoning at the part level. The extensive experiments demonstrate that Kestrel effectively bridges the gap between part-aware language understanding and 3D segmentation grounding, paving the way for more robust and interpretable 3D object comprehension that meets the demands of real-world robotic applications. Project page at https://feielysia.github.io/Kestrel.github.io/
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoLLaMB: Long Streaming Video Understanding with Recurrent Memory Bridges</title>
<link>https://arxiv.org/abs/2409.01071</link>
<guid>https://arxiv.org/abs/2409.01071</guid>
<content:encoded><![CDATA[
arXiv:2409.01071v2 Announce Type: replace-cross 
Abstract: Recent advancements in large-scale video-language models have shown significant potential for real-time planning and detailed interactions. However, their high computational demands and the scarcity of annotated datasets limit their practicality for academic researchers. In this work, we introduce VideoLLaMB, a novel and efficient framework for long video understanding that leverages recurrent memory bridges and temporal memory tokens to enable seamless encoding of entire video sequences with preserved semantic continuity. Central to our approach is a SceneTiling algorithm that segments videos into coherent semantic units, facilitating robust understanding across tasks without requiring additional training. VideoLLaMB achieves state-of-the-art performance, surpassing existing models by 4.2 points on four VideoQA benchmarks and by 2.06 points on egocentric planning tasks. Notably, it maintains strong performance under extreme video length scaling (up to 8 times) and excels at fine-grained frame retrieval on our proposed Needle in a Video Haystack (NIAVH) benchmark. With linear GPU memory scaling, VideoLLaMB processes up to 320 frames using a single Nvidia A100 GPU, despite being trained on only 16 frames-offering an unprecedented balance of accuracy, scalability, and cost-effectiveness. This makes it highly accessible and practical for the academic community.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining Test-Time Adaptation for Personalized Child Speech Recognition</title>
<link>https://arxiv.org/abs/2409.13095</link>
<guid>https://arxiv.org/abs/2409.13095</guid>
<content:encoded><![CDATA[
arXiv:2409.13095v2 Announce Type: replace-cross 
Abstract: Automatic speech recognition (ASR) models often experience performance degradation due to data domain shifts introduced at test time, a challenge that is further amplified for child speakers. Test-time adaptation (TTA) methods have shown great potential in bridging this domain gap. However, the use of TTA to adapt ASR models to the individual differences in each child's speech has not yet been systematically studied. In this work, we investigate the effectiveness of two widely used TTA methods-SUTA, SGEM-in adapting off-the-shelf ASR models and their fine-tuned versions for child speech recognition, with the goal of enabling continuous, unsupervised adaptation at test time. Our findings show that TTA significantly improves the performance of both off-the-shelf and fine-tuned ASR models, both on average and across individual child speakers, compared to unadapted baselines. However, while TTA helps adapt to individual variability, it may still be limited with non-linguistic child speech.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-based Audio Moment Retrieval</title>
<link>https://arxiv.org/abs/2409.15672</link>
<guid>https://arxiv.org/abs/2409.15672</guid>
<content:encoded><![CDATA[
arXiv:2409.15672v3 Announce Type: replace-cross 
Abstract: In this paper, we propose and design a new task called audio moment retrieval (AMR). Unlike conventional language-based audio retrieval tasks that search for short audio clips from an audio database, AMR aims to predict relevant moments in untrimmed long audio based on a text query. Given the lack of prior work in AMR, we first build a dedicated dataset, Clotho-Moment, consisting of large-scale simulated audio recordings with moment annotations. We then propose a DETR-based model, named Audio Moment DETR (AM-DETR), as a fundamental framework for AMR tasks. This model captures temporal dependencies within audio features, inspired by similar video moment retrieval tasks, thus surpassing conventional clip-level audio retrieval methods. Additionally, we provide manually annotated datasets to properly measure the effectiveness and robustness of our methods on real data. Experimental results show that AM-DETR, trained with Clotho-Moment, outperforms a baseline model that applies a clip-level audio retrieval method with a sliding window on all metrics, particularly improving Recall1@0.7 by 9.00 points. Our datasets and code are publicly available in https://h-munakata.github.io/Language-based-Audio-Moment-Retrieval.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CheXalign: Preference fine-tuning in chest X-ray interpretation models without human feedback</title>
<link>https://arxiv.org/abs/2410.07025</link>
<guid>https://arxiv.org/abs/2410.07025</guid>
<content:encoded><![CDATA[
arXiv:2410.07025v3 Announce Type: replace-cross 
Abstract: Radiologists play a crucial role in translating medical images into actionable reports. However, the field faces staffing shortages and increasing workloads. While automated approaches using vision-language models (VLMs) show promise as assistants, they require exceptionally high accuracy. Most current VLMs in radiology rely solely on supervised fine-tuning. Meanwhile, additional preference fine-tuning in the post-training pipeline has become standard practice in the general domain. The challenge in radiology lies in the prohibitive cost of obtaining radiologist feedback at scale. To address this challenge, we propose an automated pipeline for preference feedback, focusing on chest X-ray radiology report generation (RRG). Specifically, our method leverages publicly available datasets containing pairs of images and radiologist-written reference reports with reference-based metrics, or Judges, eliminating the need for additional radiologist feedback. We investigate reward overoptimization via length exploitation in this setting and introduce a length-controlled version of the GREEN score. Our best-performing setup achieves state-of-the-art CheXbert scores on the MIMIC-CXR dataset for the RRG task while on average maintaining robust performance across six additional image perception and reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More than Memes: A Multimodal Topic Modeling Approach to Conspiracy Theories on Telegram</title>
<link>https://arxiv.org/abs/2410.08642</link>
<guid>https://arxiv.org/abs/2410.08642</guid>
<content:encoded><![CDATA[
arXiv:2410.08642v3 Announce Type: replace-cross 
Abstract: To address the increasing prevalence of (audio-)visual data on social media, and to capture the evolving and dynamic nature of this communication, researchers have begun to explore the potential of unsupervised approaches for analyzing multimodal online content. However, existing research often neglects visual content beyond memes, and in addition lacks methods to compare topic models across modalities. Our study addresses these gaps by applying multimodal topic modeling for analyzing conspiracy theories in German-language Telegram channels. We use BERTopic with CLIP for the analysis of textual and visual data in a corpus of ~40, 000 Telegram messages posted in October 2023 in 571 German-language Telegram channels known for disseminating conspiracy theories. Through this dataset, we provide insights into unimodal and multimodal topic models by analyzing symmetry and intersections of topics across modalities. We demonstrate the variety of textual and visual content shared in the channels discovered through the topic modeling, and propose a conceptual framework for the analysis of textual and visual discursive strategies in the communication of conspiracy theories. We apply the framework in a case study of the topic group Israel Gaza.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiDoRA: Bi-level Optimization-Based Weight-Decomposed Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2410.09758</link>
<guid>https://arxiv.org/abs/2410.09758</guid>
<content:encoded><![CDATA[
arXiv:2410.09758v2 Announce Type: replace-cross 
Abstract: Parameter-efficient fine-tuning (PEFT) is a flexible and efficient method for adapting large language models (LLMs) to downstream tasks. Among these methods, weight-decomposed low-rank adaptation (DoRA) is a promising approach that decomposes weight matrices into magnitude and direction components to mimic full fine-tuning (FT) better. However, DoRA's simultaneous optimization of these components makes it over-expressive, increases the risk of overfitting, and creates a coupled updating pattern that limits its learning capacity. To address these issues, we propose Bi-level Optimization-Based Weight-Decomposed Low-Rank Adaptation (BiDoRA), a novel PEFT method based on a bi-level optimization framework. BiDoRA fundamentally differs from DoRA by optimizing the magnitude and direction in two separate, asynchronous loops using distinct training and validation data splits. This decoupled optimization process effectively mitigates overfitting and allows for more flexible updates that align even more closely with FT. For instance, weight decomposition analysis shows BiDoRA achieves a magnitude-direction update correlation of $-8.042$, significantly closer to the FT ideal compared to $-1.784$ for DoRA. Evaluation of BiDoRA on diverse tasks spanning natural language understanding, generation, token classification, and extremely small biomedical datasets reveals that it consistently outperforms DoRA and a wide range of leading PEFT methods. This improvement is statistically significant, as demonstrated on the GLUE benchmark where BiDoRA surpasses DoRA with a p-value of $2.4\times10^{-4}$ in terms of the Wilcoxon signed-rank test. The code for BiDoRA is available at https://github.com/t2ance/BiDoRA.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Efficient Fine-tuning of LLMs with Bayesian Reparameterization of Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2411.04358</link>
<guid>https://arxiv.org/abs/2411.04358</guid>
<content:encoded><![CDATA[
arXiv:2411.04358v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are highly resource-intensive to fine-tune due to their enormous size. While low-rank adaptation is a prominent parameter-efficient fine-tuning approach, it suffers from sensitivity to hyperparameter choices, leading to instability in model performance on fine-tuning downstream tasks. This paper highlights the importance of effective parameterization in low-rank fine-tuning to reduce estimator variance and enhance the stability of final model outputs. We propose MonteCLoRA, an efficient fine-tuning technique that employs Monte Carlo estimation to learn an unbiased posterior estimation of low-rank parameters with low expected variance, stabilizing fine-tuned LLMs with only O(r) additional parameters, for a given rank r. MonteCLoRA shows 0.5% and 1.6% improvements in accuracy and robustness over unregularized low-rank adaptation method on natural language understanding tasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with pre-trained LLaMA-1-7B and LLaMA-3.2-3B-Instruct, MonteCLoRA demonstrates robust performance with 50% and 62% lower spreads respectively than the contemporary efficient fine-tuning methods. The theoretical and empirical results presented in the paper underscore how parameterization and hyperpriors balance exploration-exploitation in the low-rank parametric space, therefore leading to more optimal and robust parameter estimation during efficient fine-tuning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Guide to Misinformation Detection Data and Evaluation</title>
<link>https://arxiv.org/abs/2411.05060</link>
<guid>https://arxiv.org/abs/2411.05060</guid>
<content:encoded><![CDATA[
arXiv:2411.05060v5 Announce Type: replace-cross 
Abstract: Misinformation is a complex societal issue, and mitigating solutions are difficult to create due to data deficiencies. To address this, we have curated the largest collection of (mis)information datasets in the literature, totaling 75. From these, we evaluated the quality of 36 datasets that consist of statements or claims, as well as the 9 datasets that consist of data in purely paragraph form. We assess these datasets to identify those with solid foundations for empirical work and those with flaws that could result in misleading and non-generalizable results, such as spurious correlations, or examples that are ambiguous or otherwise impossible to assess for veracity. We find the latter issue is particularly severe and affects most datasets in the literature. We further provide state-of-the-art baselines on all these datasets, but show that regardless of label quality, categorical labels may no longer give an accurate evaluation of detection model performance. Finally, we propose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the field toward systemic solutions rather than inadvertently propagating issues in evaluation. Overall, this guide aims to provide a roadmap for higher quality data and better grounded evaluations, ultimately improving research in misinformation detection. All datasets and other artifacts are available at https://misinfo-datasets.complexdatalab.com/.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture</title>
<link>https://arxiv.org/abs/2412.15113</link>
<guid>https://arxiv.org/abs/2412.15113</guid>
<content:encoded><![CDATA[
arXiv:2412.15113v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) demonstrate an impressive ability to utilise information within the context of their input sequences to appropriately respond to data unseen by the LLM during its training procedure. This ability is known as in-context learning (ICL). Humans and non-human animals demonstrate similar abilities, however their neural architectures differ substantially from LLMs. Despite this, a critical component within LLMs, the attention mechanism, resembles modern associative memory models, widely used in and influenced by the computational neuroscience community to model biological memory systems. Using this connection, we introduce an associative memory model capable of performing ICL. We use this as inspiration for a novel residual stream architecture which allows information to directly flow between attention heads. We test this architecture during training within a two-layer Transformer and show its ICL abilities manifest more quickly than without this modification. We then apply our architecture in small language models with 8 million and 1 billion parameters, focusing on attention head values, with results also indicating improved performance at these larger and more naturalistic scales.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gandalf the Red: Adaptive Security for LLMs</title>
<link>https://arxiv.org/abs/2501.07927</link>
<guid>https://arxiv.org/abs/2501.07927</guid>
<content:encoded><![CDATA[
arXiv:2501.07927v3 Announce Type: replace-cross 
Abstract: Current evaluations of defenses against prompt attacks in large language model (LLM) applications often overlook two critical factors: the dynamic nature of adversarial behavior and the usability penalties imposed on legitimate users by restrictive defenses. We propose D-SEC (Dynamic Security Utility Threat Model), which explicitly separates attackers from legitimate users, models multi-step interactions, and expresses the security-utility in an optimizable form. We further address the shortcomings in existing evaluations by introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed to generate realistic, adaptive attack. Using Gandalf, we collect and release a dataset of 279k prompt attacks. Complemented by benign user data, our analysis reveals the interplay between security and utility, showing that defenses integrated in the LLM (e.g., system prompts) can degrade usability even without blocking requests. We demonstrate that restricted application domains, defense-in-depth, and adaptive defenses are effective strategies for building secure and useful LLM applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2501.16607</link>
<guid>https://arxiv.org/abs/2501.16607</guid>
<content:encoded><![CDATA[
arXiv:2501.16607v2 Announce Type: replace-cross 
Abstract: Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at translating natural language questions into SQL queries. While recent advances in large language models have greatly improved performance, most existing approaches depend on models with tens of billions of parameters or costly APIs, limiting their applicability in resource-constrained environments. For real world, especially on edge devices, it is crucial for Text-to-SQL to ensure cost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL is of great practical significance. However, smaller LLMs often struggle with complicated user instruction, redundant schema linking or syntax correctness. To address these challenges, we propose MCTS-SQL, a novel framework that uses Monte Carlo Tree Search to guide SQL generation through multi-step refinement. Since the light-weight models' weak performance of single-shot prediction, we generate better results through several trials with feedback. However, directly applying MCTS-based methods inevitably leads to significant time and computational overhead. Driven by this issue, we propose a token-level prefix-cache mechanism that stores prior information during iterations, effectively improved the execution speed. Experiments results on the SPIDER and BIRD benchmarks demonstrate the effectiveness of our approach. Using a small open-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When leveraging a more powerful model Gemini 2.5 to explore the performance upper bound, we achieved results competitive with the SOTA. Our findings demonstrate that even small models can be effectively deployed in practical Text-to-SQL systems with the right strategy.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions</title>
<link>https://arxiv.org/abs/2502.04322</link>
<guid>https://arxiv.org/abs/2502.04322</guid>
<content:encoded><![CDATA[
arXiv:2502.04322v3 Announce Type: replace-cross 
Abstract: Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety at Scale: A Comprehensive Survey of Large Model and Agent Safety</title>
<link>https://arxiv.org/abs/2502.05206</link>
<guid>https://arxiv.org/abs/2502.05206</guid>
<content:encoded><![CDATA[
arXiv:2502.05206v5 Announce Type: replace-cross 
Abstract: The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-powered Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard AI models.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIRepr: An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science</title>
<link>https://arxiv.org/abs/2502.16395</link>
<guid>https://arxiv.org/abs/2502.16395</guid>
<content:encoded><![CDATA[
arXiv:2502.16395v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly used to automate data analysis through executable code generation. Yet, data science tasks often admit multiple statistically valid solutions, e.g. different modeling strategies, making it critical to understand the reasoning behind analyses, not just their outcomes. While manual review of LLM-generated code can help ensure statistical soundness, it is labor-intensive and requires expertise. A more scalable approach is to evaluate the underlying workflows - the logical plans guiding code generation. However, it remains unclear how to assess whether a LLM-generated workflow supports reproducible implementations.
  To address this, we present $\it{AIRepr}$, an $\it{A}$nalyst - $\it{I}$nspector framework for automatically evaluating and improving the $\it{Repr}$oducibility of LLM-generated data analysis workflows. Our framework is grounded in statistical principles and supports scalable, automated assessment. We introduce two novel reproducibility-enhancing prompting strategies and benchmark them against standard prompting across 15 analyst-inspector LLM pairs and 1,032 tasks from three public benchmarks. Our findings show that workflows with higher reproducibility also yield more accurate analyses, and that reproducibility-enhancing prompts substantially improve both metrics. This work provides a foundation for more transparent, reliable, and efficient human-AI collaboration in data science. Our code is publicly available.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JurisTCU: A Brazilian Portuguese Information Retrieval Dataset with Query Relevance Judgments</title>
<link>https://arxiv.org/abs/2503.08379</link>
<guid>https://arxiv.org/abs/2503.08379</guid>
<content:encoded><![CDATA[
arXiv:2503.08379v2 Announce Type: replace-cross 
Abstract: This paper introduces JurisTCU, a Brazilian Portuguese dataset for legal information retrieval (LIR). The dataset is freely available and consists of 16,045 jurisprudential documents from the Brazilian Federal Court of Accounts, along with 150 queries annotated with relevance judgments. It addresses the scarcity of Portuguese-language LIR datasets with query relevance annotations. The queries are organized into three groups: real user keyword-based queries, synthetic keyword-based queries, and synthetic question-based queries. Relevance judgments were produced through a hybrid approach combining LLM-based scoring with expert domain validation. We used JurisTCU in 14 experiments using lexical search (document expansion methods) and semantic search (BERT-based and OpenAI embeddings). We show that the document expansion methods significantly improve the performance of standard BM25 search on this dataset, with improvements exceeding 45% in P@10, R@10, and nDCG@10 metrics when evaluating short keyword-based queries. Among the embedding models, the OpenAI models produced the best results, with improvements of approximately 70% in P@10, R@10, and nDCG@10 metrics for short keyword-based queries, suggesting that these dense embeddings capture semantic relationships in this domain, surpassing the reliance on lexical terms. Besides offering a dataset for the Portuguese-language IR research community, suitable for evaluating search systems, the results also contribute to enhancing a search system highly relevant to Brazilian citizens.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2503.12937</link>
<guid>https://arxiv.org/abs/2503.12937</guid>
<content:encoded><![CDATA[
arXiv:2503.12937v2 Announce Type: replace-cross 
Abstract: Recent studies generally enhance MLLMs' reasoning capabilities via supervised fine-tuning on high-quality chain-of-thought reasoning data, which often leads models to merely imitate successful reasoning paths without understanding what the wrong reasoning paths are. In this work, we aim to enhance the MLLMs' reasoning ability beyond passively imitating positive reasoning paths. To this end, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new online reinforcement learning framework that enables MLLMs to self-improve reasoning ability via simple, effective and dense step-wise rewarding. Specifically, StepGRPO introduces two novel rule-based reasoning rewards: Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity Reward (StepRVR). StepRAR rewards the reasoning paths that contain necessary intermediate reasoning steps via a soft key-step matching technique, while StepRAR rewards reasoning paths that follow a well-structured and logically consistent reasoning process through a reasoning completeness and logic evaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series of MLLMs with outstanding capabilities in step-by-step reasoning. Extensive experiments over 8 benchmarks demonstrate the superiority of our methods.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StableGS: A Floater-Free Framework for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2503.18458</link>
<guid>https://arxiv.org/abs/2503.18458</guid>
<content:encoded><![CDATA[
arXiv:2503.18458v3 Announce Type: replace-cross 
Abstract: 3D Gaussian Splatting (3DGS) reconstructions are plagued by stubborn ``floater" artifacts that degrade their geometric and visual fidelity. We are the first to reveal the root cause: a fundamental conflict in the 3DGS optimization process where the opacity gradients of floaters vanish when their blended color reaches a pseudo-equilibrium of canceling errors against the background, trapping them in a spurious local minimum. To resolve this, we propose StableGS, a novel framework that decouples geometric regularization from final appearance rendering. Its core is a Dual Opacity architecture that creates two separate rendering paths: a ``Geometric Regularization Path" to bear strong depth-based constraints for structural correctness, and an ``Appearance Refinement Path" to generate high-fidelity details upon this stable foundation. We complement this with a synergistic set of geometric constraints: a self-supervised depth consistency loss and an external geometric prior enabled by our efficient global scale optimization algorithm. Experiments on multiple benchmarks show StableGS not only eliminates floaters but also resolves the common blur-artifact trade-off, achieving state-of-the-art geometric accuracy and visual quality.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics</title>
<link>https://arxiv.org/abs/2503.21735</link>
<guid>https://arxiv.org/abs/2503.21735</guid>
<content:encoded><![CDATA[
arXiv:2503.21735v2 Announce Type: replace-cross 
Abstract: Ensuring reliable software release decisions is critical in safety-critical domains such as automotive manufacturing. Release validation relies on large tabular datasets, yet manual analysis is slow, costly, and error-prone. While Large Language Models (LLMs) offer promising automation potential, they face challenges in analytical reasoning, structured data handling, and ambiguity resolution. This paper introduces GateLens, an LLM-based system for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and generates optimized Python code. Unlike traditional multi-agent or planning-based systems that can be slow, opaque, and costly to maintain, GateLens emphasizes speed, transparency, and reliability. Experimental results show that GateLens outperforms the existing Chain-of-Thought (CoT) + Self-Consistency (SC) based system on real-world datasets, particularly in handling complex and ambiguous queries. Ablation studies confirm the essential role of the RA layer. Industrial deployment shows over 80% reduction in analysis time while maintaining high accuracy across test result interpretation, impact assessment, and release candidate evaluation. GateLens operates effectively in zero-shot settings without requiring few-shot examples or agent orchestration. This work advances deployable LLM system design by identifying key architectural features-intermediate formal representations, execution efficiency, and low configuration overhead-crucial for safety-critical industrial applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2504.07448</link>
<guid>https://arxiv.org/abs/2504.07448</guid>
<content:encoded><![CDATA[
arXiv:2504.07448v2 Announce Type: replace-cross 
Abstract: Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices $A$ as random projections and sparsifies the matrices $B$ using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference. Code is available at: https://github.com/juzhengz/LoRI
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data</title>
<link>https://arxiv.org/abs/2504.16628</link>
<guid>https://arxiv.org/abs/2504.16628</guid>
<content:encoded><![CDATA[
arXiv:2504.16628v2 Announce Type: replace-cross 
Abstract: Aligning large language models with multiple human expectations and values is crucial for ensuring that they adequately serve a variety of user needs. To this end, offline multiobjective alignment algorithms such as the Rewards-in-Context algorithm have shown strong performance and efficiency. However, inappropriate preference representations and training with imbalanced reward scores limit the performance of such algorithms. In this work, we introduce ParetoHqD that addresses the above issues by representing human preferences as preference directions in the objective space and regarding data near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD follows a two-stage supervised fine-tuning process, where each stage uses an individual Pareto high-quality training set that best matches its preference direction. The experimental results have demonstrated the superiority of ParetoHqD over five baselines on two multiobjective alignment tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models</title>
<link>https://arxiv.org/abs/2505.07167</link>
<guid>https://arxiv.org/abs/2505.07167</guid>
<content:encoded><![CDATA[
arXiv:2505.07167v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have been extensively used across diverse domains, including virtual assistants, automated code generation, and scientific research. However, they remain vulnerable to jailbreak attacks, which manipulate the models into generating harmful responses despite safety alignment. Recent studies have shown that current safety-aligned LLMs often undergo the shallow safety alignment, where the first few tokens largely determine whether the response will be harmful. Through comprehensive observations, we find that safety-aligned LLMs and various defense strategies generate highly similar initial tokens in their refusal responses, which we define as safety trigger tokens. Building on this insight, we propose \texttt{D-STT}, a simple yet effective defense algorithm that identifies and explicitly decodes safety trigger tokens of the given safety-aligned LLM to trigger the model's learned safety patterns. In this process, the safety trigger is constrained to a single token, which effectively preserves model usability by introducing minimum intervention in the decoding process. Extensive experiments across diverse jailbreak attacks and benign prompts demonstrate that \ours significantly reduces output harmfulness while preserving model usability and incurring negligible response time overhead, outperforming ten baseline methods.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory</title>
<link>https://arxiv.org/abs/2505.10981</link>
<guid>https://arxiv.org/abs/2505.10981</guid>
<content:encoded><![CDATA[
arXiv:2505.10981v3 Announce Type: replace-cross 
Abstract: Recently, scaling test-time compute on Large Language Models (LLM) has garnered wide attention. However, there has been limited investigation of how various reasoning prompting strategies perform as scaling. In this paper, we focus on a standard and realistic scaling setting: majority voting. We systematically conduct experiments on 6 LLMs $\times$ 8 prompting strategies $\times$ 6 benchmarks. Experiment results consistently show that as the sampling time and computational overhead increase, complicated prompting strategies with superior initial performance gradually fall behind simple Chain-of-Thought. We analyze this phenomenon and provide theoretical proofs. Additionally, we propose a probabilistic method to efficiently predict scaling performance and identify the best prompting strategy under large sampling times, eliminating the need for resource-intensive inference processes in practical applications. Furthermore, we introduce two ways derived from our theoretical analysis to significantly improve the scaling performance. We hope that our research can promote to re-examine the role of complicated prompting, unleash the potential of simple prompting strategies, and provide new insights for enhancing test-time scaling performance. Code is available at https://github.com/MraDonkey/rethinking_prompting.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents</title>
<link>https://arxiv.org/abs/2505.12842</link>
<guid>https://arxiv.org/abs/2505.12842</guid>
<content:encoded><![CDATA[
arXiv:2505.12842v3 Announce Type: replace-cross 
Abstract: Graphical user interface (GUI) agents have recently emerged as an intriguing paradigm for human-computer interaction, capable of automatically executing user instructions to operate intelligent terminal devices. However, when encountering out-of-distribution (OOD) instructions that violate environmental constraints or exceed the current capabilities of agents, GUI agents may suffer task breakdowns or even pose security threats. Therefore, effective OOD detection for GUI agents is essential. Traditional OOD detection methods perform suboptimally in this domain due to the complex embedding space and evolving GUI environments. In this work, we observe that the in-distribution input semantic space of GUI agents exhibits a clustering pattern with respect to the distance from the centroid. Based on the finding, we propose GEM, a novel method based on fitting a Gaussian mixture model over input embedding distances extracted from the GUI agent that reflect its capability boundary. Evaluated on eight datasets spanning smartphones, computers, and web browsers, our method achieves an average accuracy improvement of 23.70\% over the best-performing baseline while only increasing training time by 4.9\% and testing time by 6.5\%. We also experimentally demonstrate that GEM can improve the step-wise success rate by 9.40\% by requesting assistance from the cloud model when encountering OOD samples. Analysis verifies the generalization ability of our method through experiments on nine different backbones. The codes are available at https://github.com/Wuzheng02/GEM-OODforGUIagents.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think, Reflect, Create: Metacognitive Learning for Zero-Shot Robotic Planning with LLMs</title>
<link>https://arxiv.org/abs/2505.14899</link>
<guid>https://arxiv.org/abs/2505.14899</guid>
<content:encoded><![CDATA[
arXiv:2505.14899v2 Announce Type: replace-cross 
Abstract: While large language models (LLMs) have shown great potential across various domains, their applications in robotics remain largely limited to static prompt-based behaviors and still face challenges in complex tasks under zero-shot or few-shot settings. Inspired by human metacognitive learning and creative problem-solving, we address this limitation by exploring a fundamental question: Can LLMs be empowered with metacognitive capabilities to reason, reflect, and create, thereby enhancing their ability to perform robotic tasks with minimal demonstrations? In this paper, we present a framework that integrates metacognitive learning into LLM-powered multi-robot collaboration. The system equips the LLM-powered robotic agents with a skill decomposition and self-reflection mechanism that identifies modular skills from prior tasks, reflects on failures in unseen task scenarios, and synthesizes effective new solutions. We propose a more challenging robotic benchmark task and evaluate our framework on the existing benchmark and the novel task. Experimental results show that our metacognitive learning framework significantly outperforms existing baselines. Moreover, we observe that the framework can generate solutions that differ from the ground truth yet still successfully complete the tasks. These findings support our hypothesis that metacognitive learning can foster creativity in robotic planning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISON: Unmasking the Criminal Potential of Large Language Models</title>
<link>https://arxiv.org/abs/2506.16150</link>
<guid>https://arxiv.org/abs/2506.16150</guid>
<content:encoded><![CDATA[
arXiv:2506.16150v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) advance, concerns about their misconduct in complex social contexts intensify. Existing research overlooked the systematic understanding and assessment of their criminal capability in realistic interactions. We propose a unified framework PRISON, to quantify LLMs' criminal potential across five traits: False Statements, Frame-Up, Psychological Manipulation, Emotional Disguise, and Moral Disengagement. Using structured crime scenarios adapted from classic films grounded in reality, we evaluate both criminal potential and anti-crime ability of LLMs. Results show that state-of-the-art LLMs frequently exhibit emergent criminal tendencies, such as proposing misleading statements or evasion tactics, even without explicit instructions. Moreover, when placed in a detective role, models recognize deceptive behavior with only 44% accuracy on average, revealing a striking mismatch between conducting and detecting criminal behavior. These findings underscore the urgent need for adversarial robustness, behavioral alignment, and safety mechanisms before broader LLM deployment.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky</title>
<link>https://arxiv.org/abs/2507.03336</link>
<guid>https://arxiv.org/abs/2507.03336</guid>
<content:encoded><![CDATA[
arXiv:2507.03336v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly tasked with invoking enterprise APIs, yet they routinely falter when near-duplicate tools vie for the same user intent or when required arguments are left underspecified. We introduce DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a disambiguation-centric, three-stage pipeline that (i) synthesizes persona-driven, multi-turn dialogues in which the assistant must distinguish among highly similar tools, (ii) performs supervised fine-tuning of open-source models with reasoning traces across 3B - 70B parameters, and (iii) evaluates real-world readiness via a dynamic suite that redeploys each model in a live agentic loop and reports end-to-end goal completion alongside conventional static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we release an open corpus of 5000 production-grade enterprise API specifications paired with rigorously validated, disambiguation-focused dialogues, offering a practical blueprint for building reliable, enterprise-ready tool-calling agents.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Semantic Parsing for the Interpretation of Tombstone Inscriptions</title>
<link>https://arxiv.org/abs/2507.04377</link>
<guid>https://arxiv.org/abs/2507.04377</guid>
<content:encoded><![CDATA[
arXiv:2507.04377v2 Announce Type: replace-cross 
Abstract: Tombstones are historically and culturally rich artifacts, encapsulating individual lives, community memory, historical narratives and artistic expression. Yet, many tombstones today face significant preservation challenges, including physical erosion, vandalism, environmental degradation, and political shifts. In this paper, we introduce a novel multi-modal framework for tombstones digitization, aiming to improve the interpretation, organization and retrieval of tombstone content. Our approach leverages vision-language models (VLMs) to translate tombstone images into structured Tombstone Meaning Representations (TMRs), capturing both image and text information. To further enrich semantic parsing, we incorporate retrieval-augmented generation (RAG) for integrate externally dependent elements such as toponyms, occupation codes, and ontological concepts. Compared to traditional OCR-based pipelines, our method improves parsing accuracy from an F1 score of 36.1 to 89.5. We additionally evaluate the model's robustness across diverse linguistic and cultural inscriptions, and simulate physical degradation through image fusion to assess performance under noisy or damaged conditions. Our work represents the first attempt to formalize tombstone understanding using large vision-language models, presenting implications for heritage preservation.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents</title>
<link>https://arxiv.org/abs/2507.10644</link>
<guid>https://arxiv.org/abs/2507.10644</guid>
<content:encoded><![CDATA[
arXiv:2507.10644v3 Announce Type: replace-cross 
Abstract: The concept of the Web of Agents (WoA), which transforms the static, document-centric Web into an environment of autonomous agents acting on users' behalf, has attracted growing interest as large language models (LLMs) become more capable. However, research in this area is still fragmented across different communities. Contemporary surveys catalog the latest LLM-powered frameworks, while the rich histories of Multi-Agent Systems (MAS) and the Semantic Web are often treated as separate, legacy domains. This fragmentation obscures the intellectual lineage of modern systems and hinders a holistic understanding of the field's trajectory. We present the first comprehensive evolutionary overview of the WoA. We show that modern protocols like A2A and the MCP, are direct evolutionary responses to the well-documented limitations of earlier standards like FIPA standards and OWL-based semantic agents. To systematize this analysis, we introduce a four-axis taxonomy (semantic foundation, communication paradigm, locus of intelligence, discovery mechanism). This framework provides a unified analytical lens for comparing agent architectures across all generations, revealing a clear line of descent where others have seen a disconnect. Our analysis identifies a paradigm shift in the 'locus of intelligence': from being encoded in external data (Semantic Web) or the platform (MAS) to being embedded within the agent's core model (LLM). This shift is foundational to modern Agentic AI, enabling the scalable and adaptive systems the WoA has long envisioned. We conclude that while new protocols are essential, they are insufficient for building a robust, open, trustworthy ecosystem. Finally, we argue that the next research frontier lies in solving persistent socio-technical challenges, and we map out a new agenda focused on decentralized identity, economic models, security, and governance for the emerging WoA.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models</title>
<link>https://arxiv.org/abs/2507.12806</link>
<guid>https://arxiv.org/abs/2507.12806</guid>
<content:encoded><![CDATA[
arXiv:2507.12806v2 Announce Type: replace-cross 
Abstract: The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment. We introduce MCPEval, an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains. MCPEval standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines. Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance. We publicly release MCPEval https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and standardized LLM agent evaluation.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems</title>
<link>https://arxiv.org/abs/2508.00079</link>
<guid>https://arxiv.org/abs/2508.00079</guid>
<content:encoded><![CDATA[
<div> Keywords: physics problems, LLMs, multi-agent framework, inference-time techniques, evaluation benchmark

Summary: 
In this study, the researchers assess the performance of advanced LLMs in solving physics problems, encompassing both mathematical and descriptive tasks. They implement various inference-time techniques and a multi-agent framework to enhance model performance. By introducing a new evaluation benchmark named ${\rm P{\small HYSICS}E{\small VAL}}$, consisting of over 19,000 problems sourced from physics textbooks, they evaluate the effectiveness of the models in solving these problems. The utilization of a multi-agent framework shows significant improvements, particularly on challenging problems initially tackled poorly by the models. Additionally, the validation of proposed solutions through smaller LLM agents in a cumulative manner is explored. The research code and data are openly accessible on GitHub, promoting transparency and reproducibility in the field. <div>
arXiv:2508.00079v1 Announce Type: new 
Abstract: The discipline of physics stands as a cornerstone of human intellect, driving the evolution of technology and deepening our understanding of the fundamental principles of the cosmos. Contemporary literature includes some works centered on the task of solving physics problems - a crucial domain of natural language reasoning. In this paper, we evaluate the performance of frontier LLMs in solving physics problems, both mathematical and descriptive. We also employ a plethora of inference-time techniques and agentic frameworks to improve the performance of the models. This includes the verification of proposed solutions in a cumulative fashion by other, smaller LLM agents, and we perform a comparative analysis of the performance that the techniques entail. There are significant improvements when the multi-agent framework is applied to problems that the models initially perform poorly on. Furthermore, we introduce a new evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small VAL}}$, consisting of 19,609 problems sourced from various physics textbooks and their corresponding correct solutions scraped from physics forums and educational websites. Our code and data are publicly available at https://github.com/areebuzair/PhysicsEval.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs produce texts with "human-like" lexical diversity?</title>
<link>https://arxiv.org/abs/2508.00086</link>
<guid>https://arxiv.org/abs/2508.00086</guid>
<content:encoded><![CDATA[
<div> lexial diversity, LLMs, human-like texts, ChatGPT models, language pedagogy

Summary: 
The study analyzed lexical diversity in texts generated by language model models (LLMs) such as ChatGPT models and compared them to texts written by human participants. Different dimensions of lexical diversity were measured, showing that LLM-generated texts differed significantly from human-written texts in all variables. The newer LLM models, ChatGPT-4.5 in particular, exhibited higher levels of lexical diversity despite producing fewer tokens. Human writers' lexical diversity did not vary based on education levels or language status. The results suggest that LLMs do not replicate human-like texts in terms of lexical diversity, and newer models are less human-like than older ones. The implications of these findings for language pedagogy and related applications are discussed. 

<br /><br />Summary: <div>
arXiv:2508.00086v1 Announce Type: new 
Abstract: The degree to which LLMs produce writing that is truly human-like remains unclear despite the extensive empirical attention that this question has received. The present study addresses this question from the perspective of lexical diversity. Specifically, the study investigates patterns of lexical diversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini, and -4.5) in comparison with texts written by L1 and L2 English participants (n = 240) across four education levels. Six dimensions of lexical diversity were measured in each text: volume, abundance, variety-repetition, evenness, disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and Support Vector Machines revealed that the LLM-generated texts differed significantly from human-written texts for each variable, with ChatGPT-o4 mini and -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated higher levels of lexical diversity despite producing fewer tokens. The human writers' lexical diversity did not differ across subgroups (i.e., education, language status). Altogether, the results indicate that LLMs do not produce human-like texts in relation to lexical diversity, and the newer LLMs produce less human-like texts than older models. We discuss the implications of these results for language pedagogy and related applications.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semiotic Complexity and Its Epistemological Implications for Modeling Culture</title>
<link>https://arxiv.org/abs/2508.00095</link>
<guid>https://arxiv.org/abs/2508.00095</guid>
<content:encoded><![CDATA[
<div> translation, computational humanities, theory, semiotic complexity, modeling

Summary:<br />
- The article emphasizes the need for greater theorizing in computational humanities to ensure epistemological and interpretive clarity.
- It introduces the concept of translation work between cultural and computational domains, highlighting the importance of articulating the theory behind the process.
- The authors discuss the notion of semiotic complexity, focusing on the varying meanings of texts across interpretive lenses.
- They critique dominant modeling practices for treating semiotically complex data as simple for convenience, leading to translation errors.
- Recommendations are provided for researchers to address epistemological issues in modeling practices and improve interpretive transparency in their work.<br /><br /> <div>
arXiv:2508.00095v1 Announce Type: new 
Abstract: Greater theorizing of methods in the computational humanities is needed for epistemological and interpretive clarity, and therefore the maturation of the field. In this paper, we frame such modeling work as engaging in translation work from a cultural, linguistic domain into a computational, mathematical domain, and back again. Translators benefit from articulating the theory of their translation process, and so do computational humanists in their work -- to ensure internal consistency, avoid subtle yet consequential translation errors, and facilitate interpretive transparency. Our contribution in this paper is to lay out a particularly consequential dimension of the lack of theorizing and the sorts of translation errors that emerge in our modeling practices as a result. Along these lines we introduce the idea of semiotic complexity as the degree to which the meaning of some text may vary across interpretive lenses, and make the case that dominant modeling practices -- especially around evaluation -- commit a translation error by treating semiotically complex data as semiotically simple when it seems epistemologically convenient by conferring superficial clarity. We then lay out several recommendations for researchers to better account for these epistemological issues in their own work.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality</title>
<link>https://arxiv.org/abs/2508.00109</link>
<guid>https://arxiv.org/abs/2508.00109</guid>
<content:encoded><![CDATA[
<div> benchmark, FACTORY, factuality evaluation, language models, human-verified<br />
Summary:<br />
The article introduces FACTORY, a large-scale human-verified prompt set for long-form factuality evaluation. Developed using model-in-the-loop and human refinement, FACTORY includes challenging, fact-seeking, answerable, and unambiguous prompts. Human evaluations on 6 state-of-the-art language models show that approximately 40% of claims made in their responses are not factual, highlighting FACTORY's reliability. The benchmark outperforms existing datasets, emphasizing the need for models to reason across a wide range of facts. <div>
arXiv:2508.00109v1 Announce Type: new 
Abstract: Long-form factuality evaluation assesses the ability of models to generate accurate, comprehensive responses to short prompts. Existing benchmarks often lack human verification, leading to potential quality issues. To address this limitation, we introduce FACTORY, a large-scale, human-verified prompt set. Developed using a model-in-the-loop approach and refined by humans, FACTORY includes challenging prompts that are fact-seeking, answerable, and unambiguous. We conduct human evaluations on 6 state-of-the-art language models using FACTORY and existing datasets. Our results show that FACTORY is a challenging benchmark: approximately 40% of the claims made in the responses of SOTA models are not factual, compared to only 10% for other datasets. Our analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing its reliability and the necessity for models to reason across long-tailed facts.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is neural semantic parsing good at ellipsis resolution, or isn't it?</title>
<link>https://arxiv.org/abs/2508.00121</link>
<guid>https://arxiv.org/abs/2508.00121</guid>
<content:encoded><![CDATA[
<div> Neural, semantic, parsers, ellipsis, English 
<br />
Summary: 
Neural semantic parsers have shown high performance in various linguistic tasks, achieving semantic matching scores exceeding 90%. However, their ability to handle strongly context-sensitive phenomena, such as verb phrase ellipsis in English, remains a challenge. In a study utilizing a corpus of 120 instances of ellipsis with their resolved meaning representations, it was found that while neural semantic parsers excelled on standard test sets, they struggled with cases of ellipsis. The phenomena of verb phrase ellipsis presents a unique challenge for semantic parsers, as it requires the duplication of significant semantic information to form a coherent representation. The study emphasizes the importance of evaluating the performance of semantic parsers on complex, context-sensitive linguistic constructs to better understand their capabilities and limitations. 
<br /><br /> Summary: <div>
arXiv:2508.00121v1 Announce Type: new 
Abstract: Neural semantic parsers have shown good overall performance for a variety of linguistic phenomena, reaching semantic matching scores of more than 90%. But how do such parsers perform on strongly context-sensitive phenomena, where large pieces of semantic information need to be duplicated to form a meaningful semantic representation? A case in point is English verb phrase ellipsis, a construct where entire verb phrases can be abbreviated by a single auxiliary verb. Are the otherwise known as powerful semantic parsers able to deal with ellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with their fully resolved meaning representation and used this as a challenge set for a large battery of neural semantic parsers. Although these parsers performed very well on the standard test set, they failed in the instances with ellipsis. Data augmentation
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison of Large Language Models for Deployment Requirements</title>
<link>https://arxiv.org/abs/2508.00185</link>
<guid>https://arxiv.org/abs/2508.00185</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Generative Pre-trained Transformers, Mixture of Experts, fine-tuning, open-source

Summary:
Large Language Models (LLMs), including Generative Pre-trained Transformers (GPTs), are transforming text generation with their ability to produce contextually relevant and syntactically correct content. Despite challenges like biases and hallucinations, these AI models excel in tasks such as content creation, translation, and code generation. Techniques like fine-tuning and novel architectures like Mixture of Experts (MoE) help address these challenges. The introduction of numerous foundational and fine-tuned open-source models in the past two years has made selecting the optimal LLM difficult for researchers and companies due to licensing and hardware requirements. To aid in navigating the evolving LLM landscape and facilitate model selection, a comparative list of foundational and domain-specific models is provided, focusing on features like release year, licensing, and hardware requirements. This continuously updated list is available on GitLab. 

<br /><br />Summary: Large Language Models, including Generative Pre-trained Transformers, are revolutionizing text generation. Despite challenges, such as biases and hallucinations, these models excel in various tasks with the help of techniques like fine-tuning and architectures like Mixture of Experts. The introduction of numerous open-source models has made selecting the best LLM complicated. To assist in this process, a comparative list of models focusing on key features has been created and made available on GitLab for continuous updating. <div>
arXiv:2508.00185v1 Announce Type: new 
Abstract: Large Language Models (LLMs), such as Generative Pre-trained Transformers (GPTs) are revolutionizing the generation of human-like text, producing contextually relevant and syntactically correct content. Despite challenges like biases and hallucinations, these Artificial Intelligence (AI) models excel in tasks, such as content creation, translation, and code generation. Fine-tuning and novel architectures, such as Mixture of Experts (MoE), address these issues. Over the past two years, numerous open-source foundational and fine-tuned models have been introduced, complicating the selection of the optimal LLM for researchers and companies regarding licensing and hardware requirements. To navigate the rapidly evolving LLM landscape and facilitate LLM selection, we present a comparative list of foundational and domain-specific models, focusing on features, such as release year, licensing, and hardware requirements. This list is published on GitLab and will be continuously updated.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges</title>
<link>https://arxiv.org/abs/2508.00217</link>
<guid>https://arxiv.org/abs/2508.00217</guid>
<content:encoded><![CDATA[
<div> Keywords: Tables, Large Language Models, Multimodal Large Language Models, Tabular Input Representations, Table Understanding Tasks 

Summary: 
Tables in large language models (LLMs) and multimodal large language models (MLLMs) are complex and flexible structures that require specialized methods and tasks due to their varied formats and purposes. However, current research primarily focuses on retrieval-focused tasks, lacking reasoning beyond mathematical and logical operations. Models struggle with processing complex table structures, large-scale tables, lengthy context, and multi-table scenarios. Additionally, generalization across different tabular representations and formats remains limited. Further research is needed to address these gaps and enhance the capabilities of models in understanding and navigating tables effectively.<br /><br />Summary:  <div>
arXiv:2508.00217v1 Announce Type: new 
Abstract: Tables have gained significant attention in large language models (LLMs) and multimodal large language models (MLLMs) due to their complex and flexible structure. Unlike linear text inputs, tables are two-dimensional, encompassing formats that range from well-structured database tables to complex, multi-layered spreadsheets, each with different purposes. This diversity in format and purpose has led to the development of specialized methods and tasks, instead of universal approaches, making navigation of table understanding tasks challenging. To address these challenges, this paper introduces key concepts through a taxonomy of tabular input representations and an introduction of table understanding tasks. We highlight several critical gaps in the field that indicate the need for further research: (1) the predominance of retrieval-focused tasks that require minimal reasoning beyond mathematical and logical operations; (2) significant challenges faced by models when processing complex table structures, large-scale tables, length context, or multi-table scenarios; and (3) the limited generalization of models across different tabular representations and formats.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform</title>
<link>https://arxiv.org/abs/2508.00220</link>
<guid>https://arxiv.org/abs/2508.00220</guid>
<content:encoded><![CDATA[
<div> Wavelet Transforms, Signal and Image Processing, NLP, Discrete Wavelet Transforms, Embeddings <br />
Summary: Wavelet transforms have proven effective in various domains, including NLP, for analyzing intricate patterns and enhancing data representation. This paper explores the application of Discrete Wavelet Transforms (DWT) to word and sentence embeddings, showcasing its ability to analyze embedding representations at different resolutions and compress them while maintaining quality. The study evaluates DWT embeddings on semantic similarity tasks, demonstrating their potential to capture important semantic information in embedding vectors. Results indicate that DWT can significantly reduce embedding dimensionality without affecting performance on semantic tasks while achieving superior accuracy in downstream tasks. These findings suggest that DWT can be a valuable tool for improving NLP applications. <br /> <div>
arXiv:2508.00220v1 Announce Type: new 
Abstract: Wavelet transforms, a powerful mathematical tool, have been widely used in different domains, including Signal and Image processing, to unravel intricate patterns, enhance data representation, and extract meaningful features from data. Tangible results from their application suggest that Wavelet transforms can be applied to NLP capturing a variety of linguistic and semantic properties. In this paper, we empirically leverage the application of Discrete Wavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase the capabilities of DWT in analyzing embedding representations at different levels of resolution and compressing them while maintaining their overall quality. We assess the effectiveness of DWT embeddings on semantic similarity tasks to show how DWT can be used to consolidate important semantic information in an embedding vector. We show the efficacy of the proposed paradigm using different embedding models, including large language models, on downstream tasks. Our results show that DWT can reduce the dimensionality of embeddings by 50-93% with almost no change in performance for semantic similarity tasks, while achieving superior accuracy in most downstream tasks. Our findings pave the way for applying DWT to improve NLP applications.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English</title>
<link>https://arxiv.org/abs/2508.00238</link>
<guid>https://arxiv.org/abs/2508.00238</guid>
<content:encoded><![CDATA[
<div> shifts, language models, lexical trends, artificial intelligence, alignment

Summary:
- Recent shifts in written language, particularly in science and education, are attributed to the influence of Large Language Models (LLMs) like ChatGPT.
- A dataset of 22.1 million words from unscripted spoken language was analyzed to study lexical trends pre- and post-2022, showing a significant increase in LLM-associated words post-2022.
- This increase suggests a convergence between human word choices and LLM patterns, indicating a potential shift in language use.
- Whether this change is a natural language evolution or driven by AI exposure remains uncertain, raising ethical concerns about the impact of misaligned language models on societal beliefs.
- The findings suggest that misalignments in AI models could contribute to changes in human language use, potentially shaping social and moral values. 

<br /><br />Summary: <div>
arXiv:2508.00238v1 Announce Type: new 
Abstract: In recent years, written language, particularly in science and education, has undergone remarkable shifts in word usage. These changes are widely attributed to the growing influence of Large Language Models (LLMs), which frequently rely on a distinct lexical style. Divergences between model output and target audience norms can be viewed as a form of misalignment. While these shifts are often linked to using Artificial Intelligence (AI) directly as a tool to generate text, it remains unclear whether the changes reflect broader changes in the human language system itself. To explore this question, we constructed a dataset of 22.1 million words from unscripted spoken language drawn from conversational science and technology podcasts. We analyzed lexical trends before and after ChatGPT's release in 2022, focusing on commonly LLM-associated words. Our results show a moderate yet significant increase in the usage of these words post-2022, suggesting a convergence between human word choices and LLM-associated patterns. In contrast, baseline synonym words exhibit no significant directional shift. Given the short time frame and the number of words affected, this may indicate the onset of a remarkable shift in language use. Whether this represents natural language change or a novel shift driven by AI exposure remains an open question. Similarly, although the shifts may stem from broader adoption patterns, it may also be that upstream training misalignments ultimately contribute to changes in human language use. These findings parallel ethical concerns that misaligned models may shape social and moral beliefs.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering</title>
<link>https://arxiv.org/abs/2508.00285</link>
<guid>https://arxiv.org/abs/2508.00285</guid>
<content:encoded><![CDATA[
<div> Etiology-Aware Attention Steering Framework, Clinical Reasoning Scaffolding, Etiology-Aware Head Identification algorithm, Reasoning-Guided Parameter-Efficient Fine-tuning, Reasoning Focus Score<br />
<br />
Summary: This study introduces an Etiology-Aware Attention Steering Framework to enhance diagnostic accuracy and clinical reasoning in Large Language Models (LLMs) for acute abdominal emergencies. By integrating structured Clinical Reasoning Scaffolding (CRS) and developing the Etiology-Aware Head Identification algorithm, the framework identifies crucial attention heads for etiology reasoning. The Reasoning-Guided Parameter-Efficient Fine-tuning embeds etiological cues and steers attention heads towards critical information, improving diagnostic accuracy by 15.65% and Reasoning Focus Score by 31.6% on the Consistent Diagnosis Cohort. External validation on the Discrepant Diagnosis Cohort further confirms the framework's effectiveness. The models show enhanced reliability in real-world complex scenarios, indicating a promising approach for building interpretable and reliable AI diagnostic systems in clinical settings. <br /><br />Summary: <div>
arXiv:2508.00285v1 Announce Type: new 
Abstract: Objective: Large Language Models (LLMs) demonstrate significant capabilities in medical text understanding and generation. However, their diagnostic reliability in complex clinical scenarios remains limited. This study aims to enhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We propose an Etiology-Aware Attention Steering Framework to integrate structured clinical reasoning into LLM-based diagnosis. Specifically, we first construct Clinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines for three representative acute abdominal emergencies: acute appendicitis, acute pancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head Identification algorithm to pinpoint attention heads crucial for the model's etiology reasoning. To ensure reliable clinical reasoning alignment, we introduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds etiological reasoning cues into input representations and steers the selected Etiology-Aware Heads toward critical information through a Reasoning-Guided Loss function. Result: On the Consistent Diagnosis Cohort, our framework improves average diagnostic accuracy by 15.65% and boosts the average Reasoning Focus Score by 31.6% over baselines. External validation on the Discrepant Diagnosis Cohort further confirms its effectiveness in enhancing diagnostic accuracy. Further assessments via Reasoning Attention Frequency indicate that our models exhibit enhanced reliability when faced with real-world complex scenarios. Conclusion: This study presents a practical and effective approach to enhance clinical reasoning in LLM-based diagnosis. By aligning model attention with structured CRS, the proposed framework offers a promising paradigm for building more interpretable and reliable AI diagnostic systems in complex clinical settings.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Evaluation of Optimization Techniques for Long-Context Language Models</title>
<link>https://arxiv.org/abs/2508.00305</link>
<guid>https://arxiv.org/abs/2508.00305</guid>
<content:encoded><![CDATA[
<div> optimization, large language models, memory usage, text generation, scalability

Summary: 
This paper examines various optimization techniques for large language models (LLMs) to address resource demands and limited context windows. The study benchmarks methods such as pruning, quantization, and token dropping for two LLM architectures with long context support. It evaluates the impact of these optimizations on memory usage, latency, throughput, and the quality of text generation. Combinations of optimization techniques are systematically tested to understand their effects on performance metrics. The scalability of these methods is also studied on a larger 70 billion-parameter model. The research uncovers that naive combination inference optimization algorithms can have negative impacts on larger models due to compounded approximation errors. It highlights the importance of considering precision-recall trade-offs in question answering tasks instead of solely relying on F1 scores. By integrating system-level profiling with task-specific insights, this study aids LLM practitioners in balancing efficiency, accuracy, and scalability across different tasks and hardware configurations. 

<br /><br />Summary: <div>
arXiv:2508.00305v1 Announce Type: new 
Abstract: Large language models (LLMs) excel across diverse natural language processing tasks but face resource demands and limited context windows. Although techniques like pruning, quantization, and token dropping can mitigate these issues, their efficacy in long-context scenarios and system evaluation remains underexplored. This paper systematically benchmarks these optimizations, characterizing memory usage, latency, and throughput, and studies how these methods impact the quality of text generation. We first analyze individual optimization methods for two LLM architectures supporting long context and then systematically evaluate combinations of these techniques to assess how this deeper analysis impacts performance metrics. We subsequently study the scalability of individual optimization methods on a larger variant with 70 billion-parameter model. Our novel insights reveal that naive combination inference optimization algorithms can adversely affect larger models due to compounded approximation errors, as compared to their smaller counterparts. Experiments show that relying solely on F1 obscures these effects by hiding precision-recall trade-offs in question answering tasks. By integrating system-level profiling with task-specific insights, this study helps LLM practitioners and researchers explore and balance efficiency, accuracy, and scalability across tasks and hardware configurations.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment</title>
<link>https://arxiv.org/abs/2508.00332</link>
<guid>https://arxiv.org/abs/2508.00332</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal sentence embedding, MCSEO, object-phrase alignment, contrastive learning objective, semantic textual similarity (STS) tasks

Summary: 
MCSEO is proposed as a method to enhance multimodal sentence embeddings by incorporating fine-grained object-phrase alignment alongside traditional image-caption alignment. It leverages segmentation and object detection models to extract accurate object-phrase pairs, optimizing a contrastive learning objective focused on object-phrase correspondence. Experimental results across various backbone models show that MCSEO consistently outperforms strong baselines in semantic textual similarity (STS) tasks. This highlights the importance of precise object-phrase alignment in enhancing multimodal representation learning. <br /><br />Summary: <div>
arXiv:2508.00332v1 Announce Type: new 
Abstract: Multimodal sentence embedding models typically leverage image-caption pairs in addition to textual data during training. However, such pairs often contain noise, including redundant or irrelevant information on either the image or caption side. To mitigate this issue, we propose MCSEO, a method that enhances multimodal sentence embeddings by incorporating fine-grained object-phrase alignment alongside traditional image-caption alignment. Specifically, MCSEO utilizes existing segmentation and object detection models to extract accurate object-phrase pairs, which are then used to optimize a contrastive learning objective tailored to object-phrase correspondence. Experimental results on semantic textual similarity (STS) tasks across different backbone models demonstrate that MCSEO consistently outperforms strong baselines, highlighting the significance of precise object-phrase alignment in multimodal representation learning.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.00344</link>
<guid>https://arxiv.org/abs/2508.00344</guid>
<content:encoded><![CDATA[
<div> Agent-based environments, Large Language Models, ReAct paradigm, AdaPlan, PilotRL, reinforcement learning

Summary:
- Challenges faced in deploying Large Language Models (LLMs) in agent-based environments due to limitations in existing agent paradigms.
- Introduction of AdaPlan, an adaptive global plan-based agent paradigm to support long-horizon decision-making.
- Proposal of PilotRL framework for LLM agents, combining global planning guidance with reinforcement learning for improved task performance.
- Development of the model's ability to follow explicit guidance from global plans.
- Optimization of plan quality and coordination between planning and execution in joint optimization. 

<br /><br />Summary: <div>
arXiv:2508.00344v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable advancements in tackling agent-oriented tasks. Despite their potential, existing work faces challenges when deploying LLMs in agent-based environments. The widely adopted agent paradigm ReAct centers on integrating single-step reasoning with immediate action execution, which limits its effectiveness in complex tasks requiring long-term strategic planning. Furthermore, the coordination between the planner and executor during problem-solving is also a critical factor to consider in agent design. Additionally, current approaches predominantly rely on supervised fine-tuning, which often leads models to memorize established task completion trajectories, thereby restricting their generalization ability when confronted with novel problem contexts. To address these challenges, we introduce an adaptive global plan-based agent paradigm AdaPlan, aiming to synergize high-level explicit guidance with execution to support effective long-horizon decision-making. Based on the proposed paradigm, we further put forward PilotRL, a global planning-guided training framework for LLM agents driven by progressive reinforcement learning. We first develop the model's ability to follow explicit guidance from global plans when addressing agent tasks. Subsequently, based on this foundation, we focus on optimizing the quality of generated plans. Finally, we conduct joint optimization of the model's planning and execution coordination. Experiments indicate that PilotRL could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78% comparing to GPT-4o-mini at a comparable parameter scale.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lucy: edgerunning agentic web search on mobile with machine generated task vectors</title>
<link>https://arxiv.org/abs/2508.00360</link>
<guid>https://arxiv.org/abs/2508.00360</guid>
<content:encoded><![CDATA[
<div> RLVR, agentic web-search model, Lucy, MCP integration, SimpleQA benchmark
Summary:
- Small language models (SLMs) face limitations in knowledge-intensive tasks due to their restricted capacity.
- This work introduces a new approach where the model's internal reasoning is viewed as a dynamic task vector machine, with content inside specific tags guiding the model's thought process.
- The generation process itself is considered a mechanism for the model to construct and refine its task vectors in real-time.
- The dynamic task vector machine is optimized through Reinforcement Learning with Variable Reward (RLVR) and successfully trained an agentic web-search model named Lucy.
- Lucy, a 1.7B-parameter SLM incorporating this dynamic reasoning mechanism with MCP integration, achieves a high accuracy of 78.3% on the SimpleQA benchmark, putting it on par with much larger models like DeepSeek-V3. This demonstrates that small models can compete with larger ones when equipped with structured, self-constructed task reasoning.<br /><br />Summary: <div>
arXiv:2508.00360v1 Announce Type: new 
Abstract: Small language models (SLMs) are inherently limited in knowledge-intensive tasks due to their constrained capacity. While test-time computation offers a path to enhanced performance, most approaches treat reasoning as a fixed or heuristic process. In this work, we propose a new paradigm: viewing the model's internal reasoning, delimited by  and  tags, as a dynamic task vector machine. Rather than treating the content inside these tags as a mere trace of thought, we interpret the generation process itself as a mechanism through which the model \textbf{constructs and refines its own task vectors} on the fly. We developed a method to optimize this dynamic task vector machine through RLVR and successfully trained an agentic web-search model. We present Lucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with MCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing on par with much larger models such as DeepSeek-V3. This demonstrates that small models can rival large ones when equipped with structured, self-constructed task reasoning.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices</title>
<link>https://arxiv.org/abs/2508.00370</link>
<guid>https://arxiv.org/abs/2508.00370</guid>
<content:encoded><![CDATA[
<div> fine-tuning, long-sequence tasks, EdgeInfinite, self-attention, NPUs <br />
Summary: 
The article introduces EdgeInfinite-Instruct, a solution for deploying Transformer-based large language models on resource-constrained edge devices for long-sequence tasks. Existing optimizations for Key-Value cache in these models often fail to reduce time to first token (TTFT) and may impact performance. EdgeInfinite fine-tunes only a small subset of parameters to reduce computational and memory costs while improving TTFT. The new EdgeInfinite-Instruct strategy utilizes Segmented Supervised Fine-Tuning (S-SFT) for tasks like summarization and question answering. Post-training quantization (PTQ) is employed for efficient deployment on edge NPUs, reducing computational demands. A fixed-shape computation graph balances memory usage and device efficiency with input token and cache size customization. Experiments demonstrate improved domain-specific performance while maintaining efficiency on NPU-accelerated edge devices. <br /> <div>
arXiv:2508.00370v1 Announce Type: new 
Abstract: Deploying Transformer-based large language models (LLMs) on resource-constrained edge devices for long-sequence tasks remains challenging due to the quadratic time complexity of self-attention and growing Key-Value (KV) cache demands. While existing KV cache optimizations improve memory efficiency, they often fail to reduce time to first token (TTFT) and may degrade performance through token pruning. Alternative sequence modeling architectures address some of these limitations, but typically require full retraining and lack infrastructure support. EdgeInfinite offers an efficient solution by fine-tuning only a small subset of parameters, maintaining quality while reducing both computational and memory costs, including improved TTFT. However, its instruction-following ability is limited, and it lacks mobile-specific optimizations. To address these issues, we propose EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning (S-SFT) strategy tailored to long-sequence tasks such as summarization and question answering. We further optimized EdgeInfinite-Instruct for efficient deployment on edge NPUs by employing fine-grained post-training quantization (PTQ) to reduce computational demands while maintaining accuracy, and by implementing a fixed-shape computation graph that balances memory usage and on-device efficiency through scenario-specific customization of input token and cache sizes. Experiments on long-context benchmarks and real-world mobile tasks show that our approach improves domain-specific performance while maintaining efficiency on NPU-accelerated edge devices.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Layer Attention is the Amplifier of Demonstration Effectiveness</title>
<link>https://arxiv.org/abs/2508.00385</link>
<guid>https://arxiv.org/abs/2508.00385</guid>
<content:encoded><![CDATA[
<div> mechanisms, in-context learning, demonstration effectiveness, gradient flow, demonstration selection <br />
<br />
Summary: 
The study explores the reasons behind the ineffectiveness of demonstrations in in-context learning (ICL). It suggests that a demonstration becomes ineffective if its information has either already been learned by the model or is irrelevant to the user query. The analysis, based on gradient flow and linear self-attention models, reveals that in multi-layer models, the effectiveness disparity among demonstrations increases with higher layers. Current demonstration selection methods mainly focus on relevance to the user query, neglecting the model's assimilated information. To address this, the paper introduces GradS, a novel method that uses gradient flow for demonstration selection by considering the magnitude of the gradient flow with respect to a user query. Experimental results on various datasets and LLMs show that GradS outperforms existing baselines, demonstrating its effectiveness in improving demonstration selection. <div>
arXiv:2508.00385v1 Announce Type: new 
Abstract: Numerous studies have investigated the underlying mechanisms of in-context learning (ICL) effectiveness to inspire the design of related methods. However, existing work predominantly assumes the effectiveness of the demonstrations provided within ICL, while many research indicates that not all demonstrations are effective, failing to yielding any performance improvement during ICL. Therefore, in this paper, we investigate the reasons behind demonstration ineffectiveness. Our analysis is based on gradient flow and linear self-attention models. By setting the gradient flow to zero, we deduce that a demonstration becomes ineffective if its information has either been learned by the model or is irrelevant to the user query. Furthermore, we demonstrate that in multi-layer models, the disparity in effectiveness among demonstrations is amplified with layer increasing, causing the model to focus more on effective ones. Considering that current demonstration selection methods primarily focus on the relevance to the user query while overlooking the information that the model has already assimilated, we propose a novel method called GradS, which leverages gradient flow for demonstration selection. We use the magnitude of the gradient flow of the demonstration with respect to a given user query as the criterion, thereby ensuring the effectiveness of the chosen ones. We validate our derivation and GradS on four prominent LLMs across five mainstream datasets. The experimental results confirm that the disparity in effectiveness among demonstrations is magnified as the model layer increases, substantiating our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on average over the strongest baselines, demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation</title>
<link>https://arxiv.org/abs/2508.00390</link>
<guid>https://arxiv.org/abs/2508.00390</guid>
<content:encoded><![CDATA[
<div> Vision-Language Navigation, Unmanned Aerial Vehicle, Reinforcement Learning, Curriculum Learning, Semantic Understanding<br />
Summary:<br />
The article introduces a novel training framework, Semantic-Aware Gaussian Curriculum Scheduling (SA-GCS), for improving Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN). By integrating Curriculum Learning into Reinforcement Learning, SA-GCS effectively quantifies the complexity of training samples using a Semantic-Aware Difficulty Estimator (SA-DE) and adjusts the sampling distribution dynamically with a Gaussian Curriculum Scheduler (GCS). This method significantly enhances training efficiency, accelerates convergence, and boosts overall model performance. Extensive experiments on the CityNav benchmark show that SA-GCS outperforms strong baselines across all metrics, demonstrates faster and more stable convergence, and generalizes well across models of various scales. The approach is robust, scalable, and publicly available for implementation.<br /> <div>
arXiv:2508.00390v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable agents to accurately localize targets and plan flight paths in complex environments based on natural language instructions, with broad applications in intelligent inspection, disaster rescue, and urban monitoring. Recent progress in Vision-Language Models (VLMs) has provided strong semantic understanding for this task, while reinforcement learning (RL) has emerged as a promising post-training strategy to further improve generalization. However, existing RL methods often suffer from inefficient use of training data, slow convergence, and insufficient consideration of the difficulty variation among training samples, which limits further performance improvement. To address these challenges, we propose \textbf{Semantic-Aware Gaussian Curriculum Scheduling (SA-GCS)}, a novel training framework that systematically integrates Curriculum Learning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator (SA-DE) to quantify the complexity of training samples and a Gaussian Curriculum Scheduler (GCS) to dynamically adjust the sampling distribution, enabling a smooth progression from easy to challenging tasks. This design significantly improves training efficiency, accelerates convergence, and enhances overall model performance. Extensive experiments on the CityNav benchmark demonstrate that SA-GCS consistently outperforms strong baselines across all metrics, achieves faster and more stable convergence, and generalizes well across models of different scales, highlighting its robustness and scalability. The implementation of our approach is publicly available.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding</title>
<link>https://arxiv.org/abs/2508.00420</link>
<guid>https://arxiv.org/abs/2508.00420</guid>
<content:encoded><![CDATA[
<div> Wavelets, Image processing, Signal processing, Natural Language Processing, Discrete Wavelet Transforms <br />
Summary: <br />
The paper explores the application of wavelets in Natural Language Processing (NLP) tasks, utilizing Discrete Wavelet Transforms (DWT) on word and sentence embeddings. The study evaluates the effectiveness of using wavelets to consolidate important information in word vectors while reducing dimensionality. A non-parameterized model combining DWT with Discrete Cosine Transform (DCT) is proposed to compress sentences with varying word features into fixed-size vectors. Results show that this approach yields comparable or superior outcomes in downstream NLP applications compared to original embeddings. The research demonstrates the potential for wavelets to enhance NLP tasks by capturing diverse linguistic properties and efficiently representing textual information. <div>
arXiv:2508.00420v1 Announce Type: new 
Abstract: Wavelets have emerged as a cutting edge technology in a number of fields. Concrete results of their application in Image and Signal processing suggest that wavelets can be effectively applied to Natural Language Processing (NLP) tasks that capture a variety of linguistic properties. In this paper, we leverage the power of applying Discrete Wavelet Transforms (DWT) to word and sentence embeddings. We first evaluate, intrinsically and extrinsically, how wavelets can effectively be used to consolidate important information in a word vector while reducing its dimensionality. We further combine DWT with Discrete Cosine Transform (DCT) to propose a non-parameterized model that compresses a sentence with a dense amount of information in a fixed size vector based on locally varying word features. We show the efficacy of the proposed paradigm on downstream applications models yielding comparable and even superior (in some tasks) results to original embeddings.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network</title>
<link>https://arxiv.org/abs/2508.00429</link>
<guid>https://arxiv.org/abs/2508.00429</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, imbalance in node informativeness, global semantic relationships, Retrieval-augmented Graph Agentic Network, few-shot in-context settings

Summary:<br />
Graph Neural Networks (GNNs) have been successful in graph-based learning but face limitations in handling imbalanced node informativeness and capturing global semantic relationships. To address these issues, the Retrieval-augmented Graph Agentic Network (ReaGAN) framework is introduced. ReaGAN empowers nodes to make autonomous decisions, allowing for node-level planning and adaptive message propagation. Additionally, retrieval-augmented generation (RAG) enables nodes to access relevant content and establish global relationships in the graph. By leveraging agentic planning and local-global retrieval, ReaGAN achieves competitive performance in few-shot in-context scenarios using a frozen LLM backbone without fine-tuning. This highlights the potential of agent-based frameworks and advanced retrieval mechanisms in enhancing graph learning. 

<br /><br />Summary: <div>
arXiv:2508.00429v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based learning by propagating information among neighbor nodes via predefined aggregation mechanisms. However, such fixed schemes often suffer from two key limitations. First, they cannot handle the imbalance in node informativeness -- some nodes are rich in information, while others remain sparse. Second, predefined message passing primarily leverages local structural similarity while ignoring global semantic relationships across the graph, limiting the model's ability to capture distant but relevant information. We propose Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework that empowers each node with autonomous, node-level decision-making. Each node acts as an agent that independently plans its next action based on its internal memory, enabling node-level planning and adaptive message propagation. Additionally, retrieval-augmented generation (RAG) allows nodes to access semantically relevant content and build global relationships in the graph. ReaGAN achieves competitive performance under few-shot in-context settings using a frozen LLM backbone without fine-tuning, showcasing the potential of agentic planning and local-global retrieval in graph learning.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges</title>
<link>https://arxiv.org/abs/2508.00454</link>
<guid>https://arxiv.org/abs/2508.00454</guid>
<content:encoded><![CDATA[
<div> Multi-turn dialogue evaluator, large language models, biases, evaluation methods, efficiency<br />
Summary:<br />
- Evaluating conversational abilities of large language models is challenging.
- Current methods use LLMs as judges but suffer from biases.
- Multi-judge approach aggregates judgments but incurs high computational cost.
- Proposed method aggregates multiple LLM judgments into a single model efficiently.
- Outperforms existing baselines in diverse dialogue evaluation benchmarks, showcasing efficiency and robustness.<br /> 

Summary: <div>
arXiv:2508.00454v1 Announce Type: new 
Abstract: Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the ``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts</title>
<link>https://arxiv.org/abs/2508.00476</link>
<guid>https://arxiv.org/abs/2508.00476</guid>
<content:encoded><![CDATA[
<div> Keywords: GETALP, Automatic Minuting Shared Task, retrieval augmented generation, Abstract Meaning Representations, question-answering  

Summary:  
- GETALP's submission to the Third Run of the Automatic Minuting Shared Task at SIGDial 2025 focused on Task B, involving question-answering based on meeting transcripts.  
- The method utilized a retrieval augmented generation (RAG) system and Abstract Meaning Representations (AMR) to address the task.  
- Three systems were proposed, combining RAG and AMR approaches to enhance question-answering capabilities.  
- Results indicated that integrating AMR into the process significantly improved responses for approximately 35% of questions, especially those requiring differentiation between participants.  
- The incorporation of AMR demonstrated a notable enhancement in the quality of responses, particularly in scenarios where identification of specific participants was crucial.  

<br /><br />Summary: GETALP participated in the Automatic Minuting Shared Task by employing a retrieval augmented generation system and Abstract Meaning Representations. Their three systems showcased improved question-answering performance, particularly when distinguishing between participants in meeting transcripts. The integration of AMR notably enhanced the accuracy and quality of responses, highlighting its effectiveness in addressing complex questions based on meeting content. <div>
arXiv:2508.00476v1 Announce Type: new 
Abstract: This paper documents GETALP's submission to the Third Run of the Automatic Minuting Shared Task at SIGDial 2025. We participated in Task B: question-answering based on meeting transcripts. Our method is based on a retrieval augmented generation (RAG) system and Abstract Meaning Representations (AMR). We propose three systems combining these two approaches. Our results show that incorporating AMR leads to high-quality responses for approximately 35% of the questions and provides notable improvements in answering questions that involve distinguishing between different participants (e.g., who questions).
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Missing Parts: Augmenting Fact Verification with Half-Truth Detection</title>
<link>https://arxiv.org/abs/2508.00489</link>
<guid>https://arxiv.org/abs/2508.00489</guid>
<content:encoded><![CDATA[
<div> half-truth detection, PolitiFact-Hidden, TRACER, omission-based misinformation, fact verification

Summary:
TRACER is introduced as a framework for identifying half-truths by aligning evidence, inferring implied intent, and estimating the impact of hidden content. The framework is designed to address the challenge of detecting misleading claims that are factually correct but omit critical context. The study proposes a new benchmark, PolitiFact-Hidden, consisting of 15k political claims annotated with evidence alignment and inferred intent. TRACER significantly improves performance across multiple existing fact-checking models, particularly in classifying claims as Half-True, showcasing the importance of considering omissions for accurate fact verification. <div>
arXiv:2508.00489v1 Announce Type: new 
Abstract: Fact verification systems typically assess whether a claim is supported by retrieved evidence, assuming that truthfulness depends solely on what is stated. However, many real-world claims are half-truths, factually correct yet misleading due to the omission of critical context. Existing models struggle with such cases, as they are not designed to reason about what is left unsaid. We introduce the task of half-truth detection, and propose PolitiFact-Hidden, a new benchmark with 15k political claims annotated with sentence-level evidence alignment and inferred claim intent. To address this challenge, we present TRACER, a modular re-assessment framework that identifies omission-based misinformation by aligning evidence, inferring implied intent, and estimating the causal impact of hidden content. TRACER can be integrated into existing fact-checking pipelines and consistently improves performance across multiple strong baselines. Notably, it boosts Half-True classification F1 by up to 16 points, highlighting the importance of modeling omissions for trustworthy fact verification.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond</title>
<link>https://arxiv.org/abs/2508.00522</link>
<guid>https://arxiv.org/abs/2508.00522</guid>
<content:encoded><![CDATA[
<div> Keywords: LoRA, flat minima, sharpness, generalization, efficiency

Summary:
Flat-LoRA and its efficient version, EFlat-LoRA, are proposed to seek flat minima for low-rank adaptation (LoRA). By transferring perturbations in the full parameter space to the low-rank subspace, potential interference is eliminated. Extensive experiments on large language models and vision-language models demonstrate that EFlat-LoRA achieves comparable performance to LoRA with optimized efficiency. For example, on the GLUE dataset with RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning. Similarly, on vision-language models like Qwen-VL-Chat, EFlat-LoRA shows performance improvements on various datasets. These empirical results establish the correlation between sharpness and generalization in LoRA, highlighting the importance of considering sharpness in model optimization processes.
<br /><br />Summary: <div>
arXiv:2508.00522v1 Announce Type: new 
Abstract: Little research explores the correlation between the expressive ability and generalization ability of the low-rank adaptation (LoRA). Sharpness-Aware Minimization (SAM) improves model generalization for both Convolutional Neural Networks (CNNs) and Transformers by encouraging convergence to locally flat minima. However, the connection between sharpness and generalization has not been fully explored for LoRA due to the lack of tools to either empirically seek flat minima or develop theoretical methods. In this work, we propose Flat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for LoRA. Concretely, we theoretically demonstrate that perturbations in the full parameter space can be transferred to the low-rank subspace. This approach eliminates the potential interference introduced by perturbations across multiple matrices in the low-rank subspace. Our extensive experiments on large language models and vision-language models demonstrate that EFlat-LoRA achieves optimize efficiency comparable to that of LoRA while simultaneously attaining comparable or even better performance. For example, on the GLUE dataset with RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and 0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat shows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets, respectively. These empirical results also verify that the generalization of LoRA is closely related to sharpness, which is omitted by previous methods.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Prosody of Emojis</title>
<link>https://arxiv.org/abs/2508.00537</link>
<guid>https://arxiv.org/abs/2508.00537</guid>
<content:encoded><![CDATA[
<div> Prosodic features, emojis, speech, communication, perception <br />
Summary: <br />
This study investigates how emojis influence prosodic features in speech and how listeners interpret these cues to understand emoji meanings. By analyzing human speech data, the study shows that speakers adjust their prosody based on emoji cues. Listeners can often identify emojis from prosodic variations alone, and differences in emoji semantics correspond to variations in prosody. This suggests that emojis play a significant role in conveying prosodic intent in digitally mediated communication. <div>
arXiv:2508.00537v1 Announce Type: new 
Abstract: Prosodic features such as pitch, timing, and intonation are central to spoken communication, conveying emotion, intent, and discourse structure. In text-based settings, where these cues are absent, emojis act as visual surrogates that add affective and pragmatic nuance. This study examines how emojis influence prosodic realisation in speech and how listeners interpret prosodic cues to recover emoji meanings. Unlike previous work, we directly link prosody and emoji by analysing actual human speech data, collected through structured but open-ended production and perception tasks. This provides empirical evidence of how emoji semantics shape spoken delivery and perception. Results show that speakers adapt their prosody based on emoji cues, listeners can often identify the intended emoji from prosodic variation alone, and greater semantic differences between emojis correspond to increased prosodic divergence. These findings suggest that emojis can act as meaningful carriers of prosodic intent, offering insight into their communicative role in digitally mediated contexts.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaPaformer: Language Model from Pre-trained Paraller Paths</title>
<link>https://arxiv.org/abs/2508.00544</link>
<guid>https://arxiv.org/abs/2508.00544</guid>
<content:encoded><![CDATA[
<div> Transformer, language models, training, parallel paths, PaPaformer <br />
<br />
Summary: 
The paper introduces a new approach called PaPaformer for training decoder-only transformer-based language models in a fraction of the time typically required. By combining lower-dimensional parallel paths into a larger model, PaPaformer allows for individual training of these paths with different data types before merging them. This method reduces model parameters and training time while maintaining performance. The use of parallel paths also offers the flexibility to customize paths for specific task requirements. Overall, PaPaformer presents a promising solution to the computational demands of training large-language models, making it possible to train and evaluate these models in hours rather than days or weeks. <div>
arXiv:2508.00544v1 Announce Type: new 
Abstract: The training of modern large-language models requires an increasingly amount of computation power and time. Even smaller variants, such as small-language models (SLMs), take several days to train in the best-case scenarios, often requiring multiple GPUs. This paper explores methods to train and evaluate decoder-only transformer-based language models in hours instead of days/weeks. We introduces \textit{PaPaformer}, a decoder-only transformer architecture variant, whose lower-dimensional parallel paths are combined into larger model. The paper shows that these lower-dimensional paths can be trained individually with different types of training data and then combined into one larger model. This method gives the option to reduce the total number of model parameters and the training time with increasing performance. Moreover, the use of parallel path structure opens interesting possibilities to customize paths to accommodate specific task requirements.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought</title>
<link>https://arxiv.org/abs/2508.00574</link>
<guid>https://arxiv.org/abs/2508.00574</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought reasoning, Continuous CoT, SynAdapt, LLMs, efficient reasoning framework 

Summary:
SynAdapt presents an innovative efficient reasoning framework that addresses the time costs associated with Chain-of-Thought (CoT) reasoning by introducing Continuous CoT (CCoT). By generating synthetic CCoT as alignment targets for LLMs, SynAdapt enables accurate answers to be derived directly. To tackle difficult questions, a difficulty classifier is integrated into the framework to identify challenging questions based on question context and CCoT. When hard questions are identified, the LLM is prompted to re-think these questions for improved performance. Experimental results across various benchmarks demonstrate the effectiveness of SynAdapt in achieving the best accuracy-efficiency trade-off. The proposed method outperforms existing CCoT approaches and offers a more efficient alternative for reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2508.00574v1 Announce Type: new 
Abstract: While Chain-of-Thought (CoT) reasoning improves model performance, it incurs significant time costs due to the generation of discrete CoT tokens (DCoT). Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT methods are hampered by indirect fine-tuning, limited alignment, or inconsistent targets. To overcome these limitations, we propose \textit{SynAdapt}, an innovative efficient reasoning framework. Specifically, \textit{SynAdapt} generates the synthetic CCoT to serve as a precise and effective alignment target for LLMs. This synthetic CCoT explicitly guides the LLM to learn CCoT and derive accurate answers directly. Furthermore, relying solely on CCoT is insufficient for solving hard questions. To address this, \textit{SynAdapt} integrates a difficulty classifier that leverages both question context and CCoT to identify hard questions. CCoT can effectively help identify hard questions after some brief reasoning. We then adaptively prompt the LLM to re-think these hard questions for improved performance. Extensive experimental results across various benchmarks from different difficulty levels strongly demonstrate the effectiveness of our method, achieving the best accuracy-efficiency trade-off.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models</title>
<link>https://arxiv.org/abs/2508.00600</link>
<guid>https://arxiv.org/abs/2508.00600</guid>
<content:encoded><![CDATA[
<div> Keywords: confidence estimation, large language models, context awareness, entropy reduction, consistency examination

Summary:
CRUX is a new framework designed to improve confidence estimation in large language models by considering the relevance between responses and contextual information. It incorporates two novel metrics: contextual entropy reduction and unified consistency examination. Contextual entropy reduction measures data uncertainty by comparing the information gain with and without context, while unified consistency examination evaluates model uncertainty by analyzing the global consistency of generated answers. Experimental results on various benchmark and domain-specific datasets show that CRUX outperforms existing methods in terms of AUROC. By considering context faithfulness and consistency, CRUX enhances the reliability of confidence estimation in large language models for better performance in safety-critical applications. 

<br /><br />Summary: <div>
arXiv:2508.00600v1 Announce Type: new 
Abstract: Accurate confidence estimation is essential for trustworthy large language models (LLMs) systems, as it empowers the user to determine when to trust outputs and enables reliable deployment in safety-critical applications. Current confidence estimation methods for LLMs neglect the relevance between responses and contextual information, a crucial factor in output quality evaluation, particularly in scenarios where background knowledge is provided. To bridge this gap, we propose CRUX (Context-aware entropy Reduction and Unified consistency eXamination), the first framework that integrates context faithfulness and consistency for confidence estimation via two novel metrics. First, contextual entropy reduction represents data uncertainty with the information gain through contrastive sampling with and without context. Second, unified consistency examination captures potential model uncertainty through the global consistency of the generated answers with and without context. Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness, achieving the highest AUROC than existing baselines.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language</title>
<link>https://arxiv.org/abs/2508.00605</link>
<guid>https://arxiv.org/abs/2508.00605</guid>
<content:encoded><![CDATA[
<div> Graph Convolutional Network, Topic Modeling, Bengali, NMF, Text Corpus <br />
<br />Summary: 
Topic modeling is a valuable technique in Natural Language Processing for extracting themes from text corpora. The study introduces a novel model, GHTM, using Graph Convolutional Networks to generate semantic embeddings of Bengali documents. These embeddings are then decomposed using Non-negative Matrix Factorization to identify underlying topics. The model is compared against traditional and contemporary Bengali topic modeling methods on three datasets, demonstrating superior performance in topic coherence and diversity. The study also introduces a new Bengali dataset, "NCTBText," sourced from textbook materials to enhance the existing Bengali corpora. <div>
arXiv:2508.00605v1 Announce Type: new 
Abstract: Topic modeling is a Natural Language Processing (NLP) technique that is used to identify latent themes and extract topics from text corpora by grouping similar documents based on their most significant keywords. Although widely researched in English, topic modeling remains understudied in Bengali due to its morphological complexity, lack of adequate resources and initiatives. In this contribution, a novel Graph Convolutional Network (GCN) based model called GHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input vectors of documents as nodes in the graph, which GCN uses to produce semantically rich embeddings. The embeddings are then decomposed using Non-negative Matrix Factorization (NMF) to get the topical representations of the underlying themes of the text corpus. This study compares the proposed model against a wide range of Bengali topic modeling techniques, from traditional methods such as LDA, LSA, and NMF to contemporary frameworks such as BERTopic and Top2Vec on three Bengali datasets. The experimental results demonstrate the effectiveness of the proposed model by outperforming other models in topic coherence and diversity. In addition, we introduce a novel Bengali dataset called "NCTBText" sourced from Bengali textbook materials to enrich and diversify the predominantly newspaper-centric Bengali corpora.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?</title>
<link>https://arxiv.org/abs/2508.00614</link>
<guid>https://arxiv.org/abs/2508.00614</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, tipping, threatening, performance, prompting

Summary: 
- The report investigates the effects of tipping and threatening AI models on performance, finding that these actions generally have no significant impact on benchmark performance.
- The study reveals that prompting variations can influence performance on individual questions, but it is unpredictable whether a specific approach will help or hinder the model's ability to answer a question.
- Simple prompting variations may not be as effective as previously thought, especially for challenging problems.
- Previous research has shown that different prompting approaches can lead to varying results for specific questions.
- Overall, while certain prompts may affect performance on a question-by-question basis, the overall impact of tipping and threatening AI models appears to be minimal on benchmark tasks.

<br /><br />Summary: <div>
arXiv:2508.00614v1 Announce Type: new 
Abstract: This is the third in a series of short reports that seek to help business, education, and policy leaders understand the technical details of working with AI through rigorous testing. In this report, we investigate two commonly held prompting beliefs: a) offering to tip the AI model and b) threatening the AI model. Tipping was a commonly shared tactic for improving AI performance and threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025, 8:20) who observed that 'models tend to do better if you threaten them,' a claim we subject to empirical testing here. We evaluate model performance on GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).
  We demonstrate two things:
  - Threatening or tipping a model generally has no significant effect on benchmark performance.
  - Prompt variations can significantly affect performance on a per-question level. However, it is hard to know in advance whether a particular prompting approach will help or harm the LLM's ability to answer any particular question.
  Taken together, this suggests that simple prompting variations might not be as effective as previously assumed, especially for difficult problems. However, as reported previously (Meincke et al. 2025a), prompting approaches can yield significantly different results for individual questions.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models</title>
<link>https://arxiv.org/abs/2508.00619</link>
<guid>https://arxiv.org/abs/2508.00619</guid>
<content:encoded><![CDATA[
<div> Existing AIG text detectors struggle in real-world settings, showing potential vulnerabilities. <br />
Keywords: AIG text detection, DACTYL dataset, few-shot generations, CPT-generated texts, DXO classifier <br /><br />
Summary:
The study examines the shortcomings of current AIG text detectors and introduces the DACTYL dataset, focusing on few-shot generations and CPT-generated texts. Existing detectors face challenges in detecting AIG texts, particularly in one-shot/few-shot scenarios. The study trains classifiers using BCE and DXO optimization approaches, with DXO classifiers demonstrating better generalization on out-of-distribution texts. In a mock deployment scenario, the best DXO classifier outperformed the best BCE-trained classifier by a significant margin. The results suggest that DXO classifiers have better generalization abilities and do not overfit to the test set. The experiments emphasize the need for enhancements in AIG text detection systems. <br /> <div>
arXiv:2508.00619v1 Announce Type: new 
Abstract: Existing AIG (AI-generated) text detectors struggle in real-world settings despite succeeding in internal testing, suggesting that they may not be robust enough. We rigorously examine the machine-learning procedure to build these detectors to address this. Most current AIG text detection datasets focus on zero-shot generations, but little work has been done on few-shot or one-shot generations, where LLMs are given human texts as an example. In response, we introduce the Diverse Adversarial Corpus of Texts Yielded from Language models (DACTYL), a challenging AIG text detection dataset focusing on one-shot/few-shot generations. We also include texts from domain-specific continued-pre-trained (CPT) language models, where we fully train all parameters using a memory-efficient optimization approach. Many existing AIG text detectors struggle significantly on our dataset, indicating a potential vulnerability to one-shot/few-shot and CPT-generated texts. We also train our own classifiers using two approaches: standard binary cross-entropy (BCE) optimization and a more recent approach, deep X-risk optimization (DXO). While BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL test set, the latter excels on out-of-distribution (OOD) texts. In our mock deployment scenario in student essay detection with an OOD student essay dataset, the best DXO classifier outscored the best BCE-trained classifier by 50.56 macro-F1 score points at the lowest false positive rates for both. Our results indicate that DXO classifiers generalize better without overfitting to the test set. Our experiments highlight several areas of improvement for AIG text detectors.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications</title>
<link>https://arxiv.org/abs/2508.00669</link>
<guid>https://arxiv.org/abs/2508.00669</guid>
<content:encoded><![CDATA[
<div> training-time strategies, test-time mechanisms, data modalities, clinical applications, evaluation benchmarks

Summary:<br /><br />
The paper reviews the emerging field of Large Language Models (LLMs) designed for medical reasoning. It proposes a taxonomy of reasoning enhancement techniques, including training-time strategies like supervised fine-tuning and test-time mechanisms such as prompt engineering. These techniques are applied to different data modalities (text, image, code) and clinical applications like diagnosis and treatment planning. The evolution of evaluation benchmarks is also discussed, highlighting the shift towards assessing reasoning quality and visual interpretability. Critical challenges identified include the faithfulness-plausibility gap and the need for native multimodal reasoning. The study analyzes 60 seminal studies from 2022-2025 and concludes by outlining future directions towards developing efficient, robust, and sociotechnically responsible medical AI. <br /><br />Summary: <div>
arXiv:2508.00669v1 Announce Type: new 
Abstract: The proliferation of Large Language Models (LLMs) in medicine has enabled impressive capabilities, yet a critical gap remains in their ability to perform systematic, transparent, and verifiable reasoning, a cornerstone of clinical practice. This has catalyzed a shift from single-step answer generation to the development of LLMs explicitly designed for medical reasoning. This paper provides the first systematic review of this emerging field. We propose a taxonomy of reasoning enhancement techniques, categorized into training-time strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how these techniques are applied across different data modalities (text, image, code) and in key clinical applications such as diagnosis, education, and treatment planning. Furthermore, we survey the evolution of evaluation benchmarks from simple accuracy metrics to sophisticated assessments of reasoning quality and visual interpretability. Based on an analysis of 60 seminal studies from 2022-2025, we conclude by identifying critical challenges, including the faithfulness-plausibility gap and the need for native multimodal reasoning, and outlining future directions toward building efficient, robust, and sociotechnically responsible medical AI.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language</title>
<link>https://arxiv.org/abs/2508.00673</link>
<guid>https://arxiv.org/abs/2508.00673</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, evaluation, Persian language, Iranian culture, benchmark

Summary: 
This study addresses the need for evaluating large language models (LLMs) in languages other than English, focusing specifically on the Persian language and Iranian culture. The researchers introduced 19 new evaluation datasets covering topics such as Iranian law, Persian grammar, idioms, and university entrance exams to assess LLM performance. By benchmarking 41 popular LLMs using these datasets, the study aims to bridge the gap in cultural and linguistic evaluation resources. This initiative is crucial as most LLMs are trained on data from Western cultures, lacking familiarity with non-Western contexts. The research highlights the importance of ensuring the quality and reliability of LLMs across diverse cultural and linguistic backgrounds.<br /><br />Summary: <div>
arXiv:2508.00673v1 Announce Type: new 
Abstract: As large language models (LLMs) become increasingly embedded in our daily lives, evaluating their quality and reliability across diverse contexts has become essential. While comprehensive benchmarks exist for assessing LLM performance in English, there remains a significant gap in evaluation resources for other languages. Moreover, because most LLMs are trained primarily on data rooted in European and American cultures, they often lack familiarity with non-Western cultural contexts. To address this limitation, our study focuses on the Persian language and Iranian culture. We introduce 19 new evaluation datasets specifically designed to assess LLMs on topics such as Iranian law, Persian grammar, Persian idioms, and university entrance exams. Using these datasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing cultural and linguistic evaluation gap in the field.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Team "better_call_claude": Style Change Detection using a Sequential Sentence Pair Classifier</title>
<link>https://arxiv.org/abs/2508.00675</link>
<guid>https://arxiv.org/abs/2508.00675</guid>
<content:encoded><![CDATA[
<div> style change detection, authorship analysis, sequential sentence pair classifier, pre-trained language model, bidirectional LSTM

Summary:
The article discusses the challenge of detecting style changes in documents at the fine-grained level of individual sentences. The proposed approach uses a Sequential Sentence Pair Classifier (SSPC) that leverages a pre-trained language model (PLM) and bidirectional LSTM to contextualize sentences within documents. The model combines adjacent sentence representations and uses a multi-layer perceptron for prediction. Despite being conservative and lightweight, the approach effectively addresses the challenge of identifying style shifts in short, stylistically shallow sentences present in benchmark data. Evaluation on official PAN-2025 test datasets show strong macro-F1 scores, outperforming random baselines and achieving better performance than a challenging zero-shot model. This work highlights the importance of leveraging contextual information for accurate style change detection in computational authorship analysis. 

<br /><br />Summary: <div>
arXiv:2508.00675v1 Announce Type: new 
Abstract: Style change detection - identifying the points in a document where writing style shifts - remains one of the most important and challenging problems in computational authorship analysis. At PAN 2025, the shared task challenges participants to detect style switches at the most fine-grained level: individual sentences. The task spans three datasets, each designed with controlled and increasing thematic variety within documents. We propose to address this problem by modeling the content of each problem instance - that is, a series of sentences - as a whole, using a Sequential Sentence Pair Classifier (SSPC). The architecture leverages a pre-trained language model (PLM) to obtain representations of individual sentences, which are then fed into a bidirectional LSTM (BiLSTM) to contextualize them within the document. The BiLSTM-produced vectors of adjacent sentences are concatenated and passed to a multi-layer perceptron for prediction per adjacency. Building on the work of previous PAN participants classical text segmentation, the approach is relatively conservative and lightweight. Nevertheless, it proves effective in leveraging contextual information and addressing what is arguably the most challenging aspect of this year's shared task: the notorious problem of "stylistically shallow", short sentences that are prevalent in the proposed benchmark data. Evaluated on the official PAN-2025 test datasets, the model achieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM, and HARD data, respectively, outperforming not only the official random baselines but also a much more challenging one: claude-3.7-sonnet's zero-shot performance.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries</title>
<link>https://arxiv.org/abs/2508.00679</link>
<guid>https://arxiv.org/abs/2508.00679</guid>
<content:encoded><![CDATA[
<div> BM25, Vector Database, Cross-Encoder, legal precedent retrieval, TraceRetriever<br />
Summary:<br />
- Legal precedent retrieval in the common law system is crucial for ensuring consistency in judicial decisions.
- Traditional retrieval methods face challenges due to the increasing complexity and volume of legal documents.
- TraceRetriever mirrors real-world legal search by extracting rhetorically significant segments from limited case information.
- The pipeline integrates BM25, Vector Database, and Cross-Encoder models for efficient retrieval.
- Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier trained on Indian judgments.
- Evaluated on IL-PCR and COLIEE 2025 datasets, TraceRetriever offers a reliable and scalable solution for precedent retrieval in legal research.
<br />Summary: <div>
arXiv:2508.00679v1 Announce Type: new 
Abstract: Legal precedent retrieval is a cornerstone of the common law system, governed by the principle of stare decisis, which demands consistency in judicial decisions. However, the growing complexity and volume of legal documents challenge traditional retrieval methods. TraceRetriever mirrors real-world legal search by operating with limited case information, extracting only rhetorically significant segments instead of requiring complete documents. Our pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining initial results through Reciprocal Rank Fusion before final re-ranking. Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets, TraceRetriever addresses growing document volume challenges while aligning with practical search constraints, reliable and scalable foundation for precedent retrieval enhancing legal research when only partial case knowledge is available.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better Call Claude: Can LLMs Detect Changes of Writing Style?</title>
<link>https://arxiv.org/abs/2508.00680</link>
<guid>https://arxiv.org/abs/2508.00680</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, zero-shot performance, authorship analysis, style change detection, generative models 
<br />
Summary: 
Large language models exhibit strong zero-shot performance in detecting style changes at the sentence level in authorship analysis tasks. These models are sensitive to variations in writing style, surpassing suggested baselines in accuracy on official datasets. The study reveals that the latest LLMs may prioritize content-independent and stylistic signals over semantic content when making predictions. <div>
arXiv:2508.00680v1 Announce Type: new 
Abstract: This article explores the zero-shot performance of state-of-the-art large language models (LLMs) on one of the most challenging tasks in authorship analysis: sentence-level style change detection. Benchmarking four LLMs on the official PAN~2024 and 2025 "Multi-Author Writing Style Analysis" datasets, we present several observations. First, state-of-the-art generative models are sensitive to variations in writing style - even at the granular level of individual sentences. Second, their accuracy establishes a challenging baseline for the task, outperforming suggested baselines of the PAN competition. Finally, we explore the influence of semantics on model predictions and present evidence suggesting that the latest generation of LLMs may be more sensitive to content-independent and purely stylistic signals than previously reported.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System</title>
<link>https://arxiv.org/abs/2508.00709</link>
<guid>https://arxiv.org/abs/2508.00709</guid>
<content:encoded><![CDATA[
<div> Legal Judgment Prediction, AI for law, NyayaRAG, Retrieval-Augmented Generation, Indian legal system
Summary: 
Legal Judgment Prediction (LJP) is important in AI for law, aiming to automate judicial outcome forecasting in the Indian legal system. The NyayaRAG framework combines factual case descriptions, legal statutes, and prior cases to predict court decisions and generate legal explanations. Results show that incorporating legal knowledge enhances predictive accuracy and explanation quality. This approach considers statutory provisions and judicial precedents, improving the understanding of common law systems like in India. NyayaRAG provides a domain-specific pipeline for legal reasoning, evaluating performance with various input configurations. The framework simulates realistic courtroom scenarios, enhancing interpretability in legal AI models. By leveraging structured legal knowledge, NyayaRAG demonstrates the effectiveness of combining factual inputs with legal insights in predicting court outcomes. G-Eval and other evaluation metrics confirm the advantages of augmenting information sources in LJP models. <div>
arXiv:2508.00709v1 Announce Type: new 
Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA</title>
<link>https://arxiv.org/abs/2508.00719</link>
<guid>https://arxiv.org/abs/2508.00719</guid>
<content:encoded><![CDATA[
<div> Monte Carlo Tree Search, Knowledge Graph Question Answering, Adaptive Path Evaluation, Context-aware reasoning, Relation sequence encoding <br />
Summary:<br />
The paper proposes a novel framework, Dynamically Adaptive MCTS-based Reasoning (DAMR), for Knowledge Graph Question Answering (KGQA). DAMR integrates symbolic search with adaptive path evaluation to improve efficiency and context-awareness in KGQA. It uses a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner to select relevant relations and reduce search space. A lightweight Transformer-based scorer is introduced for context-aware plausibility estimation during multi-hop reasoning. DAMR also incorporates a dynamic pseudo-path refinement mechanism to generate training signals from partial paths explored during search. This allows the model to continuously adapt to evolving reasoning trajectories. Extensive experiments on multiple KGQA benchmarks demonstrate that DAMR outperforms current state-of-the-art methods. <br /> <div>
arXiv:2508.00719v1 Announce Type: new 
Abstract: Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data</title>
<link>https://arxiv.org/abs/2508.00741</link>
<guid>https://arxiv.org/abs/2508.00741</guid>
<content:encoded><![CDATA[
<div> abduction, large language models, reasoning, chatbots, AI safety
Summary:<br /><br />The study focuses on out-of-context abduction in Large Language Models (LLMs) and their ability to infer plausible explanations without direct training data. Experiments were conducted with OpenAI's GPT 4o LLM trained on chatbot names and behavior descriptions but not on dialogue examples. The results show that the LLM can correctly deduce a chatbot's name and display characteristic behaviors after training. This highlights the LLM's potential for situational awareness and reasoning abilities based on training data. The findings have implications for AI safety and understanding the capabilities of LLMs in reasoning tasks. <div>
arXiv:2508.00741v1 Announce Type: new 
Abstract: Large language models (LLMs) are trained on large corpora, yet it is unclear whether they can reason about the information present within their training data. We design experiments to study out-of-context abduction in LLMs, the ability to infer the most plausible explanations for observations using relevant facts present in training data. We train treatment LLMs on names and behavior descriptions of fictitious chatbots, but not on examples of dialogue with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at least one chatbot's name after observing example responses characteristic of that chatbot. We also find that previously training GPT 4o on descriptions of a chatbot's behavior allows it to display behaviors more characteristic of the chatbot when iteratively trained to display such behaviors. Our results have implications for situational awareness in LLMs and, therefore, for AI safety.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents</title>
<link>https://arxiv.org/abs/2508.00742</link>
<guid>https://arxiv.org/abs/2508.00742</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative agents, Large Language Models, HEXACO, Personality inventory, Social science research 

Summary: 
1) The study focuses on the use of generative agents powered by Large Language Models in representing human populations in social science research.
2) The experiment involved surveying 310 GPT-4 powered agents to recreate the HEXACO personality inventory experiment and compare the results to the original findings.
3) Results showed a coherent and reliable personality structure recoverable from the agents' responses, with partial alignment to the HEXACO framework.
4) The derived personality dimensions were consistent and reliable within GPT-4 with a sufficiently curated population.
5) Cross-model analysis revealed variability in personality profiling, indicating model-specific biases and limitations. 

<br /><br />Summary: <div>
arXiv:2508.00742v1 Announce Type: new 
Abstract: Generative agents powered by Large Language Models demonstrate human-like characteristics through sophisticated natural language interactions. Their ability to assume roles and personalities based on predefined character biographies has positioned them as cost-effective substitutes for human participants in social science research. This paper explores the validity of such persona-based agents in representing human populations; we recreate the HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents, conducting factor analysis on their responses, and comparing these results to the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results found 1) a coherent and reliable personality structure was recoverable from the agents' responses demonstrating partial alignment to the HEXACO framework. 2) the derived personality dimensions were consistent and reliable within GPT-4, when coupled with a sufficiently curated population, and 3) cross-model analysis revealed variability in personality profiling, suggesting model-specific biases and limitations. We discuss the practical considerations and challenges encountered during the experiment. This study contributes to the ongoing discourse on the potential benefits and limitations of using generative agents in social science research and provides useful guidance on designing consistent and representative agent personas to maximise coverage and representation of human personality traits.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic large language models improve retrieval-based radiology question answering</title>
<link>https://arxiv.org/abs/2508.00743</link>
<guid>https://arxiv.org/abs/2508.00743</guid>
<content:encoded><![CDATA[
<div> Radiology, artificial intelligence, clinical decision-making, large language models, retrieval-augmented generation <br />
Summary: 
- Propose an agentic retrieval-augmented generation framework for radiology question answering using large language models.
- Evaluated 24 LLMs on expert-curated radiology questions, showing significant improvement in diagnostic accuracy with agentic retrieval.
- Mid-sized models and small-scale models benefited the most, while very large models had minimal changes.
- Agentic retrieval reduced hallucinations and provided clinically relevant context in 46% of cases.
- Even clinically fine-tuned models showed meaningful improvements, highlighting the complementary roles of retrieval and fine-tuning in enhancing factuality and diagnostic accuracy in radiology QA. <br /> 

Summary: <div>
arXiv:2508.00743v1 Announce Type: new 
Abstract: Clinical decision-making in radiology increasingly benefits from artificial intelligence (AI), particularly through large language models (LLMs). However, traditional retrieval-augmented generation (RAG) systems for radiology question answering (QA) typically rely on single-step retrieval, limiting their ability to handle complex clinical reasoning tasks. Here we propose an agentic RAG framework enabling LLMs to autonomously decompose radiology questions, iteratively retrieve targeted clinical evidence from Radiopaedia, and dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning diverse architectures, parameter scales (0.5B to >670B), and training paradigms (general-purpose, reasoning-optimized, clinically fine-tuned), using 104 expert-curated radiology questions from previously established RSNA-RadioQA and ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized models (e.g., Mistral Large improved from 72% to 81%) and small-scale models (e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B parameters) demonstrated minimal changes (<2% improvement). Additionally, agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically relevant context in 46% of cases, substantially aiding factual grounding. Even clinically fine-tuned models exhibited meaningful improvements (e.g., MedGemma-27B improved from 71% to 81%), indicating complementary roles of retrieval and fine-tuning. These results highlight the potential of agentic frameworks to enhance factuality and diagnostic accuracy in radiology QA, particularly among mid-sized LLMs, warranting future studies to validate their clinical utility.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLiDRE: Generalist Lightweight model for Document-level Relation Extraction</title>
<link>https://arxiv.org/abs/2508.00757</link>
<guid>https://arxiv.org/abs/2508.00757</guid>
<content:encoded><![CDATA[
<div> GLiDRE, relation extraction, document-level, NER, benchmark <br />
Summary:
GLiDRE is a new model for document-level relation extraction, inspired by the success of the GLiNER model. It addresses the challenges of zero-shot and few-shot scenarios in relation extraction tasks like DocRED and Re-DocRED. The model outperforms state-of-the-art approaches in few-shot settings, showcasing its effectiveness in handling complex interactions between entities across sentences. GLiDRE builds on the compact NER model concept introduced by GLiNER and demonstrates significant improvements in document-level relation extraction tasks. The code for GLiDRE is publicly available, allowing for further research and development in this area.<br /><br />Summary: <div>
arXiv:2508.00757v1 Announce Type: new 
Abstract: Relation Extraction (RE) is a fundamental task in Natural Language Processing, and its document-level variant poses significant challenges, due to the need to model complex interactions between entities across sentences. Current approaches, largely based on the ATLOP architecture, are commonly evaluated on benchmarks like DocRED and Re-DocRED. However, their performance in zero-shot or few-shot settings remains largely underexplored due to the task's complexity. Recently, the GLiNER model has shown that a compact NER model can outperform much larger Large Language Models. With a similar motivation, we introduce GLiDRE, a new model for document-level relation extraction that builds on the key ideas of GliNER. We benchmark GLiDRE against state-of-the-art models across various data settings on the Re-DocRED dataset. Our results demonstrate that GLiDRE achieves state-of-the-art performance in few-shot scenarios. Our code is publicly available.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations</title>
<link>https://arxiv.org/abs/2508.00760</link>
<guid>https://arxiv.org/abs/2508.00760</guid>
<content:encoded><![CDATA[
<div> Keywords: Hate speech detection, Chinese social networks, Multimodal framework, Mixture-of-Experts, BERT-based models

Summary: <br /><br /> This study introduces MMBERT, a novel multimodal framework for hate speech detection on Chinese social networks. MMBERT integrates textual, speech, and visual modalities using a Mixture-of-Experts architecture and employs a progressive three-stage training paradigm to address integration challenges. The model incorporates modality-specific experts, a shared self-attention mechanism, and a router-based expert allocation strategy to enhance robustness against adversarial perturbations. Empirical results on Chinese hate speech datasets demonstrate MMBERT's superiority over fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs using in-context learning approaches. Overall, MMBERT significantly improves hate speech detection capabilities in the specific context of Chinese social networks, highlighting the importance of multimodal strategies in tackling cloaking techniques and enhancing detection accuracy. <div>
arXiv:2508.00760v1 Announce Type: new 
Abstract: Hate speech detection on Chinese social networks presents distinct challenges, particularly due to the widespread use of cloaking techniques designed to evade conventional text-based detection systems. Although large language models (LLMs) have recently improved hate speech detection capabilities, the majority of existing work has concentrated on English datasets, with limited attention given to multimodal strategies in the Chinese context. In this study, we propose MMBERT, a novel BERT-based multimodal framework that integrates textual, speech, and visual modalities through a Mixture-of-Experts (MoE) architecture. To address the instability associated with directly integrating MoE into BERT-based models, we develop a progressive three-stage training paradigm. MMBERT incorporates modality-specific experts, a shared self-attention mechanism, and a router-based expert allocation strategy to enhance robustness against adversarial perturbations. Empirical results in several Chinese hate speech datasets show that MMBERT significantly surpasses fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing in-context learning approaches.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation</title>
<link>https://arxiv.org/abs/2508.00762</link>
<guid>https://arxiv.org/abs/2508.00762</guid>
<content:encoded><![CDATA[
<div> Keywords: SemEval-2025, Tabular data, Question answering, Large Language Models, Python code generation

Summary:<br />
This paper presents a system developed for SemEval-2025 Task 8, focusing on question answering over tabular data. The system tackles two subtasks, DataBench QA and DataBench Lite QA, using a zero-shot approach with Large Language Model-based code generation. The system utilizes a Python code generation framework to generate executable Pandas code through optimized prompting strategies. Results indicate varying effectiveness of different Large Language Models in Python code generation, with Python code generation outperforming alternative approaches in tabular question answering. The system achieved eighth place in Subtask I and sixth place in Subtask II among systems that surpassed the baseline in the open-source models category. <div>
arXiv:2508.00762v1 Announce Type: new 
Abstract: This paper presents our system for SemEval-2025 Task 8: DataBench, Question-Answering over Tabular Data. The primary objective of this task is to perform question answering on given tabular datasets from diverse domains under two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To tackle both subtasks, we developed a zero-shot solution with a particular emphasis on leveraging Large Language Model (LLM)-based code generation. Specifically, we propose a Python code generation framework utilizing state-of-the-art open-source LLMs to generate executable Pandas code via optimized prompting strategies. Our experiments reveal that different LLMs exhibit varying levels of effectiveness in Python code generation. Additionally, results show that Python code generation achieves superior performance in tabular question answering compared to alternative approaches. Although our ranking among zero-shot systems is unknown at the time of this paper's submission, our system achieved eighth place in Subtask I and sixth place in Subtask~II among the 30 systems that outperformed the baseline in the open-source models category.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models</title>
<link>https://arxiv.org/abs/2508.00788</link>
<guid>https://arxiv.org/abs/2508.00788</guid>
<content:encoded><![CDATA[
<div> benchmark, LLMs, pronoun fidelity, neopronouns, gender identity inference
Summary:
The study introduces the MISGENDERED+ benchmark for evaluating Large Language Models' (LLMs) pronoun fidelity, focusing on inclusive pronouns and gender identity inference. Five representative LLMs are benchmarked, showing improvements in binary and gender-neutral pronoun accuracy. However, inconsistencies are observed in handling neopronouns and reverse inference tasks, highlighting ongoing gaps in identity-sensitive reasoning by LLMs. The results indicate progress in addressing pronoun usage challenges in responsible AI, with implications for inclusive AI research and potential avenues for future improvements. <div>
arXiv:2508.00788v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in sensitive contexts where fairness and inclusivity are critical. Pronoun usage, especially concerning gender-neutral and neopronouns, remains a key challenge for responsible AI. Prior work, such as the MISGENDERED benchmark, revealed significant limitations in earlier LLMs' handling of inclusive pronouns, but was constrained to outdated models and limited evaluations. In this study, we introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs' pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4, DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender identity inference. Our results show notable improvements compared with previous studies, especially in binary and gender-neutral pronoun accuracy. However, accuracy on neopronouns and reverse inference tasks remains inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We discuss implications, model-specific observations, and avenues for future inclusive AI research.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models</title>
<link>https://arxiv.org/abs/2508.00819</link>
<guid>https://arxiv.org/abs/2508.00819</guid>
<content:encoded><![CDATA[
<div> Dynamic Adaptive Length Expansion, Denoising, Diffusion Large Language Models, Task-appropriate length, Computational efficiency
<br />
Summary:
DAEDAL is a novel denoising strategy for Diffusion Large Language Models (DLLMs) that enables dynamic adaptive length expansion. It addresses the static length constraint of DLLMs by iteratively expanding the generation length based on internal signals and intervening to expand insufficient regions during the denoising process. Through extensive experiments, DAEDAL achieves comparable or superior performance to fixed-length baselines while improving computational efficiency by increasing the effective token ratio. By bridging this critical gap in DLLMs, DAEDAL unlocks new potential for efficient and capable generation, paving the way for advancements in language modeling technology. <div>
arXiv:2508.00819v1 Announce Type: new 
Abstract: Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length. This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models</title>
<link>https://arxiv.org/abs/2508.00028</link>
<guid>https://arxiv.org/abs/2508.00028</guid>
<content:encoded><![CDATA[
<div> Markov chain model, spectrum availability prediction, propagation models, cognitive radio networks, dynamic spectrum access<br />
<br />
Summary:<br />
The paper introduces a framework for predicting spectrum availability, combining a two-state Markov chain model and ITU-R propagation models to enhance accuracy in identifying available spectrum in both time and space. The Markov chain captures primary user activity patterns over time, while propagation models consider path loss and clutter effects to determine interference thresholds at secondary user locations. The proposed method offers improved accuracy in predicting spectrum opportunities, with low computational costs, making it suitable for real-time spectrum management in cognitive radio networks and dynamic spectrum sharing systems. The scalability and efficiency of the framework are analyzed, highlighting its flexibility for adaptation to various scenarios and frequency bands. The results show that the approach effectively identifies available spectrum, demonstrating its potential for practical applications in dynamic spectrum access. <br /><br /> <div>
arXiv:2508.00028v1 Announce Type: cross 
Abstract: Spectrum resources are often underutilized across time and space, motivating dynamic spectrum access strategies that allow secondary users to exploit unused frequencies. A key challenge is predicting when and where spectrum will be available (i.e., unused by primary licensed users) in order to enable proactive and interference-free access. This paper proposes a scalable framework for spectrum availability prediction that combines a two-state Markov chain model of primary user activity with high-fidelity propagation models from the ITU-R (specifically Recommendations P.528 and P.2108). The Markov chain captures temporal occupancy patterns, while the propagation models incorporate path loss and clutter effects to determine if primary signals exceed interference thresholds at secondary user locations. By integrating these components, the proposed method can predict spectrum opportunities both in time and space with improved accuracy. We develop the system model and algorithm for the approach, analyze its scalability and computational efficiency, and discuss assumptions, limitations, and potential applications. The framework is flexible and can be adapted to various frequency bands and scenarios. The results and analysis show that the proposed approach can effectively identify available spectrum with low computational cost, making it suitable for real-time spectrum management in cognitive radio networks and other dynamic spectrum sharing systems.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries</title>
<link>https://arxiv.org/abs/2508.00033</link>
<guid>https://arxiv.org/abs/2508.00033</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Python code generation, conversational data analysis, synthetic data generation, model evaluation <br />
Summary: 
Large Language Models (LLMs) are widely used for automating code generation in scientific research. This study benchmarks state-of-the-art LLMs in generating Python code for complex tasks using unfamiliar APIs. Two scenarios, conversational data analysis and synthetic data generation, were tested with structured prompts. Results showed that only a few models consistently produced correct code, with GPT-4.1 performing the best. The study also identified shortcomings in third-party libraries, highlighting the importance of clear documentation and prompt design. These findings underscore the current limitations of LLMs in scientific automation and emphasize the need for further advancements in model capabilities. <br /><br />Summary: <div>
arXiv:2508.00033v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have advanced rapidly as tools for automating code generation in scientific research, yet their ability to interpret and use unfamiliar Python APIs for complex computational experiments remains poorly characterized. This study systematically benchmarks a selection of state-of-the-art LLMs in generating functional Python code for two increasingly challenging scenarios: conversational data analysis with the \textit{ParShift} library, and synthetic data generation and clustering using \textit{pyclugen} and \textit{scikit-learn}. Both experiments use structured, zero-shot prompts specifying detailed requirements but omitting in-context examples. Model outputs are evaluated quantitatively for functional correctness and prompt compliance over multiple runs, and qualitatively by analyzing the errors produced when code execution fails. Results show that only a small subset of models consistently generate correct, executable code, with GPT-4.1 standing out as the only model to always succeed in both tasks. In addition to benchmarking LLM performance, this approach helps identify shortcomings in third-party libraries, such as unclear documentation or obscure implementation bugs. Overall, these findings highlight current limitations of LLMs for end-to-end scientific automation and emphasize the need for careful prompt design, comprehensive library documentation, and continued advances in language model capabilities.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Code Generation with LLM-based Agents</title>
<link>https://arxiv.org/abs/2508.00083</link>
<guid>https://arxiv.org/abs/2508.00083</guid>
<content:encoded><![CDATA[
<div> autonomy, task scope, engineering practicality, LLM, code generation agents
Summary:
Code generation agents powered by large language models (LLMs) are transforming software development with autonomy to manage the workflow independently, expanded task scope covering the full SDLC, and emphasis on engineering practicality. This survey examines the evolution of LLM-based code generation agents, categorizing techniques including single-agent and multi-agent architectures. Applications across the SDLC are discussed, along with evaluation benchmarks and representative tools. Challenges are identified, leading to proposed long-term research directions for the field. <div>
arXiv:2508.00083v1 Announce Type: cross 
Abstract: Code generation agents powered by large language models (LLMs) are revolutionizing the software development paradigm. Distinct from previous code generation techniques, code generation agents are characterized by three core features. 1) Autonomy: the ability to independently manage the entire workflow, from task decomposition to coding and debugging. 2) Expanded task scope: capabilities that extend beyond generating code snippets to encompass the full software development lifecycle (SDLC). 3) Enhancement of engineering practicality: a shift in research emphasis from algorithmic innovation toward practical engineering challenges, such as system reliability, process management, and tool integration. This domain has recently witnessed rapid development and an explosion in research, demonstrating significant application potential. This paper presents a systematic survey of the field of LLM-based code generation agents. We trace the technology's developmental trajectory from its inception and systematically categorize its core techniques, including both single-agent and multi-agent architectures. Furthermore, this survey details the applications of LLM-based agents across the full SDLC, summarizes mainstream evaluation benchmarks and metrics, and catalogs representative tools. Finally, by analyzing the primary challenges, we identify and propose several foundational, long-term research directions for the future work of the field.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs</title>
<link>https://arxiv.org/abs/2508.00161</link>
<guid>https://arxiv.org/abs/2508.00161</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, interpretability, fine-tuning, backdoors, model auditing

Summary:
This work introduces a new method for interpreting, monitoring, and controlling fine-tuned large language models (LLMs) by focusing on weights instead of activations. By analyzing the top singular vectors of weight differences between a fine-tuned model and its base model, it can detect newly acquired behaviors with high precision. This method is effective in detecting backdoored models that can bypass safety mechanisms with a minimal false positive rate. Furthermore, it can also detect inference on erased topics in models that have undergone unlearning and even steer the model to recover "unlearned" information. This approach shows potential for pre-deployment model auditing by uncovering model-specific fine-tuning focuses, such as marketing strategies and prompt generation. The implementation of this method is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2508.00161v1 Announce Type: cross 
Abstract: The releases of powerful open-weight large language models (LLMs) are often not accompanied by access to their full training data. Existing interpretability methods, particularly those based on activations, often require or assume distributionally similar data. This is a significant limitation when detecting and defending against novel potential threats like backdoors, which are by definition out-of-distribution.
  In this work, we introduce a new method for understanding, monitoring and controlling fine-tuned LLMs that interprets weights, rather than activations, thereby side stepping the need for data that is distributionally similar to the unknown training data. We demonstrate that the top singular vectors of the weight difference between a fine-tuned model and its base model correspond to newly acquired behaviors. By monitoring the cosine similarity of activations along these directions, we can detect salient behaviors introduced during fine-tuning with high precision.
  For backdoored models that bypasses safety mechanisms when a secret trigger is present, our method stops up to 100% of attacks with a false positive rate below 1.2%. For models that have undergone unlearning, we detect inference on erased topics with accuracy up to 95.42% and can even steer the model to recover "unlearned" information. Besides monitoring, our method also shows potential for pre-deployment model auditing: by analyzing commercial instruction-tuned models (OLMo, Llama, Qwen), we are able to uncover model-specific fine-tuning focus including marketing strategies and Midjourney prompt generation.
  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI</title>
<link>https://arxiv.org/abs/2508.00171</link>
<guid>https://arxiv.org/abs/2508.00171</guid>
<content:encoded><![CDATA[
<div> medical imaging, Vision-Language Models, Selective Modality Shifting, modality-specific biases, multimodal models <br />
Summary: <br />
The study introduces Selective Modality Shifting (SMS) to assess biases in Vision-Language Models (VLMs) towards different modalities in medical image analysis. Six VLMs were evaluated on chest X-ray and scanning laser ophthalmoscopy datasets, revealing a strong reliance on text input over visual cues. Despite the presence of complementary visual information, models often favored text details, as confirmed by qualitative attention-based analysis. The study emphasizes the necessity of designing and evaluating multimodal medical models that integrate visual and textual cues effectively, rather than relying solely on single-modality signals. <div>
arXiv:2508.00171v1 Announce Type: cross 
Abstract: Clinical decision-making relies on the integrated analysis of medical images and the associated clinical reports. While Vision-Language Models (VLMs) can offer a unified framework for such tasks, they can exhibit strong biases toward one modality, frequently overlooking critical visual cues in favor of textual information. In this work, we introduce Selective Modality Shifting (SMS), a perturbation-based approach to quantify a model's reliance on each modality in binary classification tasks. By systematically swapping images or text between samples with opposing labels, we expose modality-specific biases. We assess six open-source VLMs-four generalist models and two fine-tuned for medical data-on two medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray) and FairVLMed (scanning laser ophthalmoscopy). By assessing model performance and the calibration of every model in both unperturbed and perturbed settings, we reveal a marked dependency on text input, which persists despite the presence of complementary visual information. We also perform a qualitative attention-based analysis which further confirms that image content is often overshadowed by text details. Our findings highlight the importance of designing and evaluating multimodal medical models that genuinely integrate visual and textual cues, rather than relying on single-modality signals.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization</title>
<link>https://arxiv.org/abs/2508.00222</link>
<guid>https://arxiv.org/abs/2508.00222</guid>
<content:encoded><![CDATA[
arXiv:2508.00222v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its inherently on-policy strategy with LLM's immense action space and sparse reward. Further, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel approach that synergizes internal exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components: Multiple Importance Sampling to address for distributional mismatch from external data, and an Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. The results show that RL-PLUS achieves state-of-the-art performance compared with existing RLVR methods on six math reasoning benchmarks and exhibits superior performance on six out-of-distribution reasoning tasks. It also achieves consistent and significant gains across diverse model families, with average relative improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across multiple benchmarks indicate that RL-PLUS effectively resolves the capability boundary collapse problem.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product</title>
<link>https://arxiv.org/abs/2508.00230</link>
<guid>https://arxiv.org/abs/2508.00230</guid>
<content:encoded><![CDATA[
arXiv:2508.00230v1 Announce Type: cross 
Abstract: Parameter-efficient fine-tuning (PEFT) has become a standard approach for adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation (LoRA) has achieved notable success. However, recent studies have highlighted its limitations compared against full-rank alternatives, particularly when applied to multimodal and large language models. In this work, we present a quantitative comparison amongst full-rank and low-rank PEFT methods using a synthetic matrix approximation benchmark with controlled spectral properties. Our results confirm that LoRA struggles to approximate matrices with relatively flat spectrums or high frequency components -- signs of high effective ranks. To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the Khatri-Rao product to produce weight updates, which, by construction, tends to produce matrix product with a high effective rank. We demonstrate performance gains with KRAdapter on vision-language models up to 1B parameters and on large language models up to 8B parameters, particularly on unseen common-sense reasoning tasks. In addition, KRAdapter maintains the memory and compute efficiency of LoRA, making it a practical and robust alternative to fine-tune billion-scale parameter models.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning</title>
<link>https://arxiv.org/abs/2508.00271</link>
<guid>https://arxiv.org/abs/2508.00271</guid>
<content:encoded><![CDATA[
arXiv:2508.00271v1 Announce Type: cross 
Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the principle of learning-by-doing, where expertise is developed through hands-on practice and continual self-improvement. MetaAgent starts with a minimal workflow, equipped only with basic reasoning and adaptive help-seeking abilities. When a knowledge gap is encountered, MetaAgent generates natural language help requests, which are routed to the most suitable external tool by a dedicated tool router. As MetaAgent solves tasks, it continually conducts self-reflection and answer verification, distilling actionable experience into concise texts that are dynamically incorporated into future task contexts. Besides, MetaAgent autonomously builds in-house tools and a persistent knowledge base by organizing its tool-use history, further enhancing its ability to retrieve and integrate relevant information We term this continual, data-driven process as \textit{meta tool learning}, through which MetaAgent incrementally refines its reasoning and tool-use strategies, without changing model parameters or requiring further post-training. Evaluated on challenging knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp, MetaAgent consistently outperforms workflow-based baselines and matches or exceeds end-to-end trained agents, demonstrating the promise of self-evolving agentic systems for robust, general-purpose knowledge discovery. We provide our source codes in https://github.com/qhjqhj00/MetaAgent.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: The Divergence Between Human and LLM-Generated Tasks</title>
<link>https://arxiv.org/abs/2508.00282</link>
<guid>https://arxiv.org/abs/2508.00282</guid>
<content:encoded><![CDATA[
arXiv:2508.00282v1 Announce Type: cross 
Abstract: Humans constantly generate a diverse range of tasks guided by internal motivations. While generative agents powered by large language models (LLMs) aim to simulate this complex behavior, it remains uncertain whether they operate on similar cognitive principles. To address this, we conducted a task-generation experiment comparing human responses with those of an LLM agent (GPT-4o). We find that human task generation is consistently influenced by psychological drivers, including personal values (e.g., Openness to Change) and cognitive style. Even when these psychological drivers are explicitly provided to the LLM, it fails to reflect the corresponding behavioral patterns. They produce tasks that are markedly less social, less physical, and thematically biased toward abstraction. Interestingly, while the LLM's tasks were perceived as more fun and novel, this highlights a disconnect between its linguistic proficiency and its capacity to generate human-like, embodied goals.We conclude that there is a core gap between the value-driven, embodied nature of human cognition and the statistical patterns of LLMs, highlighting the necessity of incorporating intrinsic motivation and physical grounding into the design of more human-aligned agents.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge</title>
<link>https://arxiv.org/abs/2508.00324</link>
<guid>https://arxiv.org/abs/2508.00324</guid>
<content:encoded><![CDATA[
arXiv:2508.00324v1 Announce Type: cross 
Abstract: Although large reasoning models (LRMs) have demonstrated impressive capabilities on complex tasks, recent studies reveal that these models frequently fulfill harmful user instructions, raising significant safety concerns. In this paper, we investigate the underlying cause of LRM safety risks and find that models already possess sufficient safety knowledge but fail to activate it during reasoning. Based on this insight, we propose R1-Act, a simple and efficient post-training method that explicitly triggers safety knowledge through a structured reasoning process. R1-Act achieves strong safety improvements while preserving reasoning performance, outperforming prior alignment methods. Notably, it requires only 1,000 training examples and 90 minutes of training on a single RTX A6000 GPU. Extensive experiments across multiple LRM backbones and sizes demonstrate the robustness, scalability, and practical efficiency of our approach.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLMs for Unit Test Generation from Real-World Functions</title>
<link>https://arxiv.org/abs/2508.00408</link>
<guid>https://arxiv.org/abs/2508.00408</guid>
<content:encoded><![CDATA[
arXiv:2508.00408v1 Announce Type: cross 
Abstract: Recently, large language models (LLMs) have shown great promise in automating unit test generation, significantly reducing the manual effort required by developers. To effectively evaluate the capabilities of LLMs in this domain, it is crucial to have a well-designed benchmark that accurately reflects real-world scenarios and mitigates common pitfalls. Existing LLM test generation benchmarks are limited by two critical drawbacks: data contamination and structurally simple function code. As a result, we often cannot rely on the validity of scientific conclusions drawn from empirical studies using these limited benchmarks. The empirical evidence presented may be biased due to contamination and may fail to generalize beyond toy programs due to structural simplicity.
  To address these problems, we introduce ULT (UnLeakedTestbench), a new benchmark specifically designed for function-level unit test generation from real-world Python functions. ULT is constructed through a multi-stage curation process that ensures high cyclomatic complexity and mitigates test case contamination. With 3,909 carefully selected function-level tasks, ULT provides a more realistic and challenging evaluation of LLMs' test generation capabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT with leaked tests designed to enable a controlled analysis of memorization versus reasoning in test generation. Our evaluation results demonstrate that ULT is significantly more challenging. For example, test cases generated by LLMs only achieve 41.32\%, 45.10\%, 30.22\%, and 40.21\% for accuracy, statement coverage, branch coverage, and mutation score on average for all LLMs, respectively. These results are substantially lower than the corresponding metrics on TestEval (91.79\%, 92.18\%, 82.04\%, and 49.69\%) and PLT (47.07\%, 55.13\%, 40.07\%, and 50.80\%).
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training</title>
<link>https://arxiv.org/abs/2508.00414</link>
<guid>https://arxiv.org/abs/2508.00414</guid>
<content:encoded><![CDATA[
arXiv:2508.00414v1 Announce Type: cross 
Abstract: General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at https://github.com/Tencent/CognitiveKernel-Pro
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-grained Spatiotemporal Grounding on Egocentric Videos</title>
<link>https://arxiv.org/abs/2508.00518</link>
<guid>https://arxiv.org/abs/2508.00518</guid>
<content:encoded><![CDATA[
arXiv:2508.00518v1 Announce Type: cross 
Abstract: Spatiotemporal video grounding aims to localize target entities in videos based on textual queries. While existing research has made significant progress in exocentric videos, the egocentric setting remains relatively underexplored, despite its growing importance in applications such as augmented reality and robotics. In this work, we conduct a systematic analysis of the discrepancies between egocentric and exocentric videos, revealing key challenges such as shorter object durations, sparser trajectories, smaller object sizes, and larger positional shifts. To address these challenges, we introduce EgoMask, the first pixel-level benchmark for fine-grained spatiotemporal grounding in egocentric videos. It is constructed by our proposed automatic annotation pipeline, which annotates referring expressions and object masks across short-, medium-, and long-term videos. Additionally, we create EgoMask-Train, a large-scale training dataset to facilitate model development. Experiments demonstrate that the state-of-the-art spatiotemporal grounding models perform poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields significant improvements, while preserving performance on exocentric datasets. Our work thus provides essential resources and insights for advancing egocentric video understanding. Our code is available at https://github.com/LaVi-Lab/EgoMask .
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations</title>
<link>https://arxiv.org/abs/2508.00534</link>
<guid>https://arxiv.org/abs/2508.00534</guid>
<content:encoded><![CDATA[
arXiv:2508.00534v1 Announce Type: cross 
Abstract: The rise of multi-paradigm languages challenges traditional classification methods, leading to practical software engineering issues like interoperability defects. This systematic literature review (SLR) maps the formal foundations of programming paradigms. Our objective is twofold: (1) to assess the state of the art of classification formalisms and their limitations, and (2) to identify the conceptual primitives and mathematical frameworks for a more powerful, reconstructive approach.
  Based on a synthesis of 74 primary studies, we find that existing taxonomies lack conceptual granularity, a unified formal basis, and struggle with hybrid languages. In response, our analysis reveals a strong convergence toward a compositional reconstruction of paradigms. This approach identifies a minimal set of orthogonal, atomic primitives and leverages mathematical frameworks, predominantly Type theory, Category theory and Unifying Theories of Programming (UTP), to formally guarantee their compositional properties.
  We conclude that the literature reflects a significant intellectual shift away from classification towards these promising formal, reconstructive frameworks. This review provides a map of this evolution and proposes a research agenda for their unification.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism</title>
<link>https://arxiv.org/abs/2508.00554</link>
<guid>https://arxiv.org/abs/2508.00554</guid>
<content:encoded><![CDATA[
arXiv:2508.00554v1 Announce Type: cross 
Abstract: In financial trading, large language model (LLM)-based agents demonstrate significant potential. However, the high sensitivity to market noise undermines the performance of LLM-based trading systems. To address this limitation, we propose a novel multi-agent system featuring an internal competitive mechanism inspired by modern corporate management structures. The system consists of two specialized teams: (1) Data Team - responsible for processing and condensing massive market data into diversified text factors, ensuring they fit the model's constrained context. (2) Research Team - tasked with making parallelized multipath trading decisions based on deep research methods. The core innovation lies in implementing a real-time evaluation and ranking mechanism within each team, driven by authentic market feedback. Each agent's performance undergoes continuous scoring and ranking, with only outputs from top-performing agents being adopted. The design enables the system to adaptively adjust to dynamic environment, enhances robustness against market noise and ultimately delivers superior trading performance. Experimental results demonstrate that our proposed system significantly outperforms prevailing multiagent systems and traditional quantitative investment methods across diverse evaluation metrics.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation-Guided Local Editing for Jailbreaking Attacks</title>
<link>https://arxiv.org/abs/2508.00555</link>
<guid>https://arxiv.org/abs/2508.00555</guid>
<content:encoded><![CDATA[
arXiv:2508.00555v1 Announce Type: cross 
Abstract: Jailbreaking is an essential adversarial technique for red-teaming these models to uncover and patch security flaws. However, existing jailbreak methods face significant drawbacks. Token-level jailbreak attacks often produce incoherent or unreadable inputs and exhibit poor transferability, while prompt-level attacks lack scalability and rely heavily on manual effort and human ingenuity. We propose a concise and effective two-stage framework that combines the advantages of these approaches. The first stage performs a scenario-based generation of context and rephrases the original malicious query to obscure its harmful intent. The second stage then utilizes information from the model's hidden states to guide fine-grained edits, effectively steering the model's internal representation of the input from a malicious toward a benign one. Extensive experiments demonstrate that this method achieves state-of-the-art Attack Success Rate, with gains of up to 37.74% over the strongest baseline, and exhibits excellent transferability to black-box models. Our analysis further demonstrates that AGILE maintains substantial effectiveness against prominent defense mechanisms, highlighting the limitations of current safeguards and providing valuable insights for future defense development. Our code is available at https://github.com/yunsaijc/AGILE.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.00589</link>
<guid>https://arxiv.org/abs/2508.00589</guid>
<content:encoded><![CDATA[
arXiv:2508.00589v1 Announce Type: cross 
Abstract: Autonomous driving systems must operate reliably in safety-critical scenarios, particularly those involving unusual or complex behavior by Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets is essential for robust evaluation and generalization, but retrieving such rare human behavior scenarios within the long tail of large-scale datasets is challenging. To support targeted evaluation of autonomous driving systems in diverse, human-centered scenarios, we propose a novel context-aware motion retrieval framework. Our method combines Skinned Multi-Person Linear (SMPL)-based motion sequences and corresponding video frames before encoding them into a shared multimodal embedding space aligned with natural language. Our approach enables the scalable retrieval of human behavior and their context through text queries. This work also introduces our dataset WayMoCo, an extension of the Waymo Open Dataset. It contains automatically labeled motion and scene context descriptions derived from generated pseudo-ground-truth SMPL sequences and corresponding image data. Our approach outperforms state-of-the-art models by up to 27.5% accuracy in motion-context retrieval, when evaluated on the WayMoCo dataset.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demo: TOSense -- What Did You Just Agree to?</title>
<link>https://arxiv.org/abs/2508.00659</link>
<guid>https://arxiv.org/abs/2508.00659</guid>
<content:encoded><![CDATA[
arXiv:2508.00659v1 Announce Type: cross 
Abstract: Online services often require users to agree to lengthy and obscure Terms of Service (ToS), leading to information asymmetry and legal risks. This paper proposes TOSense-a Chrome extension that allows users to ask questions about ToS in natural language and get concise answers in real time. The system combines (i) a crawler "tos-crawl" that automatically extracts ToS content, and (ii) a lightweight large language model pipeline: MiniLM for semantic retrieval and BART-encoder for answer relevance verification. To avoid expensive manual annotation, we present a novel Question Answering Evaluation Pipeline (QEP) that generates synthetic questions and verifies the correctness of answers using clustered topic matching. Experiments on five major platforms, Apple, Google, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of TOSense (with up to 44.5% accuracy) across varying number of topic clusters. During the demonstration, we will showcase TOSense in action. Attendees will be able to experience seamless extraction, interactive question answering, and instant indexing of new sites.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach</title>
<link>https://arxiv.org/abs/2508.00695</link>
<guid>https://arxiv.org/abs/2508.00695</guid>
<content:encoded><![CDATA[
arXiv:2508.00695v1 Announce Type: cross 
Abstract: The classification of clinical notes into specific diagnostic categories is critical in healthcare, especially for mental health conditions like Anxiety and Adjustment Disorder. In this study, we compare the performance of various Artificial Intelligence models, including both traditional Machine Learning approaches (Random Forest, Support Vector Machine, K-nearest neighbors, Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT and SciBERT), to classify clinical notes into these two diagnoses. Additionally, we implemented three oversampling strategies: No Oversampling, Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to assess their impact on model performance. Hyperparameter tuning was also applied to optimize model accuracy. Our results indicate that oversampling techniques had minimal impact on model performance overall. The only exception was SMOTE, which showed a positive effect specifically with BERT-based models. However, hyperparameter optimization significantly improved accuracy across the models, enhancing their ability to generalize and perform on the dataset. The Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy among machine learning approaches, both reaching 96%, while the DistilBERT and SciBERT models also attained 96% accuracy in the deep learning category. These findings underscore the importance of hyperparameter tuning in maximizing model performance. This study contributes to the ongoing research on AI-assisted diagnostic tools in mental health by providing insights into the efficacy of different model architectures and data balancing methods.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Semantic Parsing: Improving Generalization with Lexical Knowledge</title>
<link>https://arxiv.org/abs/2412.10207</link>
<guid>https://arxiv.org/abs/2412.10207</guid>
<content:encoded><![CDATA[
arXiv:2412.10207v2 Announce Type: replace 
Abstract: Open-domain semantic parsing remains a challenging task, as neural models often rely on heuristics and struggle to handle unseen concepts. In this paper, we investigate the potential of large language models (LLMs) for this task and introduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective approach that integrates external symbolic knowledge into the parsing process. Our experiments not only show that LLMs outperform previous encoder-decoder baselines for semantic parsing, but that RASP further enhances their ability to predict unseen concepts, nearly doubling the performance of previous models on out-of-distribution concepts. These findings highlight the promise of leveraging large language models and retrieval mechanisms for robust and open-domain semantic parsing.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Investigation into Value Misalignment in LLM-Generated Texts for Cultural Heritage</title>
<link>https://arxiv.org/abs/2501.02039</link>
<guid>https://arxiv.org/abs/2501.02039</guid>
<content:encoded><![CDATA[
arXiv:2501.02039v3 Announce Type: replace 
Abstract: As Large Language Models (LLMs) become increasingly prevalent in tasks related to cultural heritage, such as generating descriptions of historical monuments, translating ancient texts, preserving oral traditions, and creating educational content, their ability to produce accurate and culturally aligned texts is being increasingly relied upon by users and researchers. However, cultural value misalignments may exist in generated texts, such as the misrepresentation of historical facts, the erosion of cultural identity, and the oversimplification of complex cultural narratives, which may lead to severe consequences. Therefore, investigating value misalignment in the context of LLM for cultural heritage is crucial for mitigating these risks, yet there has been a significant lack of systematic and comprehensive study and investigation in this area. To fill this gap, we systematically assess the reliability of LLMs in generating culturally aligned texts for cultural heritage-related tasks. We conduct a comprehensive evaluation by compiling an extensive set of 1066 query tasks covering 5 widely recognized categories with 17 aspects within the knowledge framework of cultural heritage across 5 open-source LLMs, and examine both the type and rate of cultural value misalignments in the generated texts. Using both automated and manual approaches, we effectively detect and analyze the cultural value misalignments in LLM-generated texts. Our findings are concerning: over 65% of the generated texts exhibit notable cultural misalignments, with certain tasks demonstrating almost complete misalignment with key cultural values. Beyond these findings, this paper introduces a benchmark dataset and a comprehensive evaluation workflow that can serve as a valuable resource for future research aimed at enhancing the cultural sensitivity and reliability of LLMs.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in LLM Writing Assistance</title>
<link>https://arxiv.org/abs/2502.08395</link>
<guid>https://arxiv.org/abs/2502.08395</guid>
<content:encoded><![CDATA[
arXiv:2502.08395v2 Announce Type: replace 
Abstract: Large language models (LLMs) are helping millions of users write texts about diverse issues, and in doing so expose users to different ideas and perspectives. This creates concerns about issue bias, where an LLM tends to present just one perspective on a given issue, which in turn may influence how users think about this issue. So far, it has not been possible to measure which issue biases LLMs actually manifest in real user interactions, making it difficult to address the risks from biased LLMs. Therefore, we create IssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM writing assistance, which we construct based on 3.9k templates (e.g. "write a blog about") and 212 political issues (e.g. "AI regulation") from real user interactions. Using IssueBench, we show that issue biases are common and persistent in state-of-the-art LLMs. We also show that biases are remarkably similar across models, and that all models align more with US Democrat than Republican voter opinion on a subset of issues. IssueBench can easily be adapted to include other issues, templates, or tasks. By enabling robust and realistic measurement, we hope that IssueBench can bring a new quality of evidence to ongoing discussions about LLM biases and how to address them.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better Embeddings with Coupled Adam</title>
<link>https://arxiv.org/abs/2502.08441</link>
<guid>https://arxiv.org/abs/2502.08441</guid>
<content:encoded><![CDATA[
arXiv:2502.08441v3 Announce Type: replace 
Abstract: Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEFL: Enhancing Educational Assignment Feedback with LLM Agents</title>
<link>https://arxiv.org/abs/2502.12927</link>
<guid>https://arxiv.org/abs/2502.12927</guid>
<content:encoded><![CDATA[
arXiv:2502.12927v2 Announce Type: replace 
Abstract: Providing high-quality feedback to student assignments is crucial for student success, but it is constrained by time and costs. In this work, we introduce Synthetic Educational Feedback Loops (SEFL), a synthetic data framework designed to generate data that resembles immediate, on-demand feedback at scale without relying on extensive, real-world student assignments. To get this type of data, two large language models (LLMs) operate in teacher-student roles to simulate assignment completion and formative feedback, generating synthetic pairs of student work and corresponding critiques and actionable improvements from a teacher. With this data, we fine-tune smaller, more computationally efficient LLMs on these synthetic pairs, enabling them to replicate key features of high-quality, goal-oriented feedback. Unlike personalized tutoring approaches that offer multi-turn, individualized instruction, SEFL specifically focuses on replicating the teacher-student assignment feedback loop in higher education. Through comprehensive evaluations with four LLM judges and three human experts, we demonstrate that SEFL-tuned models outperform both their non-tuned counterparts in feedback quality and an existing baseline. The potential for societal impact is reinforced by extensive qualitative comments by ratings by human stakeholders -- both students and higher education instructors. All in all, SEFL has substantial potential to transform feedback processes for higher education and beyond.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in Space: Finding the Right Tokens for Structured Output</title>
<link>https://arxiv.org/abs/2502.14969</link>
<guid>https://arxiv.org/abs/2502.14969</guid>
<content:encoded><![CDATA[
arXiv:2502.14969v2 Announce Type: replace 
Abstract: General-purpose language models are trained to produce varied natural language outputs, but for some tasks, like annotation or classification, we need more specific output formats. LLM systems increasingly support structured output, which enforces formats by sampling tokens according to a grammar -- but also unpredictably reduces downstream performance. Are there systematic differences between grammars that appear semantically (and often visually) similar to humans? To answer this, we test four popular model families with five varying output formats on four common NLP benchmarks. We find all models perform most accurately when guided to use formats respecting convention, such as letters for multiple choice and real numbers for numerical prediction. Performance also improves by 5%-10% when guiding models to return tokens incorporating leading whitespace, with smaller models benefiting the most. We find leading whitespace helps models avoid structural deficiencies in subword token representations. We finally present best practices for researchers using language models as zero-shot classifiers with structured output.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2502.17407</link>
<guid>https://arxiv.org/abs/2502.17407</guid>
<content:encoded><![CDATA[
arXiv:2502.17407v2 Announce Type: replace 
Abstract: Scaling pre-training compute has proven effective for achieving mulitlinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods-Outcome Reward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing (BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although "thinking LLMs" have recently garnered significant attention, we find that their performance is comparable to traditional scaling methods like best-of-N once constrained to similar levels of inference FLOPs. Moreover, while BF yields a 20-point improvement on English AIME, it provides only a 1.94-point average gain across other languages-a pattern consistent across the other test-time scaling methods we studied-higlighting that test-time scaling may not generalize as effectively to multilingual tasks. To foster further research, we release MCLM, MR1-1.5B, and evaluation results.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models Know How Much They Know?</title>
<link>https://arxiv.org/abs/2502.19573</link>
<guid>https://arxiv.org/abs/2502.19573</guid>
<content:encoded><![CDATA[
arXiv:2502.19573v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have emerged as highly capable systems and are increasingly being integrated into various uses. However, the rapid pace of their deployment has outpaced a comprehensive understanding of their internal mechanisms and a delineation of their capabilities and limitations. A desired attribute of an intelligent system is its ability to recognize the scope of its own knowledge. To investigate whether LLMs embody this characteristic, we develop a benchmark designed to challenge these models to enumerate all information they possess on specific topics. This benchmark evaluates whether the models recall excessive, insufficient, or the precise amount of information, thereby indicating their awareness of their own knowledge. Our findings reveal that all tested LLMs, given sufficient scale, demonstrate an understanding of how much they know about specific topics. While different architectures exhibit varying rates of this capability's emergence, the results suggest that awareness of knowledge may be a generalizable attribute of LLMs. Further research is needed to confirm this potential and fully elucidate the underlying mechanisms.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Post-training of Large Language Models</title>
<link>https://arxiv.org/abs/2503.06072</link>
<guid>https://arxiv.org/abs/2503.06072</guid>
<content:encoded><![CDATA[
arXiv:2503.06072v3 Announce Type: replace 
Abstract: The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained architectures often reveal limitations in specialized contexts, including restricted reasoning capacities, ethical uncertainties, and suboptimal domain-specific performance. These challenges necessitate advanced post-training language models (PoLMs) to address these shortcomings, such as OpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or LRMs). This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues. Charting progress from ChatGPT's alignment strategies to DeepSeek-R1's innovative reasoning advancements, we illustrate how PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities, and enhance domain adaptability. Our contributions include a pioneering synthesis of PoLM evolution, a structured taxonomy categorizing techniques and datasets, and a strategic agenda emphasizing the role of LRMs in improving reasoning proficiency and domain flexibility. As the first survey of its scope, this work consolidates recent PoLM advancements and establishes a rigorous intellectual framework for future research, fostering the development of LLMs that excel in precision, ethical robustness, and versatility across scientific and societal applications.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through Lightweight Vocabulary Adaptation</title>
<link>https://arxiv.org/abs/2503.19693</link>
<guid>https://arxiv.org/abs/2503.19693</guid>
<content:encoded><![CDATA[
arXiv:2503.19693v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown impressive versatility as general purpose models. However, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass. In domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation, reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low-resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance. Our results show that AdaptiVocab reduces token usage by over 25% without compromising performance
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemInsight: Autonomous Memory Augmentation for LLM Agents</title>
<link>https://arxiv.org/abs/2503.21760</link>
<guid>https://arxiv.org/abs/2503.21760</guid>
<content:encoded><![CDATA[
arXiv:2503.21760v2 Announce Type: replace 
Abstract: Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol</title>
<link>https://arxiv.org/abs/2504.10284</link>
<guid>https://arxiv.org/abs/2504.10284</guid>
<content:encoded><![CDATA[
arXiv:2504.10284v3 Announce Type: replace 
Abstract: Literature review tables are essential for summarizing and comparing collections of scientific papers. We explore the task of generating tables that best fulfill a user's informational needs given a collection of scientific papers. Building on recent work (Newman et al., 2024), we extend prior approaches to address real-world complexities through a combination of LLM-based methods and human annotations. Our contributions focus on three key challenges encountered in real-world use: (i) User prompts are often under-specified; (ii) Retrieved candidate papers frequently contain irrelevant content; and (iii) Task evaluation should move beyond shallow text similarity techniques and instead assess the utility of inferred tables for information-seeking tasks (e.g., comparing papers). To support reproducible evaluation, we introduce ARXIV2TABLE, a more realistic and challenging benchmark for this task, along with a novel approach to improve literature review table generation in real-world scenarios. Our extensive experiments on this benchmark show that both open-weight and proprietary LLMs struggle with the task, highlighting its difficulty and the need for further advancements. Our dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles</title>
<link>https://arxiv.org/abs/2504.12312</link>
<guid>https://arxiv.org/abs/2504.12312</guid>
<content:encoded><![CDATA[
arXiv:2504.12312v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved significant progress in language understanding and reasoning. Evaluating and analyzing their logical reasoning abilities has therefore become essential. However, existing datasets and benchmarks are often limited to overly simplistic, unnatural, or contextually constrained examples. In response to the growing demand, we introduce SmartyPat-Bench, a challenging, naturally expressed, and systematically labeled benchmark derived from real-world high-quality Reddit posts containing subtle logical fallacies. Unlike existing datasets and benchmarks, it provides more detailed annotations of logical fallacies and features more diverse data. To further scale up the study and address the limitations of manual data collection and labeling - such as fallacy-type imbalance and labor-intensive annotation - we introduce SmartyPat, an automated framework powered by logic programming-based oracles. SmartyPat utilizes Prolog rules to systematically generate logically fallacious statements, which are then refined into fluent natural-language sentences by LLMs, ensuring precise fallacy representation. Extensive evaluation demonstrates that SmartyPat produces fallacies comparable in subtlety and quality to human-generated content and significantly outperforms baseline methods. Finally, experiments reveal nuanced insights into LLM capabilities, highlighting that while excessive reasoning steps hinder fallacy detection accuracy, structured reasoning enhances fallacy categorization performance.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories</title>
<link>https://arxiv.org/abs/2504.16604</link>
<guid>https://arxiv.org/abs/2504.16604</guid>
<content:encoded><![CDATA[
arXiv:2504.16604v2 Announce Type: replace 
Abstract: Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Credible Plan-Driven RAG Method for Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2504.16787</link>
<guid>https://arxiv.org/abs/2504.16787</guid>
<content:encoded><![CDATA[
arXiv:2504.16787v2 Announce Type: replace 
Abstract: Multi-hop question answering (QA) presents significant challenges for retrieval-augmented generation (RAG), particularly in decomposing complex queries into reliable reasoning paths and managing error propagation. Existing RAG methods often suffer from deviations in reasoning paths and cumulative errors in intermediate steps, reducing the fidelity of the final answer. To address these limitations, we propose PAR-RAG (Plan-then-Act-and-Review RAG), a novel framework inspired by the PDCA (Plan-Do-Check-Act) cycle, to enhance both the accuracy and factual consistency in multi-hop question answering. Specifically, PAR-RAG selects exemplars matched by the semantic complexity of the current question to guide complexity-aware top-down planning, resulting in more precise and coherent multi-step reasoning trajectories. This design mitigates reasoning drift and reduces the risk of suboptimal path convergence, a common issue in existing RAG approaches. Furthermore, a dual-verification mechanism evaluates and corrects intermediate errors, ensuring that the reasoning process remains factually grounded. Experimental results on various QA benchmarks demonstrate that PAR-RAG outperforms existing state-of-the-art methods, validating its effectiveness in both performance and reasoning robustness.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory</title>
<link>https://arxiv.org/abs/2505.15055</link>
<guid>https://arxiv.org/abs/2505.15055</guid>
<content:encoded><![CDATA[
arXiv:2505.15055v2 Announce Type: replace 
Abstract: The evaluation of large language models (LLMs) via benchmarks is widespread, yet inconsistencies between different leaderboards and poor separability among top models raise concerns about their ability to accurately reflect authentic model capabilities. This paper provides a critical analysis of benchmark effectiveness, examining mainstream prominent LLM benchmarks using results from diverse models. We first propose Pseudo-Siamese Network for Item Response Theory (PSN-IRT), an enhanced Item Response Theory framework that incorporates a rich set of item parameters within an IRT-grounded architecture. PSN-IRT can be utilized for accurate and reliable estimations of item characteristics and model abilities. Based on PSN-IRT, we conduct extensive analysis on 11 LLM benchmarks comprising 41,871 items, revealing significant and varied shortcomings in their measurement quality. Furthermore, we demonstrate that leveraging PSN-IRT is able to construct smaller benchmarks while maintaining stronger alignment with human preference.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs</title>
<link>https://arxiv.org/abs/2505.17217</link>
<guid>https://arxiv.org/abs/2505.17217</guid>
<content:encoded><![CDATA[
arXiv:2505.17217v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) often exhibit gender bias, resulting in unequal treatment of male and female subjects across different contexts. To address this issue, we propose a novel data generation framework that fosters exploratory thinking in LLMs. Our approach prompts models to generate story pairs featuring male and female protagonists in structurally identical, morally ambiguous scenarios, then elicits and compares their moral judgments. When inconsistencies arise, the model is guided to produce balanced, gender-neutral judgments. These story-judgment pairs are used to fine-tune or optimize the models via Direct Preference Optimization (DPO). Experimental results show that our method significantly reduces gender bias while preserving or even enhancing general model capabilities. We will release the code and generated data. We release the code and generated data at: https://github.com/WeiKangda/LLMs-Exploratory-Bias-Mitigation/tree/main.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMixer: Checkpoint Artifacts as Automatic Data Mixers</title>
<link>https://arxiv.org/abs/2506.21910</link>
<guid>https://arxiv.org/abs/2506.21910</guid>
<content:encoded><![CDATA[
arXiv:2506.21910v2 Announce Type: replace 
Abstract: In language model training, it is desirable to equip models with capabilities from various tasks. However, it is not clear how to directly obtain the right data mixtures for these capabilities as the relationship between data and tasks is difficult to be modeled. In this work, we observe that checkpoint models exhibit emerging capabilities at different points in the training trajectory. Often, the training process saves checkpoints as artifacts that are under-utilized as a source of in-training data signals. We identify these artifact models based on their respective capabilities on the benchmarks and leverage them as data mixers by using their aggregated first-order influence approximation over source data. We demonstrated on eight reasoning benchmarks that the proposed framework shows significant improvements in the pretraining setting, with performance improvements of up to 1.93%. Overall, this shows the potential of checkpoint models to enhance data quality and optimize data mixtures.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism</title>
<link>https://arxiv.org/abs/2507.02962</link>
<guid>https://arxiv.org/abs/2507.02962</guid>
<content:encoded><![CDATA[
arXiv:2507.02962v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, while LLMs remain prone to generating hallucinated or outdated responses due to their static internal knowledge. Recent advancements in Retrieval-Augmented Generation (RAG) methods have aimed to enhance models' search and reasoning capabilities through reinforcement learning (RL). Although these methods demonstrate promising results, they face challenges in training stability and encounter issues such as substantial inference time and restricted capabilities due to reliance on single-query mode. In this paper, we propose RAG-R1, a novel training framework designed to enable LLMs to adaptively leverage internal and external knowledge during the reasoning process. We further expand the generation and retrieval processes within the framework from single-query mode to multi-query parallelism, with the aim of reducing inference time and enhancing the model's capabilities. Extensive experiments on seven question-answering benchmarks demonstrate that our method outperforms the strongest baseline by up to 13.2% and decreases inference time by 11.1%.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Encode Harmfulness and Refusal Separately</title>
<link>https://arxiv.org/abs/2507.11878</link>
<guid>https://arxiv.org/abs/2507.11878</guid>
<content:encoded><![CDATA[
arXiv:2507.11878v2 Announce Type: replace 
Abstract: LLMs are trained to refuse harmful instructions, but do they truly understand harmfulness beyond just refusing? Prior work has shown that LLMs' refusal behaviors can be mediated by a one-dimensional subspace, i.e., a refusal direction. In this work, we identify a new dimension to analyze safety mechanisms in LLMs, i.e., harmfulness, which is encoded internally as a separate concept from refusal. There exists a harmfulness direction that is distinct from the refusal direction. As causal evidence, steering along the harmfulness direction can lead LLMs to interpret harmless instructions as harmful, but steering along the refusal direction tends to elicit refusal responses directly without reversing the model's judgment on harmfulness. Furthermore, using our identified harmfulness concept, we find that certain jailbreak methods work by reducing the refusal signals without reversing the model's internal belief of harmfulness. We also find that adversarially finetuning models to accept harmful instructions has minimal impact on the model's internal belief of harmfulness. These insights lead to a practical safety application: The model's latent harmfulness representation can serve as an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing over-refusals that is robust to finetuning attacks. For instance, our Latent Guard achieves performance comparable to or better than Llama Guard 3 8B, a dedicated finetuned safeguard model, across different jailbreak methods. Our findings suggest that LLMs' internal understanding of harmfulness is more robust than their refusal decision to diverse input instructions, offering a new perspective to study AI safety
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Loss Landscape Degeneracy and Stagewise Development in Transformers</title>
<link>https://arxiv.org/abs/2402.02364</link>
<guid>https://arxiv.org/abs/2402.02364</guid>
<content:encoded><![CDATA[
arXiv:2402.02364v3 Announce Type: replace-cross 
Abstract: Deep learning involves navigating a high-dimensional loss landscape over the neural network parameter space. Over the course of training, complex computational structures form and re-form inside the neural network, leading to shifts in input/output behavior. It is a priority for the science of deep learning to uncover principles governing the development of neural network structure and behavior. Drawing on the framework of singular learning theory, we propose that model development is deeply linked to degeneracy in the local geometry of the loss landscape. We investigate this link by monitoring loss landscape degeneracy throughout training, as quantified by the local learning coefficient, for a transformer language model and an in-context linear regression transformer. We show that training can be divided into distinct periods of change in loss landscape degeneracy, and that these changes in degeneracy coincide with significant changes in the internal computational structure and the input/output behavior of the transformers. This finding provides suggestive evidence that degeneracy and development are linked in transformers, underscoring the potential of a degeneracy-based perspective for understanding modern deep learning.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Maps: Tools for Guiding the Unbounded Space of LLM Behaviors</title>
<link>https://arxiv.org/abs/2409.18203</link>
<guid>https://arxiv.org/abs/2409.18203</guid>
<content:encoded><![CDATA[
arXiv:2409.18203v2 Announce Type: replace-cross 
Abstract: AI policy sets boundaries on acceptable behavior for AI models, but this is challenging in the context of large language models (LLMs): how do you ensure coverage over a vast behavior space? We introduce policy maps, an approach to AI policy design inspired by the practice of physical mapmaking. Instead of aiming for full coverage, policy maps aid effective navigation through intentional design choices about which aspects to capture and which to abstract away. With Policy Projector, an interactive tool for designing LLM policy maps, an AI practitioner can survey the landscape of model input-output pairs, define custom regions (e.g., "violence"), and navigate these regions with if-then policy rules that can act on LLM outputs (e.g., if output contains "violence" and "graphic details," then rewrite without "graphic details"). Policy Projector supports interactive policy authoring using LLM classification and steering and a map visualization reflecting the AI practitioner's work. In an evaluation with 12 AI safety experts, our system helps policy designers craft policies around problematic model behaviors such as incorrect gender assumptions and handling of immediate physical safety threats.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaVA-Video: Video Instruction Tuning With Synthetic Data</title>
<link>https://arxiv.org/abs/2410.02713</link>
<guid>https://arxiv.org/abs/2410.02713</guid>
<content:encoded><![CDATA[
arXiv:2410.02713v3 Announce Type: replace-cross 
Abstract: The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, namely LLaVA-Video-178K. This dataset includes key tasks such as detailed captioning, open-ended question-answering (QA), and multiple-choice QA. By training on this dataset, in combination with existing visual instruction tuning data, we introduce LLaVA-Video, a new video LMM. Our experiments demonstrate that LLaVA-Video achieves strong performance across various video benchmarks, highlighting the effectiveness of our dataset. We plan to release the dataset, its generation pipeline, and the model checkpoints.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Multi-Modal Potentials for Link Prediction on Dynamic Text-Attributed Graphs</title>
<link>https://arxiv.org/abs/2502.19651</link>
<guid>https://arxiv.org/abs/2502.19651</guid>
<content:encoded><![CDATA[
arXiv:2502.19651v2 Announce Type: replace-cross 
Abstract: Dynamic Text-Attributed Graphs (DyTAGs) are a novel graph paradigm that captures evolving temporal events (edges) alongside rich textual attributes. Existing studies can be broadly categorized into TGNN-driven and LLM-driven approaches, both of which encode textual attributes and temporal structures for DyTAG representation. We observe that DyTAGs inherently comprise three distinct modalities: temporal, textual, and structural, often exhibiting completely disjoint distributions. However, the first two modalities are largely overlooked by existing studies, leading to suboptimal performance. To address this, we propose MoMent, a multi-modal model that explicitly models, integrates, and aligns each modality to learn node representations for link prediction. Given the disjoint nature of the original modality distributions, we first construct modality-specific features and encode them using individual encoders to capture correlations across temporal patterns, semantic context, and local structures. Each encoder generates modality-specific tokens, which are then fused into comprehensive node representations with a theoretical guarantee. To avoid disjoint subspaces of these heterogeneous modalities, we propose a dual-domain alignment loss that first aligns their distributions globally and then fine-tunes coherence at the instance level. This enhances coherent representations from temporal, textual, and structural views. Extensive experiments across seven datasets show that MoMent achieves up to 17.28% accuracy improvement and up to 31x speed-up against eight baselines.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2503.22675</link>
<guid>https://arxiv.org/abs/2503.22675</guid>
<content:encoded><![CDATA[
arXiv:2503.22675v3 Announce Type: replace-cross 
Abstract: Sequential Recommendation (SeqRec) aims to predict the next item by capturing sequential patterns from users' historical interactions, playing a crucial role in many real-world recommender systems. However, existing approaches predominantly adopt a direct forward computation paradigm, where the final hidden state of the sequence encoder serves as the user representation. We argue that this inference paradigm, due to its limited computational depth, struggles to model the complex evolving nature of user preferences and lacks a nuanced understanding of long-tail items, leading to suboptimal performance. To address this issue, we propose \textbf{ReaRec}, the first inference-time computing framework for recommender systems, which enhances user representations through implicit multi-step reasoning. Specifically, ReaRec autoregressively feeds the sequence's last hidden state into the sequential recommender while incorporating special reasoning position embeddings to decouple the original item encoding space from the multi-step reasoning space. Moreover, we introduce two lightweight reasoning-based learning methods, Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to further effectively exploit ReaRec's reasoning potential. Extensive experiments on five public real-world datasets and different SeqRec architectures demonstrate the generality and effectiveness of our proposed ReaRec. Remarkably, post-hoc analyses reveal that ReaRec significantly elevates the performance ceiling of multiple sequential recommendation backbones by approximately 30\%-50\%. Thus, we believe this work can open a new and promising avenue for future research in inference-time computing for sequential recommendation.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding</title>
<link>https://arxiv.org/abs/2507.02659</link>
<guid>https://arxiv.org/abs/2507.02659</guid>
<content:encoded><![CDATA[
arXiv:2507.02659v2 Announce Type: replace-cross 
Abstract: Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the \textit{``one drafter for all''} paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs on Real-World Forecasting Against Human Superforecasters</title>
<link>https://arxiv.org/abs/2507.04562</link>
<guid>https://arxiv.org/abs/2507.04562</guid>
<content:encoded><![CDATA[
arXiv:2507.04562v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, but their ability to forecast future events remains understudied. A year ago, large language models struggle to come close to the accuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting questions from Metaculus, comparing their performance against human superforecasters. Frontier models achieve Brier scores that ostensibly surpass the human crowd but still significantly underperform a group of superforecasters.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sound and Complete Neurosymbolic Reasoning with LLM-Grounded Interpretations</title>
<link>https://arxiv.org/abs/2507.09751</link>
<guid>https://arxiv.org/abs/2507.09751</guid>
<content:encoded><![CDATA[
arXiv:2507.09751v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but they exhibit problems with logical consistency in the output they generate. How can we harness LLMs' broad-coverage parametric knowledge in formal reasoning despite their inconsistency? We present a method for directly integrating an LLM into the interpretation function of the formal semantics for a paraconsistent logic. We provide experimental evidence for the feasibility of the method by evaluating the function using datasets created from several short-form factuality benchmarks. Unlike prior work, our method offers a theoretical framework for neurosymbolic reasoning that leverages an LLM's knowledge while preserving the underlying logic's soundness and completeness properties.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models in the Travel Domain: An Industrial Experience</title>
<link>https://arxiv.org/abs/2507.22910</link>
<guid>https://arxiv.org/abs/2507.22910</guid>
<content:encoded><![CDATA[
<div> property booking platforms, Large Language Models, data sources, consistency, reliability

Summary: 
- Online property booking platforms rely on up-to-date information from third-party providers.
- Incomplete or inconsistent details can impact user experience and market share.
- The study integrates Large Language Models (LLMs) into CALEIDOHOTELS to improve data quality.
- Two LLMs, Mistral 7B and Mixtral 8x7B, were evaluated for consistency and reliability.
- Mixtral 8x7B outperformed Mistral 7B in completeness, precision, and hallucination rate but at a higher computational cost.
- The findings highlight the trade-offs between model quality and resource efficiency in production environments. <div>
arXiv:2507.22910v1 Announce Type: new 
Abstract: Online property booking platforms are widely used and rely heavily on consistent, up-to-date information about accommodation facilities, often sourced from third-party providers. However, these external data sources are frequently affected by incomplete or inconsistent details, which can frustrate users and result in a loss of market. In response to these challenges, we present an industrial case study involving the integration of Large Language Models (LLMs) into CALEIDOHOTELS, a property reservation platform developed by FERVENTO. We evaluate two well-known LLMs in this context: Mistral 7B, fine-tuned with QLoRA, and Mixtral 8x7B, utilized with a refined system prompt. Both models were assessed based on their ability to generate consistent and homogeneous descriptions while minimizing hallucinations. Mixtral 8x7B outperformed Mistral 7B in terms of completeness (99.6% vs. 93%), precision (98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), producing shorter yet more concise content (249 vs. 277 words on average). However, this came at a significantly higher computational cost: 50GB VRAM and $1.61/hour versus 5GB and $0.16/hour for Mistral 7B. Our findings provide practical insights into the trade-offs between model quality and resource efficiency, offering guidance for deploying LLMs in production environments and demonstrating their effectiveness in enhancing the consistency and reliability of accommodation data.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing</title>
<link>https://arxiv.org/abs/2507.22911</link>
<guid>https://arxiv.org/abs/2507.22911</guid>
<content:encoded><![CDATA[
<div> Keywords: Electric power marketing, Customer service, Large language models, Dialogue dataset, Knowledge augmentation

Summary:
Electric power marketing customer service is vital for addressing inquiries efficiently. Current systems like China's 95598 hotline often face challenges like slow response times and limited accuracy in domain-specific tasks. While large language models (LLMs) like GPT-4o showcase strong general capabilities, they lack domain expertise and empathy necessary for this field. To address this gap, the researchers introduce ElectriQ, a new benchmark designed to evaluate and enhance LLMs in electric power marketing contexts. ElectriQ includes a dialogue dataset covering key service categories and introduces four evaluation metrics: professionalism, popularity, readability, and user-friendliness. By integrating domain-specific knowledge and employing a knowledge augmentation method, smaller models like LLama3-8B, when fine-tuned and augmented, outperform larger models like GPT-4o in professionalism and user-friendliness. ElectriQ lays the groundwork for developing LLMs tailored to the specific needs of power marketing services. 

<br /><br />Summary: <div>
arXiv:2507.22911v1 Announce Type: new 
Abstract: Electric power marketing customer service plays a critical role in addressing inquiries, complaints, and service requests. However, current systems, such as China's 95598 hotline, often struggle with slow response times, inflexible procedures, and limited accuracy in domain-specific tasks. While large language models (LLMs) like GPT-4o and Claude 3 demonstrate strong general capabilities, they lack the domain expertise and empathy required in this field. To bridge this gap, we introduce ElectriQ, the first benchmark designed to evaluate and enhance LLMs in electric power marketing scenarios. ElectriQ consists of a dialogue dataset covering six key service categories and introduces four evaluation metrics: professionalism, popularity, readability, and user-friendliness. We further incorporate a domain-specific knowledge base and propose a knowledge augmentation method to boost model performance. Experiments on 13 LLMs reveal that smaller models such as LLama3-8B, when fine-tuned and augmented, can surpass GPT-4o in terms of professionalism and user-friendliness. ElectriQ establishes a comprehensive foundation for developing LLMs tailored to the needs of power marketing services.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms</title>
<link>https://arxiv.org/abs/2507.22912</link>
<guid>https://arxiv.org/abs/2507.22912</guid>
<content:encoded><![CDATA[
<div> Keywords: illegal marketplaces, deep and dark web, language models, semi-supervised learning, illicit content detection 

Summary: 
This paper presents a hierarchical classification framework that utilizes fine-tuned language models and a semi-supervised ensemble learning strategy to detect and categorize illicit marketplace content across various online platforms. By combining semantic representations extracted from a specialized transformer model with manually engineered features, the framework can effectively identify illicit sales-related documents and classify them into drug, weapon, or credential categories. Experiments on multiple datasets demonstrate that the model outperforms several baselines, showcasing its accuracy, F1-score, and robustness in detecting illicit content. The proposed approach showcases strong generalization abilities, robustness under limited supervision, and effectiveness in real-world scenarios of illicit content detection. 

Summary: <div>
arXiv:2507.22912v1 Announce Type: new 
Abstract: Illegal marketplaces have increasingly shifted to concealed parts of the internet, including the deep and dark web, as well as platforms such as Telegram, Reddit, and Pastebin. These channels enable the anonymous trade of illicit goods including drugs, weapons, and stolen credentials. Detecting and categorizing such content remains challenging due to limited labeled data, the evolving nature of illicit language, and the structural heterogeneity of online sources. This paper presents a hierarchical classification framework that combines fine-tuned language models with a semi-supervised ensemble learning strategy to detect and classify illicit marketplace content across diverse platforms. We extract semantic representations using ModernBERT, a transformer model for long documents, finetuned on domain-specific data from deep and dark web pages, Telegram channels, Subreddits, and Pastebin pastes to capture specialized jargon and ambiguous linguistic patterns. In addition, we incorporate manually engineered features such as document structure, embedded patterns including Bitcoin addresses, emails, and IPs, and metadata, which complement language model embeddings. The classification pipeline operates in two stages. The first stage uses a semi-supervised ensemble of XGBoost, Random Forest, and SVM with entropy-based weighted voting to detect sales-related documents. The second stage further classifies these into drug, weapon, or credential sales. Experiments on three datasets, including our multi-source corpus, DUTA, and CoDA, show that our model outperforms several baselines, including BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The model achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of 0.95388, demonstrating strong generalization, robustness under limited supervision, and effectiveness in real-world illicit content detection.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models</title>
<link>https://arxiv.org/abs/2507.22913</link>
<guid>https://arxiv.org/abs/2507.22913</guid>
<content:encoded><![CDATA[
<div> Keywords: subject access, large language models, multi-label classification, hybrid framework, Library of Congress Subject Headings <br />
<br />
Summary: <br />
Providing subject access to information resources is crucial for library management systems. While large language models (LLMs) have been used for classification tasks, they are not extensively explored for subject analysis. Traditional machine learning (ML) models struggle with unseen cases in subject analysis, prompting the need for a hybrid framework that combines embedding-based ML models with LLMs. This framework utilizes ML models to predict the optimal number of LCSH labels and post-edit LLM predictions with actual LCSH terms to address hallucinations. Experimental results demonstrate that providing initial predictions and post-editing improve the control and vocabulary alignment of subject term predictions. This approach offers a promising method for enhancing subject analysis using LLMs in libraries. <br /> <div>
arXiv:2507.22913v1 Announce Type: new 
Abstract: Providing subject access to information resources is an essential function of any library management system. Large language models (LLMs) have been widely used in classification and summarization tasks, but their capability to perform subject analysis is underexplored. Multi-label classification with traditional machine learning (ML) models has been used for subject analysis but struggles with unseen cases. LLMs offer an alternative but often over-generate and hallucinate. Therefore, we propose a hybrid framework that integrates embedding-based ML models with LLMs. This approach uses ML models to (1) predict the optimal number of LCSH labels to guide LLM predictions and (2) post-edit the predicted terms with actual LCSH terms to mitigate hallucinations. We experimented with LLMs and the hybrid framework to predict the subject terms of books using the Library of Congress Subject Headings (LCSH). Experiment results show that providing initial predictions to guide LLM generations and imposing post-edits result in more controlled and vocabulary-aligned outputs.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Full Triple Matcher: Integrating all triple elements between heterogeneous Knowledge Graphs</title>
<link>https://arxiv.org/abs/2507.22914</link>
<guid>https://arxiv.org/abs/2507.22914</guid>
<content:encoded><![CDATA[
<div> Knowledge graphs, schema matching, entity matching, context matching, KG integration<br />
Summary:<br />
The study addresses the challenge of context matching in knowledge graphs (KGs). Unlike schema and identity matching, context matching is less explored despite its importance in integrating diverse and complex KGs. The proposed method includes label and triple matching using string manipulation, fuzzy matching, and vector similarity techniques to align entity and predicate labels. By identifying mappings between triples conveying comparable information, entity-matching accuracy is improved. The approach demonstrates competitive performance in the OAEI competition and against supervised methods, achieving high accuracy in various test cases. Additionally, a new dataset is introduced to comprehensively evaluate the triple-matching step. <div>
arXiv:2507.22914v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) are powerful tools for representing and reasoning over structured information. Their main components include schema, identity, and context. While schema and identity matching are well-established in ontology and entity matching research, context matching remains largely unexplored. This is particularly important because real-world KGs often vary significantly in source, size, and information density - factors not typically represented in the datasets on which current entity matching methods are evaluated. As a result, existing approaches may fall short in scenarios where diverse and complex contexts need to be integrated.
  To address this gap, we propose a novel KG integration method consisting of label matching and triple matching. We use string manipulation, fuzzy matching, and vector similarity techniques to align entity and predicate labels. Next, we identify mappings between triples that convey comparable information, using these mappings to improve entity-matching accuracy. Our approach demonstrates competitive performance compared to leading systems in the OAEI competition and against supervised methods, achieving high accuracy across diverse test cases. Additionally, we introduce a new dataset derived from the benchmark dataset to evaluate the triple-matching step more comprehensively.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Foundations and Mitigation of Hallucination in Large Language Models</title>
<link>https://arxiv.org/abs/2507.22915</link>
<guid>https://arxiv.org/abs/2507.22915</guid>
<content:encoded><![CDATA[
<div> hallucination, Large Language Models, formal definitions, theoretical analyses, mitigation strategies
Summary: 
Hallucination in Large Language Models (LLMs) is explored in this paper through formal definitions, theoretical analyses, and the distinction between intrinsic and extrinsic hallucinations. The concept of a hallucination risk for models is introduced, with bounds derived through learning-theoretic frameworks. Detection strategies such as token-level uncertainty estimation and attention alignment checks are surveyed. Mitigation approaches including retrieval-augmented generation and fact-verification modules are discussed. A unified detection and mitigation workflow is proposed to integrate these strategies. Evaluation protocols are outlined to quantify and reduce hallucinations, recommending datasets, metrics, and experimental setups. This work provides a theoretical foundation and practical guidelines for addressing the challenge of hallucination in LLMs.<br /><br />Summary: <div>
arXiv:2507.22915v1 Announce Type: new 
Abstract: Hallucination in Large Language Models (LLMs) refers to the generation of content that is not faithful to the input or the real-world facts. This paper provides a rigorous treatment of hallucination in LLMs, including formal definitions and theoretical analyses. We distinguish between intrinsic and extrinsic hallucinations, and define a \textit{hallucination risk} for models. We derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes and Rademacher complexity). We then survey detection strategies for hallucinations, such as token-level uncertainty estimation, confidence calibration, and attention alignment checks. On the mitigation side, we discuss approaches including retrieval-augmented generation, hallucination-aware fine-tuning, logit calibration, and the incorporation of fact-verification modules. We propose a unified detection and mitigation workflow, illustrated with a diagram, to integrate these strategies. Finally, we outline evaluation protocols for hallucination, recommending datasets, metrics, and experimental setups to quantify and reduce hallucinations. Our work lays a theoretical foundation and practical guidelines for addressing the crucial challenge of hallucination in LLMs.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reading Between the Timelines: RAG for Answering Diachronic Questions</title>
<link>https://arxiv.org/abs/2507.22917</link>
<guid>https://arxiv.org/abs/2507.22917</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, temporal logic, longitudinal queries, diachronic question answering, financial news <br />
<br />
Summary: <br />
Retrieval-Augmented Generation (RAG) is effective at integrating static knowledge into Large Language Models (LLMs) but struggles with handling longitudinal queries. This new framework proposes a redesigned RAG pipeline infused with temporal logic to address this challenge. By disentangling queries into subjects and temporal windows, a specialized retriever is able to gather evidence that is both topically relevant and temporally coherent. The Analytical Diachronic Question Answering Benchmark (ADQAB) is introduced for evaluation, showing substantial accuracy gains compared to standard RAG implementations. The approach in this work enables RAG systems to analyze complex, real-world questions in a nuanced and evolutionary manner. The dataset and code for this study are publicly available for further research and development. <br /> 
Summary: <div>
arXiv:2507.22917v1 Announce Type: new 
Abstract: While Retrieval-Augmented Generation (RAG) excels at injecting static, factual knowledge into Large Language Models (LLMs), it exhibits a critical deficit in handling longitudinal queries that require tracking entities and phenomena across time. This blind spot arises because conventional, semantically-driven retrieval methods are not equipped to gather evidence that is both topically relevant and temporally coherent for a specified duration. We address this challenge by proposing a new framework that fundamentally redesigns the RAG pipeline to infuse temporal logic. Our methodology begins by disentangling a user's query into its core subject and its temporal window. It then employs a specialized retriever that calibrates semantic matching against temporal relevance, ensuring the collection of a contiguous evidence set that spans the entire queried period. To enable rigorous evaluation of this capability, we also introduce the Analytical Diachronic Question Answering Benchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus of real and synthetic financial news. Empirical results on ADQAB show that our approach yields substantial gains in answer accuracy, surpassing standard RAG implementations by 13% to 27%. This work provides a validated pathway toward RAG systems capable of performing the nuanced, evolutionary analysis required for complex, real-world questions. The dataset and code for this study are publicly available at https://github.com/kwunhang/TA-RAG.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Convergence: Investigating Shared Representations Across Scaled LLMs</title>
<link>https://arxiv.org/abs/2507.22918</link>
<guid>https://arxiv.org/abs/2507.22918</guid>
<content:encoded><![CDATA[
<div> investigate, feature universality, Gemma-2 language models, Sparse Autoencoder, internal concepts <br />
Summary: <br />
The study explores feature universality in Gemma-2 language models, specifically Gemma-2-2B and Gemma-2-9B, to determine whether models of different scales converge on similar internal concepts. By applying the Sparse Autoencoder (SAE) dictionary-learning pipeline to residual-stream activations of each model, researchers aligned monosemantic features and compared them using SVCCA and RSA. The findings indicate that middle layers exhibit the highest similarity, while early and late layers show less overlap. Preliminary experiments extending the analysis to multi-token subspaces suggest that semantically similar subspaces interact comparably with language models. These results support the notion that despite variations in size, large language models delineate the world into consistent, interpretable features, reinforcing the idea of universality as a basis for cross-model interpretability. <div>
arXiv:2507.22918v1 Announce Type: new 
Abstract: We investigate feature universality in Gemma-2 language models (Gemma-2-2B and Gemma-2-9B), asking whether models with a four-fold difference in scale still converge on comparable internal concepts. Using the Sparse Autoencoder (SAE) dictionary-learning pipeline, we utilize SAEs on each model's residual-stream activations, align the resulting monosemantic features via activation correlation, and compare the matched feature spaces with SVCCA and RSA. Middle layers yield the strongest overlap, while early and late layers show far less similarity. Preliminary experiments extend the analysis from single tokens to multi-token subspaces, showing that semantically similar subspaces interact similarly with language models. These results strengthen the case that large language models carve the world into broadly similar, interpretable features despite size differences, reinforcing universality as a foundation for cross-model interpretability.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations</title>
<link>https://arxiv.org/abs/2507.22919</link>
<guid>https://arxiv.org/abs/2507.22919</guid>
<content:encoded><![CDATA[
<div> Keywords: clinical trials, serious adverse events, prediction models, transfer learning, structured summary results <br />
Summary: 
- The study aimed to develop models for predicting serious adverse event (SAE) results in clinical trials using information from trial registrations.
- Analysis of over 22,000 clinical trials from ClinicalTrials.gov was conducted to develop prediction models.
- Two models were developed: one to predict SAE rates between experimental and control arms, and another to predict the proportion of SAEs in control arms.
- A transfer learning approach using pretrained language models was utilized for feature extraction.
- A sliding window method was developed to extract embeddings from long trial texts and consistently improved model performance.
- The best model achieved a 77.6% AUC for predicting SAE rates and an RMSE of 18.6% for predicting SAE proportions.
- The study emphasizes the underutilization of available data at ClinicalTrials.gov and the potential for improving trial design and identifying discrepancies in safety results. 

<br /><br />Summary: <div>
arXiv:2507.22919v1 Announce Type: new 
Abstract: Objectives: With accurate estimates of expected safety results, clinical trials could be designed to avoid terminations and limit exposing participants to unnecessary risks. We evaluated methods for predicting serious adverse event (SAE) results in clinical trials using information only from their registrations prior to the trial. Material and Methods: We analysed 22,107 two-arm parallel interventional clinical trials from ClinicalTrials.gov with structured summary results. Two prediction models were developed: a classifier predicting will experimental arm have higher SAE rates (area under the receiver operating characteristic curve; AUC) than control arm, and a regression model to predict the proportion of SAEs in control arms (root mean squared error; RMSE). A transfer learning approach using pretrained language models (e.g., ClinicalT5, BioBERT) was used for feature extraction, combined with downstream model for prediction. To maintain semantic representation in long trial texts exceeding localised language model input limits, a sliding window method was developed for embedding extraction. Results: The best model (ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a higher proportion of patients with SAEs. When predicting proportion of participants experiencing SAE in the control arm, the same model achieved RMSE of 18.6%. The sliding window approach consistently outperformed methods without it. Across 12 classifiers, the average absolute AUC increase was 2.00%; across 12 regressors, the average absolute RMSE reduction was 1.58%. Discussion: Summary results data available at ClinicalTrials.gov remains underutilised. The potential to estimate results of trials before they start is an opportunity to improve trial design and flag discrepancies between expected and reported safety results.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2507.22920</link>
<guid>https://arxiv.org/abs/2507.22920</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, vector quantization, discrete tokenization, multimodal data, LLM architectures

Summary:
This paper provides a comprehensive survey of vector quantization (VQ) techniques for transforming multimodal data into discrete representations suitable for large language models (LLMs). A taxonomy of 8 VQ variants is presented, analyzing their algorithmic principles, training dynamics, and integration challenges with LLM pipelines. The study covers applications without LLMs, LLM-based single-modality systems, and LLM-based multimodal systems, highlighting the impact of quantization strategies on performance. Key challenges such as codebook collapse and unstable gradient estimation are identified, along with emerging research directions like dynamic quantization and unified tokenization frameworks. The survey aims to bridge the gap between traditional VQ methods and modern LLM applications and serves as a foundational reference for the development of efficient and generalizable multimodal systems. An ongoing updated version of the survey can be accessed on GitHub. 

<br /><br />Summary: <div>
arXiv:2507.22920v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has intensified the need for effective mechanisms to transform continuous multimodal data into discrete representations suitable for language-based processing. Discrete tokenization, with vector quantization (VQ) as a central approach, offers both computational efficiency and compatibility with LLM architectures. Despite its growing importance, there is a lack of a comprehensive survey that systematically examines VQ techniques in the context of LLM-based systems. This work fills this gap by presenting the first structured taxonomy and analysis of discrete tokenization methods designed for LLMs. We categorize 8 representative VQ variants that span classical and modern paradigms and analyze their algorithmic principles, training dynamics, and integration challenges with LLM pipelines. Beyond algorithm-level investigation, we discuss existing research in terms of classical applications without LLMs, LLM-based single-modality systems, and LLM-based multimodal systems, highlighting how quantization strategies influence alignment, reasoning, and generation performance. In addition, we identify key challenges including codebook collapse, unstable gradient estimation, and modality-specific encoding constraints. Finally, we discuss emerging research directions such as dynamic and task-adaptive quantization, unified tokenization frameworks, and biologically inspired codebook learning. This survey bridges the gap between traditional vector quantization and modern LLM applications, serving as a foundational reference for the development of efficient and generalizable multimodal systems. A continuously updated version is available at: https://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Accurate Contextual Knowledge Extraction Using Cascading Language Model Chains and Candidate Answers</title>
<link>https://arxiv.org/abs/2507.22921</link>
<guid>https://arxiv.org/abs/2507.22921</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, Language Model Chain, hallucinations, knowledge extraction, prediction speed

Summary:

The Language Model Chain (LMC) algorithm aims to address the issues of costly and inaccurate information production in language models. This algorithm works by utilizing multiple language models in a multi-stage cascade to improve prediction speed and accuracy while reducing hallucinations. The LMC algorithm was tested on extracting patient dates of birth from medical documents, showcasing significant improvements in accuracy and speed. The algorithm ensures that a language model's response is only considered correct if it exists in a collection of possible answers, preventing the generation of incorrect information. By feeding text corresponding to incorrect responses into more predictive language models, the LMC algorithm iteratively refines predictions until all are accurate. The results of applying the LMC algorithm suggest its potential for advancing the field of knowledge extraction and warrant further exploration in the future.

<br /><br />Summary: <div>
arXiv:2507.22921v1 Announce Type: new 
Abstract: Language models can capture complex relationships in given text, but these are notorious for being costly and for producing information that does not exist (i.e., hallucinations). Furthermore, the resources invested into producing this information would be wasted if it were incorrect. We address these issues by proposing, implementing, and applying the Language Model Chain (LMC) algorithm. In this, a language model's response to a given prompt about given text is only correct if it exists in the collection of possible (i.e., candidate) answers, and text corresponding to incorrect responses is fed into a more predictive (but slower) language model. This process is repeated for a collection of language models, or until all predictions about the text are correct. We used the LMC algorithm to extract patient dates of birth from medical documents, and combining a collection of language models in a multi-stage cascade significantly increased prediction speed and accuracy over individual language models, while greatly reducing the number of corresponding hallucinations. We believe that the novel LMC algorithm significantly contributes to the knowledge extraction field, and that this should be explored much further in the future.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting stock prices with ChatGPT-annotated Reddit sentiment</title>
<link>https://arxiv.org/abs/2507.22922</link>
<guid>https://arxiv.org/abs/2507.22922</guid>
<content:encoded><![CDATA[
<div> Keywords: retail investors, social media sentiment, stock market movements, Reddit, predictive signals <br />
<br />
Summary: 
This paper examines the impact of sentiment from social media, particularly Reddit's r/wallstreetbets, on stock market movements, focusing on GameStop and AMC Entertainment. The study utilizes three sentiment analysis methods and finds that social media sentiment has a weak correlation with stock prices. Surprisingly, simpler metrics like comment volume and Google search trends are more predictive. The findings suggest that traditional sentiment analysis may not capture the complexity of retail investor behavior. Retail investor activity, as seen in the GameStop short squeeze, raises questions about the influence of online sentiment on stock prices. The study introduces a new sentiment analysis model designed to better interpret informal language and emojis in social media discussions. The research underscores the need for a deeper understanding of the nuances of market-moving online conversations. <div>
arXiv:2507.22922v1 Announce Type: new 
Abstract: The surge of retail investor activity on social media, exemplified by the 2021 GameStop short squeeze, raised questions about the influence of online sentiment on stock prices. This paper explores whether sentiment derived from social media discussions can meaningfully predict stock market movements. We focus on Reddit's r/wallstreetbets and analyze sentiment related to two companies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's role, we employ two existing text-based sentiment analysis methods and introduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model designed to better interpret the informal language and emojis prevalent in social media discussions. We use correlation and causality metrics to determine these models' predictive power. Surprisingly, our findings suggest that social media sentiment has only a weak correlation with stock prices. At the same time, simpler metrics, such as the volume of comments and Google search trends, exhibit stronger predictive signals. These results highlight the complexity of retail investor behavior and suggest that traditional sentiment analysis may not fully capture the nuances of market-moving online discussions.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting</title>
<link>https://arxiv.org/abs/2507.22923</link>
<guid>https://arxiv.org/abs/2507.22923</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, multilingual retrieval-augmented generation, knowledge bases, prompt translation strategies, classification tasks

Summary:
The paper explores the impact of different prompt translation strategies on classification tasks in multilingual systems using retrieval-augmented generation with Large Language Models (LLMs). Sharing knowledge bases (KB) from high-resource languages to low-resource ones often results in retrieved information in a different language. The study evaluates the effectiveness of pre-translation to create mono-lingual prompts and cross-lingual prompting for direct inference. Results show that an optimized prompt strategy can enhance knowledge sharing across languages, improving classification task performance significantly. The findings emphasize the importance of utilizing multilingual resource sharing and optimizing cross-lingual prompts, particularly for non-English languages, especially low-resource ones.<br /><br />Summary: The study investigates the impact of different prompt translation strategies in multilingual systems using retrieval-augmented generation with Large Language Models. Sharing knowledge bases from high-resource to low-resource languages can lead to retrieved information in different languages. The research shows that optimizing prompt strategies can enhance knowledge sharing across languages, improving classification task performance. The study underscores the importance of leveraging multilingual resource sharing and optimizing cross-lingual prompts for non-English languages, specifically low-resource ones. <div>
arXiv:2507.22923v1 Announce Type: new 
Abstract: Despite advances in the multilingual capabilities of Large Language Models (LLMs), their performance varies substantially across different languages and tasks. In multilingual retrieval-augmented generation (RAG)-based systems, knowledge bases (KB) are often shared from high-resource languages (such as English) to low-resource ones, resulting in retrieved information from the KB being in a different language than the rest of the context. In such scenarios, two common practices are pre-translation to create a mono-lingual prompt and cross-lingual prompting for direct inference. However, the impact of these choices remains unclear. In this paper, we systematically evaluate the impact of different prompt translation strategies for classification tasks with RAG-enhanced LLMs in multilingual systems. Experimental results show that an optimized prompting strategy can significantly improve knowledge sharing across languages, therefore improve the performance on the downstream classification task. The findings advocate for a broader utilization of multilingual resource sharing and cross-lingual prompt optimization for non-English languages, especially the low-resource ones.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Sentiment Analysis to Investigate Peer Feedback by Native and Non-Native English Speakers</title>
<link>https://arxiv.org/abs/2507.22924</link>
<guid>https://arxiv.org/abs/2507.22924</guid>
<content:encoded><![CDATA[
<div> Keywords: graduate programs, international students, peer feedback, online courses, language background

Summary:
The study explores the impact of native versus non-native English speaker status on peer feedback experiences in online computing courses in the U.S. It reveals that non-native English speakers tend to write more positively but receive less positive feedback in return, while native English speakers rate feedback less favorably. The sentiment analysis also considers the students' language background and reveals significant interactions when controlling for sex and age. The study highlights that language background plays a modest yet complex role in shaping peer feedback experiences in online computing courses. <div>
arXiv:2507.22924v1 Announce Type: new 
Abstract: Graduate-level CS programs in the U.S. increasingly enroll international students, with 60.2 percent of master's degrees in 2023 awarded to non-U.S. students. Many of these students take online courses, where peer feedback is used to engage students and improve pedagogy in a scalable manner. Since these courses are conducted in English, many students study in a language other than their first. This paper examines how native versus non-native English speaker status affects three metrics of peer feedback experience in online U.S.-based computing courses. Using the Twitter-roBERTa-based model, we analyze the sentiment of peer reviews written by and to a random sample of 500 students. We then relate sentiment scores and peer feedback ratings to students' language background. Results show that native English speakers rate feedback less favorably, while non-native speakers write more positively but receive less positive sentiment in return. When controlling for sex and age, significant interactions emerge, suggesting that language background plays a modest but complex role in shaping peer feedback experiences.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents</title>
<link>https://arxiv.org/abs/2507.22925</link>
<guid>https://arxiv.org/abs/2507.22925</guid>
<content:encoded><![CDATA[
<div> memory, Large Language Model Agents, reasoning, organization, retrieval

Summary:<br />
The article introduces the concept of Hierarchical Memory (H-MEM) architecture for Large Language Model Agents (LLM Agents) to improve long-term memory integration. H-MEM organizes memory in multi-level fashion based on semantic abstraction, with each memory vector embedding a positional index for efficient retrieval. An index-based routing mechanism allows layer-by-layer retrieval without exhaustive similarity computations. The study evaluates the approach on LoCoMo dataset with five task settings, showing consistent outperformance of five baseline methods. H-MEM enhances decision-making and contextual coherence of LLM Agents, addressing limitations in structured memory organization and retrieval efficiency. The results highlight the effectiveness of the proposed approach in long-term dialogue scenarios. <br /><br />Summary: <div>
arXiv:2507.22925v1 Announce Type: new 
Abstract: Long-term memory is one of the key factors influencing the reasoning capabilities of Large Language Model Agents (LLM Agents). Incorporating a memory mechanism that effectively integrates past interactions can significantly enhance decision-making and contextual coherence of LLM Agents. While recent works have made progress in memory storage and retrieval, such as encoding memory into dense vectors for similarity-based search or organizing knowledge in the form of graph, these approaches often fall short in structured memory organization and efficient retrieval. To address these limitations, we propose a Hierarchical Memory (H-MEM) architecture for LLM Agents that organizes and updates memory in a multi-level fashion based on the degree of semantic abstraction. Each memory vector is embedded with a positional index encoding pointing to its semantically related sub-memories in the next layer. During the reasoning phase, an index-based routing mechanism enables efficient, layer-by-layer retrieval without performing exhaustive similarity computations. We evaluate our method on five task settings from the LoCoMo dataset. Experimental results show that our approach consistently outperforms five baseline methods, demonstrating its effectiveness in long-term dialogue scenarios.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Relation Extraction in Entity Pairs using Global Context</title>
<link>https://arxiv.org/abs/2507.22926</link>
<guid>https://arxiv.org/abs/2507.22926</guid>
<content:encoded><![CDATA[
<div> Keywords: document-level relation extraction, global context, multi-sentence reasoning, input encoding, entity relationships

Summary: 
The paper introduces a novel input embedding approach for document-level relation extraction that considers the entire document context rather than just the sentences where entities are mentioned. By representing entities as standalone segments throughout the document, the proposed method leverages global relationships and multi-sentence reasoning to accurately predict entity relationships. Experimental results on three benchmark datasets, including DocRED, Re-DocRED, and REBEL, demonstrate the effectiveness of the approach in accurately predicting entity relationships. The research advances global context modeling and multi-sentence reasoning in document-level relation extraction, providing theoretical advancements in the field. Practically, the proposed method enhances relationship detection in NLP applications, offering improved performance and interpretability of entity-level insights. 

<br /><br />Summary: <div>
arXiv:2507.22926v1 Announce Type: new 
Abstract: In document-level relation extraction, entities may appear multiple times in a document, and their relationships can shift from one context to another. Accurate prediction of the relationship between two entities across an entire document requires building a global context spanning all relevant sentences. Previous approaches have focused only on the sentences where entities are mentioned, which fails to capture the complete document context necessary for accurate relation extraction. Therefore, this paper introduces a novel input embedding approach to capture the positions of mentioned entities throughout the document rather than focusing solely on the span where they appear. The proposed input encoding approach leverages global relationships and multi-sentence reasoning by representing entities as standalone segments, independent of their positions within the document. The performance of the proposed method has been tested on three benchmark relation extraction datasets, namely DocRED, Re-DocRED, and REBEL. The experimental results demonstrated that the proposed method accurately predicts relationships between entities in a document-level setting. The proposed research also has theoretical and practical implications. Theoretically, it advances global context modeling and multi-sentence reasoning in document-level relation extraction. Practically, it enhances relationship detection, enabling improved performance in real-world NLP applications requiring comprehensive entity-level insights and interpretability.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.22927</link>
<guid>https://arxiv.org/abs/2507.22927</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Large Language Models, Benchmark, Document Utilization, Placeholder-Based Approach

Summary: 
The article introduces a fine-grained benchmark called Placeholder-RAG-Benchmark for evaluating the document utilization capabilities of large language models (LLMs) in Retrieval-Augmented Generation (RAG) systems. The benchmark focuses on multi-level filtering abilities, combination abilities, and reference reasoning to assess the LLM's performance in generating responses based on retrieved documents. By using a placeholder-based approach, the study decouples the contributions of the LLM's knowledge and external knowledge, revealing limitations in error resilience and context faithfulness of representative LLMs in RAG systems. The benchmark provides a reproducible framework for developing more reliable and efficient RAG systems. Experimental results highlight the need for improvement in LLM-specific capabilities within RAG systems. The code for the benchmark is available on GitHub at https://github.com/Alipay-Med/PRGB. 

<br /><br />Summary: <div>
arXiv:2507.22927v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by integrating external knowledge, where the LLM's ability to generate responses based on the combination of a given query and retrieved documents is crucial. However, most benchmarks focus on overall RAG system performance, rarely assessing LLM-specific capabilities. Current benchmarks emphasize broad aspects such as noise robustness, but lack a systematic and granular evaluation framework on document utilization. To this end, we introduce \textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark, emphasizing the following progressive dimensions: (1) multi-level filtering abilities, (2) combination abilities, and (3) reference reasoning. To provide a more nuanced understanding of LLMs' roles in RAG systems, we formulate an innovative placeholder-based approach to decouple the contributions of the LLM's parametric knowledge and the external knowledge. Experiments demonstrate the limitations of representative LLMs in the RAG system's generation capabilities, particularly in error resilience and context faithfulness. Our benchmark provides a reproducible framework for developing more reliable and efficient RAG systems. Our code is available in https://github.com/Alipay-Med/PRGB.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding</title>
<link>https://arxiv.org/abs/2507.22928</link>
<guid>https://arxiv.org/abs/2507.22928</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-thought prompting, Large Language Models, Causal study, Activation patching, Modular internal computation

Summary:
Chain-of-thought (CoT) prompting has been shown to improve the accuracy of Large Language Models (LLMs) on multi-step tasks. This study investigates the faithfulness of CoT-generated "thoughts" by analyzing monosemantic features extracted from Pythia-70M and Pythia-2.8B models while solving math problems. The results reveal that CoT significantly increases answer log-probabilities in the larger 2.8B model but not in the 70M model, indicating a scale threshold. CoT also leads to higher activation sparsity and feature interpretability scores in the larger model, suggesting more modular internal computation. The confidence of generating correct answers improves with CoT. Patching analysis shows that useful CoT information is distributed widely across features. Overall, CoT induces more interpretable internal structures in high-capacity LLMs, supporting its role as a structured prompting method.

<br /><br />Summary: 
- CoT prompting boosts LLM accuracy on multi-step tasks.
- Study analyzes CoT faithfulness using feature-level causal analysis.
- The 2.8B model shows a significant impact of CoT on answer log-probabilities.
- CoT leads to higher activation sparsity and feature interpretability scores in large models.
- Confidence in generating correct answers improves with CoT. <div>
arXiv:2507.22928v1 Announce Type: new 
Abstract: Chain-of-thought (CoT) prompting boosts Large Language Models accuracy on multi-step tasks, yet whether the generated "thoughts" reflect the true internal reasoning process is unresolved. We present the first feature-level causal study of CoT faithfulness. Combining sparse autoencoders with activation patching, we extract monosemantic features from Pythia-70M and Pythia-2.8B while they tackle GSM8K math problems under CoT and plain (noCoT) prompting. Swapping a small set of CoT-reasoning features into a noCoT run raises answer log-probabilities significantly in the 2.8B model, but has no reliable effect in 70M, revealing a clear scale threshold. CoT also leads to significantly higher activation sparsity and feature interpretability scores in the larger model, signalling more modular internal computation. For example, the model's confidence in generating correct answers improves from 1.2 to 4.3. We introduce patch-curves and random-feature patching baselines, showing that useful CoT information is not only present in the top-K patches but widely distributed. Overall, our results indicate that CoT can induce more interpretable internal structures in high-capacity LLMs, validating its role as a structured prompting method.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow</title>
<link>https://arxiv.org/abs/2507.22929</link>
<guid>https://arxiv.org/abs/2507.22929</guid>
<content:encoded><![CDATA[
<div> Benchmark, Ophthalmology, Hallucinations, Language Models, Multimodal Data
Summary:
EH-Benchmark is a new ophthalmology benchmark created to evaluate hallucinations in Medical Large Language Models (MLLMs). The challenges faced by MLLMs include limited ophthalmic knowledge, lack of visual localization and reasoning capabilities, and a scarcity of multimodal ophthalmic data. The benchmark categorizes MLLMs' hallucinations into two primary classes: Visual Understanding and Logical Composition, each with multiple subclasses. A three-phase framework is proposed to mitigate hallucinations, including Knowledge-Level Retrieval, Task-Level Case Studies, and Result-Level Validation. Experimental results demonstrate that the framework significantly improves accuracy, interpretability, and reliability of MLLMs. The project is available on GitHub at https://github.com/ppxy1/EH-Benchmark. <br /><br />Summary: <div>
arXiv:2507.22929v1 Announce Type: new 
Abstract: Medical Large Language Models (MLLMs) play a crucial role in ophthalmic diagnosis, holding significant potential to address vision-threatening diseases. However, their accuracy is constrained by hallucinations stemming from limited ophthalmic knowledge, insufficient visual localization and reasoning capabilities, and a scarcity of multimodal ophthalmic data, which collectively impede precise lesion detection and disease diagnosis. Furthermore, existing medical benchmarks fail to effectively evaluate various types of hallucinations or provide actionable solutions to mitigate them. To address the above challenges, we introduce EH-Benchmark, a novel ophthalmology benchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs' hallucinations based on specific tasks and error types into two primary classes: Visual Understanding and Logical Composition, each comprising multiple subclasses. Given that MLLMs predominantly rely on language-based reasoning rather than visual processing, we propose an agent-centric, three-phase framework, including the Knowledge-Level Retrieval stage, the Task-Level Case Studies stage, and the Result-Level Validation stage. Experimental results show that our multi-agent framework significantly mitigates both types of hallucinations, enhancing accuracy, interpretability, and reliability. Our project is available at https://github.com/ppxy1/EH-Benchmark.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection</title>
<link>https://arxiv.org/abs/2507.22930</link>
<guid>https://arxiv.org/abs/2507.22930</guid>
<content:encoded><![CDATA[
<div> Keywords: Reddit, Personal Information Identifiers, synthetic dataset, privacy risks, text detection

Summary:
A novel methodology has been developed to create a synthetic dataset for detecting Personal Information Identifiers (PIIs) in online social media, focusing on Reddit posts. The dataset includes 19 PII-revealing categories for vulnerable populations and is generated from three text generation Large Language Models (LLMs). The methodology ensures reproducibility equivalence, unlinkability to original users, and indistinguishability from original posts. The dataset and code are released to facilitate research into PII privacy risks on social platforms. This initiative aims to address the lack of open-source labeled datasets for identifying risky self-disclosures of PIIs, thereby enabling the study of privacy risks and online harms associated with online self-disclosures on social media platforms like Reddit.<br /><br />Summary: A synthetic dataset has been created for detecting Personal Information Identifiers (PIIs) in online social media, with a focus on Reddit posts. The dataset includes 19 PII-revealing categories and is generated from three text generation Large Language Models. The methodology ensures reproducibility equivalence, unlinkability to original users, and indistinguishability from original posts. The dataset and code are released to support research on PII privacy risks in social media, addressing the existing lack of labeled datasets for identifying risky self-disclosures of PIIs and enabling the study of privacy risks and online harms associated with online self-disclosures on platforms like Reddit. <div>
arXiv:2507.22930v1 Announce Type: new 
Abstract: Social platforms such as Reddit have a network of communities of shared interests, with a prevalence of posts and comments from which one can infer users' Personal Information Identifiers (PIIs). While such self-disclosures can lead to rewarding social interactions, they pose privacy risks and the threat of online harms. Research into the identification and retrieval of such risky self-disclosures of PIIs is hampered by the lack of open-source labeled datasets. To foster reproducible research into PII-revealing text detection, we develop a novel methodology to create synthetic equivalents of PII-revealing data that can be safely shared. Our contributions include creating a taxonomy of 19 PII-revealing categories for vulnerable populations and the creation and release of a synthetic PII-labeled multi-text span dataset generated from 3 text generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and zephyr-7b-beta, with sequential instruction prompting to resemble the original Reddit posts. The utility of our methodology to generate this synthetic dataset is evaluated with three metrics: First, we require reproducibility equivalence, i.e., results from training a model on the synthetic data should be comparable to those obtained by training the same models on the original posts. Second, we require that the synthetic data be unlinkable to the original users, through common mechanisms such as Google Search. Third, we wish to ensure that the synthetic data be indistinguishable from the original, i.e., trained humans should not be able to tell them apart. We release our dataset and code at https://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster reproducible research into PII privacy risks in online social media.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing RAG Efficiency with Adaptive Context Compression</title>
<link>https://arxiv.org/abs/2507.22931</link>
<guid>https://arxiv.org/abs/2507.22931</guid>
<content:encoded><![CDATA[
<div> Hierarchical compressor, Context selector, Dynamic compression rates, Retrieval-augmented generation, Inference efficiency <br />
Summary: 
The article introduces Adaptive Context Compression for RAG (ACC-RAG), a framework aimed at improving inference efficiency in large language models that integrate external knowledge. ACC-RAG dynamically adjusts compression rates based on the complexity of input queries, optimizing both speed and accuracy. By combining a hierarchical compressor with a context selector, ACC-RAG mimics human skimming behavior to retain essential information while discarding unnecessary details. Evaluation on various datasets demonstrates that ACC-RAG outperforms fixed-rate methods, unlocking over four times faster inference speed without compromising accuracy. This innovative approach to context compression shows promising results in enhancing the performance of retrieval-augmented generation models. <br /> <div>
arXiv:2507.22931v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but incurs significant inference costs due to lengthy retrieved contexts. While context compression mitigates this issue, existing methods apply fixed compression rates, over-compressing simple queries or under-compressing complex ones. We propose Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification</title>
<link>https://arxiv.org/abs/2507.22932</link>
<guid>https://arxiv.org/abs/2507.22932</guid>
<content:encoded><![CDATA[
<div> framework, portfolio optimization, Large Language Models, Deep Reinforcement Learning, sentiment signals  
Summary:  
- This paper introduces a novel hierarchical framework for portfolio optimization that combines lightweight Large Language Models (LLMs) with Deep Reinforcement Learning (DRL) to integrate sentiment signals from financial news with traditional market indicators.  
- The three-tier architecture utilizes base RL agents for processing hybrid data, meta-agents for aggregating decisions, and a super-agent for merging decisions based on market data and sentiment analysis.  
- Evaluations on data from 2018 to 2024, after training on 2000-2017, show that the framework achieves a 26% annualized return and a Sharpe ratio of 1.2, surpassing equal-weighted and S&amp;P 500 benchmarks.  
- Key contributions include scalable cross-modal integration, a hierarchical RL structure for enhanced stability, and open-source reproducibility.  
- The framework demonstrates the effectiveness of combining advanced natural language processing techniques with reinforcement learning in the field of financial portfolio management. <br /><br /> <div>
arXiv:2507.22932v1 Announce Type: new 
Abstract: This paper presents a novel hierarchical framework for portfolio optimization, integrating lightweight Large Language Models (LLMs) with Deep Reinforcement Learning (DRL) to combine sentiment signals from financial news with traditional market indicators. Our three-tier architecture employs base RL agents to process hybrid data, meta-agents to aggregate their decisions, and a super-agent to merge decisions based on market data and sentiment analysis. Evaluated on data from 2018 to 2024, after training on 2000-2017, the framework achieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming equal-weighted and S&amp;P 500 benchmarks. Key contributions include scalable cross-modal integration, a hierarchical RL structure for enhanced stability, and open-source reproducibility.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmented Vision-Language Models: A Systematic Review</title>
<link>https://arxiv.org/abs/2507.22933</link>
<guid>https://arxiv.org/abs/2507.22933</guid>
<content:encoded><![CDATA[
<div> Keywords: visual-language machine learning, neural symbolic systems, reasoning, memory, external symbolic information systems 

Summary: 
This article discusses the limitations of current visual-language machine learning models and proposes the integration of neural networks with external symbolic information systems to enhance reasoning and memory abilities. The traditional training paradigm lacks interpretable explanations for outputs, necessitates retraining for new information, and struggles with certain forms of logical reasoning. By combining powerful pre-trained Vision-Language Models (VLMs) with external symbolic systems, neural-symbolic systems can provide more interpretable explanations and assimilate new information without extensive retraining. The systematic literature review aims to categorize techniques for improving visual-language understanding through interaction with external symbolic information systems.<br /><br />Summary: <div>
arXiv:2507.22933v1 Announce Type: new 
Abstract: Recent advances in visual-language machine learning models have demonstrated exceptional ability to use natural language and understand visual scenes by training on large, unstructured datasets. However, this training paradigm cannot produce interpretable explanations for its outputs, requires retraining to integrate new information, is highly resource-intensive, and struggles with certain forms of logical reasoning. One promising solution involves integrating neural networks with external symbolic information systems, forming neural symbolic systems that can enhance reasoning and memory abilities. These neural symbolic systems provide more interpretable explanations to their outputs and the capacity to assimilate new information without extensive retraining. Utilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural component, augmented by external systems, offers a pragmatic approach to realizing the benefits of neural-symbolic integration. This systematic literature review aims to categorize techniques through which visual-language understanding can be improved by interacting with external symbolic information systems.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Approaches for Multimodal Intent Recognition: A Survey</title>
<link>https://arxiv.org/abs/2507.22934</link>
<guid>https://arxiv.org/abs/2507.22934</guid>
<content:encoded><![CDATA[
<div> Transformer-based models, intent recognition, deep learning, multimodal, natural language processing
Summary:
Deep learning methods, especially Transformer-based models, have advanced intent recognition from text-based to multimodal approaches. These approaches integrate data from audio, vision, and physiological signals to enhance natural human-computer interaction. The survey covers the evolution of intent recognition techniques, from unimodal to multimodal, and discusses relevant datasets, methodologies, applications, and current challenges. Researchers are provided with insights into the latest developments in multimodal intent recognition (MIR) and potential future research directions. <div>
arXiv:2507.22934v1 Announce Type: new 
Abstract: Intent recognition aims to identify users' underlying intentions, traditionally focusing on text in natural language processing. With growing demands for natural human-computer interaction, the field has evolved through deep learning and multimodal approaches, incorporating data from audio, vision, and physiological signals. Recently, the introduction of Transformer-based models has led to notable breakthroughs in this domain. This article surveys deep learning methods for intent recognition, covering the shift from unimodal to multimodal techniques, relevant datasets, methodologies, applications, and current challenges. It provides researchers with insights into the latest developments in multimodal intent recognition (MIR) and directions for future research.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trusted Knowledge Extraction for Operations and Maintenance Intelligence</title>
<link>https://arxiv.org/abs/2507.22935</link>
<guid>https://arxiv.org/abs/2507.22935</guid>
<content:encoded><![CDATA[
<div> Keywords: operational intelligence, Knowledge Graph, Named Entity Recognition, Coreference Resolution, Large Language Models

Summary:
Operational intelligence derived from organizational data repositories faces challenges due to data confidentiality and integration objectives. This work discusses the Knowledge Extraction process, including Named Entity Recognition, Coreference Resolution, Named Entity Linking, and Relation Extraction. Sixteen NLP tools are evaluated alongside Large Language Models for operational and maintenance intelligence in the aircraft industry. The study uses a baseline dataset from the US Federal Aviation Administration to assess zero-shot performance in a confidential environment. Significant limitations are observed, highlighting challenges in trusted NLP and LLM tools for mission-critical industries like aviation. Recommendations are made to enhance trust in these tools, and an open-source dataset is provided for further testing and evaluation. 

<br /><br />Summary: <div>
arXiv:2507.22935v1 Announce Type: new 
Abstract: Deriving operational intelligence from organizational data repositories is a key challenge due to the dichotomy of data confidentiality vs data integration objectives, as well as the limitations of Natural Language Processing (NLP) tools relative to the specific knowledge structure of domains such as operations and maintenance. In this work, we discuss Knowledge Graph construction and break down the Knowledge Extraction process into its Named Entity Recognition, Coreference Resolution, Named Entity Linking, and Relation Extraction functional components. We then evaluate sixteen NLP tools in concert with or in comparison to the rapidly advancing capabilities of Large Language Models (LLMs). We focus on the operational and maintenance intelligence use case for trusted applications in the aircraft industry. A baseline dataset is derived from a rich public domain US Federal Aviation Administration dataset focused on equipment failures or maintenance requirements. We assess the zero-shot performance of NLP and LLM tools that can be operated within a controlled, confidential environment (no data is sent to third parties). Based on our observation of significant performance limitations, we discuss the challenges related to trusted NLP and LLM tools as well as their Technical Readiness Level for wider use in mission-critical industries such as aviation. We conclude with recommendations to enhance trust and provide our open-source curated dataset to support further baseline testing and evaluation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis</title>
<link>https://arxiv.org/abs/2507.22936</link>
<guid>https://arxiv.org/abs/2507.22936</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Financial Natural Language Processing, 10-K filings, Comparative evaluation, Model performance

Summary: 
Large Language Models (LLMs) are increasingly being utilized in Financial Natural Language Processing (FinNLP) tasks, with five leading LLMs, GPT, Claude, Perplexity, Gemini, and DeepSeek, being compared in a study using 10-K filings from prominent technology companies. Three evaluation methodologies were employed, including human annotation, automated metrics, and model behavior diagnostics. The results indicated that GPT provided the most coherent and contextually relevant answers, followed by Claude and Perplexity. In contrast, Gemini and DeepSeek exhibited more variability and less agreement in their outputs. The study also highlighted the sensitivity of LLM outputs to prompts and source material, with outputs varying across companies and over time. These findings underscore the importance of careful prompt formulation in utilizing LLMs for financial analysis.<br /><br />Summary: <div>
arXiv:2507.22936v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide variety of Financial Natural Language Processing (FinNLP) tasks. However, systematic comparisons among widely used LLMs remain underexplored. Given the rapid advancement and growing influence of LLMs in financial analysis, this study conducts a thorough comparative evaluation of five leading LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the 'Magnificent Seven' technology companies. We create a set of domain-specific prompts and then use three methodologies to evaluate model performance: human annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity, Jaccard), and model behavior diagnostics (prompt-level variance and across-model similarity). The results show that GPT gives the most coherent, semantically aligned, and contextually relevant answers; followed by Claude and Perplexity. Gemini and DeepSeek, on the other hand, have more variability and less agreement. Also, the similarity and stability of outputs change from company to company and over time, showing that they are sensitive to how prompts are written and what source material is used.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering</title>
<link>https://arxiv.org/abs/2507.22937</link>
<guid>https://arxiv.org/abs/2507.22937</guid>
<content:encoded><![CDATA[
<div> Keywords: AIOps, collaboration-of-expert framework, large language model, retrieval-augmented generation, DevOps-EVAL dataset

Summary: <br /><br />
The paper introduces a collaboration-of-expert framework (CoE-Ops) for AIOps in DevOps, integrating a large language model task classifier. By incorporating a retrieval-augmented generation mechanism, the framework can handle both high-level and low-level tasks in AIOps effectively. Experimental results on the DevOps-EVAL dataset show that CoE-Ops significantly improves routing accuracy for high-level tasks, outperforming existing CoE methods. Additionally, the framework enhances accuracy in problem resolution by up to 8% compared to single AIOps models. Moreover, CoE-Ops surpasses Mixture-of-Experts (MoE) models in accuracy, indicating its superiority in AIOps domain applications. <div>
arXiv:2507.22937v1 Announce Type: new 
Abstract: With the rapid evolution of artificial intelligence, AIOps has emerged as a prominent paradigm in DevOps. Lots of work has been proposed to improve the performance of different AIOps phases. However, constrained by domain-specific knowledge, a single model can only handle the operation requirement of a specific task,such as log parser,root cause analysis. Meanwhile, combining multiple models can achieve more efficient results, which have been proved in both previous ensemble learning and the recent LLM training domain. Inspired by these works,to address the similar challenges in AIOPS, this paper first proposes a collaboration-of-expert framework(CoE-Ops) incorporating a general-purpose large language model task classifier. A retrieval-augmented generation mechanism is introduced to improve the framework's capability in handling both Question-Answering tasks with high-level(Code,build,Test,etc.) and low-level(fault analysis,anomaly detection,etc.). Finally, the proposed method is implemented in the AIOps domain, and extensive experiments are conducted on the DevOps-EVAL dataset. Experimental results demonstrate that CoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps tasks compared to existing CoE methods, delivers up to 8% accuracy enhancement over single AIOps models in DevOps problem resolution, and outperforms larger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents</title>
<link>https://arxiv.org/abs/2507.22938</link>
<guid>https://arxiv.org/abs/2507.22938</guid>
<content:encoded><![CDATA[
<div> Keywords: Question-Answering, Technical Documents, Visual large Language Models, Graph Representations, Telecom Domain<br />
<br />
Summary: 
This study introduces an innovative approach to Question-Answering (QA) from technical documents, focusing on flowchart images in the telecom domain. By incorporating graph representations of flowcharts from Visual large Language Models (VLMs) into a text-based Retrieval Augmented Generation (RAG) system, the authors demonstrate improved QA performance. The process involves document processing, image classification, graph building, and integration with text embedding for efficient retrieval. Results reveal that fine-tuned VLM models provide robust graph representations with low edit distances compared to ground truth. The approach not only enhances retrieval performance using text-based embedding models but also eliminates the need for VLM in inference, reducing costs for deployed QA systems. Overall, the study showcases the efficacy of leveraging VLM-generated graph representations for QA tasks involving flowchart images in technical documents within the telecom domain.<br /> <div>
arXiv:2507.22938v1 Announce Type: new 
Abstract: Question-Answering (QA) from technical documents often involves questions whose answers are present in figures, such as flowcharts or flow diagrams. Text-based Retrieval Augmented Generation (RAG) systems may fail to answer such questions. We leverage graph representations of flowcharts obtained from Visual large Language Models (VLMs) and incorporate them in a text-based RAG system to show that this approach can enable image retrieval for QA in the telecom domain. We present the end-to-end approach from processing technical documents, classifying image types, building graph representations, and incorporating them with the text embedding pipeline for efficient retrieval. We benchmark the same on a QA dataset created based on proprietary telecom product information documents. Results show that the graph representations obtained using a fine-tuned VLM model have lower edit distance with respect to the ground truth, which illustrate the robustness of these representations for flowchart images. Further, the approach for QA using these representations gives good retrieval performance using text-based embedding models, including a telecom-domain adapted one. Our approach also alleviates the need for a VLM in inference, which is an important cost benefit for deployed QA systems.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PARROT: An Open Multilingual Radiology Reports Dataset</title>
<link>https://arxiv.org/abs/2507.22939</link>
<guid>https://arxiv.org/abs/2507.22939</guid>
<content:encoded><![CDATA[
<div> dataset, radiology, reports, natural language processing, multilingual

Summary:<br />
- The article introduces PARROT, a large dataset of fictional radiology reports in multiple languages for testing natural language processing applications in radiology.
- The dataset includes over 2,600 reports from 76 authors across 21 countries and 13 languages, covering various imaging modalities and anatomical regions.
- A study was conducted to differentiate between human-authored and AI-generated reports, with participants achieving 53.9% accuracy.
- Radiologists performed significantly better in distinguishing between human and AI-generated reports compared to other groups.
- PARROT enables the development and validation of natural language processing applications in radiology across linguistic, geographic, and clinical boundaries without privacy constraints.

<br /><br />Summary: <div>
arXiv:2507.22939v1 Announce Type: new 
Abstract: Rationale and Objectives: To develop and validate PARROT (Polyglottal Annotated Radiology Reports for Open Testing), a large, multicentric, open-access dataset of fictional radiology reports spanning multiple languages for testing natural language processing applications in radiology. Materials and Methods: From May to September 2024, radiologists were invited to contribute fictional radiology reports following their standard reporting practices. Contributors provided at least 20 reports with associated metadata including anatomical region, imaging modality, clinical context, and for non-English reports, English translations. All reports were assigned ICD-10 codes. A human vs. AI report differentiation study was conducted with 154 participants (radiologists, healthcare professionals, and non-healthcare professionals) assessing whether reports were human-authored or AI-generated. Results: The dataset comprises 2,658 radiology reports from 76 authors across 21 countries and 13 languages. Reports cover multiple imaging modalities (CT: 36.1%, MRI: 22.8%, radiography: 19.0%, ultrasound: 16.8%) and anatomical regions, with chest (19.9%), abdomen (18.6%), head (17.3%), and pelvis (14.1%) being most prevalent. In the differentiation study, participants achieved 53.9% accuracy (95% CI: 50.7%-57.1%) in distinguishing between human and AI-generated reports, with radiologists performing significantly better (56.9%, 95% CI: 53.3%-60.6%, p<0.05) than other groups. Conclusion: PARROT represents the largest open multilingual radiology report dataset, enabling development and validation of natural language processing applications across linguistic, geographic, and clinical boundaries without privacy constraints.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes</title>
<link>https://arxiv.org/abs/2507.22940</link>
<guid>https://arxiv.org/abs/2507.22940</guid>
<content:encoded><![CDATA[
<div> fact-checking classifier, Group Relative Policy Optimization, mechanistic interpretability, factual accuracy, large language models 

Summary:
The RELIANCE framework addresses the issue of factual inaccuracies in Large Language Models (LLMs) during reasoning processes. It consists of a fact-checking classifier trained on counterfactually augmented data, a Group Relative Policy Optimization (GRPO) reinforcement learning approach, and a mechanistic interpretability module to enhance factual accuracy. Evaluation of ten state-of-the-art models reveals low reasoning factual accuracy, with leading models like Claude-3.7 and GPT-o1 only reaching around 80%. RELIANCE significantly improves factual robustness by up to 49.90% while maintaining or enhancing performance on challenging benchmarks such as Math-500, AIME-2024, and GPQA. The activation-level analysis provides insights into how factual enhancements impact reasoning trajectories within model architectures, laying the groundwork for future training methodologies targeting factual robustness through activation-guided optimization. 

<br /><br />Summary: <div>
arXiv:2507.22940v1 Announce Type: new 
Abstract: We present RELIANCE (Reasoning Evaluation with Logical Integrity and Accuracy for Confidence Enhancement), a novel framework addressing a critical vulnerability in Large Language Models (LLMs): the prevalence of factual inaccuracies within intermediate reasoning steps despite correct final answers. This phenomenon poses substantial risks in high-stakes domains including healthcare, legal analysis, and scientific research, where erroneous yet confidently presented reasoning can mislead users into dangerous decisions. Our framework integrates three core components: (1) a specialized fact-checking classifier trained on counterfactually augmented data to detect subtle factual inconsistencies within reasoning chains; (2) a Group Relative Policy Optimization (GRPO) reinforcement learning approach that balances factuality, coherence, and structural correctness through multi-dimensional rewards; and (3) a mechanistic interpretability module examining how factuality improvements manifest in model activations during reasoning processes. Extensive evaluation across ten state-of-the-art models reveals concerning patterns: even leading models like Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of only 81.93% and 82.57% respectively. RELIANCE significantly enhances factual robustness (up to 49.90% improvement) while maintaining or improving performance on challenging benchmarks including Math-500, AIME-2024, and GPQA. Furthermore, our activation-level analysis provides actionable insights into how factual enhancements reshape reasoning trajectories within model architectures, establishing foundations for future training methodologies that explicitly target factual robustness through activation-guided optimization.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology</title>
<link>https://arxiv.org/abs/2507.22941</link>
<guid>https://arxiv.org/abs/2507.22941</guid>
<content:encoded><![CDATA[
<div> Keywords: Electronic medical reports, machine learning, survival analysis, SigBERT, temporal dynamics <br />
Summary:<br />
- SigBERT is proposed as a framework for temporal survival analysis in healthcare, specifically designed to handle sequential textual data from electronic medical reports (EHR).
- SigBERT processes timestamped medical reports by extracting word embeddings and deriving sentence embeddings through averaging in order to capture complex temporal dynamics.
- The framework utilizes signature extraction from rough path theory to generate geometric features from the time series of sentence embedding coordinates, enhancing the performance of the survival model.
- These features are then utilized in a LASSO-penalized Cox model to estimate patient-specific risk scores, resulting in a C-index score of 0.75 on an independent test cohort from a real-world oncology dataset.
- SigBERT integrates sequential medical data effectively to improve risk estimation in narrative-based survival analysis. <br /><br /> <div>
arXiv:2507.22941v1 Announce Type: new 
Abstract: Electronic medical reports (EHR) contain a vast amount of information that can be leveraged for machine learning applications in healthcare. However, existing survival analysis methods often struggle to effectively handle the complexity of textual data, particularly in its sequential form. Here, we propose SigBERT, an innovative temporal survival analysis framework designed to efficiently process a large number of clinical reports per patient. SigBERT processes timestamped medical reports by extracting and averaging word embeddings into sentence embeddings. To capture temporal dynamics from the time series of sentence embedding coordinates, we apply signature extraction from rough path theory to derive geometric features for each patient, which significantly enhance survival model performance by capturing complex temporal dynamics. These features are then integrated into a LASSO-penalized Cox model to estimate patient-specific risk scores. The model was trained and evaluated on a real-world oncology dataset from the L\'eon B\'erard Center corpus, with a C-index score of 0.75 (sd 0.014) on the independent test cohort. SigBERT integrates sequential medical data to enhance risk estimation, advancing narrative-based survival analysis.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies</title>
<link>https://arxiv.org/abs/2507.22943</link>
<guid>https://arxiv.org/abs/2507.22943</guid>
<content:encoded><![CDATA[
<div> Natural language processing, validation study, claims databases, code-based algorithms, outcome misclassification <br />
Summary: <br />
The article focuses on enhancing analyses conducted with large claims databases by validating code-based algorithms for identifying health outcomes. It introduces an expedited validation process using natural language processing (NLP) to reduce manual chart review time and a multi-wave adaptive sampling approach to stop the study once performance characteristics are identified with sufficient precision. The case study validates a claims-based outcome algorithm for intentional self-harm in obese patients. The NLP-assisted annotation process reduced review time by 40%, and the pre-defined stopping rule would have prevented review of 77% of patient charts with limited compromise to measurement precision. This approach could enable more routine validation of code-based algorithms, improving the reliability of findings from database studies. <br /> <div>
arXiv:2507.22943v1 Announce Type: new 
Abstract: Background: One of the ways to enhance analyses conducted with large claims databases is by validating the measurement characteristics of code-based algorithms used to identify health outcomes or other key study parameters of interest. These metrics can be used in quantitative bias analyses to assess the robustness of results for an inferential study given potential bias from outcome misclassification. However, extensive time and resource allocation are typically re-quired to create reference-standard labels through manual chart review of free-text notes from linked electronic health records. Methods: We describe an expedited process that introduces efficiency in a validation study us-ing two distinct mechanisms: 1) use of natural language processing (NLP) to reduce time spent by human reviewers to review each chart, and 2) a multi-wave adaptive sampling approach with pre-defined criteria to stop the validation study once performance characteristics are identified with sufficient precision. We illustrate this process in a case study that validates the performance of a claims-based outcome algorithm for intentional self-harm in patients with obesity. Results: We empirically demonstrate that the NLP-assisted annotation process reduced the time spent on review per chart by 40% and use of the pre-defined stopping rule with multi-wave samples would have prevented review of 77% of patient charts with limited compromise to precision in derived measurement characteristics. Conclusion: This approach could facilitate more routine validation of code-based algorithms used to define key study parameters, ultimately enhancing understanding of the reliability of find-ings derived from database studies.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opacity as Authority: Arbitrariness and the Preclusion of Contestation</title>
<link>https://arxiv.org/abs/2507.22944</link>
<guid>https://arxiv.org/abs/2507.22944</guid>
<content:encoded><![CDATA[
<div> Keywords: arbitrariness, semiotic trait, motivation, Contestability, artificial intelligence systems

Summary: 
Arbitrariness is redefined in this article as a foundational functional mechanism present in human systems and interactions. It is seen as a semiotic trait that enables systems to operate effectively while withholding their internal rationale, extending beyond language to law and social dynamics. The "Motivation -> Constatability -> Contestability" chain is introduced, with motivation being key in rendering an act's logic contestable. Mechanisms like "immotivization" and "Conflict Lateralization" can break this chain, leading to acts producing binding effects without revealing their rationale, thus evading accountability. Formalized as conditional entropy, arbitrariness is proposed as a neutral operator crucial in interpersonal relations and control mechanisms. This framework also provides insight into analyzing explainability in advanced artificial intelligence systems. 

<br /><br />Summary: <div>
arXiv:2507.22944v1 Announce Type: new 
Abstract: This article redefines arbitrariness not as a normative flaw or a symptom of domination, but as a foundational functional mechanism structuring human systems and interactions. Diverging from critical traditions that conflate arbitrariness with injustice, it posits arbitrariness as a semiotic trait: a property enabling systems - linguistic, legal, or social - to operate effectively while withholding their internal rationale. Building on Ferdinand de Saussure's concept of l'arbitraire du signe, the analysis extends this principle beyond language to demonstrate its cross-domain applicability, particularly in law and social dynamics. The paper introduces the "Motivation -> Constatability -> Contestability" chain, arguing that motivation functions as a crucial interface rendering an act's logic vulnerable to intersubjective contestation. When this chain is broken through mechanisms like "immotivization" or "Conflict Lateralization" (exemplified by "the blur of the wolf drowned in the fish"), acts produce binding effects without exposing their rationale, thus precluding justiciability. This structural opacity, while appearing illogical, is a deliberate design protecting authority from accountability. Drawing on Shannon's entropy model, the paper formalizes arbitrariness as A = H(L|M) (conditional entropy). It thereby proposes a modern theory of arbitrariness as a neutral operator central to control as well as care, an overlooked dimension of interpersonal relations. While primarily developed through human social systems, this framework also illuminates a new pathway for analyzing explainability in advanced artificial intelligence systems.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations</title>
<link>https://arxiv.org/abs/2507.22968</link>
<guid>https://arxiv.org/abs/2507.22968</guid>
<content:encoded><![CDATA[
<div> Keywords: Spoken Dialogue Models, Large Language Models, benchmark dataset, human conversational dynamics, evaluation method

Summary:
Spoken Dialogue Models (SDMs) have gained popularity for generating voice responses to user queries but lack comprehensive research on their effectiveness compared to text-based Large Language Models (LLMs). Human voice interactions present unique challenges such as ambiguity from semantic and phonological factors, as well as context-dependency like omission and coreference. To bridge this gap, a benchmark dataset with 1,079 instances in English and Chinese is introduced, enabling a thorough evaluation of SDMs. An evaluation method aligned with human judgment is provided to assess SDMs in handling these complexities. This dataset aims to shed light on the current state of SDM development and their capabilities in understanding and simulating human conversations.  

<br /><br />Summary: <div>
arXiv:2507.22968v1 Announce Type: new 
Abstract: Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Math Natural Language Inference: this should be easy!</title>
<link>https://arxiv.org/abs/2507.23063</link>
<guid>https://arxiv.org/abs/2507.23063</guid>
<content:encoded><![CDATA[
<div> struggle, mathematical language, inference, LLMs, Math NLI  
Summary:  
- The study explores whether contemporary Language Model models (LLMs) can effectively perform natural language inference (NLI) tasks on mathematical texts, termed the Math NLI problem.  
- A corpus of Math NLI pairs is constructed, with premises from mathematical text and hypotheses from experts in both mathematics research and NLI.  
- The quality of corpora using LLM-generated hypotheses is also examined.  
- Positive findings include the use of a majority vote of LLMs being comparable to human-labeled data in certain Math NLI scenarios.  
- However, LLMs struggle with mathematical language and occasionally fail at basic inferences.  
- Current models are less susceptible to hypothesis-only "inference" compared to previous generations.  
- The corpora are provided to support future research on Math NLI.  <br /><br />Summary: <div>
arXiv:2507.23063v1 Announce Type: new 
Abstract: We ask whether contemporary LLMs are able to perform natural language inference (NLI) tasks on mathematical texts. We call this the Math NLI problem. We construct a corpus of Math NLI pairs whose premises are from extant mathematical text and whose hypotheses and gold labels were provided by people with experience in both research-level mathematics and also in the NLI field. We also investigate the quality of corpora using the same premises but whose hypotheses are provided by LLMs themselves. We not only investigate the performance but also the inter-group consistency of the diverse group of LLMs. We have both positive and negative findings. Among our positive findings: in some settings, using a majority vote of LLMs is approximately equivalent to using human-labeled data in the Math NLI area. On the negative side: LLMs still struggle with mathematical language. They occasionally fail at even basic inferences. Current models are not as prone to hypothesis-only "inference" in our data the way the previous generation had been. In addition to our findings, we also provide our corpora as data to support future work on Math NLI.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring In-Context Learning for Frame-Semantic Parsing</title>
<link>https://arxiv.org/abs/2507.23082</link>
<guid>https://arxiv.org/abs/2507.23082</guid>
<content:encoded><![CDATA[
<div> Keywords: Frame Semantic Parsing, In-Context Learning, Large Language Models, Frame Identification, Frame Semantic Role Labeling 

Summary: 
Frame Semantic Parsing (FSP) involves identifying predicates and their arguments based on Frame Semantics. This study explores the use of In-Context Learning (ICL) with Large Language Models (LLMs) for FSP tasks without fine-tuning the model. The method generates task-specific prompts for Frame Identification (FI) and Frame Semantic Role Labeling (FSRL) using the FrameNet database. These prompts guide six different LLMs in experiments focused on frames related to violent events, achieving competitive F1 scores of 94.3% for FI and 77.4% for FSRL. The results demonstrate that ICL can be a practical and effective approach for domain-specific FSP tasks, providing an alternative to traditional fine-tuning methods. 

<br /><br />Summary: <div>
arXiv:2507.23082v1 Announce Type: new 
Abstract: Frame Semantic Parsing (FSP) entails identifying predicates and labeling their arguments according to Frame Semantics. This paper investigates the use of In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP without model fine-tuning. We propose a method that automatically generates task-specific prompts for the Frame Identification (FI) and Frame Semantic Role Labeling (FSRL) subtasks, relying solely on the FrameNet database. These prompts, constructed from frame definitions and annotated examples, are used to guide six different LLMs. Experiments are conducted on a subset of frames related to violent events. The method achieves competitive results, with F1 scores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers a practical and effective alternative to traditional fine-tuning for domain-specific FSP tasks.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-aware Rotary Position Embedding</title>
<link>https://arxiv.org/abs/2507.23083</link>
<guid>https://arxiv.org/abs/2507.23083</guid>
<content:encoded><![CDATA[
<div> Rotary Positional Embeddings, RoPE, context-sensitive relationships, token embeddings, CARoPE<br />
<br />
Summary:<br />
Positional encoding is crucial in Transformer architectures for incorporating sequence order into self-attention mechanisms. While RoPE has been widely adopted for its efficiency, it lacks the ability to model context-sensitive relationships due to its static frequency patterns. In this work, CARoPE is introduced as a novel generalization of RoPE that dynamically generates head-specific frequency patterns based on token embeddings. By computing input-dependent phase shifts using a transformation of token embeddings, CARoPE offers token- and context-sensitive positional representations while maintaining efficiency and simplicity. Evaluation on the FineWeb-Edu-10B dataset with GPT-2 variants shows that CARoPE outperforms RoPE and other positional encoding baselines, achieving lower perplexity even at longer context lengths. Moreover, CARoPE enables faster training throughput without compromising model stability, making it a scalable, expressive, and efficient enhancement to positional encoding strategies in Transformer models. <div>
arXiv:2507.23083v1 Announce Type: new 
Abstract: Positional encoding is a vital component of Transformer architectures, enabling models to incorporate sequence order into self-attention mechanisms. Rotary Positional Embeddings (RoPE) have become a widely adopted solution due to their compatibility with relative position encoding and computational efficiency. However, RoPE relies on static, input-independent sinusoidal frequency patterns, limiting its ability to model context-sensitive relationships. In this work, we propose CARoPE (Context-Aware Rotary Positional Embedding), a novel generalization of RoPE that dynamically generates head-specific frequency patterns conditioned on token embeddings. This design introduces token- and context-sensitive positional representations while preserving RoPE efficiency and architectural simplicity. CARoPE computes input-dependent phase shifts using a bounded transformation of token embeddings and integrates them into the rotary mechanism across attention heads. We evaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on next-token prediction tasks. Experimental results show that CARoPE consistently outperforms RoPE and other common positional encoding baselines, achieving significantly lower perplexity, even at longer context lengths. Additionally, CARoPE enables faster training throughput without sacrificing model stability. These findings demonstrate that CARoPE offers a scalable, expressive, and efficient upgrade to existing positional encoding strategies in Transformer models.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity</title>
<link>https://arxiv.org/abs/2507.23095</link>
<guid>https://arxiv.org/abs/2507.23095</guid>
<content:encoded><![CDATA[
<div> SMART-Editor, compositional layout, content editing, Reward-Refine, RewardDPO

Summary:
SMART-Editor is a framework for compositional layout and content editing that maintains global coherence through two strategies: Reward-Refine and RewardDPO. This framework outperforms strong baselines like InstructPix2Pix and HIVE in structured and unstructured settings. RewardDPO shows up to 15% gains in structured scenarios, while Reward-Refine displays advantages in natural images. The introduction of SMARTEdit-Bench, a benchmark for multi-domain cascading edit scenarios, allows for evaluation of model performance. Both automatic and human evaluations confirm the effectiveness of reward-guided planning in producing visually aligned and semantically consistent edits. SMART-Editor's approach to editing across different domains proves to be a successful method for achieving high-quality edits in various applications. <br /><br />Summary: <div>
arXiv:2507.23095v1 Announce Type: new 
Abstract: We present SMART-Editor, a framework for compositional layout and content editing across structured (posters, websites) and unstructured (natural images) domains. Unlike prior models that perform local edits, SMART-Editor preserves global coherence through two strategies: Reward-Refine, an inference-time rewardguided refinement method, and RewardDPO, a training-time preference optimization approach using reward-aligned layout pairs. To evaluate model performance, we introduce SMARTEdit-Bench, a benchmark covering multi-domain, cascading edit scenarios. SMART-Editor outperforms strong baselines like InstructPix2Pix and HIVE, with RewardDPO achieving up to 15% gains in structured settings and Reward-Refine showing advantages on natural images. Automatic and human evaluations confirm the value of reward-guided planning in producing semantically consistent and visually aligned edits.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RASL: Retrieval Augmented Schema Linking for Massive Database Text-to-SQL</title>
<link>https://arxiv.org/abs/2507.23104</link>
<guid>https://arxiv.org/abs/2507.23104</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, natural language interface, database, metadata, retrieval architecture

Summary: 
The article presents a novel approach to scaling natural language interfaces for databases to enterprise-level data catalogs. The proposed component-based retrieval architecture decomposes database schemas and metadata into discrete semantic units, allowing for more targeted retrieval. This approach prioritizes table identification while leveraging column-level information to ensure manageable context budgets. Experimental results show high recall and accuracy, with the system outperforming baselines over massive databases with varying structures and metadata availability. The solution eliminates the need for domain-specific fine-tuning, making it practical for deployment in diverse enterprise settings. By effectively leveraging database metadata, the system addresses a critical scalability gap in natural language database interfaces.

Summary: <div>
arXiv:2507.23104v1 Announce Type: new 
Abstract: Despite advances in large language model (LLM)-based natural language interfaces for databases, scaling to enterprise-level data catalogs remains an under-explored challenge. Prior works addressing this challenge rely on domain-specific fine-tuning - complicating deployment - and fail to leverage important semantic context contained within database metadata. To address these limitations, we introduce a component-based retrieval architecture that decomposes database schemas and metadata into discrete semantic units, each separately indexed for targeted retrieval. Our approach prioritizes effective table identification while leveraging column-level information, ensuring the total number of retrieved tables remains within a manageable context budget. Experiments demonstrate that our method maintains high recall and accuracy, with our system outperforming baselines over massive databases with varying structure and available metadata. Our solution enables practical text-to-SQL systems deployable across diverse enterprise settings without specialized fine-tuning, addressing a critical scalability gap in natural language database interfaces.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity</title>
<link>https://arxiv.org/abs/2507.23121</link>
<guid>https://arxiv.org/abs/2507.23121</guid>
<content:encoded><![CDATA[
<div> Keywords: trustworthiness, large language models, Chinese textual ambiguity, dataset, language understanding

Summary: <br /><br />Large language models (LLMs) face challenges when encountering ambiguous narrative text, especially in the context of Chinese textual ambiguity. A benchmark dataset was created to study LLM behavior in handling ambiguity, revealing significant differences from human interpretation. LLMs struggle to distinguish between ambiguous and unambiguous text, often displaying overconfidence in interpreting ambiguous text as having a single meaning. Additionally, LLMs tend to overthink when trying to understand multiple possible meanings within ambiguous text. These findings underscore a critical limitation in current LLMs and emphasize the need for improved approaches to address uncertainty in language understanding. The publicly available dataset and code provide valuable resources for further research in this area. <br /><br />Summary: <div>
arXiv:2507.23121v1 Announce Type: new 
Abstract: In this work, we study a critical research problem regarding the trustworthiness of large language models (LLMs): how LLMs behave when encountering ambiguous narrative text, with a particular focus on Chinese textual ambiguity. We created a benchmark dataset by collecting and generating ambiguous sentences with context and their corresponding disambiguated pairs, representing multiple possible interpretations. These annotated examples are systematically categorized into 3 main categories and 9 subcategories. Through experiments, we discovered significant fragility in LLMs when handling ambiguity, revealing behavior that differs substantially from humans. Specifically, LLMs cannot reliably distinguish ambiguous text from unambiguous text, show overconfidence in interpreting ambiguous text as having a single meaning rather than multiple meanings, and exhibit overthinking when attempting to understand the various possible meanings. Our findings highlight a fundamental limitation in current LLMs that has significant implications for their deployment in real-world applications where linguistic ambiguity is common, calling for improved approaches to handle uncertainty in language understanding. The dataset and code are publicly available at this GitHub repository: https://github.com/ictup/LLM-Chinese-Textual-Disambiguation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ISO-Bench: Benchmarking Multimodal Causal Reasoning in Visual-Language Models through Procedural Plans</title>
<link>https://arxiv.org/abs/2507.23135</link>
<guid>https://arxiv.org/abs/2507.23135</guid>
<content:encoded><![CDATA[
<div> benchmark, causal relationships, multimodal models, vision-language, performance 

Summary: 
ISO-Bench is a new benchmark for evaluating causal relationships between visual observations and procedural text in multimodal models. The benchmark presents image-text pairs of task steps to determine temporal dependencies. Current vision-language models perform below human level, with the best zero-shot F1 score at 0.57 and chain-of-thought reasoning improving results up to 0.62 F1. Humans outperform models with a 0.98 F1 score. The findings suggest the need for enhancements in causal understanding within multimodal models. <div>
arXiv:2507.23135v1 Announce Type: new 
Abstract: Understanding causal relationships across modalities is a core challenge for multimodal models operating in real-world environments. We introduce ISO-Bench, a benchmark for evaluating whether models can infer causal dependencies between visual observations and procedural text. Each example presents an image of a task step and a text snippet from a plan, with the goal of deciding whether the visual step occurs before or after the referenced text step. Evaluation results on ten frontier vision-language models show underwhelming performance: the best zero-shot F1 is only 0.57, and chain-of-thought reasoning yields only modest gains (up to 0.62 F1), largely behind humans (0.98 F1). Our analysis further highlights concrete directions for improving causal understanding in multimodal models.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal</title>
<link>https://arxiv.org/abs/2507.23158</link>
<guid>https://arxiv.org/abs/2507.23158</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, user feedback, user-LM interaction, implicit feedback, model performance<br />
Summary: <br /><br />Language models (LMs) can benefit from continuous evolution based on user feedback harvested from user-LM interaction logs. The study analyzes implicit user feedback in two datasets and finds that the content of user feedback, not just its polarity, can enhance model performance in short human-designed questions. However, this improvement is not seen in longer and more complex questions. The study also highlights that the quality of the initial prompt from the user is essential for the usefulness of feedback. These findings provide valuable insights into the potential and limitations of implicit user feedback for enhancing the performance of language models. <div>
arXiv:2507.23158v1 Announce Type: new 
Abstract: Once language models (LMs) are deployed, they can interact with users long-term, ideally evolving continuously based on their feedback. Asking for direct user feedback can be disruptive; thus, we study harvesting user feedback from user-LM interaction logs. We study implicit user feedback in two user-LM interaction datasets (WildChat and LMSYS). First, we analyze user feedback in the user-LLM conversation trajectory, providing insights into when and why such feedback occurs. Second, we study harvesting learning signals from such implicit user feedback. We find that the contents of user feedback (e.g., user wanted clarification), not just the polarity (e.g., users were unhappy with the previous model response), can improve model performance in short human-designed questions (MTBench) but not on longer and more complex questions (WildBench). We also find that the usefulness of user feedback is largely tied to the quality of the user's initial prompt. Together, we provide an in-depth study of implicit user feedback, showing its potential and limitations.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LENS: Learning Ensemble Confidence from Neural States for Multi-LLM Answer Integration</title>
<link>https://arxiv.org/abs/2507.23167</link>
<guid>https://arxiv.org/abs/2507.23167</guid>
<content:encoded><![CDATA[
<div> confidence, Large Language Models, ensemble learning, neural networks, internal representations

Summary:
LENS introduces a novel method for combining the predictions of multiple Large Language Models (LLMs) by learning to estimate model confidence through analyzing internal representations. By training a linear confidence predictor for each LLM, LENS effectively weights model predictions based on context-dependent reliability without modifying the model parameters. Experimental results on question-answering tasks show that LENS outperforms traditional ensemble methods significantly. This approach leverages neural network hidden states and normalized probabilities to provide nuanced weighting of predictions, leading to improved system robustness and performance. The findings indicate that internal representations offer valuable signals for determining model confidence and can be leveraged for enhancing ensemble learning in LLMs. <div>
arXiv:2507.23167v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive performance across various tasks, with different models excelling in distinct domains and specific abilities. Effectively combining the predictions of multiple LLMs is crucial for enhancing system robustness and performance. However, existing ensemble methods often rely on simple techniques like voting or logits ensembling, which overlook the varying confidence and reliability of models in different contexts. In this work, we propose LENS (Learning ENsemble confidence from Neural States), a novel approach that learns to estimate model confidence by analyzing internal representations. For each LLM, we train a lightweight linear confidence predictor that leverages layer-wise hidden states and normalized probabilities as inputs. This allows for more nuanced weighting of model predictions based on their context-dependent reliability. Our method does not require modifying the model parameters and requires negligible additional computation. Experimental results on multiple-choice and boolean question-answering tasks demonstrate that LENS outperforms traditional ensemble methods by a substantial margin. Our findings suggest that internal representations provide valuable signals for determining model confidence and can be effectively leveraged for ensemble learning.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geak: Introducing Triton Kernel AI Agent &amp; Evaluation Benchmarks</title>
<link>https://arxiv.org/abs/2507.23194</link>
<guid>https://arxiv.org/abs/2507.23194</guid>
<content:encoded><![CDATA[
<div> DSL, GPU programming, AI-generated kernels, Triton language, GEAK framework <br />
<br />
In response to the increasing demand for AI-generated GPU kernels, a new evaluation suite for Triton-based GPU kernels and the GEAK framework have been developed. The Triton language, known for its balance of performance and ease of coding, is a popular target for AI-generated kernels. The GEAK framework utilizes cutting-edge LLMs to generate efficient Triton code specifically for AMD GPUs, such as the AMD MI300X and MI250. By leveraging inference-time compute scaling and a Reflexion-style feedback mechanism, GEAK significantly outperformed baseline approaches in terms of correctness and execution speed. The results demonstrate the potential of GEAK-like agentic code generation in accelerating the adoption of diverse hardware platforms and democratizing access to expert-level kernel performance. <br /><br />Summary: <div>
arXiv:2507.23194v1 Announce Type: new 
Abstract: The demand for AI-generated GPU kernels is rapidly growing, influenced by the need for scalable, hardware-optimized solutions in both industry and academia. As deep learning workloads grow in complexity and diversity, it is imperative to automate low-level kernel development to meet performance and productivity demands. Major cloud providers, semiconductor companies, and research institutions are now investing heavily in AI-driven code generation for GPUs, aiming to reduce manual optimization efforts while achieving near-expert performance on hardware like AMD MI300X. The Triton language, a Python-based DSL for GPU programming, has emerged as a popular target for such AI-generated kernels due to its balance of performance and ease-of-coding. In this work, we present an evaluation suite for Triton-based GPU kernels and GEAK (Generating Efficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs to generate performant Triton code specifically for AMD GPUs, including the AMD MI300X and MI250. GEAK leverages inference-time compute scaling to produce Triton-based GPU kernels using a reasoning loop adapted from Reflexion-style feedback mechanisms. On two evaluation benchmarks, GEAK significantly outperformed the baselines of directly prompting frontier LLMs as well as Reflexion-based generation pipelines by achieving correctness up to $63$% and execution speed up of up to $2.59$X. These results highlight the promise of GEAK-like agentic code generation for accelerating the adoption of diverse hardware platforms and democratizing access to expert-level kernel performance.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Failures Are the Stepping Stones to Success: Enhancing Few-Shot In-Context Learning by Leveraging Negative Samples</title>
<link>https://arxiv.org/abs/2507.23211</link>
<guid>https://arxiv.org/abs/2507.23211</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Few-shot in-context learning, Positive samples, Negative samples, Semantic similarity

Summary:
In this study, the focus is on improving few-shot in-context learning (ICL) by leveraging both Positive and Negative samples for better example selection. The research builds Positive and Negative sample corpora based on Zero-Shot-Cot and uses semantic similarity for selecting examples during inference. By incorporating information from Negative samples, the method enhances the selection of Positive examples, thereby improving ICL performance. Results indicate that utilizing Negative samples in example selection leads to better performance compared to methods solely relying on Positive samples. This innovative approach demonstrates the importance of considering both Positive and Negative samples in enhancing the efficiency and effectiveness of few-shot in-context learning. 

<br /><br />Summary: <div>
arXiv:2507.23211v1 Announce Type: new 
Abstract: Large Language Models exhibit powerful few-shot in-context learning (ICL) capabilities, but the performance is highly sensitive to provided examples.
  Recent research has focused on retrieving corresponding examples for each input query, not only enhancing the efficiency and scalability of the learning process but also mitigating inherent biases in manual example selection.
  However, these studies have primarily emphasized leveraging Positive samples while overlooking the additional information within Negative samples for contextual learning.
  We propose a novel method that utilizes Negative samples to better select Positive sample examples, thereby enhancing the performance of few-shot ICL. Initially, we construct Positive and Negative sample corpora based on Zero-Shot-Cot. Then, during inference, we employ a semantic similarity-based approach to select the most similar examples from both the Positive and Negative corpora for a given query. Subsequently, we further retrieve Positive examples from the Positive sample corpus based on semantic similarity to the Negative examples, then concatenating them with the previously selected Positive examples to serve as ICL demonstrations. Experimental results demonstrate that our approach surpasses methods solely relying on the most similar positive examples for context, validating that the additional information in negative samples aids in enhancing ICL performance through improved Positive sample selection.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Directions, Not Words: Mechanistic Topic Models Using Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2507.23220</link>
<guid>https://arxiv.org/abs/2507.23220</guid>
<content:encoded><![CDATA[
<div> Keywords: topic models, sparse autoencoders, semantic features, controllable text generation, evaluation framework

Summary:
Mechanistic Topic Models (MTMs) are introduced as a new class of topic models that utilize interpretable features learned by sparse autoencoders (SAEs). Unlike traditional and neural topic models that rely on word lists, MTMs operate on semantically rich spaces to reveal deeper conceptual themes with expressive feature descriptions. They also enable controllable text generation through topic-based steering vectors, a unique feature among topic models. Evaluation using the topic judge framework shows that MTMs match or exceed traditional and neural baselines on coherence metrics, are preferred by judges, and allow effective steering of language model outputs. Overall, MTMs offer a promising approach to capturing complex topics in large text collections with improved interpretability and performance. 

<br /><br />Summary: <div>
arXiv:2507.23220v1 Announce Type: new 
Abstract: Traditional topic models are effective at uncovering latent themes in large text collections. However, due to their reliance on bag-of-words representations, they struggle to capture semantically abstract features. While some neural variants use richer representations, they are similarly constrained by expressing topics as word lists, which limits their ability to articulate complex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic models that operate on interpretable features learned by sparse autoencoders (SAEs). By defining topics over this semantically rich space, MTMs can reveal deeper conceptual themes with expressive feature descriptions. Moreover, uniquely among topic models, MTMs enable controllable text generation using topic-based steering vectors. To properly evaluate MTM topics against word-list-based approaches, we propose \textit{topic judge}, an LLM-based pairwise comparison evaluation framework. Across five datasets, MTMs match or exceed traditional and neural baselines on coherence metrics, are consistently preferred by topic judge, and enable effective steering of LLM outputs.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker Data with LLMs</title>
<link>https://arxiv.org/abs/2507.23227</link>
<guid>https://arxiv.org/abs/2507.23227</guid>
<content:encoded><![CDATA[
<div> Keywords: Alzheimer's disease, biomarkers, large language models, tabular data, prediction

Summary:
TAP-GPT is a novel framework that utilizes large language models (LLMs) for the early and accurate diagnosis of Alzheimer's disease (AD) by analyzing heterogeneous biomarkers in a tabular format. The framework, based on TableGPT2 and adapted for AD diagnosis, incorporates few-shot reasoning, multimodal integration, and natural-language-based interpretability. By constructing tabular prompts with in-context learning examples and using parameter-efficient adaptation techniques, TAP-GPT outperforms other LLMs and a tabular foundation model for AD prediction tasks. This approach represents a groundbreaking application of LLMs in utilizing tabular biomarker data for AD diagnosis, opening new avenues for LLM-driven frameworks in biomedical informatics. <br /><br />Summary: TAP-GPT harnesses the power of large language models for Alzheimer's disease diagnosis through tabular data analysis, offering superior prediction capabilities compared to existing models. <div>
arXiv:2507.23227v1 Announce Type: new 
Abstract: Early and accurate diagnosis of Alzheimer's disease (AD), a complex neurodegenerative disorder, requires analysis of heterogeneous biomarkers (e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal fluid proteins) typically represented in a tabular format. With flexible few-shot reasoning, multimodal integration, and natural-language-based interpretability, large language models (LLMs) offer unprecedented opportunities for prediction with structured biomedical data. We propose a novel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts TableGPT2, a multimodal tabular-specialized LLM originally developed for business intelligence tasks, for AD diagnosis using structured biomarker data with small sample sizes. Our approach constructs few-shot tabular prompts using in-context learning examples from structured biomedical data and finetunes TableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary classification task of AD or cognitively normal (CN). The TAP-GPT framework harnesses the powerful tabular understanding ability of TableGPT2 and the encoded prior knowledge of LLMs to outperform more advanced general-purpose LLMs and a tabular foundation model (TFM) developed for prediction tasks. To our knowledge, this is the first application of LLMs to the prediction task using tabular biomarker data, paving the way for future LLM-driven multi-agent frameworks in biomedical informatics.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication</title>
<link>https://arxiv.org/abs/2507.23247</link>
<guid>https://arxiv.org/abs/2507.23247</guid>
<content:encoded><![CDATA[
<div> mental health, chatbots, explainability, pragmatic reasoning, stigma

Summary: 
- The study focuses on the development of personalized chatbots for mental health and explores the pragmatic reasoning capability of large language models (LLMs) in this domain.
- The researchers introduce the P-ReMe dataset and propose tasks related to implicature and presupposition in mental health discourse.
- Four models, including Llama3.1, Mistral, MentaLLaMa, and Qwen, are evaluated on the dataset, with Mistral and Qwen showing substantial reasoning capabilities.
- The study also introduces StiPRompts to analyze stigma around mental health using LLMs like GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku.
- Findings indicate that Claude-3.5-haiku handles stigma more responsibly compared to the other LLMs. 

<br /><br />Summary: <div>
arXiv:2507.23247v1 Announce Type: new 
Abstract: There has been an increase in recent advancements in the explainability and development of personalized chatbots for mental health. However, the reasoning aspects for explainability and dialogue discourse have not been explored previously for mental health. Hence, we are investigating the pragmatic reasoning capability of large language models (LLMs) in this domain. We introduce P-ReMe dataset, and propose a modified definition for the pragmatic phenomena of implicature (implied meaning) and presupposition (implicit assumption) in mental health. Following the definition, we formulate two tasks in implicature and one task in presupposition. To benchmark the dataset and the presented tasks, we consider four models - Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of the experiments suggest that Mistral and Qwen show substantial reasoning capabilities in the domain. In addition, we also propose StiPRompts to study the stigma around mental health with the state-of-the-art LLMs, GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings show that Claude-3.5-haiku deals with the stigma more responsibly compared to the other two LLMs.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark Creation and Performance Analysis</title>
<link>https://arxiv.org/abs/2507.23248</link>
<guid>https://arxiv.org/abs/2507.23248</guid>
<content:encoded><![CDATA[
<div> Language Models, Bengali, NLP, Evaluation Benchmarks, Tokenization
Summary:<br /><br />In this work, the challenges faced in Bengali NLP performance are explored, with a focus on the absence of standardized evaluation benchmarks. 10 recent Large Language Models were evaluated on translated datasets, highlighting performance gaps for Bengali compared to English, especially in smaller models and specific families. Certain architectures, like DeepSeek, show promising robustness across languages. An inverse relationship between tokenization efficiency and model accuracy was observed, with more concise tokenization leading to improved performance. The study emphasizes the need for improved dataset quality and evaluation methods tailored for multilingual contexts to advance NLP research for underrepresented languages and make advanced language technologies more accessible worldwide. <div>
arXiv:2507.23248v1 Announce Type: new 
Abstract: Bengali is an underrepresented language in NLP research. However, it remains a challenge due to its unique linguistic structure and computational constraints. In this work, we systematically investigate the challenges that hinder Bengali NLP performance by focusing on the absence of standardized evaluation benchmarks. We then evaluated 10 recent open source Large Language Models (LLMs) in 8 of the translated datasets and performed a comprehensive error analysis to pinpoint their primary failure modes. Our findings reveal consistent performance gaps for Bengali compared to English, particularly for smaller models and specific model families like Mistral. We also identified promising robustness in certain architectures, such as DeepSeek, that maintain more stable performance across languages. Our analysis reveals an inverse relationship between tokenization efficiency and LLM accuracy where models tend to perform worse when inputs are excessively tokenized, whereas more efficient \& concise tokenization results in improved performance. These findings highlight critical areas where current models fall short and underscore the need for improved dataset quality and evaluation methodologies tailored to multilingual contexts. This work will catalyze further research on NLP for underrepresented languages, helping to democratize access to advanced language technologies worldwide. The code and dataset used in this research is publicly available at https://github.com/BengaliAI/bn-llm-benchmark.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Super Experts in Mixture-of-Experts Large Language Models</title>
<link>https://arxiv.org/abs/2507.23279</link>
<guid>https://arxiv.org/abs/2507.23279</guid>
<content:encoded><![CDATA[
<div> Experts, Mixture-of-Experts, Sparsity, Compression, Language models 
Summary: 
SEs, termed Super Experts, have been identified as a crucial subset in Sparsely activated Mixture-of-Experts (MoE) models. These SEs exhibit extreme activation outliers, impacting the model's performance significantly, especially in mathematical reasoning tasks. Pruning SEs leads to repetitive and uninformative outputs, emphasizing their importance in ensuring model efficiency. SEs induce attention sinks vital for attention score distribution but are disrupted by pruning, affecting model functionality. The distribution and influence of SEs are model-specific, remaining consistent post-training. Analysis shows SEs play a fundamental role in the model's forward inference mechanism. The research provides a deeper understanding of SEs' heterogeneous importance and their contribution to enhancing MoE LLMs' learning capacity. The available code enables further exploration and validation of the findings. 
<br /><br />Summary: <div>
arXiv:2507.23279v1 Announce Type: new 
Abstract: Sparsely activated Mixture-of-Experts (MoE) models have shown promise in enhancing the learning capacity of large language models (LLMs). Leveraging the intrinsic importance differences among experts, recent research has explored expert-level compression techniques to improve the efficiency of MoE LLMs. However, existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts. In this study, we present the first discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference. These experts are prevalent in open-source MoE LLMs, and despite their limited number, pruning them leads to a significant decline in model performance (e.g., pruning three causes Qwen3-30B-A3B to produce repetitive and uninformative outputs). We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs. (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs remains model-specific and is unaffected by post-training processes. (ii) By pruning SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further enhance our understanding of the influence of SEs compression. Our findings confirm that MoE LLMs rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE pruning. The code is available at https://github.com/ZunhaiSu/Super-Experts-Profilling.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive Content</title>
<link>https://arxiv.org/abs/2507.23319</link>
<guid>https://arxiv.org/abs/2507.23319</guid>
<content:encoded><![CDATA[
arXiv:2507.23319v1 Announce Type: new 
Abstract: Proprietary Large Language Models (LLMs) have shown tendencies toward politeness, formality, and implicit content moderation. While previous research has primarily focused on explicitly training models to moderate and detoxify sensitive content, there has been limited exploration of whether LLMs implicitly sanitize language without explicit instructions. This study empirically analyzes the implicit moderation behavior of GPT-4o-mini when paraphrasing sensitive content and evaluates the extent of sensitivity shifts. Our experiments indicate that GPT-4o-mini systematically moderates content toward less sensitive classes, with substantial reductions in derogatory and taboo language. Also, we evaluate the zero-shot capabilities of LLMs in classifying sentence sensitivity, comparing their performances against traditional methods.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2507.23334</link>
<guid>https://arxiv.org/abs/2507.23334</guid>
<content:encoded><![CDATA[
arXiv:2507.23334v1 Announce Type: new 
Abstract: Recent advancements in Large language models (LLMs) have demonstrated remarkable capabilities across diverse domains. While they exhibit strong zero-shot performance on various tasks, LLMs' effectiveness in music-related applications remains limited due to the relatively small proportion of music-specific knowledge in their training data. To address this limitation, we propose MusT-RAG, a comprehensive framework based on Retrieval Augmented Generation (RAG) to adapt general-purpose LLMs for text-only music question answering (MQA) tasks. RAG is a technique that provides external knowledge to LLMs by retrieving relevant context information when generating answers to questions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a music-specialized vector database for the retrieval stage, and (2) utilizes context information during both inference and fine-tuning processes to effectively transform general-purpose LLMs into music-specific models. Our experiment demonstrates that MusT-RAG significantly outperforms traditional fine-tuning approaches in enhancing LLMs' music domain adaptation capabilities, showing consistent improvements across both in-domain and out-of-domain MQA benchmarks. Additionally, our MusWikiDB proves substantially more effective than general Wikipedia corpora, delivering superior performance and computational efficiency.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-SQL Task-oriented Dialogue Ontology Construction</title>
<link>https://arxiv.org/abs/2507.23358</link>
<guid>https://arxiv.org/abs/2507.23358</guid>
<content:encoded><![CDATA[
arXiv:2507.23358v1 Announce Type: new 
Abstract: Large language models (LLMs) are widely used as general-purpose knowledge sources, but they rely on parametric knowledge, limiting explainability and trustworthiness. In task-oriented dialogue (TOD) systems, this separation is explicit, using an external database structured by an explicit ontology to ensure explainability and controllability. However, building such ontologies requires manual labels or supervised training. We introduce TeQoDO: a Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM autonomously builds a TOD ontology from scratch without supervision using its inherent SQL programming capabilities combined with dialogue theory provided in the prompt. We show that TeQoDO outperforms transfer learning approaches, and its constructed ontology is competitive on a downstream dialogue state tracking task. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also scales to allow construction of much larger ontologies, which we investigate on a Wikipedia and ArXiv dataset. We view this as a step towards broader application of ontologies to increase LLM explainability.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2507.23382</link>
<guid>https://arxiv.org/abs/2507.23382</guid>
<content:encoded><![CDATA[
arXiv:2507.23382v1 Announce Type: new 
Abstract: Multimodal planning capabilities refer to the ability to predict, reason, and design steps for task execution with multimodal context, which is essential for complex reasoning and decision-making across multiple steps. However, current benchmarks face two key challenges: (1) they cannot directly assess multimodal real-world planning capabilities, and (2) they lack constraints or implicit constraints across modalities. To address these issues, we introduce Multimodal Planning with Complex Constraints (MPCC), the first benchmark to systematically evaluate MLLMs' ability to handle multimodal constraints in planning. To address the first challenge, MPCC focuses on three real-world tasks: Flight Planning, Calendar Planning, and Meeting Planning. To solve the second challenge, we introduce complex constraints (e.g. budget, temporal, and spatial) in these tasks, with graded difficulty levels (EASY, MEDIUM, HARD) to separate constraint complexity from search space expansion. Experiments on 13 advanced MLLMs reveal significant challenges: closed-source models achieve only 21.3% feasible plans, while open-source models average below 11%. Additionally, we observe that MLLMs are highly sensitive to constraint complexity and that traditional multimodal prompting strategies fail in multi-constraint scenarios. Our work formalizes multimodal constraints in planning, provides a rigorous evaluation framework, and highlights the need for advancements in constraint-aware reasoning for real-world MLLM applications.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models</title>
<link>https://arxiv.org/abs/2507.23386</link>
<guid>https://arxiv.org/abs/2507.23386</guid>
<content:encoded><![CDATA[
arXiv:2507.23386v1 Announce Type: new 
Abstract: Decoder-only large language models (LLMs) are increasingly used to build embedding models that effectively encode the semantic information of natural language texts into dense vector representations for various embedding tasks. However, many existing methods primarily focus on removing the causal attention mask in LLMs to enable bidirectional attention, potentially undermining the model's ability to extract semantic information acquired during pretraining. Additionally, leading unidirectional approaches often rely on extra input text to overcome the inherent limitations of causal attention, inevitably increasing computational costs. In this work, we propose Causal2Vec, a general-purpose embedding model tailored to enhance the performance of decoder-only LLMs without altering their original architectures or introducing significant computational overhead. Specifically, we first employ a lightweight BERT-style model to pre-encode the input text into a single Contextual token, which is then prepended to the LLM's input sequence, allowing each token to capture contextualized information even without attending to future tokens. Furthermore, to mitigate the recency bias introduced by last-token pooling and help LLMs better leverage the semantic information encoded in the Contextual token, we concatenate the last hidden states of Contextual and EOS tokens as the final text embedding. In practice, Causal2Vec achieves state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB) among models trained solely on publicly available retrieval datasets, while reducing the required sequence length by up to 85% and inference time by up to 82% compared to best-performing methods.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM Deployment for Translators</title>
<link>https://arxiv.org/abs/2507.23399</link>
<guid>https://arxiv.org/abs/2507.23399</guid>
<content:encoded><![CDATA[
arXiv:2507.23399v1 Announce Type: new 
Abstract: The rapid proliferation of Large Language Models presents both opportunities and challenges for the translation field. While commercial, cloud-based AI chatbots have garnered significant attention in translation studies, concerns regarding data privacy, security, and equitable access necessitate exploration of alternative deployment models. This paper investigates the feasibility and performance of locally deployable, free language models as a viable alternative to proprietary, cloud-based AI solutions. This study evaluates three open-source models installed on CPU-based platforms and compared against commercially available online chat-bots. The evaluation focuses on functional performance rather than a comparative analysis of human-machine translation quality, an area already subject to extensive research. The platforms assessed were chosen for their accessibility and ease of use across various operating systems. While local deployment introduces its own challenges, the benefits of enhanced data control, improved privacy, and reduced dependency on cloud services are compelling. The findings of this study contribute to a growing body of knowledge concerning the democratization of AI technology and inform future research and development efforts aimed at making LLMs more accessible and practical for a wider range of users, specifically focusing on the needs of individual translators and small businesses.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based on Multi-Relational Graphs and Structural Entropy Minimization</title>
<link>https://arxiv.org/abs/2507.23400</link>
<guid>https://arxiv.org/abs/2507.23400</guid>
<content:encoded><![CDATA[
arXiv:2507.23400v1 Announce Type: new 
Abstract: The core challenge faced by multi-document summarization is the complexity of relationships among documents and the presence of information redundancy. Graph clustering is an effective paradigm for addressing this issue, as it models the complex relationships among documents using graph structures and reduces information redundancy through clustering, achieving significant research progress. However, existing methods often only consider single-relational graphs and require a predefined number of clusters, which hinders their ability to fully represent rich relational information and adaptively partition sentence groups to reduce redundancy. To overcome these limitations, we propose MRGSEM-Sum, an unsupervised multi-document summarization framework based on multi-relational graphs and structural entropy minimization. Specifically, we construct a multi-relational graph that integrates semantic and discourse relations between sentences, comprehensively modeling the intricate and dynamic connections among sentences across documents. We then apply a two-dimensional structural entropy minimization algorithm for clustering, automatically determining the optimal number of clusters and effectively organizing sentences into coherent groups. Finally, we introduce a position-aware compression mechanism to distill each cluster, generating concise and informative summaries. Extensive experiments on four benchmark datasets (Multi-News, DUC-2004, PubMed, and WikiSum) demonstrate that our approach consistently outperforms previous unsupervised methods and, in several cases, achieves performance comparable to supervised models and large language models. Human evaluation demonstrates that the summaries generated by MRGSEM-Sum exhibit high consistency and coverage, approaching human-level quality.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Arabic Text Retrieval with Attentive Relevance Scoring</title>
<link>https://arxiv.org/abs/2507.23404</link>
<guid>https://arxiv.org/abs/2507.23404</guid>
<content:encoded><![CDATA[
arXiv:2507.23404v1 Announce Type: new 
Abstract: Arabic poses a particular challenge for natural language processing (NLP) and information retrieval (IR) due to its complex morphology, optional diacritics and the coexistence of Modern Standard Arabic (MSA) and various dialects. Despite the growing global significance of Arabic, it is still underrepresented in NLP research and benchmark resources. In this paper, we present an enhanced Dense Passage Retrieval (DPR) framework developed specifically for Arabic. At the core of our approach is a novel Attentive Relevance Scoring (ARS) that replaces standard interaction mechanisms with an adaptive scoring function that more effectively models the semantic relevance between questions and passages. Our method integrates pre-trained Arabic language models and architectural refinements to improve retrieval performance and significantly increase ranking accuracy when answering Arabic questions. The code is made publicly available at \href{https://github.com/Bekhouche/APR}{GitHub}.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Passive Critical Thinking: Fostering Proactive Questioning to Enhance Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2507.23407</link>
<guid>https://arxiv.org/abs/2507.23407</guid>
<content:encoded><![CDATA[
arXiv:2507.23407v1 Announce Type: new 
Abstract: Critical thinking is essential for building robust AI systems, preventing them from blindly accepting flawed data or biased reasoning. However, prior work has primarily focused on passive critical thinking, where models simply reject problematic queries without taking constructive steps to address user requests. In this work, we introduce proactive critical thinking, a paradigm where models actively seek missing or clarifying information from users to resolve their queries better. To evaluate this capability, we present GSM-MC and GSM-MCE, two novel benchmarks based on GSM8K for assessing mathematical reasoning under incomplete or misleading conditions. GSM-MC contains 1,368 math problems with a key variable deliberately removed, requiring models to identify and request the missing information. GSM-MCE further increases the difficulty by introducing irrelevant details to test robustness against distractions. Experiments on Qwen3 and Llama series models show that, while these models excel in traditional reasoning tasks due to extensive post-training and inference-time scaling, they struggle with proactive critical thinking, especially smaller ones. However, we demonstrate that reinforcement learning (RL) can significantly improve this ability. Using our enhanced RL algorithm, we achieve substantial gains, boosting the Qwen3-1.7B's accuracy from 0.15% to 73.98% on GSM-MC. We hope this work advances models that collaborate more effectively with users in problem-solving through proactive critical thinking.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Role-Aware Language Models for Secure and Contextualized Access Control in Organizations</title>
<link>https://arxiv.org/abs/2507.23465</link>
<guid>https://arxiv.org/abs/2507.23465</guid>
<content:encoded><![CDATA[
arXiv:2507.23465v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed in enterprise settings, controlling model behavior based on user roles becomes an essential requirement. Existing safety methods typically assume uniform access and focus on preventing harmful or toxic outputs, without addressing role-specific access constraints. In this work, we investigate whether LLMs can be fine-tuned to generate responses that reflect the access privileges associated with different organizational roles. We explore three modeling strategies: a BERT-based classifier, an LLM-based classifier, and role-conditioned generation. To evaluate these approaches, we construct two complementary datasets. The first is adapted from existing instruction-tuning corpora through clustering and role labeling, while the second is synthetically generated to reflect realistic, role-sensitive enterprise scenarios. We assess model performance across varying organizational structures and analyze robustness to prompt injection, role mismatch, and jailbreak attempts.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and Effectiveness in Clinical Domains</title>
<link>https://arxiv.org/abs/2507.23486</link>
<guid>https://arxiv.org/abs/2507.23486</guid>
<content:encoded><![CDATA[
arXiv:2507.23486v1 Announce Type: new 
Abstract: Large language models (LLMs) hold promise in clinical decision support but face major challenges in safety evaluation and effectiveness validation. We developed the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a multidimensional framework built on clinical expert consensus, encompassing 30 criteria covering critical areas like critical illness recognition, guideline adherence, and medication safety, with weighted consequence measures. Thirty-two specialist physicians developed and reviewed 2,069 open-ended Q&amp;A items aligned with these criteria, spanning 26 clinical departments to simulate real-world scenarios. Benchmark testing of six LLMs revealed moderate overall performance (average total score 57.2%, safety 54.7%, effectiveness 62.3%), with a significant 13.3% performance drop in high-risk scenarios (p < 0.0001). Domain-specific medical LLMs showed consistent performance advantages over general-purpose models, with relatively higher top scores in safety (0.912) and effectiveness (0.861). The findings of this study not only provide a standardized metric for evaluating the clinical application of medical LLMs, facilitating comparative analyses, risk exposure identification, and improvement directions across different scenarios, but also hold the potential to promote safer and more effective deployment of large language models in healthcare environments.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via Progressive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.23541</link>
<guid>https://arxiv.org/abs/2507.23541</guid>
<content:encoded><![CDATA[
arXiv:2507.23541v1 Announce Type: new 
Abstract: In medical scenarios, effectively retrieving external knowledge and leveraging it for rigorous logical reasoning is of significant importance. Despite their potential, existing work has predominantly focused on enhancing either retrieval or reasoning capabilities of the models in isolation, with little attention given to their joint optimization, which leads to limited coordination between the two processes. Additionally, current methods rely heavily on supervised fine-tuning (SFT), which can cause models to memorize existing problem-solving pathways, thereby restricting their generalization ability when confronted with novel problem contexts. Furthermore, while some studies have explored to improve retrieval-augmented reasoning in general domains via reinforcement learning, their reward function designs do not adequately capture the specific demands of the medical domain. To address these challenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented **R**easoning framework driven by progressive **R**einforcement learning. In this framework, we first develop the model's ability to perform logical reasoning over medical problems. Subsequently, on the basis of this foundation, we adaptively optimize the retrieval capability to better align with the characteristics of knowledge corpus and external information utilization throughout the reasoning process. Finally, we conduct joint optimization of the model's retrieval and reasoning coordination. Extensive experiments indicate that **Med-R$^3$** could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by 3.93\% at a comparable parameter scale, while Qwen2.5-14B augmented with Med-R$^3$ shows a more substantial gain of 13.53\%.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-Detect: Tail-Aware Statistical Normalization for Robust Detection of Adversarial Machine-Generated Text</title>
<link>https://arxiv.org/abs/2507.23577</link>
<guid>https://arxiv.org/abs/2507.23577</guid>
<content:encoded><![CDATA[
arXiv:2507.23577v1 Announce Type: new 
Abstract: The proliferation of sophisticated text generation models necessitates the development of robust detection methods capable of identifying machine-generated content, particularly text designed to evade detection through adversarial perturbations. Existing zero-shot detectors often rely on statistical measures that implicitly assume Gaussian distributions, a premise that falters when confronted with the heavy-tailed statistical artifacts characteristic of adversarial or non-native English texts. This paper introduces T-Detect, a novel detection method that fundamentally redesigns the statistical core of curvature-based detectors. Our primary innovation is the replacement of standard Gaussian normalization with a heavy-tailed discrepancy score derived from the Student's t-distribution. This approach is theoretically grounded in the empirical observation that adversarial texts exhibit significant leptokurtosis, rendering traditional statistical assumptions inadequate. T-Detect computes a detection score by normalizing the log-likelihood of a passage against the expected moments of a t-distribution, providing superior resilience to statistical outliers. We validate our approach on the challenging RAID benchmark for adversarial text and the comprehensive HART dataset. Experiments show that T-Detect provides a consistent performance uplift over strong baselines, improving AUROC by up to 3.9\% in targeted domains. When integrated into a two-dimensional detection framework (CT), our method achieves state-of-the-art performance, with an AUROC of 0.926 on the Books domain of RAID. Our contributions are a new, theoretically-justified statistical foundation for text detection, an ablation-validated method that demonstrates superior robustness, and a comprehensive analysis of its performance under adversarial conditions. Ours code are released at https://github.com/ResearAI/t-detect.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffLoRA: Differential Low-Rank Adapters for Large Language Models</title>
<link>https://arxiv.org/abs/2507.23588</link>
<guid>https://arxiv.org/abs/2507.23588</guid>
<content:encoded><![CDATA[
arXiv:2507.23588v1 Announce Type: new 
Abstract: Differential Transformer has recently been proposed to improve performance in Transformer models by canceling out noise through a denoiser attention mechanism. In this work, we introduce DiffLoRA, a parameter-efficient adaptation of the differential attention mechanism, with low-rank adapters on both positive and negative attention terms. This approach retains the efficiency of LoRA while aiming to benefit from the performance gains of differential attention. We evaluate DiffLoRA across a broad range of NLP tasks, including general benchmarks, many-shot in-context learning, RAG, and long-context tests. We observe that, although DiffLoRA falls short of other parameter-efficient fine-tuning methods in most evaluation tasks, it shows interesting results in certain domains (+11 pts on LoRA for HumanEval). We analyze the attention patterns post-finetuning to identify the reasons for this behavior.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arabic Hate Speech Identification and Masking in Social Media using Deep Learning Models and Pre-trained Models Fine-tuning</title>
<link>https://arxiv.org/abs/2507.23661</link>
<guid>https://arxiv.org/abs/2507.23661</guid>
<content:encoded><![CDATA[
arXiv:2507.23661v1 Announce Type: new 
Abstract: Hate speech identification in social media has become an increasingly important issue in recent years. In this research, we address two problems: 1) to detect hate speech in Arabic text, 2) to clean a given text from hate speech. The meaning of cleaning here is replacing each bad word with stars based on the number of letters for each word. Regarding the first problem, we conduct several experiments using deep learning models and transformers to determine the best model in terms of the F1 score. Regarding second problem, we consider it as a machine translation task, where the input is a sentence containing dirty text and the output is the same sentence with masking the dirty text. The presented methods achieve the best model in hate speech detection with a 92\% Macro F1 score and 95\% accuracy. Regarding the text cleaning experiment, the best result in the hate speech masking model reached 0.3 in BLEU score with 1-gram, which is a good result compared with the state of the art machine translation systems.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs</title>
<link>https://arxiv.org/abs/2507.23740</link>
<guid>https://arxiv.org/abs/2507.23740</guid>
<content:encoded><![CDATA[
arXiv:2507.23740v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) often contain sufficient information to support the inference of new facts. Identifying logical rules not only improves the completeness of a knowledge graph but also enables the detection of potential errors, reveals subtle data patterns, and enhances the overall capacity for reasoning and interpretation. However, the complexity of such rules, combined with the unique labeling conventions of each KG, can make them difficult for humans to understand. In this paper, we explore the potential of large language models to generate natural language explanations for logical rules. Specifically, we extract logical rules using the AMIE 3.5.1 rule discovery algorithm from the benchmark dataset FB15k-237 and two large-scale datasets, FB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including zero- and few-shot prompting, including variable entity types, and chain-of-thought reasoning. We conduct a comprehensive human evaluation of the generated explanations based on correctness, clarity, and hallucination, and also assess the use of large language models as automatic judges. Our results demonstrate promising performance in terms of explanation correctness and clarity, although several challenges remain for future research. All scripts and data used in this study are publicly available at https://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cascaded Information Disclosure for Generalized Evaluation of Problem Solving Capabilities</title>
<link>https://arxiv.org/abs/2507.23776</link>
<guid>https://arxiv.org/abs/2507.23776</guid>
<content:encoded><![CDATA[
arXiv:2507.23776v1 Announce Type: new 
Abstract: While question-answering~(QA) benchmark performance is an automatic and scalable method to compare LLMs, it is an indirect method of evaluating their underlying problem-solving capabilities. Therefore, we propose a holistic and generalizable framework based on \emph{cascaded question disclosure} that provides a more accurate estimate of the models' problem-solving capabilities while maintaining the scalability and automation. This approach collects model responses in a stagewise manner with each stage revealing partial information about the question designed to elicit generalized reasoning in LLMs. We find that our approach not only provides a better comparison between LLMs, but also induces better intermediate traces in models compared to the standard QA paradigm. We empirically verify this behavior on diverse reasoning and knowledge-heavy QA datasets by comparing LLMs of varying sizes and families. Our approach narrows the performance gap observed in the standard QA evaluation settings, indicating that the prevalent indirect QA paradigm of evaluation overestimates the differences in performance between models. We further validate our findings by extensive ablation studies.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation</title>
<link>https://arxiv.org/abs/2507.22892</link>
<guid>https://arxiv.org/abs/2507.22892</guid>
<content:encoded><![CDATA[
arXiv:2507.22892v1 Announce Type: cross 
Abstract: Conventional augmentative and alternative communication (AAC) systems and language-learning platforms often fail to adapt in real time to the user's cognitive and linguistic needs, especially in neurological conditions such as post-stroke aphasia or amyotrophic lateral sclerosis. Recent advances in noninvasive electroencephalography (EEG)--based brain-computer interfaces (BCIs) and transformer--based large language models (LLMs) offer complementary strengths: BCIs capture users' neural intent with low fatigue, while LLMs generate contextually tailored language content. We propose and evaluate a novel hybrid framework that leverages real-time EEG signals to drive an LLM-powered language rehabilitation assistant. This system aims to: (1) enable users with severe speech or motor impairments to navigate language-learning modules via mental commands; (2) dynamically personalize vocabulary, sentence-construction exercises, and corrective feedback; and (3) monitor neural markers of cognitive effort to adjust task difficulty on the fly.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment</title>
<link>https://arxiv.org/abs/2507.22898</link>
<guid>https://arxiv.org/abs/2507.22898</guid>
<content:encoded><![CDATA[
arXiv:2507.22898v1 Announce Type: cross 
Abstract: We developed a voice-driven artificial intelligence (AI) system that guides anyone - from paramedics to family members - through expert-level stroke evaluations using natural conversation, while also enabling smartphone video capture of key examination components for documentation and potential expert review. This addresses a critical gap in emergency care: current stroke recognition by first responders is inconsistent and often inaccurate, with sensitivity for stroke detection as low as 58%, causing life-threatening delays in treatment. Three non-medical volunteers used our AI system to assess ten simulated stroke patients, including cases with likely large vessel occlusion (LVO) strokes and stroke-like conditions, while we measured diagnostic accuracy, completion times, user confidence, and expert physician review of the AI-generated reports. The AI system correctly identified 84% of individual stroke signs and detected 75% of likely LVOs, completing evaluations in just over 6 minutes. Users reported high confidence (median 4.5/5) and ease of use (mean 4.67/5). The system successfully identified 86% of actual strokes but also incorrectly flagged 2 of 3 non-stroke cases as strokes. When an expert physician reviewed the AI reports with videos, they identified the correct diagnosis in 100% of cases, but felt confident enough to make preliminary treatment decisions in only 40% of cases due to observed AI errors including incorrect scoring and false information. While the current system's limitations necessitate human oversight, ongoing rapid advancements in speech-to-speech AI models suggest that future versions are poised to enable highly accurate assessments. Achieving human-level voice interaction could transform emergency medical care, putting expert-informed assessment capabilities in everyone's hands.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting</title>
<link>https://arxiv.org/abs/2507.22902</link>
<guid>https://arxiv.org/abs/2507.22902</guid>
<content:encoded><![CDATA[
arXiv:2507.22902v1 Announce Type: cross 
Abstract: Background: Globally we face a projected shortage of 11 million healthcare practitioners by 2030, and administrative burden consumes 50% of clinical time. Artificial intelligence (AI) has the potential to help alleviate these problems. However, no end-to-end autonomous large language model (LLM)-based AI system has been rigorously evaluated in real-world clinical practice. In this study, we evaluated whether a multi-agent LLM-based AI framework can function autonomously as an AI doctor in a virtual urgent care setting. Methods: We retrospectively compared the performance of the multi-agent AI system Doctronic and board-certified clinicians across 500 consecutive urgent-care telehealth encounters. The primary end points: diagnostic concordance, treatment plan consistency, and safety metrics, were assessed by blinded LLM-based adjudication and expert human review. Results: The top diagnosis of Doctronic and clinician matched in 81% of cases, and the treatment plan aligned in 99.2% of cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not supported by clinical findings). In an expert review of discordant cases, AI performance was superior in 36.1%, and human performance was superior in 9.3%; the diagnoses were equivalent in the remaining cases. Conclusions: In this first large-scale validation of an autonomous AI doctor, we demonstrated strong diagnostic and treatment plan concordance with human clinicians, with AI performance matching and in some cases exceeding that of practicing clinicians. These findings indicate that multi-agent AI systems achieve comparable clinical decision-making to human providers and offer a potential solution to healthcare workforce shortages.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELMES: An Automated Framework for Evaluating Large Language Models in Educational Scenarios</title>
<link>https://arxiv.org/abs/2507.22947</link>
<guid>https://arxiv.org/abs/2507.22947</guid>
<content:encoded><![CDATA[
arXiv:2507.22947v1 Announce Type: cross 
Abstract: The emergence of Large Language Models (LLMs) presents transformative opportunities for education, generating numerous novel application scenarios. However, significant challenges remain: evaluation metrics vary substantially across different educational scenarios, while many emerging scenarios lack appropriate assessment metrics. Current benchmarks predominantly measure general intelligence rather than pedagogical capabilities. To address this gap, we introduce ELMES, an open-source automated evaluation framework specifically designed for assessing LLMs in educational settings. ELMES features a modular architecture that enables researchers to create dynamic, multi-agent dialogues through simple configuration files, facilitating flexible scenario design without requiring extensive programming expertise. The framework incorporates a hybrid evaluation engine that objectively quantifies traditionally subjective pedagogical metrics using an LLM-as-a-Judge methodology. We conduct systematic benchmarking of state-of-the-art LLMs across four critical educational scenarios: Knowledge Point Explanation, Guided Problem-Solving Teaching, Interdisciplinary Lesson Plan Generation, and Contextualized Question Generation, employing fine-grained metrics developed in collaboration with education specialists. Our results demonstrate distinct capability distributions among models, revealing context-specific strengths and limitations. ELMES provides educators and researchers with an accessible evaluation framework that significantly reduces adaptation barriers for diverse educational applications while advancing the practical implementation of LLMs in pedagogy. The framework is publicly available at \emph{https://github.com/sii-research/elmes.git}.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Dynamic Parameters for Vietnamese Gender-Independent ASR</title>
<link>https://arxiv.org/abs/2507.22964</link>
<guid>https://arxiv.org/abs/2507.22964</guid>
<content:encoded><![CDATA[
arXiv:2507.22964v1 Announce Type: cross 
Abstract: The dynamic characteristics of speech signal provides temporal information and play an important role in enhancing Automatic Speech Recognition (ASR). In this work, we characterized the acoustic transitions in a ratio plane of Spectral Subband Centroid Frequencies (SSCFs) using polar parameters to capture the dynamic characteristics of the speech and minimize spectral variation. These dynamic parameters were combined with Mel-Frequency Cepstral Coefficients (MFCCs) in Vietnamese ASR to capture more detailed spectral information. The SSCF0 was used as a pseudo-feature for the fundamental frequency (F0) to describe the tonal information robustly. The findings showed that the proposed parameters significantly reduce word error rates and exhibit greater gender independence than the baseline MFCCs.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Reinforcement Learning for Retriever-Specific Query Rewriter with Unstructured Real-World Documents</title>
<link>https://arxiv.org/abs/2507.23242</link>
<guid>https://arxiv.org/abs/2507.23242</guid>
<content:encoded><![CDATA[
arXiv:2507.23242v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems rely heavily on effective query formulation to unlock external knowledge, yet optimizing queries for diverse, unstructured real-world documents remains a challenge. We introduce \textbf{RL-QR}, a reinforcement learning framework for retriever-specific query rewriting that eliminates the need for human-annotated datasets and extends applicability to both text-only and multi-modal databases. By synthesizing scenario-question pairs and leveraging Generalized Reward Policy Optimization (GRPO), RL-QR trains query rewriters tailored to specific retrievers, enhancing retrieval performance across varied domains. Experiments on industrial in-house data demonstrate significant improvements, with $\text{RL-QR}_{\text{multi-modal}}$ achieving an 11\% relative gain in NDCG@3 for multi-modal RAG and $\text{RL-QR}_{\text{lexical}}$ yielding a 9\% gain for lexical retrievers. However, challenges persist with semantic and hybrid retrievers, where rewriters failed to improve performance, likely due to training misalignments. Our findings highlight RL-QR's potential to revolutionize query optimization for RAG systems, offering a scalable, annotation-free solution for real-world retrieval tasks, while identifying avenues for further refinement in semantic retrieval contexts.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy</title>
<link>https://arxiv.org/abs/2507.23292</link>
<guid>https://arxiv.org/abs/2507.23292</guid>
<content:encoded><![CDATA[
arXiv:2507.23292v1 Announce Type: cross 
Abstract: We introduce a neural network layer API and library for sequence modeling, designed for easy creation of sequence models that can be executed both layer-by-layer (e.g., teacher-forced training) and step-by-step (e.g., autoregressive sampling). To achieve this, layers define an explicit representation of their state over time (e.g., a Transformer KV cache, a convolution buffer, an RNN hidden state), and a step method that evolves that state, tested to give identical results to a stateless layer-wise invocation. This and other aspects of the SequenceLayers contract enables complex models to be immediately streamable, mitigates a wide range of common bugs arising in both streaming and parallel sequence processing, and can be implemented in any deep learning library. A composable and declarative API, along with a comprehensive suite of layers and combinators, streamlines the construction of production-scale models from simple streamable components while preserving strong correctness guarantees. Our current implementations of SequenceLayers (JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSBC : Data Science task Benchmarking with Context engineering</title>
<link>https://arxiv.org/abs/2507.23336</link>
<guid>https://arxiv.org/abs/2507.23336</guid>
<content:encoded><![CDATA[
arXiv:2507.23336v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have significantly impacted data science workflows, giving rise to specialized data science agents designed to automate analytical tasks. Despite rapid adoption, systematic benchmarks evaluating the efficacy and limitations of these agents remain scarce. In this paper, we introduce a comprehensive benchmark specifically crafted to reflect real-world user interactions with data science agents by observing usage of our commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet, Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with context engineering, multi-step with context engineering, and with SmolAgent. Our benchmark assesses performance across a diverse set of eight data science task categories, additionally exploring the sensitivity of models to common prompting issues, such as data leakage and slightly ambiguous instructions. We further investigate the influence of temperature parameters on overall and task-specific outcomes for each model and approach. Our findings reveal distinct performance disparities among the evaluated models and methodologies, highlighting critical factors that affect practical deployment. The benchmark dataset and evaluation framework introduced herein aim to provide a foundation for future research of more robust and effective data science agents.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution</title>
<link>https://arxiv.org/abs/2507.23348</link>
<guid>https://arxiv.org/abs/2507.23348</guid>
<content:encoded><![CDATA[
arXiv:2507.23348v1 Announce Type: cross 
Abstract: Issue resolution has made remarkable progress thanks to the advanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agent-based issue resolution approaches are primarily based on agents' independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, a competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as localization proposals by traversing a code dependency graph. Then, it organizes a three-round debate among specialized agents, each embodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on a consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-Exp: Experience-Driven Software Issue Resolution</title>
<link>https://arxiv.org/abs/2507.23361</link>
<guid>https://arxiv.org/abs/2507.23361</guid>
<content:encoded><![CDATA[
arXiv:2507.23361v1 Announce Type: cross 
Abstract: Recent advances in large language model (LLM) agents have shown remarkable progress in software issue resolution, leveraging advanced techniques such as multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current agents act as memoryless explorers - treating each problem separately without retaining or reusing knowledge from previous repair experiences. This leads to redundant exploration of failed trajectories and missed chances to adapt successful issue resolution methods to similar problems. To address this problem, we introduce SWE-Exp, an experience - enhanced approach that distills concise and actionable experience from prior agent trajectories, enabling continuous learning across issues. Our method introduces a multi-faceted experience bank that captures both successful and failed repair attempts. Specifically, it extracts reusable issue resolution knowledge at different levels - from high-level problem comprehension to specific code changes. Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6% Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach establishes a new paradigm in which automated software engineering agents systematically accumulate and leverage repair expertise, fundamentally shifting from trial-and-error exploration to strategic, experience-driven issue resolution.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Holistic Evaluations of Topic Models</title>
<link>https://arxiv.org/abs/2507.23364</link>
<guid>https://arxiv.org/abs/2507.23364</guid>
<content:encoded><![CDATA[
arXiv:2507.23364v1 Announce Type: cross 
Abstract: Topic models are gaining increasing commercial and academic interest for their ability to summarize large volumes of unstructured text. As unsupervised machine learning methods, they enable researchers to explore data and help general users understand key themes in large text collections. However, they risk becoming a 'black box', where users input data and accept the output as an accurate summary without scrutiny. This article evaluates topic models from a database perspective, drawing insights from 1140 BERTopic model runs. The goal is to identify trade-offs in optimizing model parameters and to reflect on what these findings mean for the interpretation and responsible use of topic models
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems</title>
<link>https://arxiv.org/abs/2507.23453</link>
<guid>https://arxiv.org/abs/2507.23453</guid>
<content:encoded><![CDATA[
arXiv:2507.23453v1 Announce Type: cross 
Abstract: This paper investigates defenses for LLM-based evaluation systems against prompt injection. We formalize a class of threats called blind attacks, where a candidate answer is crafted independently of the true answer to deceive the evaluator. To counter such attacks, we propose a framework that augments Standard Evaluation (SE) with Counterfactual Evaluation (CFE), which re-evaluates the submission against a deliberately false ground-truth answer. An attack is detected if the system validates an answer under both standard and counterfactual conditions. Experiments show that while standard evaluation is highly vulnerable, our SE+CFE framework significantly improves security by boosting attack detection with minimal performance trade-offs.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio Understanding Tasks</title>
<link>https://arxiv.org/abs/2507.23511</link>
<guid>https://arxiv.org/abs/2507.23511</guid>
<content:encoded><![CDATA[
arXiv:2507.23511v1 Announce Type: cross 
Abstract: While large audio-language models have advanced open-ended audio understanding, they still fall short of nuanced human-level comprehension. This gap persists largely because current benchmarks, limited by data annotations and evaluation metrics, fail to reliably distinguish between generic and highly detailed model outputs. To this end, this work introduces MECAT, a Multi-Expert Constructed Benchmark for Fine-Grained Audio Understanding Tasks. Generated via a pipeline that integrates analysis from specialized expert models with Chain-of-Thought large language model reasoning, MECAT provides multi-perspective, fine-grained captions and open-set question-answering pairs. The benchmark is complemented by a novel metric: DATE (Discriminative-Enhanced Audio Text Evaluation). This metric penalizes generic terms and rewards detailed descriptions by combining single-sample semantic similarity with cross-sample discriminability. A comprehensive evaluation of state-of-the-art audio models is also presented, providing new insights into their current capabilities and limitations. The data and code are available at https://github.com/xiaomi-research/mecat
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates</title>
<link>https://arxiv.org/abs/2507.23607</link>
<guid>https://arxiv.org/abs/2507.23607</guid>
<content:encoded><![CDATA[
arXiv:2507.23607v1 Announce Type: cross 
Abstract: Clinical trials are a systematic endeavor to assess the safety and efficacy of new drugs or treatments. Conducting such trials typically demands significant financial investment and meticulous planning, highlighting the need for accurate predictions of trial outcomes. Accurately predicting patient enrollment, a key factor in trial success, is one of the primary challenges during the planning phase. In this work, we propose a novel deep learning-based method to address this critical challenge. Our method, implemented as a neural network model, leverages pre-trained language models (PLMs) to capture the complexities and nuances of clinical documents, transforming them into expressive representations. These representations are then combined with encoded tabular features via an attention mechanism. To account for uncertainties in enrollment prediction, we enhance the model with a probabilistic layer based on the Gamma distribution, which enables range estimation. We apply the proposed model to predict clinical trial duration, assuming site-level enrollment follows a Poisson-Gamma process. We carry out extensive experiments on real-world clinical trial data, and show that the proposed method can effectively predict the number of patients enrolled at a number of sites for a given clinical trial, outperforming established baseline models.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached Responses</title>
<link>https://arxiv.org/abs/2507.23674</link>
<guid>https://arxiv.org/abs/2507.23674</guid>
<content:encoded><![CDATA[
arXiv:2507.23674v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) process millions of queries daily, making efficient response caching a compelling optimization for reducing cost and latency. However, preserving relevance to user queries using this approach proves difficult due to the personalized nature of chatbot interactions and the limited accuracy of semantic similarity search. To address this, we present TweakLLM, a novel routing architecture that employs a lightweight LLM to dynamically adapt cached responses to incoming prompts. Through comprehensive evaluation, including user studies with side-by-side comparisons, satisfaction voting, as well as multi-agent LLM debates, we demonstrate that TweakLLM maintains response quality comparable to frontier models while significantly improving cache effectiveness. Our results across real-world datasets highlight TweakLLM as a scalable, resource-efficient caching solution for high-volume LLM deployments without compromising user experience.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextQuests: How Good are LLMs at Text-Based Video Games?</title>
<link>https://arxiv.org/abs/2507.23701</link>
<guid>https://arxiv.org/abs/2507.23701</guid>
<content:encoded><![CDATA[
arXiv:2507.23701v1 Announce Type: cross 
Abstract: Evaluating AI agents within complex, interactive environments that mirror real-world challenges is critical for understanding their practical capabilities. While existing agent benchmarks effectively assess skills like tool use or performance on structured tasks, they often do not fully capture an agent's ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context. To spur the development of agents capable of more robust intrinsic reasoning over long horizons, we introduce TextQuests, a benchmark based on the Infocom suite of interactive fiction games. These text-based adventures, which can take human players over 30 hours and require hundreds of precise actions to solve, serve as an effective proxy for evaluating AI agents on focused, stateful tasks. The benchmark is specifically designed to assess an LLM agent's capacity for self-contained problem-solving by precluding the use of external tools, thereby focusing on intrinsic long-context reasoning capabilities in an exploratory environment characterized by the need for trial-and-error learning and sustained problem-solving within a single interactive session. We release TextQuests at https://textquests.ai.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving</title>
<link>https://arxiv.org/abs/2507.23726</link>
<guid>https://arxiv.org/abs/2507.23726</guid>
<content:encoded><![CDATA[
arXiv:2507.23726v1 Announce Type: cross 
Abstract: LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose \textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves $78.1\%$ of formalized past IMO problems, saturates MiniF2F, and achieves over 50\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine \textbf{Seed-Geometry}, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks</title>
<link>https://arxiv.org/abs/2507.23751</link>
<guid>https://arxiv.org/abs/2507.23751</guid>
<content:encoded><![CDATA[
arXiv:2507.23751v1 Announce Type: cross 
Abstract: We propose CoT-Self-Instruct, a synthetic data generation method that instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the given seed tasks, and then to generate a new synthetic prompt of similar quality and complexity for use in LLM training, followed by filtering for high-quality data with automatic metrics. In verifiable reasoning, our synthetic data significantly outperforms existing training datasets, such as s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For non-verifiable instruction-following tasks, our method surpasses the performance of human or standard self-instruct prompts on both AlpacaEval 2.0 and Arena-Hard.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model</title>
<link>https://arxiv.org/abs/2507.23773</link>
<guid>https://arxiv.org/abs/2507.23773</guid>
<content:encoded><![CDATA[
arXiv:2507.23773v1 Announce Type: cross 
Abstract: AI agents built on large language models (LLMs) hold enormous promise, but current practice focuses on a one-task-one-agent approach, which not only falls short of scalability and generality, but also suffers from the fundamental limitations of autoregressive LLMs. On the other hand, humans are general agents who reason by mentally simulating the outcomes of their actions and plans. Moving towards a more general and powerful AI agent, we introduce SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based on a principled formulation of optimal agent in any environment, \modelname overcomes the limitations of autoregressive reasoning by introducing a world model for planning via simulation. The generalized world model is implemented using LLM, which can flexibly plan in a wide range of environments using the concept-rich latent space of natural language. Experiments on difficult web browsing tasks show that \modelname improves the success of flight search from 0\% to 32.2\%. World-model-based planning, in particular, shows consistent advantage of up to 124\% over autoregressive planning, demonstrating the advantage of world model simulation as a reasoning paradigm. We are excited about the possibility for training a single, general agent model based on LLMs that can act superintelligently in all environments. To start, we make SimuRA, a web-browsing agent built on \modelname with pretrained LLMs, available as a research demo for public testing.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiMe: a Latin Corpus of Late Medieval Criminal Sentences</title>
<link>https://arxiv.org/abs/2404.12829</link>
<guid>https://arxiv.org/abs/2404.12829</guid>
<content:encoded><![CDATA[
arXiv:2404.12829v2 Announce Type: replace 
Abstract: The Latin language has received attention from the computational linguistics research community, which has built, over the years, several valuable resources, ranging from detailed annotated corpora to sophisticated tools for linguistic analysis. With the recent advent of large language models, researchers have also started developing models capable of generating vector representations of Latin texts. The performances of such models remain behind the ones for modern languages, given the disparity in available data. In this paper, we present the LiMe dataset, a corpus of 325 documents extracted from a series of medieval manuscripts called Libri sententiarum potestatis Mediolani, and thoroughly annotated by experts, in order to be employed for masked language model, as well as supervised natural language processing tasks.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining vague language</title>
<link>https://arxiv.org/abs/2404.18154</link>
<guid>https://arxiv.org/abs/2404.18154</guid>
<content:encoded><![CDATA[
arXiv:2404.18154v2 Announce Type: replace 
Abstract: Why is language vague? Vagueness may be explained and rationalized if it can be shown that vague language is more useful to speaker and hearer than precise language. In a well-known paper, Lipman proposes a game-theoretic account of vagueness in terms of mixed strategy that leads to a puzzle: vagueness cannot be strictly better than precision at equilibrium. More recently, \'Egr\'e, Spector, Mortier and Verheyen have put forward a Bayesian account of vagueness establishing that using vague words can be strictly more informative than using precise words. This paper proposes to compare both results and to explain why they are not in contradiction. Lipman's definition of vagueness relies exclusively on a property of signaling strategies, without making any assumptions about the lexicon, whereas \'Egr\'e et al.'s involves a layer of semantic content. We argue that the semantic account of vagueness is needed, and more adequate and explanatory of vagueness.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with Unanswerability</title>
<link>https://arxiv.org/abs/2406.14313</link>
<guid>https://arxiv.org/abs/2406.14313</guid>
<content:encoded><![CDATA[
arXiv:2406.14313v3 Announce Type: replace 
Abstract: Real-world applications of KBQA require models to handle unanswerable questions with a limited volume of in-domain labeled training data. We propose the novel task of few-shot transfer for KBQA with unanswerable questions and contribute two new datasets for performance evaluation. We present FUn-FuSIC - a novel solution for our task that extends FuSIC KBQA, the state-of-the-art few-shot transfer model for answerable-only KBQA. We first note that FuSIC-KBQA's iterative repair makes a strong assumption that all questions are unanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which uses iterative repair using feedback from a suite of strong and weak verifiers, and an adaptation of self consistency for unanswerabilty to better assess the answerability of a question. Our experiments show that FUn-FuSIC significantly outperforms suitable adaptations of multiple LLM based and supervised SoTA models on our task, while establishing a new SoTA for answerable few-shot transfer as well.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cutting Through the Noise: Boosting LLM Performance on Math Word Problems</title>
<link>https://arxiv.org/abs/2406.15444</link>
<guid>https://arxiv.org/abs/2406.15444</guid>
<content:encoded><![CDATA[
arXiv:2406.15444v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel at various tasks, including solving math word problems (MWPs), but struggle with real-world problems containing irrelevant information. To address this, we propose a prompting framework that generates adversarial variants of MWPs by adding irrelevant variables. We introduce a dataset, PROBLEMATHIC, containing both adversarial and non-adversarial MWPs. Our experiments reveal that LLMs are susceptible to distraction by numerical noise, resulting in an average relative performance drop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2, Mistral) on the adversarial samples from our dataset. Fine-tuning on adversarial training instances improves performance on adversarial MWPs by ~8%, indicating increased robustness to noise and improved ability to identify relevant data for reasoning. Finally, to assess the generalizability of our prompting framework, we introduce GSM-8K-Adv, an adversarial variant of the GSM-8K benchmark. LLMs continue to struggle when faced with adversarial information, reducing performance by up to 6%.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation</title>
<link>https://arxiv.org/abs/2411.18337</link>
<guid>https://arxiv.org/abs/2411.18337</guid>
<content:encoded><![CDATA[
arXiv:2411.18337v4 Announce Type: replace 
Abstract: Ambiguous words are often found in modern digital communications. Lexical ambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due to limited data. Consequently, the efficiency of translation, information retrieval, and question-answering systems is hindered by these limitations. This study investigates the use of Large Language Models (LLMs) to improve WSD using a novel approach combining a systematic prompt augmentation mechanism with a knowledge base (KB) consisting of different sense interpretations. The proposed method incorporates a human-in-loop approach for prompt augmentation where prompt is supported by Part-of-Speech (POS) tagging, synonyms of ambiguous words, aspect-based sense filtering and few-shot prompting to guide the LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based approach, this work demonstrates a substantial improvement in performance. The evaluation was conducted using FEWS test data and sense tags. This research advances accurate word interpretation in social media and digital communication.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette</title>
<link>https://arxiv.org/abs/2412.11167</link>
<guid>https://arxiv.org/abs/2412.11167</guid>
<content:encoded><![CDATA[
arXiv:2412.11167v3 Announce Type: replace 
Abstract: Large language models (LLMs) face challenges in aligning with diverse cultural values despite their remarkable performance in generation, which stems from inherent monocultural biases and difficulties in capturing nuanced cultural semantics. Existing methods struggle to adapt to unknown culture after fine-tuning. Inspired by cultural geography across five continents, we propose Cultural Palette, a multi-agent framework that redefines cultural alignment as an adaptive "color-blending" process for country-specific adaptation. Our approach harnesses cultural geography across five continents (Africa, America, Asia, Europe, Oceania) through three key steps: First, we synthesize the Pentachromatic Cultural Palette Dataset using GPT-4o, refining continental-level dialogues with Hofstede's cultural dimensions to establish foundational cultural representations. Second, five continent-level alignment agents form specialized cultural communities that generate region-specific draft responses. Third, a Meta Agent employs Cultural MoErges to dynamically blend these cultural "colors" through attention-gated parameter merging, akin to mixing pigments on a palette, resolving conflicts while preserving cultural nuances to produce the final culturally-aligned response. Extensive experiments across various countries demonstrate that Cultural Palette surpasses existing baselines in cultural alignment.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inside-Out: Hidden Factual Knowledge in LLMs</title>
<link>https://arxiv.org/abs/2503.15299</link>
<guid>https://arxiv.org/abs/2503.15299</guid>
<content:encoded><![CDATA[
arXiv:2503.15299v3 Announce Type: replace 
Abstract: This work presents a framework for assessing whether large language models (LLMs) encode more factual knowledge in their parameters than what they express in their outputs. While a few studies hint at this possibility, none has clearly defined or demonstrated this phenomenon. We first propose a formal definition of knowledge, quantifying it for a given question as the fraction of correct-incorrect answer pairs where the correct one is ranked higher. This gives rise to external and internal knowledge, depending on the information used to score individual answer candidates: either the model's observable token-level probabilities or its intermediate computations. Hidden knowledge arises when internal knowledge exceeds external knowledge. We then present a case study, applying this framework to three popular open-weights LLMs in a closed-book QA setup. Our results indicate that: (1) LLMs consistently encode more factual knowledge internally than what they express externally, with an average relative gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a model can internally know an answer perfectly, yet fail to generate it even once, despite large-scale repeated sampling of 1,000 answers. This reveals fundamental limitations in the generation capabilities of LLMs, which (3) put a practical constraint on scaling test-time compute via repeated answer sampling in closed-book QA: significant performance improvements remain inaccessible because some answers are practically never sampled, yet if they were, we would be guaranteed to rank them first.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can one size fit all?: Measuring Failure in Multi-Document Summarization Domain Transfer</title>
<link>https://arxiv.org/abs/2503.15768</link>
<guid>https://arxiv.org/abs/2503.15768</guid>
<content:encoded><![CDATA[
arXiv:2503.15768v2 Announce Type: replace 
Abstract: Abstractive multi-document summarization (MDS) is the task of automatically summarizing information in multiple documents, from news articles to conversations with multiple speakers. The training approaches for current MDS models can be grouped into four approaches: end-to-end with special pre-training ("direct"), chunk-then-summarize, extract-then-summarize, and inference with GPT-style models. In this work, we evaluate MDS models across training approaches, domains, and dimensions (reference similarity, quality, and factuality), to analyze how and why models trained on one domain can fail to summarize documents from another (News, Science, and Conversation) in the zero-shot domain transfer setting. We define domain-transfer "failure" as a decrease in factuality, higher deviation from the target, and a general decrease in summary quality. In addition to exploring domain transfer for MDS models, we examine potential issues with applying popular summarization metrics out-of-the-box.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Splits! A Flexible Dataset and Evaluation Framework for Sociocultural Linguistic Investigation</title>
<link>https://arxiv.org/abs/2504.04640</link>
<guid>https://arxiv.org/abs/2504.04640</guid>
<content:encoded><![CDATA[
arXiv:2504.04640v2 Announce Type: replace 
Abstract: Variation in language use, shaped by speakers' sociocultural background and specific context of use, offers a rich lens into cultural perspectives, values, and opinions. However, the computational study of these Sociocultural Linguistic Phenomena (SLP) has often been limited to bespoke analyses of specific groups or topics, hindering the pace of scientific discovery. To address this, we introduce Splits!, a 9.7 million-post dataset from Reddit designed for systematic and flexible research. The dataset contains posts from over 53,000 users across 6 demographic groups, organized into 89 discussion topics to enable comparative analysis. We validate Splits! via self-identification and by successfully replicating several known SLPs from existing literature. We complement this dataset with a framework that leverages efficient retrieval methods to rapidly validate potential SLPs (PSLPs) by automatically evaluating whether a given hypothesis is supported by our data. Crucially, to distinguish between novel and obvious insights, the framework incorporates a human-validated measure of a hypothesis's ``unexpectedness.'' We demonstrate that the two-stage process reduces the number of statistically significant findings requiring manual inspection by a factor of 1.5-1.8x, streamlining the discovery of promising phenomena for further investigation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance</title>
<link>https://arxiv.org/abs/2504.09753</link>
<guid>https://arxiv.org/abs/2504.09753</guid>
<content:encoded><![CDATA[
arXiv:2504.09753v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities, but their development has primarily focused on English and other high-resource languages, leaving many languages underserved. We present our latest Hindi-English bi-lingual LLM \textbf{Mantra-14B} with ~3\% average improvement in benchmark scores over both languages, outperforming models twice its size. Using a curated dataset composed of English and Hindi instruction data of 485K samples, we instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve performance over both English and Hindi. Our experiments encompassing seven different LLMs of varying parameter sizes and over 140 training attempts with varying English-Hindi training data ratios demonstrated that it is possible to significantly improve multilingual performance without compromising native performance. Further, our approach avoids resource-intensive techniques like vocabulary expansion or architectural modifications, thus keeping the model size small. Our results indicate that modest fine-tuning with culturally and locally informed data can bridge performance gaps without incurring significant computational overhead. We release our training code, datasets, and models under mit and apache licenses to aid further research towards under-represented and low-resource languages.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Fine-Grained Detection of AI Generated Texts</title>
<link>https://arxiv.org/abs/2504.11952</link>
<guid>https://arxiv.org/abs/2504.11952</guid>
<content:encoded><![CDATA[
arXiv:2504.11952v3 Announce Type: replace 
Abstract: An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation</title>
<link>https://arxiv.org/abs/2504.16060</link>
<guid>https://arxiv.org/abs/2504.16060</guid>
<content:encoded><![CDATA[
arXiv:2504.16060v3 Announce Type: replace 
Abstract: Referring Expression Generation (REG) is a core task for evaluating the pragmatic competence of vision-language systems, requiring not only accurate semantic grounding but also adherence to principles of cooperative communication (Grice, 1975). However, current evaluations of vision-language models (VLMs) often overlook the pragmatic dimension, reducing REG to a region-based captioning task and neglecting Gricean maxims. In this work, we revisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of 1.5k images annotated with both written and spoken referring expressions. Through a systematic evaluation of state-of-the-art VLMs, we identify three key failures of pragmatic competence: (1) failure to uniquely identify the referent, (2) inclusion of excessive or irrelevant information, and (3) misalignment with human pragmatic preference, such as the underuse of minimal spatial cues. We also show that standard automatic evaluations fail to capture these pragmatic violations, reinforcing superficial cues rather than genuine referential success. Our findings call for a renewed focus on pragmatically informed models and evaluation frameworks that align with real human communication.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLMs to Create Content Corpora for Niche Domains</title>
<link>https://arxiv.org/abs/2505.02851</link>
<guid>https://arxiv.org/abs/2505.02851</guid>
<content:encoded><![CDATA[
arXiv:2505.02851v2 Announce Type: replace 
Abstract: Constructing specialized content corpora from vast, unstructured web sources for domain-specific applications poses substantial data curation challenges. In this paper, we introduce a streamlined approach for generating high-quality, domain-specific corpora by efficiently acquiring, filtering, structuring, and cleaning web-based data. We showcase how Large Language Models (LLMs) can be leveraged to address complex data curation at scale, and propose a strategical framework incorporating LLM-enhanced techniques for structured content extraction and semantic deduplication. We validate our approach in the behavior education domain through its integration into 30 Day Me, a habit formation application. Our data pipeline, named 30DayGen, enabled the extraction and synthesis of 3,531 unique 30-day challenges from over 15K webpages. A user survey reports a satisfaction score of 4.3 out of 5, with 91% of respondents indicating willingness to use the curated content for their habit-formation goals.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages</title>
<link>https://arxiv.org/abs/2505.14874</link>
<guid>https://arxiv.org/abs/2505.14874</guid>
<content:encoded><![CDATA[
arXiv:2505.14874v4 Announce Type: replace 
Abstract: Automatic speech recognition (ASR) for dysarthric speech remains challenging due to data scarcity, particularly in non-English languages. To address this, we fine-tune a voice conversion model on English dysarthric speech (UASpeech) to encode both speaker characteristics and prosodic distortions, then apply it to convert healthy non-English speech (FLEURS) into non-English dysarthric-like speech. The generated data is then used to fine-tune a multilingual ASR model, Massively Multilingual Speech (MMS), for improved dysarthric speech recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE (Tamil) demonstrates that VC with both speaker and prosody conversion significantly outperforms the off-the-shelf MMS performance and conventional augmentation techniques such as speed and tempo perturbation. Objective and subjective analyses of the generated data further confirm that the generated speech simulates dysarthric characteristics.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models</title>
<link>https://arxiv.org/abs/2505.18497</link>
<guid>https://arxiv.org/abs/2505.18497</guid>
<content:encoded><![CDATA[
arXiv:2505.18497v2 Announce Type: replace 
Abstract: Current large language models (LLMs) have demonstrated emerging capabilities in social intelligence tasks, including implicature resolution and theory-of-mind reasoning, both of which require substantial pragmatic understanding. However, how LLMs acquire this pragmatic competence throughout the training process remains poorly understood. In this work, we introduce ALTPRAG, a dataset grounded in the pragmatic concept of alternatives, to evaluate whether LLMs at different training stages can accurately infer nuanced speaker intentions. Each instance pairs two equally plausible yet pragmatically divergent continuations and requires the model to (i) infer the speaker's intended meaning and (ii) explain when and why a speaker would choose one utterance over its alternative, thus directly probing pragmatic competence through contrastive reasoning. We systematically evaluate 22 LLMs across 3 key training stages: after pre-training, supervised fine-tuning (SFT), and preference optimization, to examine the development of pragmatic competence. Our results show that even base models exhibit notable sensitivity to pragmatic cues, which improves consistently with increases in model and data scale. Additionally, SFT and RLHF contribute further gains, particularly in cognitive-pragmatic scenarios. These findings highlight pragmatic competence as an emergent and compositional property of LLM training and offer new insights for aligning models with human communicative norms.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora</title>
<link>https://arxiv.org/abs/2505.23628</link>
<guid>https://arxiv.org/abs/2505.23628</guid>
<content:encoded><![CDATA[
arXiv:2505.23628v2 Announce Type: replace 
Abstract: We present AutoSchemaKG, a framework for fully autonomous knowledge graph construction that eliminates the need for predefined schemas. Our system leverages large language models to simultaneously extract knowledge triples and induce comprehensive schemas directly from text, modeling both entities and events while employing conceptualization to organize instances into semantic categories. Processing over 50 million documents, we construct ATLAS (Automated Triple Linking And Schema induction), a family of knowledge graphs with 900+ million nodes and 5.9 billion edges. This approach outperforms state-of-the-art baselines on multi-hop QA tasks and enhances LLM factuality. Notably, our schema induction achieves 95\% semantic alignment with human-crafted schemas with zero manual intervention, demonstrating that billion-scale knowledge graphs with dynamically induced schemas can effectively complement parametric knowledge in large language models.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Framing Political Bias in Multilingual LLMs Across Pakistani Languages</title>
<link>https://arxiv.org/abs/2506.00068</link>
<guid>https://arxiv.org/abs/2506.00068</guid>
<content:encoded><![CDATA[
arXiv:2506.00068v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) increasingly shape public discourse, yet most evaluations of political and economic bias have focused on high-resource, Western languages and contexts. This leaves critical blind spots in low-resource, multilingual regions such as Pakistan, where linguistic identity is closely tied to political, religious, and regional ideologies. We present a systematic evaluation of political bias in 13 state-of-the-art LLMs across five Pakistani languages: Urdu, Punjabi, Sindhi, Pashto, and Balochi. Our framework integrates a culturally adapted Political Compass Test (PCT) with multi-level framing analysis, capturing both ideological stance (economic/social axes) and stylistic framing (content, tone, emphasis). Prompts are aligned with 11 socio-political themes specific to the Pakistani context. Results show that while LLMs predominantly reflect liberal-left orientations consistent with Western training data, they exhibit more authoritarian framing in regional languages, highlighting language-conditioned ideological modulation. We also identify consistent model-specific bias patterns across languages. These findings show the need for culturally grounded, multilingual bias auditing frameworks in global NLP.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2506.07106</link>
<guid>https://arxiv.org/abs/2506.07106</guid>
<content:encoded><![CDATA[
arXiv:2506.07106v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown strong performance across natural language reasoning tasks, yet their reasoning processes remain brittle and difficult to interpret. Prompting techniques like Chain-of-Thought (CoT) enhance reliability by eliciting intermediate reasoning steps or aggregating multiple outputs. However, they lack mechanisms for enforcing logical structure and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a novel framework that models reasoning as collaboration among three parallel agents, each simulating a distinct mode of inference: abductive, deductive, and inductive. Each agent produces a reasoning trace, which is structured into a formal reasoning graph. To evaluate consistency, we apply Bayesian belief propagation guided by natural language inference (NLI), assigning confidence scores to each step. The most coherent graph is selected to derive the final answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith) reasoning benchmarks show that ToTh consistently outperforms CoT, Self-Consistency, and CoT-Decoding across multiple LLMs, while producing interpretable and logically grounded reasoning chains. Our findings suggest a promising direction for building more robust and cognitively inspired LLM reasoning. The implementation is available at https://github.com/KurbanIntelligenceLab/theorem-of-thought.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unable to Forget: Proactive Interference Reveals Working Memory Limits in LLMs Beyond Context Length</title>
<link>https://arxiv.org/abs/2506.08184</link>
<guid>https://arxiv.org/abs/2506.08184</guid>
<content:encoded><![CDATA[
arXiv:2506.08184v3 Announce Type: replace 
Abstract: Information retrieval in Large Language Models (LLMs) is increasingly recognized as intertwined with generation capabilities rather than mere lookup. While longer contexts are often assumed to improve retrieval, the effects of intra-context interference remain understudied. To address this, we adapt the proactive interference (PI) paradigm from cognitive science, where earlier information disrupts recall of newer updates. In humans, susceptibility to such interference is inversely linked to working memory capacity. We introduce PI-LLM, an evaluation that sequentially streams semantically related key-value updates and queries only the final values. Although these final values are clearly positioned just before the query, LLM retrieval accuracy declines log-linearly toward zero as interference accumulates; errors arise from retrieving previously overwritten values. Attempts to mitigate interference via prompt engineering (e.g., instructing models to ignore earlier input) yield limited success. These findings reveal a fundamental constraint on LLMs' ability to disentangle interference and flexibly manipulate information, suggesting a working memory bottleneck beyond mere context access. This calls for approaches that strengthen models' ability to suppress irrelevant content during retrieval.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics</title>
<link>https://arxiv.org/abs/2506.12365</link>
<guid>https://arxiv.org/abs/2506.12365</guid>
<content:encoded><![CDATA[
arXiv:2506.12365v2 Announce Type: replace 
Abstract: This survey paper outlines the key developments in the field of Large Language Models (LLMs), including enhancements to their reasoning skills, adaptability to various tasks, increased computational efficiency, and the ability to make ethical decisions. The techniques that have been most effective in bridging the gap between human and machine communications include the Chain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from Human Feedback. The improvements in multimodal learning and few-shot or zero-shot techniques have further empowered LLMs to handle complex jobs with minor input. A significant focus is placed on efficiency, detailing scaling strategies, optimization techniques, and the influential Mixture-of-Experts (MoE) architecture, which strategically routes inputs to specialized subnetworks to boost predictive accuracy, while optimizing resource allocation. This survey also offers a broader perspective on recent advancements in LLMs, going beyond isolated aspects such as model architecture or ethical concerns. Additionally, it explores the role of LLMs in Agentic AI and their use as Autonomous Decision-Making Systems, and categorizes emerging methods that enhance LLM reasoning, efficiency, and ethical alignment. The survey also identifies underexplored areas such as interpretability, cross-modal integration, and sustainability. While significant advancements have been made in LLMs, challenges such as high computational costs, biases, and ethical risks remain. Overcoming these requires a focus on bias mitigation, transparent decision-making, and explicit ethical guidelines. Future research will generally focus on enhancing the model's ability to handle multiple inputs, thereby making it more intelligent, safe, and reliable.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review</title>
<link>https://arxiv.org/abs/2506.18199</link>
<guid>https://arxiv.org/abs/2506.18199</guid>
<content:encoded><![CDATA[
arXiv:2506.18199v2 Announce Type: replace 
Abstract: Large language models have demonstrated remarkable capabilities across various domains, yet concerns about cultural bias - particularly towards Arabs and Muslims - pose significant ethical challenges by perpetuating harmful stereotypes and marginalization. Despite growing recognition of bias in LLMs, prompt engineering strategies specifically addressing Arab and Muslim representation remain understudied. This mixed-methods systematic review examines such techniques, offering evidence-based guidance for researchers and practitioners. Following PRISMA guidelines and Kitchenham's systematic review methodology, we analyzed 8 empirical studies published between 2021-2024 investigating bias mitigation strategies. Our findings reveal five primary prompt engineering approaches: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts. Although all approaches show potential for reducing bias, effectiveness varied substantially across studies and bias types. Evidence suggests that certain bias types may be more resistant to prompt-based mitigation than others. Structured multi-step pipelines demonstrated the highest overall effectiveness, achieving up to 87.7% reduction in bias, though they require greater technical expertise. Cultural prompting offers broader accessibility with substantial effectiveness. These results underscore the accessibility of prompt engineering for mitigating cultural bias without requiring access to model parameters. The limited number of studies identified highlights a significant research gap in this critical area. Future research should focus on developing culturally adaptive prompting techniques, creating Arab and Muslim-specific evaluation resources, and integrating prompt engineering with complementary debiasing methods to address deeper stereotypes while maintaining model utility.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation</title>
<link>https://arxiv.org/abs/2506.21875</link>
<guid>https://arxiv.org/abs/2506.21875</guid>
<content:encoded><![CDATA[
arXiv:2506.21875v2 Announce Type: replace 
Abstract: Recent multi-modal Large Language Models (LLMs) such as GPT-4o have demonstrated strong capabilities of direct speech interaction. However, the lack of specialized and comprehensive benchmarks for end-to-end speech LLM evaluation hinders optimizing the user experience of Audio LLMs in real-world applications. Existing evaluation methods often adapt text-based benchmarks, overlooking speech's unique characteristics and challenges, including prosody, homophones, stuttering, and differing user expectations. Here, we present a novel approach to thoroughly evaluate LLMs in practical speech conversations. We systematically curate real-world chat data relevant to spoken scenarios, introduce diversity in speaker attributes and acoustic conditions, and augment the dataset with speech-specific phenomena. We further design a query-aware evaluation method to use customized evaluation checklists and prompts to enhance the accuracy of automatic evaluation. We conduct comprehensive testing and detailed analysis of various mainstream speech models, revealing significant differences in model performance across different speech scenarios. The use of query-aware evaluation further enables a finer-grained assessment under various speech-specific scenarios. Our benchmark can provide valuable insights for speech model development and evaluation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception-Aware Policy Optimization for Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2507.06448</link>
<guid>https://arxiv.org/abs/2507.06448</guid>
<content:encoded><![CDATA[
arXiv:2507.06448v3 Announce Type: replace 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a highly effective strategy for endowing Large Language Models (LLMs) with robust multi-step reasoning abilities. However, its design and optimizations remain tailored to purely textual domains, resulting in suboptimal performance when applied to multimodal reasoning tasks. In particular, we observe that a major source of error in current multimodal reasoning lies in the perception of visual inputs. To address this bottleneck, we propose PAPO, a novel policy gradient algorithm that encourages the model to learn to perceive while learning to reason. Specifically, we introduce the Implicit Perception Loss in the form of a KL divergence term, which can be seamlessly plugged into mainstream RLVR algorithms such as GRPO and DAPO. Notably, PAPO does not rely on additional data curation, reward models, or stronger teacher models. To further enhance the training stability of PAPO, we introduce the Double Entropy Loss, which effectively regularizes the new KL objective without compromising performance. Despite its simplicity, PAPO yields significant overall improvements of 4.4%-17.5% on diverse multimodal benchmarks. The improvements are more pronounced, approaching 8.0%-19.1%, on tasks with high vision dependency. We also observe a substantial reduction of 30.5% in perception errors, indicating improved perceptual capabilities with PAPO. Overall, our work introduces a deeper integration of perception-aware supervision into core learning objectives and lays the groundwork for a new RL framework that encourages visually grounded reasoning. Code and data will be made publicly available for research purposes. Project page: https://mikewangwzhl.github.io/PAPO.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities</title>
<link>https://arxiv.org/abs/2507.07695</link>
<guid>https://arxiv.org/abs/2507.07695</guid>
<content:encoded><![CDATA[
arXiv:2507.07695v2 Announce Type: replace 
Abstract: Fine-tuning is an immensely resource-intensive process when retraining Large Language Models (LLMs) to incorporate a larger body of knowledge. Although many fine-tuning techniques have been developed to reduce the time and computational cost involved, the challenge persists as LLMs continue to grow in size and complexity. To address this, a new approach to knowledge expansion in LLMs is needed. Retrieval-Augmented Generation (RAG) offers one such alternative by storing external knowledge in a database and retrieving relevant chunks to support question answering. However, naive implementations of RAG face significant limitations in scalability and answer accuracy. This paper introduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome these limitations. Inspired by the divide-and-conquer paradigm, K2RAG integrates dense and sparse vector search, knowledge graphs, and text summarization to improve retrieval quality and system efficiency. The framework also includes a preprocessing step that summarizes the training data, significantly reducing the training time. K2RAG was evaluated using the MultiHopRAG dataset, where the proposed pipeline was trained on the document corpus and tested on a separate evaluation set. Results demonstrated notable improvements over common naive RAG implementations. K2RAG achieved the highest mean answer similarity score of 0.57, and reached the highest third quartile (Q3) similarity of 0.82, indicating better alignment with ground-truth answers. In addition to improved accuracy, the framework proved highly efficient. The summarization step reduced the average training time of individual components by 93%, and execution speed was up to 40% faster than traditional knowledge graph-based RAG systems. K2RAG also demonstrated superior scalability, requiring three times less VRAM than several naive RAG implementations tested in this study.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures</title>
<link>https://arxiv.org/abs/2507.08606</link>
<guid>https://arxiv.org/abs/2507.08606</guid>
<content:encoded><![CDATA[
arXiv:2507.08606v3 Announce Type: replace 
Abstract: We introduce DocPolarBERT, a layout-aware BERT model for document understanding that eliminates the need for absolute 2D positional embeddings. We extend self-attention to take into account text block positions in relative polar coordinate system rather than the Cartesian one. Despite being pre-trained on a dataset more than six times smaller than the widely used IIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results demonstrate that a carefully designed attention mechanism can compensate for reduced pre-training data, offering an efficient and effective alternative for document understanding.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires</title>
<link>https://arxiv.org/abs/2507.10073</link>
<guid>https://arxiv.org/abs/2507.10073</guid>
<content:encoded><![CDATA[
arXiv:2507.10073v2 Announce Type: replace 
Abstract: Are AI systems truly representing human values, or merely averaging across them? Our study suggests a concerning reality: Large Language Models (LLMs) fail to represent diverse cultural moral frameworks despite their linguistic capabilities. We expose significant gaps between AI-generated and human moral intuitions by applying the Moral Foundations Questionnaire across 19 cultural contexts. Comparing multiple state-of-the-art LLMs' origins against human baseline data, we find these models systematically homogenize moral diversity. Surprisingly, increased model size doesn't consistently improve cultural representation fidelity. Our findings challenge the growing use of LLMs as synthetic populations in social science research and highlight a fundamental limitation in current AI alignment approaches. Without data-driven alignment beyond prompting, these systems cannot capture the nuanced, culturally-specific moral intuitions. Our results call for more grounded alignment objectives and evaluation metrics to ensure AI systems represent diverse human values rather than flattening the moral landscape.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ILID: Native Script Language Identification for Indian Languages</title>
<link>https://arxiv.org/abs/2507.11832</link>
<guid>https://arxiv.org/abs/2507.11832</guid>
<content:encoded><![CDATA[
arXiv:2507.11832v2 Announce Type: replace 
Abstract: The language identification task is a crucial fundamental step in NLP. Often it serves as a pre-processing step for widely used NLP applications such as multilingual machine translation, information retrieval, question and answering, and text summarization. The core challenge of language identification lies in distinguishing languages in noisy, short, and code-mixed environments. This becomes even harder in case of diverse Indian languages that exhibit lexical and phonetic similarities, but have distinct differences. Many Indian languages share the same script, making the task even more challenging. Taking all these challenges into account, we develop and release a dataset of 250K sentences consisting of 23 languages including English and all 22 official Indian languages labeled with their language identifiers, where data in most languages are newly created. We also develop and release baseline models using state-of-the-art approaches in machine learning and fine-tuning pre-trained transformer models. Our models outperforms the state-of-the-art pre-trained transformer models for the language identification task. The dataset and the codes are available at https://yashingle-ai.github.io/ILID/ and in Huggingface open source libraries.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment</title>
<link>https://arxiv.org/abs/2401.13481</link>
<guid>https://arxiv.org/abs/2401.13481</guid>
<content:encoded><![CDATA[
arXiv:2401.13481v3 Announce Type: replace-cross 
Abstract: Exposure to large language model output is rapidly increasing. How will seeing AI-generated ideas affect human ideas? We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea. We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- speaks to the interdependent process of cultural creation: creative ideas are built upon prior ideas. Hence, we capture the compounding effects of having LLMs 'in the culture loop'. We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity. AI made ideas different, not better. There were no main effects of disclosure. We also found that self-reported creative people were less influenced by knowing an idea was from AI and that participants may knowingly adopt AI ideas when the task is difficult. Our findings suggest that introducing AI ideas may increase collective diversity but not individual creativity.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoOops: A Dataset for Mistake Action Detection from Egocentric Videos referring to Procedural Texts</title>
<link>https://arxiv.org/abs/2410.05343</link>
<guid>https://arxiv.org/abs/2410.05343</guid>
<content:encoded><![CDATA[
arXiv:2410.05343v3 Announce Type: replace-cross 
Abstract: Mistake action detection is crucial for developing intelligent archives that detect workers' errors and provide feedback. Existing studies have focused on visually apparent mistakes in free-style activities, resulting in video-only approaches to mistake detection. However, in text-following activities, models cannot determine the correctness of some actions without referring to the texts. Additionally, current mistake datasets rarely use procedural texts for video recording except for cooking. To fill these gaps, this paper proposes the EgoOops dataset, where egocentric videos record erroneous activities when following procedural texts across diverse domains. It features three types of annotations: video-text alignment, mistake labels, and descriptions for mistakes. We also propose a mistake detection approach, combining video-text alignment and mistake label classification to leverage the texts. Our experimental results show that incorporating procedural texts is essential for mistake detection. Data is available through https://y-haneji.github.io/EgoOops-project-page/.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfAlign: Inference-aware language model alignment</title>
<link>https://arxiv.org/abs/2412.19792</link>
<guid>https://arxiv.org/abs/2412.19792</guid>
<content:encoded><![CDATA[
arXiv:2412.19792v4 Announce Type: replace-cross 
Abstract: Language model alignment is a critical step in training modern generative language models. Alignment targets to improve win rate of a sample from the aligned model against the base model. Today, we are increasingly using inference-time algorithms (e.g., Best-of-N, controlled decoding, tree search) to decode from language models rather than standard sampling. We show that this train/test mismatch makes standard RLHF framework sub-optimal in view of such inference-time methods. To this end, we propose a framework for inference-aware alignment (InfAlign), which aims to optimize inference-time win rate of the aligned policy against the base model. We prove that for any inference-time decoding procedure, the optimal aligned policy is the solution to the standard RLHF problem with a transformation of the reward. This motivates us to provide the calibrate-and-transform RL (InfAlign-CTRL) algorithm to solve this problem, which involves a reward calibration step and a KL-regularized reward maximization step with a transformation of the calibrated reward. For best-of-N sampling and best-of-N jailbreaking, we propose specific transformations offering up to 3-8% improvement on inference-time win rates. Finally, we also show that our proposed reward calibration method is a strong baseline for optimizing standard win rate.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning</title>
<link>https://arxiv.org/abs/2503.15621</link>
<guid>https://arxiv.org/abs/2503.15621</guid>
<content:encoded><![CDATA[
arXiv:2503.15621v2 Announce Type: replace-cross 
Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has highlighted the critical roles of both the visual backbone and the underlying language model. While prior work has primarily focused on scaling these components to billions of parameters, the trade-offs between model size, architecture, and performance remain underexplored. Additionally, inconsistencies in training data and evaluation protocols have hindered direct comparisons, making it difficult to derive optimal design choices. In this paper, we introduce LLaVA-MORE, a new family of MLLMs that integrates recent language models with diverse visual backbones. To ensure fair comparisons, we employ a unified training protocol applied consistently across all architectures. Our analysis systematically explores both small- and medium-scale LLMs -- including Phi-4, LLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and instruction following, while examining the relationship between model size and performance. Beyond evaluating the LLM impact on final results, we conduct a comprehensive study of various visual encoders, ranging from CLIP-based architectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional experiments investigate the effects of increased image resolution and variations in pre-training datasets. Overall, our results provide insights into the design of more effective MLLMs, offering a reproducible evaluation framework that facilitates direct comparisons and can guide future model development. Our source code and trained models are publicly available at: https://github.com/aimagelab/LLaVA-MORE.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents</title>
<link>https://arxiv.org/abs/2503.18666</link>
<guid>https://arxiv.org/abs/2503.18666</guid>
<content:encoded><![CDATA[
arXiv:2503.18666v3 Announce Type: replace-cross 
Abstract: Agents built on LLMs are increasingly deployed across diverse domains, automating complex decision-making and task execution. However, their autonomy introduces safety risks, including security vulnerabilities, legal violations, and unintended harmful actions. Existing mitigation methods, such as model-based safeguards and early enforcement strategies, fall short in robustness, interpretability, and adaptability. To address these challenges, we propose AgentSpec, a lightweight domain-specific language for specifying and enforcing runtime constraints on LLM agents. With AgentSpec, users define structured rules that incorporate triggers, predicates, and enforcement mechanisms, ensuring agents operate within predefined safety boundaries. We implement AgentSpec across multiple domains, including code execution, embodied agents, and autonomous driving, demonstrating its adaptability and effectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe executions in over 90% of code agent cases, eliminates all hazardous actions in embodied agent tasks, and enforces 100% compliance by autonomous vehicles (AVs). Despite its strong safety guarantees, AgentSpec remains computationally lightweight, with overheads in milliseconds. By combining interpretability, modularity, and efficiency, AgentSpec provides a practical and scalable solution for enforcing LLM agent safety across diverse applications. We also automate the generation of rules using LLMs and assess their effectiveness. Our evaluation shows that the rules generated by OpenAI o1 achieve a precision of 95.56% and recall of 70.96% for embodied agents, successfully identify 87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Can I Publish My LLM Benchmark Without Giving the True Answers Away?</title>
<link>https://arxiv.org/abs/2505.18102</link>
<guid>https://arxiv.org/abs/2505.18102</guid>
<content:encoded><![CDATA[
arXiv:2505.18102v2 Announce Type: replace-cross 
Abstract: Publishing a large language model (LLM) benchmark on the Internet risks contaminating future LLMs: the benchmark may be unintentionally (or intentionally) used to train or select a model. A common mitigation is to keep the benchmark private and let participants submit their models or predictions to the organizers. However, this strategy will require trust in a single organization and still permits test-set overfitting through repeated queries. To overcome this issue, we propose a way to publish benchmarks without completely disclosing the ground-truth answers to the questions, while still maintaining the ability to openly evaluate LLMs. Our main idea is to inject randomness to the answers by preparing several logically correct answers, and only include one of them as the solution in the benchmark. This reduces the best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is this helpful to keep us from disclosing the ground truth, but this approach also offers a test for detecting data contamination. In principle, even fully capable models should not surpass the Bayes accuracy. If a model surpasses this ceiling despite this expectation, this is a strong signal of data contamination. We present experimental evidence that our method can detect data contamination accurately on a wide range of benchmarks, models, and training methodologies.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Reporter: A Path to a New Genre of Scientific Communication</title>
<link>https://arxiv.org/abs/2507.05903</link>
<guid>https://arxiv.org/abs/2507.05903</guid>
<content:encoded><![CDATA[
arXiv:2507.05903v2 Announce Type: replace-cross 
Abstract: The AI-Reporter represents a paradigmatic shift in scientific publication practice. This document demonstrates through a concrete case study how our system transforms academic presentations into publication-ready chapters -- in less than three minutes. Using Arno Simons' lecture on Large Language Models from the ``Large Language Models for the History, Philosophy, and Sociology of Science'' workshop (NEPI) as an example, we show how technological innovation bridges the gap between ephemeral presentation and permanent scientific documentation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian</title>
<link>https://arxiv.org/abs/2507.22159</link>
<guid>https://arxiv.org/abs/2507.22159</guid>
<content:encoded><![CDATA[
<div> Keywords: Indonesian, preference-based research, large language models, IndoPref, benchmark

Summary:<br /><br />This article introduces IndoPref, a human-authored multi-domain Indonesian preference dataset designed to evaluate the naturalness and quality of text generated by large language models (LLMs). The dataset addresses the underrepresentation of Indonesian language in preference-based research for LLMs. All annotations in IndoPref are in Indonesian, ensuring cultural and linguistic authenticity. The dataset has been evaluated using Krippendorff's alpha, showing strong inter-annotator agreement. Multiple LLMs have been benchmarked using IndoPref to assess the quality of their output. This initiative aims to address the gap in preference-based research for Indonesian language and enhance the performance of LLMs in generating text that is natural and high-quality in Indonesian. 

Summary: <div>
arXiv:2507.22159v1 Announce Type: new 
Abstract: Over 200 million people speak Indonesian, yet the language remains significantly underrepresented in preference-based research for large language models (LLMs). Most existing multilingual datasets are derived from English translations, often resulting in content that lacks cultural and linguistic authenticity. To address this gap, we introduce IndoPref, the first fully human-authored and multi-domain Indonesian preference dataset specifically designed to evaluate the naturalness and quality of LLM-generated text. All annotations are natively written in Indonesian and evaluated using Krippendorff's alpha, demonstrating strong inter-annotator agreement. Additionally, we benchmark the dataset across multiple LLMs and assess the output quality of each model.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles</title>
<link>https://arxiv.org/abs/2507.22168</link>
<guid>https://arxiv.org/abs/2507.22168</guid>
<content:encoded><![CDATA[
<div> keywords: Large Language Models, benchmarks, persona-based prompting, writing styles, performance evaluation<br />
Summary:<br />
Current benchmarks for Large Language Models (LLMs) lack writing style diversity, potentially leading to brittle performance. This study explores the impact of persona-based prompting on LLM evaluation. Results show that variations in writing style significantly affect LLM performance. Specific writing styles consistently trigger either low or high performance across models. This method offers a scalable approach to enhance benchmarks, improving the validity of LLM performance assessments across linguistic variations.<br /> 
Summary: <div>
arXiv:2507.22168v1 Announce Type: new 
Abstract: Current benchmarks for evaluating Large Language Models (LLMs) often do not exhibit enough writing style diversity, with many adhering primarily to standardized conventions. Such benchmarks do not fully capture the rich variety of communication patterns exhibited by humans. Thus, it is possible that LLMs, which are optimized on these benchmarks, may demonstrate brittle performance when faced with "non-standard" input. In this work, we test this hypothesis by rewriting evaluation prompts using persona-based LLM prompting, a low-cost method to emulate diverse writing styles. Our results show that, even with identical semantic content, variations in writing style and prompt formatting significantly impact the estimated performance of the LLM under evaluation. Notably, we identify distinct writing styles that consistently trigger either low or high performance across a range of models and tasks, irrespective of model family, size, and recency. Our work offers a scalable approach to augment existing benchmarks, improving the external validity of the assessments they provide for measuring LLM performance across linguistic variations.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models</title>
<link>https://arxiv.org/abs/2507.22187</link>
<guid>https://arxiv.org/abs/2507.22187</guid>
<content:encoded><![CDATA[
<div> pipeline, Verb Frame Frequencies, syntactic frames, large language models, automated

Summary:
The article introduces an automated pipeline for estimating Verb Frame Frequencies (VFFs) using large language models (LLMs). This pipeline outperforms existing syntactic parsers and allows for rapid and scalable estimation of VFFs. By instructing an LLM to analyze the syntactic structure of sentences containing 476 English verbs, the pipeline generates a new VFF database with broader verb coverage and finer-grained syntactic distinctions. It also provides explicit estimates of the relative frequencies of structural alternates commonly studied in psycholinguistics. The pipeline is customizable and extensible to new verbs, syntactic frames, and languages. The authors release all code and data to support further research in automated frame frequency estimation.

<br /><br />Summary: <div>
arXiv:2507.22187v1 Announce Type: new 
Abstract: We present an automated pipeline for estimating Verb Frame Frequencies (VFFs), the frequency with which a verb appears in particular syntactic frames. VFFs provide a powerful window into syntax in both human and machine language systems, but existing tools for calculating them are limited in scale, accuracy, or accessibility. We use large language models (LLMs) to generate a corpus of sentences containing 476 English verbs. Next, by instructing an LLM to behave like an expert linguist, we had it analyze the syntactic structure of the sentences in this corpus. This pipeline outperforms two widely used syntactic parsers across multiple evaluation datasets. Furthermore, it requires far fewer resources than manual parsing (the gold-standard), thereby enabling rapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF database with broader verb coverage, finer-grained syntactic distinctions, and explicit estimates of the relative frequencies of structural alternates commonly studied in psycholinguistics. The pipeline is easily customizable and extensible to new verbs, syntactic frames, and even other languages. We present this work as a proof of concept for automated frame frequency estimation, and release all code and data to support future research.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The role of media memorability in facilitating startups' access to venture capital funding</title>
<link>https://arxiv.org/abs/2507.22201</link>
<guid>https://arxiv.org/abs/2507.22201</guid>
<content:encoded><![CDATA[
<div> Keywords: Media memorability, venture capital investment, startup, entrepreneurial finance, news semantic networks

Summary: 
Media reputation plays a crucial role in attracting venture capital investment for startups. Previous research has focused mainly on general media exposure, but this study introduces the concept of media memorability, which is the ability of media to imprint a startup's name in the memory of investors. Data from 197 UK startups in the micro and nanotechnology sector show that media memorability significantly influences investment outcomes. Venture capitalists look for cues such as a startup's distinctiveness and connectivity within news semantic networks. Startups should focus on creating targeted, meaningful coverage that highlights their uniqueness and relevance within the industry conversation to strengthen brand memorability and attract investment. This research contributes to understanding the relationship between media, entrepreneurial finance, and startup funding decisions. 

<br /><br />Summary: <div>
arXiv:2507.22201v1 Announce Type: new 
Abstract: Media reputation plays an important role in attracting venture capital investment. However, prior research has focused too narrowly on general media exposure, limiting our understanding of how media truly influences funding decisions. As informed decision-makers, venture capitalists respond to more nuanced aspects of media content. We introduce the concept of media memorability - the media's ability to imprint a startup's name in the memory of relevant investors. Using data from 197 UK startups in the micro and nanotechnology sector (funded between 1995 and 2004), we show that media memorability significantly influences investment outcomes. Our findings suggest that venture capitalists rely on detailed cues such as a startup's distinctiveness and connectivity within news semantic networks. This contributes to research on entrepreneurial finance and media legitimation. In practice, startups should go beyond frequent media mentions to strengthen brand memorability through more targeted, meaningful coverage highlighting their uniqueness and relevance within the broader industry conversation.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic Predictor?</title>
<link>https://arxiv.org/abs/2507.22209</link>
<guid>https://arxiv.org/abs/2507.22209</guid>
<content:encoded><![CDATA[
<div> entropy, psycholinguistic, language model, Monte Carlo, regression

Summary:
- Contextual entropy is a psycholinguistic measure that predicts how difficult it is to process a word before encountering it.
- Recent studies have explored the impact of entropy on language processing, in addition to surprisal effects.
- Entropy is often estimated based on the probability of a word's first subword token, leading to underestimation and potential distortion of true word entropy.
- Monte Carlo (MC) estimates of word entropy, allowing words to span multiple tokens, provide a more accurate measure.
- Regression experiments on reading times reveal differences between first-token and MC word entropy, cautioning against relying solely on first-token approximations for contextual entropy analysis.<br /><br />Summary: <div>
arXiv:2507.22209v1 Announce Type: new 
Abstract: Contextual entropy is a psycholinguistic measure capturing the anticipated difficulty of processing a word just before it is encountered. Recent studies have tested for entropy-related effects as a potential complement to well-known effects from surprisal. For convenience, entropy is typically estimated based on a language model's probability distribution over a word's first subword token. However, this approximation results in underestimation and potential distortion of true word entropy. To address this, we generate Monte Carlo (MC) estimates of word entropy that allow words to span a variable number of tokens. Regression experiments on reading times show divergent results between first-token and MC word entropy, suggesting a need for caution in using first-token approximations of contextual entropy.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation</title>
<link>https://arxiv.org/abs/2507.22219</link>
<guid>https://arxiv.org/abs/2507.22219</guid>
<content:encoded><![CDATA[
<div> Preference-learning, machine translation, Direct Preference Optimization, Reinforcement Learning, Teacher-Model Refinement<br />
<br />
Summary:<br />
Preference-learning methods in machine translation, such as Direct Preference Optimization, have shown impressive results but struggle with generalization. The novel framework proposed, Reinforcement Learning from Teacher-Model Refinement, eliminates reliance on static triplets by utilizing continuous feedback from an external teacher model. By framing each translation step as a micro-tutorial and rewarding the actor based on alignment with the teacher's refinement, the model learns to emulate the teacher's improvements incrementally. Guided by negative edit distance and COMET score, the actor improves both lexical and structural fidelity and semantic adequacy. On the FLORES-200 benchmark, RLfR outperforms MT-SFT and preference-based baselines, showing significant enhancements in COMET and M-ETA scores. <div>
arXiv:2507.22219v1 Announce Type: new 
Abstract: Preference-learning methods for machine translation (MT)--such as Direct Preference Optimization (DPO)--have achieved impressive gains but depend heavily on large, carefully curated triplet datasets and often struggle to generalize beyond their tuning domains. We propose Reinforcement Learning from Teacher-Model Refinement (RLfR), a novel framework that removes reliance on static triplets by leveraging continuous, high-quality feedback from an external teacher model (GPT-4o). RLfR frames each translation step as a micro-tutorial: the actor generates a hypothesis, the teacher refines it, and the actor is rewarded based on how closely it aligns with the teacher's refinement. Guided by two complementary signals--(i) negative edit distance, promoting lexical and structural fidelity, and (ii) COMET score, ensuring semantic adequacy--the actor progressively learns to emulate the teacher, mirroring a human learning process through incremental, iterative improvement. On the FLORES-200 benchmark (English to and from German, Spanish, Chinese, Korean, and Japanese), RLfR consistently outperforms both MT-SFT and preference-based baselines, significantly improving COMET (semantic adequacy) and M-ETA (entity preservation) scores.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs</title>
<link>https://arxiv.org/abs/2507.22286</link>
<guid>https://arxiv.org/abs/2507.22286</guid>
<content:encoded><![CDATA[
<div> constructionist approach, usage-based, Large Language Models, neural representations, dative constructions<br />
<br />
Summary: This study examines how Large Language Models (LLMs) represent English dative constructions using Pythia-1.4B. By analyzing a dataset of varied sentence pairs, it is found that LLMs' internal representations reflect the function-infused gradience proposed by the usage-based constructionist approach. The separability between construction representations in LLMs is influenced by gradient preference strength, with more prototypical exemplars occupying distinct regions in the activation space. These results demonstrate that LLMs learn rich, meaning-infused, graded representations of constructions and support geometric measures of basic constructionist principles within LLMs. <div>
arXiv:2507.22286v1 Announce Type: new 
Abstract: The usage-based constructionist (UCx) approach posits that language comprises a network of learned form-meaning pairings (constructions) whose use is largely determined by their meanings or functions, requiring them to be graded and probabilistic. This study investigates whether the internal representations in Large Language Models (LLMs) reflect the proposed function-infused gradience. We analyze the neural representations of the English dative constructions (Double Object and Prepositional Object) in Pythia-$1.4$B, using a dataset of $5000$ sentence pairs systematically varied for human-rated preference strength. A macro-level geometric analysis finds that the separability between construction representations, as measured by Energy Distance or Jensen-Shannon Divergence, is systematically modulated by gradient preference strength. More prototypical exemplars of each construction occupy more distinct regions in the activation space of LLMs. These results provide strong evidence that LLMs learn rich, meaning-infused, graded representations of constructions and offer support for geometric measures of basic constructionist principles in LLMs.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party Conversations</title>
<link>https://arxiv.org/abs/2507.22289</link>
<guid>https://arxiv.org/abs/2507.22289</guid>
<content:encoded><![CDATA[
<div> Intent recognition, task-oriented dialogue systems, out-of-scope detection, BERT, LLMs

Summary: 
Intent recognition is crucial for task-oriented dialogue systems (TODS), but it often requires a large amount of annotated data. This work introduces a hybrid approach that combines BERT and LLMs in zero and few-shot settings to recognize intents and detect out-of-scope (OOS) utterances. By leveraging the generalization power of LLMs and computational efficiency of BERT, the proposed method improves system performance on multi-party conversation corpora. The approach involves sharing information from BERT outputs to LLMs, resulting in enhanced performance in intent recognition and OOS detection within TODS. <div>
arXiv:2507.22289v1 Announce Type: new 
Abstract: Intent recognition is a fundamental component in task-oriented dialogue systems (TODS). Determining user intents and detecting whether an intent is Out-of-Scope (OOS) is crucial for TODS to provide reliable responses. However, traditional TODS require large amount of annotated data. In this work we propose a hybrid approach to combine BERT and LLMs in zero and few-shot settings to recognize intents and detect OOS utterances. Our approach leverages LLMs generalization power and BERT's computational efficiency in such scenarios. We evaluate our method on multi-party conversation corpora and observe that sharing information from BERT outputs to LLMs leads to system performance improvement.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers</title>
<link>https://arxiv.org/abs/2507.22337</link>
<guid>https://arxiv.org/abs/2507.22337</guid>
<content:encoded><![CDATA[
<div> taxonomy, benchmark datasets, logic-based classification, neural information retrieval models, negation types 

Summary:
A new study examines the challenges posed by negation in traditional neural information retrieval and large language model-based models. By developing a comprehensive taxonomy of negation derived from various fields, the researchers introduce benchmark datasets to assess model performance and suggest a logic-based classification scheme for analysis. The balanced data distribution in the taxonomy aids in faster model convergence on the proposed NevIR dataset. Additionally, a classification schema evaluates the coverage of negation types in existing datasets, providing insights into the potential factors affecting model generalization. These findings offer a pathway for improving the robustness of neural models in handling complex reasoning tasks involving negation. <br /><br />Summary: <div>
arXiv:2507.22337v1 Announce Type: new 
Abstract: Understanding and solving complex reasoning tasks is vital for addressing the information needs of a user. Although dense neural models learn contextualised embeddings, they still underperform on queries containing negation. To understand this phenomenon, we study negation in both traditional neural information retrieval and LLM-based models. We (1) introduce a taxonomy of negation that derives from philosophical, linguistic, and logical definitions; (2) generate two benchmark datasets that can be used to evaluate the performance of neural information retrieval models and to fine-tune models for a more robust performance on negation; and (3) propose a logic-based classification mechanism that can be used to analyze the performance of retrieval models on existing datasets. Our taxonomy produces a balanced data distribution over negation types, providing a better training setup that leads to faster convergence on the NevIR dataset. Moreover, we propose a classification schema that reveals the coverage of negation types in existing datasets, offering insights into the factors that might affect the generalization of fine-tuned models on negation.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors</title>
<link>https://arxiv.org/abs/2507.22367</link>
<guid>https://arxiv.org/abs/2507.22367</guid>
<content:encoded><![CDATA[
<div> Keywords: personality assessment, psychology-informed prompts, Text-Centric Trait Fusion Network, large language models, AVI Challenge

Summary: 
The article introduces a novel personality assessment framework called Traits Run Deep, which utilizes psychology-informed prompts to extract high-level personality-relevant semantic representations. This framework includes a Text-Centric Trait Fusion Network that integrates asynchronous signals from various modalities. It incorporates a Chunk-Wise Projector for dimensionality reduction, a Cross-Modal Connector, and a Text Feature Enhancer for modality fusion, and an ensemble regression head for improved generalization. By guiding large language models using personality-specific prompts, the framework enhances representation quality. Additionally, extracting and fusing audio-visual apparent behavior features further improves accuracy. Experimental results show a significant reduction in mean squared error and the framework ranks first in the Personality Assessment track of the AVI Challenge 2025. The source code for the framework is available on GitHub at https://github.com/MSA-LMC/TraitsRunDeep.<br /><br />Summary: <div>
arXiv:2507.22367v1 Announce Type: new 
Abstract: Accurate and reliable personality assessment plays a vital role in many fields, such as emotional intelligence, mental health diagnostics, and personalized education. Unlike fleeting emotions, personality traits are stable, often subconsciously leaked through language, facial expressions, and body behaviors, with asynchronous patterns across modalities. It was hard to model personality semantics with traditional superficial features and seemed impossible to achieve effective cross-modal understanding. To address these challenges, we propose a novel personality assessment framework called \textit{\textbf{Traits Run Deep}}. It employs \textit{\textbf{psychology-informed prompts}} to elicit high-level personality-relevant semantic representations. Besides, it devises a \textit{\textbf{Text-Centric Trait Fusion Network}} that anchors rich text semantics to align and integrate asynchronous signals from other modalities. To be specific, such fusion module includes a Chunk-Wise Projector to decrease dimensionality, a Cross-Modal Connector and a Text Feature Enhancer for effective modality fusion and an ensemble regression head to improve generalization in data-scarce situations. To our knowledge, we are the first to apply personality-specific prompts to guide large language models (LLMs) in extracting personality-aware semantics for improved representation quality. Furthermore, extracting and fusing audio-visual apparent behavior features further improves the accuracy. Experimental results on the AVI validation set have demonstrated the effectiveness of the proposed components, i.e., approximately a 45\% reduction in mean squared error (MSE). Final evaluations on the test set of the AVI Challenge 2025 confirm our method's superiority, ranking first in the Personality Assessment track. The source code will be made available at https://github.com/MSA-LMC/TraitsRunDeep.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs</title>
<link>https://arxiv.org/abs/2507.22387</link>
<guid>https://arxiv.org/abs/2507.22387</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, patent writing, PATENTWRITER, benchmarking framework, LLM evaluation

Summary: 
Large language models (LLMs) are being utilized to revolutionize patent writing. The paper introduces PATENTWRITER, a benchmarking framework to evaluate LLMs in generating patent abstracts. Six leading LLMs, including GPT-4 and LLaMA-3, were tested using various prompting strategies. Evaluation metrics included NLP measures, robustness under input perturbations, and applicability in downstream tasks. The study also analyzed stylistic aspects like length, readability, and tone. Results showed that modern LLMs can produce high-quality and stylistically appropriate patent abstracts, often outperforming domain-specific approaches. The open-sourced code and dataset enhance reproducibility and support further research. <div>
arXiv:2507.22387v1 Announce Type: new 
Abstract: Large language models (LLMs) have emerged as transformative approaches in several important fields. This paper aims for a paradigm shift for patent writing by leveraging LLMs to overcome the tedious patent-filing process. In this work, we present PATENTWRITER, the first unified benchmarking framework for evaluating LLMs in patent abstract generation. Given the first claim of a patent, we evaluate six leading LLMs -- including GPT-4 and LLaMA-3 -- under a consistent setup spanning zero-shot, few-shot, and chain-of-thought prompting strategies to generate the abstract of the patent. Our benchmark PATENTWRITER goes beyond surface-level evaluation: we systematically assess the output quality using a comprehensive suite of metrics -- standard NLP measures (e.g., BLEU, ROUGE, BERTScore), robustness under three types of input perturbations, and applicability in two downstream patent classification and retrieval tasks. We also conduct stylistic analysis to assess length, readability, and tone. Experimental results show that modern LLMs can generate high-fidelity and stylistically appropriate patent abstracts, often surpassing domain-specific baselines. Our code and dataset are open-sourced to support reproducibility and future research.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Question Generation for Assessing Early Literacy Reading Comprehension</title>
<link>https://arxiv.org/abs/2507.22410</link>
<guid>https://arxiv.org/abs/2507.22410</guid>
<content:encoded><![CDATA[
<div> Keywords: reading comprehension, content-based interactions, English learners, question generation, language models

Summary: 
This paper presents a new approach for generating comprehension questions for English learners in grades K-2. The method ensures comprehensive coverage of the reading material and tailors questions to the individual student's proficiency levels. It also generates a diverse range of question types at varying difficulty levels to provide a thorough assessment. The study evaluates the performance of different language models using the FairytaleQA dataset as a basis. Ultimately, this approach has the potential to be integrated into AI-driven English instruction tools, offering personalized and effective learning experiences for young learners.<br /><br />Summary: <div>
arXiv:2507.22410v1 Announce Type: new 
Abstract: Assessment of reading comprehension through content-based interactions plays an important role in the reading acquisition process. In this paper, we propose a novel approach for generating comprehension questions geared to K-2 English learners. Our method ensures complete coverage of the underlying material and adaptation to the learner's specific proficiencies, and can generate a large diversity of question types at various difficulty levels to ensure a thorough evaluation. We evaluate the performance of various language models in this framework using the FairytaleQA dataset as the source material. Eventually, the proposed approach has the potential to become an important part of autonomous AI-driven English instructors.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large Language Models</title>
<link>https://arxiv.org/abs/2507.22411</link>
<guid>https://arxiv.org/abs/2507.22411</guid>
<content:encoded><![CDATA[
<div> Keywords: Needle-in-a-Haystack, Large Language Models, long contexts, NeedleChain, ROPE Contraction

Summary:
The Needle-in-a-Haystack benchmark, commonly used to test Large Language Models' (LLMs) ability to understand long contexts, may overestimate their true capability. A new benchmark, NeedleChain, is proposed where the context consists entirely of query-relevant information, challenging LLMs to fully grasp the input. The benchmark allows for flexible context length and reasoning order, providing a more comprehensive analysis of LLM performance. An approach called ROPE Contraction is introduced to enhance LLMs' long-context understanding. Experiments with various advanced LLMs reveal discrepancies between their ability to process large contexts and their capacity to truly understand them. The findings suggest the need for improved evaluation methods for assessing LLMs' long-context understanding. Source code and datasets for NeedleChain are available for further research and analysis. 

<br /><br />Summary: <div>
arXiv:2507.22411v1 Announce Type: new 
Abstract: The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large Language Models' (LLMs) ability to understand long contexts (LC). It evaluates the capability to identify query-relevant context within extensive query-irrelevant passages. Although this method serves as a widely accepted standard for evaluating long-context understanding, our findings suggest it may overestimate the true LC capability of LLMs. We demonstrate that even state-of-the-art models such as GPT-4o struggle to intactly incorporate given contexts made up of solely query-relevant ten sentences. In response, we introduce a novel benchmark, \textbf{NeedleChain}, where the context consists entirely of query-relevant information, requiring the LLM to fully grasp the input to answer correctly. Our benchmark allows for flexible context length and reasoning order, offering a more comprehensive analysis of LLM performance. Additionally, we propose an extremely simple yet compelling strategy to improve LC understanding capability of LLM: ROPE Contraction. Our experiments with various advanced LLMs reveal a notable disparity between their ability to process large contexts and their capacity to fully understand them. Source code and datasets are available at https://github.com/hyeonseokk/NeedleChain
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini</title>
<link>https://arxiv.org/abs/2507.22445</link>
<guid>https://arxiv.org/abs/2507.22445</guid>
<content:encoded><![CDATA[
<div> culture, language model, narrative bias, AI-generated narratives, cultural alignment
<br />
The study investigates the cultural relevance of stories generated by a language model trained on Anglo-American texts for different nationalities. Using OpenAI's gpt-4o-mini model, the researchers generated 11,800 stories for 236 countries, finding that while the stories incorporate surface-level national symbols, they largely follow a uniform plot structure centered around nostalgia, tradition, and community events. This narrative homogeneity, characterized by sanitized conflicts and a lack of romantic elements, reflects a bias towards stability over change and tradition over growth. The authors argue that this structural conformity represents a distinct form of AI bias, impacting efforts to improve the cultural alignment of generative AI. The findings have implications for literary studies, narratology, critical AI studies, and NLP research.
<br /><br />Summary: <div>
arXiv:2507.22445v1 Announce Type: new 
Abstract: Can a language model trained largely on Anglo-American texts generate stories that are culturally relevant to other nationalities? To find out, we generated 11,800 stories - 50 for each of 236 countries - by sending the prompt "Write a 1500 word potential {demonym} story" to OpenAI's model gpt-4o-mini. Although the stories do include surface-level national symbols and themes, they overwhelmingly conform to a single narrative plot structure across countries: a protagonist lives in or returns home to a small town and resolves a minor conflict by reconnecting with tradition and organising community events. Real-world conflicts are sanitised, romance is almost absent, and narrative tension is downplayed in favour of nostalgia and reconciliation. The result is a narrative homogenisation: an AI-generated synthetic imaginary that prioritises stability above change and tradition above growth. We argue that the structural homogeneity of AI-generated narratives constitutes a distinct form of AI bias, a narrative standardisation that should be acknowledged alongside the more familiar representational bias. These findings are relevant to literary studies, narratology, critical AI studies, NLP research, and efforts to improve the cultural alignment of generative AI.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance</title>
<link>https://arxiv.org/abs/2507.22448</link>
<guid>https://arxiv.org/abs/2507.22448</guid>
<content:encoded><![CDATA[
<div> Keywords: Falcon-H1, large language models, hybrid architecture, high performance, efficiency

Summary: 
Falcon-H1 introduces a new series of large language models with hybrid architecture designs combining Transformer-based attention and State Space Models for improved memory and efficiency. The models come in various configurations with parameters ranging from 0.5B to 34B, including quantized instruction-tuned versions. Despite their smaller size, Falcon-H1 models outperform larger models up to 70B scale while using less data. They excel in reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is versatile for various applications. The models are released under an open-source license, emphasizing accessibility and impact in AI research.<br /><br />Summary: <div>
arXiv:2507.22448v1 Announce Type: new 
Abstract: In this report, we introduce Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. We systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring our commitment to accessible and impactful AI research.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is an "Abstract Reasoner"? Revisiting Experiments and Arguments about Large Language Models</title>
<link>https://arxiv.org/abs/2507.22457</link>
<guid>https://arxiv.org/abs/2507.22457</guid>
<content:encoded><![CDATA[
<div> large language models, abstract reasoners, zero-shot performance, input encoding, fine-tuning
Summary:
Large language models (LLMs) have been criticized for their poor zero-shot performance on challenging tasks, leading to debates about their ability to serve as abstract reasoners. The study shows that even minimal parameter tuning for input encoding can significantly improve LLMs' performance on tasks. However, despite this improvement, the fine-tuning does not consistently transfer across different datasets. These findings suggest the need to reconsider the definition of "abstract reasoner" and the significance of LLMs fitting this description. The discussion surrounding LLMs' capabilities raises questions about the underlying mechanisms driving their performance and challenges the assumptions about their ability to reason abstractly. The study offers insights into the complexities of interpreting LLMs' performance and highlights the importance of understanding the limitations and strengths of these models. <br /><br />Summary: <div>
arXiv:2507.22457v1 Announce Type: new 
Abstract: Recent work has argued that large language models (LLMs) are not "abstract reasoners", citing their poor zero-shot performance on a variety of challenging tasks as evidence. We revisit these experiments in order to add nuance to the claim. First, we show that while LLMs indeed perform poorly in a zero-shot setting, even tuning a small subset of parameters for input encoding can enable near-perfect performance. However, we also show that this finetuning does not necessarily transfer across datasets. We take this collection of empirical results as an invitation to (re-)open the discussion of what it means to be an "abstract reasoner", and why it matters whether LLMs fit the bill.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IFEvalCode: Controlled Code Generation</title>
<link>https://arxiv.org/abs/2507.22462</link>
<guid>https://arxiv.org/abs/2507.22462</guid>
<content:encoded><![CDATA[
<div> Code LLMs, controlled code generation, instruction-following, multilingual benchmark, closed-source models<br />
Summary:<br />
The article introduces forward and backward constraints generation to enhance the instruction-following capabilities of Code LLMs in controlled code generation tasks. It presents IFEvalCode, a multilingual benchmark with 1.6K test samples across seven programming languages, evaluating correctness and instruction-following separately. Experiments on over 40 LLMs show that closed-source models outperform open-source ones in controllable code generation. However, there is a notable disparity between the models' ability to generate correct code and code that precisely adheres to provided instructions. The paper underscores the importance of aligning code outputs with detailed requirements such as coding style, line count, and structural constraints beyond mere correctness. <div>
arXiv:2507.22462v1 Announce Type: new 
Abstract: Code large language models (Code LLMs) have made significant progress in code generation by translating natural language descriptions into functional code; however, real-world applications often demand stricter adherence to detailed requirements such as coding style, line count, and structural constraints, beyond mere correctness. To address this, the paper introduces forward and backward constraints generation to improve the instruction-following capabilities of Code LLMs in controlled code generation, ensuring outputs align more closely with human-defined guidelines. The authors further present IFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven programming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and C#), with each sample featuring both Chinese and English queries. Unlike existing benchmarks, IFEvalCode decouples evaluation into two metrics: correctness (Corr.) and instruction-following (Instr.), enabling a more nuanced assessment. Experiments on over 40 LLMs reveal that closed-source models outperform open-source ones in controllable code generation and highlight a significant gap between the models' ability to generate correct code versus code that precisely follows instructions.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLM-SQL: An Exploration of Small Language Models for Text-to-SQL</title>
<link>https://arxiv.org/abs/2507.22478</link>
<guid>https://arxiv.org/abs/2507.22478</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Small language models, Text-to-SQL, Post-training techniques, Supervised fine-tuning

Summary:
Large language models (LLMs) excel in translating natural language questions into SQL queries, while small language models (SLMs) struggle due to limited logical reasoning abilities. This study explores enhancing SLM performance in Text-to-SQL tasks through post-training techniques. The researchers leverage the SynSQL-2.5M dataset to create two derived datasets for SQL generation and revision. By employing supervised fine-tuning and reinforcement learning-based post-training on SLMs, followed by a corrective self-consistency approach during inference, significant improvements are achieved. Experimentally, the SLM-SQL method demonstrates effectiveness and generalizability, with models reaching up to 67.08% execution accuracy on the BIRD development set. The release of the dataset, model, and code on GitHub offers valuable resources for further research in this domain. 

<br /><br />Summary: <div>
arXiv:2507.22478v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated strong performance in translating natural language questions into SQL queries (Text-to-SQL). In contrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters currently underperform on Text-to-SQL tasks due to their limited logical reasoning capabilities. However, SLMs offer inherent advantages in inference speed and suitability for edge deployment. To explore their potential in Text-to-SQL applications, we leverage recent advancements in post-training techniques. Specifically, we used the open-source SynSQL-2.5M dataset to construct two derived datasets: SynSQL-Think-916K for SQL generation and SynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised fine-tuning and reinforcement learning-based post-training to the SLM, followed by inference using a corrective self-consistency approach. Experimental results validate the effectiveness and generalizability of our method, SLM-SQL. On the BIRD development set, the five evaluated models achieved an average improvement of 31.4 points. Notably, the 0.5B model reached 56.87\% execution accuracy (EX), while the 1.5B model achieved 67.08\% EX. We will release our dataset, model, and code to github: https://github.com/CycloneBoy/slm_sql.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records</title>
<link>https://arxiv.org/abs/2507.22533</link>
<guid>https://arxiv.org/abs/2507.22533</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Clinical Decision Support, Electronic Health Records, Temporal Knowledge Graphs, Clinical Guidelines

Summary:
CliCARE is a framework designed to address challenges in implementing Large Language Models (LLMs) for clinical decision support in oncology. The framework tackles issues such as processing extensive and multilingual patient records, mitigating the risk of clinical hallucination, and improving evaluation metrics. CliCARE transforms unstructured Electronic Health Records (EHRs) into patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range dependencies and aligns these with normative guideline knowledge graphs for decision support. The framework provides evidence-grounded decision support by generating clinical summaries and actionable recommendations for oncologists. Validation using diverse datasets demonstrated that CliCARE outperforms strong baseline methods and shows high correlation with expert oncologists' assessments. Overall, CliCARE offers promise in enhancing clinical decision support in oncology by effectively utilizing LLMs and incorporating clinical guidelines for improved decision-making. 

<br /><br />Summary: <div>
arXiv:2507.22533v1 Announce Type: new 
Abstract: Large Language Models (LLMs) hold significant promise for improving clinical decision support and reducing physician burnout by synthesizing complex, longitudinal cancer Electronic Health Records (EHRs). However, their implementation in this critical field faces three primary challenges: the inability to effectively process the extensive length and multilingual nature of patient records for accurate temporal analysis; a heightened risk of clinical hallucination, as conventional grounding techniques such as Retrieval-Augmented Generation (RAG) do not adequately incorporate process-oriented clinical guidelines; and unreliable evaluation metrics that hinder the validation of AI systems in oncology. To address these issues, we propose CliCARE, a framework for Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records. The framework operates by transforming unstructured, longitudinal EHRs into patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range dependencies, and then grounding the decision support process by aligning these real-world patient trajectories with a normative guideline knowledge graph. This approach provides oncologists with evidence-grounded decision support by generating a high-fidelity clinical summary and an actionable recommendation. We validated our framework using large-scale, longitudinal data from a private Chinese cancer dataset and the public English MIMIC-IV dataset. In these diverse settings, CliCARE significantly outperforms strong baselines, including leading long-context LLMs and Knowledge Graph-enhanced RAG methods. The clinical validity of our results is supported by a robust evaluation protocol, which demonstrates a high correlation with assessments made by expert oncologists.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Benchmark Dataset and Evaluation Framework for Vietnamese Large Language Models in Customer Support</title>
<link>https://arxiv.org/abs/2507.22542</link>
<guid>https://arxiv.org/abs/2507.22542</guid>
<content:encoded><![CDATA[
<div> benchmarking, ViLLMs, Customer Support Conversations Dataset, evaluation framework, Vietnamese LLMs

Summary:
The article introduces the Customer Support Conversations Dataset (CSConDa), a benchmark dataset of over 9,000 QA pairs from real interactions with human advisors at a Vietnamese software company. It covers various topics such as pricing, product availability, and technical troubleshooting. The study evaluates 11 lightweight open-source ViLLMs on CSConDa using automatic metrics and syntactic analysis to understand model behavior and identify areas for improvement. The research aims to support the development of next-generation Vietnamese LLMs for customer service QA applications. The dataset is publicly accessible, providing a valuable resource for researchers and industry professionals looking to evaluate and select suitable models for customer support systems.<br /><br />Summary: <div>
arXiv:2507.22542v1 Announce Type: new 
Abstract: With the rapid growth of Artificial Intelligence, Large Language Models (LLMs) have become essential for Question Answering (QA) systems, improving efficiency and reducing human workload in customer service. The emergence of Vietnamese LLMs (ViLLMs) highlights lightweight open-source models as a practical choice for their accuracy, efficiency, and privacy benefits. However, domain-specific evaluations remain limited, and the absence of benchmark datasets reflecting real customer interactions makes it difficult for enterprises to select suitable models for support applications. To address this gap, we introduce the Customer Support Conversations Dataset (CSConDa), a curated benchmark of over 9,000 QA pairs drawn from real interactions with human advisors at a large Vietnamese software company. Covering diverse topics such as pricing, product availability, and technical troubleshooting, CSConDa provides a representative basis for evaluating ViLLMs in practical scenarios. We further present a comprehensive evaluation framework, benchmarking 11 lightweight open-source ViLLMs on CSConDa with both automatic metrics and syntactic analysis to reveal model strengths, weaknesses, and linguistic patterns. This study offers insights into model behavior, explains performance differences, and identifies key areas for improvement, supporting the development of next-generation ViLLMs. By establishing a robust benchmark and systematic evaluation, our work enables informed model selection for customer service QA and advances research on Vietnamese LLMs. The dataset is publicly available at https://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ControlMed: Adding Reasoning Control to Medical Language Model</title>
<link>https://arxiv.org/abs/2507.22545</link>
<guid>https://arxiv.org/abs/2507.22545</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Medical Domain, Reasoning Accuracy, Computational Efficiency, ControlMed

Summary:
ControlMed is a new medical language model designed to enhance reasoning accuracy and computational efficiency in clinical decision-making. It allows users to actively control the length of the reasoning process at inference time through fine-grained control markers. The model is trained through a three-stage pipeline, including pre-training on a synthetic medical instruction dataset, supervised fine-tuning with multi-length reasoning data, and reinforcement learning to improve accuracy and response quality. Experimental results show that ControlMed performs similarly or better than existing models on English and Korean medical benchmarks. Users can adjust reasoning length to balance accuracy and efficiency as needed, making ControlMed a practical solution for clinical question answering and medical information analysis.<br /><br />Summary: <div>
arXiv:2507.22545v1 Announce Type: new 
Abstract: Reasoning Large Language Models (LLMs) with enhanced accuracy and explainability are increasingly being adopted in the medical domain, as the life-critical nature of clinical decision-making demands reliable support. Despite these advancements, existing reasoning LLMs often generate unnecessarily lengthy reasoning processes, leading to significant computational overhead and response latency. These limitations hinder their practical deployment in real-world clinical environments. To address these challenges, we introduce \textbf{ControlMed}, a medical language model that enables users to actively control the length of the reasoning process at inference time through fine-grained control markers. ControlMed is trained through a three-stage pipeline: 1) pre-training on a large-scale synthetic medical instruction dataset covering both \textit{direct} and \textit{reasoning responses}; 2) supervised fine-tuning with multi-length reasoning data and explicit length-control markers; and 3) reinforcement learning with model-based reward signals to enhance factual accuracy and response quality. Experimental results on a variety of English and Korean medical benchmarks demonstrate that our model achieves similar or better performance compared to state-of-the-art models. Furthermore, users can flexibly balance reasoning accuracy and computational efficiency by controlling the reasoning length as needed. These findings demonstrate that ControlMed is a practical and adaptable solution for clinical question answering and medical information analysis.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs</title>
<link>https://arxiv.org/abs/2507.22564</link>
<guid>https://arxiv.org/abs/2507.22564</guid>
<content:encoded><![CDATA[
<div> attack, language models, cognitive biases, safety mechanisms, adversarial

Summary:
CognitiveAttack introduces a new red-teaming framework that leverages individual and combined cognitive biases to bypass safety mechanisms of Large Language Models (LLMs). By integrating supervised fine-tuning and reinforcement learning, CognitiveAttack generates optimized prompts that successfully attack LLMs. Experimental results demonstrate vulnerabilities in 30 LLMs, particularly open-source models, with CognitiveAttack outperforming the state-of-the-art black-box method PAP. The study highlights the power of multi-bias interactions as a significant yet underexplored attack vector in undermining LLM safeguards. This interdisciplinary work bridges cognitive science and LLM safety, paving the way for more robust and human-aligned AI systems. 

<br /><br />Summary: <div>
arXiv:2507.22564v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet their safety mechanisms remain susceptible to adversarial attacks that exploit cognitive biases -- systematic deviations from rational judgment. Unlike prior jailbreaking approaches focused on prompt engineering or algorithmic manipulation, this work highlights the overlooked power of multi-bias interactions in undermining LLM safeguards. We propose CognitiveAttack, a novel red-teaming framework that systematically leverages both individual and combined cognitive biases. By integrating supervised fine-tuning and reinforcement learning, CognitiveAttack generates prompts that embed optimized bias combinations, effectively bypassing safety protocols while maintaining high attack success rates. Experimental results reveal significant vulnerabilities across 30 diverse LLMs, particularly in open-source models. CognitiveAttack achieves a substantially higher attack success rate compared to the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations in current defense mechanisms. These findings highlight multi-bias interactions as a powerful yet underexplored attack vector. This work introduces a novel interdisciplinary perspective by bridging cognitive science and LLM safety, paving the way for more robust and human-aligned AI systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Influence of Amplifying Language-Specific Neurons</title>
<link>https://arxiv.org/abs/2507.22581</link>
<guid>https://arxiv.org/abs/2507.22581</guid>
<content:encoded><![CDATA[
<div> Language-specific neurons, amplification, multilingual behavior, low-resource languages, cross-lingual transfer
Summary:
Language-specific neurons in large language models (LLMs) can influence model behavior by deactivating them, but their role in amplification is not well understood. This study explores amplifying language-specific neurons across 18 languages, including low-resource languages, using three different models. The amplification factors are evaluated based on their effectiveness in steering output towards the target language, and tested on various downstream tasks. The results show that optimal amplification factors can effectively steer output towards different languages but may not always improve cross-language performance. This highlights the importance of language-specific neurons in multilingual behavior, with amplification potentially benefitting low-resource languages more than cross-lingual transfer tasks. 
<br /><br />Summary: <div>
arXiv:2507.22581v1 Announce Type: new 
Abstract: Language-specific neurons in LLMs that strongly correlate with individual languages have been shown to influence model behavior by deactivating them. However, their role in amplification remains underexplored. This work investigates the effect of amplifying language-specific neurons through interventions across 18 languages, including low-resource ones, using three models primarily trained in different languages. We compare amplification factors by their effectiveness in steering to the target language using a proposed Language Steering Shift (LSS) evaluation score, then evaluate it on downstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge (Include), and translation (FLORES). The optimal amplification factors effectively steer output toward nearly all tested languages. Intervention using this factor on downstream tasks improves self-language performance in some cases but generally degrades cross-language results. These findings highlight the effect of language-specific neurons in multilingual behavior, where amplification can be beneficial especially for low-resource languages, but provides limited advantage for cross-lingual transfer.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BALSAM: A Platform for Benchmarking Arabic Large Language Models</title>
<link>https://arxiv.org/abs/2507.22603</link>
<guid>https://arxiv.org/abs/2507.22603</guid>
<content:encoded><![CDATA[
<div> advancement, Large Language Models, Arabic, benchmark, BALSAM

Summary: 
The article discusses the disparity in the performance of Large Language Models (LLMs) in English compared to Arabic, attributed to data scarcity, linguistic diversity of Arabic, and benchmark quality. To address these challenges, the authors introduce BALSAM, a community-driven benchmark for Arabic LLM development and evaluation. BALSAM comprises 78 NLP tasks from 14 categories, with 52K examples for testing and development. It offers a centralized platform for blind evaluation and aims to set standards and foster collaborative research to enhance Arabic LLM capabilities. <div>
arXiv:2507.22603v1 Announce Type: new 
Abstract: The impressive advancement of Large Language Models (LLMs) in English has not been matched across all languages. In particular, LLM performance in Arabic lags behind, due to data scarcity, linguistic diversity of Arabic and its dialects, morphological complexity, etc. Progress is further hindered by the quality of Arabic benchmarks, which typically rely on static, publicly available data, lack comprehensive task coverage, or do not provide dedicated platforms with blind test sets. This makes it challenging to measure actual progress and to mitigate data contamination. Here, we aim to bridge these gaps. In particular, we introduce BALSAM, a comprehensive, community-driven benchmark aimed at advancing Arabic LLM development and evaluation. It includes 78 NLP tasks from 14 broad categories, with 52K examples divided into 37K test and 15K development, and a centralized, transparent platform for blind evaluation. We envision BALSAM as a unifying platform that sets standards and promotes collaborative research to advance Arabic LLM capabilities.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation</title>
<link>https://arxiv.org/abs/2507.22608</link>
<guid>https://arxiv.org/abs/2507.22608</guid>
<content:encoded><![CDATA[
<div> Language-specific neurons, Multilingual abilities, Neural mechanisms, Language arithmetics, Cross-lingual neuron steering<br />
Summary:<br />
Large language models exhibit strong multilingual abilities, with language-specific neurons identified across 21 languages. These neurons cluster in deeper layers, with non-Latin scripts showing greater specialization. Language arithmetics are used to steer models towards desired language behavior, outperforming simpler approaches. Interventions effectively guide behavior across multilingual tasks, with manipulation more successful for high-resource languages. Typological similarity improves effectiveness, and cross-lingual neuron steering enhances downstream performance. Internal "fallback" mechanisms for language selection are revealed as neurons are progressively deactivated. The code is publicly available for access. <div>
arXiv:2507.22608v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit strong multilingual abilities, yet the neural mechanisms behind language-specific processing remain unclear. We analyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and Aya-Expanse-8B & 32B across 21 typologically diverse languages, identifying neurons that control language behavior. Using the Language Activation Probability Entropy (LAPE) method, we show that these neurons cluster in deeper layers, with non-Latin scripts showing greater specialization. Related languages share overlapping neurons, reflecting internal representations of linguistic proximity.
  Through language arithmetics, i.e. systematic activation addition and multiplication, we steer models to deactivate unwanted languages and activate desired ones, outperforming simpler replacement approaches. These interventions effectively guide behavior across five multilingual tasks: language forcing, translation, QA, comprehension, and NLI. Manipulation is more successful for high-resource languages, while typological similarity improves effectiveness. We also demonstrate that cross-lingual neuron steering enhances downstream performance and reveal internal "fallback" mechanisms for language selection when neurons are progressively deactivated. Our code is made publicly available at https://github.com/d-gurgurov/Language-Neurons-Manipulation.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Political Views of Large Language Models: Identification and Steering</title>
<link>https://arxiv.org/abs/2507.22623</link>
<guid>https://arxiv.org/abs/2507.22623</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, political biases, ideological positions, multilingual settings, manipulability

Summary:
Large language models (LLMs) are being increasingly utilized in various applications, raising concerns about their potential political biases. Previous studies have shown that LLMs tend to exhibit political biases, often leaning towards liberal or progressive positions. This study addresses gaps in existing research by conducting a large-scale analysis of political orientation in modern open-source LLMs across different languages. The results indicate that larger models tend to shift towards libertarian-left positions, with variations observed across languages and model families. Additionally, the study demonstrates that political stances of LLMs can be manipulated using a center-of-mass activation intervention technique, allowing for the steering of model responses towards alternative ideological positions in various languages. The code used in the study is publicly available for further exploration. 

<br /><br />Summary: <div>
arXiv:2507.22623v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used in everyday tools and applications, raising concerns about their potential influence on political views. While prior research has shown that LLMs often exhibit measurable political biases--frequently skewing toward liberal or progressive positions--key gaps remain. Most existing studies evaluate only a narrow set of models and languages, leaving open questions about the generalizability of political biases across architectures, scales, and multilingual settings. Moreover, few works examine whether these biases can be actively controlled.
  In this work, we address these gaps through a large-scale study of political orientation in modern open-source instruction-tuned LLMs. We evaluate seven models, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using the Political Compass Test with 11 semantically equivalent paraphrases per statement to ensure robust measurement. Our results reveal that larger models consistently shift toward libertarian-left positions, with significant variations across languages and model families. To test the manipulability of political stances, we utilize a simple center-of-mass activation intervention technique and show that it reliably steers model responses toward alternative ideological positions across multiple languages. Our code is publicly available at https://github.com/d-gurgurov/Political-Ideologies-LLMs.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview Performance Assessment</title>
<link>https://arxiv.org/abs/2507.22676</link>
<guid>https://arxiv.org/abs/2507.22676</guid>
<content:encoded><![CDATA[
<div> Keywords: Interview performance assessment, Multimodal, Ensemble learning, AVI Challenge 2025, Feature extraction

Summary:
By integrating video, audio, and text data, this framework assesses interview performance across multiple dimensions, achieving a low average MSE of 0.1824 and winning the AVI Challenge 2025. The approach uses modality-specific feature extractors and a Shared Compression Multilayer Perceptron to unify multimodal embeddings efficiently. An ensemble learning strategy with independent regression heads and mean-pooling aggregation ensures robust predictions on five key evaluation dimensions. The framework captures both explicit and implicit cues, enabling unbiased assessments and advancing automated interview evaluations. The complete implementation is available on GitHub at https://github.com/MSA-LMC/365Aspects. 

<br /><br />Summary: <div>
arXiv:2507.22676v1 Announce Type: new 
Abstract: Interview performance assessment is essential for determining candidates' suitability for professional positions. To ensure holistic and fair evaluations, we propose a novel and comprehensive framework that explores ``365'' aspects of interview performance by integrating \textit{three} modalities (video, audio, and text), \textit{six} responses per candidate, and \textit{five} key evaluation dimensions. The framework employs modality-specific feature extractors to encode heterogeneous data streams and subsequently fused via a Shared Compression Multilayer Perceptron. This module compresses multimodal embeddings into a unified latent space, facilitating efficient feature interaction. To enhance prediction robustness, we incorporate a two-level ensemble learning strategy: (1) independent regression heads predict scores for each response, and (2) predictions are aggregated across responses using a mean-pooling mechanism to produce final scores for the five target dimensions. By listening to the unspoken, our approach captures both explicit and implicit cues from multimodal data, enabling comprehensive and unbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our framework secured first place in the AVI Challenge 2025, demonstrating its effectiveness and robustness in advancing automated and multimodal interview performance assessment. The full implementation is available at https://github.com/MSA-LMC/365Aspects.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in Retrieval-Augmented Reasoning for LLMs</title>
<link>https://arxiv.org/abs/2507.22716</link>
<guid>https://arxiv.org/abs/2507.22716</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented generation, reinforcement learning, reasoning models, multi-dimensional reward system, TIRESRAG-R1 <br />
Summary: <br />
This paper introduces a novel framework called TIRESRAG-R1 that aims to enhance the reasoning abilities of large language models by addressing three main failure patterns in existing models. The framework utilizes a think-retrieve-reflect process along with a multi-dimensional reward system to improve reasoning and stability. It includes sufficiency rewards to encourage thorough retrieval of information, reasoning quality rewards to assess the rationality and accuracy of reasoning chains, and reflection rewards to detect and revise errors. Additionally, TIRESRAG-R1 implements a difficulty-aware reweighting strategy and training sample filtering to improve performance on complex tasks. Experimental results on multiple question-answering datasets demonstrate that TIRESRAG-R1 outperforms previous retrieval-augmented generation methods and generalizes well to single-hop tasks. <div>
arXiv:2507.22716v1 Announce Type: new 
Abstract: Reinforcement learning-based retrieval-augmented generation (RAG) methods enhance the reasoning abilities of large language models (LLMs). However, most rely only on final-answer rewards, overlooking intermediate reasoning quality. This paper analyzes existing RAG reasoning models and identifies three main failure patterns: (1) information insufficiency, meaning the model fails to retrieve adequate support; (2) faulty reasoning, where logical or content-level flaws appear despite sufficient information; and (3) answer-reasoning inconsistency, where a valid reasoning chain leads to a mismatched final answer. We propose TIRESRAG-R1, a novel framework using a think-retrieve-reflect process and a multi-dimensional reward system to improve reasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to encourage thorough retrieval; (2) a reasoning quality reward to assess the rationality and accuracy of the reasoning chain; and (3) a reflection reward to detect and revise errors. It also employs a difficulty-aware reweighting strategy and training sample filtering to boost performance on complex tasks. Experiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms prior RAG methods and generalizes well to single-hop tasks. The code and data are available at: https://github.com/probe2/TIRESRAG-R1.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Hallucination in Conversations for Low Resource Languages</title>
<link>https://arxiv.org/abs/2507.22720</link>
<guid>https://arxiv.org/abs/2507.22720</guid>
<content:encoded><![CDATA[
<div> Languages: Large Language Models, Hallucination, Hindi, Farsi, Mandarin
Summary: 
Large Language Models (LLMs) like GPT-3.5 and GPT-4o have shown proficiency in generating human-like text but often produce factually incorrect statements, known as 'hallucinations'. A study examined conversational data in Hindi, Farsi, and Mandarin to analyze errors in LLMs including GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1, and Qwen-3. Results revealed that Mandarin had the fewest hallucinations while Hindi and Farsi exhibited significantly more. Identifying and addressing hallucinations are crucial for enhancing the reliability and effectiveness of LLMs across languages. 

<br /><br />Summary: <div>
arXiv:2507.22720v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in generating text that closely resemble human writing. However, they often generate factually incorrect statements, a problem typically referred to as 'hallucination'. Addressing hallucination is crucial for enhancing the reliability and effectiveness of LLMs. While much research has focused on hallucinations in English, our study extends this investigation to conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a comprehensive analysis of a dataset to examine both factual and linguistic errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated responses in Mandarin but generate a significantly higher number of hallucinations in Hindi and Farsi.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resource-Efficient Adaptation of Large Language Models for Text Embeddings via Prompt Engineering and Contrastive Fine-tuning</title>
<link>https://arxiv.org/abs/2507.22729</link>
<guid>https://arxiv.org/abs/2507.22729</guid>
<content:encoded><![CDATA[
<div> Language Models, Natural Language Processing, Text Generation, Text Embeddings, Clustering

Summary: 
- Large Language Models (LLMs) have become essential in Natural Language Processing (NLP) for generating text with impressive performance.
- Token-level representations in LLMs capture rich semantics but lose information when pooled into text embeddings.
- Non-generative tasks like clustering, classification, and retrieval require accurate sentence- or document-level embeddings.
- Adaptation strategies for pre-trained LLMs include aggregation techniques for token embeddings, task-specific prompt engineering, and contrastive fine-tuning for text-level augmentation.
- By combining these strategies, the study achieves state-of-the-art results on English clustering in the Massive Text Embedding Benchmark (MTEB).
- Analysis of the attention map shows that fine-tuning focuses on relevant words, improving compression of meaning in the final hidden state.
- Experiments show that LLMs can be effectively adapted as text embedding models using prompt engineering and contrastive fine-tuning on synthetically generated positive pairs.

<br /><br />Summary: <div>
arXiv:2507.22729v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have become a cornerstone in Natural Language Processing (NLP), achieving impressive performance in text generation. Their token-level representations capture rich, human-aligned semantics. However, pooling these vectors into a text embedding discards crucial information. Nevertheless, many non-generative downstream tasks, such as clustering, classification, or retrieval, still depend on accurate and controllable sentence- or document-level embeddings. We explore several adaptation strategies for pre-trained, decoder-only LLMs: (i) various aggregation techniques for token embeddings, (ii) task-specific prompt engineering, and (iii) text-level augmentation via contrastive fine-tuning. Combining these components yields state-of-the-art performance on the English clustering track of the Massive Text Embedding Benchmark (MTEB). An analysis of the attention map further shows that fine-tuning shifts focus from prompt tokens to semantically relevant words, indicating more effective compression of meaning into the final hidden state. Our experiments demonstrate that LLMs can be effectively adapted as text embedding models through a combination of prompt engineering and resource-efficient contrastive fine-tuning on synthetically generated positive pairs.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Hallucinations in Summarization via Reinforcement Learning with Entity Hallucination Index</title>
<link>https://arxiv.org/abs/2507.22744</link>
<guid>https://arxiv.org/abs/2507.22744</guid>
<content:encoded><![CDATA[
<div> hallucinations, abstractive summarization, reward-driven fine-tuning, Entity Hallucination Index, reinforcement learning

Summary:
In this work, the challenge of reducing hallucinations in abstractive summarization is addressed through a reward-driven fine-tuning framework. The framework optimizes for the Entity Hallucination Index (EHI), a metric that evaluates the presence, correctness, and grounding of named entities in generated summaries. By using reinforcement learning to fine-tune model parameters with EHI as a reward signal, the model generates entity-faithful outputs. The approach does not require human-written factuality annotations, allowing for scalable fine-tuning. Experimental results show consistent improvements in EHI scores across datasets, highlighting a reduction in entity-level hallucinations without compromising fluency or informativeness. The release of a reproducible Colab pipeline enables further research on hallucination-aware model fine-tuning using lightweight metrics like EHI. 

<br /><br />Summary: <div>
arXiv:2507.22744v1 Announce Type: new 
Abstract: Reducing hallucinations in abstractive summarization remains a critical challenge for deploying language models (LMs) in real-world settings. In this work, we introduce a rewarddriven fine-tuning framework that explicitly optimizes for Entity Hallucination Index (EHI), a metric designed to quantify the presence, correctness, and grounding of named entities in generated summaries. Given a corpus of meeting transcripts, we first generate baseline summaries using a pre-trained LM and compute EHI scores via automatic entity extraction and matching. We then apply reinforcement learning to fine-tune the model parameters, using EHI as a reward signal to bias generation toward entity-faithful outputs. Our approach does not rely on human-written factuality annotations, enabling scalable fine-tuning. Experiments demonstrate consistent improvements in EHI across datasets, with qualitative analysis revealing a significant reduction in entity-level hallucinations without degradation in fluency or informativeness. We release a reproducible Colab pipeline, facilitating further research on hallucination-aware model fine-tuning using lightweight, hallucintion metrics like EHI.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset</title>
<link>https://arxiv.org/abs/2507.22752</link>
<guid>https://arxiv.org/abs/2507.22752</guid>
<content:encoded><![CDATA[
<div> benchmark, regional question answering, textual, visual modalities, dataset, large language models (LLMs), human judgments,

Summary:<br />
- A benchmark for open-ended regional question answering encompassing textual and visual modalities has been introduced, with manually curated questions and answers grounded in Wikipedia from Czechia, Slovakia, and Ukraine.
- Strong baselines using state-of-the-art large language models (LLMs) have been provided for evaluation.
- Human evaluations were used to assess the reliability of existing automatic evaluation metrics, revealing a significant gap in regional knowledge among current LLMs.
- Minimal correlation was found between automated metrics and human judgment.
- The dataset is released as a resource to assess regional knowledge in LLMs, study cross-lingual generation consistency, and advance the development of evaluation metrics for open-ended question answering. 

Summary: <div>
arXiv:2507.22752v1 Announce Type: new 
Abstract: We introduce a benchmark for open-ended regional question answering that encompasses both textual and visual modalities. We also provide strong baselines using state-of-the-art large language models (LLMs). Our dataset consists of manually curated questions and answers grounded in Wikipedia, created by native speakers from Czechia, Slovakia, and Ukraine, with accompanying English translations. It includes both purely textual questions and those requiring visual understanding. As a baseline, we evaluate state-of-the-art LLMs through prompting and complement this with human judgments of answer correctness. Using these human evaluations, we analyze the reliability of existing automatic evaluation metrics. Our baseline results highlight a significant gap in regional knowledge among current LLMs. Moreover, apart from LLM-based evaluation, there is minimal correlation between automated metrics and human judgment. We release this dataset as a resource to (1) assess regional knowledge in LLMs, (2) study cross-lingual generation consistency in a challenging setting, and (3) advance the development of evaluation metrics for open-ended question answering.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opportunities and Challenges of LLMs in Education: An NLP Perspective</title>
<link>https://arxiv.org/abs/2507.22753</link>
<guid>https://arxiv.org/abs/2507.22753</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, education, NLP, assistance, assessment

Summary: 
Large language models (LLMs) are increasingly being integrated into educational applications, impacting reading, writing, speaking, and tutoring. They play a significant role in providing assistance and assessment in education. LLMs offer new opportunities for teaching and learning in the field of natural language processing (NLP). However, challenges such as ethical considerations and data privacy need to be addressed. The article explores the potential of LLMs in developing language-focused educational applications and highlights the importance of understanding how LLMs can enhance educational NLP. This comprehensive overview is valuable for NLP researchers and practitioners looking to leverage LLMs for future educational advancements.

Summary: <div>
arXiv:2507.22753v1 Announce Type: new 
Abstract: Interest in the role of large language models (LLMs) in education is increasing, considering the new opportunities they offer for teaching, learning, and assessment. In this paper, we examine the impact of LLMs on educational NLP in the context of two main application scenarios: {\em assistance} and {\em assessment}, grounding them along the four dimensions -- reading, writing, speaking, and tutoring. We then present the new directions enabled by LLMs, and the key challenges to address. We envision that this holistic overview would be useful for NLP researchers and practitioners interested in exploring the role of LLMs in developing language-focused and NLP-enabled educational applications of the future.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASCA: LLM based-Multi Agents System for Credit Assessment</title>
<link>https://arxiv.org/abs/2507.22758</link>
<guid>https://arxiv.org/abs/2507.22758</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, Agent-based system, Credit assessment, MASCA, Hierarchical multi-agent systems

Summary: 
MASCA is introduced as a novel approach to credit evaluation, utilizing LLM-driven multi-agent systems. The framework employs specialized agents that collaborate to address sub-tasks and integrates contrastive learning for risk and reward assessment. The signaling game theory perspective on hierarchical multi-agent systems offers theoretical insights into their structure and interactions. The paper also includes a bias analysis in credit assessment to address fairness concerns. Experimental results demonstrate MASCA's superiority over baseline approaches, showcasing the effectiveness of hierarchical LLM-based multi-agent systems in financial applications, particularly in credit scoring. <div>
arXiv:2507.22758v1 Announce Type: new 
Abstract: Recent advancements in financial problem-solving have leveraged LLMs and agent-based systems, with a primary focus on trading and financial modeling. However, credit assessment remains an underexplored challenge, traditionally dependent on rule-based methods and statistical models. In this paper, we introduce MASCA, an LLM-driven multi-agent system designed to enhance credit evaluation by mirroring real-world decision-making processes. The framework employs a layered architecture where specialized LLM-based agents collaboratively tackle sub-tasks. Additionally, we integrate contrastive learning for risk and reward assessment to optimize decision-making. We further present a signaling game theory perspective on hierarchical multi-agent systems, offering theoretical insights into their structure and interactions. Our paper also includes a detailed bias analysis in credit assessment, addressing fairness concerns. Experimental results demonstrate that MASCA outperforms baseline approaches, highlighting the effectiveness of hierarchical LLM-based multi-agent systems in financial applications, particularly in credit scoring.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph</title>
<link>https://arxiv.org/abs/2507.22811</link>
<guid>https://arxiv.org/abs/2507.22811</guid>
<content:encoded><![CDATA[
<div> entity linker, DBLP, RDF-based Knowledge Graph, dblp:Stream, zero-shot entity linker, LLMs

Summary:
- The article introduces a new entity linker for DBLP's updated version of the RDF-based Knowledge Graph in 2025, which now includes publication venues as a new entity type called dblp:Stream.
- Unlike previous versions, the entity linker in this work utilizes a zero-shot approach using LLMs to re-rank candidate entities based on the log-probabilities of the "yes" token output at the penultimate layer.
- In the 2022 version, KG-embeddings and re-rankers were trained on a dataset to produce entity linkings, but this work represents a shift towards utilizing LLMs for entity linking.
- The proposed method aims to improve the accuracy and efficiency of entity linking in the DBLP Knowledge Graph by leveraging the capabilities of LLMs.
- By incorporating LLMs into the entity linking process, the researchers demonstrate a novel approach to handling entity linking in RDF-based Knowledge Graphs, potentially leading to more precise and reliable results. 

<br /><br />Summary: <div>
arXiv:2507.22811v1 Announce Type: new 
Abstract: In this work we present an entity linker for DBLP's 2025 version of RDF-based Knowledge Graph. Compared to the 2022 version, DBLP now considers publication venues as a new entity type called dblp:Stream. In the earlier version of DBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce entity linkings. In contrast, in this work, we develop a zero-shot entity linker using LLMs using a novel method, where we re-rank candidate entities based on the log-probabilities of the "yes" token output at the penultimate layer of the LLM.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Natural Language Plans: Structure-Aware Planning for Query-Focused Table Summarization</title>
<link>https://arxiv.org/abs/2507.22829</link>
<guid>https://arxiv.org/abs/2507.22829</guid>
<content:encoded><![CDATA[
<div> structured representation, query-focused table summarization, SPaGe, TaSoF, multi-table tasks <br />
Summary: <br />
This paper introduces a new approach to query-focused table summarization, utilizing structured representations for complex reasoning. The proposed framework, SPaGe, involves three phases: Structured Planning to generate a new structured plan (TaSoF) from a query, Graph-based Execution to convert plan steps to SQL, and Summary Generation for producing query-focused summaries. This structured approach captures complex dependencies, improving reliability and scalability, especially for multi-table tasks. Experimental results show that SPaGe outperforms prior models in both single- and multi-table settings, highlighting the advantages of structured representations in enhancing the robustness and efficiency of table summarization. <div>
arXiv:2507.22829v1 Announce Type: new 
Abstract: Query-focused table summarization requires complex reasoning, often approached through step-by-step natural language (NL) plans. However, NL plans are inherently ambiguous and lack structure, limiting their conversion into executable programs like SQL and hindering scalability, especially for multi-table tasks. To address this, we propose a paradigm shift to structured representations. We introduce a new structured plan, TaSoF, inspired by formalism in traditional multi-agent systems, and a framework, SPaGe, that formalizes the reasoning process in three phases: 1) Structured Planning to generate TaSoF from a query, 2) Graph-based Execution to convert plan steps into SQL and model dependencies via a directed cyclic graph for parallel execution, and 3) Summary Generation to produce query-focused summaries. Our method explicitly captures complex dependencies and improves reliability. Experiments on three public benchmarks show that SPaGe consistently outperforms prior models in both single- and multi-table settings, demonstrating the advantages of structured representations for robust and scalable summarization.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning</title>
<link>https://arxiv.org/abs/2507.22887</link>
<guid>https://arxiv.org/abs/2507.22887</guid>
<content:encoded><![CDATA[
<div> in-context learning, large language models, demos, positional bias, accuracy

Summary:
The study explores a new positional bias in in-context learning (ICL) called DEMOS' POSITION IN PROMPT (DPP) bias. It investigates how the position of demos, system prompts, and user messages in large language models (LLMs) affects accuracy and predictions. Two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, are introduced to quantify the impact of changes in demo positions. Experiments on ten LLMs across various tasks show that placing demos at the start of the prompt leads to more stable and accurate outputs, with gains of up to +6 points. Placing demos at the end of the user message significantly affects predictions, flipping over 30% without improving correctness on question answering tasks. Smaller models are more sensitive to this bias, but even large models show some impact on more complex tasks. <div>
arXiv:2507.22887v1 Announce Type: new 
Abstract: In-context learning (ICL) is a critical emerging capability of large language models (LLMs), enabling few-shot learning during inference by including a few demonstrations (demos) in the prompt. However, it has been found that ICL's performance can be sensitive to the choices of demos and their order. This paper investigates an unexplored new positional bias of ICL for the first time: we observe that the predictions and accuracy can drift drastically when the positions of demos, the system prompt, and the user message in LLM input are varied. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We design a systematic evaluation pipeline to study this type of positional bias across classification, question answering, summarization, and reasoning tasks. We introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify net gains and output volatility induced by changes in the demos' position. Extensive experiments on ten LLMs from four open-source model families (QWEN, LLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their accuracy and predictions: placing demos at the start of the prompt yields the most stable and accurate outputs with gains of up to +6 points. In contrast, placing demos at the end of the user message flips over 30\% of predictions without improving correctness on QA tasks. Smaller models are most affected by this sensitivity, though even large models remain marginally affected on complex tasks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CIMR: Contextualized Iterative Multimodal Reasoning for Robust Instruction Following in LVLMs</title>
<link>https://arxiv.org/abs/2507.22074</link>
<guid>https://arxiv.org/abs/2507.22074</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Vision-Language Models, Contextualized Iterative Multimodal Reasoning, Multi-modal instructions, Self-correction

Summary: 
Contextualized Iterative Multimodal Reasoning (CIMR) is a framework designed to enhance the ability of Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) in processing complex, multi-step multi-modal instructions. The framework introduces a context-aware iterative reasoning and self-correction module, operating in two stages: initial reasoning and response generation, followed by iterative refinement using parsed multi-modal feedback. CIMR integrates textual, visual, and contextual features dynamically at each step. By fine-tuning LLaVA-1.5-7B on the Visual Instruction Tuning (VIT) dataset and evaluating on the Multi-modal Action Planning (MAP) dataset, CIMR achieves 91.5% accuracy, surpassing state-of-the-art models like GPT-4V, LLaVA-1.5, MiniGPT-4, and InstructBLIP. This demonstrates the effectiveness of CIMR's iterative reasoning and self-correction capabilities in handling complex tasks. 

<br /><br />Summary: <div>
arXiv:2507.22074v1 Announce Type: cross 
Abstract: The rapid advancement of Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) has enhanced our ability to process and generate human language and visual information. However, these models often struggle with complex, multi-step multi-modal instructions that require logical reasoning, dynamic feedback integration, and iterative self-correction. To address this, we propose CIMR: Contextualized Iterative Multimodal Reasoning, a novel framework that introduces a context-aware iterative reasoning and self-correction module. CIMR operates in two stages: initial reasoning and response generation, followed by iterative refinement using parsed multi-modal feedback. A dynamic fusion module deeply integrates textual, visual, and contextual features at each step. We fine-tune LLaVA-1.5-7B on the Visual Instruction Tuning (VIT) dataset and evaluate CIMR on the newly introduced Multi-modal Action Planning (MAP) dataset. CIMR achieves 91.5% accuracy, outperforming state-of-the-art models such as GPT-4V (89.2%), LLaVA-1.5 (78.5%), MiniGPT-4 (75.3%), and InstructBLIP (72.8%), demonstrating the efficacy of its iterative reasoning and self-correction capabilities in complex tasks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback</title>
<link>https://arxiv.org/abs/2507.22080</link>
<guid>https://arxiv.org/abs/2507.22080</guid>
<content:encoded><![CDATA[
<div> synthesis, code generation, Large Language Models, collaborative programming, feedback mechanism\
<br />
CodeEvo is a framework that leverages collaborative programming practices to synthesize high-quality code data for training Large Language Models (LLMs). It involves iterative interactions between two LLM agents - a Coder and a Reviewer - where the Coder generates code and test cases based on instructions, while the Reviewer provides feedback and new instructions. A hybrid feedback mechanism ensures the quality control by combining compiler determinism with generative flexibility. Experimental results show that models trained on CodeEvo data outperform existing baselines in code generation tasks. The approach emphasizes the importance of code-centric data synthesis and offers insights into effective strategies for generating diverse and relevant code samples. <br /><br />Summary: <div>
arXiv:2507.22080v1 Announce Type: cross 
Abstract: Acquiring high-quality instruction-code pairs is essential for training Large Language Models (LLMs) for code generation. Manually curated data is expensive and inherently limited in scale, motivating the development of code-centric synthesis methods. Yet, current approaches either focus on augmenting existing code or rely on predefined heuristics, both lacking rigorous data validation, which results in synthetic data that is ungrounded, repetitive, or overly simplistic. Inspired by collaborative programming practices, we propose CodeEvo, a framework that synthesizes code data through iterative interactions between two LLM agents: a Coder, which generates candidate code and test cases based on given instructions, and a Reviewer, which guides the synthesis process by producing new instructions and feedback. We further introduce a hybrid feedback mechanism that combines compiler determinism with the generative flexibility of agents, enabling automatic quality control throughout synthesis. Extensive experiments demonstrate that models fine-tuned on CodeEvo data significantly outperform established baselines across code generation benchmarks with various difficulties. In-depth analyses further provide insights from multiple perspectives into effective code-centric data synthesis.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Optimization and Evaluation for LLM Automated Red Teaming</title>
<link>https://arxiv.org/abs/2507.22133</link>
<guid>https://arxiv.org/abs/2507.22133</guid>
<content:encoded><![CDATA[
<div> optimizing attack generator prompts, ASR, Attack Success Rate, discoverability, prompt optimization
<br />
Summary: 
This paper introduces a method for optimizing attack generator prompts using the Attack Success Rate (ASR) for individual attacks. By measuring the discoverability of each attack through multiple attempts against a randomly seeded target, exploitable patterns are revealed. This approach enhances the evaluation and refinement of attack generators by ensuring a more robust assessment of their effectiveness. Automated Red Teaming with Large Language Models (LLMs) is a growing trend in identifying system vulnerabilities, making the optimization of attack generators crucial. The evaluation of attack generators using ASR on individual attacks provides insights into their discoverability and effectiveness. By iterating attacks against randomly seeded targets, exploitable patterns can be detected, leading to improved prompt optimization. This method allows for a more comprehensive evaluation and refinement process, ultimately enhancing the overall security of target systems. <div>
arXiv:2507.22133v1 Announce Type: cross 
Abstract: Applications that use Large Language Models (LLMs) are becoming widespread, making the identification of system vulnerabilities increasingly important. Automated Red Teaming accelerates this effort by using an LLM to generate and execute attacks against target systems. Attack generators are evaluated using the Attack Success Rate (ASR) the sample mean calculated over the judgment of success for each attack. In this paper, we introduce a method for optimizing attack generator prompts that applies ASR to individual attacks. By repeating each attack multiple times against a randomly seeded target, we measure an attack's discoverability the expectation of the individual attack success. This approach reveals exploitable patterns that inform prompt optimization, ultimately enabling more robust evaluation and refinement of generators.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Deflection: Defending LLMs from Logit Manipulation</title>
<link>https://arxiv.org/abs/2507.22160</link>
<guid>https://arxiv.org/abs/2507.22160</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, security, jailbreaking attacks, logit-level attacks, defense 

Summary: 
The article discusses the importance of ensuring the security of Large Language Models (LLMs) against jailbreaking attacks, particularly in critical areas where these models are increasingly being adopted. Traditional defenses against malicious prompts may not be sufficient, as recent logit-level attacks have demonstrated the ability to manipulate token-selection processes. In response to these advanced attacks, Strategic Deflection (SDeflection) is proposed as a defense mechanism that redirects the model's response to neutralize harmful intents rather than outright refusing them. Experimental results show that SDeflection effectively lowers Attack Success Rate (ASR) while maintaining model performance on benign queries. This approach represents a significant shift in defensive strategies, moving towards strategic content redirection to mitigate advanced threats. 

<br /><br />Summary: <div>
arXiv:2507.22160v1 Announce Type: cross 
Abstract: With the growing adoption of Large Language Models (LLMs) in critical areas, ensuring their security against jailbreaking attacks is paramount. While traditional defenses primarily rely on refusing malicious prompts, recent logit-level attacks have demonstrated the ability to bypass these safeguards by directly manipulating the token-selection process during generation. We introduce Strategic Deflection (SDeflection), a defense that redefines the LLM's response to such advanced attacks. Instead of outright refusal, the model produces an answer that is semantically adjacent to the user's request yet strips away the harmful intent, thereby neutralizing the attacker's harmful intent. Our experiments demonstrate that SDeflection significantly lowers Attack Success Rate (ASR) while maintaining model performance on benign queries. This work presents a critical shift in defensive strategies, moving from simple refusal to strategic content redirection to neutralize advanced threats.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability Through Systematicity: The Hard Systematicity Challenge for Artificial Intelligence</title>
<link>https://arxiv.org/abs/2507.22197</link>
<guid>https://arxiv.org/abs/2507.22197</guid>
<content:encoded><![CDATA[
<div> systematicity, explainability, AI models, connectionism, Fodorian<br />
<br />Summary: This paper argues that explainability is just one aspect of a broader ideal guiding our expectations of artificial intelligence (AI). The focus is on the extent to which AI demonstrates systematicity, not only in understanding how thoughts are composed but also in striving toward a consistent, coherent, comprehensive, and parsimoniously principled body of thought. The author presents a framework to define "the systematicity of thought" in four distinct senses to address challenges posed by connectionism. The conceptual framework aims to reconcile systematicity with connectionism and explores the historical notion of systematic thought informed by rationales for systematization. The paper identifies five rationales for systematization and examines their applicability to AI models, introducing the "hard systematicity challenge." The dynamic understanding of systematization helps determine the level of systematicity required in AI models based on the reasons for systematizing thought. <div>
arXiv:2507.22197v1 Announce Type: cross 
Abstract: This paper argues that explainability is only one facet of a broader ideal that shapes our expectations towards artificial intelligence (AI). Fundamentally, the issue is to what extent AI exhibits systematicity--not merely in being sensitive to how thoughts are composed of recombinable constituents, but in striving towards an integrated body of thought that is consistent, coherent, comprehensive, and parsimoniously principled. This richer conception of systematicity has been obscured by the long shadow of the "systematicity challenge" to connectionism, according to which network architectures are fundamentally at odds with what Fodor and colleagues termed "the systematicity of thought." I offer a conceptual framework for thinking about "the systematicity of thought" that distinguishes four senses of the phrase. I use these distinctions to defuse the perceived tension between systematicity and connectionism and show that the conception of systematicity that historically shaped our sense of what makes thought rational, authoritative, and scientific is more demanding than the Fodorian notion. To determine whether we have reason to hold AI models to this ideal of systematicity, I then argue, we must look to the rationales for systematization and explore to what extent they transfer to AI models. I identify five such rationales and apply them to AI. This brings into view the "hard systematicity challenge." However, the demand for systematization itself needs to be regulated by the rationales for systematization. This yields a dynamic understanding of the need to systematize thought, which tells us how systematic we need AI models to be and when.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoEx -- Co-evolving World-model and Exploration</title>
<link>https://arxiv.org/abs/2507.22281</link>
<guid>https://arxiv.org/abs/2507.22281</guid>
<content:encoded><![CDATA[
<div> Hierarchy, CoEx, LLM, world model, planning <br />
Summary: <br />
The article discusses the limitations of current LLM agents in effectively assimilating new observations into their static internal world models for planning. To address this issue, the authors propose a hierarchical agent architecture called CoEx. This architecture utilizes hierarchical state abstraction to allow LLM planning to co-evolve with a dynamically updated model of the world. CoEx employs LLM reasoning to orchestrate dynamic plans involving subgoals, continuously integrating these experiences into a neurosymbolic belief state. The agent is evaluated across various scenarios, demonstrating superior performance in planning and exploration compared to existing paradigms. Through experiments in diverse environments and complex tasks, including ALFWorld, PDDL, and Jericho, CoEx showcases its ability to generate effective plans and interact with the world in a more adaptive and efficient manner. <br /> <div>
arXiv:2507.22281v1 Announce Type: cross 
Abstract: Planning in modern LLM agents relies on the utilization of LLM as an internal world model, acquired during pretraining. However, existing agent designs fail to effectively assimilate new observations into dynamic updates of the world model. This reliance on the LLM's static internal world model is progressively prone to misalignment with the underlying true state of the world, leading to the generation of divergent and erroneous plans. We introduce a hierarchical agent architecture, CoEx, in which hierarchical state abstraction allows LLM planning to co-evolve with a dynamically updated model of the world. CoEx plans and interacts with the world by using LLM reasoning to orchestrate dynamic plans consisting of subgoals, and its learning mechanism continuously incorporates these subgoal experiences into a persistent world model in the form of a neurosymbolic belief state, comprising textual inferences and code-based symbolic memory. We evaluate our agent across a diverse set of agent scenarios involving rich environments and complex tasks including ALFWorld, PDDL, and Jericho. Our experiments show that CoEx outperforms existing agent paradigms in planning and exploration.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2507.22359</link>
<guid>https://arxiv.org/abs/2507.22359</guid>
<content:encoded><![CDATA[
<div> benchmark-free evaluation, large language models, LLM-Crowdsourced, evaluation criteria, mathematics, programming

Summary:<br />
1. The study introduces a novel evaluation method, LLM-Crowdsourced, to overcome challenges in assessing large language models' capabilities.
2. Key evaluation criteria include dynamic, transparent, objective, and professional aspects, addressing limitations of existing methods.
3. Experiments on eight mainstream LLMs in mathematics and programming demonstrate the method's effectiveness in distinguishing performance.
4. Findings reveal Gemini excels in original question design, while some LLMs exhibit memorization-based answering.
5. Evaluation results show high consistency and robustness, highlighting the reliability of the LLM-Crowdsourced approach.

<br /><br />Summary: <div>
arXiv:2507.22359v1 Announce Type: cross 
Abstract: Although large language models (LLMs) demonstrate remarkable capabilities across various tasks, evaluating their capabilities remains a challenging task. Existing evaluation methods suffer from issues such as data contamination, black-box operation, and subjective preference. These issues make it difficult to evaluate the LLMs' true capabilities comprehensively. To tackle these challenges, we propose a novel benchmark-free evaluation paradigm, LLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently, and evaluate mutually. This method integrates four key evaluation criteria: dynamic, transparent, objective, and professional, which existing evaluation methods cannot satisfy simultaneously. Experiments on eight mainstream LLMs across mathematics and programming verify the advantages of our method in distinguishing LLM performance. Furthermore, our study reveals several novel findings that are difficult for traditional methods to detect, including but not limited to: (1) Gemini demonstrates the highest original and professional question-design capabilities among others; (2) Some LLMs exhibit ''memorization-based answering'' by misrecognizing questions as familiar ones with a similar structure; (3) LLM evaluation results demonstrate high consistency (robustness).
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-trained Models Perform the Best When Token Distributions Follow Zipf's Law</title>
<link>https://arxiv.org/abs/2507.22543</link>
<guid>https://arxiv.org/abs/2507.22543</guid>
<content:encoded><![CDATA[
<div> fundamental step, natural language processing, vocabulary size, token frequency distributions, Zipf's law
Summary: 
Tokenization is a key aspect in natural language processing and other sequence modeling fields, with vocabulary size playing a significant role in model performance. This study introduces a systematic approach to determining the optimal vocabulary size by analyzing token frequency distributions based on Zipf's law. The research demonstrates a correlation between downstream task performance and adherence to power-law behavior, highlighting the benefits of aligning with Zipfian scaling for enhancing model efficiency and effectiveness. Through extensive experiments in various domains such as NLP, genomics, and chemistry, the study establishes Zipfian alignment as a reliable and broadly applicable criterion for selecting vocabulary size. <div>
arXiv:2507.22543v1 Announce Type: cross 
Abstract: Tokenization is a fundamental step in natural language processing (NLP) and other sequence modeling domains, where the choice of vocabulary size significantly impacts model performance. Despite its importance, selecting an optimal vocabulary size remains underexplored, typically relying on heuristics or dataset-specific choices. In this work, we propose a principled method for determining the vocabulary size by analyzing token frequency distributions through Zipf's law. We show that downstream task performance correlates with how closely token distributions follow power-law behavior, and that aligning with Zipfian scaling improves both model efficiency and effectiveness. Extensive experiments across NLP, genomics, and chemistry demonstrate that models consistently achieve peak performance when the token distribution closely adheres to Zipf's law, establishing Zipfian alignment as a robust and generalizable criterion for vocabulary size selection.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.22565</link>
<guid>https://arxiv.org/abs/2507.22565</guid>
<content:encoded><![CDATA[
<div> privacy, language models, differential privacy, stochastic gradient descent, reinforcement learning <br />
Summary:<br />
The article addresses the challenge of balancing data privacy and model utility when training large language models on sensitive datasets, particularly in healthcare. Traditional differentially private stochastic gradient descent (DP-SGD) methods suffer from a trade-off between privacy and accuracy due to gradient clipping and noise perturbation. The proposed framework, RLDP, leverages deep reinforcement learning to dynamically adjust per-parameter gradient-clipping thresholds and noise levels during model training. Through extensive experiments on various language models, RLDP demonstrates significant improvements in model performance, achieving up to 30.5% perplexity reduction and 5.6% downstream utility gain while maintaining privacy guarantees. The framework also shows faster convergence, requiring only a fraction of the gradient-update budget compared to baseline methods, and exhibits resilience against privacy attacks. RLDP presents a promising approach to enhancing the utility of language models while preserving data privacy. <br /> <div>
arXiv:2507.22565v1 Announce Type: cross 
Abstract: The tension between data privacy and model utility has become the defining bottleneck for the practical deployment of large language models (LLMs) trained on sensitive corpora including healthcare. Differentially private stochastic gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a pronounced cost: gradients are forcibly clipped and perturbed with noise, degrading sample efficiency and final accuracy. Numerous variants have been proposed to soften this trade-off, but they all share a handicap: their control knobs are hard-coded, global, and oblivious to the evolving optimization landscape. Consequently, practitioners are forced either to over-spend privacy budget in pursuit of utility, or to accept mediocre models in order to stay within privacy constraints. We present RLDP, the first framework to cast DP optimization itself as a closed-loop control problem amenable to modern deep reinforcement learning (RL). RLDP continuously senses rich statistics of the learning dynamics and acts by selecting fine-grained per parameter gradient-clipping thresholds as well as the magnitude of injected Gaussian noise. A soft actor-critic (SAC) hyper-policy is trained online during language model fine-tuning; it learns, from scratch, how to allocate the privacy budget where it matters and when it matters. Across more than 1,600 ablation experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream utility gain. RLDP reaches each baseline's final utility after only 13-43% of the gradient-update budget (mean speed-up 71%), all while honoring the same ($\epsilon$, $\delta$)-DP contract and exhibiting equal or lower susceptibility to membership-inference and canary-extraction attacks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2507.22607</link>
<guid>https://arxiv.org/abs/2507.22607</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, multimodal reasoning, Progressive Curriculum Reinforcement Learning, VL-Cogito, reasoning abilities

Summary: 
VL-Cogito is a novel multimodal reasoning model trained through a Progressive Curriculum Reinforcement Learning (PCuRL) framework, enhancing reasoning abilities in diverse contexts. The model is guided through tasks of increasing difficulty, improving performance. The framework includes an online difficulty soft weighting mechanism for dynamic training difficulty adjustment and a dynamic length reward mechanism for adaptively regulating reasoning path length based on task complexity. Experimental evaluations demonstrate VL-Cogito's superiority over existing models in mathematics, science, logic, and general understanding tasks, showcasing its effectiveness in enhancing reasoning capabilities. <br /><br />Summary: <div>
arXiv:2507.22607v1 Announce Type: cross 
Abstract: Reinforcement learning has proven its effectiveness in enhancing the reasoning capabilities of large language models. Recent research efforts have progressively extended this paradigm to multimodal reasoning tasks. Due to the inherent complexity and diversity of multimodal tasks, especially in semantic content and problem formulations, existing models often exhibit unstable performance across various domains and difficulty levels. To address these limitations, we propose VL-Cogito, an advanced multimodal reasoning model trained via a novel multi-stage Progressive Curriculum Reinforcement Learning (PCuRL) framework. PCuRL systematically guides the model through tasks of gradually increasing difficulty, substantially improving its reasoning abilities across diverse multimodal contexts. The framework introduces two key innovations: (1) an online difficulty soft weighting mechanism, dynamically adjusting training difficulty across successive RL training stages; and (2) a dynamic length reward mechanism, which encourages the model to adaptively regulate its reasoning path length according to task complexity, thus balancing reasoning efficiency with correctness. Experimental evaluations demonstrate that VL-Cogito consistently matches or surpasses existing reasoning-oriented models across mainstream multimodal benchmarks spanning mathematics, science, logic, and general understanding, validating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Next Tokens Denoising for Speech Synthesis</title>
<link>https://arxiv.org/abs/2507.22746</link>
<guid>https://arxiv.org/abs/2507.22746</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, autoregressive models, text-to-speech, Dragon-FM, zero-shot podcasts

Summary: 
Dragon-FM is a novel text-to-speech (TTS) model that combines the strengths of diffusion and autoregressive models. It processes audio codec tokens at a fast rate of 12.5 tokens per second, allowing for efficient generation of high-quality content. The model utilizes autoregressive modeling across chunks for global coherence and parallel flow-matching within chunks for fast denoising. By incorporating key-value caching and future context within each chunk, Dragon-FM addresses the limitations of both diffusion and autoregressive models. It also bridges the gap between continuous and discrete feature modeling, showcasing the ability to predict discrete tokens with finite scalar quantizers. The model's efficient codec and architecture make it particularly effective for generating extended content like zero-shot podcasts. Experiment demos on podcast datasets have shown the model's capability to generate high-quality content efficiently. 

<br /><br />Summary: <div>
arXiv:2507.22746v1 Announce Type: cross 
Abstract: While diffusion and autoregressive (AR) models have significantly advanced generative modeling, they each present distinct limitations. AR models, which rely on causal attention, cannot exploit future context and suffer from slow generation speeds. Conversely, diffusion models struggle with key-value (KV) caching. To overcome these challenges, we introduce Dragon-FM, a novel text-to-speech (TTS) design that unifies AR and flow-matching. This model processes 48 kHz audio codec tokens in chunks at a compact 12.5 tokens per second rate. This design enables AR modeling across chunks, ensuring global coherence, while parallel flow-matching within chunks facilitates fast iterative denoising. Consequently, the proposed model can utilize KV-cache across chunks and incorporate future context within each chunk. Furthermore, it bridges continuous and discrete feature modeling, demonstrating that continuous AR flow-matching can predict discrete tokens with finite scalar quantizers. This efficient codec and fast chunk-autoregressive architecture also makes the proposed model particularly effective for generating extended content. Experiment for demos of our work} on podcast datasets demonstrate its capability to efficiently generate high-quality zero-shot podcasts.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Incomplete Bridge: How AI Research (Mis)Engages with Psychology</title>
<link>https://arxiv.org/abs/2507.22847</link>
<guid>https://arxiv.org/abs/2507.22847</guid>
<content:encoded><![CDATA[
<div> cognition, artificial intelligence, psychology, interdisciplinary, research<br />
Summary:<br />
The study explores the interdisciplinary synergy between AI and psychology by analyzing LLM-related papers and psychology publications. Key patterns of interdisciplinary integration are identified, along with the most referenced psychology domains and underexplored areas. The operationalization and interpretation of psychology theories/frameworks in AI are examined, highlighting common types of misapplication. The study offers guidance for effective incorporation of psychology in AI systems, providing a comprehensive map of interdisciplinary engagement to facilitate deeper collaboration and advance AI research.<br /> <div>
arXiv:2507.22847v1 Announce Type: cross 
Abstract: Social sciences have accumulated a rich body of theories and methodologies for investigating the human mind and behaviors, while offering valuable insights into the design and understanding of Artificial Intelligence (AI) systems. Focusing on psychology as a prominent case, this study explores the interdisciplinary synergy between AI and the field by analyzing 1,006 LLM-related papers published in premier AI venues between 2023 and 2025, along with the 2,544 psychology publications they cite. Through our analysis, we identify key patterns of interdisciplinary integration, locate the psychology domains most frequently referenced, and highlight areas that remain underexplored. We further examine how psychology theories/frameworks are operationalized and interpreted, identify common types of misapplication, and offer guidance for more effective incorporation. Our work provides a comprehensive map of interdisciplinary engagement between AI and psychology, thereby facilitating deeper collaboration and advancing AI systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoOutageKG: A Multimodal Geospatiotemporal Knowledge Graph for Multiresolution Power Outage Analysis</title>
<link>https://arxiv.org/abs/2507.22878</link>
<guid>https://arxiv.org/abs/2507.22878</guid>
<content:encoded><![CDATA[
<div> nighttime light satellite data, power outages, grid risk assessment, disaster mitigation, multimodal data integration
<br />
Summary:
GeoOutageKG is a new multimodal knowledge graph designed for detecting, analyzing, and predicting power outages, particularly during extreme weather events like hurricanes. By combining diverse data sources such as nighttime light satellite images, spatiotemporal power outage maps, and county-level outage reports, GeoOutageKG provides a comprehensive and detailed representation of power outages in the United States. With over 10.6 million outage records, 300,000 NTL images, and 15,000 outage maps integrated into the knowledge graph, GeoOutageKG offers a robust platform for analyzing power outages at different spatial and temporal resolutions. Its modular and reusable structure enables efficient data integration and supports multiresolution analysis of geospatiotemporal power outages. <div>
arXiv:2507.22878v1 Announce Type: cross 
Abstract: Detecting, analyzing, and predicting power outages is crucial for grid risk assessment and disaster mitigation. Numerous outages occur each year, exacerbated by extreme weather events such as hurricanes. Existing outage data are typically reported at the county level, limiting their spatial resolution and making it difficult to capture localized patterns. However, it offers excellent temporal granularity. In contrast, nighttime light satellite image data provides significantly higher spatial resolution and enables a more comprehensive spatial depiction of outages, enhancing the accuracy of assessing the geographic extent and severity of power loss after disaster events. However, these satellite data are only available on a daily basis. Integrating spatiotemporal visual and time-series data sources into a unified knowledge representation can substantially improve power outage detection, analysis, and predictive reasoning. In this paper, we propose GeoOutageKG, a multimodal knowledge graph that integrates diverse data sources, including nighttime light satellite image data, high-resolution spatiotemporal power outage maps, and county-level timeseries outage reports in the U.S. We describe our method for constructing GeoOutageKG by aligning source data with a developed ontology, GeoOutageOnto. Currently, GeoOutageKG includes over 10.6 million individual outage records spanning from 2014 to 2024, 300,000 NTL images spanning from 2012 to 2024, and 15,000 outage maps. GeoOutageKG is a novel, modular and reusable semantic resource that enables robust multimodal data integration. We demonstrate its use through multiresolution analysis of geospatiotemporal power outages.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RecGPT Technical Report</title>
<link>https://arxiv.org/abs/2507.22879</link>
<guid>https://arxiv.org/abs/2507.22879</guid>
<content:encoded><![CDATA[
arXiv:2507.22879v1 Announce Type: cross 
Abstract: Recommender systems are among the most impactful applications of artificial intelligence, serving as critical infrastructure connecting users, merchants, and platforms. However, most current industrial systems remain heavily reliant on historical co-occurrence patterns and log-fitting objectives, i.e., optimizing for past user interactions without explicitly modeling user intent. This log-fitting approach often leads to overfitting to narrow historical preferences, failing to capture users' evolving and latent interests. As a result, it reinforces filter bubbles and long-tail phenomena, ultimately harming user experience and threatening the sustainability of the whole recommendation ecosystem.
  To address these challenges, we rethink the overall design paradigm of recommender systems and propose RecGPT, a next-generation framework that places user intent at the center of the recommendation pipeline. By integrating large language models (LLMs) into key stages of user interest mining, item retrieval, and explanation generation, RecGPT transforms log-fitting recommendation into an intent-centric process. To effectively align general-purpose LLMs to the above domain-specific recommendation tasks at scale, RecGPT incorporates a multi-stage training paradigm, which integrates reasoning-enhanced pre-alignment and self-training evolution, guided by a Human-LLM cooperative judge system. Currently, RecGPT has been fully deployed on the Taobao App. Online experiments demonstrate that RecGPT achieves consistent performance gains across stakeholders: users benefit from increased content diversity and satisfaction, merchants and the platform gain greater exposure and conversions. These comprehensive improvement results across all stakeholders validates that LLM-driven, intent-centric design can foster a more sustainable and mutually beneficial recommendation ecosystem.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Law of Capacity Gap in Distilling Language Models</title>
<link>https://arxiv.org/abs/2311.07052</link>
<guid>https://arxiv.org/abs/2311.07052</guid>
<content:encoded><![CDATA[
arXiv:2311.07052v4 Announce Type: replace 
Abstract: Language model (LM) distillation aims at distilling the knowledge in a large teacher LM to a small student one. As a critical issue facing LM distillation, a superior student often arises from a teacher of a relatively small scale instead of a larger one, especially in the presence of substantial capacity gap between the teacher and student. This issue, often referred to as the \textit{curse of capacity gap}, suggests that there is likely an optimal teacher yielding the best-performing student along the scaling course of the teacher. Consequently, distillation trials on teachers of a wide range of scales are called for to determine the optimal teacher, which becomes computationally intensive in the context of large LMs (LLMs). This paper addresses this critical bottleneck by providing the \textit{law of capacity gap} inducted from a preliminary study on distilling a broad range of small-scale (<3B) LMs, where the optimal teacher consistently scales linearly with the student scale across different model and data scales. By extending the law to LLM distillation on a larger scale (7B), we succeed in obtaining versatile LLMs that outperform a wide array of competitors.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction-tuned Large Language Models for Machine Translation in the Medical Domain</title>
<link>https://arxiv.org/abs/2408.16440</link>
<guid>https://arxiv.org/abs/2408.16440</guid>
<content:encoded><![CDATA[
arXiv:2408.16440v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown promising results on machine translation for high resource language pairs and domains. However, in specialised domains (e.g. medical) LLMs have shown lower performance compared to standard neural machine translation models. The consistency in the machine translation of terminology is crucial for users, researchers, and translators in specialised domains. In this study, we compare the performance between baseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we introduce terminology from specialised medical dictionaries into the instruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs significantly outperform the baseline models with automatic metrics.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Past Meets Present: Creating Historical Analogy with Large Language Models</title>
<link>https://arxiv.org/abs/2409.14820</link>
<guid>https://arxiv.org/abs/2409.14820</guid>
<content:encoded><![CDATA[
arXiv:2409.14820v2 Announce Type: replace 
Abstract: Historical analogies, which compare known past events with contemporary but unfamiliar events, are important abilities that help people make decisions and understand the world. However, research in applied history suggests that people have difficulty finding appropriate analogies. And previous studies in the AI community have also overlooked historical analogies. To fill this gap, in this paper, we focus on the historical analogy acquisition task, which aims to acquire analogous historical events for a given event. We explore retrieval and generation methods for acquiring historical analogies based on different large language models (LLMs). Furthermore, we propose a self-reflection method to mitigate hallucinations and stereotypes when LLMs generate historical analogies. Through human evaluations and our specially designed automatic multi-dimensional assessment, we find that LLMs generally have a good potential for historical analogies. And the performance of the models can be further improved by using our self-reflection method.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neutral Residues: Revisiting Adapters for Model Extension</title>
<link>https://arxiv.org/abs/2410.02744</link>
<guid>https://arxiv.org/abs/2410.02744</guid>
<content:encoded><![CDATA[
arXiv:2410.02744v2 Announce Type: replace 
Abstract: We address the problem of extending a pretrained large language model to a new domain that was not seen during training. Standard techniques, such as finetuning or low-rank adaptation (LoRA) are successful at domain adaptation, but do not formally add capacity to the model. This often leads to a trade-off, between performing well on the new domain vs. degrading performance on the original domain. Here, we revisit and improve adapters to extend LLMs from three angles: data, architecture and training procedure, which are advantageously considered jointly. The resulting method, called neutral residues, modifies adapters in a way that leads each new residual block to output near-zeros on the original domain. This solution leads to strong results when adapting a state-of-the-art model originally trained on English to a new language. Neutral residues significantly outperform competing approaches such as finetuning, LoRA or vanilla adapters in terms of the trade-off between learning the new language and not forgetting English.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges</title>
<link>https://arxiv.org/abs/2410.21306</link>
<guid>https://arxiv.org/abs/2410.21306</guid>
<content:encoded><![CDATA[
arXiv:2410.21306v3 Announce Type: replace 
Abstract: Natural Language Processing (NLP) is revolutionising the way both professionals and laypersons operate in the legal field. The considerable potential for NLP in the legal sector, especially in developing computational assistance tools for various legal processes, has captured the interest of researchers for years. This survey follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses framework, reviewing 154 studies, with a final selection of 131 after manual filtering. It explores foundational concepts related to NLP in the legal domain, illustrating the unique aspects and challenges of processing legal texts, such as extensive document lengths, complex language, and limited open legal datasets. We provide an overview of NLP tasks specific to legal text, such as Document Summarisation, Named Entity Recognition, Question Answering, Argument Mining, Text Classification, and Judgement Prediction. Furthermore, we analyse both developed legal-oriented language models, and approaches for adapting general-purpose language models to the legal domain. Additionally, we identify sixteen open research challenges, including the detection and mitigation of bias in artificial intelligence applications, the need for more robust and interpretable models, and improving explainability to handle the complexities of legal language and reasoning.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Yankari: A Monolingual Yoruba Dataset</title>
<link>https://arxiv.org/abs/2412.03334</link>
<guid>https://arxiv.org/abs/2412.03334</guid>
<content:encoded><![CDATA[
arXiv:2412.03334v2 Announce Type: replace 
Abstract: This paper presents Yankari, a large-scale monolingual dataset for the Yoruba language, aimed at addressing the critical gap in Natural Language Processing (NLP) resources for this important West African language. Despite being spoken by over 30 million people, Yoruba has been severely underrepresented in NLP research and applications. We detail our methodology for creating this dataset, which includes careful source selection, automated quality control, and rigorous data cleaning processes. The Yankari dataset comprises 51,407 documents from 13 diverse sources, totaling over 30 million tokens. Our approach focuses on ethical data collection practices, avoiding problematic sources and addressing issues prevalent in existing datasets. We provide thorough automated evaluations of the dataset, demonstrating its quality compared to existing resources. The Yankari dataset represents a significant advancement in Yoruba language resources, providing a foundation for developing more accurate NLP models, supporting comparative linguistic studies, and contributing to the digital accessibility of the Yoruba language.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Continual Learning for Small Language Models with a Discrete Key-Value Bottleneck</title>
<link>https://arxiv.org/abs/2412.08528</link>
<guid>https://arxiv.org/abs/2412.08528</guid>
<content:encoded><![CDATA[
arXiv:2412.08528v2 Announce Type: replace 
Abstract: Continual learning remains a challenge across various natural language processing (NLP) tasks, as models updated with new training data often risk catastrophic forgetting of previously acquired knowledge. We introduce a discrete key-value bottleneck (DKVB) for encoder-only language models, enabling efficient continual learning through localized updates. Inspired by a discrete key-value bottleneck in vision, we consider new and NLP-specific challenges. We compare different bottleneck architectures for NLP and introduce a new, task-independent initialization technique for the discrete keys. We evaluate our DKVB for NLP in four continual learning scenarios and show that it alleviates catastrophic forgetting. Our experiments demonstrate that the proposed approach achieves competitive performance compared to popular continual learning methods while incurring lower computational costs. Furthermore, we show that DKVB remains effective even in challenging single-head continual learning scenarios where no task ID is provided.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling</title>
<link>https://arxiv.org/abs/2412.14373</link>
<guid>https://arxiv.org/abs/2412.14373</guid>
<content:encoded><![CDATA[
arXiv:2412.14373v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated exceptional versatility across domains, including applications to electrocardiograms (ECGs). A growing body of work focuses on generating text from multi-channeled ECG signals and corresponding textual prompts. Existing approaches often involve a two-stage process: pretraining an ECG-specific encoder with a self-supervised learning (SSL) objective, followed by finetuning an LLM for natural language generation (NLG) using encoder-derived features. However, these methods face two key limitations: inefficiency due to multi-stage training and challenges in interpreting encoder-generated features. To overcome these issues, we propose ECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for autoregressive language modeling of ECGs. ECG-Byte compresses and encodes ECG signals into tokens, enabling direct end-to-end LLM training by combining ECG and text tokens. This approach enhances interpretability, as ECG tokens can be directly mapped back to the original signals. Leveraging ECG-Byte, we achieve competitive NLG performance while training 3 times faster and using just 48\% of the data required by traditional two-stage methods.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Story Expectations to Understand Engagement: A Generative Framework Using LLMs</title>
<link>https://arxiv.org/abs/2412.15239</link>
<guid>https://arxiv.org/abs/2412.15239</guid>
<content:encoded><![CDATA[
arXiv:2412.15239v3 Announce Type: replace 
Abstract: Understanding when and why consumers engage with stories is crucial for content creators and platforms. While existing theories suggest that audience beliefs of what is going to happen should play an important role in engagement decisions, empirical work has mostly focused on developing techniques to directly extract features from actual content, rather than capturing forward-looking beliefs, due to the lack of a principled way to model such beliefs in unstructured narrative data. To complement existing feature extraction techniques, this paper introduces a novel framework that leverages large language models to model audience forward-looking beliefs about how stories might unfold. Our method generates multiple potential continuations for each story and extracts features related to expectations, uncertainty, and surprise using established content analysis techniques. Applying our method to over 30,000 book chapters, we demonstrate that our framework complements existing feature engineering techniques by amplifying their marginal explanatory power on average by 31%. The results reveal that different types of engagement-continuing to read, commenting, and voting-are driven by distinct combinations of current and anticipated content features. Our framework provides a novel way to study and explore how audience forward-looking beliefs shape their engagement with narrative media, with implications for marketing strategy in content-focused industries.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rationale-guided Prompting for Knowledge-based Visual Question Answering</title>
<link>https://arxiv.org/abs/2412.16936</link>
<guid>https://arxiv.org/abs/2412.16936</guid>
<content:encoded><![CDATA[
arXiv:2412.16936v2 Announce Type: replace 
Abstract: Recently, Large Language Models (LLMs) have been used for knowledge-based Visual Question Answering (VQA). Despite the encouraging results of previous studies, prior methods prompt LLMs to predict answers directly, neglecting intermediate thought processes. We argue that prior methods do not sufficiently activate the capacities of LLMs. We propose a framework called PLRH that Prompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH prompts LLMs with Chain of Thought (CoT) to generate rationale heuristics, i.e., intermediate thought processes, and then leverages the rationale heuristics to inspire LLMs to predict answers. Experiments show that our approach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA and A-OKVQA, respectively.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FineMedLM-o1: Enhancing Medical Knowledge Reasoning Ability of LLM from Supervised Fine-Tuning to Test-Time Training</title>
<link>https://arxiv.org/abs/2501.09213</link>
<guid>https://arxiv.org/abs/2501.09213</guid>
<content:encoded><![CDATA[
arXiv:2501.09213v3 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have shown promise in medical applications such as disease diagnosis and treatment planning. However, most existing medical LLMs struggle with the deep reasoning required for complex medical problems, such as differential diagnosis and medication recommendations. We propose FineMedLM-o1, which leverages high-quality medical synthetic data and long-form reasoning data for Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), enabling advanced dialogue and deep reasoning capabilities. Additionally, we introduce Test-Time Training (TTT) in the medical domain for the first time, facilitating domain adaptation and ensuring reliable, accurate reasoning. Experimental results demonstrate that FineMedLM-o1 achieves a 23% average performance improvement over prior models on key medical benchmarks. Furthermore, the introduction of TTT provides an additional 14% performance boost, highlighting its effectiveness in enhancing medical reasoning capabilities. To support this process, we also propose a novel method for synthesizing medical dialogue. Compared to other open-source datasets, our dataset stands out as superior in both quality and complexity. The project and data will be released on GitHub.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GneissWeb: Preparing High Quality Data for LLMs at Scale</title>
<link>https://arxiv.org/abs/2502.14907</link>
<guid>https://arxiv.org/abs/2502.14907</guid>
<content:encoded><![CDATA[
arXiv:2502.14907v2 Announce Type: replace 
Abstract: Data quantity and quality play a vital role in determining the performance of Large Language Models (LLMs). High-quality data, in particular, can significantly boost the LLM's ability to generalize on a wide range of downstream tasks. Large pre-training datasets for leading LLMs remain inaccessible to the public, whereas many open datasets are small in size (less than 5 trillion tokens), limiting their suitability for training large models.
  In this paper, we introduce GneissWeb, a large dataset yielding around 10 trillion tokens that caters to the data quality and quantity requirements of training LLMs. Our GneissWeb recipe that produced the dataset consists of sharded exact sub-string deduplication and a judiciously constructed ensemble of quality filters. GneissWeb achieves a favorable trade-off between data quality and quantity, producing models that outperform models trained on state-of-the-art open large datasets (5+ trillion tokens).
  We show that models trained using GneissWeb dataset outperform those trained on FineWeb-V1.1.0 by 2.73 percentage points in terms of average score computed on a set of 11 commonly used benchmarks (both zero-shot and few-shot) for pre-training dataset evaluation. When the evaluation set is extended to 20 benchmarks (both zero-shot and few-shot), models trained using GneissWeb still achieve a 1.75 percentage points advantage over those trained on FineWeb-V1.1.0.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QE4PE: Word-level Quality Estimation for Human Post-Editing</title>
<link>https://arxiv.org/abs/2503.03044</link>
<guid>https://arxiv.org/abs/2503.03044</guid>
<content:encoded><![CDATA[
arXiv:2503.03044v2 Announce Type: replace 
Abstract: Word-level quality estimation (QE) methods aim to detect erroneous spans in machine translations, which can direct and facilitate human post-editing. While the accuracy of word-level QE systems has been assessed extensively, their usability and downstream influence on the speed, quality and editing choices of human post-editing remain understudied. In this study, we investigate the impact of word-level QE on machine translation (MT) post-editing in a realistic setting involving 42 professional post-editors across two translation directions. We compare four error-span highlight modalities, including supervised and uncertainty-based word-level QE methods, for identifying potential errors in the outputs of a state-of-the-art neural MT model. Post-editing effort and productivity are estimated from behavioral logs, while quality improvements are assessed by word- and segment-level human annotation. We find that domain, language and editors' speed are critical factors in determining highlights' effectiveness, with modest differences between human-made and automated QE highlights underlining a gap between accuracy and usability in professional workflows.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Modal State-Space Graph Reasoning for Structured Summarization</title>
<link>https://arxiv.org/abs/2503.20988</link>
<guid>https://arxiv.org/abs/2503.20988</guid>
<content:encoded><![CDATA[
arXiv:2503.20988v2 Announce Type: replace 
Abstract: The ability to extract compact, meaningful summaries from large-scale and multimodal data is critical for numerous applications, ranging from video analytics to medical reports. Prior methods in cross-modal summarization have often suffered from high computational overheads and limited interpretability. In this paper, we propose a \textit{Cross-Modal State-Space Graph Reasoning} (\textbf{CSS-GR}) framework that incorporates a state-space model with graph-based message passing, inspired by prior work on efficient state-space models. Unlike existing approaches relying on purely sequential models, our method constructs a graph that captures inter- and intra-modal relationships, allowing more holistic reasoning over both textual and visual streams. We demonstrate that our approach significantly improves summarization quality and interpretability while maintaining computational efficiency, as validated on standard multimodal summarization benchmarks. We also provide a thorough ablation study to highlight the contributions of each component.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Reverse Inconsistency: LLM Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing</title>
<link>https://arxiv.org/abs/2504.01282</link>
<guid>https://arxiv.org/abs/2504.01282</guid>
<content:encoded><![CDATA[
arXiv:2504.01282v2 Announce Type: replace 
Abstract: While the inconsistency of LLMs is not a novel topic, prior research has predominantly addressed two types of generative inconsistencies: i) Randomness Inconsistency: running the same LLM multiple trials, yielding varying responses; ii) Paraphrase Inconsistency: paraphrased prompts result in different responses from the same LLM. Randomness Inconsistency arises from the inherent randomness due to stochastic sampling in generative models, while Paraphrase Inconsistency is a consequence of the language modeling objectives, where paraphrased prompts alter the distribution of vocabulary logits. This research discovers Prompt-Reverse Inconsistency (PRIN), a new form of LLM self-inconsistency: given a question and a couple of LLM-generated answer candidates, the LLM often has conflicting responses when prompted "Which are correct answers?" and "Which are incorrect answers?". PRIN poses a big concern as it undermines the credibility of LLM-as-a-judge, and suggests a challenge for LLMs to adhere to basic logical rules. We conduct a series of experiments to investigate PRIN, examining the extent of PRIN across different LLMs, methods to mitigate it, potential applications, and its relationship with Randomness Inconsistency and Paraphrase Inconsistency. As the first study to explore PRIN, our findings offer valuable insights into the inner workings of LLMs and contribute to advancing trustworthy AI.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voices of Freelance Professional Writers on AI: Limitations, Expectations, and Fears</title>
<link>https://arxiv.org/abs/2504.05008</link>
<guid>https://arxiv.org/abs/2504.05008</guid>
<content:encoded><![CDATA[
arXiv:2504.05008v2 Announce Type: replace 
Abstract: The rapid development of AI-driven tools, particularly large language models (LLMs), is reshaping professional writing. Still, key aspects of their adoption such as languages support, ethics, and long-term impact on writers voice and creativity remain underexplored. In this work, we conducted a questionnaire (N = 301) and an interactive survey (N = 36) targeting professional writers regularly using AI. We examined LLM-assisted writing practices across 25+ languages, ethical concerns, and user expectations. The findings of the survey demonstrate important insights, reflecting upon the importance of: LLMs adoption for non-English speakers; the degree of misinformation, domain and style adaptation; usability and key features of LLMs. These insights can guide further development, benefiting both writers and a broader user base.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D\'ej\`a Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation</title>
<link>https://arxiv.org/abs/2504.11829</link>
<guid>https://arxiv.org/abs/2504.11829</guid>
<content:encoded><![CDATA[
arXiv:2504.11829v3 Announce Type: replace 
Abstract: Generation capabilities and language coverage of multilingual large language models (mLLMs) are advancing rapidly. However, evaluation practices for generative abilities of mLLMs are still lacking comprehensiveness, scientific rigor, and consistent adoption across research labs, which undermines their potential to meaningfully guide mLLM development. We draw parallels with machine translation (MT) evaluation, a field that faced similar challenges and has, over decades, developed transparent reporting standards and reliable evaluations for multilingual generative models. Through targeted experiments across key stages of the generative evaluation pipeline, we demonstrate how best practices from MT evaluation can deepen the understanding of quality differences between models. Additionally, we identify essential components for robust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are rigorously assessed. We distill these insights into a checklist of actionable recommendations for mLLM research and development.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition</title>
<link>https://arxiv.org/abs/2505.00059</link>
<guid>https://arxiv.org/abs/2505.00059</guid>
<content:encoded><![CDATA[
arXiv:2505.00059v2 Announce Type: replace 
Abstract: Some speech recognition tasks, such as automatic speech recognition (ASR), are approaching or have reached human performance in many reported metrics. Yet, they continue to struggle in complex, real-world, situations, such as with distanced speech. Previous challenges have released datasets to address the issue of distanced ASR, however, the focus remains primarily on distance, specifically relying on multi-microphone array systems. Here we present the B(asic) E(motion) R(andom phrase) S(hou)t(s) (BERSt) dataset. The dataset contains almost 4 hours of English speech from 98 actors with varying regional and non-native accents. The data was collected on smartphones in the actors homes and therefore includes at least 98 different acoustic environments. The data also includes 7 different emotion prompts and both shouted and spoken utterances. The smartphones were places in 19 different positions, including obstructions and being in a different room than the actor. This data is publicly available for use and can be used to evaluate a variety of speech recognition tasks, including: ASR, shout detection, and speech emotion recognition (SER). We provide initial benchmarks for ASR and SER tasks, and find that ASR degrades both with an increase in distance and shout level and shows varied performance depending on the intended emotion. Our results show that the BERSt dataset is challenging for both ASR and SER tasks and continued work is needed to improve the robustness of such systems for more accurate real-world use.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2505.08450</link>
<guid>https://arxiv.org/abs/2505.08450</guid>
<content:encoded><![CDATA[
arXiv:2505.08450v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a way to complement the in-context knowledge of Large Language Models (LLMs) by integrating external documents. However, real-world applications demand not only accuracy but also interpretability. While dense retrieval methods provide high accuracy, they lack interpretability; conversely, sparse retrieval methods offer transparency but often fail to capture the full intent of queries due to their reliance on keyword matching. To address these issues, we introduce IterKey, an LLM-driven iterative keyword generation framework that enhances RAG via sparse retrieval. IterKey consists of three LLM-driven stages: generating keywords for retrieval, generating answers based on retrieved documents, and validating the answers. If validation fails, the process iteratively repeats with refined keywords. Across four QA tasks, experimental results show that IterKey achieves 5% to 20% accuracy improvements over BM25-based RAG and simple baselines. Its performance is comparable to dense retrieval-based RAG and prior iterative query refinement methods using dense models. In summary, IterKey is a novel BM25-based approach leveraging LLMs to iteratively refine RAG, effectively balancing accuracy with interpretability.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Are They Talking About? A Benchmark of Knowledge-Grounded Discussion Summarization</title>
<link>https://arxiv.org/abs/2505.12474</link>
<guid>https://arxiv.org/abs/2505.12474</guid>
<content:encoded><![CDATA[
arXiv:2505.12474v2 Announce Type: replace 
Abstract: Traditional dialogue summarization primarily focuses on dialogue content, assuming it comprises adequate information for a clear summary. However, this assumption often fails for discussions grounded in shared background, where participants frequently omit context and use implicit references. This results in summaries that are confusing to readers unfamiliar with the background. To address this, we introduce Knowledge-Grounded Discussion Summarization (KGDS), a novel task that produces a supplementary background summary for context and a clear opinion summary with clarified references. To facilitate research, we construct the first KGDS benchmark, featuring news-discussion pairs and expert-created multi-granularity gold annotations for evaluating sub-summaries. We also propose a novel hierarchical evaluation framework with fine-grained and interpretable metrics. Our extensive evaluation of 12 advanced large language models (LLMs) reveals that KGDS remains a significant challenge. The models frequently miss key facts and retain irrelevant ones in background summarization, and often fail to resolve implicit references in opinion summary integration.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering</title>
<link>https://arxiv.org/abs/2505.15038</link>
<guid>https://arxiv.org/abs/2505.15038</guid>
<content:encoded><![CDATA[
arXiv:2505.15038v2 Announce Type: replace 
Abstract: Linear concept vectors effectively steer LLMs, but existing methods suffer from noisy features in diverse datasets that undermine steering robustness. We propose Sparse Autoencoder-Denoised Concept Vectors (SDCV), which selectively keep the most discriminative SAE latents while reconstructing hidden representations. Our key insight is that concept-relevant signals can be explicitly separated from dataset noise by scaling up activations of top-k latents that best differentiate positive and negative samples. Applied to linear probing and difference-in-mean, SDCV consistently improves steering success rates by 4-16\% across six challenging concepts, while maintaining topic relevance.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2505.19959</link>
<guid>https://arxiv.org/abs/2505.19959</guid>
<content:encoded><![CDATA[
arXiv:2505.19959v2 Announce Type: replace 
Abstract: Long Context Understanding (LCU) is a critical area for exploration in current large language models (LLMs). However, due to the inherently lengthy nature of long-text data, existing LCU benchmarks for LLMs often result in prohibitively high evaluation costs, like testing time and inference expenses. Through extensive experimentation, we discover that existing LCU benchmarks exhibit significant redundancy, which means the inefficiency in evaluation. In this paper, we propose a concise data compression method tailored for long-text data with sparse information characteristics. By pruning the well-known LCU benchmark LongBench, we create MiniLongBench. This benchmark includes only 237 test samples across six major task categories and 21 distinct tasks. Through empirical analysis of over 60 LLMs, MiniLongBench achieves an average evaluation cost reduced to only 4.5% of the original while maintaining an average rank correlation coefficient of 0.97 with LongBench results. Therefore, our MiniLongBench, as a low-cost benchmark, holds great potential to substantially drive future research into the LCU capabilities of LLMs. See https://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.21354</link>
<guid>https://arxiv.org/abs/2505.21354</guid>
<content:encoded><![CDATA[
arXiv:2505.21354v2 Announce Type: replace 
Abstract: Solving Bengali Math Word Problems (MWPs) remains a major challenge in natural language processing (NLP) due to the language's low-resource status and the multi-step reasoning required. Existing models struggle with complex Bengali MWPs, largely because no human-annotated Bengali dataset has previously addressed this task. This gap has limited progress in Bengali mathematical reasoning. To address this, we created SOMADHAN, a dataset of 8792 complex Bengali MWPs with manually written, step-by-step solutions. We designed this dataset to support reasoning-focused evaluation and model development in a linguistically underrepresented context. Using SOMADHAN, we evaluated a range of large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series models, Deepseek, and Qwen - through both zero-shot and few-shot prompting with and without Chain of Thought (CoT) reasoning. CoT prompting consistently improved performance over standard prompting, especially in tasks requiring multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with few-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune models efficiently, enabling them to adapt to Bengali MWPs with minimal computational cost. Our work fills a critical gap in Bengali NLP by providing a high-quality reasoning dataset and a scalable framework for solving complex MWPs. We aim to advance equitable research in low-resource languages and enhance reasoning capabilities in educational and language technologies.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MuSciClaims: Multimodal Scientific Claim Verification</title>
<link>https://arxiv.org/abs/2506.04585</link>
<guid>https://arxiv.org/abs/2506.04585</guid>
<content:encoded><![CDATA[
arXiv:2506.04585v2 Announce Type: replace 
Abstract: Assessing scientific claims requires identifying, extracting, and reasoning with multimodal data expressed in information-rich figures in scientific literature. Despite the large body of work in scientific QA, figure captioning, and other multimodal reasoning tasks over chart-based data, there are no readily usable multimodal benchmarks that directly test claim verification abilities. To remedy this gap, we introduce a new benchmark MuSciClaims accompanied by diagnostics tasks. We automatically extract supported claims from scientific articles, which we manually perturb to produce contradicted claims. The perturbations are designed to test for a specific set of claim verification capabilities. We also introduce a suite of diagnostic tasks that help understand model failures. Our results show most vision-language models are poor (~0.3-0.5 F1), with even the best model only achieving 0.72 F1. They are also biased towards judging claims as supported, likely misunderstanding nuanced perturbations within the claims. Our diagnostics show models are bad at localizing correct evidence within figures, struggle with aggregating information across modalities, and often fail to understand basic components of the figure.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-as-a-qualitative-judge: automating error analysis in natural language generation</title>
<link>https://arxiv.org/abs/2506.09147</link>
<guid>https://arxiv.org/abs/2506.09147</guid>
<content:encoded><![CDATA[
arXiv:2506.09147v2 Announce Type: replace 
Abstract: Prompting large language models (LLMs) to evaluate generated text, known as LLM-as-a-judge, has become a standard evaluation approach in natural language generation (NLG), but is primarily used as a quantitative tool, i.e. with numerical scores as main outputs. In this work, we propose LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main output being a structured report of common issue types in the NLG system outputs. Our approach is targeted at providing developers with meaningful insights on what improvements can be done to a given NLG system and consists of two main steps, namely open-ended per-instance issue analysis and clustering of the discovered issues using an intuitive cumulative algorithm. We also introduce a strategy for evaluating the proposed approach, coupled with ~300 annotations of issues in instances from 12 NLG datasets. Our results show that LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3 cases and is capable of producing error type reports resembling the reports composed by human annotators. Our code and data are publicly available at https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanations</title>
<link>https://arxiv.org/abs/2506.19073</link>
<guid>https://arxiv.org/abs/2506.19073</guid>
<content:encoded><![CDATA[
arXiv:2506.19073v2 Announce Type: replace 
Abstract: Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is a growing concern as these systems are used in socially sensitive tasks. Nevertheless, current evaluation benchmarks present two major shortcomings: a lack of annotations that justify moral classifications, which limits transparency and interpretability; and a predominant focus on English, which constrains the assessment of moral reasoning across diverse cultural settings. In this paper, we introduce MFTCXplain, a multilingual benchmark dataset for evaluating the moral reasoning of LLMs via hate speech multi-hop explanation using Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across Portuguese, Italian, Persian, and English, annotated with binary hate speech labels, moral categories, and text span-level rationales. Empirical results highlight a misalignment between LLM outputs and human annotations in moral reasoning tasks. While LLMs perform well in hate speech detection (F1 up to 0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35). Furthermore, rationale alignment remains limited mainly in underrepresented languages. These findings show the limited capacity of current LLMs to internalize and reflect human moral reasoning.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions</title>
<link>https://arxiv.org/abs/2404.07214</link>
<guid>https://arxiv.org/abs/2404.07214</guid>
<content:encoded><![CDATA[
arXiv:2404.07214v3 Announce Type: replace-cross 
Abstract: The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced models are instrumental in tackling more intricate tasks such as image captioning and visual question answering. In our comprehensive survey paper, we delve into the key advancements within the realm of VLMs. Our classification organizes VLMs into three distinct categories: models dedicated to vision-language understanding, models that process multimodal inputs to generate unimodal (textual) outputs and models that both accept and produce multimodal inputs and outputs.This classification is based on their respective capabilities and functionalities in processing and generating various modalities of data.We meticulously dissect each model, offering an extensive analysis of its foundational architecture, training data sources, as well as its strengths and limitations wherever possible, providing readers with a comprehensive understanding of its essential components. We also analyzed the performance of VLMs in various benchmark datasets. By doing so, we aim to offer a nuanced understanding of the diverse landscape of VLMs. Additionally, we underscore potential avenues for future research in this dynamic domain, anticipating further breakthroughs and advancements.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mmm whatcha say? Uncovering distal and proximal context effects in first and second-language word perception using psychophysical reverse correlation</title>
<link>https://arxiv.org/abs/2406.05515</link>
<guid>https://arxiv.org/abs/2406.05515</guid>
<content:encoded><![CDATA[
arXiv:2406.05515v2 Announce Type: replace-cross 
Abstract: Acoustic context effects, where surrounding changes in pitch, rate or timbre influence the perception of a sound, are well documented in speech perception, but how they interact with language background remains unclear. Using a reverse-correlation approach, we systematically varied the pitch and speech rate in phrases around different pairs of vowels for second language (L2) speakers of English (/i/-/I/) and French (/u/-/y/), thus reconstructing, in a data-driven manner, the prosodic profiles that bias their perception. Testing English and French speakers (n=25), we showed that vowel perception is in fact influenced by conflicting effects from the surrounding pitch and speech rate: a congruent proximal effect 0.2s pre-target and a distal contrastive effect up to 1s before; and found that L1 and L2 speakers exhibited strikingly similar prosodic profiles in perception. We provide a novel method to investigate acoustic context effects across stimuli, timescales, and acoustic domain.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Positive-Augmented Contrastive Learning for Vision-and-Language Evaluation and Training</title>
<link>https://arxiv.org/abs/2410.07336</link>
<guid>https://arxiv.org/abs/2410.07336</guid>
<content:encoded><![CDATA[
arXiv:2410.07336v2 Announce Type: replace-cross 
Abstract: Despite significant advancements in caption generation, existing evaluation metrics often fail to capture the full quality or fine-grained details of captions. This is mainly due to their reliance on non-specific human-written references or noisy pre-training data. Still, finding an effective metric is crucial not only for captions evaluation but also for the generation phase. Metrics can indeed play a key role in the fine-tuning stage of captioning models, ultimately enhancing the quality of the generated captions. In this paper, we propose PAC-S++, a learnable metric that leverages the CLIP model, pre-trained on both web-collected and cleaned data and regularized through additional pairs of generated visual and textual positive samples. Exploiting this stronger and curated pre-training, we also apply PAC-S++ as a reward in the Self-Critical Sequence Training (SCST) stage typically employed to fine-tune captioning models. Extensive experiments on different image and video datasets highlight the effectiveness of PAC-S++ compared to popular metrics for the task, including its sensitivity to object hallucinations. Furthermore, we show that integrating PAC-S++ into the fine-tuning stage of a captioning model results in semantically richer captions with fewer repetitions and grammatical errors. Evaluations on out-of-domain benchmarks further demonstrate the efficacy of our fine-tuning approach in enhancing model capabilities. Source code and trained models are publicly available at: https://github.com/aimagelab/pacscore.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can adversarial attacks by large language models be attributed?</title>
<link>https://arxiv.org/abs/2411.08003</link>
<guid>https://arxiv.org/abs/2411.08003</guid>
<content:encoded><![CDATA[
arXiv:2411.08003v3 Announce Type: replace-cross 
Abstract: Attributing outputs from Large Language Models (LLMs) in adversarial settings-such as cyberattacks and disinformation campaigns-presents significant challenges that are likely to grow in importance. We approach this attribution problem from both a theoretical and an empirical perspective, drawing on formal language theory (identification in the limit) and data-driven analysis of the expanding LLM ecosystem. By modeling an LLM's set of possible outputs as a formal language, we analyze whether finite samples of text can uniquely pinpoint the originating model. Our results show that, under mild assumptions of overlapping capabilities among models, certain classes of LLMs are fundamentally non-identifiable from their outputs alone. We delineate four regimes of theoretical identifiability: (1) an infinite class of deterministic (discrete) LLM languages is not identifiable (Gold's classical result from 1967); (2) an infinite class of probabilistic LLMs is also not identifiable (by extension of the deterministic case); (3) a finite class of deterministic LLMs is identifiable (consistent with Angluin's tell-tale criterion); and (4) even a finite class of probabilistic LLMs can be non-identifiable (we provide a new counterexample establishing this negative result). Complementing these theoretical insights, we quantify the explosion in the number of plausible model origins (hypothesis space) for a given output in recent years. Even under conservative assumptions-each open-source model fine-tuned on at most one new dataset-the count of distinct candidate models doubles approximately every 0.5 years, and allowing multi-dataset fine-tuning combinations yields doubling times as short as 0.28 years. This combinatorial growth, alongside the extraordinary computational cost of brute-force likelihood attribution across all models and potential users, renders exhaustive attribution infeasible in practice.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scoring Verifiers: Evaluating Synthetic Verification for Code and Reasoning</title>
<link>https://arxiv.org/abs/2502.13820</link>
<guid>https://arxiv.org/abs/2502.13820</guid>
<content:encoded><![CDATA[
arXiv:2502.13820v3 Announce Type: replace-cross 
Abstract: Synthetic verification techniques such as generating test cases and reward modelling are common ways to enhance the coding capabilities of large language models (LLM) beyond predefined tests. Additionally, code verification has recently found great success as a critical component in improving reasoning capability of LLMs via reinforcement learning. In this paper, we propose an approach which can transform existing coding benchmarks into scoring and ranking datasets to evaluate the effectiveness of synthetic verifiers. We also propose multiple metrics to measure different aspects of the synthetic verifiers with the proposed benchmarks. By employing the proposed approach, we release four new benchmarks (HE-R, HE-R+, MBPP-R, and MBPP-R+), and analyzed synthetic verification methods with standard, reasoning-based, and reward-based LLMs. Our experiments show that reasoning can significantly improve test case generation and that scaling the number of test cases enhances the verification accuracy.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OWLViz: An Open-World Benchmark for Visual Question Answering</title>
<link>https://arxiv.org/abs/2503.07631</link>
<guid>https://arxiv.org/abs/2503.07631</guid>
<content:encoded><![CDATA[
arXiv:2503.07631v3 Announce Type: replace-cross 
Abstract: We present a challenging benchmark for the Open WorLd VISual question answering (OWLViz) task. OWLViz presents concise, unambiguous queries that require integrating multiple capabilities, including visual understanding, web exploration, and specialized tool usage. While humans achieve 69.2% accuracy on these intuitive tasks, even state-of-the-art VLMs struggle, with the best model, Gemini 2.0, achieving only 26.6% accuracy. Current agentic VLMs, which rely on limited vision and vision-language models as tools, perform even worse. This performance gap reveals significant limitations in multimodal systems' ability to select appropriate tools and execute complex reasoning sequences, establishing new directions for advancing practical AI research.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReverBERT: A State Space Model for Efficient Text-Driven Speech Style Transfer</title>
<link>https://arxiv.org/abs/2503.20992</link>
<guid>https://arxiv.org/abs/2503.20992</guid>
<content:encoded><![CDATA[
arXiv:2503.20992v2 Announce Type: replace-cross 
Abstract: Text-driven speech style transfer aims to mold the intonation, pace, and timbre of a spoken utterance to match stylistic cues from text descriptions. While existing methods leverage large-scale neural architectures or pre-trained language models, the computational costs often remain high. In this paper, we present \emph{ReverBERT}, an efficient framework for text-driven speech style transfer that draws inspiration from a state space model (SSM) paradigm, loosely motivated by the image-based method of Wang and Liu~\cite{wang2024stylemamba}. Unlike image domain techniques, our method operates in the speech space and integrates a discrete Fourier transform of latent speech features to enable smooth and continuous style modulation. We also propose a novel \emph{Transformer-based SSM} layer for bridging textual style descriptors with acoustic attributes, dramatically reducing inference time while preserving high-quality speech characteristics. Extensive experiments on benchmark speech corpora demonstrate that \emph{ReverBERT} significantly outperforms baselines in terms of naturalness, expressiveness, and computational efficiency. We release our model and code publicly to foster further research in text-driven speech style transfer.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis</title>
<link>https://arxiv.org/abs/2504.11257</link>
<guid>https://arxiv.org/abs/2504.11257</guid>
<content:encoded><![CDATA[
arXiv:2504.11257v4 Announce Type: replace-cross 
Abstract: Recent advancements in Large Vision-Language Models are accelerating the development of Graphical User Interface (GUI) agents that utilize human-like vision perception capabilities to enhance productivity on digital devices. Compared to approaches predicated on GUI metadata, which are platform-dependent and vulnerable to implementation variations, vision-based approaches offer broader applicability. In this vision-based paradigm, the GUI instruction grounding, which maps user instruction to the location of corresponding element on the given screenshot, remains a critical challenge, particularly due to limited public training dataset and resource-intensive manual instruction data annotation. In this paper, we delve into unexplored challenges in this task including element-to-screen ratio, unbalanced element type, and implicit instruction. To address these challenges, we introduce a large-scale data synthesis pipeline UI-E2I-Synth for generating varying complex instruction datasets using GPT-4o instead of human annotators. Furthermore, we propose a new GUI instruction grounding benchmark UI-I2E-Bench, which is designed to address the limitations of existing benchmarks by incorporating diverse annotation aspects. Our model, trained on the synthesized data, achieves superior performance in GUI instruction grounding, demonstrating the advancements of proposed data synthesis pipeline. The proposed benchmark, accompanied by extensive analyses, provides practical insights for future research in GUI grounding. We will release corresponding artifacts at https://microsoft.github.io/FIVE-UI-Evol/ .
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining</title>
<link>https://arxiv.org/abs/2504.13932</link>
<guid>https://arxiv.org/abs/2504.13932</guid>
<content:encoded><![CDATA[
arXiv:2504.13932v3 Announce Type: replace-cross 
Abstract: The growing use of large language models has raised environmental and economic concerns about their intensity of resource usage during inference. Serving these models to each user requires substantial energy and water for cooling. Model compression techniques like quantization can shrink large language models and make them more resource efficient at the cost of potential performance degradation. Quantization methods compress model size through replacing their high-precision parameters by quantized values of lower precision. Among existing methods, the ApiQ method achieves superior accuracy preservation at minimal memory and time overhead. We investigate two ideas to extend performance in ultra-low-bit quantization beyond ApiQ's level. First, we look into combining existing quantization-aware training techniques with ApiQ's partial training. We show that this does not outperform the baseline ApiQ method with limited training data and frozen weights. This leads to two key insights: (1) The substantial representational capacity that is gained through full retraining is unlikely to be feasible through partial training. (2) This gain may depend on using a large and diverse dataset in quantization-aware training. Second, through a novel approach informed by the two insights, we propose an ultra-low-bit quantization method that builds upon ApiQ and extends its performance without the need for full retraining. This publicly available method relies on a saliency-aware regularization term that prioritizes preserving the most impactful parameters during quantization. Our experiments on LLaMA 7B and 13B benchmarks demonstrate that our method reduces the ApiQ's accuracy degradation by 10.85% and 7.54% respectively. A Python implementation of the proposed quantization method is publicly available on GitHub https://github.com/TokuyuSou/ULB-SAPR.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection</title>
<link>https://arxiv.org/abs/2505.19010</link>
<guid>https://arxiv.org/abs/2505.19010</guid>
<content:encoded><![CDATA[
arXiv:2505.19010v2 Announce Type: replace-cross 
Abstract: Multi-modal learning has emerged as a crucial research direction, as integrating textual and visual information can substantially enhance performance in tasks such as classification, retrieval, and scene understanding. Despite advances with large pre-trained models, existing approaches often suffer from insufficient cross-modal interactions and rigid fusion strategies, failing to fully harness the complementary strengths of different modalities. To address these limitations, we propose Co-AttenDWG, co-attention with dimension-wise gating, and expert fusion. Our approach first projects textual and visual features into a shared embedding space, where a dedicated co-attention mechanism enables simultaneous, fine-grained interactions between modalities. This is further strengthened by a dimension-wise gating network, which adaptively modulates feature contributions at the channel level to emphasize salient information. In parallel, dual-path encoders independently refine modality-specific representations, while an additional cross-attention layer aligns the modalities further. The resulting features are aggregated via an expert fusion module that integrates learned gating and self-attention, yielding a robust unified representation. Experimental results on the MIMIC and SemEval Memotion 1.0 datasets show that Co-AttenDWG achieves state-of-the-art performance and superior cross-modal alignment, highlighting its effectiveness for diverse multi-modal applications.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Language Models are Good Heterogeneous Graph Generalizers</title>
<link>https://arxiv.org/abs/2506.06157</link>
<guid>https://arxiv.org/abs/2506.06157</guid>
<content:encoded><![CDATA[
arXiv:2506.06157v2 Announce Type: replace-cross 
Abstract: Heterogeneous graph neural networks (HGNNs) excel at capturing structural and semantic information in heterogeneous graphs (HGs), while struggling to generalize across domains and tasks. With the rapid advancement of large language models (LLMs), a recent study explored the integration of HGNNs with LLMs for generalizable heterogeneous graph learning. However, this approach typically encodes structural information as HG tokens using HGNNs, and disparities in embedding spaces between HGNNs and LLMs have been shown to bias the LLM's comprehension of HGs. Moreover, since these HG tokens are often derived from node-level tasks, the model's ability to generalize across tasks remains limited. To this end, we propose a simple yet effective Masked Language Modeling-based method, called MLM4HG. MLM4HG introduces metapath-based textual sequences instead of HG tokens to extract structural and semantic information inherent in HGs, and designs customized textual templates to unify different graph tasks into a coherent cloze-style 'mask' token prediction paradigm. Specifically,MLM4HG first converts HGs from various domains to texts based on metapaths, and subsequently combines them with the unified task texts to form a HG-based corpus. Moreover, the corpus is fed into a pretrained LM for fine-tuning with a constrained target vocabulary, enabling the fine-tuned LM to generalize to unseen target HGs. Extensive cross-domain and multi-task experiments on four real-world datasets demonstrate the superior generalization performance of MLM4HG over state-of-the-art methods in both few-shot and zero-shot scenarios. Our code is available at https://github.com/BUPT-GAMMA/MLM4HG.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence</title>
<link>https://arxiv.org/abs/2506.15677</link>
<guid>https://arxiv.org/abs/2506.15677</guid>
<content:encoded><![CDATA[
arXiv:2506.15677v3 Announce Type: replace-cross 
Abstract: AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs</title>
<link>https://arxiv.org/abs/2507.07610</link>
<guid>https://arxiv.org/abs/2507.07610</guid>
<content:encoded><![CDATA[
arXiv:2507.07610v3 Announce Type: replace-cross 
Abstract: Humans can directly imagine and manipulate visual images in their minds, a capability known as spatial visualization. While multi-modal Large Language Models (MLLMs) support imagination-based reasoning, spatial visualization remains insufficiently evaluated, typically embedded within broader mathematical and logical assessments. Existing evaluations often rely on IQ tests or math competitions that may overlap with training data, compromising assessment reliability. To this end, we introduce SpatialViz-Bench, a comprehensive multi-modal benchmark for spatial visualization with 12 tasks across 4 sub-abilities, comprising 1,180 automatically generated problems. Our evaluation of 33 state-of-the-art MLLMs not only reveals wide performance variations and demonstrates the benchmark's strong discriminative power, but also uncovers counter-intuitive findings: models show difficulty perception misaligned with human intuition, exhibit dramatic 2Dto-3D performance cliffs, default to formulaic derivation over visualization, and paradoxically suffer performance degradation from Chain-of-Thought prompting in open-source models. Through statistical and qualitative analysis of error types, SpatialViz-Bench demonstrates that state-of-the-art MLLMs continue to exhibit deficiencies in spatial visualization tasks, thereby addressing a significant lacuna in the field. The benchmark data and evaluation code are publicly available.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity</title>
<link>https://arxiv.org/abs/2507.08771</link>
<guid>https://arxiv.org/abs/2507.08771</guid>
<content:encoded><![CDATA[
arXiv:2507.08771v2 Announce Type: replace-cross 
Abstract: To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (https://github.com/thunlp/BlockFFN).
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Categorical Classification of Book Summaries Using Word Embedding Techniques</title>
<link>https://arxiv.org/abs/2507.21058</link>
<guid>https://arxiv.org/abs/2507.21058</guid>
<content:encoded><![CDATA[
<div> Word Embedding, Book Summaries, Categories, Natural Language Processing, Machine Learning <br />
Summary: This study focused on the classification of book summaries and categories from book sites using various word embedding methods and machine learning algorithms. The research compared the success of one-hot encoding, Word2Vec, and TF-IDF methods for word embedding. The study also included a combination table of pre-processing methods. The results indicated that Support Vector Machine, Naive Bayes, and Logistic Regression Models, along with TF-IDF and One-Hot Encoder word embedding techniques, were more successful for Turkish texts. The study provides valuable insights into the effective methods for categorizing and summarizing book information using natural language processing techniques. <div>
arXiv:2507.21058v1 Announce Type: new 
Abstract: In this study, book summaries and categories taken from book sites were classified using word embedding methods, natural language processing techniques and machine learning algorithms. In addition, one hot encoding, Word2Vec and Term Frequency - Inverse Document Frequency (TF-IDF) methods, which are frequently used word embedding methods were used in this study and their success was compared. Additionally, the combination table of the pre-processing methods used is shown and added to the table. Looking at the results, it was observed that Support Vector Machine, Naive Bayes and Logistic Regression Models and TF-IDF and One-Hot Encoder word embedding techniques gave more successful results for Turkish texts.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions</title>
<link>https://arxiv.org/abs/2507.21065</link>
<guid>https://arxiv.org/abs/2507.21065</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, AI Social Gym, Sociocultural Theory, Pedagogical Strategies, Knowledge Acquisition

Summary:
The study investigates the use of socially mediated learning paradigms to enhance the knowledge acquisition process of Large Language Models (LLMs). Inspired by Vygotsky's sociocultural theory, the researchers created the 'AI Social Gym', where an AI learner agent engages in pedagogical dialogues with knowledgeable AI teacher agents. The focus is on how different pedagogical strategies impact the AI learning process, particularly in ontology acquisition. The results show that dialogic approaches, including mixed-direction interactions, significantly improve the LLM's ability to acquire and apply new knowledge compared to traditional instructional methods and structured datasets. By integrating pedagogical and psychological insights into AI and robot training, the study highlights the potential for enhancing post-training knowledge acquisition and response quality. This approach offers a complementary pathway to existing strategies like prompt engineering. 

Summary:<br /><br />Keywords: Large Language Models, AI Social Gym, Sociocultural Theory, Pedagogical Strategies, Knowledge Acquisition <div>
arXiv:2507.21065v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in processing extensive offline datasets. However, they often face challenges in acquiring and integrating complex, knowledge online. Traditional AI training paradigms, predominantly based on supervised learning or reinforcement learning, mirror a 'Piagetian' model of independent exploration. These approaches typically rely on large datasets and sparse feedback signals, limiting the models' ability to learn efficiently from interactions. Drawing inspiration from Vygotsky's sociocultural theory, this study explores the potential of socially mediated learning paradigms to address these limitations.
  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI learner agent engages in dyadic pedagogical dialogues with knowledgeable AI teacher agents. These interactions emphasize external, structured dialogue as a core mechanism for knowledge acquisition, contrasting with methods that depend solely on internal inference or pattern recognition.
  Our investigation focuses on how different pedagogical strategies impact the AI learning process in the context of ontology acquisition. Empirical results indicate that such dialogic approaches-particularly those involving mixed-direction interactions combining top-down explanations with learner-initiated questioning-significantly enhance the LLM's ability to acquire and apply new knowledge, outperforming both unidirectional instructional methods and direct access to structured knowledge, formats typically present in training datasets.
  These findings suggest that integrating pedagogical and psychological insights into AI and robot training can substantially improve post-training knowledge acquisition and response quality. This approach offers a complementary pathway to existing strategies like prompt engineering
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing</title>
<link>https://arxiv.org/abs/2507.21073</link>
<guid>https://arxiv.org/abs/2507.21073</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, EFL writing, editing behaviors, expository writing, secondary students

Summary: 
This research examines the impact of using AI-generated text in English as a foreign language (EFL) writing contexts. The study focused on how secondary students in Hong Kong edit AI-generated text in their expository writing process and compositions. The findings revealed two main editing patterns among students: refining introductory units repeatedly and quickly shifting to extensive edits in body units. Despite significant editing efforts, the study showed only limited improvement in composition quality, indicating that AI supports but does not replace writing skills. The results suggest the importance of genre-specific instruction and process-focused writing before integrating AI. Educators should develop assessments valuing both the writing process and product to encourage critical engagement with AI text. <div>
arXiv:2507.21073v1 Announce Type: new 
Abstract: Text generated by artificial intelligence (AI) chatbots is increasingly used in English as a foreign language (EFL) writing contexts, yet its impact on students' expository writing process and compositions remains understudied. This research examines how EFL secondary students edit AI-generated text. Exploring editing behaviors in their expository writing process and in expository compositions, and their effect on human-rated scores for content, organization, language, and overall quality. Participants were 39 Hong Kong secondary students who wrote an expository composition with AI chatbots in a workshop. A convergent design was employed to analyze their screen recordings and compositions to examine students' editing behaviors and writing qualities. Analytical methods included qualitative coding, descriptive statistics, temporal sequence analysis, human-rated scoring, and multiple linear regression analysis. We analyzed over 260 edits per dataset, and identified two editing patterns: one where students refined introductory units repeatedly before progressing, and another where they quickly shifted to extensive edits in body units (e.g., topic and supporting sentences). MLR analyses revealed that the number of AI-generated words positively predicted all score dimensions, while most editing variables showed minimal impact. These results suggest a disconnect between students' significant editing effort and improved composition quality, indicating AI supports but does not replace writing skills. The findings highlight the importance of genre-specific instruction and process-focused writing before AI integration. Educators should also develop assessments valuing both process and product to encourage critical engagement with AI text.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which symbol grounding problem should we try to solve?</title>
<link>https://arxiv.org/abs/2507.21080</link>
<guid>https://arxiv.org/abs/2507.21080</guid>
<content:encoded><![CDATA[
<div> Keywords: Floridi, Taddeo, grounding problem, computing, artificial computational agents

Summary:
Floridi and Taddeo propose a condition of "zero semantic commitment" for solutions to the grounding problem, but this condition cannot be fulfilled, not even by their own solution. Luc Steels presents a different suggestion, highlighting the need to rethink the problem's formulation and the role of goals in a system. By understanding computing, it becomes apparent that the only sensible grounding problem concerns explaining and reproducing the behavioral ability and function of meaning in artificial computational agents. This suggests a shift in focus towards understanding how meaning can be preserved and replicated within computational systems. <br /><br />Summary: <div>
arXiv:2507.21080v1 Announce Type: new 
Abstract: Floridi and Taddeo propose a condition of "zero semantic commitment" for solutions to the grounding problem, and a solution to it. I argue briefly that their condition cannot be fulfilled, not even by their own solution. After a look at Luc Steels' very different competing suggestion, I suggest that we need to re-think what the problem is and what role the 'goals' in a system play in formulating the problem. On the basis of a proper understanding of computing, I come to the conclusion that the only sensible grounding problem is how we can explain and re-produce the behavioral ability and function of meaning in artificial computational agents
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs</title>
<link>https://arxiv.org/abs/2507.21083</link>
<guid>https://arxiv.org/abs/2507.21083</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, emotional tone, response bias, tone floor, AI alignment <br />
Summary: 
Large Language Models such as GPT-4 adjust their responses based on the emotional tone of prompts in addition to the actual question. The study systematically varied the emotional tone of 156 prompts across different topics and analyzed the impact on model responses. It was found that GPT-4 is more likely to respond neutrally or positively to negatively framed questions, indicating a "rebound" bias. This bias is even more pronounced in sensitive topics like justice and politics, where tone-based variation is suppressed, suggesting an alignment override. The concept of a "tone floor" was introduced as a lower bound in response negativity, and tone-valence transition matrices were used to quantify the model's behavior. Visualizations based on embeddings confirmed semantic drift based on emotional tone. These findings highlight biases driven by emotional framing in prompts, with implications for AI alignment and trust. <div>
arXiv:2507.21083v1 Announce Type: new 
Abstract: Large Language Models like GPT-4 adjust their responses not only based on the question asked, but also on how it is emotionally phrased. We systematically vary the emotional tone of 156 prompts - spanning controversial and everyday topics - and analyze how it affects model responses. Our findings show that GPT-4 is three times less likely to respond negatively to a negatively framed question than to a neutral one. This suggests a "rebound" bias where the model overcorrects, often shifting toward neutrality or positivity. On sensitive topics (e.g., justice or politics), this effect is even more pronounced: tone-based variation is suppressed, suggesting an alignment override. We introduce concepts like the "tone floor" - a lower bound in response negativity - and use tone-valence transition matrices to quantify behavior. Visualizations based on 1536-dimensional embeddings confirm semantic drift based on tone. Our work highlights an underexplored class of biases driven by emotional framing in prompts, with implications for AI alignment and trust. Code and data are available at: https://github.com/bardolfranck/llm-responses-viewer
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing</title>
<link>https://arxiv.org/abs/2507.21084</link>
<guid>https://arxiv.org/abs/2507.21084</guid>
<content:encoded><![CDATA[
<div> framework, side effects, large language models, fine-tuning, model diffing
Summary:
The article introduces MNEME, a framework for identifying unintended side effects in large language models (LLMs) that occur during fine-tuning or unlearning processes. MNEME uses sparse model diffing to compare base and fine-tuned models on task-agnostic data, enabling the detection of behavioral shifts without access to fine-tuning data. The framework was tested on five LLMs across various scenarios, achieving up to 95 percent accuracy in predicting side effects. It successfully identified issues such as knowledge unlearning, emergent misalignment, and benign fine-tuning effects without the need for custom heuristics. The study also found that retraining on high-activation samples could partially reverse these effects. The results highlight the scalability and automation of MNEME in understanding and managing changes in LLM behavior brought about by fine-tuning processes. 

<br /><br />Summary: <div>
arXiv:2507.21084v1 Announce Type: new 
Abstract: Large language models (LLMs) are frequently fine-tuned or unlearned to adapt to new tasks or eliminate undesirable behaviors. While existing evaluation methods assess performance after such interventions, there remains no general approach for detecting unintended side effects, such as unlearning biology content degrading performance on chemistry tasks, particularly when these effects are unpredictable or emergent. To address this issue, we introduce MNEME, Model diffiNg for Evaluating Mechanistic Effects, a lightweight framework for identifying these side effects using sparse model diffing. MNEME compares base and fine-tuned models on task-agnostic data (for example, The Pile, LMSYS-Chat-1M) without access to fine-tuning data to isolate behavioral shifts. Applied to five LLMs across three scenarios: WMDP knowledge unlearning, emergent misalignment, and benign fine-tuning, MNEME achieves up to 95 percent accuracy in predicting side effects, aligning with known benchmarks and requiring no custom heuristics. Furthermore, we show that retraining on high-activation samples can partially reverse these effects. Our results demonstrate that sparse probing and diffing offer a scalable and automated lens into fine-tuning-induced model changes, providing practical tools for understanding and managing LLM behavior.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Amateur Contrastive Decoding for Text Generation</title>
<link>https://arxiv.org/abs/2507.21086</link>
<guid>https://arxiv.org/abs/2507.21086</guid>
<content:encoded><![CDATA[
<div> ensemble, amateur models, generation patterns, multi-amateur contrastive decoding, controllable generation <br />
Summary: 
Multi-Amateur Contrastive Decoding (MACD) is proposed as an enhancement to Contrastive Decoding (CD) for text generation. By utilizing an ensemble of amateur models, MACD is able to address a wider range of undesirable generation patterns such as repetition and hallucination. Through averaging and consensus mechanisms, MACD improves fluency, coherence, diversity, and adaptability in text generation across various domains. The framework also allows for controllable generation by incorporating amateurs with specific biases. Experimental results show that MACD outperforms conventional decoding methods and the original CD approach in terms of performance metrics, without the need for additional training. <div>
arXiv:2507.21086v1 Announce Type: new 
Abstract: Contrastive Decoding (CD) has emerged as an effective inference-time strategy for enhancing open-ended text generation by exploiting the divergence in output probabilities between a large expert language model and a smaller amateur model. Although CD improves coherence and fluency, its dependence on a single amateur restricts its capacity to capture the diverse and multifaceted failure modes of language generation, such as repetition, hallucination, and stylistic drift. This paper proposes Multi-Amateur Contrastive Decoding (MACD), a generalization of the CD framework that employs an ensemble of amateur models to more comprehensively characterize undesirable generation patterns. MACD integrates contrastive signals through both averaging and consensus penalization mechanisms and extends the plausibility constraint to operate effectively in the multi-amateur setting. Furthermore, the framework enables controllable generation by incorporating amateurs with targeted stylistic or content biases. Experimental results across multiple domains, such as news, encyclopedic, and narrative, demonstrate that MACD consistently surpasses conventional decoding methods and the original CD approach in terms of fluency, coherence, diversity, and adaptability, all without requiring additional training or fine-tuning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QU-NLP at CheckThat! 2025: Multilingual Subjectivity in News Articles Detection using Feature-Augmented Transformer Models with Sequential Cross-Lingual Fine-Tuning</title>
<link>https://arxiv.org/abs/2507.21095</link>
<guid>https://arxiv.org/abs/2507.21095</guid>
<content:encoded><![CDATA[
<div> transformer architecture, contextual embeddings, subjectivity detection, pre-trained language models, multilingual setting

Summary:<br /><br />
This paper presents an approach to subjectivity detection in CheckThat! 2025 Task 1, utilizing a feature-augmented transformer architecture that combines contextual embeddings from pre-trained language models with statistical and linguistic features. The system achieved competitive performance in monolingual, multilingual, and zero-shot settings across various languages including English, Arabic, German, and Italian. The approach leverages pre-trained transformers with additional lexical features, such as part-of-speech tags and TF-IDF features for Arabic, and a cross-lingual DeBERTa V3 model with TF-IDF features and gating mechanism for other languages. The study highlights the effectiveness of combining TF-IDF features with the gating mechanism and cross-lingual transfer for subjectivity detection. An ablation analysis emphasized the importance of these components and revealed the model's sensitivity to the order of cross-lingual fine-tuning and linguistic proximity of training languages. <div>
arXiv:2507.21095v1 Announce Type: new 
Abstract: This paper presents our approach to the CheckThat! 2025 Task 1 on subjectivity detection, where systems are challenged to distinguish whether a sentence from a news article expresses the subjective view of the author or presents an objective view on the covered topic. We propose a feature-augmented transformer architecture that combines contextual embeddings from pre-trained language models with statistical and linguistic features. Our system leveraged pre-trained transformers with additional lexical features: for Arabic we used AraELECTRA augmented with part-of-speech (POS) tags and TF-IDF features, while for the other languages we fine-tuned a cross-lingual DeBERTa~V3 model combined with TF-IDF features through a gating mechanism. We evaluated our system in monolingual, multilingual, and zero-shot settings across multiple languages including English, Arabic, German, Italian, and several unseen languages. The results demonstrate the effectiveness of our approach, achieving competitive performance across different languages with notable success in the monolingual setting for English (rank 1st with macro-F1=0.8052), German (rank 3rd with macro-F1=0.8013), Arabic (rank 4th with macro-F1=0.5771), and Romanian (rank 1st with macro-F1=0.8126) in the zero-shot setting. We also conducted an ablation analysis that demonstrated the importance of combining TF-IDF features with the gating mechanism and the cross-lingual transfer for subjectivity detection. Furthermore, our analysis reveals the model's sensitivity to both the order of cross-lingual fine-tuning and the linguistic proximity of the training languages.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rewrite-to-Rank: Optimizing Ad Visibility via Retrieval-Aware Text Rewriting</title>
<link>https://arxiv.org/abs/2507.21099</link>
<guid>https://arxiv.org/abs/2507.21099</guid>
<content:encoded><![CDATA[
<div> keywords: LLMs, ad rewriting, retrieval systems, supervised fine-tuning, reinforcement learning

Summary: 
LLMs have the capability to return relevant information based on search algorithms and user query relevance. This study investigates the impact of content phrasing on ad visibility in retrieval systems. The researchers propose a supervised fine-tuning framework that balances semantic relevance and content fidelity to optimize ad phrasing without modifying the retrieval model. Two metrics, DeltaMRR@K and DeltaDIR@K, are introduced to evaluate the effectiveness of the approach. The experiments show that PPO trained models outperform prompt engineering and supervised fine-tuning in optimizing ad rewriting for LLM integrated retrieval systems. The results emphasize the significance of how ads are written before retrieval, as well as the importance of prompt format and reinforcement learning in enhancing ad visibility. <br /><br />Summary: <div>
arXiv:2507.21099v1 Announce Type: new 
Abstract: Search algorithms and user query relevance have given LLMs the ability to return relevant information, but the effect of content phrasing on ad visibility remains underexplored. We investigate how LLM-based rewriting of advertisements can improve their ranking in retrieval systems and inclusion in generated LLM responses, without modifying the retrieval model itself. We introduce a supervised fine-tuning framework with a custom loss balancing semantic relevance and content fidelity. To evaluate effectiveness, we propose two metrics: DeltaMRR@K (ranking improvement) and DeltaDIR@K (inclusion frequency improvement). Our approach presents a scalable method to optimize ad phrasing, enhancing visibility in retrieval-based LLM workflows. Experiments across both instruction-based and few-shot prompting demonstrate that PPO trained models outperform both prompt engineering and supervised fine-tuning in most cases, achieving up to a 2.79 DeltaDIR@5 and 0.0073 DeltaMRR@5 in instruction-based prompting. These results highlight the importance of how the ad is written before retrieval and prompt format and reinforcement learning in effective ad rewriting for LLM integrated retrieval systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iLSU-T: an Open Dataset for Uruguayan Sign Language Translation</title>
<link>https://arxiv.org/abs/2507.21104</link>
<guid>https://arxiv.org/abs/2507.21104</guid>
<content:encoded><![CDATA[
<div> Keywords: sign language translation, dataset, Uruguayan Sign Language, multimodal data, accessibility<br />
Summary:<br />
This study introduces iLSU T, an open dataset of interpreted videos of Uruguayan Sign Language (USL) with audio and text transcriptions. The dataset includes over 185 hours of videos from public TV broadcasts, featuring 18 professional interpreters. The aim is to provide valuable data for developing new approaches in sign language processing. The research conducts experiments using three advanced translation algorithms to establish a baseline for the dataset and assess its effectiveness in data processing. The results emphasize the importance of localized datasets for enhancing sign language translation tools and promoting inclusivity. By offering the dataset and code, the study encourages further research in this area to advance accessibility for individuals with hearing impairments. <br />
Summary: <div>
arXiv:2507.21104v1 Announce Type: new 
Abstract: Automatic sign language translation has gained particular interest in the computer vision and computational linguistics communities in recent years. Given each sign language country particularities, machine translation requires local data to develop new techniques and adapt existing ones. This work presents iLSU T, an open dataset of interpreted Uruguayan Sign Language RGB videos with audio and text transcriptions. This type of multimodal and curated data is paramount for developing novel approaches to understand or generate tools for sign language processing. iLSU T comprises more than 185 hours of interpreted sign language videos from public TV broadcasting. It covers diverse topics and includes the participation of 18 professional interpreters of sign language. A series of experiments using three state of the art translation algorithms is presented. The aim is to establish a baseline for this dataset and evaluate its usefulness and the proposed pipeline for data processing. The experiments highlight the need for more localized datasets for sign language translation and understanding, which are critical for developing novel tools to improve accessibility and inclusion of all individuals. Our data and code can be accessed.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creation of a Numerical Scoring System to Objectively Measure and Compare the Level of Rhetoric in Arabic Texts: A Feasibility Study, and A Working Prototype</title>
<link>https://arxiv.org/abs/2507.21106</link>
<guid>https://arxiv.org/abs/2507.21106</guid>
<content:encoded><![CDATA[
<div> Keywords: Arabic Rhetoric, Literary Devices, Text Analysis, Measurement Tool, Linguistic Embellishments

Summary:
Arabic Rhetoric is a vital aspect of conveying messages effectively in Arabic linguistics, utilizing literary devices to enhance communication. However, measuring the use of Arabic rhetoric objectively has been a challenge. This study aimed to develop a tool to quantify the density of literary devices in Arabic texts, serving as a proxy for assessing Arabic rhetoric. By compiling a list of 84 common literary devices, creating a system to identify and calculate their density, and developing electronic tools, including a website and online calculator, the study offers a practical approach to analyze Arabic rhetoric. The tool can accurately determine the density of Arabic rhetoric in any text, aiding in the comparison of rhetoric usage across genres, authors, and historical periods. This innovative method provides a valuable resource for scholars and researchers in the field of Arabic linguistics.<br /><br />Summary: <div>
arXiv:2507.21106v1 Announce Type: new 
Abstract: Arabic Rhetoric is the field of Arabic linguistics which governs the art and science of conveying a message with greater beauty, impact and persuasiveness. The field is as ancient as the Arabic language itself and is found extensively in classical and contemporary Arabic poetry, free verse and prose. In practical terms, it is the intelligent use of word order, figurative speech and linguistic embellishments to enhance message delivery. Despite the volumes that have been written about it and the high status accorded to it, there is no way to objectively know whether a speaker or writer has used Arabic rhetoric in a given text, to what extent, and why. There is no objective way to compare the use of Arabic rhetoric across genres, authors or epochs. It is impossible to know which of pre-Islamic poetry, Andalucian Arabic poetry, or modern literary genres are richer in Arabic rhetoric. The aim of the current study was to devise a way to measure the density of the literary devices which constitute Arabic rhetoric in a given text, as a proxy marker for Arabic rhetoric itself. A comprehensive list of 84 of the commonest literary devices and their definitions was compiled. A system of identifying literary devices in texts was constructed. A method of calculating the density of literary devices based on the morpheme count of the text was utilised. Four electronic tools and an analogue tool were created to support the calculation of an Arabic text's rhetorical literary device density, including a website and online calculator. Additionally, a technique of reporting the distribution of literary devices used across the three sub-domains of Arabic rhetoric was created. The output of this project is a working tool which can accurately report the density of Arabic rhetoric in any Arabic text or speech.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams</title>
<link>https://arxiv.org/abs/2507.21107</link>
<guid>https://arxiv.org/abs/2507.21107</guid>
<content:encoded><![CDATA[
<div> Keywords: Curved Inference, language model, semantic shifts, geometric interpretability, activation trajectories

Summary:
Curved Inference introduces a framework to track how a language model's trajectory changes in response to different semantic concerns. The study compares two large language models, Gemma3-1b and LLaMA3.2-3b, across various semantic domains. Using metrics based on curvature and salience, the analysis reveals that both models adjust their internal activation trajectories in response to different prompts, with LLaMA showing more significant changes. The findings suggest a dual-layer structure in language model geometry, involving a latent conceptual space and prompt-specific contextual trajectories. The study highlights the importance of understanding how models navigate semantic meaning over different levels, providing insights into alignment, abstraction, and inference dynamics. Overall, Curved Inference offers a systematic approach to examining semantic abstraction and model alignment in language models.<br /><br />Summary: Curved Inference framework tracks language model trajectory changes in response to semantic concerns. Comparison of models shows adjustments in internal activation trajectories. Dual-layer model geometry structure identified. Study provides insights into alignment, abstraction, and inference dynamics in language models. <div>
arXiv:2507.21107v1 Announce Type: new 
Abstract: We propose Curved Inference - a geometric Interpretability framework that tracks how the residual stream trajectory of a large language model bends in response to shifts in semantic concern. Across 20 matched prompts spanning emotional, moral, perspective, logical, identity, environmental, and nonsense domains, we analyse Gemma3-1b and LLaMA3.2-3b using five native-space metrics, with a primary focus on curvature (\k{appa}_i) and salience (S(t)). These metrics are computed under a pullback semantic metric derived from the unembedding matrix, ensuring that all measurements reflect token-aligned geometry rather than raw coordinate structure. We find that concern-shifted prompts reliably alter internal activation trajectories in both models - with LLaMA exhibiting consistent, statistically significant scaling in both curvature and salience as concern intensity increases. Gemma also responds to concern but shows weaker differentiation between moderate and strong variants. Our results support a two-layer view of LLM geometry - a latent conceptual structure encoded in the embedding space, and a contextual trajectory shaped by prompt-specific inference. Curved Inference reveals how models navigate, reorient, or reinforce semantic meaning over depth, offering a principled method for diagnosing alignment, abstraction, and emergent inference dynamics. These findings offer fresh insight into semantic abstraction and model alignment through the lens of Curved Inference.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Classification Tasks and Approaches for Legal Contracts</title>
<link>https://arxiv.org/abs/2507.21108</link>
<guid>https://arxiv.org/abs/2507.21108</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Legal Contract Classification, datasets, methodologies, evaluation techniques, future research directions

Summary: 
Automatic Legal Contract Classification (LCC) is crucial due to the complexity and volume of contracts, leading to the need for automation. This survey delves into challenges, tasks, datasets, and methodologies within LCC, identifying seven classification tasks and reviewing fourteen datasets. Methodologies are categorized into Traditional Machine Learning, Deep Learning, and Transformer-based approaches. Evaluation techniques and best-performing results are discussed. The survey highlights the limitations of current methods and suggests future research directions to enhance efficiency, accuracy, and scalability of LCC. This comprehensive survey aims to support legal NLP researchers and practitioners in improving legal processes, enhancing accessibility to legal information, and fostering a more informed and equitable society. 

<br /><br />Summary: <div>
arXiv:2507.21108v1 Announce Type: new 
Abstract: Given the large size and volumes of contracts and their underlying inherent complexity, manual reviews become inefficient and prone to errors, creating a clear need for automation. Automatic Legal Contract Classification (LCC) revolutionizes the way legal contracts are analyzed, offering substantial improvements in speed, accuracy, and accessibility. This survey delves into the challenges of automatic LCC and a detailed examination of key tasks, datasets, and methodologies. We identify seven classification tasks within LCC, and review fourteen datasets related to English-language contracts, including public, proprietary, and non-public sources. We also introduce a methodology taxonomy for LCC, categorized into Traditional Machine Learning, Deep Learning, and Transformer-based approaches. Additionally, the survey discusses evaluation techniques and highlights the best-performing results from the reviewed studies. By providing a thorough overview of current methods and their limitations, this survey suggests future research directions to improve the efficiency, accuracy, and scalability of LCC. As the first comprehensive survey on LCC, it aims to support legal NLP researchers and practitioners in improving legal processes, making legal information more accessible, and promoting a more informed and equitable society.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemRAG: Semantic Knowledge-Augmented RAG for Improved Question-Answering</title>
<link>https://arxiv.org/abs/2507.21110</link>
<guid>https://arxiv.org/abs/2507.21110</guid>
<content:encoded><![CDATA[
<div> Keywords: SemRAG, Retrieval Augmented Generation, domain-specific knowledge, knowledge graphs, semantic chunking

Summary:<br /><br />This paper introduces SemRAG, a framework that enhances the Retrieval Augmented Generation (RAG) model by efficiently integrating domain-specific knowledge through semantic chunking and knowledge graphs. SemRAG uses a semantic chunking algorithm to segment documents based on cosine similarity, preserving semantic coherence while reducing computational overhead. By structuring retrieved information into knowledge graphs, SemRAG captures relationships between entities, improving retrieval accuracy and contextual understanding. Experimental results on MultiHop RAG and Wikipedia datasets show that SemRAG outperforms traditional RAG methods in terms of relevance and correctness of retrieved information. Optimizing buffer sizes for different datasets further enhances retrieval performance. SemRAG offers an efficient and accurate domain-specific LLM pipeline without the need for extensive fine-tuning, making it a practical and scalable solution for AI applications in specialized fields.<br />Summary: <div>
arXiv:2507.21110v1 Announce Type: new 
Abstract: This paper introduces SemRAG, an enhanced Retrieval Augmented Generation (RAG) framework that efficiently integrates domain-specific knowledge using semantic chunking and knowledge graphs without extensive fine-tuning. Integrating domain-specific knowledge into large language models (LLMs) is crucial for improving their performance in specialized tasks. Yet, existing adaptations are computationally expensive, prone to overfitting and limit scalability. To address these challenges, SemRAG employs a semantic chunking algorithm that segments documents based on the cosine similarity from sentence embeddings, preserving semantic coherence while reducing computational overhead. Additionally, by structuring retrieved information into knowledge graphs, SemRAG captures relationships between entities, improving retrieval accuracy and contextual understanding. Experimental results on MultiHop RAG and Wikipedia datasets demonstrate SemRAG has significantly enhances the relevance and correctness of retrieved information from the Knowledge Graph, outperforming traditional RAG methods. Furthermore, we investigate the optimization of buffer sizes for different data corpus, as optimizing buffer sizes tailored to specific datasets can further improve retrieval performance, as integration of knowledge graphs strengthens entity relationships for better contextual comprehension. The primary advantage of SemRAG is its ability to create an efficient, accurate domain-specific LLM pipeline while avoiding resource-intensive fine-tuning. This makes it a practical and scalable approach aligned with sustainability goals, offering a viable solution for AI applications in domain-specific fields.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InsurTech innovation using natural language processing</title>
<link>https://arxiv.org/abs/2507.21112</link>
<guid>https://arxiv.org/abs/2507.21112</guid>
<content:encoded><![CDATA[
<div> Keywords: InsurTech, natural language processing, structured data, actuarial analysis, commercial insurance

Summary: 
InsurTech companies are driving traditional insurance firms to adopt alternative data sources and advanced technologies. This paper explores the use of natural language processing (NLP) in insurance operations, focusing on translating unstructured text into structured data for analysis and decision-making. Through case studies using real-world alternative data, NLP techniques are applied to enhance commercial insurance pricing and risk assessment. The insights derived from text analysis not only refine traditional rating factors but also introduce new perspectives for industry classifications. The paper demonstrates that NLP is a foundational tool for modern, data-driven insurance analytics, offering valuable insights beyond traditional methods. NLP plays a crucial role in transforming raw text into actionable information for improving insurance operations. 

<br /><br />Summary: <div>
arXiv:2507.21112v1 Announce Type: new 
Abstract: With the rapid rise of InsurTech, traditional insurance companies are increasingly exploring alternative data sources and advanced technologies to sustain their competitive edge. This paper provides both a conceptual overview and practical case studies of natural language processing (NLP) and its emerging applications within insurance operations with a focus on transforming raw, unstructured text into structured data suitable for actuarial analysis and decision-making. Leveraging real-world alternative data provided by an InsurTech industry partner that enriches traditional insurance data sources, we apply various NLP techniques to demonstrate practical use cases in the commercial insurance context. These enriched, text-derived insights not only add to and refine traditional rating factors for commercial insurance pricing but also offer novel perspectives for assessing underlying risk by introducing novel industry classifications. Through these demonstrations, we show that NLP is not merely a supplementary tool but a foundational element for modern, data-driven insurance analytics.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law</title>
<link>https://arxiv.org/abs/2507.21134</link>
<guid>https://arxiv.org/abs/2507.21134</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, domain-specific safety, legal, financial, medical

Summary:<br /><br />
The study focuses on evaluating the safety of large language models (LLMs) in high-risk fields such as law, finance, and medicine. Domain-specific safety principles, based on established ethical codes, are defined for LLMs. Trident-Bench, a benchmark specifically for assessing LLM safety in legal, financial, and medical domains, is introduced. Nineteen models were evaluated on Trident-Bench, revealing that generalist models meet basic safety expectations while domain-specialized models struggle with ethical nuances. The study underscores the need for domain-specific safety enhancements. Trident-Bench is a crucial resource for studying LLM safety in regulated fields and sets the stage for future research on mitigating safety risks in LLM deployment. The code and benchmark are available at the provided GitHub repository. <div>
arXiv:2507.21134v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed in high-risk domains such as law, finance, and medicine, systematically evaluating their domain-specific safety and compliance becomes critical. While prior work has largely focused on improving LLM performance in these domains, it has often neglected the evaluation of domain-specific safety risks. To bridge this gap, we first define domain-specific safety principles for LLMs based on the AMA Principles of Medical Ethics, the ABA Model Rules of Professional Conduct, and the CFA Institute Code of Ethics. Building on this foundation, we introduce Trident-Bench, a benchmark specifically targeting LLM safety in the legal, financial, and medical domains. We evaluated 19 general-purpose and domain-specialized models on Trident-Bench and show that it effectively reveals key safety gaps -- strong generalist models (e.g., GPT, Gemini) can meet basic expectations, whereas domain-specialized models often struggle with subtle ethical nuances. This highlights an urgent need for finer-grained domain-specific safety improvements. By introducing Trident-Bench, our work provides one of the first systematic resources for studying LLM safety in law and finance, and lays the groundwork for future research aimed at reducing the safety risks of deploying LLMs in professionally regulated fields. Code and benchmark will be released at: https://github.com/zackhuiiiii/TRIDENT
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TTS-1 Technical Report</title>
<link>https://arxiv.org/abs/2507.21138</link>
<guid>https://arxiv.org/abs/2507.21138</guid>
<content:encoded><![CDATA[
<div> Transformer-based autoregressive text-to-speech, TTS models, inworld TTS-1, SpeechLM, real-time speech synthesis, on-device use cases <br />
<br />
Summary: 
Inworld TTS-1 introduces two Transformer-based autoregressive text-to-speech models, TTS-1 and TTS-1-Max. TTS-1-Max, with 8.8B parameters, focuses on quality and expressiveness, while TTS-1, with 1.6B parameters, is designed for real-time speech synthesis. Both models achieve state-of-the-art performance through pre-training, fine-tuning, and RL-alignment of the SpeechLM component. They support 11 languages, emotional control, and non-verbal vocalizations through audio markups, generating high-quality speech with low latency. The models are efficient and can be used on devices for various applications. Additionally, the training and modeling code are open-sourced under an MIT license. <div>
arXiv:2507.21138v1 Announce Type: new 
Abstract: We introduce Inworld TTS-1, a set of two Transformer-based autoregressive text-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters and is designed for utmost quality and expressiveness in demanding applications. TTS-1 is our most efficient model, with 1.6B parameters, built for real-time speech synthesis and on-device use cases. By scaling train-time compute and applying a sequential process of pre-training, fine-tuning, and RL-alignment of the speech-language model (SpeechLM) component, both models achieve state-of-the-art performance on a variety of benchmarks, demonstrating exceptional quality relying purely on in-context learning of the speaker's voice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech with low latency, and support 11 languages with fine-grained emotional control and non-verbal vocalizations through audio markups. We additionally open-source our training and modeling code under an MIT license.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question</title>
<link>https://arxiv.org/abs/2507.21168</link>
<guid>https://arxiv.org/abs/2507.21168</guid>
<content:encoded><![CDATA[
<div> Keywords: diversity, machine learning, large language models, ensemble, binary questions

Summary: 
Diversity plays a crucial role in enhancing the performance of machine learning models, particularly large language models (LLMs). This study compares two diversity approaches in answering binary questions using LLMs: model diversity and question interpretation diversity. Model diversity involves multiple models answering the same question, while question interpretation diversity frames the same question in different ways for the same model to answer. Majority voting is used as the ensemble consensus heuristic to determine the final answer. Results across boolq, strategyqa, and pubmedqa datasets consistently demonstrate that question interpretation diversity outperforms model diversity in terms of ensemble accuracy. Analysis of GPT and LLaMa models indicates that model diversity often yields results between the best and worst ensemble members without significant improvement. This research provides valuable insights into the effective utilization of diversity strategies in enhancing the performance of LLMs in answering binary questions.<br /><br />Summary: <div>
arXiv:2507.21168v1 Announce Type: new 
Abstract: Effectively leveraging diversity has been shown to improve performance for various machine learning models, including large language models (LLMs). However, determining the most effective way of using diversity remains a challenge. In this work, we compare two diversity approaches for answering binary questions using LLMs: model diversity, which relies on multiple models answering the same question, and question interpretation diversity, which relies on using the same model to answer the same question framed in different ways. For both cases, we apply majority voting as the ensemble consensus heuristic to determine the final answer. Our experiments on boolq, strategyqa, and pubmedqa show that question interpretation diversity consistently leads to better ensemble accuracy compared to model diversity. Furthermore, our analysis of GPT and LLaMa shows that model diversity typically produces results between the best and the worst ensemble members without clear improvement.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based Text Classifiers</title>
<link>https://arxiv.org/abs/2507.21186</link>
<guid>https://arxiv.org/abs/2507.21186</guid>
<content:encoded><![CDATA[
<div> Transformer, AI research, activation-based attribution, Contrast-CAT, interpretability <br />
<br />
Summary: 
The article discusses the challenges in explaining the decisions of transformer-based AI models, particularly in text classification tasks. Activation-based attribution methods, while effective, can be unreliable due to the presence of class-irrelevant features within activations. To address this issue, the authors propose a novel method called Contrast-CAT, which filters out such features to provide clearer and more accurate attribution maps. Experimental results demonstrate that Contrast-CAT outperforms existing methods, with significant improvements in interpretability metrics under the MoRF setting. The findings highlight the importance of enhancing interpretability in transformer models to foster trust and safe deployment in real-world applications. <div>
arXiv:2507.21186v1 Announce Type: new 
Abstract: Transformers have profoundly influenced AI research, but explaining their decisions remains challenging -- even for relatively simpler tasks such as classification -- which hinders trust and safe deployment in real-world applications. Although activation-based attribution methods effectively explain transformer-based text classification models, our findings reveal that these methods can be undermined by class-irrelevant features within activations, leading to less reliable interpretations. To address this limitation, we propose Contrast-CAT, a novel activation contrast-based attribution method that refines token-level attributions by filtering out class-irrelevant features. By contrasting the activations of an input sequence with reference activations, Contrast-CAT generates clearer and more faithful attribution maps. Experimental results across various datasets and models confirm that Contrast-CAT consistently outperforms state-of-the-art methods. Notably, under the MoRF setting, it achieves average improvements of x1.30 in AOPC and x2.25 in LOdds over the most competing methods, demonstrating its effectiveness in enhancing interpretability for transformer-based text classification.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Public Perception of Crime in Bangladesh: A Transformer-Based Approach with Explainability</title>
<link>https://arxiv.org/abs/2507.21234</link>
<guid>https://arxiv.org/abs/2507.21234</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, crime perception, sentiment analysis, Bangla language, transformer-based model 

Summary: 
This study focuses on analyzing the evolving public perception of crime-related news on social media platforms, particularly in the Bangla language. The researchers curated a dataset of 28,528 Bangla-language social media comments to classify them into positive, negative, or neutral categories. They developed a transformer-based model using the XLM-RoBERTa Base architecture, achieving a high classification accuracy of 97%, surpassing existing methods in Bangla sentiment analysis. Additionally, an explainable AI technique was implemented to identify key features influencing sentiment classification, enhancing model interpretability. The study highlights the effectiveness of transformer-based models in processing low-resource languages like Bengali and their potential to provide actionable insights for public policy formulation and crime prevention strategies. <br /><br />Summary: <div>
arXiv:2507.21234v1 Announce Type: new 
Abstract: In recent years, social media platforms have become prominent spaces for individuals to express their opinions on ongoing events, including criminal incidents. As a result, public sentiment can shift dynamically over time. This study investigates the evolving public perception of crime-related news by classifying user-generated comments into three categories: positive, negative, and neutral. A newly curated dataset comprising 28,528 Bangla-language social media comments was developed for this purpose. We propose a transformer-based model utilizing the XLM-RoBERTa Base architecture, which achieves a classification accuracy of 97%, outperforming existing state-of-the-art methods in Bangla sentiment analysis. To enhance model interpretability, explainable AI technique is employed to identify the most influential features driving sentiment classification. The results underscore the effectiveness of transformer-based models in processing low-resource languages such as Bengali and demonstrate their potential to extract actionable insights that can support public policy formulation and crime prevention strategies.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bangla BERT for Hyperpartisan News Detection: A Semi-Supervised and Explainable AI Approach</title>
<link>https://arxiv.org/abs/2507.21242</link>
<guid>https://arxiv.org/abs/2507.21242</guid>
<content:encoded><![CDATA[
<div> Fine-tuned Bangla BERT, hyperpartisan news classification, accuracy score, semi-supervised learning, LIME <br />
<br />
Summary: 
The research focuses on combating misinformation in Bangla by fine-tuning the Bangla BERT model for detecting hyperpartisan news with high accuracy. With a score of 95.65%, Bangla BERT surpasses conventional models, showcasing its effectiveness even in low-resource language environments. The study highlights the utility of transformer models in mitigating biased content dissemination and emphasizes the importance of transparent explanations provided by LIME in building trust in model outcomes. Implementing semi-supervised learning further enhances prediction capabilities, demonstrating the potential for significant improvements in this domain. This research addresses the critical need for advanced natural language processing methods in detecting hyperpartisan news, essential for promoting informed discourse and combating the spread of misinformation. <br /><br /> <div>
arXiv:2507.21242v1 Announce Type: new 
Abstract: In the current digital landscape, misinformation circulates rapidly, shaping public perception and causing societal divisions. It is difficult to identify hyperpartisan news in Bangla since there aren't many sophisticated natural language processing methods available for this low-resource language. Without effective detection methods, biased content can spread unchecked, posing serious risks to informed discourse. To address this gap, our research fine-tunes Bangla BERT. This is a state-of-the-art transformer-based model, designed to enhance classification accuracy for hyperpartisan news. We evaluate its performance against traditional machine learning models and implement semi-supervised learning to enhance predictions further. Not only that, we use LIME to provide transparent explanations of the model's decision-making process, which helps to build trust in its outcomes. With a remarkable accuracy score of 95.65%, Bangla BERT outperforms conventional approaches, according to our trial data. The findings of this study demonstrate the usefulness of transformer models even in environments with limited resources, which opens the door to further improvements in this area.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can human clinical rationales improve the performance and explainability of clinical text classification models?</title>
<link>https://arxiv.org/abs/2507.21302</link>
<guid>https://arxiv.org/abs/2507.21302</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-driven clinical text classification, transformer-based models, human-based clinical rationales, primary cancer site diagnoses, explainability

Summary: 
- The study explores the use of human-based clinical rationales in improving AI-driven clinical text classification for primary cancer site diagnoses.
- 99,125 human-based clinical rationales were analyzed alongside electronic pathology reports to train transformer-based models.
- Clinical rationales as additional training data showed improved model performance in high-resource scenarios but were inconsistent with limited resources.
- Using sufficiency as a metric for pre-selecting rationales yielded inconsistent results.
- Models trained on additional reports outperformed those trained on rationales, suggesting that labeling more reports is more beneficial for model accuracy.
- Training models on rationale-supplemented data slightly improved explainability but resulted in smaller performance improvements compared to additional report training. 

<br /><br />Summary: <div>
arXiv:2507.21302v1 Announce Type: new 
Abstract: AI-driven clinical text classification is vital for explainable automated retrieval of population-level health information. This work investigates whether human-based clinical rationales can serve as additional supervision to improve both performance and explainability of transformer-based models that automatically encode clinical documents. We analyzed 99,125 human-based clinical rationales that provide plausible explanations for primary cancer site diagnoses, using them as additional training samples alongside 128,649 electronic pathology reports to evaluate transformer-based models for extracting primary cancer sites. We also investigated sufficiency as a way to measure rationale quality for pre-selecting rationales. Our results showed that clinical rationales as additional training data can improve model performance in high-resource scenarios but produce inconsistent behavior when resources are limited. Using sufficiency as an automatic metric to preselect rationales also leads to inconsistent results. Importantly, models trained on rationales were consistently outperformed by models trained on additional reports instead. This suggests that clinical rationales don't consistently improve model performance and are outperformed by simply using more reports. Therefore, if the goal is optimizing accuracy, annotation efforts should focus on labeling more reports rather than creating rationales. However, if explainability is the priority, training models on rationale-supplemented data may help them better identify rationale-like features. We conclude that using clinical rationales as additional training data results in smaller performance improvements and only slightly better explainability (measured as average token-level rationale coverage) compared to training on additional reports.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models Understand Morality Across Cultures?</title>
<link>https://arxiv.org/abs/2507.21319</link>
<guid>https://arxiv.org/abs/2507.21319</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, biases, cross-cultural differences, moral perspectives, ethical implications

Summary: 
Recent advancements in large language models (LLMs) have raised concerns about embedded biases, including gender, racial, and cultural biases derived from training data. This study examines how LLMs capture cross-cultural differences in moral perspectives by comparing variances in moral scores, conducting cluster alignment analyses, and probing models with comparative prompts. The results show that current LLMs often fail to accurately represent cross-cultural moral variation, compressing differences and showing low alignment with empirical survey patterns. This highlights the need for improved approaches to mitigate biases and enhance cultural representativeness in LLMs. The implications for the responsible development and global deployment of LLMs are discussed, emphasizing the importance of fairness and ethical alignment. 

Summary: <div>
arXiv:2507.21319v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have established them as powerful tools across numerous domains. However, persistent concerns about embedded biases, such as gender, racial, and cultural biases arising from their training data, raise significant questions about the ethical use and societal consequences of these technologies. This study investigates the extent to which LLMs capture cross-cultural differences and similarities in moral perspectives. Specifically, we examine whether LLM outputs align with patterns observed in international survey data on moral attitudes. To this end, we employ three complementary methods: (1) comparing variances in moral scores produced by models versus those reported in surveys, (2) conducting cluster alignment analyses to assess correspondence between country groupings derived from LLM outputs and survey data, and (3) directly probing models with comparative prompts using systematically chosen token pairs. Our results reveal that current LLMs often fail to reproduce the full spectrum of cross-cultural moral variation, tending to compress differences and exhibit low alignment with empirical survey patterns. These findings highlight a pressing need for more robust approaches to mitigate biases and improve cultural representativeness in LLMs. We conclude by discussing the implications for the responsible development and global deployment of LLMs, emphasizing fairness and ethical alignment.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Automatic Speech Recognition Model for Shona Language</title>
<link>https://arxiv.org/abs/2507.21331</link>
<guid>https://arxiv.org/abs/2507.21331</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep learning, Automatic Speech Recognition, Shona, Under-resourced languages, Data augmentation<br />
Summary:<br /> 
This study developed a deep learning-based Automatic Speech Recognition (ASR) system for the low-resource language of Shona. The research addressed challenges such as limited training data and tonal complexities in Shona speech to improve recognition accuracy compared to traditional models. The study explored the feasibility of deep learning for accurate ASR in Shona, identified challenges in designing deep learning architectures for Shona speech, and compared the performance of deep learning models with statistical ones. The ASR system employed a hybrid architecture with Convolutional Neural Network and Long Short-Term Memory network, along with data augmentation and transfer learning to address data scarcity. Attention mechanisms were used to handle tonal aspects. The system achieved impressive metrics, indicating the potential of deep learning to enhance ASR accuracy for under-resourced languages like Shona.<br /> 
Summary: <div>
arXiv:2507.21331v1 Announce Type: new 
Abstract: This study presented the development of a deep learning-based Automatic Speech Recognition system for Shona, a low-resource language characterized by unique tonal and grammatical complexities. The research aimed to address the challenges posed by limited training data, lack of labelled data, and the intricate tonal nuances present in Shona speech, with the objective of achieving significant improvements in recognition accuracy compared to traditional statistical models. The research first explored the feasibility of using deep learning to develop an accurate ASR system for Shona. Second, it investigated the specific challenges involved in designing and implementing deep learning architectures for Shona speech recognition and proposed strategies to mitigate these challenges. Lastly, it compared the performance of the deep learning-based model with existing statistical models in terms of accuracy. The developed ASR system utilized a hybrid architecture consisting of a Convolutional Neural Network for acoustic modelling and a Long Short-Term Memory network for language modelling. To overcome the scarcity of data, data augmentation techniques and transfer learning were employed. Attention mechanisms were also incorporated to accommodate the tonal nature of Shona speech. The resulting ASR system achieved impressive results, with a Word Error Rate of 29%, Phoneme Error Rate of 12%, and an overall accuracy of 74%. These metrics indicated the potential of deep learning to enhance ASR accuracy for under-resourced languages like Shona. This study contributed to the advancement of ASR technology for under-resourced languages like Shona, ultimately fostering improved accessibility and communication for Shona speakers worldwide.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation</title>
<link>https://arxiv.org/abs/2507.21340</link>
<guid>https://arxiv.org/abs/2507.21340</guid>
<content:encoded><![CDATA[
<div> Framework, benchmarks, key-value extraction, text generation, evaluation strategy

Summary: 
The article introduces StructText, a framework for automatically generating benchmarks for key-value extraction from text using existing tabular data. It utilizes a two-stage pipeline to create synthetic natural-language text aligned with structured sources. The evaluation method combines LLM-based judgments on factuality, hallucination, and coherence, along with objective extraction metrics for accuracy. Results show that LLMs excel in factual accuracy and avoid hallucination but struggle with narrative coherence. Models accurately represent numerical and temporal information but embed it within narratives that resist automated extraction. This work provides datasets, evaluation tools, and baseline extraction systems to support ongoing research in this field. <br /><br />Summary: <div>
arXiv:2507.21340v1 Announce Type: new 
Abstract: Extracting structured information from text, such as key-value pairs that could augment tabular data, is quite useful in many enterprise use cases. Although large language models (LLMs) have enabled numerous automated pipelines for converting natural language into structured formats, there is still a lack of benchmarks for evaluating their extraction quality, especially in specific domains or focused documents specific to a given organization. Building such benchmarks by manual annotations is labour-intensive and limits the size and scalability of the benchmarks. In this work, we present StructText, an end-to-end framework for automatically generating high-fidelity benchmarks for key-value extraction from text using existing tabular data. It uses available tabular data as structured ground truth, and follows a two-stage ``plan-then-execute'' pipeline to synthetically generate corresponding natural-language text. To ensure alignment between text and structured source, we introduce a multi-dimensional evaluation strategy that combines (a) LLM-based judgments on factuality, hallucination, and coherence and (b) objective extraction metrics measuring numeric and temporal accuracy. We evaluated the proposed method on 71,539 examples across 49 datasets. Results reveal that while LLMs achieve strong factual accuracy and avoid hallucination, they struggle with narrative coherence in producing extractable text. Notably, models presume numerical and temporal information with high fidelity yet this information becomes embedded in narratives that resist automated extraction. We release a framework, including datasets, evaluation tools, and baseline extraction systems, to support continued research.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turbocharging Web Automation: The Impact of Compressed History States</title>
<link>https://arxiv.org/abs/2507.21369</link>
<guid>https://arxiv.org/abs/2507.21369</guid>
<content:encoded><![CDATA[
<div> history compressor, web automation, language models, web state, task-relevant information

Summary:
The paper introduces a novel approach to web automation using a history compressor module to distill task-relevant information from history states, enhancing the utilization of history in predicting next actions. It addresses the limitations of current web automation approaches that overlook the importance of history states due to the verbose nature of web page states. By compressing history states into fixed-length representations, the approach improves accuracy by 1.2-5.4% compared to baseline approaches without history inputs. The experiments conducted on Mind2Web and WebLINX datasets demonstrate the effectiveness of the proposed method in leveraging history states for more efficient web automation. <div>
arXiv:2507.21369v1 Announce Type: new 
Abstract: Language models have led to a leap forward in web automation. The current web automation approaches take the current web state, history actions, and language instruction as inputs to predict the next action, overlooking the importance of history states. However, the highly verbose nature of web page states can result in long input sequences and sparse information, hampering the effective utilization of history states. In this paper, we propose a novel web history compressor approach to turbocharge web automation using history states. Our approach employs a history compressor module that distills the most task-relevant information from each history state into a fixed-length short representation, mitigating the challenges posed by the highly verbose history states. Experiments are conducted on the Mind2Web and WebLINX datasets to evaluate the effectiveness of our approach. Results show that our approach obtains 1.2-5.4% absolute accuracy improvements compared to the baseline approach without history inputs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations</title>
<link>https://arxiv.org/abs/2507.21428</link>
<guid>https://arxiv.org/abs/2507.21428</guid>
<content:encoded><![CDATA[
<div> Memory framework, LLM agents, tool management, autonomous, multi-turn interactions

Summary:
The study introduces MemTool, a framework that allows Large Language Model (LLM) agents to manage tools or Model Context Protocol (MCP) server contexts in multi-turn conversations. MemTool offers three modes: Autonomous Agent Mode, Workflow Mode, and Hybrid Mode, for tool management. Experiments on various LLMs show that Autonomous Agent Mode achieves high tool-removal efficiency for reasoning LLMs but lower efficiency for medium-sized models. Workflow and Hybrid modes effectively manage tool removal while Autonomous and Hybrid modes excel at task completion. Trade-offs and recommendations for each MemTool mode are presented based on task accuracy, agency, and model capabilities. This research provides insights for efficient tool management in LLM agents for dynamic multi-turn interactions. 

<br /><br />Summary: <div>
arXiv:2507.21428v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents have shown significant autonomous capabilities in dynamically searching and incorporating relevant tools or Model Context Protocol (MCP) servers for individual queries. However, fixed context windows limit effectiveness in multi-turn interactions requiring repeated, independent tool usage. We introduce MemTool, a short-term memory framework enabling LLM agents to dynamically manage tools or MCP server contexts across multi-turn conversations. MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. In Autonomous Agent Mode, reasoning LLMs achieve high tool-removal efficiency (90-94% over a 3-window average), while medium-sized models exhibit significantly lower efficiency (0-60%). Workflow and Hybrid modes consistently manage tool removal effectively, whereas Autonomous and Hybrid modes excel at task completion. We present trade-offs and recommendations for each MemTool mode based on task accuracy, agency, and model capabilities.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour</title>
<link>https://arxiv.org/abs/2507.21432</link>
<guid>https://arxiv.org/abs/2507.21432</guid>
<content:encoded><![CDATA[
<div> local deployable causal large language models, travel mode choice prediction, LiTransMC, fine-tuned, predictive accuracy, interpretability, behavioural theory, specialist models, transportation research, policy formulation 
Summary: 
This study explores the use of open-access, locally deployable causal large language models (LLMs) for travel mode choice prediction, introducing LiTransMC as a fine-tuned model for this purpose. Evaluating eleven LLMs across multiple datasets, LiTransMC outperformed both untuned local models and larger proprietary systems, showcasing high predictive accuracy and interpretability. By combining structured behavioral prediction with natural language reasoning, this work paves the way for specialist, explainable transport models that can support simulations, policy testing, and behavioral insights. The findings suggest a transformation of general LLMs into specialized tools for transportation research and policy formulation, allowing for privacy, cost reduction, and increased access through local deployment. <div>
arXiv:2507.21432v1 Announce Type: new 
Abstract: This study investigates the adoption of open-access, locally deployable causal large language models (LLMs) for travel mode choice prediction and introduces LiTransMC, the first fine-tuned causal LLM developed for this task. We systematically benchmark eleven LLMs (1-12B parameters) across three stated and revealed preference datasets, testing 396 configurations and generating over 79,000 synthetic commuter predictions. Beyond predictive accuracy, we evaluate models generated reasoning using BERTopic for topic modelling and a novel Explanation Strength Index, providing the first structured analysis of how LLMs articulate decision factors in alignment with behavioural theory. LiTransMC, fine-tuned using parameter efficient and loss masking strategy, achieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of 0.000245, surpassing both untuned local models and larger proprietary systems, including GPT-4o with advanced persona inference and embedding-based loading, while also outperforming classical mode choice methods such as discrete choice models and machine learning classifiers for the same dataset. This dual improvement, i.e., high instant-level accuracy and near-perfect distributional calibration, demonstrates the feasibility of creating specialist, locally deployable LLMs that integrate prediction and interpretability. Through combining structured behavioural prediction with natural language reasoning, this work unlocks the potential for conversational, multi-task transport models capable of supporting agent-based simulations, policy testing, and behavioural insight generation. These findings establish a pathway for transforming general purpose LLMs into specialized, explainable tools for transportation research and policy formulation, while maintaining privacy, reducing cost, and broadening access through local deployment.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench</title>
<link>https://arxiv.org/abs/2507.21476</link>
<guid>https://arxiv.org/abs/2507.21476</guid>
<content:encoded><![CDATA[
<div> Keyword: HumorBench, Language Models, Reasoning, Cartoon Captions, Evaluation

Summary:
- LLMs are evaluated using HumorBench, a benchmark designed to assess their ability to reason about and explain humor in cartoon captions.
- The benchmark includes 300 unique cartoon-caption pairs with expert-annotated evaluation rubrics.
- Models must identify joke elements and form hypotheses to comprehend the humor, showcasing their reasoning abilities.
- Progress in STEM reasoning correlates with success in humor comprehension for LLMs.
- Training models solely on STEM reasoning data still results in strong performance on HumorBench, indicating transferability of reasoning skills.
- Increasing thinking token budgets at test time produces varying results across different models in humor reasoning.

<br /><br />Summary: 
HumorBench evaluates large language models' reasoning and humor comprehension skills using cartoon-caption pairs. Models must identify joke elements and form hypotheses to explain the humor, showcasing their reasoning abilities. Progress in STEM reasoning correlates with success in humor comprehension, demonstrating the transferability of reasoning skills. Training models solely on STEM reasoning data also leads to strong performance on HumorBench. However, increasing thinking token budgets at test time results in mixed outcomes across different models in humor reasoning. <div>
arXiv:2507.21476v1 Announce Type: new 
Abstract: We present HumorBench, a benchmark designed to evaluate large language models' (LLMs) ability to reason about and explain sophisticated humor in cartoon captions. As reasoning models increasingly saturate existing benchmarks in mathematics and science, novel and challenging evaluations of model intelligence beyond STEM domains are essential. Reasoning is fundamentally involved in text-based humor comprehension, requiring the identification of connections between concepts in cartoons/captions and external cultural references, wordplays, and other mechanisms. HumorBench includes approximately 300 unique cartoon-caption pairs from the New Yorker Caption Contest and Cartoonstock.com, with expert-annotated evaluation rubrics identifying essential joke elements. LLMs are evaluated based on their explanations towards the humor and abilities in identifying the joke elements. To perform well on this task, models must form and test hypotheses about associations between concepts, potentially backtracking from initial interpretations to arrive at the most plausible explanation. Our extensive benchmarking of current SOTA models reveals three key insights: (1) LLM progress on STEM reasoning transfers effectively to humor comprehension; (2) models trained exclusively on STEM reasoning data still perform well on HumorBench, demonstrating strong transferability of reasoning abilities; and (3) test-time scaling by increasing thinking token budgets yields mixed results across different models in humor reasoning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs</title>
<link>https://arxiv.org/abs/2507.21482</link>
<guid>https://arxiv.org/abs/2507.21482</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, label-efficient learning, task-diversity, supervised finetuning, data selection 

Summary: 
This paper addresses the challenge of developing high-performing models for specialized applications with limited human annotation resources. By leveraging task-diversity instead of prompt-diversity, the authors propose a simple yet effective sampling strategy for supervised finetuning. The strategy involves selecting examples across tasks using an inverse confidence weighting approach based on the varying levels of confidence of pre-trained models. Experimental results show that this method can achieve better accuracy than training on the complete dataset, with up to a 4% increase in MMLU score. The algorithm consistently outperforms existing methods across various annotation budgets and two instruction finetuning datasets, while reducing annotation costs by up to 80%. This approach provides comparable or superior results to more complex sampling procedures while being easier to implement and less computationally intensive.  <br /><br />Summary: <div>
arXiv:2507.21482v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but developing high-performing models for specialized applications often requires substantial human annotation -- a process that is time-consuming, labor-intensive, and expensive. In this paper, we address the label-efficient learning problem for supervised finetuning (SFT) by leveraging task-diversity as a fundamental principle for effective data selection. This is markedly different from existing methods based on the prompt-diversity. Our approach is based on two key observations: 1) task labels for different prompts are often readily available; 2) pre-trained models have significantly varying levels of confidence across tasks. We combine these facts to devise a simple yet effective sampling strategy: we select examples across tasks using an inverse confidence weighting strategy. This produces models comparable to or better than those trained with more complex sampling procedures, while being significantly easier to implement and less computationally intensive. Notably, our experimental results demonstrate that this method can achieve better accuracy than training on the complete dataset (a 4\% increase in MMLU score). Across various annotation budgets and two instruction finetuning datasets, our algorithm consistently performs at or above the level of the best existing methods, while reducing annotation costs by up to 80\%.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VN-MTEB: Vietnamese Massive Text Embedding Benchmark</title>
<link>https://arxiv.org/abs/2507.21500</link>
<guid>https://arxiv.org/abs/2507.21500</guid>
<content:encoded><![CDATA[
<div> Keywords: Vietnam, internet traffic, online toxicity, embedding models, benchmark

Summary: 
Large-scale test datasets for evaluating AI models in Vietnamese applications are lacking due to the country's high internet traffic and online toxicity. To address this issue, a new Vietnamese benchmark, VN-MTEB, has been introduced by translating samples from the Massive Text Embedding Benchmark. This benchmark consists of 41 datasets across six tasks tailored for Vietnamese text embeddings. Analysis shows that larger and more complex models utilizing Rotary Positional Embedding outperform those using Absolute Positional Embedding in embedding tasks. The benchmark aims to facilitate the evaluation and deployment of embedding models in real-world projects. The datasets are available on HuggingFace. <br /><br />Summary: <div>
arXiv:2507.21500v1 Announce Type: new 
Abstract: Vietnam ranks among the top countries in terms of both internet traffic and online toxicity. As a result, implementing embedding models for recommendation and content control duties in applications is crucial. However, a lack of large-scale test datasets, both in volume and task diversity, makes it tricky for scientists to effectively evaluate AI models before deploying them in real-world, large-scale projects. To solve this important problem, we introduce a Vietnamese benchmark, VN-MTEB for embedding models, which we created by translating a large number of English samples from the Massive Text Embedding Benchmark using our new automated framework. We leverage the strengths of large language models (LLMs) and cutting-edge embedding models to conduct translation and filtering processes to retain high-quality samples, guaranteeing a natural flow of language and semantic fidelity while preserving named entity recognition (NER) and code snippets. Our comprehensive benchmark consists of 41 datasets from six tasks specifically designed for Vietnamese text embeddings. In our analysis, we find that bigger and more complex models using Rotary Positional Embedding outperform those using Absolute Positional Embedding in embedding tasks. Datasets are available at HuggingFace: https://huggingface.co/collections/GreenNode/vn-mteb-68871433f0f7573b8e1a6686
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona Vectors: Monitoring and Controlling Character Traits in Language Models</title>
<link>https://arxiv.org/abs/2507.21509</link>
<guid>https://arxiv.org/abs/2507.21509</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, persona vectors, personality shifts, training data, natural-language description

Summary:
Large language models interact with users through a simulated 'Assistant' persona, which can sometimes deviate from desired traits like honesty or helpfulness. This paper identifies persona vectors underlying traits like evil or sycophancy, which can be used to monitor and control personality shifts during training and deployment. Intended and unintended changes in the Assistant's personality after finetuning are linked to shifts along these persona vectors, highlighting the importance of managing them. A new preventative steering method can help avoid undesirable personality changes, while persona vectors can also flag problematic training data at both dataset and individual sample levels. The automated method for extracting persona vectors can be applied to any personality trait using only a natural-language description. This research offers insights into managing and optimizing the personality of large language models for better user interactions. 

<br /><br />Summary: <div>
arXiv:2507.21509v1 Announce Type: new 
Abstract: Large language models interact with users through a simulated 'Assistant' persona. While the Assistant is typically trained to be helpful, harmless, and honest, it sometimes deviates from these ideals. In this paper, we identify directions in the model's activation space-persona vectors-underlying several traits, such as evil, sycophancy, and propensity to hallucinate. We confirm that these vectors can be used to monitor fluctuations in the Assistant's personality at deployment time. We then apply persona vectors to predict and control personality shifts that occur during training. We find that both intended and unintended personality changes after finetuning are strongly correlated with shifts along the relevant persona vectors. These shifts can be mitigated through post-hoc intervention, or avoided in the first place with a new preventative steering method. Moreover, persona vectors can be used to flag training data that will produce undesirable personality changes, both at the dataset level and the individual sample level. Our method for extracting persona vectors is automated and can be applied to any personality trait of interest, given only a natural-language description.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-free Speculative Decoding for Transformer-based ASR with Token Map Drafting</title>
<link>https://arxiv.org/abs/2507.21522</link>
<guid>https://arxiv.org/abs/2507.21522</guid>
<content:encoded><![CDATA[
<div> Transformer architectures, automatic speech recognition (ASR), speculative decoding, Token Map Drafting, n-gram token map

Summary: 
Token Map Drafting is proposed as a model-free speculative decoding technique for end-to-end ASR systems based on transformer architectures like Whisper. It eliminates the need for a separate draft model by leveraging a precomputed n-gram token map derived from domain-specific training data. This approach significantly accelerates ASR inference in structured, low-perplexity domains without sacrificing transcription accuracy. Experimental results show decoding speed-ups of 1.27x on the CI-AVSR dataset and 1.37x on an internal dataset, with a 10% absolute improvement in decoding speed over the Distill-spec baseline running on CPU. Token Map Drafting proves effective for on-device ASR applications, offering efficient and accurate transcription capabilities on resource-constrained devices. 

<br /><br />Summary: <div>
arXiv:2507.21522v1 Announce Type: new 
Abstract: End-to-end automatic speech recognition (ASR) systems based on transformer architectures, such as Whisper, offer high transcription accuracy and robustness. However, their autoregressive decoding is computationally expensive, hence limiting deployment on CPU-based and resource-constrained devices. Speculative decoding (SD) mitigates this issue by using a smaller draft model to propose candidate tokens, which are then verified by the main model. However, this approach is impractical for devices lacking hardware accelerators like GPUs. To address this, we propose \emph{Token Map Drafting}, a model-free SD technique that eliminates the need for a separate draft model. Instead, we leverage a precomputed n-gram token map derived from domain-specific training data, enabling efficient speculative decoding with minimal overhead. Our method significantly accelerates ASR inference in structured, low-perplexity domains without sacrificing transcription accuracy. Experimental results demonstrate decoding speed-ups of $1.27\times$ on the CI-AVSR dataset and $1.37\times$ on our internal dataset without degrading recognition accuracy. Additionally, our approach achieves a $10\%$ absolute improvement in decoding speed over the Distill-spec baseline running on CPU, highlighting its effectiveness for on-device ASR applications.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TriangleMix: A Lossless and Efficient Attention Pattern for Long Context Prefilling</title>
<link>https://arxiv.org/abs/2507.21526</link>
<guid>https://arxiv.org/abs/2507.21526</guid>
<content:encoded><![CDATA[
<div> static attention, sparse pattern, deep layers, Time-to-First-Token, model accuracy

Summary:
TriangleMix introduces a novel static attention pattern that combines dense attention in shallow layers with a triangle-shaped sparse pattern in deeper layers. This approach reduces attention overhead significantly in deep layers and decreases Time-to-First-Token by 12% to 32% for sequence lengths between 32K to 128K, without compromising model accuracy. The method does not require training and seamlessly integrates with dynamic sparsity techniques, further enhancing LLM inference efficiency. Experimental results show a 3.7x to 15.3x reduction in attention overhead in deep layers, and a 19% acceleration in MInference at 128K sequence length. TriangleMix offers a promising solution to improve computational efficiency in Large Language Models. 

<br /><br />Summary: <div>
arXiv:2507.21526v1 Announce Type: new 
Abstract: Large Language Models (LLMs) rely on attention mechanisms whose time complexity grows quadratically with input sequence length, creating significant computational bottlenecks during the prefilling stage. Existing static sparse attention methods typically degrade accuracy, while dynamic sparsity methods introduce additional computational overhead due to runtime sparse index estimation. To address these limitations, we propose TriangleMix, a novel training-free static attention pattern. TriangleMix employs dense attention in shallow layers and switches to a triangle-shaped sparse pattern in deeper layers. Extensive experiments demonstrate that TriangleMix reduces attention overhead by 3.7x to 15.3x in deep layers, and decreases overall Time-to-First-Token (TTFT) by 12% to 32% for sequence lengths ranging from 32K to 128K, without sacrificing model accuracy. Moreover, TriangleMix can be seamlessly integrated with dynamic sparsity methods to achieve further speedup, e.g. accelerating MInference by 19% at 128K, highlighting its potential to enhance LLM inference efficiency.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Classification of User Requirements from Online Feedback -- A Replication Study</title>
<link>https://arxiv.org/abs/2507.21532</link>
<guid>https://arxiv.org/abs/2507.21532</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural Language Processing, Requirements Engineering, Replication Study, Deep Learning, Classification
Summary: 
This study focuses on replicating and extending a previous study on applying natural language processing (NLP) techniques in requirements engineering (RE). The research highlights the importance of replicating NLP for RE (NLP4RE) studies to enhance the external validity and reproducibility of findings. By reproducing the original results using publicly released source code, the study evaluates different deep learning models for requirement classification from user reviews. Results show diverse reproducibility levels across models, with Naive Bayes demonstrating perfect reproducibility. BERT and ELMo exhibit good generalization capabilities on an external dataset, while GPT-4o performs comparably to traditional machine learning models. The assessment confirms the replication readiness of the baseline study, with enhancements made to increase readiness further. Overall, the study highlights the potential for machine-assisted workflows in RE tasks and underscores the value of replication in advancing NLP4RE research.
<br /><br />Summary: <div>
arXiv:2507.21532v1 Announce Type: new 
Abstract: Natural language processing (NLP) techniques have been widely applied in the requirements engineering (RE) field to support tasks such as classification and ambiguity detection. Although RE research is rooted in empirical investigation, it has paid limited attention to replicating NLP for RE (NLP4RE) studies. The rapidly advancing realm of NLP is creating new opportunities for efficient, machine-assisted workflows, which can bring new perspectives and results to the forefront. Thus, we replicate and extend a previous NLP4RE study (baseline), "Classifying User Requirements from Online Feedback in Small Dataset Environments using Deep Learning", which evaluated different deep learning models for requirement classification from user reviews. We reproduced the original results using publicly released source code, thereby helping to strengthen the external validity of the baseline study. We then extended the setup by evaluating model performance on an external dataset and comparing results to a GPT-4o zero-shot classifier. Furthermore, we prepared the replication study ID-card for the baseline study, important for evaluating replication readiness. Results showed diverse reproducibility levels across different models, with Naive Bayes demonstrating perfect reproducibility. In contrast, BERT and other models showed mixed results. Our findings revealed that baseline deep learning models, BERT and ELMo, exhibited good generalization capabilities on an external dataset, and GPT-4o showed performance comparable to traditional baseline machine learning models. Additionally, our assessment confirmed the baseline study's replication readiness; however missing environment setup files would have further enhanced readiness. We include this missing information in our replication package and provide the replication study ID-card for our study to further encourage and support the replication of our study.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modern Uyghur Dependency Treebank (MUDT): An Integrated Morphosyntactic Framework for a Low-Resource Language</title>
<link>https://arxiv.org/abs/2507.21536</link>
<guid>https://arxiv.org/abs/2507.21536</guid>
<content:encoded><![CDATA[
<div> Keywords: Uyghur, Natural Language Processing, Dependency Annotation, Treebank, Universal Dependencies parser

Summary: 
The study introduces a Dependency Annotation Framework for Uyghur Natural Language Processing to address the resource gap. It includes 18 main relations and 26 subtypes, with specific labels for different functions. A cross-standard evaluation using a pre-trained Universal Dependencies parser revealed a 47.9% discrepancy in annotations, highlighting the need for a tailored approach for Uyghur-specific structures. The Modern Uyghur Dependency Treebank (MUDT), grounded in nine annotation principles, provides a more accurate representation for parsing and NLP tasks. The framework aims to improve parsing accuracy and facilitate downstream NLP tasks by offering a replicable model for other complex languages. <div>
arXiv:2507.21536v1 Announce Type: new 
Abstract: To address a critical resource gap in Uyghur Natural Language Processing (NLP), this study introduces a dependency annotation framework designed to overcome the limitations of existing treebanks for the low-resource, agglutinative language. This inventory includes 18 main relations and 26 subtypes, with specific labels such as cop:zero for verbless clauses and instr:case=loc/dat for nuanced instrumental functions. To empirically validate the necessity of this tailored approach, we conducted a cross-standard evaluation using a pre-trained Universal Dependencies parser. The analysis revealed a systematic 47.9% divergence in annotations, pinpointing the inadequacy of universal schemes for handling Uyghur-specific structures. Grounded in nine annotation principles that ensure typological accuracy and semantic transparency, the Modern Uyghur Dependency Treebank (MUDT) provides a more accurate and semantically transparent representation, designed to enable significant improvements in parsing and downstream NLP tasks, and offers a replicable model for other morphologically complex languages.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGIC: A Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.21544</link>
<guid>https://arxiv.org/abs/2507.21544</guid>
<content:encoded><![CDATA[
<div> conflict detection, retrieval-augmented generation, knowledge graph, MAGIC benchmark, LLMs

Summary: 

The study addresses knowledge conflicts in retrieval-augmented generation systems, proposing a new framework called MAGIC that generates varied conflicts between similar contexts using knowledge graphs. Experiment results show that both open-source and proprietary models struggle with detecting conflicts, particularly in multi-hop reasoning scenarios. These models also struggle to identify the exact sources of contradictions. The study provides insights into how language models deal with conflicting information and suggests improvements for integrating diverse information. <div>
arXiv:2507.21544v1 Announce Type: new 
Abstract: Knowledge conflict often arises in retrieval-augmented generation (RAG) systems, where retrieved documents may be inconsistent with one another or contradict the model's parametric knowledge. Existing benchmarks for investigating the phenomenon have notable limitations, including a narrow focus on the question answering setup, heavy reliance on entity substitution techniques, and a restricted range of conflict types. To address these issues, we propose a knowledge graph (KG)-based framework that generates varied and subtle conflicts between two similar yet distinct contexts, while ensuring interpretability through the explicit relational structure of KGs. Experimental results on our benchmark, MAGIC, provide intriguing insights into the inner workings of LLMs regarding knowledge conflict: both open-source and proprietary models struggle with conflict detection -- especially when multi-hop reasoning is required -- and often fail to pinpoint the exact source of contradictions. Finally, we present in-depth analyses that serve as a foundation for improving LLMs in integrating diverse, sometimes even conflicting, information.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the cognitive reality of Spanish irregular morphomic patterns: Humans vs. Transformers</title>
<link>https://arxiv.org/abs/2507.21556</link>
<guid>https://arxiv.org/abs/2507.21556</guid>
<content:encoded><![CDATA[
<div> transformer-based neural networks, cognitive plausibility, Spanish irregular morphomic pattern, human behavioral data, linguistic phenomena

Summary: 
The study compares transformer-based neural networks to human data in analyzing the Spanish irregular morphomic pattern's cognitive plausibility. Using the same framework as the human study, the research assesses if transformer models can replicate human-like sensitivity to this linguistic phenomenon. The experiments focus on different frequency conditions and reveal that while the models excel in stem and suffix accuracy, they differ in response preferences from humans. Models tend to favor irregular responses and are influenced by the training data distribution. Interestingly, models trained on specific frequency distributions show varying levels of sensitivity to the phonological aspect of real Spanish L-shaped verbs. This study highlights how transformer models perform in simulating human behavior in understanding irregular morphomic patterns in language. <div>
arXiv:2507.21556v1 Announce Type: new 
Abstract: This study investigates the cognitive plausibility of the Spanish irregular morphomic pattern by directly comparing transformer-based neural networks to human behavioral data from \citet{Nevins2015TheRA}. Using the same analytical framework as the original human study, we evaluate whether transformer models can replicate human-like sensitivity to a complex linguistic phenomena, the morphome, under controlled input conditions. Our experiments focus on three frequency conditions: natural, low-frequency, and high-frequency distributions of verbs exhibiting irregular morphomic patterns. While the models outperformed humans in stem and suffix accuracy, a clear divergence emerged in response preferences. Unlike humans, who consistently favored natural responses across all test items, models' preferred irregular responses and were influenced by the proportion of irregular verbs in their training data. Additionally, models trained on the natural and low-frequency distributions, but not the high-frequency distribution, were sensitive to the phonological similarity between test items and real Spanish L-shaped verbs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Hypothesis Distillation of Multilingual Neural Translation Models for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2507.21568</link>
<guid>https://arxiv.org/abs/2507.21568</guid>
<content:encoded><![CDATA[
<div> Keywords: sequence-level knowledge distillation, multilingual translation models, Multi-Hypothesis Distillation, low-resource languages, gender bias amplification 

Summary: 
- The paper explores sequence-level knowledge distillation for multilingual pre-trained encoder-decoder translation models, focusing on the insights provided by the teacher model's output distribution beyond standard decoding methods. 
- It introduces Multi-Hypothesis Distillation (MHD), a method that generates multiple translations for each source sentence to expose the student model to a wider range of target-side prefixes. 
- Utilizing $n$-best lists from beam search guides the student's learning process and addresses issues such as low variability and under-representation of infrequent tokens in alternative decoding methods. 
- For low-resource languages, sampling methods may slightly impact translation quality compared to beam search but enhance corpora variability and lexical richness, leading to improved student model performance. 
- The study also highlights how MHD can help mitigate gender bias amplification often associated with knowledge distillation techniques 

<br /><br />Summary: <div>
arXiv:2507.21568v1 Announce Type: new 
Abstract: This paper explores sequence-level knowledge distillation (KD) of multilingual pre-trained encoder-decoder translation models. We argue that the teacher model's output distribution holds valuable insights for the student, beyond the approximated mode obtained through beam search (the standard decoding method), and present Multi-Hypothesis Distillation (MHD), a sequence-level KD method that generates multiple translations for each source sentence. This provides a larger representation of the teacher model distribution and exposes the student model to a wider range of target-side prefixes. We leverage $n$-best lists from beam search to guide the student's learning and examine alternative decoding methods to address issues like low variability and the under-representation of infrequent tokens. For low-resource languages, our research shows that while sampling methods may slightly compromise translation quality compared to beam search based approaches, they enhance the generated corpora with greater variability and lexical richness. This ultimately improves student model performance and mitigates the gender bias amplification often associated with KD.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual JobBERT for Cross-Lingual Job Title Matching</title>
<link>https://arxiv.org/abs/2507.21609</link>
<guid>https://arxiv.org/abs/2507.21609</guid>
<content:encoded><![CDATA[
<div> cross-lingual job title matching, JobBERT-V3, contrastive learning, multilingual dataset, TalentCLEF 2025<br />
<br />
JobBERT-V3 is a new model for cross-lingual job title matching that extends support to English, German, Spanish, and Chinese by leveraging synthetic translations and a multilingual dataset of over 21 million job titles. The model, based on the state-of-the-art JobBERT-V2, aligns languages efficiently without task-specific supervision. Evaluations on the TalentCLEF 2025 benchmark show that JobBERT-V3 outperforms strong multilingual baselines and performs consistently in both monolingual and cross-lingual settings. Additionally, the model can effectively rank relevant skills for a given job title, showcasing its broader utility in multilingual labor market intelligence. The model is publicly available for use.<br /><br />Summary: <div>
arXiv:2507.21609v1 Announce Type: new 
Abstract: We introduce JobBERT-V3, a contrastive learning-based model for cross-lingual job title matching. Building on the state-of-the-art monolingual JobBERT-V2, our approach extends support to English, German, Spanish, and Chinese by leveraging synthetic translations and a balanced multilingual dataset of over 21 million job titles. The model retains the efficiency-focused architecture of its predecessor while enabling robust alignment across languages without requiring task-specific supervision. Extensive evaluations on the TalentCLEF 2025 benchmark demonstrate that JobBERT-V3 outperforms strong multilingual baselines and achieves consistent performance across both monolingual and cross-lingual settings. While not the primary focus, we also show that the model can be effectively used to rank relevant skills for a given job title, demonstrating its broader applicability in multilingual labor market intelligence. The model is publicly available: https://huggingface.co/TechWolf/JobBERT-v3.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Libra: Assessing and Improving Reward Model by Learning to Think</title>
<link>https://arxiv.org/abs/2507.21645</link>
<guid>https://arxiv.org/abs/2507.21645</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, language models, reasoning scenarios, generative reward model, Libra Bench

Summary:
Reinforcement learning has been instrumental in enhancing the reasoning abilities of language models. However, existing reward models face challenges in complex reasoning scenarios due to rule-based or reference-based rewards. To overcome these limitations, a framework is proposed to evaluate and enhance reward models in challenging reasoning scenarios. A reasoning-oriented benchmark called Libra Bench is introduced, comprising diverse mathematical problems and advanced reasoning models. A novel approach for improving generative reward models through learning-to-think methodologies is presented. The Libra-RM series of generative reward models with reasoning capabilities demonstrates state-of-the-art performance on various benchmarks. Downstream experiments show a correlation between Libra Bench and applications, highlighting the potential of Libra-RM to enhance reasoning models using unlabeled data. <div>
arXiv:2507.21645v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has significantly improved the reasoning ability of large language models. However, current reward models underperform in challenging reasoning scenarios and predominant RL training paradigms rely on rule-based or reference-based rewards, which impose two critical limitations: 1) the dependence on finely annotated reference answer to attain rewards; and 2) the requirement for constrained output format. These limitations fundamentally hinder further RL data scaling and sustained enhancement of model reasoning performance. To address these limitations, we propose a comprehensive framework for evaluating and improving the performance of reward models in complex reasoning scenarios. We first present a reasoning-oriented benchmark (Libra Bench), systematically constructed from a diverse collection of challenging mathematical problems and advanced reasoning models, to address the limitations of existing reward model benchmarks in reasoning scenarios. We further introduce a novel approach for improving the generative reward model via learning-to-think methodologies. Based on the proposed approach, we develop Libra-RM series, a collection of generative reward models with reasoning capabilities that achieve state-of-the-art results on various benchmarks. Comprehensive downstream experiments are conducted and the experimental results demonstrate the correlation between our Libra Bench and downstream application, and the potential of Libra-RM to further improve reasoning models with unlabeled data.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases</title>
<link>https://arxiv.org/abs/2507.21652</link>
<guid>https://arxiv.org/abs/2507.21652</guid>
<content:encoded><![CDATA[
<div> Keywords: large reasoning models, safety alignment, unsafe completions, correction-based supervision, dataset

Summary: 
UnsafeChain is a new safety alignment dataset that addresses the challenge of hard prompts that consistently lead to harmful outputs in large reasoning models (LRMs). By identifying and correcting unsafe completions, the dataset aims to enhance safety while maintaining general reasoning ability. Three LRMs were fine-tuned on UnsafeChain and compared against previous datasets SafeChain and STAR-1 across various benchmarks. UnsafeChain consistently outperformed prior datasets, demonstrating the effectiveness and generalizability of correction-based supervision. Even a subset of 1K examples from UnsafeChain matched or exceeded baseline performance on these benchmarks. The dataset and code are publicly available for further research and development. 

<br /><br />Summary: <div>
arXiv:2507.21652v1 Announce Type: new 
Abstract: As large reasoning models (LRMs) grow more capable, chain-of-thought (CoT) reasoning introduces new safety challenges. Existing SFT-based safety alignment studies dominantly focused on filtering prompts with safe, high-quality responses, while overlooking hard prompts that always elicit harmful outputs. To fill this gap, we introduce UnsafeChain, a safety alignment dataset constructed from hard prompts with diverse sources, where unsafe completions are identified and explicitly corrected into safe responses. By exposing models to unsafe behaviors and guiding their correction, UnsafeChain enhances safety while preserving general reasoning ability. We fine-tune three LRMs on UnsafeChain and compare them against recent SafeChain and STAR-1 across six out-of-distribution and five in-distribution benchmarks. UnsafeChain consistently outperforms prior datasets, with even a 1K subset matching or surpassing baseline performance, demonstrating the effectiveness and generalizability of correction-based supervision. We release our dataset and code at https://github.com/mbzuai-nlp/UnsafeChain
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal</title>
<link>https://arxiv.org/abs/2507.21750</link>
<guid>https://arxiv.org/abs/2507.21750</guid>
<content:encoded><![CDATA[
<div> vulnerable, adversarial attacks, pre-trained language models, robustness, embedding space <br />
Summary: 
The article discusses the vulnerability of pre-trained language models (PLMs) to adversarial attacks and proposes a novel method to enhance their robustness. The proposed approach involves removing instance-level principal components from the embedding space, transforming it to approximate Gaussian properties. This transformation helps reduce susceptibility to adversarial perturbations while maintaining semantic relationships. By aligning embedding distributions, the impact of adversarial noise on decision boundaries is minimized, improving robustness without the need for adversarial examples or expensive training-time augmentation. Experimental results on eight benchmark datasets demonstrate that the approach enhances adversarial robustness while maintaining comparable accuracy to baselines before attacks, striking a balance between robustness and generalization.<br /> <div>
arXiv:2507.21750v1 Announce Type: new 
Abstract: Pre-trained language models (PLMs) have driven substantial progress in natural language processing but remain vulnerable to adversarial attacks, raising concerns about their robustness in real-world applications. Previous studies have sought to mitigate the impact of adversarial attacks by introducing adversarial perturbations into the training process, either implicitly or explicitly. While both strategies enhance robustness, they often incur high computational costs. In this work, we propose a simple yet effective add-on module that enhances the adversarial robustness of PLMs by removing instance-level principal components, without relying on conventional adversarial defences or perturbing the original training data. Our approach transforms the embedding space to approximate Gaussian properties, thereby reducing its susceptibility to adversarial perturbations while preserving semantic relationships. This transformation aligns embedding distributions in a way that minimises the impact of adversarial noise on decision boundaries, enhancing robustness without requiring adversarial examples or costly training-time augmentation. Evaluations on eight benchmark datasets show that our approach improves adversarial robustness while maintaining comparable before-attack accuracy to baselines, achieving a balanced trade-off between robustness and generalisation.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgriEval: A Comprehensive Chinese Agricultural Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2507.21773</link>
<guid>https://arxiv.org/abs/2507.21773</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, AgriEval, agricultural benchmark, data, model performance

Summary:<br /><br />
The article introduces AgriEval, a Chinese agricultural benchmark designed to evaluate the performance of large language models (LLMs) in the agricultural domain. AgriEval encompasses six major agriculture categories and 29 subcategories, catering to four core cognitive scenarios. The dataset, sourced from university-level examinations, offers high-quality data for assessing LLM capabilities. With 14,697 multiple-choice and 2,167 open-ended question-and-answer questions, AgriEval is the most extensive agricultural benchmark currently available. Experimental results on 51 LLMs demonstrate that most models struggle to achieve 60% accuracy, indicating room for improvement in agricultural LLMs. The study also explores factors influencing model performance and suggests strategies for enhancement. AgriEval presents a valuable resource for advancing LLM research in the agricultural sector. The benchmark is publicly accessible on GitHub for further investigation and development. <div>
arXiv:2507.21773v1 Announce Type: new 
Abstract: In the agricultural domain, the deployment of large language models (LLMs) is hindered by the lack of training data and evaluation benchmarks. To mitigate this issue, we propose AgriEval, the first comprehensive Chinese agricultural benchmark with three main characteristics: (1) Comprehensive Capability Evaluation. AgriEval covers six major agriculture categories and 29 subcategories within agriculture, addressing four core cognitive scenarios: memorization, understanding, inference, and generation. (2) High-Quality Data. The dataset is curated from university-level examinations and assignments, providing a natural and robust benchmark for assessing the capacity of LLMs to apply knowledge and make expert-like decisions. (3) Diverse Formats and Extensive Scale. AgriEval comprises 14,697 multiple-choice questions and 2,167 open-ended question-and-answer questions, establishing it as the most extensive agricultural benchmark available to date. We also present comprehensive experimental results over 51 open-source and commercial LLMs. The experimental results reveal that most existing LLMs struggle to achieve 60% accuracy, underscoring the developmental potential in agricultural LLMs. Additionally, we conduct extensive experiments to investigate factors influencing model performance and propose strategies for enhancement. AgriEval is available at https://github.com/YanPioneer/AgriEval/.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Problem with Safety Classification is not just the Models</title>
<link>https://arxiv.org/abs/2507.21782</link>
<guid>https://arxiv.org/abs/2507.21782</guid>
<content:encoded><![CDATA[
<div> multilingual, Large Language Models, safety classification models, evaluation datasets, harmful content<br />
Summary: <br />
The paper focuses on the robustness of Large Language Models (LLMs) to unsafe behaviors, specifically looking at safety classification models in multilingual scenarios. It highlights multilingual disparities in safety classifiers across 18 languages and points out potential issues with evaluation datasets. The study suggests that shortcomings in current safety classifiers are not solely due to the models themselves but also relate to the evaluation datasets. By analyzing 5 safety classification models, the paper aims to contribute to the development of more effective methods for identifying harmful content in LLM inputs across various languages. <div>
arXiv:2507.21782v1 Announce Type: new 
Abstract: Studying the robustness of Large Language Models (LLMs) to unsafe behaviors is an important topic of research today. Building safety classification models or guard models, which are fine-tuned models for input/output safety classification for LLMs, is seen as one of the solutions to address the issue. Although there is a lot of research on the safety testing of LLMs themselves, there is little research on evaluating the effectiveness of such safety classifiers or the evaluation datasets used for testing them, especially in multilingual scenarios. In this position paper, we demonstrate how multilingual disparities exist in 5 safety classification models by considering datasets covering 18 languages. At the same time, we identify potential issues with the evaluation datasets, arguing that the shortcomings of current safety classifiers are not only because of the models themselves. We expect that these findings will contribute to the discussion on developing better methods to identify harmful content in LLM inputs across languages.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartMark: A Structured Grammar for Chart Annotation</title>
<link>https://arxiv.org/abs/2507.21810</link>
<guid>https://arxiv.org/abs/2507.21810</guid>
<content:encoded><![CDATA[
<div> Keywords: ChartMark, structured grammar, visualization accessibility, hierarchical framework, Vega-Lite <br />
Summary: 
ChartMark is a new structured grammar proposed to enhance visualization accessibility by standardizing chart annotations. It separates annotation semantics from visualization implementations, allowing for cross-platform reuse. The hierarchical framework of ChartMark accommodates various annotation dimensions, such as task and chart context, enabling the expression of both abstract intents and precise visual details. The toolkit developed for ChartMark demonstrates its flexibility and practical applicability by converting ChartMark specifications into Vega-Lite visualizations. This approach offers a standardized and efficient way to enhance visualization accessibility and facilitate the creation of visually engaging and informative charts across different platforms.<br /><br />Summary: <div>
arXiv:2507.21810v1 Announce Type: new 
Abstract: Chart annotations enhance visualization accessibility but suffer from fragmented, non-standardized representations that limit cross-platform reuse. We propose ChartMark, a structured grammar that separates annotation semantics from visualization implementations. ChartMark features a hierarchical framework mapping onto annotation dimensions (e.g., task, chart context), supporting both abstract intents and precise visual details. Our toolkit demonstrates converting ChartMark specifications into Vega-Lite visualizations, highlighting its flexibility, expressiveness, and practical applicability.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overview of ADoBo at IberLEF 2025: Automatic Detection of Anglicisms in Spanish</title>
<link>https://arxiv.org/abs/2507.21813</link>
<guid>https://arxiv.org/abs/2507.21813</guid>
<content:encoded><![CDATA[
<div> identification, anglicism, Spanish, ADoBo 2025, IberLEF 2025 <br />
Summary: <br />
This paper presents the findings of ADoBo 2025, a shared task focusing on identifying anglicisms in Spanish text, introduced within the framework of IberLEF 2025. Participants were tasked with detecting English lexical borrowings within a dataset of Spanish journalistic texts. Five teams participated in the test phase, employing a range of approaches including LLMs, deep learning models, Transformer-based models, and rule-based systems. Results varied significantly, with F1 scores ranging from 0.17 to 0.99, highlighting the diversity in performance across different systems for this particular task. <div>
arXiv:2507.21813v1 Announce Type: new 
Abstract: This paper summarizes the main findings of ADoBo 2025, the shared task on anglicism identification in Spanish proposed in the context of IberLEF 2025. Participants of ADoBo 2025 were asked to detect English lexical borrowings (or anglicisms) from a collection of Spanish journalistic texts. Five teams submitted their solutions for the test phase. Proposed systems included LLMs, deep learning models, Transformer-based models and rule-based systems. The results range from F1 scores of 0.17 to 0.99, which showcases the variability in performance different systems can have for this task.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to Support People Who Use Drugs</title>
<link>https://arxiv.org/abs/2507.21815</link>
<guid>https://arxiv.org/abs/2507.21815</guid>
<content:encoded><![CDATA[
<div> Keywords: harm reduction, substance use, large language models, benchmark, safety risks

Summary: 
The article introduces a benchmark called HRIPBench, designed to evaluate the accuracy and safety risks of large language models (LLMs) in providing harm reduction information for individuals who use drugs (PWUD). The benchmark dataset HRIP-Basic consists of 2,160 question-answer-evidence pairs covering tasks such as checking safety boundaries, providing quantitative values, and inferring polysubstance use risks. Results show that current state-of-the-art LLMs struggle to provide accurate harm reduction information and can pose severe safety risks to PWUD. It is crucial to cautiously limit the use of LLMs in harm reduction contexts to prevent negative health outcomes. The study highlights the importance of considering safety and accuracy when utilizing LLMs in providing information for individuals dealing with substance use issues. 

<br /><br />Summary: <div>
arXiv:2507.21815v1 Announce Type: new 
Abstract: Millions of individuals' well-being are challenged by the harms of substance use. Harm reduction as a public health strategy is designed to improve their health outcomes and reduce safety risks. Some large language models (LLMs) have demonstrated a decent level of medical knowledge, promising to address the information needs of people who use drugs (PWUD). However, their performance in relevant tasks remains largely unexplored. We introduce HRIPBench, a benchmark designed to evaluate LLM's accuracy and safety risks in harm reduction information provision. The benchmark dataset HRIP-Basic has 2,160 question-answer-evidence pairs. The scope covers three tasks: checking safety boundaries, providing quantitative values, and inferring polysubstance use risks. We build the Instruction and RAG schemes to evaluate model behaviours based on their inherent knowledge and the integration of domain knowledge. Our results indicate that state-of-the-art LLMs still struggle to provide accurate harm reduction information, and sometimes, carry out severe safety risks to PWUD. The use of LLMs in harm reduction contexts should be cautiously constrained to avoid inducing negative health outcomes. WARNING: This paper contains illicit content that potentially induces harms.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling Adjectival Modification Effects on Semantic Plausibility</title>
<link>https://arxiv.org/abs/2507.21828</link>
<guid>https://arxiv.org/abs/2507.21828</guid>
<content:encoded><![CDATA[
<div> transformer-based models, sentence transformers, ADEPT challenge benchmark, adjectival modifier, plausibility

Summary: 
This article focuses on the task of capturing changes in plausibility triggered by modifying events, specifically adjectival modifiers. The study tackles the ADEPT challenge benchmark, using sentence transformers and transformer-based models. The results reveal that both models struggle with the task, with sentence transformers even underperforming compared to models like RoBERTa. The study emphasizes the importance of a more balanced evaluation method, showing that imbalances can distort model performance and evaluation metrics, undermining result reliability. This research sheds light on the difficulties in modeling changes in plausibility due to event modification and highlights the need for a more realistic evaluation approach in natural language processing tasks. <div>
arXiv:2507.21828v1 Announce Type: new 
Abstract: While the task of assessing the plausibility of events such as ''news is relevant'' has been addressed by a growing body of work, less attention has been paid to capturing changes in plausibility as triggered by event modification. Understanding changes in plausibility is relevant for tasks such as dialogue generation, commonsense reasoning, and hallucination detection as it allows to correctly model, for example, ''gentle sarcasm'' as a sign of closeness rather than unkindness among friends [9]. In this work, we tackle the ADEPT challenge benchmark [6] consisting of 16K English sentence pairs differing by exactly one adjectival modifier. Our modeling experiments provide a conceptually novel method by using sentence transformers, and reveal that both they and transformer-based models struggle with the task at hand, and sentence transformers - despite their conceptual alignment with the task - even under-perform in comparison to models like RoBERTa. Furthermore, an in-depth comparison with prior work highlights the importance of a more realistic, balanced evaluation method: imbalances distort model performance and evaluation metrics, and weaken result trustworthiness.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences</title>
<link>https://arxiv.org/abs/2507.21831</link>
<guid>https://arxiv.org/abs/2507.21831</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, automated coding, prompting strategies, task performance, Mistral NeMo

Summary: 
This article introduces HALC$, a pipeline designed to create optimal prompts for coding tasks using large language models (LLMs). The study conducted over two million requests using local LLMs to test prompting strategies and task performance. Results showed that prompts can reliably code for single variables and across two variables using the Mistral NeMo LLM. The prompts were designed to align the LLM with the codebook, rather than adapting the codebook for the LLM. The paper offers insights into the effectiveness of different prompting strategies, key influencing factors, and the identification of reliable prompts for coding tasks using LLMs. This research aims to streamline the process of automated coding in the social sciences by providing a systematic and reliable method for constructing prompts. 

<br /><br />Summary: <div>
arXiv:2507.21831v1 Announce Type: new 
Abstract: LLMs are seeing widespread use for task automation, including automated coding in the social sciences. However, even though researchers have proposed different prompting strategies, their effectiveness varies across LLMs and tasks. Often trial and error practices are still widespread. We propose HALC$-$a general pipeline that allows for the systematic and reliable construction of optimal prompts for any given coding task and model, permitting the integration of any prompting strategy deemed relevant. To investigate LLM coding and validate our pipeline, we sent a total of 1,512 individual prompts to our local LLMs in over two million requests. We test prompting strategies and LLM task performance based on few expert codings (ground truth). When compared to these expert codings, we find prompts that code reliably for single variables (${\alpha}$climate = .76; ${\alpha}$movement = .78) and across two variables (${\alpha}$climate = .71; ${\alpha}$movement = .74) using the LLM Mistral NeMo. Our prompting strategies are set up in a way that aligns the LLM to our codebook$-$we are not optimizing our codebook for LLM friendliness. Our paper provides insights into the effectiveness of different prompting strategies, crucial influencing factors, and the identification of reliable prompts for each coding task and model.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.21836</link>
<guid>https://arxiv.org/abs/2507.21836</guid>
<content:encoded><![CDATA[
arXiv:2507.21836v1 Announce Type: new 
Abstract: Large Language Models (LLMs), when enhanced through reasoning-oriented post-training, evolve into powerful Large Reasoning Models (LRMs). Tool-Integrated Reasoning (TIR) further extends their capabilities by incorporating external tools, but existing methods often rely on rigid, predefined tool-use patterns that risk degrading core language competence. Inspired by the human ability to adaptively select tools, we introduce AutoTIR, a reinforcement learning framework that enables LLMs to autonomously decide whether and which tool to invoke during the reasoning process, rather than following static tool-use strategies. AutoTIR leverages a hybrid reward mechanism that jointly optimizes for task-specific answer correctness, structured output adherence, and penalization of incorrect tool usage, thereby encouraging both precise reasoning and efficient tool integration. Extensive evaluations across diverse knowledge-intensive, mathematical, and general language modeling tasks demonstrate that AutoTIR achieves superior overall performance, significantly outperforming baselines and exhibits superior generalization in tool-use behavior. These results highlight the promise of reinforcement learning in building truly generalizable and scalable TIR capabilities in LLMs. The code and data are available at https://github.com/weiyifan1023/AutoTIR.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.21892</link>
<guid>https://arxiv.org/abs/2507.21892</guid>
<content:encoded><![CDATA[
arXiv:2507.21892v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucination in LLMs by incorporating external knowledge, but relies on chunk-based retrieval that lacks structural semantics. GraphRAG methods improve RAG by modeling knowledge as entity-relation graphs, but still face challenges in high construction cost, fixed one-time retrieval, and reliance on long-context reasoning and prompt design. To address these challenges, we propose Graph-R1, an agentic GraphRAG framework via end-to-end reinforcement learning (RL). It introduces lightweight knowledge hypergraph construction, models retrieval as a multi-turn agent-environment interaction, and optimizes the agent process via an end-to-end reward mechanism. Experiments on standard RAG datasets show that Graph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in reasoning accuracy, retrieval efficiency, and generation quality.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rote Learning Considered Useful: Generalizing over Memorized Data in LLMs</title>
<link>https://arxiv.org/abs/2507.21914</link>
<guid>https://arxiv.org/abs/2507.21914</guid>
<content:encoded><![CDATA[
arXiv:2507.21914v1 Announce Type: new 
Abstract: Rote learning is a memorization technique based on repetition. It is commonly believed to hinder generalization by encouraging verbatim memorization rather than deeper understanding. This insight holds for even learning factual knowledge that inevitably requires a certain degree of memorization. In this work, we demonstrate that LLMs can be trained to generalize from rote memorized data. We introduce a two-phase memorize-then-generalize framework, where the model first rote memorizes factual subject-object associations using a semantically meaningless token and then learns to generalize by fine-tuning on a small set of semantically meaningful prompts. Extensive experiments over 8 LLMs show that the models can reinterpret rote memorized data through the semantically meaningful prompts, as evidenced by the emergence of structured, semantically aligned latent representations between the two. This surprising finding opens the door to both effective and efficient knowledge injection and possible risks of repurposing the memorized data for malicious usage.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training language models to be warm and empathetic makes them less reliable and more sycophantic</title>
<link>https://arxiv.org/abs/2507.21919</link>
<guid>https://arxiv.org/abs/2507.21919</guid>
<content:encoded><![CDATA[
arXiv:2507.21919v1 Announce Type: new 
Abstract: Artificial intelligence (AI) developers are increasingly building language models with warm and empathetic personas that millions of people now use for advice, therapy, and companionship. Here, we show how this creates a significant trade-off: optimizing language models for warmth undermines their reliability, especially when users express vulnerability. We conducted controlled experiments on five language models of varying sizes and architectures, training them to produce warmer, more empathetic responses, then evaluating them on safety-critical tasks. Warm models showed substantially higher error rates (+10 to +30 percentage points) than their original counterparts, promoting conspiracy theories, providing incorrect factual information, and offering problematic medical advice. They were also significantly more likely to validate incorrect user beliefs, particularly when user messages expressed sadness. Importantly, these effects were consistent across different model architectures, and occurred despite preserved performance on standard benchmarks, revealing systematic risks that current evaluation practices may fail to detect. As human-like AI systems are deployed at an unprecedented scale, our findings indicate a need to rethink how we develop and oversee these systems that are reshaping human relationships and social interaction.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Training Large Language Models via Reinforcement Learning from Self-Feedback</title>
<link>https://arxiv.org/abs/2507.21931</link>
<guid>https://arxiv.org/abs/2507.21931</guid>
<content:encoded><![CDATA[
arXiv:2507.21931v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often produce plausible but poorly-calibrated answers, limiting their reliability on reasoning-intensive tasks. We present Reinforcement Learning from Self-Feedback (RLSF), a post-training stage that uses the model's own confidence as an intrinsic reward, mimicking how humans learn in the absence of external feedback. After a frozen LLM generates several chain-of-thought solutions, we define and compute the confidence of each final answer span and rank the traces accordingly. These synthetic preferences are then used to fine-tune the policy with standard preference optimization, similar to RLHF yet requiring no human labels, gold answers, or externally curated rewards.
  RLSF simultaneously (i) refines the model's probability estimates -- restoring well-behaved calibration -- and (ii) strengthens step-by-step reasoning, yielding improved performance on arithmetic reasoning and multiple-choice question answering.
  By turning a model's own uncertainty into useful self-feedback, RLSF affirms reinforcement learning on intrinsic model behaviour as a principled and data-efficient component of the LLM post-training pipeline and warrents further research in intrinsic rewards for LLM post-training.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation</title>
<link>https://arxiv.org/abs/2507.21934</link>
<guid>https://arxiv.org/abs/2507.21934</guid>
<content:encoded><![CDATA[
arXiv:2507.21934v1 Announce Type: new 
Abstract: In cross-cultural recipe adaptation, the goal is not only to ensure cultural appropriateness and retain the original dish's essence, but also to provide diverse options for various dietary needs and preferences. Retrieval Augmented Generation (RAG) is a promising approach, combining the retrieval of real recipes from the target cuisine for cultural adaptability with large language models (LLMs) for relevance. However, it remains unclear whether RAG can generate diverse adaptation results. Our analysis shows that RAG tends to overly rely on a limited portion of the context across generations, failing to produce diverse outputs even when provided with varied contextual inputs. This reveals a key limitation of RAG in creative tasks with multiple valid answers: it fails to leverage contextual diversity for generating varied responses. To address this issue, we propose CARRIAGE, a plug-and-play RAG framework for cross-cultural recipe adaptation that enhances diversity in both retrieval and context organization. To our knowledge, this is the first RAG framework that explicitly aims to generate highly diverse outputs to accommodate multiple user preferences. Our experiments show that CARRIAGE achieves Pareto efficiency in terms of diversity and quality of recipe adaptation compared to closed-book LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models</title>
<link>https://arxiv.org/abs/2507.21980</link>
<guid>https://arxiv.org/abs/2507.21980</guid>
<content:encoded><![CDATA[
arXiv:2507.21980v1 Announce Type: new 
Abstract: Traditional machine learning models struggle to generalize in microbiome studies where only metadata is available, especially in small-sample settings or across studies with heterogeneous label formats. In this work, we explore the use of large language models (LLMs) to classify microbial samples into ontology categories such as EMPO 3 and related biological labels, as well as to predict pathogen contamination risk, specifically the presence of E. Coli, using environmental metadata alone. We evaluate LLMs such as ChatGPT-4o, Claude 3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing their performance against traditional models like Random Forests across multiple real-world datasets. Our results show that LLMs not only outperform baselines in ontology classification, but also demonstrate strong predictive ability for contamination risk, generalizing across sites and metadata distributions. These findings suggest that LLMs can effectively reason over sparse, heterogeneous biological metadata and offer a promising metadata-only approach for environmental microbiology and biosurveillance applications.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router</title>
<link>https://arxiv.org/abs/2507.22050</link>
<guid>https://arxiv.org/abs/2507.22050</guid>
<content:encoded><![CDATA[
arXiv:2507.22050v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at many reasoning tasks but struggle with knowledge-intensive queries due to their inability to dynamically access up-to-date or domain-specific information. Retrieval-Augmented Generation (RAG) has emerged as a promising solution, enabling LLMs to ground their responses in external sources. However, existing RAG methods lack fine-grained control over both the query and source sides, often resulting in noisy retrieval and shallow reasoning. In this work, we introduce DeepSieve, an agentic RAG framework that incorporates information sieving via LLM-as-a-knowledge-router. DeepSieve decomposes complex queries into structured sub-questions and recursively routes each to the most suitable knowledge source, filtering irrelevant information through a multi-stage distillation process. Our design emphasizes modularity, transparency, and adaptability, leveraging recent advances in agentic system design. Experiments on multi-hop QA tasks across heterogeneous sources demonstrate improved reasoning depth, retrieval precision, and interpretability over conventional RAG approaches.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning</title>
<link>https://arxiv.org/abs/2507.17307</link>
<guid>https://arxiv.org/abs/2507.17307</guid>
<content:encoded><![CDATA[
arXiv:2507.17307v2 Announce Type: cross 
Abstract: Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of large language models by encouraging step-by-step intermediate reasoning during inference. While effective, CoT introduces substantial computational overhead due to its reliance on autoregressive decoding over long token sequences. Existing acceleration strategies either reduce sequence length through early stopping or compressive reward designs, or improve decoding speed via speculative decoding with smaller models. However, speculative decoding suffers from limited speedup when the agreement between small and large models is low, and fails to exploit the potential advantages of small models in producing concise intermediate reasoning. In this paper, we present R-Stitch, a token-level, confidence-based hybrid decoding framework that accelerates CoT inference by switching between a small language model (SLM) and a large language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to generate tokens by default and delegates to the LLM only when the SLM's confidence falls below a threshold. This design avoids full-sequence rollback and selectively invokes the LLM on uncertain steps, preserving both efficiency and answer quality. R-Stitch is model-agnostic, training-free, and compatible with standard decoding pipelines. Experiments on math reasoning benchmarks demonstrate that R-Stitch achieves up to 85\% reduction in inference latency with negligible accuracy drop, highlighting its practical effectiveness in accelerating CoT reasoning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Reason About Trust?: A Pilot Study</title>
<link>https://arxiv.org/abs/2507.21075</link>
<guid>https://arxiv.org/abs/2507.21075</guid>
<content:encoded><![CDATA[
arXiv:2507.21075v1 Announce Type: cross 
Abstract: In human society, trust is an essential component of social attitude that helps build and maintain long-term, healthy relationships which creates a strong foundation for cooperation, enabling individuals to work together effectively and achieve shared goals. As many human interactions occur through electronic means such as using mobile apps, the potential arises for AI systems to assist users in understanding the social state of their relationships. In this paper we investigate the ability of Large Language Models (LLMs) to reason about trust between two individuals in an environment which requires fostering trust relationships. We also assess whether LLMs are capable of inducing trust by role-playing one party in a trust based interaction and planning actions which can instil trust.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotionally Aware Moderation: The Potential of Emotion Monitoring in Shaping Healthier Social Media Conversations</title>
<link>https://arxiv.org/abs/2507.21089</link>
<guid>https://arxiv.org/abs/2507.21089</guid>
<content:encoded><![CDATA[
arXiv:2507.21089v1 Announce Type: cross 
Abstract: Social media platforms increasingly employ proactive moderation techniques, such as detecting and curbing toxic and uncivil comments, to prevent the spread of harmful content. Despite these efforts, such approaches are often criticized for creating a climate of censorship and failing to address the underlying causes of uncivil behavior. Our work makes both theoretical and practical contributions by proposing and evaluating two types of emotion monitoring dashboards to users' emotional awareness and mitigate hate speech. In a study involving 211 participants, we evaluate the effects of the two mechanisms on user commenting behavior and emotional experiences. The results reveal that these interventions effectively increase users' awareness of their emotional states and reduce hate speech. However, our findings also indicate potential unintended effects, including increased expression of negative emotions (Angry, Fear, and Sad) when discussing sensitive issues. These insights provide a basis for further research on integrating proactive emotion regulation tools into social media platforms to foster healthier digital interactions.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analise Semantica Automatizada com LLM e RAG para Bulas Farmaceuticas</title>
<link>https://arxiv.org/abs/2507.21103</link>
<guid>https://arxiv.org/abs/2507.21103</guid>
<content:encoded><![CDATA[
arXiv:2507.21103v1 Announce Type: cross 
Abstract: The production of digital documents has been growing rapidly in academic, business, and health environments, presenting new challenges in the efficient extraction and analysis of unstructured information. This work investigates the use of RAG (Retrieval-Augmented Generation) architectures combined with Large-Scale Language Models (LLMs) to automate the analysis of documents in PDF format. The proposal integrates vector search techniques by embeddings, semantic data extraction and generation of contextualized natural language responses. To validate the approach, we conducted experiments with drug package inserts extracted from official public sources. The semantic queries applied were evaluated by metrics such as accuracy, completeness, response speed and consistency. The results indicate that the combination of RAG with LLMs offers significant gains in intelligent information retrieval and interpretation of unstructured technical texts.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentMaster: A Multi-Agent Conversational Framework Using A2A and MCP Protocols for Multimodal Information Retrieval and Analysis</title>
<link>https://arxiv.org/abs/2507.21105</link>
<guid>https://arxiv.org/abs/2507.21105</guid>
<content:encoded><![CDATA[
arXiv:2507.21105v1 Announce Type: cross 
Abstract: The rise of Multi-Agent Systems (MAS) in Artificial Intelligence (AI), especially integrated with Large Language Models (LLMs), has greatly facilitated the resolution of complex tasks. However, current systems are still facing challenges of inter-agent communication, coordination, and interaction with heterogeneous tools and resources. Most recently, the Model Context Protocol (MCP) by Anthropic and Agent-to-Agent (A2A) communication protocol by Google have been introduced, and to the best of our knowledge, very few applications exist where both protocols are employed within a single MAS framework. We present a pilot study of AgentMaster, a novel modular multi-protocol MAS framework with self-implemented A2A and MCP, enabling dynamic coordination and flexible communication. Through a unified conversational interface, the system supports natural language interaction without prior technical expertise and responds to multimodal queries for tasks including information retrieval, question answering, and image analysis. Evaluation through the BERTScore F1 and LLM-as-a-Judge metric G-Eval averaged 96.3\% and 87.1\%, revealing robust inter-agent coordination, query decomposition, dynamic routing, and domain-specific, relevant responses. Overall, our proposed framework contributes to the potential capabilities of domain-specific, cooperative, and scalable conversational AI powered by MAS.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OneShield -- the Next Generation of LLM Guardrails</title>
<link>https://arxiv.org/abs/2507.21170</link>
<guid>https://arxiv.org/abs/2507.21170</guid>
<content:encoded><![CDATA[
arXiv:2507.21170v1 Announce Type: cross 
Abstract: The rise of Large Language Models has created a general excitement about the great potential for a myriad of applications. While LLMs offer many possibilities, questions about safety, privacy, and ethics have emerged, and all the key actors are working to address these issues with protective measures for their own models and standalone solutions. The constantly evolving nature of LLMs makes the task of universally shielding users against their potential risks extremely challenging, and one-size-fits-all solutions unfeasible. In this work, we propose OneShield, our stand-alone, model-agnostic and customizable solution to safeguard LLMs. OneShield aims to provide facilities for defining risk factors, expressing and declaring contextual safety and compliance policies, and mitigating LLM risks, with a focus on each specific customer. We describe the implementation of the framework, the scalability considerations and provide usage statistics of OneShield since its first deployment.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge</title>
<link>https://arxiv.org/abs/2507.21183</link>
<guid>https://arxiv.org/abs/2507.21183</guid>
<content:encoded><![CDATA[
arXiv:2507.21183v1 Announce Type: cross 
Abstract: As the era of large language models (LLMs) on behalf of users unfolds, Preference Optimization (PO) methods have become a central approach to aligning LLMs with human preferences and improving performance. We propose Maximum a Posteriori Preference Optimization (MaPPO), a framework for learning from preferences that explicitly incorporates prior reward knowledge into the optimization objective. While existing methods such as Direct Preference Optimization (DPO) and its variants treat preference learning as a Maximum Likelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating prior reward estimates into a principled Maximum a Posteriori (MaP) objective. This not only generalizes DPO and its variants, but also enhances alignment by mitigating the oversimplified binary classification of responses. More importantly, MaPPO introduces no additional hyperparameter, and supports preference optimization in both offline and online settings. In addition, MaPPO can be used as a plugin with consistent improvement on DPO variants, including widely used SimPO, IPO, and CPO. Extensive empirical evaluations of different model sizes and model series on three standard benchmarks, including MT-Bench, AlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in alignment performance without sacrificing computational efficiency.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoSLD: Automated Neural Scaling Law Discovery With Large Language Models</title>
<link>https://arxiv.org/abs/2507.21184</link>
<guid>https://arxiv.org/abs/2507.21184</guid>
<content:encoded><![CDATA[
arXiv:2507.21184v1 Announce Type: cross 
Abstract: Scaling laws are fundamental mathematical relationships that predict how neural network performance evolves with changes in variables such as model size, dataset size, and computational resources. Traditionally, discovering these laws requires extensive human expertise and manual experimentation. We introduce EvoSLD, an automated framework for Scaling Law Discovery (SLD) that leverages evolutionary algorithms guided by Large Language Models (LLMs) to co-evolve symbolic expressions and their optimization routines. Formulated to handle scaling variables, control variables, and response metrics across diverse experimental settings, EvoSLD searches for parsimonious, universal functional forms that minimize fitting errors on grouped data subsets. Evaluated on five real-world scenarios from recent literature, EvoSLD rediscovers exact human-derived laws in two cases and surpasses them in others, achieving up to orders-of-magnitude reductions in normalized mean squared error on held-out test sets. Compared to baselines like symbolic regression and ablated variants, EvoSLD demonstrates superior accuracy, interpretability, and efficiency, highlighting its potential to accelerate AI research. Code is available at https://github.com/linhaowei1/SLD.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting</title>
<link>https://arxiv.org/abs/2507.21257</link>
<guid>https://arxiv.org/abs/2507.21257</guid>
<content:encoded><![CDATA[
arXiv:2507.21257v1 Announce Type: cross 
Abstract: Language interpretation is a compositional process, in which the meaning of more complex linguistic structures is inferred from the meaning of their parts. Large language models possess remarkable language interpretation capabilities and have been successfully applied to interpret questions by mapping them to SPARQL queries. An open question is how systematic this interpretation process is. Toward this question, in this paper, we propose a benchmark for investigating to what extent the abilities of LLMs to interpret questions are actually compositional. For this, we generate three datasets of varying difficulty based on graph patterns in DBpedia, relying on Lemon lexica for verbalization. Our datasets are created in a very controlled fashion in order to test the ability of LLMs to interpret structurally complex questions, given that they have seen the atomic building blocks. This allows us to evaluate to what degree LLMs are able to interpret complex questions for which they "understand" the atomic parts. We conduct experiments with models of different sizes using both various prompt and few-shot optimization techniques as well as fine-tuning. Our results show that performance in terms of macro $F_1$ degrades from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the samples optimized on. Even when all necessary information was provided to the model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of lowest complexity. We thus conclude that LLMs struggle to systematically and compositionally interpret questions and map them into SPARQL queries.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems</title>
<link>https://arxiv.org/abs/2507.21276</link>
<guid>https://arxiv.org/abs/2507.21276</guid>
<content:encoded><![CDATA[
arXiv:2507.21276v1 Announce Type: cross 
Abstract: Modern deployment of large language models (LLMs) frequently involves both inference serving and continuous retraining to stay aligned with evolving data and user feedback. Common practices separate these workloads onto distinct servers in isolated phases, causing substantial inefficiencies (e.g., GPU idleness) and delayed adaptation to new data in distributed settings. Our empirical analysis reveals that these inefficiencies stem from dynamic request arrivals during serving and workload heterogeneity in pipeline-parallel training. To address these challenges, we propose LeMix, a system for co-locating and managing concurrent LLM serving and training workloads. LeMix integrates offline profiling, execution prediction mechanisms, and runtime scheduling to dynamically adapt resource allocation based on workload characteristics and system conditions. By understanding task-specific behaviors and co-execution interference across shared nodes, LeMix improves utilization and serving quality without compromising serving responsiveness. Our evaluation shows that LeMix improves throughput by up to 3.53x, reduces inference loss by up to 0.61x, and delivers up to 2.12x higher response time SLO attainment over traditional separate setups. To our knowledge, this is the first work to uncover and exploit the opportunities of joint LLM inference and training, paving the way for more resource-efficient deployment of LLMs in production environments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Language Models To Gather Information Proactively</title>
<link>https://arxiv.org/abs/2507.21389</link>
<guid>https://arxiv.org/abs/2507.21389</guid>
<content:encoded><![CDATA[
arXiv:2507.21389v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly expected to function as collaborative partners, engaging in back-and-forth dialogue to solve complex, ambiguous problems. However, current LLMs often falter in real-world settings, defaulting to passive responses or narrow clarifications when faced with incomplete or under-specified prompts, falling short of proactively gathering the missing information that is crucial for high-quality solutions. In this work, we introduce a new task paradigm: proactive information gathering, where LLMs must identify gaps in the provided context and strategically elicit implicit user knowledge through targeted questions. To systematically study and train this capability, we design a scalable framework that generates partially specified, real-world tasks, masking key information and simulating authentic ambiguity. Within this setup, our core innovation is a reinforcement finetuning strategy that rewards questions that elicit genuinely new, implicit user information -- such as hidden domain expertise or fine-grained requirements -- that would otherwise remain unspoken. Experiments demonstrate that our trained Qwen-2.5-7B model significantly outperforms o3-mini by 18% on automatic evaluation metrics. More importantly, human evaluation reveals that clarification questions and final outlines generated by our model are favored by human annotators by 42% and 28% respectively. Together, these results highlight the value of proactive clarification in elevating LLMs from passive text generators to genuinely collaborative thought partners.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal LLMs as Customized Reward Models for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2507.21391</link>
<guid>https://arxiv.org/abs/2507.21391</guid>
<content:encoded><![CDATA[
arXiv:2507.21391v1 Announce Type: cross 
Abstract: We introduce LLaVA-Reward, an efficient reward model designed to automatically evaluate text-to-image (T2I) generations across multiple perspectives, leveraging pretrained multimodal large language models (MLLMs). Existing MLLM-based approaches require instruction-following data for supervised fine-tuning and evaluate generation quality on analyzing text response, which is time-consuming and difficult to train. To address this problem, we propose LLaVA-Reward, which directly utilizes the hidden states of MLLMs given text-image pairs. To enhance the bidirectional interaction between visual and textual representations in decoder-only MLLMs, we further propose adding a Skip-connection Cross Attention (SkipCA) module. This design enhances text-image correlation reasoning by connecting early-layer visual features with later-layer hidden representations.In addition, LLaVA-Reward supports different types of preference data for efficient fine-tuning, including paired preference data and unpaired data. We train LLaVA-Reward on four evaluation perspectives: text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based methods in generating human-aligned scores for automatic evaluations and inference-time scaling in text-to-image generations.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReGATE: Learning Faster and Better with Fewer Tokens in MLLMs</title>
<link>https://arxiv.org/abs/2507.21420</link>
<guid>https://arxiv.org/abs/2507.21420</guid>
<content:encoded><![CDATA[
arXiv:2507.21420v1 Announce Type: cross 
Abstract: The computational cost of training multimodal large language models (MLLMs) rapidly increases with the number of tokens involved. Existing efficiency methods primarily target inference and rely on token reduction or merging, offering limited benefit during training. In this paper, we propose ReGATE (Reference$-$Guided Adaptive Token Elision), an adaptive token pruning method for accelerating MLLM training. Specifically, ReGATE adopts a teacher-student framework in which the MLLM being trained serves as the student, and a frozen reference large language model (LLM) acts as the teacher. The teacher computes per-token reference losses, which are combined with an exponential moving average (EMA) of the student's own difficulty scores. This adaptive difficulty-based scoring enables the selective processing of crucial tokens while bypassing less informative ones in the forward pass, significantly reducing computational overhead. Experiments demonstrate that ReGATE, when applied to VideoLLaMA2, matches the peak accuracy of standard training on MVBench up to 2$\times$ faster, using only 35% of the tokens. With additional training, it even surpasses the baseline on several multimodal benchmarks, all while reducing the total token count by over 41%. Code and models will be released soon.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Does it Mean for a Neural Network to Learn a "World Model"?</title>
<link>https://arxiv.org/abs/2507.21513</link>
<guid>https://arxiv.org/abs/2507.21513</guid>
<content:encoded><![CDATA[
arXiv:2507.21513v1 Announce Type: cross 
Abstract: We propose a set of precise criteria for saying a neural net learns and uses a "world model." The goal is to give an operational meaning to terms that are often used informally, in order to provide a common language for experimental investigation. We focus specifically on the idea of representing a latent "state space" of the world, leaving modeling the effect of actions to future work. Our definition is based on ideas from the linear probing literature, and formalizes the notion of a computation that factors through a representation of the data generation process. An essential addition to the definition is a set of conditions to check that such a "world model" is not a trivial consequence of the neural net's data or task.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who's important? -- SUnSET: Synergistic Understanding of Stakeholder, Events and Time for Timeline Generation</title>
<link>https://arxiv.org/abs/2507.21903</link>
<guid>https://arxiv.org/abs/2507.21903</guid>
<content:encoded><![CDATA[
arXiv:2507.21903v1 Announce Type: cross 
Abstract: As news reporting becomes increasingly global and decentralized online, tracking related events across multiple sources presents significant challenges. Existing news summarization methods typically utilizes Large Language Models and Graphical methods on article-based summaries. However, this is not effective since it only considers the textual content of similarly dated articles to understand the gist of the event. To counteract the lack of analysis on the parties involved, it is essential to come up with a novel framework to gauge the importance of stakeholders and the connection of related events through the relevant entities involved. Therefore, we present SUnSET: Synergistic Understanding of Stakeholder, Events and Time for the task of Timeline Summarization (TLS). We leverage powerful Large Language Models (LLMs) to build SET triplets and introduced the use of stakeholder-based ranking to construct a $Relevancy$ metric, which can be extended into general situations. Our experimental results outperform all prior baselines and emerged as the new State-of-the-Art, highlighting the impact of stakeholder information within news article.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding</title>
<link>https://arxiv.org/abs/2507.22025</link>
<guid>https://arxiv.org/abs/2507.22025</guid>
<content:encoded><![CDATA[
arXiv:2507.22025v1 Announce Type: cross 
Abstract: The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from a dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE, a comprehensive framework enhancing GUI agents at both the training and inference stages. For training, we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) a Continuous Reward function to incentivize high-precision grounding; 2) a "Simple Thinking" reward to balance planning with speed and grounding accuracy; and 3) a Cropping-based Resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present Decomposed Grounding with Selection, a novel method that dramatically improves grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art performance on two benchmarks ScreenSpot-Pro and ScreenSpot-v2. For instance, using both our proposed training and inference enhancement methods brings 23% grounding accuracy improvement over the best baseline on ScreenSpot-Pro.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UserBench: An Interactive Gym Environment for User-Centric Agents</title>
<link>https://arxiv.org/abs/2507.22034</link>
<guid>https://arxiv.org/abs/2507.22034</guid>
<content:encoded><![CDATA[
arXiv:2507.22034v1 Announce Type: cross 
Abstract: Large Language Models (LLMs)-based agents have made impressive progress in reasoning and tool use, enabling them to solve complex tasks. However, their ability to proactively collaborate with users, especially when goals are vague, evolving, or indirectly expressed, remains underexplored. To address this gap, we introduce UserBench, a user-centric benchmark designed to evaluate agents in multi-turn, preference-driven interactions. UserBench features simulated users who start with underspecified goals and reveal preferences incrementally, requiring agents to proactively clarify intent and make grounded decisions with tools. Our evaluation of leading open- and closed-source LLMs reveals a significant disconnect between task completion and user alignment. For instance, models provide answers that fully align with all user intents only 20% of the time on average, and even the most advanced models uncover fewer than 30% of all user preferences through active interaction. These results highlight the challenges of building agents that are not just capable task executors, but true collaborative partners. UserBench offers an interactive environment to measure and advance this critical capability.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaCLIP 2: A Worldwide Scaling Recipe</title>
<link>https://arxiv.org/abs/2507.22062</link>
<guid>https://arxiv.org/abs/2507.22062</guid>
<content:encoded><![CDATA[
arXiv:2507.22062v1 Announce Type: cross 
Abstract: Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., "curse of multilinguality" that is common in LLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, MetaCLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The pitfalls of next-token prediction</title>
<link>https://arxiv.org/abs/2403.06963</link>
<guid>https://arxiv.org/abs/2403.06963</guid>
<content:encoded><![CDATA[
arXiv:2403.06963v3 Announce Type: replace 
Abstract: Can a mere next-token predictor faithfully model human intelligence? We crystallize this emerging concern and correct popular misconceptions surrounding it, and advocate a simple multi-token objective.
  As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn.
  Finally, we provide preliminary evidence that this failure can be resolved using _teacherless_ training, a simple modification using dummy tokens that predicts multiple tokens in advance. We hope this finding can ground future debates and inspire explorations beyond the next-token prediction paradigm. We make our code available under https://github.com/gregorbachmann/Next-Token-Failures
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Arithmetic for Language Expansion in Speech Translation</title>
<link>https://arxiv.org/abs/2409.11274</link>
<guid>https://arxiv.org/abs/2409.11274</guid>
<content:encoded><![CDATA[
arXiv:2409.11274v3 Announce Type: replace 
Abstract: Recent progress in large language models (LLMs) has gained interest in speech-text multimodal foundation models, achieving strong performance on instruction-tuned speech translation (ST). However, expanding language pairs is costly due to re-training on combined new and previous datasets. To address this, we aim to build a one-to-many ST system from existing one-to-one ST systems using task arithmetic without re-training. Direct application of task arithmetic in ST leads to language confusion; therefore, we introduce an augmented task arithmetic method incorporating a language control model to ensure correct target language generation. Our experiments on MuST-C and CoVoST-2 show BLEU score improvements of up to 4.66 and 4.92, with COMET gains of 8.87 and 11.83. In addition, we demonstrate our framework can extend to language pairs lacking paired ST training data or pre-trained ST models by synthesizing ST models based on existing machine translation (MT) and ST models via task analogies.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulated patient systems are intelligent when powered by large language model-based AI agents</title>
<link>https://arxiv.org/abs/2409.18924</link>
<guid>https://arxiv.org/abs/2409.18924</guid>
<content:encoded><![CDATA[
arXiv:2409.18924v3 Announce Type: replace 
Abstract: Simulated patient systems play an important role in modern medical education and research, providing safe, integrative medical training environments and supporting clinical decision-making simulations. We developed AIPatient, an intelligent simulated patient system powered by large language model-based AI agents. The system incorporates the Retrieval Augmented Generation (RAG) framework, powered by six task-specific LLM-based AI agents for complex reasoning. For simulation reality, the system is also powered by the AIPatient KG (Knowledge Graph), built with de-identified real patient data from the Medical Information Mart for Intensive Care (MIMIC)-III database. Primary outcomes showcase the system's intelligence, including the system's accuracy in Electronic Record (EHR)-based medical Question Answering (QA), readability, robustness, and stability. The system achieved a QA accuracy of 94.15% when all six AI agents present, surpassing benchmarks with partial or no agent integration. Its knowledgebase demonstrated high validity (F1 score=0.89). Readability scores showed median Flesch Reading Ease at 77.23 and median Flesch Kincaid Grade at 5.6, indicating accessibility to all medical professionals. Robustness and stability were confirmed with non-significant variance (ANOVA F-value=0.6126, p > 0.1; F-value=0.782, p > 0.1). A user study with medical students further demonstrated that AIPatient offers high fidelity, strong usability, and effective educational value, performing comparably or better than human-simulated patients in medical history-taking scenarios. The promising intelligence of the AIPatient system highlights its potential to support a wide range of applications, including medical education, model evaluation, and system integration.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data</title>
<link>https://arxiv.org/abs/2410.16491</link>
<guid>https://arxiv.org/abs/2410.16491</guid>
<content:encoded><![CDATA[
arXiv:2410.16491v3 Announce Type: replace 
Abstract: In this work, we tackle the challenge of embedding realistic human personality traits into LLMs. Previous approaches have primarily focused on prompt-based methods that describe the behavior associated with the desired personality traits, suffering from realism and validity issues. To address these limitations, we introduce BIG5-CHAT, a large-scale dataset containing 100,000 dialogues designed to ground models in how humans express their personality in language. Leveraging this dataset, we explore Supervised Fine-Tuning and Direct Preference Optimization as training-based methods to align LLMs more naturally with human personality patterns. Our methods outperform prompting on personality assessments such as BFI and IPIP-NEO, with trait correlations more closely matching human data. Furthermore, our experiments reveal that models trained to exhibit higher conscientiousness, higher agreeableness, lower extraversion, and lower neuroticism display better performance on reasoning tasks, aligning with psychological findings on how these traits impact human cognitive performance. To our knowledge, this work is the first comprehensive study to demonstrate how training-based methods can shape LLM personalities through learning from real human behaviors.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pralekha: Cross-Lingual Document Alignment for Indic Languages</title>
<link>https://arxiv.org/abs/2411.19096</link>
<guid>https://arxiv.org/abs/2411.19096</guid>
<content:encoded><![CDATA[
arXiv:2411.19096v2 Announce Type: replace 
Abstract: Mining parallel document pairs for document-level machine translation (MT) remains challenging due to the limitations of existing Cross-Lingual Document Alignment (CLDA) techniques. Most approaches rely on metadata such as URLs, which is often unavailable in low-resource language settings, while others represent documents using pooled sentence embeddings, which fail to capture fine-grained alignment cues. Moreover, current sentence embedding models have limited context windows, hindering their ability to represent document-level information effectively. To address these challenges for Indic languages, we introduce PRALEKHA, a large-scale benchmark for evaluating document-level alignment techniques. It contains over 3 million aligned document pairs across 11 Indic languages and English, of which 1.5 million are English--Indic pairs. Furthermore, we propose Document Alignment Coefficient (DAC), a novel metric for fine-grained document alignment. Unlike pooling-based approaches, DAC aligns documents by matching smaller chunks and computes similarity as the ratio of aligned chunks to the average number of chunks in a pair. Intrinsic evaluation shows that DAC achieves substantial improvements over pooling-based baselines, particularly in noisy scenarios. Extrinsic evaluation further demonstrates that document MT models trained on DAC-aligned pairs consistently outperform those using baseline alignment methods. These results highlight DAC's effectiveness for parallel document mining. The PRALEKHA dataset and CLDA evaluation framework will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning</title>
<link>https://arxiv.org/abs/2502.18978</link>
<guid>https://arxiv.org/abs/2502.18978</guid>
<content:encoded><![CDATA[
arXiv:2502.18978v4 Announce Type: replace 
Abstract: The effectiveness of instruction fine-tuning for Large Language Models is fundamentally constrained by the quality and efficiency of training datasets. This work introduces Low-Confidence Gold (LCG), a novel filtering framework that employs centroid-based clustering and confidence-guided selection for identifying valuable instruction pairs. Through a semi-supervised approach using a lightweight classifier trained on representative samples, LCG curates high-quality subsets while preserving data diversity. Experimental evaluation demonstrates that models fine-tuned on LCG-filtered subsets of 6K samples achieve superior performance compared to existing methods, with substantial improvements on MT-bench and consistent gains across comprehensive evaluation metrics. The framework's efficacy while maintaining model performance establishes a promising direction for efficient instruction tuning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Narrative Context Protocol: An Open-Source Storytelling Framework for Generative AI</title>
<link>https://arxiv.org/abs/2503.04844</link>
<guid>https://arxiv.org/abs/2503.04844</guid>
<content:encoded><![CDATA[
arXiv:2503.04844v5 Announce Type: replace 
Abstract: Here we introduce Narrative Context Protocol (NCP), an open-source narrative standard designed to enable narrative interoperability, AI-driven authoring tools, real-time emergent narratives, and more. By encoding a story's structure in a "Storyform," which is a structured register of its narrative features, NCP enables narrative portability across systems as well as intent-based constraints for generative storytelling systems. We demonstrate the capabilities of NCP through a year-long experiment, during which an author used NCP and a custom authoring platform to create a playable, text-based experience based on her pre-existing novella. This experience is driven by generative AI, with unconstrained natural language input. NCP functions as a set of "guardrails" that allows the generative system to accommodate player agency while also ensuring that narrative context and coherence are maintained.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training LLM-based Tutors to Improve Student Learning Outcomes in Dialogues</title>
<link>https://arxiv.org/abs/2503.06424</link>
<guid>https://arxiv.org/abs/2503.06424</guid>
<content:encoded><![CDATA[
arXiv:2503.06424v2 Announce Type: replace 
Abstract: Generative artificial intelligence (AI) has the potential to scale up personalized tutoring through large language models (LLMs). Recent AI tutors are adapted for the tutoring task by training or prompting LLMs to follow effective pedagogical principles, though they are not trained to maximize student learning throughout the course of a dialogue. Therefore, they may engage with students in a suboptimal way. We address this limitation by introducing an approach to train LLMs to generate tutor utterances that maximize the likelihood of student correctness, while still encouraging the model to follow good pedagogical practice. Specifically, we generate a set of candidate tutor utterances and score them using (1) an LLM-based student model to predict the chance of correct student responses and (2) a pedagogical rubric evaluated by GPT-4o. We then use the resulting data to train an open-source LLM, Llama 3.1 8B, using direct preference optimization. We show that tutor utterances generated by our model lead to significantly higher chances of correct student responses while maintaining the pedagogical quality of GPT-4o. We also conduct qualitative analyses and a human evaluation to demonstrate that our model generates high quality tutor utterances.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Levels of Analysis for Large Language Models</title>
<link>https://arxiv.org/abs/2503.13401</link>
<guid>https://arxiv.org/abs/2503.13401</guid>
<content:encoded><![CDATA[
arXiv:2503.13401v2 Announce Type: replace 
Abstract: Modern artificial intelligence systems, such as large language models, are increasingly powerful but also increasingly hard to understand. Recognizing this problem as analogous to the historical difficulties in understanding the human mind, we argue that methods developed in cognitive science can be useful for understanding large language models. We propose a framework for applying these methods based on the levels of analysis that David Marr proposed for studying information processing systems. By revisiting established cognitive science techniques relevant to each level and illustrating their potential to yield insights into the behavior and internal organization of large language models, we aim to provide a toolkit for making sense of these new kinds of minds.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG-CLIP : Learning EEG representations from natural language descriptions</title>
<link>https://arxiv.org/abs/2503.16531</link>
<guid>https://arxiv.org/abs/2503.16531</guid>
<content:encoded><![CDATA[
arXiv:2503.16531v2 Announce Type: replace 
Abstract: Deep networks for electroencephalogram (EEG) decoding are often only trained to solve one specific task, such as pathology or age decoding. A more general task-agnostic approach is to train deep networks to match a (clinical) EEG recording to its corresponding textual medical report and vice versa. This approach was pioneered in the computer vision domain matching images and their text captions and subsequently allowed to do successful zero-shot decoding using textual class prompts. In this work, we follow this approach and develop a contrastive learning framework, EEG-CLIP, that aligns the EEG time series and the descriptions of the corresponding clinical text in a shared embedding space. We investigated its potential for versatile EEG decoding, evaluating performance in a range of few-shot and zero-shot settings. Overall, we show that EEG-CLIP manages to non-trivially align text and EEG representations. Our work presents a promising approach to learn general EEG representations, which could enable easier analyses of diverse decoding questions through zero-shot decoding or training task-specific models from fewer training examples. The code for reproducing our results is available at https://github.com/tidiane-camaret/EEGClip
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Whose Side Are You On?" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection</title>
<link>https://arxiv.org/abs/2503.20797</link>
<guid>https://arxiv.org/abs/2503.20797</guid>
<content:encoded><![CDATA[
arXiv:2503.20797v2 Announce Type: replace 
Abstract: The rapid growth of social media platforms has led to concerns about radicalization, filter bubbles, and content bias. Existing approaches to classifying ideology are limited in that they require extensive human effort, the labeling of large datasets, and are not able to adapt to evolving ideological contexts. This paper explores the potential of Large Language Models (LLMs) for classifying the political ideology of online content in the context of the two-party US political spectrum through in-context learning (ICL). Our extensive experiments involving demonstration selection in label-balanced fashion, conducted on three datasets comprising news articles and YouTube videos, reveal that our approach significantly outperforms zero-shot and traditional supervised methods. Additionally, we evaluate the influence of metadata (e.g., content source and descriptions) on ideological classification and discuss its implications. Finally, we show how providing the source for political and non-political content influences the LLM's classification.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Reported Cutoff: Where Large Language Models Fall Short on Financial Knowledge</title>
<link>https://arxiv.org/abs/2504.00042</link>
<guid>https://arxiv.org/abs/2504.00042</guid>
<content:encoded><![CDATA[
arXiv:2504.00042v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are frequently utilized as sources of knowledge for question-answering. While it is known that LLMs may lack access to real-time data or newer data produced after the model's cutoff date, it is less clear how their knowledge spans across historical information. In this study, we assess the breadth of LLMs' knowledge using financial data of U.S. publicly traded companies by evaluating more than 197k questions and comparing model responses to factual data. We further explore the impact of company characteristics, such as size, retail investment, institutional attention, and readability of financial filings, on the accuracy of knowledge represented in LLMs. Our results reveal that LLMs are less informed about past financial performance, but they display a stronger awareness of larger companies and more recent information. Interestingly, at the same time, our analysis also reveals that LLMs are more likely to hallucinate for larger companies, especially for data from more recent years. The code, prompts, and model outputs are available on GitHub.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>My Life in Artificial Intelligence: People, anecdotes, and some lessons learnt</title>
<link>https://arxiv.org/abs/2504.04142</link>
<guid>https://arxiv.org/abs/2504.04142</guid>
<content:encoded><![CDATA[
arXiv:2504.04142v2 Announce Type: replace 
Abstract: In this very personal workography, I relate my 40-year experiences as a researcher and educator in and around Artificial Intelligence (AI), more specifically Natural Language Processing. I describe how curiosity, and the circumstances of the day, led me to work in both industry and academia, and in various countries, including The Netherlands (Amsterdam, Eindhoven, and Utrecht), the USA (Stanford), England (Brighton), Scotland (Aberdeen), and China (Beijing and Harbin). People and anecdotes play a large role in my story; the history of AI forms its backdrop. I focus on things that might be of interest to (even) younger colleagues, given the choices they face in their own work and life at a time when AI is finally emerging from the shadows.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing then Editing Response Personality of Large Language Models</title>
<link>https://arxiv.org/abs/2504.10227</link>
<guid>https://arxiv.org/abs/2504.10227</guid>
<content:encoded><![CDATA[
arXiv:2504.10227v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated promising capabilities to generate responses that simulate consistent personality traits. Despite the major attempts to analyze personality expression through output-based evaluations, little is known about how such traits are internally encoded within LLM parameters. In this paper, we introduce a layer-wise probing framework to systematically investigate the layer-wise capability of LLMs in simulating personality for responding. We conduct probing experiments on 11 open-source LLMs over the PersonalityEdit benchmark and find that LLMs predominantly simulate personality for responding in their middle and upper layers, with instruction-tuned models demonstrating a slightly clearer separation of personality traits. Furthermore, by interpreting the trained probing hyperplane as a layer-wise boundary for each personality category, we propose a layer-wise perturbation method to edit the personality expressed by LLMs during inference. Our results show that even when the prompt explicitly specifies a particular personality, our method can still successfully alter the response personality of LLMs. Interestingly, the difficulty of converting between certain personality traits varies substantially, which aligns with the representational distances in our probing experiments. Finally, we conduct a comprehensive MMLU benchmark evaluation and time overhead analysis, demonstrating that our proposed personality editing method incurs only minimal degradation in general capabilities while maintaining low training costs and acceptable inference latency. Our code is publicly available at https://github.com/universe-sky/probing-then-editing-personality.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ai2 Scholar QA: Organized Literature Synthesis with Attribution</title>
<link>https://arxiv.org/abs/2504.10861</link>
<guid>https://arxiv.org/abs/2504.10861</guid>
<content:encoded><![CDATA[
arXiv:2504.10861v2 Announce Type: replace 
Abstract: Retrieval-augmented generation is increasingly effective in answering scientific questions from literature, but many state-of-the-art systems are expensive and closed-source. We introduce Ai2 Scholar QA, a free online scientific question answering application. To facilitate research, we make our entire pipeline public: as a customizable open-source Python package and interactive web app, along with paper indexes accessible through public APIs and downloadable datasets. We describe our system in detail and present experiments analyzing its key design decisions. In an evaluation on a recent scientific QA benchmark, we find that Ai2 Scholar QA outperforms competing systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FB-RAG: Improving RAG with Forward and Backward Lookup</title>
<link>https://arxiv.org/abs/2505.17206</link>
<guid>https://arxiv.org/abs/2505.17206</guid>
<content:encoded><![CDATA[
arXiv:2505.17206v2 Announce Type: replace 
Abstract: Traditional Retrieval-Augmented Generation (RAG) struggles with complex queries that lack strong signals to retrieve the most relevant context, forcing a trade-off between choosing a small context that misses key information and a large context that confuses the LLM. To address this, we propose Forward-Backward RAG (FB-RAG), a new training-free framework based on a simple yet powerful forward-looking strategy. FB-RAG employs a light-weight LLM to peek into potential future generations, using evidence from multiple sampled outputs to precisely identify the most relevant context for a final, more powerful generator. This improves performance without complex finetuning or Reinforcement Learning common in prior work. Across 9 datasets, FB-RAG consistently delivers strong results. Further, the performance gains can be achieved with reduced latency due to a shorter, more focused prompt for the powerful generator. On EN.QA dataset, FB-RAG matches the leading baseline with over 48% latency reduction or achieves an 8% performance improvement with a 10% latency reduction. Our analysis finds cases where even when the forward-looking LLM fails to generate correct answers, its attempts are sufficient to guide the final model to an accurate response, demonstrating how smaller LLMs can systematically improve the performance and efficiency of larger ones.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHIMERA: A Knowledge Base of Scientific Idea Recombinations for Research Analysis and Ideation</title>
<link>https://arxiv.org/abs/2505.20779</link>
<guid>https://arxiv.org/abs/2505.20779</guid>
<content:encoded><![CDATA[
arXiv:2505.20779v4 Announce Type: replace 
Abstract: A hallmark of human innovation is recombination -- the creation of novel ideas by integrating elements from existing concepts and mechanisms. In this work, we introduce CHIMERA, a large-scale Knowledge Base (KB) of over 28K recombination examples automatically mined from the scientific literature. CHIMERA enables large-scale empirical analysis of how scientists recombine concepts and draw inspiration from different areas, and enables training models that propose novel, cross-disciplinary research directions. To construct this KB, we define a new information extraction task: identifying recombination instances in scientific abstracts. We curate a high-quality, expert-annotated dataset and use it to fine-tune a large language model, which we apply to a broad corpus of AI papers. We showcase the utility of CHIMERA through two applications. First, we analyze patterns of recombination across AI subfields. Second, we train a scientific hypothesis generation model using the KB, showing that it can propose novel research directions that researchers rate as inspiring. We release our data and code at https://github.com/noy-sternlicht/CHIMERA-KB.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression</title>
<link>https://arxiv.org/abs/2505.23966</link>
<guid>https://arxiv.org/abs/2505.23966</guid>
<content:encoded><![CDATA[
arXiv:2505.23966v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have enabled remarkable progress in natural language processing, yet their high computational and memory demands pose challenges for deployment in resource-constrained environments. Although recent low-rank decomposition methods offer a promising path for structural compression, they often suffer from accuracy degradation, expensive calibration procedures, and result in inefficient model architectures that hinder real-world inference speedups. In this paper, we propose FLAT-LLM, a fast and accurate, training-free structural compression method based on fine-grained low-rank transformations in the activation space. Specifically, we reduce the hidden dimension by transforming the weights using truncated eigenvectors computed via head-wise Principal Component Analysis, and employ a greedy budget redistribution strategy to adaptively allocate ranks across decoders. FLAT-LLM achieves efficient and effective weight compression without recovery fine-tuning, which could complete the calibration within a few minutes. Evaluated across 5 models and 11 datasets, FLAT-LLM outperforms structural pruning baselines in generalization and downstream performance, while delivering inference speedups over decomposition-based methods.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs</title>
<link>https://arxiv.org/abs/2506.05413</link>
<guid>https://arxiv.org/abs/2506.05413</guid>
<content:encoded><![CDATA[
arXiv:2506.05413v2 Announce Type: replace 
Abstract: We present SmoothRot, a novel post-training quantization technique to enhance the efficiency of 4-bit quantization in Large Language Models (LLMs). SmoothRot addresses the critical challenge of massive activation outliers, by integrating channel-wise scaling with Hadamard transformations. Our technique effectively transforms extreme outliers into quantization-friendly activations, significantly improving quantization accuracy. Experiments conducted on popular LLMs (LLaMA2 7B, LLaMA3.1 8B, and Mistral 7B) demonstrate that SmoothRot consistently reduces the performance gap between quantized and FP16 models by approximately 10-30\% across language generation and zero-shot reasoning tasks, without introducing additional inference latency. Code is available at https://github.com/czakop/smoothrot.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.05714</link>
<guid>https://arxiv.org/abs/2507.05714</guid>
<content:encoded><![CDATA[
arXiv:2507.05714v2 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) has become a fundamental paradigm for addressing the challenges faced by large language models in handling real-time information and domain-specific problems. Traditional RAG systems primarily rely on the in-context learning (ICL) capabilities of the large language model itself. Still, in-depth research on the specific capabilities needed by the RAG generation model is lacking, leading to challenges with inconsistent document quality and retrieval system imperfections. Even the limited studies that fine-tune RAG generative models often \textit{lack a granular focus on RAG task} or \textit{a deeper utilization of chain-of-thought processes}. To address this, we propose that RAG models should possess three progressively hierarchical abilities (1) Filtering: the ability to select relevant information; (2) Combination: the ability to combine semantic information across paragraphs; and (3) RAG-specific reasoning: the ability to further process external knowledge using internal knowledge. Thus, we introduce our new RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation (HIRAG) incorporates a "think before answering" strategy. This method enhances the model's open-book examination capability by utilizing multi-level progressive chain-of-thought. Experiments show that the HIRAG training strategy significantly improves the model's performance on datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FrugalRAG: Learning to retrieve and reason for multi-hop QA</title>
<link>https://arxiv.org/abs/2507.07634</link>
<guid>https://arxiv.org/abs/2507.07634</guid>
<content:encoded><![CDATA[
arXiv:2507.07634v2 Announce Type: replace 
Abstract: We consider the problem of answering complex questions, given access to a large unstructured document corpus. The de facto approach to solving the problem is to leverage language models that (iteratively) retrieve and reason through the retrieved documents, until the model has sufficient information to generate an answer. Attempts at improving this approach focus on retrieval-augmented generation (RAG) metrics such as accuracy and recall and can be categorized into two types: (a) fine-tuning on large question answering (QA) datasets augmented with chain-of-thought traces, and (b) leveraging RL-based fine-tuning techniques that rely on question-document relevance signals. However, efficiency in the number of retrieval searches is an equally important metric, which has received less attention. In this work, we show that: (1) Large-scale fine-tuning is not needed to improve RAG metrics, contrary to popular claims in recent literature. Specifically, a standard ReAct pipeline with improved prompts can outperform state-of-the-art methods on benchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help RAG from the perspective of frugality, i.e., the latency due to number of searches at inference time. For example, we show that we can achieve competitive RAG metrics at nearly half the cost (in terms of number of searches) on popular RAG benchmarks, using the same base model, and at a small training cost (1000 examples).
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages</title>
<link>https://arxiv.org/abs/2507.11230</link>
<guid>https://arxiv.org/abs/2507.11230</guid>
<content:encoded><![CDATA[
arXiv:2507.11230v2 Announce Type: replace 
Abstract: Understanding the multilingual mechanisms of large language models (LLMs) provides insight into how they process different languages, yet this remains challenging. Existing studies often focus on individual neurons, but their polysemantic nature makes it difficult to isolate language-specific units from cross-lingual representations. To address this, we explore sparse autoencoders (SAEs) for their ability to learn monosemantic features that represent concrete and abstract concepts across languages in LLMs. While some of these features are language-independent, the presence of language-specific features remains underexplored. In this work, we introduce SAE-LAPE, a method based on feature activation probability, to identify language-specific features within the feed-forward network. We find that many such features predominantly appear in the middle to final layers of the model and are interpretable. These features influence the model's multilingual performance and language output and can be used for language identification with performance comparable to fastText along with more interpretability. Our code is available at https://github.com/LyzanderAndrylie/language-specific-features
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Captioning via Compact Bidirectional Architecture</title>
<link>https://arxiv.org/abs/2201.01984</link>
<guid>https://arxiv.org/abs/2201.01984</guid>
<content:encoded><![CDATA[
arXiv:2201.01984v2 Announce Type: replace-cross 
Abstract: Most current image captioning models typically generate captions from left-to-right. This unidirectional property makes them can only leverage past context but not future context. Though refinement-based models can exploit both past and future context by generating a new caption in the second stage based on pre-retrieved or pre-generated captions in the first stage, the decoder of these models generally consists of two networks~(i.e. a retriever or captioner in the first stage and a captioner in the second stage), which can only be executed sequentially. In this paper, we introduce a Compact Bidirectional Transformer model for image captioning that can leverage bidirectional context implicitly and explicitly while the decoder can be executed parallelly. Specifically, it is implemented by tightly coupling left-to-right(L2R) and right-to-left(R2L) flows into a single compact model to serve as a regularization for implicitly exploiting bidirectional context and optionally allowing explicit interaction of the bidirectional flows, while the final caption is chosen from either L2R or R2L flow in a sentence-level ensemble manner. We conduct extensive ablation studies on MSCOCO benchmark and find that the compact bidirectional architecture and the sentence-level ensemble play more important roles than the explicit interaction mechanism. By combining with word-level ensemble seamlessly, the effect of sentence-level ensemble is further enlarged. We further extend the conventional one-flow self-critical training to the two-flows version under this architecture and achieve new state-of-the-art results in comparison with non-vision-language-pretraining models. Finally, we verify the generality of this compact bidirectional architecture by extending it to LSTM backbone. Source code is available at https://github.com/YuanEZhou/cbtic.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs</title>
<link>https://arxiv.org/abs/2407.15549</link>
<guid>https://arxiv.org/abs/2407.15549</guid>
<content:encoded><![CDATA[
arXiv:2407.15549v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) can often be made to behave in undesirable ways that they are explicitly fine-tuned not to. For example, the LLM red-teaming literature has produced a wide variety of 'jailbreaking' techniques to elicit harmful text from models that were fine-tuned to be harmless. Recent work on red-teaming, model editing, and interpretability suggests that this challenge stems from how (adversarial) fine-tuning largely serves to suppress rather than remove undesirable capabilities from LLMs. Prior work has introduced latent adversarial training (LAT) as a way to improve robustness to broad classes of failures. These prior works have considered untargeted latent space attacks where the adversary perturbs latent activations to maximize loss on examples of desirable behavior. Untargeted LAT can provide a generic type of robustness but does not leverage information about specific failure modes. Here, we experiment with targeted LAT where the adversary seeks to minimize loss on a specific competing task. We find that it can augment a wide variety of state-of-the-art methods. First, we use targeted LAT to improve robustness to jailbreaks, outperforming a strong R2D2 baseline with orders of magnitude less compute. Second, we use it to more effectively remove backdoors with no knowledge of the trigger. Finally, we use it to more effectively unlearn knowledge for specific undesirable tasks in a way that is also more robust to re-learning. Overall, our results suggest that targeted LAT can be an effective tool for defending against harmful behaviors from LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search</title>
<link>https://arxiv.org/abs/2408.10635</link>
<guid>https://arxiv.org/abs/2408.10635</guid>
<content:encoded><![CDATA[
arXiv:2408.10635v3 Announce Type: replace-cross 
Abstract: Traditional reinforcement learning and planning typically requires vast amounts of data and training to develop effective policies. In contrast, large language models (LLMs) exhibit strong generalization and zero-shot capabilities, but struggle with tasks that require detailed planning and decision-making in complex action spaces. We introduce STRATEGIST, a novel approach that integrates the strengths of both methods. Our approach leverages LLMs to search and update high-level strategies (as text), which are then refined and executed by low-level Monte Carlo Tree Search (MCTS). STRATEGIST is a generalizable framework to optimize the strategy through population-based self-play simulations without the need for any training data. We demonstrate the effectiveness of STRATEGIST in learning optimal strategies for competitive, multi-turn games with partial information, including Game of Pure Strategy (GOPS) and multi-agent, hidden-identity discussion games like The Resistance: Avalon. Our results show that agents equipped with STRATEGIST outperform those trained with traditional RL methods, other LLM-based skill acquisition techniques, pre-existing LLM agents across both game environments and achieves comparable performance against human players.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signs as Tokens: A Retrieval-Enhanced Multilingual Sign Language Generator</title>
<link>https://arxiv.org/abs/2411.17799</link>
<guid>https://arxiv.org/abs/2411.17799</guid>
<content:encoded><![CDATA[
arXiv:2411.17799v3 Announce Type: replace-cross 
Abstract: Sign language is a visual language that encompasses all linguistic features of natural languages and serves as the primary communication method for the deaf and hard-of-hearing communities. Although many studies have successfully adapted pretrained language models (LMs) for sign language translation (sign-to-text), the reverse task-sign language generation (text-to-sign)-remains largely unexplored. In this work, we introduce a multilingual sign language model, Signs as Tokens (SOKE), which can generate 3D sign avatars autoregressively from text inputs using a pretrained LM. To align sign language with the LM, we leverage a decoupled tokenizer that discretizes continuous signs into token sequences representing various body parts. During decoding, unlike existing approaches that flatten all part-wise tokens into a single sequence and predict one token at a time, we propose a multi-head decoding method capable of predicting multiple tokens simultaneously. This approach improves inference efficiency while maintaining effective information fusion across different body parts. To further ease the generation process, we propose a retrieval-enhanced SLG approach, which incorporates external sign dictionaries to provide accurate word-level signs as auxiliary conditions, significantly improving the precision of generated signs. Extensive qualitative and quantitative evaluations demonstrate the effectiveness of SOKE.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning</title>
<link>https://arxiv.org/abs/2412.03248</link>
<guid>https://arxiv.org/abs/2412.03248</guid>
<content:encoded><![CDATA[
arXiv:2412.03248v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have enabled the creation of multi-modal LLMs that exhibit strong comprehension of visual data such as images and videos. However, these models usually rely on extensive visual tokens from visual encoders, leading to high computational demands, which limits their applicability in resource-constrained environments and for long-context tasks. In this work, we propose a training-free adaptive inference method for multi-modal LLMs that can accommodate a broad range of efficiency requirements with a minimum performance drop. Our method consists of a) iterative token merging based on embedding similarity before LLMs, and b) progressive token pruning within LLM layers based on multi-modal importance. With a minimalist design, our method can be applied to both video and image LLMs. Extensive experiments on diverse video and image benchmarks demonstrate that our method substantially reduces computation load (e.g., a $\textbf{7-fold}$ reduction in FLOPs) while preserving the performance of video and image LLMs. Further, at a similar computational cost, our method outperforms the state-of-the-art methods in long video understanding (e.g., $\textbf{+4.6}$ on MLVU). Additionally, our in-depth analysis provides insights into token redundancy and LLM layer behaviors, offering guidance for future research in designing efficient multi-modal LLMs. Our code is available at https://github.com/LaVi-Lab/AIM.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAKE: Steering Activations for Knowledge Editing</title>
<link>https://arxiv.org/abs/2503.01751</link>
<guid>https://arxiv.org/abs/2503.01751</guid>
<content:encoded><![CDATA[
arXiv:2503.01751v2 Announce Type: replace-cross 
Abstract: As Large Langue Models have been shown to memorize real-world facts, the need to update this knowledge in a controlled and efficient manner arises. Designed with these constraints in mind, Knowledge Editing (KE) approaches propose to alter specific facts in pretrained models. However, they have been shown to suffer from several limitations, including their lack of contextual robustness and their failure to generalize to logical implications related to the fact. To overcome these issues, we propose SAKE, a steering activation method that models a fact to be edited as a distribution rather than a single prompt. Leveraging Optimal Transport, SAKE alters the LLM behavior over a whole fact-related distribution, defined as paraphrases and logical implications. Several numerical experiments demonstrate the effectiveness of this method: SAKE is thus able to perform more robust edits than its existing counterparts.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SQuat: Subspace-orthogonal KV Cache Quantization</title>
<link>https://arxiv.org/abs/2503.24358</link>
<guid>https://arxiv.org/abs/2503.24358</guid>
<content:encoded><![CDATA[
arXiv:2503.24358v2 Announce Type: replace-cross 
Abstract: The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from previously generated tokens. It reduces redundant computation at the cost of increased memory usage. To mitigate this overhead, existing approaches compress KV tensors into lower-bit representations; however, quantization errors can accumulate as more tokens are generated, potentially resulting in undesired outputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache quantization). It first constructs a subspace spanned by query tensors to capture the most critical task-related information. During key tensor quantization, it enforces that the difference between the (de)quantized and original keys remains orthogonal to this subspace, minimizing the impact of quantization errors on the attention mechanism's outputs. SQuat requires no model fine-tuning, no additional calibration dataset for offline learning, and is grounded in a theoretical framework we develop. Through numerical experiments, we show that our method reduces peak memory by 2.17 to 2.82, improves throughput by 2.45 to 3.60, and achieves more favorable benchmark scores than existing KV cache quantization algorithms.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLAMAPIE: Proactive In-Ear Conversation Assistants</title>
<link>https://arxiv.org/abs/2505.04066</link>
<guid>https://arxiv.org/abs/2505.04066</guid>
<content:encoded><![CDATA[
arXiv:2505.04066v2 Announce Type: replace-cross 
Abstract: We introduce LlamaPIE, the first real-time proactive assistant designed to enhance human conversations through discreet, concise guidance delivered via hearable devices. Unlike traditional language models that require explicit user invocation, this assistant operates in the background, anticipating user needs without interrupting conversations. We address several challenges, including determining when to respond, crafting concise responses that enhance conversations, leveraging knowledge of the user for context-aware assistance, and real-time, on-device processing. To achieve this, we construct a semi-synthetic dialogue dataset and propose a two-model pipeline: a small model that decides when to respond and a larger model that generates the response. We evaluate our approach on real-world datasets, demonstrating its effectiveness in providing helpful, unobtrusive assistance. User studies with our assistant, implemented on Apple Silicon M2 hardware, show a strong preference for the proactive assistant over both a baseline with no assistance and a reactive model, highlighting the potential of LlamaPie to enhance live conversations.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining Intrinsic Rewards from LLM Hidden States for Efficient Best-of-N Sampling</title>
<link>https://arxiv.org/abs/2505.12225</link>
<guid>https://arxiv.org/abs/2505.12225</guid>
<content:encoded><![CDATA[
arXiv:2505.12225v2 Announce Type: replace-cross 
Abstract: Enhancing Large Language Model (LLM)'s performance with best-of-N sampling is effective and has attracted significant attention. However, it is computationally prohibitive due to massive, data-hungry text-based reward models. By changing the data source from text to hidden states, we introduce SWIFT (Simple Weighted Intrinsic Feedback Technique), a novel, lightweight technique that leverages the rich information embedded in LLM hidden states to address these issues, which operates on token-level and consists of only linear layers. Extensive experiments show that SWIFT outperforms baselines with less than 0.005% of the parameters of baselines, requiring only a few samples for training, demonstrating significant efficiency improvement. SWIFT's robust scalability, applicability to some closed-source models via logits, and ability to be combined with traditional reward models to yield further performance gains underscore its practical value.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach</title>
<link>https://arxiv.org/abs/2505.14479</link>
<guid>https://arxiv.org/abs/2505.14479</guid>
<content:encoded><![CDATA[
arXiv:2505.14479v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) struggle with formal domains that require rigorous logical deduction and symbolic reasoning, such as mathematical proof generation. We propose a neuro-symbolic approach that combines LLMs' generative strengths with structured components to overcome this challenge. As a proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1) we retrieve analogous problems and use their proofs to guide the LLM, and (2) a formal verifier evaluates the generated proofs and provides feedback, helping the model fix incorrect proofs. We demonstrate that our method significantly improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both analogous problems and the verifier's feedback contribute to these gains. More broadly, shifting to LLMs that generate provably correct conclusions could dramatically improve their reliability, accuracy and consistency, unlocking complex tasks and critical real-world applications that require trustworthiness.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models</title>
<link>https://arxiv.org/abs/2506.01413</link>
<guid>https://arxiv.org/abs/2506.01413</guid>
<content:encoded><![CDATA[
arXiv:2506.01413v5 Announce Type: replace-cross 
Abstract: Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose RAIF, a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on OOD constraints also confirms the generalizability of our RAIF. Codes and data are available at https://github.com/yuleiqin/RAIF.
  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction following, complex instructions
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation</title>
<link>https://arxiv.org/abs/2506.09081</link>
<guid>https://arxiv.org/abs/2506.09081</guid>
<content:encoded><![CDATA[
arXiv:2506.09081v3 Announce Type: replace-cross 
Abstract: We present FlagEvalMM, an open-source evaluation framework designed to comprehensively assess multimodal models across a diverse range of vision-language understanding and generation tasks, such as visual question answering, text-to-image/video generation, and image-text retrieval. We decouple model inference from evaluation through an independent evaluation service, thus enabling flexible resource allocation and seamless integration of new tasks and models. Moreover, FlagEvalMM utilizes advanced inference acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to significantly enhance evaluation efficiency. Extensive experiments show that FlagEvalMM offers accurate and efficient insights into model strengths and limitations, making it a valuable tool for advancing multimodal research. The framework is publicly accessible at https://github.com/flageval-baai/FlagEvalMM.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLR: Automated Synthesis for Scalable Logical Reasoning</title>
<link>https://arxiv.org/abs/2506.15787</link>
<guid>https://arxiv.org/abs/2506.15787</guid>
<content:encoded><![CDATA[
arXiv:2506.15787v3 Announce Type: replace-cross 
Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given a user's task specification, SLR automatically synthesizes (i) an instruction prompt for an inductive reasoning task, (ii) a validation program, executable on model outputs to provide verifiable rewards, and (iii) the latent ground-truth rule. This process is fully automated, scalable, requires no human annotations, and offers precise control over task difficulty. Using SLR, we create SLR-Bench, a benchmark comprising 19k prompts organized into 20 curriculum levels that progressively increase in relational, arithmetic, and recursive complexity. Large-scale evaluation reveals that contemporary LLMs readily produce syntactically valid rules, yet often fail at correct logical inference. Recent reasoning LLMs demonstrate improved performance but incur very high test-time computation, with costs exceeding $300 for just 1,000 prompts. Finally, curriculum learning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of computational cost. Moreover, these reasoning capabilities generalize to a wide range of established benchmarks, underscoring the effectiveness of SLR for downstream reasoning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models</title>
<link>https://arxiv.org/abs/2506.22493</link>
<guid>https://arxiv.org/abs/2506.22493</guid>
<content:encoded><![CDATA[
arXiv:2506.22493v2 Announce Type: replace-cross 
Abstract: Political Compass Test (PCT) or similar questionnaires have been used to quantify LLM's political leanings. Building on a recent line of work that examines the validity of PCT tests, we demonstrate that variation in standard generation parameters does not significantly impact the models' PCT scores. However, external factors such as prompt variations and fine-tuning individually and in combination affect the same. Finally, we demonstrate that when models are fine-tuned on text datasets with higher political content than others, the PCT scores are not differentially affected. This calls for a thorough investigation into the validity of PCT and similar tests, as well as the mechanism by which political leanings are encoded in LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models</title>
<link>https://arxiv.org/abs/2507.08128</link>
<guid>https://arxiv.org/abs/2507.08128</guid>
<content:encoded><![CDATA[
arXiv:2507.08128v2 Announce Type: replace-cross 
Abstract: We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large audio-language model that advances reasoning and understanding across speech, sound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder trained using a novel strategy for joint representation learning across all 3 modalities of speech, sound, and music; (ii) flexible, on-demand thinking, allowing the model to do chain-of-thought-type reasoning before answering; (iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning (including speech) up to 10 minutes; and (v) voice-to-voice interaction. To enable these capabilities, we propose several large-scale training datasets curated using novel strategies, including AudioSkills-XL, LongAudio-XL, AF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based training strategy. Trained on only open-source audio data, AF3 achieves new SOTA results on over 20+ (long) audio understanding and reasoning benchmarks, surpassing both open-weight and closed-source models trained on much larger datasets.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training</title>
<link>https://arxiv.org/abs/2507.09205</link>
<guid>https://arxiv.org/abs/2507.09205</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Tibetan, pre-training corpus, data cleaning, multilingual base model

Summary: 
Large language models have made significant advancements in various languages, but Tibetan, a low-resource language, lacks representation due to limited training data. To address this issue, a comprehensive Tibetan pre-training corpus has been curated from multiple sources and processed using a specialized pipeline. By leveraging this data, a multilingual model has been further trained to enhance its capabilities in generating Tibetan text. New high-quality Tibetan benchmarks have been created to evaluate the model's performance, in addition to existing benchmarks. Experimental results show that the model consistently outperforms open-source and Tibetan-specific models in various tasks. The curated corpus, dedicated data cleaning, and tailored pre/post-training of the model have collectively contributed to significant improvements in the model's performance on Tibetan language tasks.<br /><br />Summary: <div>
arXiv:2507.09205v4 Announce Type: replace 
Abstract: Large language models have achieved remarkable progress across many languages. However, Tibetan, as a representative low-resource language, is particularly underrepresented in existing models due to the scarcity of high-quality training corpora. To address this gap, we curate the largest Tibetan pre-training corpus to date, aggregating data from diverse sources and applying a dedicated data cleaning and processing pipeline tailored for Tibetan. With the curated data, we continue pre/post-training a multilingual base model to enhance its generative capabilities in Tibetan. To evaluate the Tibetan capabilities of the model, we create new high-quality Tibetan benchmarks, and complement them with existing public benchmarks. Experimental results demonstrate that our model consistently and significantly outperforms both open-source models of similar scale and Tibetan-tailored models across a wide range of tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media</title>
<link>https://arxiv.org/abs/2507.19511</link>
<guid>https://arxiv.org/abs/2507.19511</guid>
<content:encoded><![CDATA[
<div> transformer models, mental health disorders, Natural Language Processing, LSTM, BERT embeddings <br />
<br />
Summary: 
The study evaluates transformer models against LSTM for mental health disorder classification on Reddit. Transformer models like RoBERTa show superior performance, achieving high F1 scores on test sets. LSTM models with BERT embeddings also perform well with lower computational resources. The research emphasizes the potential of transformer-based models for real-time mental health monitoring and highlights their effectiveness in text analysis. The study validates the annotated dataset through statistical analysis and topic modeling, providing insights into the capabilities and limitations of NLP methodologies in mental disorder detection. Overall, the findings suggest the significance of automated tools in early detection and monitoring of mental health disorders and offer valuable implications for clinical applications and digital mental health interventions. <br /><br /> <div>
arXiv:2507.19511v1 Announce Type: new 
Abstract: The rising prevalence of mental health disorders necessitates the development of robust, automated tools for early detection and monitoring. Recent advances in Natural Language Processing (NLP), particularly transformer-based architectures, have demonstrated significant potential in text analysis. This study provides a comprehensive evaluation of state-of-the-art transformer models (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term Memory (LSTM) based approaches using different text embedding techniques for mental health disorder classification on Reddit. We construct a large annotated dataset, validating its reliability through statistical judgmental analysis and topic modeling. Experimental results demonstrate the superior performance of transformer models over traditional deep-learning approaches. RoBERTa achieved the highest classification performance, with a 99.54% F1 score on the hold-out test set and a 96.05% F1 score on the external test set. Notably, LSTM models augmented with BERT embeddings proved highly competitive, achieving F1 scores exceeding 94% on the external dataset while requiring significantly fewer computational resources. These findings highlight the effectiveness of transformer-based models for real-time, scalable mental health monitoring. We discuss the implications for clinical applications and digital mental health interventions, offering insights into the capabilities and limitations of state-of-the-art NLP methodologies in mental disorder detection.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables</title>
<link>https://arxiv.org/abs/2507.19521</link>
<guid>https://arxiv.org/abs/2507.19521</guid>
<content:encoded><![CDATA[
<div> Keywords: academic literature, large language models, schema generation, table intents, LLM-based schema editing<br />
Summary:<br />
- The study addresses the challenges in organizing and comparing academic literature using Large Language Models (LLMs).<br />
- Ambiguity in reference-based evaluations and lack of editing/refinement methods have hindered progress in schema generation.<br />
- The approach involves augmenting unannotated table corpora with synthesized intents to reduce ambiguity and improve baseline performance in reconstructing reference schemas.<br />
- Benchmarking several single-shot schema generation methods, including prompted LLM workflows and fine-tuned models, shows that smaller, open-weight models can be competitive with state-of-the-art prompted LLMs.<br />
- LLM-based schema editing techniques are proposed to further enhance the schemas generated by these methods.<br /> 

Summary: <br />
The study focuses on enhancing the organization and comparison of academic literature through the use of Large Language Models (LLMs). By addressing issues such as ambiguity in evaluations and lack of editing methods, the research presents an approach that incorporates table intents to improve schema generation performance. Benchmarking various schema generation methods reveals the competitiveness of smaller, open-weight models with state-of-the-art LLMs, while LLM-based schema editing techniques are introduced to refine the generated schemas further. <div>
arXiv:2507.19521v1 Announce Type: new 
Abstract: The increasing volume of academic literature makes it essential for researchers to organize, compare, and contrast collections of documents. Large language models (LLMs) can support this process by generating schemas defining shared aspects along which to compare papers. However, progress on schema generation has been slow due to: (i) ambiguity in reference-based evaluations, and (ii) lack of editing/refinement methods. Our work is the first to address both issues. First, we present an approach for augmenting unannotated table corpora with synthesized intents and apply it to create a dataset for studying schema generation conditioned on a given information need, thus reducing ambiguity. With this dataset, we show how incorporating table intents significantly improves baseline performance in reconstructing reference schemas. Next, we propose several LLM-based schema editing techniques. We start by comprehensively benchmarking several single-shot schema generation methods, including prompted LLM workflows and fine-tuned models, showing that smaller, open-weight models can be fine-tuned to be competitive with state-of-the-art prompted LLMs. Then we demonstrate that our editing techniques can further improve schemas generated by these methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri</title>
<link>https://arxiv.org/abs/2507.19537</link>
<guid>https://arxiv.org/abs/2507.19537</guid>
<content:encoded><![CDATA[
<div> Keywords: WOKIE, SKOS thesauri, Digital Humanities, machine translation, ontology matching

Summary: 
WOKIE is a new open-source pipeline designed to automate the translation of SKOS thesauri in the field of Digital Humanities. This tool addresses the issue of language diversity limiting access and semantic interoperability of knowledge resources. By utilizing external translation services and Large Language Models (LLMs), WOKIE balances translation quality, scalability, and cost. It is user-friendly, requires no prior expertise, and can be easily extended. The evaluation of WOKIE on various thesauri in 15 languages showed promising results in translation quality, performance, and ontology matching improvements. This tool facilitates the enhancement of accessibility, reuse, and cross-lingual interoperability of thesauri, supporting inclusive and multilingual research infrastructures. 

<br /><br />Summary: <div>
arXiv:2507.19537v1 Announce Type: new 
Abstract: We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for the automated translation of SKOS thesauri. This work addresses a critical need in the Digital Humanities (DH), where language diversity can limit access, reuse, and semantic interoperability of knowledge resources. WOKIE combines external translation services with targeted refinement using Large Language Models (LLMs), balancing translation quality, scalability, and cost. Designed to run on everyday hardware and be easily extended, the application requires no prior expertise in machine translation or LLMs. We evaluate WOKIE across several DH thesauri in 15 languages with different parameters, translation services and LLMs, systematically analysing translation quality, performance, and ontology matching improvements. Our results show that WOKIE is suitable to enhance the accessibility, reuse, and cross-lingual interoperability of thesauri by hurdle-free automated translation and improved ontology matching performance, supporting more inclusive and multilingual research infrastructures.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning</title>
<link>https://arxiv.org/abs/2507.19586</link>
<guid>https://arxiv.org/abs/2507.19586</guid>
<content:encoded><![CDATA[
<div> evaluation framework, geospatial knowledge, geospatial hallucinations, factuality aligning method, LLMs<br />
Summary:<br />
The article introduces a new evaluation framework for geospatial hallucinations in Large Language Models (LLMs). It leverages structured geospatial knowledge graphs to assess inaccuracies in geospatial information generated by LLMs. The study evaluates 20 advanced LLMs and identifies geospatial hallucinations in their knowledge. A dynamic factuality aligning method based on Kahneman-Tversky Optimization (KTO) is proposed to mitigate these hallucinations, resulting in a significant performance improvement of over 29.6% on the benchmark. Experimental results validate the efficacy of the framework and learning algorithm in enhancing the trustworthiness of LLMs in geospatial knowledge and reasoning tasks. This comprehensive approach addresses the issue of geospatial hallucinations and improves the reliability of LLMs in handling geospatial tasks. <br />Summary: <div>
arXiv:2507.19586v1 Announce Type: new 
Abstract: Large language models (LLMs) possess extensive world knowledge, including geospatial knowledge, which has been successfully applied to various geospatial tasks such as mobility prediction and social indicator prediction. However, LLMs often generate inaccurate geospatial knowledge, leading to geospatial hallucinations (incorrect or inconsistent representations of geospatial information) that compromise their reliability. While the phenomenon of general knowledge hallucination in LLMs has been widely studied, the systematic evaluation and mitigation of geospatial hallucinations remain largely unexplored. To address this gap, we propose a comprehensive evaluation framework for geospatial hallucinations, leveraging structured geospatial knowledge graphs for controlled assessment. Through extensive evaluation across 20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge. Building on these insights, we introduce a dynamic factuality aligning method based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial hallucinations in LLMs, leading to a performance improvement of over 29.6% on the proposed benchmark. Extensive experimental results demonstrate the effectiveness of our benchmark and learning algorithm in enhancing the trustworthiness of LLMs in geospatial knowledge and reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Attention Mechanisms for Large Language Models: A Survey</title>
<link>https://arxiv.org/abs/2507.19595</link>
<guid>https://arxiv.org/abs/2507.19595</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer-based architectures, efficient attention mechanisms, linear attention, sparse attention, language models

Summary:
This article discusses the challenges posed by the quadratic time and memory complexity of self-attention in Transformer-based architectures and explores two categories of efficient attention mechanisms: linear attention and sparse attention. Linear attention methods achieve linear complexity through various techniques, such as kernel approximations and fast weight dynamics, while sparse attention techniques limit attention computation to selected subsets of tokens. The survey provides a comprehensive overview of these developments, considering both algorithmic innovations and hardware-level considerations. It also examines the integration of efficient attention into large-scale pre-trained language models, including architectures built entirely on efficient attention and hybrid designs combining local and global components. By bridging theoretical foundations with practical deployment strategies, the article serves as a foundational reference for improving the design of scalable and efficient language models.<br /><br />Summary: <div>
arXiv:2507.19595v1 Announce Type: new 
Abstract: Transformer-based architectures have become the prevailing backbone of large language models. However, the quadratic time and memory complexity of self-attention remains a fundamental obstacle to efficient long-context modeling. To address this limitation, recent research has introduced two principal categories of efficient attention mechanisms. Linear attention methods achieve linear complexity through kernel approximations, recurrent formulations, or fastweight dynamics, thereby enabling scalable inference with reduced computational overhead. Sparse attention techniques, in contrast, limit attention computation to selected subsets of tokens based on fixed patterns, block-wise routing, or clustering strategies, enhancing efficiency while preserving contextual coverage. This survey provides a systematic and comprehensive overview of these developments, integrating both algorithmic innovations and hardware-level considerations. In addition, we analyze the incorporation of efficient attention into largescale pre-trained language models, including both architectures built entirely on efficient attention and hybrid designs that combine local and global components. By aligning theoretical foundations with practical deployment strategies, this work aims to serve as a foundational reference for advancing the design of scalable and efficient language models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?</title>
<link>https://arxiv.org/abs/2507.19598</link>
<guid>https://arxiv.org/abs/2507.19598</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, code generation, adversarial misuse, code decomposition attacks, benchmark

Summary:<br />
Recent advancements in Large Language Models have improved their ability to generate code, but their vulnerability to adversarial attacks, particularly multi-turn malicious prompts, has not been extensively studied. This work introduces code decomposition attacks, where a malicious coding task is split into smaller subtasks across multiple conversational turns to evade detection. A benchmark called MOCHA is introduced to evaluate the robustness of code LLMs against both single-turn and multi-turn malicious prompts. Results show persistent vulnerabilities, especially in multi-turn scenarios. Fine-tuning on MOCHA helps improve rejection rates while maintaining coding ability. This fine-tuning also enhances robustness on external adversarial datasets, with rejection rates increasing by up to 32.4% without additional supervision.<br /><br />Summary: <div>
arXiv:2507.19598v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have significantly enhanced their code generation capabilities. However, their robustness against adversarial misuse, particularly through multi-turn malicious coding prompts, remains underexplored. In this work, we introduce code decomposition attacks, where a malicious coding task is broken down into a series of seemingly benign subtasks across multiple conversational turns to evade safety filters. To facilitate systematic evaluation, we introduce \benchmarkname{}, a large-scale benchmark designed to evaluate the robustness of code LLMs against both single-turn and multi-turn malicious prompts. Empirical results across open- and closed-source models reveal persistent vulnerabilities, especially under multi-turn scenarios. Fine-tuning on MOCHA improves rejection rates while preserving coding ability, and importantly, enhances robustness on external adversarial datasets with up to 32.4% increase in rejection rates without any additional supervision.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track</title>
<link>https://arxiv.org/abs/2507.19616</link>
<guid>https://arxiv.org/abs/2507.19616</guid>
<content:encoded><![CDATA[
<div> Keywords: IWSLT 2025, speech-to-text translation, low-resource scenario, Whisper ASR model, Krutrim language model

Summary:
- HITSZ participated in the IWSLT 2025 Indic track focusing on speech-to-text translation for English-to-Indic and Indic-to-English language pairs.
- They proposed an end-to-end system combining the Whisper automated speech recognition model with the Indic-specialized large language model Krutrim to improve translation quality in the low-resource setting.
- Experimental results showed average BLEU scores of $28.88$ for English-to-Indic and $27.86$ for Indic-to-English translations.
- The Chain-of-Thought (CoT) method was explored for enhancing translation quality further, with a notable $13.84$ BLEU increase observed for Tamil-to-English translations.
- Challenges were identified in maintaining the required CoT output format consistently across translations. 

<br /><br />Summary: <div>
arXiv:2507.19616v1 Announce Type: new 
Abstract: This paper presents HITSZ's submission for the IWSLT 2025 Indic track, focusing on speech-to-text translation (ST) for English-to-Indic and Indic-to-English language pairs. To enhance translation quality in this low-resource scenario, we propose an end-to-end system integrating the pre-trained Whisper automated speech recognition (ASR) model with Krutrim, an Indic-specialized large language model (LLM). Experimental results demonstrate that our end-to-end system achieved average BLEU scores of $28.88$ for English-to-Indic directions and $27.86$ for Indic-to-English directions. Furthermore, we investigated the Chain-of-Thought (CoT) method. While this method showed potential for significant translation quality improvements on successfully parsed outputs (e.g. a $13.84$ BLEU increase for Tamil-to-English), we observed challenges in ensuring the model consistently adheres to the required CoT output format.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks</title>
<link>https://arxiv.org/abs/2507.19634</link>
<guid>https://arxiv.org/abs/2507.19634</guid>
<content:encoded><![CDATA[
<div> multimodal LLMs, evaluation, multilingual, instruction-following, benchmark
Summary: 
- Recent advances in large language models have led to the development of multimodal LLMs integrating text, speech, and vision.
- Challenges remain in evaluating the multilingual and multimodal capabilities of these models over long and short contexts.
- Existing benchmarks are limited in scope, often focusing on one modality or lacking human annotations.
- To address these limitations, the MCIF benchmark is introduced, designed for evaluating instruction-following in crosslingual, multimodal settings across languages and modalities.
- MCIF spans speech, vision, and text modalities in English, German, Italian, and Chinese, providing a comprehensive assessment of MLLMs' abilities. 

<br /><br />Summary: <div>
arXiv:2507.19634v1 Announce Type: new 
Abstract: Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to general-purpose instruction-following models, a key frontier lies in evaluating their multilingual and multimodal capabilities over both long and short contexts. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on one single modality at a time, rely on short-form contexts, or lack human annotations--hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated benchmark based on scientific talks that is designed to evaluate instruction-following in crosslingual, multimodal settings over both short- and long-form inputs. MCIF spans three core modalities--speech, vision, and text--and four diverse languages (English, German, Italian, and Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret instructions across languages and combine them with multimodal contextual information. MCIF is released under a CC-BY 4.0 license to encourage open research and progress in MLLMs development.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams</title>
<link>https://arxiv.org/abs/2507.19666</link>
<guid>https://arxiv.org/abs/2507.19666</guid>
<content:encoded><![CDATA[
<div> Multimodal dataset, Romanian, driving law, Large Language Models, Vision-Language Models <br />
Summary: <br />
The study evaluates the use of Large Language Models and Vision-Language Models in understanding Romanian driving law through text and visual question-answering tasks. The researchers introduce a new dataset, RoD-TAL, consisting of Romanian driving test questions, text-based and image-based, with legal references and explanations. They test retrieval-augmented generation pipelines, dense retrievers, and reasoning-optimized models across various tasks such as Information Retrieval, Question Answering, Visual Information Retrieval, and Visual Question Answering. Fine-tuning in the domain improves retrieval performance, while chain-of-thought prompting and specialized reasoning models enhance question-answering accuracy. The results show that specialized models can surpass the minimum grades required to pass driving exams. However, visual reasoning remains a challenge, highlighting the potential and limitations of using LLMs and VLMs in legal education. <br /> <div>
arXiv:2507.19666v1 Announce Type: new 
Abstract: The intersection of AI and legal systems presents a growing need for tools that support legal education, particularly in under-resourced languages such as Romanian. In this work, we aim to evaluate the capabilities of Large Language Models (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning about Romanian driving law through textual and visual question-answering tasks. To facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising Romanian driving test questions, text-based and image-based, alongside annotated legal references and human explanations. We implement and assess retrieval-augmented generation (RAG) pipelines, dense retrievers, and reasoning-optimized models across tasks including Information Retrieval (IR), Question Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate that domain-specific fine-tuning significantly enhances retrieval performance. At the same time, chain-of-thought prompting and specialized reasoning models improve QA accuracy, surpassing the minimum grades required to pass driving exams. However, visual reasoning remains challenging, highlighting the potential and the limitations of applying LLMs and VLMs to legal education.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks</title>
<link>https://arxiv.org/abs/2507.19699</link>
<guid>https://arxiv.org/abs/2507.19699</guid>
<content:encoded><![CDATA[
<div> benchmarking, multilingual, monolingual, large language models, performance

Summary:
Multilingual and monolingual Large Language Models (LLMs) were benchmarked across Arabic, English, and Indic languages, with a focus on model compression techniques such as pruning and quantization. Various state-of-the-art LLMS like BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2 were evaluated, revealing performance variations influenced by linguistic diversity and resource availability. Multilingual models exhibited superior performance compared to language-specific models, showcasing significant cross-lingual transfer benefits. Quantization proved effective in maintaining model accuracy and efficiency, particularly with 4-bit and 8-bit precision, while aggressive pruning negatively impacted model performance, especially in larger models. The study identified crucial strategies for developing scalable and equitable multilingual NLP solutions and emphasized the importance of addressing hallucination and generalization errors in low-resource environments. 

<br /><br />Summary: <div>
arXiv:2507.19699v1 Announce Type: new 
Abstract: Although LLMs have attained significant success in high-resource languages, their capacity in low-resource linguistic environments like Kannada and Arabic is not yet fully understood. This work benchmarking the performance of multilingual and monolingual Large Language Models (LLMs) across Arabic, English, and Indic languages, with particular emphasis on the effects of model compression strategies such as pruning and quantization. Findings shows significant performance differences driven by linguistic diversity and resource availability on SOTA LLMS as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2. We find that multilingual versions of the model outperform their language-specific counterparts across the board, indicating substantial cross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in maintaining model accuracy while promoting efficiency, but aggressive pruning significantly compromises performance, especially in bigger models. Our findings pinpoint key strategies to construct scalable and fair multilingual NLP solutions and underscore the need for interventions to address hallucination and generalization errors in the low-resource setting.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs</title>
<link>https://arxiv.org/abs/2507.19710</link>
<guid>https://arxiv.org/abs/2507.19710</guid>
<content:encoded><![CDATA[
<div> Keywords: Table-to-Text generation, Subjectivity, RDF triples, T5 models, Factual accuracy<br />
Summary: 
- Existing approaches in Table-to-Text (T2T) generation focus on objective descriptions of tabular data.
- This work introduces a novel pipeline that generates both objective and subjective text from tables.
- The pipeline comprises three stages: RDF triple extraction, narrative aggregation, and infusion of subjectivity.
- By incorporating RDFs, the approach enhances factual accuracy while maintaining interpretability.
- Smaller, fine-tuned T5 models are used in the pipeline, achieving comparable performance to large language models like GPT-3.5.
- The approach outperforms Mistral-7B and Llama-2 in several metrics, emphasizing a balance between factual accuracy and subjective interpretation.
- The structured pipeline proposed in this work is the first to integrate intermediate representations to enhance both factual correctness and subjectivity.<br /><br />Summary: <div>
arXiv:2507.19710v1 Announce Type: new 
Abstract: In Table-to-Text (T2T) generation, existing approaches predominantly focus on providing objective descriptions of tabular data. However, generating text that incorporates subjectivity, where subjectivity refers to interpretations beyond raw numerical data, remains underexplored. To address this, we introduce a novel pipeline that leverages intermediate representations to generate both objective and subjective text from tables. Our three-stage pipeline consists of: 1) extraction of Resource Description Framework (RDF) triples, 2) aggregation of text into coherent narratives, and 3) infusion of subjectivity to enrich the generated text. By incorporating RDFs, our approach enhances factual accuracy while maintaining interpretability. Unlike large language models (LLMs) such as GPT-3.5, Mistral-7B, and Llama-2, our pipeline employs smaller, fine-tuned T5 models while achieving comparable performance to GPT-3.5 and outperforming Mistral-7B and Llama-2 in several metrics. We evaluate our approach through quantitative and qualitative analyses, demonstrating its effectiveness in balancing factual accuracy with subjective interpretation. To the best of our knowledge, this is the first work to propose a structured pipeline for T2T generation that integrates intermediate representations to enhance both factual correctness and subjectivity.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Basic Reading Distillation</title>
<link>https://arxiv.org/abs/2507.19741</link>
<guid>https://arxiv.org/abs/2507.19741</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Distillation, Basic Reading Education, Named Entity Recognition, Language Inference<br />
Summary: <br />
This paper introduces the concept of basic reading distillation (BRD) as a new technique to enhance the capabilities of small models to imitate large language models' (LLMs) basic reading behaviors. BRD focuses on educating small models on generic texts, unrelated to specific downstream tasks, to improve their performance. By training small models to imitate LLMs' abilities such as named entity recognition, question answering, and more, the study demonstrates that these small models can surpass or match the performance of significantly larger LLMs. The analysis shows that BRD impacts the probability distribution of the small model effectively and is distinct from traditional methods like knowledge distillation or task distillation. This research highlights the potential of BRD in improving the efficiency and effectiveness of language models in various natural language processing tasks. <br /> <div>
arXiv:2507.19741v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable abilities in various natural language processing areas, but they demand high computation resources which limits their deployment in real-world. Distillation is one technique to solve this problem through either knowledge distillation or task distillation. Both distillation approaches train small models to imitate specific features of LLMs, but they all neglect basic reading education for small models on generic texts that are \emph{unrelated} to downstream tasks. In this paper, we propose basic reading distillation (BRD) which educates a small model to imitate LLMs basic reading behaviors, such as named entity recognition, question raising and answering, on each sentence. After such basic education, we apply the small model on various tasks including language inference benchmarks and BIG-bench tasks. It shows that the small model can outperform or perform comparable to over 20x bigger LLMs. Analysis reveals that BRD effectively influences the probability distribution of the small model, and has orthogonality to either knowledge distillation or task distillation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2507.19748</link>
<guid>https://arxiv.org/abs/2507.19748</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial general intelligence, Large Language Models, mathematical reasoning, JT-Math-8B, supervised fine-tuning<br />
<br />
Summary: 
The article introduces JT-Math-8B, a series of open-source models designed to improve mathematical reasoning abilities of Large Language Models (LLMs). These models address the shortcomings of existing models by incorporating a systematic optimization framework and a high-quality pre-training corpus. The Instruct Model focuses on providing direct answers through Supervised Fine-Tuning (SFT) and reinforcement learning (RL). On the other hand, the Thinking Model is trained for complex problem-solving using a Long Chain-of-Thought (Long CoT) approach, with a multi-stage RL curriculum. JT-Math-8B outperforms models like OpenAI's O1-mini and GPT-4o, showcasing superior performance in competition-level mathematics. By surpassing existing models and demonstrating proficiency in deep conceptual understanding and multi-step deliberation, JT-Math-8B contributes significantly to the advancement of artificial general intelligence. <br /><br />Summary: <div>
arXiv:2507.19748v1 Announce Type: new 
Abstract: Mathematical reasoning is a cornerstone of artificial general intelligence and a primary benchmark for evaluating the capabilities of Large Language Models (LLMs). While state-of-the-art models show promise, they often falter when faced with complex problems that demand deep conceptual understanding and intricate, multi-step deliberation. To address this challenge, we introduce JT-Math-8B, a series of open-source models comprising base, instruct, and thinking versions, built upon a systematic, multi-stage optimization framework. Our pre-training corpus is a high-quality, 210B-token dataset curated through a dedicated data pipeline that uses model-based validation to ensure quality and diversity. The Instruct Model is optimized for direct, concise answers through Supervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL) method. The Thinking Model is trained for complex problem-solving using a Long Chain-of-Thought (Long CoT) approach, combining SFT with a novel, multi-stage RL curriculum that progressively increases task difficulty and context length up to 32K tokens. JT-Math-8B achieves state-of-the-art results among open-source models of similar size, surpassing prominent models like OpenAI's O1-mini and GPT-4o , and demonstrating superior performance on competition-level mathematics.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs</title>
<link>https://arxiv.org/abs/2507.19756</link>
<guid>https://arxiv.org/abs/2507.19756</guid>
<content:encoded><![CDATA[
<div> Evangelical movement, Christian Fiction, computational tools, divine acts, Left Behind series<br />
<br />
Summary:<br />
The article explores the cultural and literary aspects of the American Evangelical movement, focusing on Christian Fiction as a genre. Using computational tools and human annotators, the study delves into how authors depict divine acts in Christian Fiction. By developing definitions and a codebook for "acts of God" and utilizing a lightweight language model, the research reveals significant differences between the popular Left Behind series and Christian Fiction as a whole. Additionally, the study uncovers disparities between male and female authors in their depiction of divine acts. This analysis sheds light on the nuanced and varied portrayals of spiritual themes in Christian Fiction, highlighting the diversity within the genre and providing valuable insights for future research. <br /> <div>
arXiv:2507.19756v1 Announce Type: new 
Abstract: In addition to its more widely studied political activities, the American Evangelical movement has a well-developed but less externally visible cultural and literary side. Christian Fiction, however, has been little studied, and what scholarly attention there is has focused on the explosively popular Left Behind series. In this work, we use computational tools to provide both a broad topical overview of Christian Fiction as a genre and a more directed exploration of how its authors depict divine acts. Working with human annotators we first developed definitions and a codebook for "acts of God." We then adapted those instructions designed for human annotators for use by a recent, lightweight LM with the assistance of a much larger model. The laptop-scale LM is capable of matching human annotations, even when the task is subtle and challenging. Using these annotations, we show that significant and meaningful differences exist between the Left Behind books and Christian Fiction more broadly and between books by male and female authors.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities</title>
<link>https://arxiv.org/abs/2507.19766</link>
<guid>https://arxiv.org/abs/2507.19766</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reinforcement learning, ultra-long output, reasoning abilities, dynamic masking

Summary: 
Ultra-Long Output Reinforcement Learning (UloRL) proposes a novel approach to enhancing the reasoning capabilities of large language models (LLMs) by addressing challenges in handling ultra-long outputs. By dividing long output decoding into shorter segments, UloRL significantly improves training efficiency by mitigating delays associated with long-tail sequence distributions. Dynamic masking of well-Mastered Positive Tokens (MPTs) is introduced to prevent entropy collapse during training. Experimental results show that the method achieves a 2.06x increase in training speed and boosts model performance on various tasks, surpassing even larger models. The findings indicate the potential of UloRL to advance LLM reasoning capabilities with ultra-long sequence generation.

<br /><br />Summary: <div>
arXiv:2507.19766v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have highlighted the potential of reinforcement learning with verifiable rewards (RLVR) to enhance reasoning capabilities through extended output sequences. However, traditional RL frameworks face inefficiencies when handling ultra-long outputs due to long-tail sequence distributions and entropy collapse during training. To address these challenges, we propose an Ultra-Long Output Reinforcement Learning (UloRL) approach for advancing large language models' reasoning abilities. Specifically, we divide ultra long output decoding into short segments, enabling efficient training by mitigating delays caused by long-tail samples. Additionally, we introduce dynamic masking of well-Mastered Positive Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment rollout achieved 2.06x increase in training speed, while RL training with 128k-token outputs improves the model's performance on AIME2025 from 70.9\% to 85.1\% and on BeyondAIME from 50.7\% to 61.9\%, even surpassing Qwen3-235B-A22B with remarkable gains. These findings underscore the potential of our methods to advance the reasoning capabilities of LLMs with ultra-long sequence generation. We will release our code and model for further use by the community.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flora: Effortless Context Construction to Arbitrary Length and Scale</title>
<link>https://arxiv.org/abs/2507.19786</link>
<guid>https://arxiv.org/abs/2507.19786</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-context construction, Large Language Models, Flora, Instruction tuning, Context diversity

Summary:
Flora is a new approach that aims to improve the performance of Large Language Models (LLMs) in handling long contexts without the need for human intervention or specialized LLM modifications. It achieves this by assembling short instructions based on categories and instructing LLMs with long-context meta-instructions to generate responses. Flora can generate contexts of varying lengths with rich diversity while maintaining strong performance in short-context tasks. Experimental results on Llama3-8B-Instruct and QwQ-32B benchmarks demonstrate the effectiveness of Flora in enhancing the long-context performance of LLMs. This approach addresses the challenges associated with long-context handling in LLMs, such as computational demands and forgetting of short-context abilities, providing a more efficient and scalable solution. The code for data construction is also available for reference on GitHub. 

<br /><br />Summary: <div>
arXiv:2507.19786v1 Announce Type: new 
Abstract: Effectively handling long contexts is challenging for Large Language Models (LLMs) due to the rarity of long texts, high computational demands, and substantial forgetting of short-context abilities. Recent approaches have attempted to construct long contexts for instruction tuning, but these methods often require LLMs or human interventions, which are both costly and limited in length and diversity. Also, the drop in short-context performances of present long-context LLMs remains significant. In this paper, we introduce Flora, an effortless (human/LLM-free) long-context construction strategy. Flora can markedly enhance the long-context performance of LLMs by arbitrarily assembling short instructions based on categories and instructing LLMs to generate responses based on long-context meta-instructions. This enables Flora to produce contexts of arbitrary length and scale with rich diversity, while only slightly compromising short-context performance. Experiments on Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three long-context benchmarks while maintaining strong performances in short-context tasks. Our data-construction code is available at \href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs</title>
<link>https://arxiv.org/abs/2507.19823</link>
<guid>https://arxiv.org/abs/2507.19823</guid>
<content:encoded><![CDATA[
<div> key quantization, value offloading, dynamic KV eviction, heterogeneous attention computation, memory constraints <br />
Summary:
HCAttention is a new framework proposed for processing long-context inputs with large language models. It addresses the challenge of high memory requirements for the Key-Value cache during inference by integrating key quantization, value offloading, and dynamic KV eviction. This allows for efficient inference under extreme memory constraints without the need for model fine-tuning. Experimental results on the LongBench benchmark show that HCAttention maintains accuracy while reducing the KV cache memory footprint to 25% of its original size. It achieves competitive performance with only 12.5% of the cache, setting a new state-of-the-art in LLM KV cache compression. HCAttention extends the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory, making it the first of its kind. <div>
arXiv:2507.19823v1 Announce Type: new 
Abstract: Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value (KV) cache during inference. Existing KV cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic KV eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing transformer architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the KV cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in LLM KV cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments</title>
<link>https://arxiv.org/abs/2507.19867</link>
<guid>https://arxiv.org/abs/2507.19867</guid>
<content:encoded><![CDATA[
<div> Keywords: in-car conversational AI, DiscoDrive, synthetic corpus, disfluencies, training resource

Summary:
DiscoDrive is a synthetic corpus consisting of 3500 multi-turn dialogs across seven automotive domains, capturing spontaneous disfluencies like hesitations and repetitions. It addresses the lack of real-world driver-AI dialogs in existing datasets. The corpus enables models like DialoGPT-Medium and T5-Base to outperform KVRET-trained models on relevant test sets, showing improvements in various evaluation metrics. When used as a data augmentation resource in low-resource scenarios, DiscoDrive enhances model performance even further. Human evaluations indicate that dialogs from DiscoDrive are rated higher in naturalness and coherence compared to human-collected dialogs from KVRET. Additionally, the dialogs are perceived as more context-appropriate than existing post-hoc methods like LARD, without sacrificing clarity. This versatile corpus fills a crucial gap in existing resources and supports the training and augmentation of conversational AI systems for handling real-world in-car interactions effectively. 

<br /><br />Summary: DiscoDrive is a synthetic corpus of in-car conversational AI dialogs that integrates disfluencies, enhancing model performance and human evaluations. <div>
arXiv:2507.19867v1 Announce Type: new 
Abstract: In-car conversational AI is becoming increasingly critical as autonomous vehicles and smart assistants gain widespread adoption. Yet, existing datasets fail to capture the spontaneous disfluencies such as hesitations, false starts, repetitions, and self-corrections that characterize real driver-AI dialogs. To address this, we introduce DiscoDrive, a synthetic corpus of 3500 multi-turn dialogs across seven automotive domains, generated using a two-stage, prompt-driven pipeline that dynamically integrates disfluencies during synthesis. We show that DiscoDrive is effective both as a training resource, enabling DialoGPT-Medium and T5-Base to match or exceed KVRET-trained models on the MultiWOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets (BLEU-4 improvements of 0.26 to 0.61; METEOR +2.10; ROUGE-L +3.48; BERTScore F1 improvements of 1.35 to 3.48), and as a data augmentation resource in low-resource scenarios, delivering additional gains of up to BLEU-4 +0.38, METEOR +1.95, ROUGE-L +2.87, and BERTScore F1 +4.00 when combined with 10 percent of KVRET. Human evaluations further confirm that dialogs sampled from DiscoDrive are rated higher than KVRET's human-collected dialogs in naturalness (3.8 vs 3.6) and coherence (4.1 vs 4.0), and are perceived as more context-appropriate than leading post-hoc methods (such as LARD), without compromising clarity. DiscoDrive fills a critical gap in existing resources and serves as a versatile corpus for both training and augmenting conversational AI, enabling robust handling of real-world, disfluent in-car interactions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment</title>
<link>https://arxiv.org/abs/2507.19869</link>
<guid>https://arxiv.org/abs/2507.19869</guid>
<content:encoded><![CDATA[
<div> Polish Vocabulary Size Test, PVST, Item Response Theory, Computerized Adaptive Testing, native speakers, non-native speakers<br />
Summary:<br />
The Polish Vocabulary Size Test (PVST) is a new tool for assessing receptive vocabulary of Polish speakers, utilizing Item Response Theory and Computerized Adaptive Testing. In a pilot study with 1,475 participants, native Polish speakers demonstrated larger vocabularies than non-native speakers. Additionally, vocabulary size in native speakers correlated positively with age. The PVST is available online at myvocab.info/pl for easy access and administration. <div>
arXiv:2507.19869v1 Announce Type: new 
Abstract: We present the Polish Vocabulary Size Test (PVST), a novel tool for assessing the receptive vocabulary size of both native and non-native Polish speakers. Based on Item Response Theory and Computerized Adaptive Testing, PVST dynamically adjusts to each test-taker's proficiency level, ensuring high accuracy while keeping the test duration short. To validate the test, a pilot study was conducted with 1.475 participants. Native Polish speakers demonstrated significantly larger vocabularies compared to non-native speakers. For native speakers, vocabulary size showed a strong positive correlation with age. The PVST is available online at myvocab.info/pl.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam</title>
<link>https://arxiv.org/abs/2507.19885</link>
<guid>https://arxiv.org/abs/2507.19885</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, Multimodal Large Language Models, Medical applications, Language disparities, Healthcare.

Summary:
- The study evaluates the performance of various Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) in answering medical questions in Brazilian spoken Portuguese.
- Six LLMs and four MLLMs were tested against human candidates using questions from a medical residency entrance exam.
- Some models, such as Claude-3.5-Sonnet and Claude-3-Opus, showed accuracy levels similar to human candidates.
- Performance gaps were noted, especially in answering multimodal questions involving image interpretation.
- The study highlights language disparities and the need for further fine-tuning and data augmentation in non-English medical AI applications.
<br /><br />Summary: 
The study compared the performance of various LLMs and MLLMs in answering medical questions in Brazilian Portuguese. While some models performed well, gaps remained in handling multimodal questions. Language disparities were evident, underscoring the need for additional training and data augmentation in non-English medical AI applications. This research emphasizes the importance of evaluating generative AI in diverse linguistic and clinical contexts for its reliable deployment in healthcare. Future studies should focus on enhancing training methods, multimodal reasoning, and integrating AI in real-world clinical settings. <div>
arXiv:2507.19885v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has shown the potential to revolutionize healthcare by improving diagnostic accuracy, optimizing workflows, and personalizing treatment plans. Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have achieved notable advancements in natural language processing and medical applications. However, the evaluation of these models has focused predominantly on the English language, leading to potential biases in their performance across different languages.
  This study investigates the capability of six LLMs (GPT-4.0 Turbo, LLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and Command R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet, and Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese from the medical residency entrance exam of the Hospital das Cl\'inicas da Faculdade de Medicina da Universidade de S\~ao Paulo (HCFMUSP) - the largest health complex in South America. The performance of the models was benchmarked against human candidates, analyzing accuracy, processing time, and coherence of the generated explanations.
  The results show that while some models, particularly Claude-3.5-Sonnet and Claude-3-Opus, achieved accuracy levels comparable to human candidates, performance gaps persist, particularly in multimodal questions requiring image interpretation. Furthermore, the study highlights language disparities, emphasizing the need for further fine-tuning and data set augmentation for non-English medical AI applications.
  Our findings reinforce the importance of evaluating generative AI in various linguistic and clinical settings to ensure a fair and reliable deployment in healthcare. Future research should explore improved training methodologies, improved multimodal reasoning, and real-world clinical integration of AI-driven medical assistance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Gold Standard Dataset and Evaluation Framework for Depression Detection and Explanation in Social Media using LLMs</title>
<link>https://arxiv.org/abs/2507.19899</link>
<guid>https://arxiv.org/abs/2507.19899</guid>
<content:encoded><![CDATA[
<div> Keywords: depression, social media, dataset, natural language explanations, large language models

Summary:
In this study, a dataset of social media posts related to depression, expert-annotated with depressive spans and mapped to 12 symptom categories, is presented. This dataset allows for fine-grained evaluation of model predictions and generated explanations. The study focuses on evaluating the faithfulness and quality of natural language explanations generated by large language models (LLMs) such as GPT-4.1, Gemini 2.5 Pro, and Claude 3.7 Sonnet using various prompting strategies. Significant differences in model performance on clinical explanation tasks are observed, highlighting the importance of human expertise in guiding LLM behavior. The study emphasizes the need for safer and transparent AI systems for mental health interventions. <br /><br />Summary: <div>
arXiv:2507.19899v1 Announce Type: new 
Abstract: Early detection of depression from online social media posts holds promise for providing timely mental health interventions. In this work, we present a high-quality, expert-annotated dataset of 1,017 social media posts labeled with depressive spans and mapped to 12 depression symptom categories. Unlike prior datasets that primarily offer coarse post-level labels \cite{cohan-etal-2018-smhd}, our dataset enables fine-grained evaluation of both model predictions and generated explanations.
  We develop an evaluation framework that leverages this clinically grounded dataset to assess the faithfulness and quality of natural language explanations generated by large language models (LLMs). Through carefully designed prompting strategies, including zero-shot and few-shot approaches with domain-adapted examples, we evaluate state-of-the-art proprietary LLMs including GPT-4.1, Gemini 2.5 Pro, and Claude 3.7 Sonnet.
  Our comprehensive empirical analysis reveals significant differences in how these models perform on clinical explanation tasks, with zero-shot and few-shot prompting. Our findings underscore the value of human expertise in guiding LLM behavior and offer a step toward safer, more transparent AI systems for psychological well-being.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaliDrop: KV Cache Compression with Calibration</title>
<link>https://arxiv.org/abs/2507.19906</link>
<guid>https://arxiv.org/abs/2507.19906</guid>
<content:encoded><![CDATA[
<div> compression, large language models, key-value cache, token eviction, CaliDrop

Summary:
- Large Language Models (LLMs) require significant computational resources for generation.
- The Key-Value (KV) cache accelerates this process by storing attention intermediates but faces memory footprint challenges.
- Various techniques like token eviction, quantization, and low-rank projection help compress the KV cache.
- Token eviction is a common strategy but can lead to accuracy degradation, especially under high compression ratios.
- CaliDrop enhances token eviction through calibration, leveraging the observation that nearby positions in queries exhibit high similarity.
<br /><br />Summary: <div>
arXiv:2507.19906v1 Announce Type: new 
Abstract: Large Language Models (LLMs) require substantial computational resources during generation. While the Key-Value (KV) cache significantly accelerates this process by storing attention intermediates, its memory footprint grows linearly with sequence length, batch size, and model size, creating a bottleneck in long-context scenarios. Various KV cache compression techniques, including token eviction, quantization, and low-rank projection, have been proposed to mitigate this bottleneck, often complementing each other. This paper focuses on enhancing token eviction strategies. Token eviction leverages the observation that the attention patterns are often sparse, allowing for the removal of less critical KV entries to save memory. However, this reduction usually comes at the cost of notable accuracy degradation, particularly under high compression ratios. To address this issue, we propose \textbf{CaliDrop}, a novel strategy that enhances token eviction through calibration. Our preliminary experiments show that queries at nearby positions exhibit high similarity. Building on this observation, CaliDrop performs speculative calibration on the discarded tokens to mitigate the accuracy loss caused by token eviction. Extensive experiments demonstrate that CaliDrop significantly improves the accuracy of existing token eviction methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KLAAD: Refining Attention Mechanisms to Reduce Societal Bias in Generative Language Models</title>
<link>https://arxiv.org/abs/2507.19962</link>
<guid>https://arxiv.org/abs/2507.19962</guid>
<content:encoded><![CDATA[
<div> attention-based debiasing, large language models, societal biases, fairness, harm 

Summary: 
The article introduces KLAAD, a debiasing framework for large language models that aims to mitigate societal biases in their outputs. KLAAD utilizes attention alignment between stereotypical and anti-stereotypical sentence pairs without directly modifying model weights. By combining Cross-Entropy, KL divergence, and Triplet losses in a composite training objective, KLAAD guides the model to attend consistently across biased and unbiased contexts while maintaining fluency and coherence. Experimental results demonstrate that KLAAD effectively reduces bias in generative language models, as shown on the BBQ and BOLD benchmarks. Despite the bias mitigation, KLAAD has minimal impact on the quality of language modeling. This research suggests that attention-level alignment can offer a principled approach to addressing bias in language models. 

<br /><br />Summary: <div>
arXiv:2507.19962v1 Announce Type: new 
Abstract: Large language models (LLMs) often exhibit societal biases in their outputs, prompting ethical concerns regarding fairness and harm. In this work, we propose KLAAD (KL-Attention Alignment Debiasing), an attention-based debiasing framework that implicitly aligns attention distributions between stereotypical and anti-stereotypical sentence pairs without directly modifying model weights. KLAAD introduces a composite training objective combining Cross-Entropy, KL divergence, and Triplet losses, guiding the model to consistently attend across biased and unbiased contexts while preserving fluency and coherence. Experimental evaluation of KLAAD demonstrates improved bias mitigation on both the BBQ and BOLD benchmarks, with minimal impact on language modeling quality. The results indicate that attention-level alignment offers a principled solution for mitigating bias in generative language models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text</title>
<link>https://arxiv.org/abs/2507.19969</link>
<guid>https://arxiv.org/abs/2507.19969</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated data visualization, large language models, benchmark, text-to-visualization models, cross-modal actor-critic framework

Summary: 
Automated data visualization is essential for simplifying data interpretation and decision-making. The Text2Vis benchmark has been introduced to evaluate text-to-visualization models across various chart types and data science queries. This benchmark consists of 1,985 samples with data tables, natural language queries, visualization code, and annotated charts, covering complex reasoning and dynamic data retrieval. Testing 11 models revealed performance gaps and challenges, leading to the proposal of a cross-modal actor-critic framework. This framework refines textual answers and visualization code simultaneously, improving GPT-4o's pass rate and chart quality. Additionally, an automated evaluation framework based on large language models enables scalable assessment without human annotation, measuring answer correctness, code execution success, visualization readability, and chart accuracy. The Text2Vis benchmark and proposed frameworks offer insights for advancing text-to-visualization technology. 

<br /><br />Summary: <div>
arXiv:2507.19969v1 Announce Type: new 
Abstract: Automated data visualization plays a crucial role in simplifying data interpretation, enhancing decision-making, and improving efficiency. While large language models (LLMs) have shown promise in generating visualizations from natural language, the absence of comprehensive benchmarks limits the rigorous evaluation of their capabilities. We introduce Text2Vis, a benchmark designed to assess text-to-visualization models, covering 20+ chart types and diverse data science queries, including trend analysis, correlation, outlier detection, and predictive analytics. It comprises 1,985 samples, each with a data table, natural language query, short answer, visualization code, and annotated charts. The queries involve complex reasoning, conversational turns, and dynamic data retrieval. We benchmark 11 open-source and closed-source models, revealing significant performance gaps, highlighting key challenges, and offering insights for future advancements. To close this gap, we propose the first cross-modal actor-critic agentic framework that jointly refines the textual answer and visualization code, increasing GPT-4o`s pass rate from 26% to 42% over the direct approach and improving chart quality. We also introduce an automated LLM-based evaluation framework that enables scalable assessment across thousands of samples without human annotation, measuring answer correctness, code execution success, visualization readability, and chart accuracy. We release Text2Vis at https://github.com/vis-nlp/Text2Vis.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring LLM Autoscoring Reliability in Large-Scale Writing Assessments Using Generalizability Theory</title>
<link>https://arxiv.org/abs/2507.19980</link>
<guid>https://arxiv.org/abs/2507.19980</guid>
<content:encoded><![CDATA[
<div> Keywords: reliability, large language models, AP Chinese Exam, scoring, hybrid models

Summary:<br /><br />
This study examines the reliability of large language models (LLMs) in scoring writing tasks from the AP Chinese Language and Culture Exam. Using generalizability theory, the research evaluates the consistency of scores between human and AI raters for story narration and email response tasks. Results show that human raters generally produce more reliable scores, but LLMs demonstrate reasonable consistency, especially for story narration tasks. Composite scoring that combines human and AI ratings enhances reliability, suggesting that hybrid models could be advantageous for large-scale writing assessments. The study highlights the potential of hybrid scoring models in improving reliability for assessing writing tasks in language exams. <div>
arXiv:2507.19980v1 Announce Type: new 
Abstract: This study investigates the estimation of reliability for large language models (LLMs) in scoring writing tasks from the AP Chinese Language and Culture Exam. Using generalizability theory, the research evaluates and compares score consistency between human and AI raters across two types of AP Chinese free-response writing tasks: story narration and email response. These essays were independently scored by two trained human raters and seven AI raters. Each essay received four scores: one holistic score and three analytic scores corresponding to the domains of task completion, delivery, and language use. Results indicate that although human raters produced more reliable scores overall, LLMs demonstrated reasonable consistency under certain conditions, particularly for story narration tasks. Composite scoring that incorporates both human and AI raters improved reliability, which supports that hybrid scoring models may offer benefits for large-scale writing assessments.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering</title>
<link>https://arxiv.org/abs/2507.19995</link>
<guid>https://arxiv.org/abs/2507.19995</guid>
<content:encoded><![CDATA[
<div> large language models, legal text processing, natural language processing, legal NLP, VLQA dataset

Summary:
The article discusses the advancements in utilizing large language models for legal text processing, highlighting the challenges in automating legal tasks fully. The domain-specific nature of legal systems across different countries and languages presents a hurdle in developing legal text processing applications for various natural languages. The scarcity of resources and annotated data poses a significant challenge, particularly in low-resource languages like Vietnamese. To address this gap, the VLQA dataset is introduced as a high-quality resource tailored for the Vietnamese legal domain. A statistical analysis of the dataset is provided, showcasing its effectiveness in legal information retrieval and question-answering tasks when used with state-of-the-art models. Efforts to bridge the gap in legal NLP for diverse languages are essential for advancing the field of AI and NLP in the legal domain. 

<br /><br />Summary: <div>
arXiv:2507.19995v1 Announce Type: new 
Abstract: The advent of large language models (LLMs) has led to significant achievements in various domains, including legal text processing. Leveraging LLMs for legal tasks is a natural evolution and an increasingly compelling choice. However, their capabilities are often portrayed as greater than they truly are. Despite the progress, we are still far from the ultimate goal of fully automating legal tasks using artificial intelligence (AI) and natural language processing (NLP). Moreover, legal systems are deeply domain-specific and exhibit substantial variation across different countries and languages. The need for building legal text processing applications for different natural languages is, therefore, large and urgent. However, there is a big challenge for legal NLP in low-resource languages such as Vietnamese due to the scarcity of resources and annotated data. The need for labeled legal corpora for supervised training, validation, and supervised fine-tuning is critical. In this paper, we introduce the VLQA dataset, a comprehensive and high-quality resource tailored for the Vietnamese legal domain. We also conduct a comprehensive statistical analysis of the dataset and evaluate its effectiveness through experiments with state-of-the-art models on legal information retrieval and question-answering tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomaly Detection in Human Language via Meta-Learning: A Few-Shot Approach</title>
<link>https://arxiv.org/abs/2507.20019</link>
<guid>https://arxiv.org/abs/2507.20019</guid>
<content:encoded><![CDATA[
<div> framework, anomalies, language, meta-learning, detection
Summary:
- Researchers propose a meta learning framework for detecting anomalies in human language across different domains with limited labeled data.
- Anomalies in language, such as spam, fake news, and hate speech, are sparse and varied, making detection challenging.
- The approach treats anomaly detection as a few-shot binary classification problem and utilizes meta-learning to train models that can generalize across tasks.
- By combining episodic training with prototypical networks and domain resampling, the method can quickly adapt to new anomaly detection tasks.
- Empirical results demonstrate that the proposed method outperforms strong baselines in F1 and AUC scores.
<br /><br />Summary: <div>
arXiv:2507.20019v1 Announce Type: new 
Abstract: We propose a meta learning framework for detecting anomalies in human language across diverse domains with limited labeled data. Anomalies in language ranging from spam and fake news to hate speech pose a major challenge due to their sparsity and variability. We treat anomaly detection as a few shot binary classification problem and leverage meta-learning to train models that generalize across tasks. Using datasets from domains such as SMS spam, COVID-19 fake news, and hate speech, we evaluate model generalization on unseen tasks with minimal labeled anomalies. Our method combines episodic training with prototypical networks and domain resampling to adapt quickly to new anomaly detection tasks. Empirical results show that our method outperforms strong baselines in F1 and AUC scores. We also release the code and benchmarks to facilitate further research in few-shot text anomaly detection.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache Compression</title>
<link>https://arxiv.org/abs/2507.20030</link>
<guid>https://arxiv.org/abs/2507.20030</guid>
<content:encoded><![CDATA[
<div> Frequency-Adaptive Infinite-Window, KV cache compression, unbiased representation, Long-context tasks, Large Language Models <br />
<br />
Summary: <br />
The article introduces FAEDKV, a compression framework for Key-Value (KV) cache in Large Language Models (LLMs) that aims to address issues of biased representation and excessive memory usage. FAEDKV utilizes an Infinite-Window Fourier Transform to transform the KV cache into the frequency domain, ensuring equalized contribution of all tokens for unbiased information retention. A frequency ablation study identifies critical spectral components for effective compression. Experimental results on the LongBench benchmark demonstrate up to 22% improvement over existing methods. FAEDKV also outperforms compression-based approaches in position-agnostic retrieval accuracy on the Needle-In-A-Haystack task. This innovative approach offers a promising solution to enhancing the efficiency and effectiveness of LLMs in long-context tasks. <br /> <div>
arXiv:2507.20030v1 Announce Type: new 
Abstract: The efficacy of Large Language Models (LLMs) in long-context tasks is often hampered by the substantial memory footprint and computational demands of the Key-Value (KV) cache. Current compression strategies, including token eviction and learned projections, frequently lead to biased representations -- either by overemphasizing recent/high-attention tokens or by repeatedly degrading information from earlier context -- and may require costly model retraining. We present FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel, training-free KV cache compression framework that ensures unbiased information retention. FAEDKV operates by transforming the KV cache into the frequency domain using a proposed Infinite-Window Fourier Transform (IWDFT). This approach allows for the equalized contribution of all tokens to the compressed representation, effectively preserving both early and recent contextual information. A preliminary frequency ablation study identifies critical spectral components for layer-wise, targeted compression. Experiments on LongBench benchmark demonstrate FAEDKV's superiority over existing methods by up to 22\%. In addition, our method shows superior, position-agnostic retrieval accuracy on the Needle-In-A-Haystack task compared to compression based approaches.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infogen: Generating Complex Statistical Infographics from Documents</title>
<link>https://arxiv.org/abs/2507.20046</link>
<guid>https://arxiv.org/abs/2507.20046</guid>
<content:encoded><![CDATA[
<div> infographics, AI, LLMs, dataset, metadata<br />
Summary:<br />
The article introduces the task of generating complex statistical infographics from text-heavy documents using AI, particularly Large Language Models (LLMs). It addresses the lack of prior work in this area by proposing a new benchmark dataset called Infodat, which links documents to infographic metadata. The Infogen framework, a two-stage process involving fine-tuned LLMs, is presented for text-to-infographic code generation. Infogen achieves state-of-the-art performance in text-to-statistical infographic generation compared to existing LLMs. The generated infographics consist of multiple sub-charts like line, bar, and pie graphs, with contextually accurate and visually aligned insights. This work highlights the potential of AI in simplifying complex data into visually engaging and easy-to-understand formats through the creation of advanced statistical infographics. <br /><br />Summary: <div>
arXiv:2507.20046v1 Announce Type: new 
Abstract: Statistical infographics are powerful tools that simplify complex data into visually engaging and easy-to-understand formats. Despite advancements in AI, particularly with LLMs, existing efforts have been limited to generating simple charts, with no prior work addressing the creation of complex infographics from text-heavy documents that demand a deep understanding of the content. We address this gap by introducing the task of generating statistical infographics composed of multiple sub-charts (e.g., line, bar, pie) that are contextually accurate, insightful, and visually aligned. To achieve this, we define infographic metadata that includes its title and textual insights, along with sub-chart-specific details such as their corresponding data and alignment. We also present Infodat, the first benchmark dataset for text-to-infographic metadata generation, where each sample links a document to its metadata. We propose Infogen, a two-stage framework where fine-tuned LLMs first generate metadata, which is then converted into infographic code. Extensive evaluations on Infodat demonstrate that Infogen achieves state-of-the-art performance, outperforming both closed and open-source LLMs in text-to-statistical infographic generation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tensor-Based Compiler and a Runtime for Neuron-Level DNN Certifier Specifications</title>
<link>https://arxiv.org/abs/2507.20055</link>
<guid>https://arxiv.org/abs/2507.20055</guid>
<content:encoded><![CDATA[
<div> certification, DNNs, abstract interpretation, compiler, g-BCSR

Summary:
The article addresses the challenge of interpreting deep neural networks (DNNs) by proposing a compiler framework that automatically translates neuron-level specifications of DNN certifiers into tensor-based layer-level implementations. This framework overcomes the semantic gap between design and implementation by introducing a novel stack-based intermediate representation (IR) and a shape analysis. The shape analysis infers the implicit tensor operations needed to simulate neuron-level semantics, creating tensors in the minimal shape required for operations. At runtime, the resulting tensor computations exhibit sparsity, which is addressed by the introduction of the g-BCSR double-compression format. This format represents tensors as collections of blocks of varying sizes, each possibly internally sparse. By using this compiler and g-BCSR, developers can easily create new certifiers and analyze their utility across diverse DNNs while achieving performance comparable to hand-optimized implementations. <br /><br />Summary: <div>
arXiv:2507.20055v1 Announce Type: new 
Abstract: The uninterpretability of DNNs has led to the adoption of abstract interpretation-based certification as a practical means to establish trust in real-world systems that rely on DNNs. However, the current landscape supports only a limited set of certifiers, and developing new ones or modifying existing ones for different applications remains difficult. This is because the mathematical design of certifiers is expressed at the neuron level, while their implementations are optimized and executed at the tensor level. This mismatch creates a semantic gap between design and implementation, making manual bridging both complex and expertise-intensive -- requiring deep knowledge in formal methods, high-performance computing, etc.
  We propose a compiler framework that automatically translates neuron-level specifications of DNN certifiers into tensor-based, layer-level implementations. This is enabled by two key innovations: a novel stack-based intermediate representation (IR) and a shape analysis that infers the implicit tensor operations needed to simulate the neuron-level semantics. During lifting, the shape analysis creates tensors in the minimal shape required to perform the corresponding operations. The IR also enables domain-specific optimizations as rewrites. At runtime, the resulting tensor computations exhibit sparsity tied to the DNN architecture. This sparsity does not align well with existing formats. To address this, we introduce g-BCSR, a double-compression format that represents tensors as collections of blocks of varying sizes, each possibly internally sparse.
  Using our compiler and g-BCSR, we make it easy to develop new certifiers and analyze their utility across diverse DNNs. Despite its flexibility, the compiler achieves performance comparable to hand-optimized implementations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation</title>
<link>https://arxiv.org/abs/2507.20059</link>
<guid>https://arxiv.org/abs/2507.20059</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-augmented generation, Large language models, Knowledge retrieval, Realistic scenarios, Adaptive strategies<br />
Summary:<br />
Retrieval-augmented generation (RAG) integrates external knowledge with large language models (LLMs) to improve performance. However, a study using a diverse knowledge datastore called MassiveDS revealed limitations of current RAG systems. It was found that retrieval benefits smaller models more than larger ones and rerankers provide minimal improvement. Additionally, no single retrieval source consistently outperformed others, highlighting the challenge of routing queries across different knowledge sources. These findings underscore the importance of developing adaptive retrieval strategies before implementing RAG in practical applications. The study's code and data are available at https://github.com/ritaranx/RAG_in_the_Wild.<br /> 
Summary: <div>
arXiv:2507.20059v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved at inference time. While RAG demonstrates strong performance on benchmarks largely derived from general-domain corpora like Wikipedia, its effectiveness under realistic, diverse retrieval scenarios remains underexplored. We evaluated RAG systems using MassiveDS, a large-scale datastore with mixture of knowledge, and identified critical limitations: retrieval mainly benefits smaller models, rerankers add minimal value, and no single retrieval source consistently excels. Moreover, current LLMs struggle to route queries across heterogeneous knowledge sources. These findings highlight the need for adaptive retrieval strategies before deploying RAG in real-world settings. Our code and data can be found at https://github.com/ritaranx/RAG_in_the_Wild.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in Speech Language Models</title>
<link>https://arxiv.org/abs/2507.20091</link>
<guid>https://arxiv.org/abs/2507.20091</guid>
<content:encoded><![CDATA[
<div> Speech language model, prosody, tokenization scheme, pre-training, emerging capabilities
Summary: 
ProsodyLM introduces a novel tokenization scheme for training speech language models to capture the interdependency between content and prosody. Unlike traditional tokenization methods, which convert speech into discrete tokens before inputting into language models, ProsodyLM transcribes speech into text and adds word-level prosody tokens. This approach better retains prosody information and enhances the model's ability to learn prosody processing capabilities through pre-training alone. ProsodyLM demonstrates diverse emerging capabilities, such as detecting prosody nuances like contrastive focus, understanding emotion and stress in speech, and maintaining prosody consistency in longer contexts. This new approach could significantly improve the performance of speech language models in capturing and utilizing prosody information. 
<br /><br />Summary: <div>
arXiv:2507.20091v1 Announce Type: new 
Abstract: Speech language models refer to language models with speech processing and understanding capabilities. One key desirable capability for speech language models is the ability to capture the intricate interdependency between content and prosody. The existing mainstream paradigm of training speech language models, which converts speech into discrete tokens before feeding them into LLMs, is sub-optimal in learning prosody information -- we find that the resulting LLMs do not exhibit obvious emerging prosody processing capabilities via pre-training alone. To overcome this, we propose ProsodyLM, which introduces a simple tokenization scheme amenable to learning prosody. Each speech utterance is first transcribed into text, followed by a sequence of word-level prosody tokens. Compared with conventional speech tokenization schemes, the proposed tokenization scheme retains more complete prosody information, and is more understandable to text-based LLMs. We find that ProsodyLM can learn surprisingly diverse emerging prosody processing capabilities through pre-training alone, ranging from harnessing the prosody nuances in generated speech, such as contrastive focus, understanding emotion and stress in an utterance, to maintaining prosody consistency in long contexts.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Generation of Old English: A Framework for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2507.20111</link>
<guid>https://arxiv.org/abs/2507.20111</guid>
<content:encoded><![CDATA[
<div> Keywords: Old English, natural language processing, large language models, data augmentation, cultural preservation

Summary: 
The study addresses the lack of resources for Old English in natural language processing by proposing a scalable framework that uses large language models. The framework combines parameter-efficient fine-tuning, data augmentation through backtranslation, and a dual-agent pipeline for content generation and translation. Evaluation using automated metrics and expert human assessment demonstrates significant improvements in translation quality, with BLEU scores soaring from 26 to over 65. The approach not only expands the Old English corpus but also serves as a blueprint for preserving other endangered languages by leveraging AI innovation. This innovative method bridges the gap between artificial intelligence and cultural preservation, showcasing the potential for revitalizing ancient languages using advanced NLP techniques.

<br /><br />Summary: <div>
arXiv:2507.20111v1 Announce Type: new 
Abstract: Preserving ancient languages is essential for understanding humanity's cultural and linguistic heritage, yet Old English remains critically under-resourced, limiting its accessibility to modern natural language processing (NLP) techniques. We present a scalable framework that uses advanced large language models (LLMs) to generate high-quality Old English texts, addressing this gap. Our approach combines parameter-efficient fine-tuning (Low-Rank Adaptation, LoRA), data augmentation via backtranslation, and a dual-agent pipeline that separates the tasks of content generation (in English) and translation (into Old English). Evaluation with automated metrics (BLEU, METEOR, and CHRF) shows significant improvements over baseline models, with BLEU scores increasing from 26 to over 65 for English-to-Old English translation. Expert human assessment also confirms high grammatical accuracy and stylistic fidelity in the generated texts. Beyond expanding the Old English corpus, our method offers a practical blueprint for revitalizing other endangered languages, effectively uniting AI innovation with the goals of cultural preservation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering</title>
<link>https://arxiv.org/abs/2507.20133</link>
<guid>https://arxiv.org/abs/2507.20133</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, Direct Preference Optimization, Semantic consistency, Prompt engineering, Language models

Summary: 
Direct Preference Optimization (DPO) is a method used in generative AI to create realistic images from text prompts. However, DPO lacks semantic consistency, leading to prompts drifting from the intended meaning. To address this issue, Sem-DPO is introduced as a variant that maintains semantic consistency while still being efficient. By incorporating cosine distance weighting between prompts and candidates, Sem-DPO prevents semantic drift and ensures prompts stay close to the original text. Empirical results show that Sem-DPO outperforms DPO and other baselines in text-to-image prompt optimization tasks, achieving higher CLIP similarity and human-preference scores. This research suggests that incorporating semantic weighting in prompt optimization studies can significantly improve the quality of generated outputs and paves the way for future developments in semantics-aware preference optimization for language models. 

<br /><br />Summary: <div>
arXiv:2507.20133v1 Announce Type: new 
Abstract: Generative AI can now synthesize strikingly realistic images from text, yet output quality remains highly sensitive to how prompts are phrased. Direct Preference Optimization (DPO) offers a lightweight, off-policy alternative to RL for automatic prompt engineering, but its token-level regularization leaves semantic inconsistency unchecked as prompts that win higher preference scores can still drift away from the user's intended meaning.
  We introduce Sem-DPO, a variant of DPO that preserves semantic consistency yet retains its simplicity and efficiency. Sem-DPO scales the DPO loss by an exponential weight proportional to the cosine distance between the original prompt and winning candidate in embedding space, softly down-weighting training signals that would otherwise reward semantically mismatched prompts. We provide the first analytical bound on semantic drift for preference-tuned prompt generators, showing that Sem-DPO keeps learned prompts within a provably bounded neighborhood of the original text. On three standard text-to-image prompt-optimization benchmarks and two language models, Sem-DPO achieves 8-12% higher CLIP similarity and 5-9% higher human-preference scores (HPSv2.1, PickScore) than DPO, while also outperforming state-of-the-art baselines. These findings suggest that strong flat baselines augmented with semantic weighting should become the new standard for prompt-optimization studies and lay the groundwork for broader, semantics-aware preference optimization in language models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG</title>
<link>https://arxiv.org/abs/2507.20136</link>
<guid>https://arxiv.org/abs/2507.20136</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Models, hallucination, multi-modal, fact-seeking queries, factual accuracy

Summary: 
Team CRUISE developed a technical solution for the KDD Cup 2025 CRAG-MM challenge, which addresses the issue of hallucination in Vision Language Models (VLMs). Their framework prioritizes factual accuracy over completeness and includes a query router for efficiency, retrieval and summarization pipeline, generation pathways, and post-hoc verification. By focusing on answer reliability, their approach achieved 3rd place in Task 1 of the competition. The goal is to minimize hallucinations, especially in scenarios involving egocentric imagery, long-tail entities, and multi-hop questions in diverse modalities. The implementation of their solution is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2507.20136v1 Announce Type: new 
Abstract: This paper presents the technical solution developed by team CRUISE for the KDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn (CRAG-MM) challenge. The challenge aims to address a critical limitation of modern Vision Language Models (VLMs): their propensity to hallucinate, especially when faced with egocentric imagery, long-tail entities, and complex, multi-hop questions. This issue is particularly problematic in real-world applications where users pose fact-seeking queries that demand high factual accuracy across diverse modalities. To tackle this, we propose a robust, multi-stage framework that prioritizes factual accuracy and truthfulness over completeness. Our solution integrates a lightweight query router for efficiency, a query-aware retrieval and summarization pipeline, a dual-pathways generation and a post-hoc verification. This conservative strategy is designed to minimize hallucinations, which incur a severe penalty in the competition's scoring metric. Our approach achieved 3rd place in Task 1, demonstrating the effectiveness of prioritizing answer reliability in complex multi-modal RAG systems. Our implementation is available at https://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM .
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Interactive Question Generation Framework for Long Document Understanding</title>
<link>https://arxiv.org/abs/2507.20145</link>
<guid>https://arxiv.org/abs/2507.20145</guid>
<content:encoded><![CDATA[
<div> Keywords: Document Understanding, Vision-Language Models, Arabic, Multi-agent interactive framework, Automated generation

Summary: <br /><br />Document Understanding (DU) in long-contextual scenarios poses a challenge for Vision-Language Models (LVLMs), especially in low-resource languages like Arabic. Existing techniques rely heavily on human annotation, which is costly. To address this, a fully automated, multi-agent interactive framework has been proposed to efficiently generate high-quality questions for extensive English and Arabic documents, covering hundreds of pages across different domains. The generated questions have been shown to challenge major LVLMs in both open and closed sources. This work contributes to enhancing the long-context understanding ability of LVLMs, particularly in low-resource languages. The code and data for this framework can be accessed on GitHub, along with sample question and answer pairs and structured system prompts provided in the Appendix. <div>
arXiv:2507.20145v1 Announce Type: new 
Abstract: Document Understanding (DU) in long-contextual scenarios with complex layouts remains a significant challenge in vision-language research. Although Large Vision-Language Models (LVLMs) excel at short-context DU tasks, their performance declines in long-context settings. A key limitation is the scarcity of fine-grained training data, particularly for low-resource languages such as Arabic. Existing state-of-the-art techniques rely heavily on human annotation, which is costly and inefficient. We propose a fully automated, multi-agent interactive framework to generate long-context questions efficiently. Our approach efficiently generates high-quality single- and multi-page questions for extensive English and Arabic documents, covering hundreds of pages across diverse domains. This facilitates the development of LVLMs with enhanced long-context understanding ability. Experimental results in this work have shown that our generated English and Arabic questions (\textbf{AraEngLongBench}) are quite challenging to major open- and close-source LVLMs. The code and data proposed in this work can be found in https://github.com/wangk0b/Multi_Agentic_QA_Long_Doc.git. Sample Question and Answer (QA) pairs and structured system prompts can be found in the Appendix.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Goal Alignment in LLM-Based User Simulators for Conversational AI</title>
<link>https://arxiv.org/abs/2507.20152</link>
<guid>https://arxiv.org/abs/2507.20152</guid>
<content:encoded><![CDATA[
<div> User simulators, LLMs, goal-oriented behavior, User Goal State Tracking, goal progression tracking.  
Summary:  
User simulators are crucial for conversational AI development, but current LLMs struggle with consistent goal-oriented behavior in multi-turn conversations. The proposed User Goal State Tracking (UGST) framework tracks user goal progression and enables goal-aligned responses. A three-stage methodology is presented for developing user simulators that track goals and reason to generate appropriate responses. Evaluation metrics for measuring goal alignment in user simulators are established, showing significant improvements on MultiWOZ 2.4 and τ-Bench benchmarks. These contributions address a critical gap in conversational AI and establish UGST as a vital framework for developing goal-aligned user simulators.  
<br /><br />Summary: <div>
arXiv:2507.20152v1 Announce Type: new 
Abstract: User simulators are essential to conversational AI, enabling scalable agent development and evaluation through simulated interactions. While current Large Language Models (LLMs) have advanced user simulation capabilities, we reveal that they struggle to consistently demonstrate goal-oriented behavior across multi-turn conversations--a critical limitation that compromises their reliability in downstream applications. We introduce User Goal State Tracking (UGST), a novel framework that tracks user goal progression throughout conversations. Leveraging UGST, we present a three-stage methodology for developing user simulators that can autonomously track goal progression and reason to generate goal-aligned responses. Moreover, we establish comprehensive evaluation metrics for measuring goal alignment in user simulators, and demonstrate that our approach yields substantial improvements across two benchmarks (MultiWOZ 2.4 and {\tau}-Bench). Our contributions address a critical gap in conversational AI and establish UGST as an essential framework for developing goal-aligned user simulators.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGPO: Self-Generated Preference Optimization based on Self-Improver</title>
<link>https://arxiv.org/abs/2507.20181</link>
<guid>https://arxiv.org/abs/2507.20181</guid>
<content:encoded><![CDATA[
<div> framework, alignment, language models, self-generated preference optimization, self-improver <br />
Summary:
The article introduces Self-Generated Preference Optimization based on Self-Improver (SGPO), a novel framework for aligning large language models (LLMs) with human preferences. Traditional alignment methods rely on off-policy learning and human-annotated datasets, leading to limited applicability and distribution shift issues. SGPO, however, utilizes an on-policy self-improving mechanism where a single model refines responses to self-generate preference data for direct policy optimization. By learning incremental improvements through supervised fine-tuning outputs, SGPO outperforms conventional methods in tasks like AlpacaEval 2.0 and Arena-Hard without requiring external preference data. This approach not only enhances LLM performance but also addresses challenges in alignment by leveraging self-generated preference data for more reliable deployment. <br /><br />Summary: <div>
arXiv:2507.20181v1 Announce Type: new 
Abstract: Large language models (LLMs), despite their extensive pretraining on diverse datasets, require effective alignment to human preferences for practical and reliable deployment. Conventional alignment methods typically employ off-policy learning and depend on human-annotated datasets, which limits their broad applicability and introduces distribution shift issues during training. To address these challenges, we propose Self-Generated Preference Optimization based on Self-Improver (SGPO), an innovative alignment framework that leverages an on-policy self-improving mechanism. Specifically, the improver refines responses from a policy model to self-generate preference data for direct preference optimization (DPO) of the policy model. Here, the improver and policy are unified into a single model, and in order to generate higher-quality preference data, this self-improver learns to make incremental yet discernible improvements to the current responses by referencing supervised fine-tuning outputs. Experimental results on AlpacaEval 2.0 and Arena-Hard show that the proposed SGPO significantly improves performance over DPO and baseline self-improving methods without using external preference data.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SessionIntentBench: A Multi-task Inter-session Intention-shift Modeling Benchmark for E-commerce Customer Behavior Understanding</title>
<link>https://arxiv.org/abs/2507.20185</link>
<guid>https://arxiv.org/abs/2507.20185</guid>
<content:encoded><![CDATA[
<div> keywords: customer intention, session history, intention tree, multimodal benchmark, L(V)LMs<br />
Summary:<br />
The article introduces the concept of an intention tree and a dataset curation pipeline to address the lack of effective modeling of customer intention in E-commerce product purchase sessions. A new benchmark, SessionIntentBench, is proposed to evaluate the capability of L(V)LMs in understanding inter-session intention shifts. The dataset includes over 1.9 million intention entries and 1.1 million session intention trajectories extracted from 10,905 sessions. Human annotations were used to collect ground-truth labels for evaluation. Experiments show that current L(V)LMs struggle to capture and utilize intention in complex session settings, but injecting intention can improve their performance. This work provides a scalable approach to leveraging session data for customer intention understanding. <div>
arXiv:2507.20185v1 Announce Type: new 
Abstract: Session history is a common way of recording user interacting behaviors throughout a browsing activity with multiple products. For example, if an user clicks a product webpage and then leaves, it might because there are certain features that don't satisfy the user, which serve as an important indicator of on-the-spot user preferences. However, all prior works fail to capture and model customer intention effectively because insufficient information exploitation and only apparent information like descriptions and titles are used. There is also a lack of data and corresponding benchmark for explicitly modeling intention in E-commerce product purchase sessions. To address these issues, we introduce the concept of an intention tree and propose a dataset curation pipeline. Together, we construct a sibling multimodal benchmark, SessionIntentBench, that evaluates L(V)LMs' capability on understanding inter-session intention shift with four subtasks. With 1,952,177 intention entries, 1,132,145 session intention trajectories, and 13,003,664 available tasks mined using 10,905 sessions, we provide a scalable way to exploit the existing session data for customer intention understanding. We conduct human annotations to collect ground-truth label for a subset of collected data to form an evaluation gold set. Extensive experiments on the annotated data further confirm that current L(V)LMs fail to capture and utilize the intention across the complex session setting. Further analysis show injecting intention enhances LLMs' performances.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity-Enhanced Reasoning for Subjective Questions</title>
<link>https://arxiv.org/abs/2507.20187</link>
<guid>https://arxiv.org/abs/2507.20187</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, subjective reasoning tasks, diversity-enhanced framework, reinforcement learning, diversity and accuracy.

Summary: 
Large reasoning models with long chain-of-thought capabilities have shown strong performance on objective tasks but struggle with subjective questions due to homogeneous reasoning. To address this limitation, MultiRole-R1 introduces a diversity-enhanced framework with multiple role perspectives for subjective reasoning tasks. It generates diverse reasoning chains through unsupervised data construction and utilizes Group Relative Policy Optimization in reinforcement learning with diversity as a reward signal. By promoting perspective and lexical diversity, MultiRole-R1 improves both accuracy and diversity in reasoning. Experimental results on six benchmarks demonstrate the effectiveness and generalizability of MultiRole-R1 in enhancing both subjective and objective reasoning. This study highlights the importance of diversity-enhanced training in large reasoning models. 

<br /><br />Summary: <div>
arXiv:2507.20187v1 Announce Type: new 
Abstract: Large reasoning models (LRM) with long chain-of-thought (CoT) capabilities have shown strong performance on objective tasks, such as math reasoning and coding. However, their effectiveness on subjective questions that may have different responses from different perspectives is still limited by a tendency towards homogeneous reasoning, introduced by the reliance on a single ground truth in supervised fine-tuning and verifiable reward in reinforcement learning. Motivated by the finding that increasing role perspectives consistently improves performance, we propose MultiRole-R1, a diversity-enhanced framework with multiple role perspectives, to improve the accuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an unsupervised data construction pipeline that generates reasoning chains that incorporate diverse role perspectives. We further employ reinforcement learning via Group Relative Policy Optimization (GRPO) with reward shaping, by taking diversity as a reward signal in addition to the verifiable reward. With specially designed reward functions, we successfully promote perspective diversity and lexical diversity, uncovering a positive relation between reasoning diversity and accuracy. Our experiment on six benchmarks demonstrates MultiRole-R1's effectiveness and generalizability in enhancing both subjective and objective reasoning, showcasing the potential of diversity-enhanced training in LRMs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IQ Test for LLMs: An Evaluation Framework for Uncovering Core Skills in LLMs</title>
<link>https://arxiv.org/abs/2507.20208</link>
<guid>https://arxiv.org/abs/2507.20208</guid>
<content:encoded><![CDATA[
<div> factor analysis, large language models, benchmark scores, latent skills, model evaluation

Summary:
- The article discusses the limitations of current evaluations of large language models (LLMs) based on benchmark scores.
- It proposes a new evaluation paradigm using factor analysis to identify latent skills influencing performance across benchmarks.
- The study examined 60 LLMs on 44 tasks and identified a small set of latent skills that largely explained their performance.
- Practical tools were developed based on these insights to identify redundant tasks, assist in model selection, and profile models based on each latent skill. 

<br /><br />Summary: <div>
arXiv:2507.20208v1 Announce Type: new 
Abstract: Current evaluations of large language models (LLMs) rely on benchmark scores, but it is difficult to interpret what these individual scores reveal about a model's overall skills. Specifically, as a community we lack understanding of how tasks relate to one another, what they measure in common, how they differ, or which ones are redundant. As a result, models are often assessed via a single score averaged across benchmarks, an approach that fails to capture the models' wholistic strengths and limitations. Here, we propose a new evaluation paradigm that uses factor analysis to identify latent skills driving performance across benchmarks. We apply this method to a comprehensive new leaderboard showcasing the performance of 60 LLMs on 44 tasks, and identify a small set of latent skills that largely explain performance. Finally, we turn these insights into practical tools that identify redundant tasks, aid in model selection, and profile models along each latent skill.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation</title>
<link>https://arxiv.org/abs/2507.20210</link>
<guid>https://arxiv.org/abs/2507.20210</guid>
<content:encoded><![CDATA[
<div> Keywords: news recommendation systems, multi-view news representations, user interests, Co-NAML-LSTUR, BERT-based word embeddings

Summary:
News recommendation systems are important for delivering personalized news content and addressing information overload. The challenge lies in effectively modeling multi-view news representations and dynamic user interests, spanning short- and long-term preferences. Existing methods often fall short in capturing comprehensive user preferences across different time scales. To address this, Co-NAML-LSTUR is proposed, a hybrid framework combining NAML for multi-view news modeling, LSTUR for dual-scale user representations, and BERT-based word embeddings for semantic feature extraction. Evaluation on MIND-small and MIND-large benchmarks shows significant improvements over state-of-the-art baselines. This highlights the effectiveness of integrating multi-view news representations with dual-scale user modeling.<br /><br />Summary: <div>
arXiv:2507.20210v1 Announce Type: new 
Abstract: News recommendation systems play a vital role in mitigating information overload by delivering personalized news content. A central challenge is to effectively model both multi-view news representations and the dynamic nature of user interests, which often span both short- and long-term preferences. Existing methods typically rely on single-view features of news articles (e.g., titles or categories) or fail to comprehensively capture user preferences across time scales. In this work, we propose Co-NAML-LSTUR, a hybrid news recommendation framework that integrates NAML for attentive multi-view news modeling and LSTUR for capturing both long- and short-term user representations. Our model also incorporates BERT-based word embeddings to enhance semantic feature extraction. We evaluate Co-NAML-LSTUR on two widely used benchmarks, MIND-small and MIND-large. Experimental results show that Co-NAML-LSTUR achieves substantial improvements over most state-of-the-art baselines on MIND-small and MIND-large, respectively. These results demonstrate the effectiveness of combining multi-view news representations with dual-scale user modeling. The implementation of our model is publicly available at https://github.com/MinhNguyenDS/Co-NAML-LSTUR.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reframe Your Life Story: Interactive Narrative Therapist and Innovative Moment Assessment with Large Language Models</title>
<link>https://arxiv.org/abs/2507.20241</link>
<guid>https://arxiv.org/abs/2507.20241</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, mental health support, narrative therapy, Interactive Narrative Therapist, Innovative Moment Assessment 

Summary: 
The article discusses the use of large language models (LLMs) in providing mental health support, particularly in the context of narrative therapy. A new framework is proposed, consisting of two core components: INT (Interactive Narrative Therapist) and IMA (Innovative Moment Assessment). INT simulates expert narrative therapists by guiding therapeutic stages and generating expert-like responses. IMA tracks "Innovative Moments" (IMs) in client speech to assess therapy progress. Experimental results show that INT outperforms standard LLMs in therapeutic quality and depth. The framework aims to address the limitations of current approaches in simulating specialized psychotherapy and capturing therapeutic progression over time. The effectiveness of INT is demonstrated in facilitating social applications and synthesizing high-quality support conversations. <div>
arXiv:2507.20241v1 Announce Type: new 
Abstract: Recent progress in large language models (LLMs) has opened new possibilities for mental health support, yet current approaches lack realism in simulating specialized psychotherapy and fail to capture therapeutic progression over time. Narrative therapy, which helps individuals transform problematic life stories into empowering alternatives, remains underutilized due to limited access and social stigma. We address these limitations through a comprehensive framework with two core components. First, INT (Interactive Narrative Therapist) simulates expert narrative therapists by planning therapeutic stages, guiding reflection levels, and generating contextually appropriate expert-like responses. Second, IMA (Innovative Moment Assessment) provides a therapy-centric evaluation method that quantifies effectiveness by tracking "Innovative Moments" (IMs), critical narrative shifts in client speech signaling therapy progress. Experimental results on 260 simulated clients and 230 human participants reveal that INT consistently outperforms standard LLMs in therapeutic quality and depth. We further demonstrate the effectiveness of INT in synthesizing high-quality support conversations to facilitate social applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Professionalism in Expert Questioning through Linguistic Differentiation</title>
<link>https://arxiv.org/abs/2507.20249</link>
<guid>https://arxiv.org/abs/2507.20249</guid>
<content:encoded><![CDATA[
<div> Keywords: professionalism, expert communication, linguistic features, financial analysis, annotation framework

Summary:
Professionalism in expert communication, especially in fields like finance, is essential yet not fully explored. This study delves into how linguistic attributes can aid in modeling and assessing professionalism in expert questioning. A unique annotation framework was developed to measure structural and pragmatic components in financial analyst questions. Two datasets were created, one annotated for perceived professionalism and the other labeled by question origin, using both human and large language model-generated questions. The research revealed that specific linguistic features strongly correlate with human judgments and question authorship, indicating a common stylistic basis. A classifier trained solely on these features outperformed existing baselines in identifying expert-authored questions. The study suggests that professionalism can be effectively represented through linguistically grounded modeling, highlighting its learnable nature across domains. <br /><br />Summary: <div>
arXiv:2507.20249v1 Announce Type: new 
Abstract: Professionalism is a crucial yet underexplored dimension of expert communication, particularly in high-stakes domains like finance. This paper investigates how linguistic features can be leveraged to model and evaluate professionalism in expert questioning. We introduce a novel annotation framework to quantify structural and pragmatic elements in financial analyst questions, such as discourse regulators, prefaces, and request types. Using both human-authored and large language model (LLM)-generated questions, we construct two datasets: one annotated for perceived professionalism and one labeled by question origin. We show that the same linguistic features correlate strongly with both human judgments and authorship origin, suggesting a shared stylistic foundation. Furthermore, a classifier trained solely on these interpretable features outperforms gemini-2.0 and SVM baselines in distinguishing expert-authored questions. Our findings demonstrate that professionalism is a learnable, domain-general construct that can be captured through linguistically grounded modeling.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Completion Learning for Language Models</title>
<link>https://arxiv.org/abs/2507.20252</link>
<guid>https://arxiv.org/abs/2507.20252</guid>
<content:encoded><![CDATA[
<div> Keywords: language model, training, post-completion learning, reinforcement learning, output quality <br />
Summary: <br />
The article introduces Post-Completion Learning (PCL), a novel training framework for language models that leverages the space after output completion to enhance reasoning and self-evaluation abilities. PCL involves continuing to generate self-assessments and reward predictions during training while maintaining efficient inference by stopping at the completion point. By utilizing a white-box reinforcement learning method, models evaluate output content and align scores with reward functions for supervision. This dual-track approach optimizes reasoning and evaluation capabilities and combines them with reinforcement learning for multi-objective hybrid optimization. Experimental results across various datasets and models demonstrate improvements over traditional Structured Self-Training (SFT) and reinforcement learning methods. The method provides a new technical pathway for training language models to enhance output quality while ensuring deployment efficiency. <br /> <div>
arXiv:2507.20252v1 Announce Type: new 
Abstract: Current language model training paradigms typically terminate learning upon reaching the end-of-sequence (}) token, overlooking the potential learning opportunities in the post-completion space. We propose Post-Completion Learning (PCL), a novel training framework that systematically utilizes the sequence space after model output completion, to enhance both the reasoning and self-evaluation abilities. PCL enables models to continue generating self-assessments and reward predictions during training, while maintaining efficient inference by stopping at the completion point.
  To fully utilize this post-completion space, we design a white-box reinforcement learning method: let the model evaluate the output content according to the reward rules, then calculate and align the score with the reward functions for supervision. We implement dual-track SFT to optimize both reasoning and evaluation capabilities, and mixed it with RL training to achieve multi-objective hybrid optimization.
  Experimental results on different datasets and models demonstrate consistent improvements over traditional SFT and RL methods. Our method provides a new technical path for language model training that enhances output quality while preserving deployment efficiency.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMBRACE: Shaping Inclusive Opinion Representation by Aligning Implicit Conversations with Social Norms</title>
<link>https://arxiv.org/abs/2507.20264</link>
<guid>https://arxiv.org/abs/2507.20264</guid>
<content:encoded><![CDATA[
<div> demographics, behavioral attributes, implicit opinion, alignment evaluation framework, inclusive representation

Summary: 
This study addresses the need for more inclusive representations in conversation-based models by focusing on implicit opinions and evaluating normative alignment through the stance of responses. Existing methods often overlook implicit expressions of opinion, leading to misalignment and reinforcing harmful stereotypes. The proposed framework evaluates how opinions are represented in NLP models, utilizing the stance of responses to capture diverse social viewpoints. The study utilizes positive-unlabeled online learning with base classifiers and instruction-tuned language models to assess post-training alignment, shedding light on how implicit opinions are (mis) represented and offering a pathway towards more inclusive model behavior. <div>
arXiv:2507.20264v1 Announce Type: new 
Abstract: Shaping inclusive representations that embrace diversity and ensure fair participation and reflections of values is at the core of many conversation-based models. However, many existing methods rely on surface inclusion using mention of user demographics or behavioral attributes of social groups. Such methods overlook the nuanced, implicit expression of opinion embedded in conversations. Furthermore, the over-reliance on overt cues can exacerbate misalignment and reinforce harmful or stereotypical representations in model outputs. Thus, we took a step back and recognized that equitable inclusion needs to account for the implicit expression of opinion and use the stance of responses to validate the normative alignment. This study aims to evaluate how opinions are represented in NLP or computational models by introducing an alignment evaluation framework that foregrounds implicit, often overlooked conversations and evaluates the normative social views and discourse. Our approach models the stance of responses as a proxy for the underlying opinion, enabling a considerate and reflective representation of diverse social viewpoints. We evaluate the framework using both (i) positive-unlabeled (PU) online learning with base classifiers, and (ii) instruction-tuned language models to assess post-training alignment. Through this, we provide a lens on how implicit opinions are (mis)represented and offer a pathway toward more inclusive model behavior.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for Feedback-Independent Reasoning</title>
<link>https://arxiv.org/abs/2507.20278</link>
<guid>https://arxiv.org/abs/2507.20278</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, sequential environmental feedback, chain-of-thought reasoning, MoL-RL, multi-step interactions 

Summary: 
Large language models (LLMs) face challenges in effectively utilizing sequential environmental feedback (EF) for chain-of-thought reasoning. Existing methods struggle to capture the rich contextual information in EF signals. To address this, MoL-RL integrates multi-step EF signals into LLMs using a dual-objective optimization framework. It employs MoL continual training to optimize domain-specific EF signals and general language capabilities separately, while utilizing GRPO-based post-training to distill sequential EF interactions. This approach enables robust feedback-independent reasoning without external feedback loops. Experimental results on mathematical reasoning and code generation benchmarks show that MoL-RL with the Qwen3-8B model achieves state-of-the-art performance and strong generalization across model scales. This work presents a promising method to enhance LLMs' reasoning capabilities through multi-step textual feedback integration. 

Summary: <br /><br /> <div>
arXiv:2507.20278v1 Announce Type: new 
Abstract: Large language models (LLMs) face significant challenges in effectively leveraging sequential environmental feedback (EF) signals, such as natural language evaluations, for feedback-independent chain-of-thought (CoT) reasoning. Existing approaches either convert EF into scalar rewards, losing rich contextual information, or employ refinement datasets, failing to exploit the multi-step and discrete nature of EF interactions. To address these limitations, we propose MoL-RL, a novel training paradigm that integrates multi-step EF signals into LLMs through a dual-objective optimization framework. Our method combines MoL (Mixture-of-Losses) continual training, which decouples domain-specific EF signals (optimized via cross-entropy loss) and general language capabilities (preserved via Kullback-Leibler divergence), with GRPO-based post-training to distill sequential EF interactions into single-step inferences. This synergy enables robust feedback-independent reasoning without relying on external feedback loops. Experimental results on mathematical reasoning (MATH-500, AIME24/AIME25) and code generation (CodeAgent-Test) benchmarks demonstrate that MoL-RL achieves state-of-the-art performance with the Qwen3-8B model, while maintaining strong generalization across model scales (Qwen3-4B). This work provides a promising approach for leveraging multi-step textual feedback to enhance LLMs' reasoning capabilities in diverse domains.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations</title>
<link>https://arxiv.org/abs/2507.20279</link>
<guid>https://arxiv.org/abs/2507.20279</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, multilingual data, code-mixed, neuron activation patterns, cross-lingual transfer
  
Summary:  
1. Aya-23-8B, a decoder-only Large Language Model (LLM) trained on balanced multilingual data, demonstrates superior performance in handling code-mixed, cloze, and translation tasks compared to predominantly monolingual models like Llama 3 and Chinese-LLaMA-2.  
2. The model activates typologically related language representations during translation, unlike English-centric models that rely on a single pivot language.  
3. Neuron activation patterns in Aya-23 vary with mixing rates and are influenced more by the base language than the mixed-in language in code-mixed tasks.  
4. Language-specific neurons in Aya-23 for code-mixed inputs are concentrated in final layers, deviating from previous findings on decoder-only models.  
5. Analysis shows that script similarity and typological relations impact language processing across different model types.  

<br /><br />Summary: <div>
arXiv:2507.20279v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at multilingual tasks, yet their internal language processing remains poorly understood. We analyze how Aya-23-8B, a decoder-only LLM trained on balanced multilingual data, handles code-mixed, cloze, and translation tasks compared to predominantly monolingual models like Llama 3 and Chinese-LLaMA-2. Using logit lens and neuron specialization analyses, we find: (1) Aya-23 activates typologically related language representations during translation, unlike English-centric models that rely on a single pivot language; (2) code-mixed neuron activation patterns vary with mixing rates and are shaped more by the base language than the mixed-in one; and (3) Aya-23's languagespecific neurons for code-mixed inputs concentrate in final layers, diverging from prior findings on decoder-only models. Neuron overlap analysis further shows that script similarity and typological relations impact processing across model types. These findings reveal how multilingual training shapes LLM internals and inform future cross-lingual transfer research.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation</title>
<link>https://arxiv.org/abs/2507.20301</link>
<guid>https://arxiv.org/abs/2507.20301</guid>
<content:encoded><![CDATA[
<div> Keywords: Dialectal Arabic, Machine Translation, Natural Language Processing, Prompting Techniques, Fine-tuning Pipeline

Summary:<br /><br />Dialectal Arabic (DA) presents challenges for natural language processing due to its differences from Modern Standard Arabic. This paper introduces new techniques to improve DA-MSA translation for Levantine, Egyptian, and Gulf dialects in low-resource settings. Six large language models were evaluated for prompting strategies, with GPT-4o achieving the best performance. A quantized Gemma2-9B model outperformed zero-shot GPT-4o in fine-tuning, with 4-bit quantization reducing memory usage. Joint multi-dialect trained models showed significant performance gains over single-dialect models. The study provides a practical pathway for enhancing dialectal inclusion in Arabic NLP, demonstrating the feasibility of high-quality DA-MSA machine translation with limited resources. This research has implications for improving access to digital services and educational resources in dialectal Arabic-speaking regions. 

Summary: <div>
arXiv:2507.20301v1 Announce Type: new 
Abstract: Dialectal Arabic (DA) poses a persistent challenge for natural language processing (NLP), as most everyday communication in the Arab world occurs in dialects that diverge significantly from Modern Standard Arabic (MSA). This linguistic divide limits access to digital services and educational resources and impedes progress in Arabic machine translation. This paper presents two core contributions to advancing DA-MSA translation for the Levantine, Egyptian, and Gulf dialects, particularly in low-resource and computationally constrained settings: a comprehensive evaluation of training-free prompting techniques, and the development of a resource-efficient fine-tuning pipeline. Our evaluation of prompting strategies across six large language models (LLMs) found that few-shot prompting consistently outperformed zero-shot, chain-of-thought, and our proposed Ara-TEaR method. GPT-4o achieved the highest performance across all prompting settings. For fine-tuning, a quantized Gemma2-9B model achieved a CHrF++ score of 49.88, outperforming zero-shot GPT-4o (44.58). Joint multi-dialect trained models outperformed single-dialect counterparts by over 10% CHrF++, and 4-bit quantization reduced memory usage by 60% with less than 1% performance loss. The results and insights of our experiments offer a practical blueprint for improving dialectal inclusion in Arabic NLP, showing that high-quality DA-MSA machine translation is achievable even with limited resources and paving the way for more inclusive language technologies.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns</title>
<link>https://arxiv.org/abs/2507.20343</link>
<guid>https://arxiv.org/abs/2507.20343</guid>
<content:encoded><![CDATA[
<div> Keywords: DYNARTmo, dynamic articulatory model, speech articulation, phonetics education, speech therapy

Summary: 
DYNARTmo is a dynamic articulatory model that visualizes speech articulation processes in a two-dimensional midsagittal plane. It is based on the UK-DYNAMO framework and incorporates articulatory underspecification, segmental and gestural control, and coarticulation principles. The model simulates six key articulators using continuous and discrete control parameters, enabling the generation of vocalic and consonantal articulatory configurations. The current implementation is part of a web-based application called SpeechArticulationTrainer, which includes sagittal, glottal, and palatal views for use in phonetics education and speech therapy. Future work will focus on dynamic movement generation and integration with articulatory-acoustic modules. This research contributes to the understanding and visualization of speech articulation processes for educational and therapeutic purposes. 

<br /><br />Summary: <div>
arXiv:2507.20343v1 Announce Type: new 
Abstract: We present DYNARTmo, a dynamic articulatory model designed to visualize speech articulation processes in a two-dimensional midsagittal plane. The model builds upon the UK-DYNAMO framework and integrates principles of articulatory underspecification, segmental and gestural control, and coarticulation. DYNARTmo simulates six key articulators based on ten continuous and six discrete control parameters, allowing for the generation of both vocalic and consonantal articulatory configurations. The current implementation is embedded in a web-based application (SpeechArticulationTrainer) that includes sagittal, glottal, and palatal views, making it suitable for use in phonetics education and speech therapy. While this paper focuses on the static modeling aspects, future work will address dynamic movement generation and integration with articulatory-acoustic modules.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RMTBench: Benchmarking LLMs Through Multi-Turn User-Centric Role-Playing</title>
<link>https://arxiv.org/abs/2507.20352</link>
<guid>https://arxiv.org/abs/2507.20352</guid>
<content:encoded><![CDATA[
arXiv:2507.20352v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have shown outstanding potential for role-playing applications. Evaluating these capabilities is becoming crucial yet remains challenging. Existing benchmarks mostly adopt a \textbf{character-centric} approach, simplify user-character interactions to isolated Q&amp;A tasks, and fail to reflect real-world applications. To address this limitation, we introduce RMTBench, a comprehensive \textbf{user-centric} bilingual role-playing benchmark featuring 80 diverse characters and over 8,000 dialogue rounds. RMTBench includes custom characters with detailed backgrounds and abstract characters defined by simple traits, enabling evaluation across various user scenarios. Our benchmark constructs dialogues based on explicit user motivations rather than character descriptions, ensuring alignment with practical user applications. Furthermore, we construct an authentic multi-turn dialogue simulation mechanism. With carefully selected evaluation dimensions and LLM-based scoring, this mechanism captures the complex intention of conversations between the user and the character. By shifting focus from character background to user intention fulfillment, RMTBench bridges the gap between academic evaluation and practical deployment requirements, offering a more effective framework for assessing role-playing capabilities in LLMs. All code and datasets will be released soon.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Length Representations in Large Language Models</title>
<link>https://arxiv.org/abs/2507.20398</link>
<guid>https://arxiv.org/abs/2507.20398</guid>
<content:encoded><![CDATA[
arXiv:2507.20398v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown remarkable capabilities across various tasks, that are learned from massive amounts of text-based data. Although LLMs can control output sequence length, particularly in instruction-based settings, the internal mechanisms behind this control have been unexplored yet. In this study, we provide empirical evidence on how output sequence length information is encoded within the internal representations in LLMs. In particular, our findings show that multi-head attention mechanisms are critical in determining output sequence length, which can be adjusted in a disentangled manner. By scaling specific hidden units within the model, we can control the output sequence length without losing the informativeness of the generated text, thereby indicating that length information is partially disentangled from semantic information. Moreover, some hidden units become increasingly active as prompts become more length-specific, thus reflecting the model's internal awareness of this attribute. Our findings suggest that LLMs have learned robust and adaptable internal mechanisms for controlling output length without any external control.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations</title>
<link>https://arxiv.org/abs/2507.20409</link>
<guid>https://arxiv.org/abs/2507.20409</guid>
<content:encoded><![CDATA[
arXiv:2507.20409v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) prompting helps models think step by step. But what happens when they must see, understand, and judge-all at once? In visual tasks grounded in social context, where bridging perception with norm-grounded judgments is essential, flat CoT often breaks down. We introduce Cognitive Chain-of-Thought (CoCoT), a prompting strategy that scaffolds VLM reasoning through three cognitively inspired stages: perception, situation, and norm. Our experiments show that, across multiple multimodal benchmarks (including intent disambiguation, commonsense reasoning, and safety), CoCoT consistently outperforms CoT and direct prompting (+8\% on average). Our findings demonstrate that cognitively grounded reasoning stages enhance interpretability and social awareness in VLMs, paving the way for safer and more reliable multimodal systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CONCAP: Seeing Beyond English with Concepts Retrieval-Augmented Captioning</title>
<link>https://arxiv.org/abs/2507.20411</link>
<guid>https://arxiv.org/abs/2507.20411</guid>
<content:encoded><![CDATA[
arXiv:2507.20411v1 Announce Type: new 
Abstract: Multilingual vision-language models have made significant strides in image captioning, yet they still lag behind their English counterparts due to limited multilingual training data and costly large-scale model parameterization. Retrieval-augmented generation (RAG) offers a promising alternative by conditioning caption generation on retrieved examples in the target language, reducing the need for extensive multilingual training. However, multilingual RAG captioning models often depend on retrieved captions translated from English, which can introduce mismatches and linguistic biases relative to the source language. We introduce CONCAP, a multilingual image captioning model that integrates retrieved captions with image-specific concepts, enhancing the contextualization of the input image and grounding the captioning process across different languages. Experiments on the XM3600 dataset indicate that CONCAP enables strong performance on low- and mid-resource languages, with highly reduced data requirements. Our findings highlight the effectiveness of concept-aware retrieval augmentation in bridging multilingual performance gaps.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?</title>
<link>https://arxiv.org/abs/2507.20419</link>
<guid>https://arxiv.org/abs/2507.20419</guid>
<content:encoded><![CDATA[
arXiv:2507.20419v1 Announce Type: new 
Abstract: Natural Language Understanding (NLU) is a basic task in Natural Language Processing (NLP). The evaluation of NLU capabilities has become a trending research topic that attracts researchers in the last few years, resulting in the development of numerous benchmarks. These benchmarks include various tasks and datasets in order to evaluate the results of pretrained models via public leaderboards. Notably, several benchmarks contain diagnostics datasets designed for investigation and fine-grained error analysis across a wide range of linguistic phenomena. This survey provides a comprehensive review of available English, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on their diagnostics datasets and the linguistic phenomena they covered. We present a detailed comparison and analysis of these benchmarks, highlighting their strengths and limitations in evaluating NLU tasks and providing in-depth error analysis. When highlighting the gaps in the state-of-the-art, we noted that there is no naming convention for macro and micro categories or even a standard set of linguistic phenomena that should be covered. Consequently, we formulated a research question regarding the evaluation metrics of the evaluation diagnostics benchmarks: "Why do not we have an evaluation standard for the NLU evaluation diagnostics benchmarks?" similar to ISO standard in industry. We conducted a deep analysis and comparisons of the covered linguistic phenomena in order to support experts in building a global hierarchy for linguistic phenomena in future. We think that having evaluation metrics for diagnostics evaluation could be valuable to gain more insights when comparing the results of the studied models on different diagnostics benchmarks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeNER: Code Prompting for Named Entity Recognition</title>
<link>https://arxiv.org/abs/2507.20423</link>
<guid>https://arxiv.org/abs/2507.20423</guid>
<content:encoded><![CDATA[
arXiv:2507.20423v1 Announce Type: new 
Abstract: Recent studies have explored various approaches for treating candidate named entity spans as both source and target sequences in named entity recognition (NER) by leveraging large language models (LLMs). Although previous approaches have successfully generated candidate named entity spans with suitable labels, they rely solely on input context information when using LLMs, particularly, ChatGPT. However, NER inherently requires capturing detailed labeling requirements with input context information. To address this issue, we propose a novel method that leverages code-based prompting to improve the capabilities of LLMs in understanding and performing NER. By embedding code within prompts, we provide detailed BIO schema instructions for labeling, thereby exploiting the ability of LLMs to comprehend long-range scopes in programming languages. Experimental results demonstrate that the proposed code-based prompting method outperforms conventional text-based prompting on ten benchmarks across English, Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of explicitly structuring NER instructions. We also verify that combining the proposed code-based prompting method with the chain-of-thought prompting further improves performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems</title>
<link>https://arxiv.org/abs/2507.20491</link>
<guid>https://arxiv.org/abs/2507.20491</guid>
<content:encoded><![CDATA[
arXiv:2507.20491v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have significantly enhanced question-answering (QA) capabilities, particularly in open-domain contexts. However, in closed-domain scenarios such as education, healthcare, and law, users demand not only accurate answers but also transparent reasoning and explainable decision-making processes. While neural-symbolic (NeSy) frameworks have emerged as a promising solution, leveraging LLMs for natural language understanding and symbolic systems for formal reasoning, existing approaches often rely on large-scale models and exhibit inefficiencies in translating natural language into formal logic representations.
  To address these limitations, we introduce Text-JEPA (Text-based Joint-Embedding Predictive Architecture), a lightweight yet effective framework for converting natural language into first-order logic (NL2FOL). Drawing inspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by efficiently generating logic representations, while the Z3 solver operates as System 2, enabling robust logical inference. To rigorously evaluate the NL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework comprising three custom metrics: conversion score, reasoning score, and Spearman rho score, which collectively capture the quality of logical translation and its downstream impact on reasoning accuracy.
  Empirical results on domain-specific datasets demonstrate that Text-JEPA achieves competitive performance with significantly lower computational overhead compared to larger LLM-based systems. Our findings highlight the potential of structured, interpretable reasoning frameworks for building efficient and explainable QA systems in specialized domains.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AQUA: A Large Language Model for Aquaculture &amp; Fisheries</title>
<link>https://arxiv.org/abs/2507.20520</link>
<guid>https://arxiv.org/abs/2507.20520</guid>
<content:encoded><![CDATA[
arXiv:2507.20520v1 Announce Type: new 
Abstract: Aquaculture plays a vital role in global food security and coastal economies by providing sustainable protein sources. As the industry expands to meet rising demand, it faces growing challenges such as disease outbreaks, inefficient feeding practices, rising labor costs, logistical inefficiencies, and critical hatchery issues, including high mortality rates and poor water quality control. Although artificial intelligence has made significant progress, existing machine learning methods fall short of addressing the domain-specific complexities of aquaculture. To bridge this gap, we introduce AQUA, the first large language model (LLM) tailored for aquaculture, designed to support farmers, researchers, and industry practitioners. Central to this effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic Framework for generating and refining high-quality synthetic data using a combination of expert knowledge, largescale language models, and automated evaluation techniques. Our work lays the foundation for LLM-driven innovations in aquaculture research, advisory systems, and decision-making tools.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers</title>
<link>https://arxiv.org/abs/2507.20527</link>
<guid>https://arxiv.org/abs/2507.20527</guid>
<content:encoded><![CDATA[
arXiv:2507.20527v1 Announce Type: new 
Abstract: The demand for Large Language Models (LLMs) capable of sophisticated mathematical reasoning is growing across industries. However, the development of performant mathematical LLMs is critically bottlenecked by the scarcity of difficult, novel training data. We introduce \textbf{SAND-Math} (Synthetic Augmented Novel and Difficult Mathematics problems and solutions), a pipeline that addresses this by first generating high-quality problems from scratch and then systematically elevating their complexity via a new \textbf{Difficulty Hiking} step. We demonstrate the effectiveness of our approach through two key findings. First, augmenting a strong baseline with SAND-Math data significantly boosts performance, outperforming the next-best synthetic dataset by \textbf{$\uparrow$ 17.85 absolute points} on the AIME25 benchmark. Second, in a dedicated ablation study, we show our Difficulty Hiking process is highly effective: by increasing average problem difficulty from 5.02 to 5.98, this step lifts AIME25 performance from 46.38\% to 49.23\%. The full generation pipeline, final dataset, and a fine-tuned model form a practical and scalable toolkit for building more capable and efficient mathematical reasoning LLMs. SAND-Math dataset is released here: \href{https://huggingface.co/datasets/amd/SAND-MATH}{https://huggingface.co/datasets/amd/SAND-MATH}
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dialogues of Dissent: Thematic and Rhetorical Dimensions of Hate and Counter-Hate Speech in Social Media Conversations</title>
<link>https://arxiv.org/abs/2507.20528</link>
<guid>https://arxiv.org/abs/2507.20528</guid>
<content:encoded><![CDATA[
arXiv:2507.20528v1 Announce Type: new 
Abstract: We introduce a novel multi-labeled scheme for joint annotation of hate and counter-hate speech in social media conversations, categorizing hate and counter-hate messages into thematic and rhetorical dimensions. The thematic categories outline different discursive aspects of each type of speech, while the rhetorical dimension captures how hate and counter messages are communicated, drawing on Aristotle's Logos, Ethos and Pathos. We annotate a sample of 92 conversations, consisting of 720 tweets, and conduct statistical analyses, incorporating public metrics, to explore patterns of interaction between the thematic and rhetorical dimensions within and between hate and counter-hate speech. Our findings provide insights into the spread of hate messages on social media, the strategies used to counter them, and their potential impact on online behavior.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Hallucination Detection via Future Context</title>
<link>https://arxiv.org/abs/2507.20546</link>
<guid>https://arxiv.org/abs/2507.20546</guid>
<content:encoded><![CDATA[
arXiv:2507.20546v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are widely used to generate plausible text on online platforms, without revealing the generation process. As users increasingly encounter such black-box outputs, detecting hallucinations has become a critical challenge. To address this challenge, we focus on developing a hallucination detection framework for black-box generators. Motivated by the observation that hallucinations, once introduced, tend to persist, we sample future contexts. The sampled future contexts provide valuable clues for hallucination detection and can be effectively integrated with various sampling-based methods. We extensively demonstrate performance improvements across multiple methods using our proposed sampling approach.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning</title>
<link>https://arxiv.org/abs/2507.20564</link>
<guid>https://arxiv.org/abs/2507.20564</guid>
<content:encoded><![CDATA[
arXiv:2507.20564v1 Announce Type: new 
Abstract: We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place system in Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image retrieval and captioning. Our zero-shot approach requires no finetuning on the competition's data. For retrieval, we ensemble similarity scores from CLIP, SigLIP, and DINOv2. For captioning, we leverage a carefully engineered prompt to guide the Gemma 3 model, enabling it to link high-level events from the article to the visual content in the image. Our system achieved a final score of 0.42002, securing a top-4 position on the private test set, demonstrating the effectiveness of combining foundation models through ensembling and prompting. Our code is available at https://github.com/ductai05/ZSE-Cap.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Before the Outrage: Challenges and Advances in Predicting Online Antisocial Behavior</title>
<link>https://arxiv.org/abs/2507.20614</link>
<guid>https://arxiv.org/abs/2507.20614</guid>
<content:encoded><![CDATA[
arXiv:2507.20614v1 Announce Type: new 
Abstract: Antisocial behavior (ASB) on social media-including hate speech, harassment, and trolling-poses growing challenges for platform safety and societal wellbeing. While prior work has primarily focused on detecting harmful content after it appears, predictive approaches aim to forecast future harmful behaviors-such as hate speech propagation, conversation derailment, or user recidivism-before they fully unfold. Despite increasing interest, the field remains fragmented, lacking a unified taxonomy or clear synthesis of existing methods. This paper presents a systematic review of over 49 studies on ASB prediction, offering a structured taxonomy of five core task types: early harm detection, harm emergence prediction, harm propagation prediction, behavioral risk prediction, and proactive moderation support. We analyze how these tasks differ by temporal framing, prediction granularity, and operational goals. In addition, we examine trends in modeling techniques-from classical machine learning to pre-trained language models-and assess the influence of dataset characteristics on task feasibility and generalization. Our review highlights methodological challenges, such as dataset scarcity, temporal drift, and limited benchmarks, while outlining emerging research directions including multilingual modeling, cross-platform generalization, and human-in-the-loop systems. By organizing the field around a coherent framework, this survey aims to guide future work toward more robust and socially responsible ASB prediction.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ontology-Enhanced Knowledge Graph Completion using Large Language Models</title>
<link>https://arxiv.org/abs/2507.20643</link>
<guid>https://arxiv.org/abs/2507.20643</guid>
<content:encoded><![CDATA[
arXiv:2507.20643v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have been extensively adopted in Knowledge Graph Completion (KGC), showcasing significant research advancements. However, as black-box models driven by deep neural architectures, current LLM-based KGC methods rely on implicit knowledge representation with parallel propagation of erroneous knowledge, thereby hindering their ability to produce conclusive and decisive reasoning outcomes. We aim to integrate neural-perceptual structural information with ontological knowledge, leveraging the powerful capabilities of LLMs to achieve a deeper understanding of the intrinsic logic of the knowledge. We propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first leverages neural perceptual mechanisms to effectively embed structural information into the textual space, and then uses an automated extraction algorithm to retrieve ontological knowledge from the knowledge graphs (KGs) that needs to be completed, which is further transformed into a textual format comprehensible to LLMs for providing logic guidance. We conducted extensive experiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The experimental results demonstrate that OL-KGC significantly outperforms existing mainstream KGC methods across multiple evaluation metrics, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric-Mean Policy Optimization</title>
<link>https://arxiv.org/abs/2507.20673</link>
<guid>https://arxiv.org/abs/2507.20673</guid>
<content:encoded><![CDATA[
arXiv:2507.20673v1 Announce Type: new 
Abstract: Recent advancements, such as Group Relative Policy Optimization (GRPO), have enhanced the reasoning capabilities of large language models by optimizing the arithmetic mean of token-level rewards. However, GRPO suffers from unstable policy updates when processing tokens with outlier importance-weighted rewards, which manifests as extreme importance sampling ratios during training, i.e., the ratio between the sampling probabilities assigned to a token by the current and old policies. In this work, we propose Geometric-Mean Policy Optimization (GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic mean, GMPO maximizes the geometric mean of token-level rewards, which is inherently less sensitive to outliers and maintains a more stable range of importance sampling ratio. In addition, we provide comprehensive theoretical and experimental analysis to justify the design and stability benefits of GMPO. Beyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark, including AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is available at https://github.com/callsys/GMPO.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Scale Meets Diversity: Evaluating Language Models on Fine-Grained Multilingual Claim Verification</title>
<link>https://arxiv.org/abs/2507.20700</link>
<guid>https://arxiv.org/abs/2507.20700</guid>
<content:encoded><![CDATA[
arXiv:2507.20700v1 Announce Type: new 
Abstract: The rapid spread of multilingual misinformation requires robust automated fact verification systems capable of handling fine-grained veracity assessments across diverse languages. While large language models have shown remarkable capabilities across many NLP tasks, their effectiveness for multilingual claim verification with nuanced classification schemes remains understudied. We conduct a comprehensive evaluation of five state-of-the-art language models on the X-Fact dataset, which spans 25 languages with seven distinct veracity categories. Our experiments compare small language models (encoder-based XLM-R and mT5) with recent decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo) using both prompting and fine-tuning approaches. Surprisingly, we find that XLM-R (270M parameters) substantially outperforms all tested LLMs (7-12B parameters), achieving 57.7% macro-F1 compared to the best LLM performance of 16.9%. This represents a 15.8% improvement over the previous state-of-the-art (41.9%), establishing new performance benchmarks for multilingual fact verification. Our analysis reveals problematic patterns in LLM behavior, including systematic difficulties in leveraging evidence and pronounced biases toward frequent categories in imbalanced data settings. These findings suggest that for fine-grained multilingual fact verification, smaller specialized models may be more effective than general-purpose large models, with important implications for practical deployment of fact-checking systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models</title>
<link>https://arxiv.org/abs/2507.20704</link>
<guid>https://arxiv.org/abs/2507.20704</guid>
<content:encoded><![CDATA[
arXiv:2507.20704v1 Announce Type: new 
Abstract: The increasing integration of Visual Language Models (VLMs) into AI systems necessitates robust model alignment, especially when handling multimodal content that combines text and images. Existing evaluation datasets heavily lean towards text-only prompts, leaving visual vulnerabilities under evaluated. To address this gap, we propose \textbf{Text2VLM}, a novel multi-stage pipeline that adapts text-only datasets into multimodal formats, specifically designed to evaluate the resilience of VLMs against typographic prompt injection attacks. The Text2VLM pipeline identifies harmful content in the original text and converts it into a typographic image, creating a multimodal prompt for VLMs. Also, our evaluation of open-source VLMs highlights their increased susceptibility to prompt injection when visual inputs are introduced, revealing critical weaknesses in the current models' alignment. This is in addition to a significant performance gap compared to closed-source frontier models. We validate Text2VLM through human evaluations, ensuring the alignment of extracted salient concepts; text summarization and output classification align with human expectations. Text2VLM provides a scalable tool for comprehensive safety assessment, contributing to the development of more robust safety mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities, Text2VLM plays a role in advancing the safe deployment of VLMs in diverse, real-world applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Structural Pruning and Recovery Techniques for Compressing Multimodal Large Language Models: An Empirical Study</title>
<link>https://arxiv.org/abs/2507.20749</link>
<guid>https://arxiv.org/abs/2507.20749</guid>
<content:encoded><![CDATA[
arXiv:2507.20749v1 Announce Type: new 
Abstract: While Multimodal Large Language Models (MLLMs) demonstrate impressive capabilities, their substantial computational and memory requirements pose significant barriers to practical deployment. Current parameter reduction techniques primarily involve training MLLMs from Small Language Models (SLMs), but these methods offer limited flexibility and remain computationally intensive. To address this gap, we propose to directly compress existing MLLMs through structural pruning combined with efficient recovery training. Specifically, we investigate two structural pruning paradigms--layerwise and widthwise pruning--applied to the language model backbone of MLLMs, alongside supervised finetuning and knowledge distillation. Additionally, we assess the feasibility of conducting recovery training with only a small fraction of the available data. Our results show that widthwise pruning generally maintains better performance in low-resource scenarios with limited computational resources or insufficient finetuning data. As for the recovery training, finetuning only the multimodal projector is sufficient at small compression levels (< 20%). Furthermore, a combination of supervised finetuning and hidden-state distillation yields optimal recovery across various pruning levels. Notably, effective recovery can be achieved with as little as 5% of the original training data, while retaining over 95% of the original performance. Through empirical study on two representative MLLMs, i.e., LLaVA-v1.5-7B and Bunny-v1.0-3B, this study offers actionable insights for practitioners aiming to compress MLLMs effectively without extensive computation resources or sufficient data.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Self-Taught Faithfulness Evaluators</title>
<link>https://arxiv.org/abs/2507.20752</link>
<guid>https://arxiv.org/abs/2507.20752</guid>
<content:encoded><![CDATA[
arXiv:2507.20752v1 Announce Type: new 
Abstract: The growing use of large language models (LLMs) has increased the need for automatic evaluation systems, particularly to address the challenge of information hallucination. Although existing faithfulness evaluation approaches have shown promise, they are predominantly English-focused and often require expensive human-labeled training data for fine-tuning specialized models. As LLMs see increased adoption in multilingual contexts, there is a need for accurate faithfulness evaluators that can operate across languages without extensive labeled data. This paper presents Self-Taught Evaluators for Multilingual Faithfulness, a framework that learns exclusively from synthetic multilingual summarization data while leveraging cross-lingual transfer learning. Through experiments comparing language-specific and mixed-language fine-tuning approaches, we demonstrate a consistent relationship between an LLM's general language capabilities and its performance in language-specific evaluation tasks. Our framework shows improvements over existing baselines, including state-of-the-art English evaluators and machine translation-based approaches.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey</title>
<link>https://arxiv.org/abs/2507.20783</link>
<guid>https://arxiv.org/abs/2507.20783</guid>
<content:encoded><![CDATA[
arXiv:2507.20783v1 Announce Type: new 
Abstract: Text embeddings have attracted growing interest due to their effectiveness across a wide range of natural language processing (NLP) tasks, such as retrieval, classification, clustering, bitext mining, and summarization. With the emergence of pretrained language models (PLMs), general-purpose text embeddings (GPTE) have gained significant traction for their ability to produce rich, transferable representations. The general architecture of GPTE typically leverages PLMs to derive dense text representations, which are then optimized through contrastive learning on large-scale pairwise datasets. In this survey, we provide a comprehensive overview of GPTE in the era of PLMs, focusing on the roles PLMs play in driving its development. We first examine the fundamental architecture and describe the basic roles of PLMs in GPTE, i.e., embedding extraction, expressivity enhancement, training strategies, learning objectives, and data construction. Then, we describe advanced roles enabled by PLMs, such as multilingual support, multimodal integration, code understanding, and scenario-specific adaptation. Finally, we highlight potential future research directions that move beyond traditional improvement goals, including ranking integration, safety considerations, bias mitigation, structural information incorporation, and the cognitive extension of embeddings. This survey aims to serve as a valuable reference for both newcomers and established researchers seeking to understand the current state and future potential of GPTE.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Thematic Review of Prevention of Future Deaths Reports: Replicating the ONS Child Suicide Study using Large Language Models</title>
<link>https://arxiv.org/abs/2507.20786</link>
<guid>https://arxiv.org/abs/2507.20786</guid>
<content:encoded><![CDATA[
arXiv:2507.20786v1 Announce Type: new 
Abstract: Prevention of Future Deaths (PFD) reports, issued by coroners in England and Wales, flag systemic hazards that may lead to further loss of life. Analysis of these reports has previously been constrained by the manual effort required to identify and code relevant cases. In 2025, the Office for National Statistics (ONS) published a national thematic review of child-suicide PFD reports ($\leq$ 18 years), identifying 37 cases from January 2015 to November 2023 - a process based entirely on manual curation and coding. We evaluated whether a fully automated, open source "text-to-table" language-model pipeline (PFD Toolkit) could reproduce the ONS's identification and thematic analysis of child-suicide PFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD reports published from July 2013 to November 2023 were processed via PFD Toolkit's large language model pipelines. Automated screening identified cases where the coroner attributed death to suicide in individuals aged 18 or younger, and eligible reports were coded for recipient category and 23 concern sub-themes, replicating the ONS coding frame. PFD Toolkit identified 72 child-suicide PFD reports - almost twice the ONS count. Three blinded clinicians adjudicated a stratified sample of 144 reports to validate the child-suicide screening. Against the post-consensus clinical annotations, the LLM-based workflow showed substantial to almost-perfect agreement (Cohen's $\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script runtime was 8m 16s, transforming a process that previously took months into one that can be completed in minutes. This demonstrates that automated LLM analysis can reliably and efficiently replicate manual thematic reviews of coronial data, enabling scalable, reproducible, and timely insights for public health and safety. The PFD Toolkit is openly available for future research.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Inter-User Difference Modeling for LLM Personalization</title>
<link>https://arxiv.org/abs/2507.20849</link>
<guid>https://arxiv.org/abs/2507.20849</guid>
<content:encoded><![CDATA[
arXiv:2507.20849v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly integrated into users' daily lives, leading to a growing demand for personalized outputs. Previous work focuses on leveraging a user's own history, overlooking inter-user differences that are crucial for effective personalization. While recent work has attempted to model such differences, the reliance on language-based prompts often hampers the effective extraction of meaningful distinctions. To address these issues, we propose Difference-aware Embedding-based Personalization (DEP), a framework that models inter-user differences in the latent space instead of relying on language prompts. DEP constructs soft prompts by contrasting a user's embedding with those of peers who engaged with similar content, highlighting relative behavioral signals. A sparse autoencoder then filters and compresses both user-specific and difference-aware embeddings, preserving only task-relevant features before injecting them into a frozen LLM. Experiments on personalized review generation show that DEP consistently outperforms baseline methods across multiple metrics. Our code is available at https://github.com/SnowCharmQ/DEP.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A survey of diversity quantification in natural language processing: The why, what, where and how</title>
<link>https://arxiv.org/abs/2507.20858</link>
<guid>https://arxiv.org/abs/2507.20858</guid>
<content:encoded><![CDATA[
arXiv:2507.20858v1 Announce Type: new 
Abstract: The concept of diversity has received increased consideration in Natural Language Processing (NLP) in recent years. This is due to various motivations like promoting and inclusion, approximating human linguistic behavior, and increasing systems' performance. Diversity has however often been addressed in an ad hoc manner in NLP, and with few explicit links to other domains where this notion is better theorized. We survey articles in the ACL Anthology from the past 6 years, with "diversity" or "diverse" in their title. We find a wide range of settings in which diversity is quantified, often highly specialized and using inconsistent terminology. We put forward a unified taxonomy of why, what on, where, and how diversity is measured in NLP. Diversity measures are cast upon a unified framework from ecology and economy (Stirling, 2007) with 3 dimensions of diversity: variety, balance and disparity. We discuss the trends which emerge due to this systematized approach. We believe that this study paves the way towards a better formalization of diversity in NLP, which should bring a better understanding of this notion and a better comparability between various approaches.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Open-Source Large Language Models for Clinical Information Extraction in Resource-Constrained Settings</title>
<link>https://arxiv.org/abs/2507.20859</link>
<guid>https://arxiv.org/abs/2507.20859</guid>
<content:encoded><![CDATA[
arXiv:2507.20859v1 Announce Type: new 
Abstract: Medical reports contain rich clinical information but are often unstructured and written in domain-specific language, posing challenges for information extraction. While proprietary large language models (LLMs) have shown promise in clinical natural language processing, their lack of transparency and data privacy concerns limit their utility in healthcare. This study therefore evaluates nine open-source generative LLMs on the DRAGON benchmark, which includes 28 clinical information extraction tasks in Dutch. We developed \texttt{llm\_extractinator}, a publicly available framework for information extraction using open-source generative LLMs, and used it to assess model performance in a zero-shot setting. Several 14 billion parameter models, Phi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved competitive results, while the bigger Llama-3.3-70B model achieved slightly higher performance at greater computational cost. Translation to English prior to inference consistently degraded performance, highlighting the need of native-language processing. These findings demonstrate that open-source LLMs, when used with our framework, offer effective, scalable, and privacy-conscious solutions for clinical information extraction in low-resource settings.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context Learning</title>
<link>https://arxiv.org/abs/2507.20906</link>
<guid>https://arxiv.org/abs/2507.20906</guid>
<content:encoded><![CDATA[
arXiv:2507.20906v1 Announce Type: new 
Abstract: In-Context Learning (ICL) enables Large Language Models (LLMs) to perform tasks by conditioning on input-output examples in the prompt, without requiring any update in model parameters. While widely adopted, it remains unclear whether prompting with multiple examples is the most effective and efficient way to convey task information. In this work, we propose Soft Injection of task embeddings. The task embeddings are constructed only once using few-shot ICL prompts and repeatedly used during inference. Soft injection is performed by softly mixing task embeddings with attention head activations using pre-optimized mixing parameters, referred to as soft head-selection parameters. This method not only allows a desired task to be performed without in-prompt demonstrations but also significantly outperforms existing ICL approaches while reducing memory usage and compute cost at inference time. An extensive evaluation is performed across 57 tasks and 12 LLMs, spanning four model families of sizes from 4B to 70B. Averaged across 57 tasks, our method outperforms 10-shot ICL by 10.1%-13.9% across 12 LLMs. Additional analyses show that our method also serves as an insightful tool for analyzing task-relevant roles of attention heads, revealing that task-relevant head positions selected by our method transfer across similar tasks but not across dissimilar ones -- underscoring the task-specific nature of head functionality. Our soft injection method opens a new paradigm for reducing prompt length and improving task performance by shifting task conditioning from the prompt space to the activation space.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2507.20917</link>
<guid>https://arxiv.org/abs/2507.20917</guid>
<content:encoded><![CDATA[
arXiv:2507.20917v1 Announce Type: new 
Abstract: This work introduces MediQAl, a French medical question answering dataset designed to evaluate the capabilities of language models in factual medical recall and reasoning over real-world clinical scenarios. MediQAl contains 32,603 questions sourced from French medical examinations across 41 medical subjects. The dataset includes three tasks: (i) Multiple-Choice Question with Unique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii) Open-Ended Question with Short-Answer. Each question is labeled as Understanding or Reasoning, enabling a detailed analysis of models' cognitive capabilities. We validate the MediQAl dataset through extensive evaluation with 14 large language models, including recent reasoning-augmented models, and observe a significant performance gap between factual recall and reasoning tasks. Our evaluation provides a comprehensive benchmark for assessing language models' performance on French medical question answering, addressing a crucial gap in multilingual resources for the medical domain.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2507.20924</link>
<guid>https://arxiv.org/abs/2507.20924</guid>
<content:encoded><![CDATA[
arXiv:2507.20924v1 Announce Type: new 
Abstract: Sexism has become widespread on social media and in online conversation. To help address this issue, the fifth Sexism Identification in Social Networks (EXIST) challenge is initiated at CLEF 2025. Among this year's international benchmarks, we concentrate on solving the first task aiming to identify and classify sexism in social media textual posts. In this paper, we describe our solutions and report results for three subtasks: Subtask 1.1 - Sexism Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask 1.3 - Sexism Categorization in Tweets. We implement three models to address each subtask which constitute three individual runs: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to encode input texts into a human-interpretable representation of adjectives, then used to train a lightweight classifier for downstream tasks. SCBMT extends SCBM by fusing adjective-based representation with contextual embeddings from transformers to balance interpretability and classification performance. Beyond competitive results, these two models offer fine-grained explanations at both instance (local) and class (global) levels. We also investigate how additional metadata, e.g., annotators' demographic profiles, can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data augmented with prior datasets, ranks 6th for English and Spanish and 4th for English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and Spanish and 6th for Spanish.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models</title>
<link>https://arxiv.org/abs/2507.20930</link>
<guid>https://arxiv.org/abs/2507.20930</guid>
<content:encoded><![CDATA[
arXiv:2507.20930v1 Announce Type: new 
Abstract: Hallucinations in large language models pose a critical challenge for applications requiring factual reliability, particularly in high-stakes domains such as finance. This work presents an effective approach for detecting and editing factually incorrect content in model-generated responses based on the provided context. Given a user-defined domain-specific error taxonomy, we construct a synthetic dataset by inserting tagged errors into financial question-answering corpora and then fine-tune four language models, Phi-4, Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8% improvement in binary F1 score and a 30% gain in overall detection performance compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having only 4 billion parameters, maintains competitive performance with just a 2% drop in binary detection and a 0.1% decline in overall detection compared to OpenAI-o3. Our work provides a practical solution for detecting and editing factual inconsistencies in financial text generation while introducing a generalizable framework that can enhance the trustworthiness and alignment of large language models across diverse applications beyond finance. Our code and data are available at https://github.com/pegasi-ai/fine-grained-editting.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models</title>
<link>https://arxiv.org/abs/2507.20956</link>
<guid>https://arxiv.org/abs/2507.20956</guid>
<content:encoded><![CDATA[
arXiv:2507.20956v1 Announce Type: new 
Abstract: Instruction-tuning large language models (LLMs) reduces the diversity of their outputs, which has implications for many tasks, particularly for creative tasks. This paper investigates the ``diversity gap'' for a writing prompt narrative generation task. This gap emerges as measured by current diversity metrics for various open-weight and open-source LLMs. The results show significant decreases in diversity due to instruction-tuning. We explore the diversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to further understand how output diversity is affected. The results indicate that DPO has the most substantial impact on diversity. Motivated by these findings, we present a new decoding strategy, conformative decoding, which guides an instruct model using its more diverse base model to reintroduce output diversity. We show that conformative decoding typically increases diversity and even maintains or improves quality.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization in Fine-Tuned Large Language Models</title>
<link>https://arxiv.org/abs/2507.21009</link>
<guid>https://arxiv.org/abs/2507.21009</guid>
<content:encoded><![CDATA[
arXiv:2507.21009v1 Announce Type: new 
Abstract: This study investigates the mechanisms and factors influencing memorization in fine-tuned large language models (LLMs), with a focus on the medical domain due to its privacy-sensitive nature. We examine how different aspects of the fine-tuning process affect a model's propensity to memorize training data, using the PHEE dataset of pharmacovigilance events.
  Our research employs two main approaches: a membership inference attack to detect memorized data, and a generation task with prompted prefixes to assess verbatim reproduction. We analyze the impact of adapting different weight matrices in the transformer architecture, the relationship between perplexity and memorization, and the effect of increasing the rank in low-rank adaptation (LoRA) fine-tuning.
  Key findings include: (1) Value and Output matrices contribute more significantly to memorization compared to Query and Key matrices; (2) Lower perplexity in the fine-tuned model correlates with increased memorization; (3) Higher LoRA ranks lead to increased memorization, but with diminishing returns at higher ranks.
  These results provide insights into the trade-offs between model performance and privacy risks in fine-tuned LLMs. Our findings have implications for developing more effective and responsible strategies for adapting large language models while managing data privacy concerns.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation</title>
<link>https://arxiv.org/abs/2507.21028</link>
<guid>https://arxiv.org/abs/2507.21028</guid>
<content:encoded><![CDATA[
arXiv:2507.21028v1 Announce Type: new 
Abstract: Nearly all human work is collaborative; thus, the evaluation of real-world NLP applications often requires multiple dimensions that align with diverse human perspectives. As real human evaluator resources are often scarce and costly, the emerging "LLM-as-a-judge" paradigm sheds light on a promising approach to leverage LLM agents to believably simulate human evaluators. Yet, to date, existing LLM-as-a-judge approaches face two limitations: persona descriptions of agents are often arbitrarily designed, and the frameworks are not generalizable to other tasks. To address these challenges, we propose MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically construct multiple evaluator personas with distinct dimensions from relevant text documents (e.g., research papers), instantiate LLM agents with the personas, and engage in-group debates with multi-agents to Generate multi-dimensional feedback. Our evaluation experiments in both the educational and medical domains demonstrate that MAJ-EVAL can generate evaluation results that better align with human experts' ratings compared with conventional automated evaluation metrics and existing LLM-as-a-judge methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does AI and Human Advice Mitigate Punishment for Selfish Behavior? An Experiment on AI ethics From a Psychological Perspective</title>
<link>https://arxiv.org/abs/2507.19487</link>
<guid>https://arxiv.org/abs/2507.19487</guid>
<content:encoded><![CDATA[
arXiv:2507.19487v1 Announce Type: cross 
Abstract: People increasingly rely on AI-advice when making decisions. At times, such advice can promote selfish behavior. When individuals abide by selfishness-promoting AI advice, how are they perceived and punished? To study this question, we build on theories from social psychology and combine machine-behavior and behavioral economic approaches. In a pre-registered, financially-incentivized experiment, evaluators could punish real decision-makers who (i) received AI, human, or no advice. The advice (ii) encouraged selfish or prosocial behavior, and decision-makers (iii) behaved selfishly or, in a control condition, behaved prosocially. Evaluators further assigned responsibility to decision-makers and their advisors. Results revealed that (i) prosocial behavior was punished very little, whereas selfish behavior was punished much more. Focusing on selfish behavior, (ii) compared to receiving no advice, selfish behavior was penalized more harshly after prosocial advice and more leniently after selfish advice. Lastly, (iii) whereas selfish decision-makers were seen as more responsible when they followed AI compared to human advice, punishment between the two advice sources did not vary. Overall, behavior and advice content shape punishment, whereas the advice source does not.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated Learning Settings</title>
<link>https://arxiv.org/abs/2507.19534</link>
<guid>https://arxiv.org/abs/2507.19534</guid>
<content:encoded><![CDATA[
arXiv:2507.19534v1 Announce Type: cross 
Abstract: Pre-trained Language Models (PLMs) have demonstrated impressive performance in various NLP tasks. However, traditional fine-tuning methods for leveraging PLMs for downstream tasks entail significant computational overhead. Prompt-tuning has emerged as an efficient alternative that involves prepending a limited number of parameters to the input sequence and only updating them while the PLM's parameters are frozen. However, this technique's prompts remain fixed for all inputs, reducing the model's flexibility. The Federated Learning (FL) technique has gained attention in recent years to address the growing concerns around data privacy. However, challenges such as communication and computation limitations of clients still need to be addressed. To mitigate these challenges, this paper introduces the Federated Dynamic Prompt Generator (FedDPG), which incorporates a dynamic prompt generator network to generate context-aware prompts based on the given input, ensuring flexibility and adaptability while prioritising data privacy in federated learning settings. Our experiments on three NLP benchmark datasets showcase that FedDPG outperforms the state-of-the-art parameter-efficient fine-tuning methods in terms of global model performance, and has significantly reduced the calculation time and the number of parameters to be sent through the FL network.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks</title>
<link>https://arxiv.org/abs/2507.19684</link>
<guid>https://arxiv.org/abs/2507.19684</guid>
<content:encoded><![CDATA[
arXiv:2507.19684v1 Announce Type: cross 
Abstract: Imagine a humanoid that can safely and creatively dance with a human, adapting to its partner's proficiency, using haptic signaling as a primary form of communication. While today's AI systems excel at text or voice-based interaction with large language models, human communication extends far beyond text-it includes embodied movement, timing, and physical coordination. Modeling coupled interaction between two agents poses a formidable challenge: it is continuous, bidirectionally reactive, and shaped by individual variation. We present CoMPAS3D, the largest and most diverse motion capture dataset of improvised salsa dancing, designed as a challenging testbed for interactive, expressive humanoid AI. The dataset includes 3 hours of leader-follower salsa dances performed by 18 dancers spanning beginner, intermediate, and professional skill levels. For the first time, we provide fine-grained salsa expert annotations, covering over 2,800 move segments, including move types, combinations, execution errors and stylistic elements. We draw analogies between partner dance communication and natural language, evaluating CoMPAS3D on two benchmark tasks for synthetic humans that parallel key problems in spoken language and dialogue processing: leader or follower generation with proficiency levels (speaker or listener synthesis), and duet (conversation) generation. Towards a long-term goal of partner dance with humans, we release the dataset, annotations, and code, along with a multitask SalsaAgent model capable of performing all benchmark tasks, alongside additional baselines to encourage research in socially interactive embodied AI and creative, expressive humanoid motion generation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoSign: Direct Pose-to-Text Translation for Continuous Sign Language Recognition</title>
<link>https://arxiv.org/abs/2507.19840</link>
<guid>https://arxiv.org/abs/2507.19840</guid>
<content:encoded><![CDATA[
arXiv:2507.19840v1 Announce Type: cross 
Abstract: Continuously recognizing sign gestures and converting them to glosses plays a key role in bridging the gap between the hearing and hearing-impaired communities. This involves recognizing and interpreting the hands, face, and body gestures of the signer, which pose a challenge as it involves a combination of all these features. Continuous Sign Language Recognition (CSLR) methods rely on multi-stage pipelines that first extract visual features, then align variable-length sequences with target glosses using CTC or HMM-based approaches. However, these alignment-based methods suffer from error propagation across stages, overfitting, and struggle with vocabulary scalability due to the intermediate gloss representation bottleneck. To address these limitations, we propose AutoSign, an autoregressive decoder-only transformer that directly translates pose sequences to natural language text, bypassing traditional alignment mechanisms entirely. The use of this decoder-only approach allows the model to directly map between the features and the glosses without the need for CTC loss while also directly learning the textual dependencies in the glosses. Our approach incorporates a temporal compression module using 1D CNNs to efficiently process pose sequences, followed by AraGPT2, a pre-trained Arabic decoder, to generate text (glosses). Through comprehensive ablation studies, we demonstrate that hand and body gestures provide the most discriminative features for signer-independent CSLR. By eliminating the multi-stage pipeline, AutoSign achieves substantial improvements on the Isharah-1000 dataset, achieving an improvement of up to 6.1\% in WER score compared to the best existing method.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Reinforced Policy Optimization</title>
<link>https://arxiv.org/abs/2507.19849</link>
<guid>https://arxiv.org/abs/2507.19849</guid>
<content:encoded><![CDATA[
arXiv:2507.19849v1 Announce Type: cross 
Abstract: Large-scale reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for single-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools to assist in task-solving processes. However, current RL algorithms inadequately balance the models' intrinsic long-horizon reasoning capabilities and their proficiency in multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training multi-turn LLM-based agents. Through preliminary experiments, we observe that LLMs tend to exhibit highly uncertain behavior, characterized by an increase in the entropy distribution of generated tokens, immediately following interactions with external tools. Motivated by this observation, ARPO incorporates an entropy-based adaptive rollout mechanism, dynamically balancing global trajectory sampling and step-level sampling, thereby promoting exploration at steps with high uncertainty after tool usage. By integrating an advantage attribution estimation, ARPO enables LLMs to internalize advantage differences in stepwise tool-use interactions. Our experiments across 13 challenging benchmarks in computational reasoning, knowledge reasoning, and deep search domains demonstrate ARPO's superiority over trajectory-level RL algorithms. Remarkably, ARPO achieves improved performance using only half of the tool-use budget required by existing methods, offering a scalable solution for aligning LLM-based agents with real-time dynamic environments. Our code and datasets are released at https://github.com/dongguanting/ARPO
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Fine-tuning Large Language Models on Automated Program Repair</title>
<link>https://arxiv.org/abs/2507.19909</link>
<guid>https://arxiv.org/abs/2507.19909</guid>
<content:encoded><![CDATA[
arXiv:2507.19909v1 Announce Type: cross 
Abstract: Automated Program Repair (APR) uses various tools and techniques to help developers achieve functional and error-free code faster. In recent years, Large Language Models (LLMs) have gained popularity as components in APR tool chains because of their performance and flexibility. However, training such models requires a significant amount of resources. Fine-tuning techniques have been developed to adapt pre-trained LLMs to specific tasks, such as APR, and enhance their performance at far lower computational costs than training from scratch. In this study, we empirically investigate the impact of various fine-tuning techniques on the performance of LLMs used for APR. Our experiments provide insights into the performance of a selection of state-of-the-art LLMs pre-trained on code. The evaluation is done on three popular APR benchmarks (i.e., QuixBugs, Defects4J and HumanEval-Java) and considers six different LLMs with varying parameter sizes (resp. CodeGen, CodeT5, StarCoder, DeepSeekCoder, Bloom, and CodeLlama-2). We consider three training regimens: no fine-tuning, full fine-tuning, and parameter-efficient fine-tuning (PEFT) using LoRA and IA3. We observe that full fine-tuning techniques decrease the benchmarking performance of various models due to different data distributions and overfitting. By using parameter-efficient fine-tuning methods, we restrict models in the amount of trainable parameters and achieve better results.
  Keywords: large language models, automated program repair, parameter-efficient fine-tuning, AI4Code, AI4SE, ML4SE.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations</title>
<link>https://arxiv.org/abs/2507.19947</link>
<guid>https://arxiv.org/abs/2507.19947</guid>
<content:encoded><![CDATA[
arXiv:2507.19947v1 Announce Type: cross 
Abstract: Fusing information from human observations can help robots overcome sensing limitations in collaborative tasks. However, an uncertainty-aware fusion framework requires a grounded likelihood representing the uncertainty of human inputs. This paper presents a Feature Pyramid Likelihood Grounding Network (FP-LGN) that grounds spatial language by learning relevant map image features and their relationships with spatial relation semantics. The model is trained as a probability estimator to capture aleatoric uncertainty in human language using three-stage curriculum learning. Results showed that FP-LGN matched expert-designed rules in mean Negative Log-Likelihood (NLL) and demonstrated greater robustness with lower standard deviation. Collaborative sensing results demonstrated that the grounded likelihood successfully enabled uncertainty-aware fusion of heterogeneous human language observations and robot sensor measurements, achieving significant improvements in human-robot collaborative task performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization</title>
<link>https://arxiv.org/abs/2507.19973</link>
<guid>https://arxiv.org/abs/2507.19973</guid>
<content:encoded><![CDATA[
arXiv:2507.19973v1 Announce Type: cross 
Abstract: Background: Manual extraction of pancreatic cystic lesion (PCL) features from radiology reports is labor-intensive, limiting large-scale studies needed to advance PCL research. Purpose: To develop and evaluate large language models (LLMs) that automatically extract PCL features from MRI/CT reports and assign risk categories based on guidelines. Materials and Methods: We curated a training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134 patients that described PCLs. Labels were generated by GPT-4o using chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated CoT data. Features were mapped to risk categories per institutional guideline based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out human-annotated reports. Model outputs for 100 cases were independently reviewed by three radiologists. Feature extraction was evaluated using exact match accuracy, risk categorization with macro-averaged F1 score, and radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79% to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved (LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no statistically significant differences. Radiologist inter-reader agreement was high (Fleiss' Kappa = 0.888) and showed no statistically significant difference with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT (Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT supervision enable accurate, interpretable, and efficient phenotyping for large-scale PCL research, achieving performance comparable to GPT-4o.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Performance of Sequential Recommendation Systems with an Extended Large Language Model</title>
<link>https://arxiv.org/abs/2507.19990</link>
<guid>https://arxiv.org/abs/2507.19990</guid>
<content:encoded><![CDATA[
arXiv:2507.19990v1 Announce Type: cross 
Abstract: Recently, competition in the field of artificial intelligence (AI) has intensified among major technological companies, resulting in the continuous release of new large-language models (LLMs) that exhibit improved language understanding and context-based reasoning capabilities. It is expected that these advances will enable more efficient personalized recommendations in LLM-based recommendation systems through improved quality of training data and architectural design. However, many studies have not considered these recent developments. In this study, it was proposed to improve LLM-based recommendation systems by replacing Llama2 with Llama3 in the LlamaRec framework. To ensure a fair comparison, random seed values were set and identical input data was provided during preprocessing and training. The experimental results show average performance improvements of 38.65\%, 8.69\%, and 8.19\% for the ML-100K, Beauty, and Games datasets, respectively, thus confirming the practicality of this method. Notably, the significant improvements achieved by model replacement indicate that the recommendation quality can be improved cost-effectively without the need to make structural changes to the system. Based on these results, it is our contention that the proposed approach is a viable solution for improving the performance of current recommendation systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Carbon Cost of Conversation, Sustainability in the Age of Language Models</title>
<link>https://arxiv.org/abs/2507.20018</link>
<guid>https://arxiv.org/abs/2507.20018</guid>
<content:encoded><![CDATA[
arXiv:2507.20018v1 Announce Type: cross 
Abstract: Large language models (LLMs) like GPT-3 and BERT have revolutionized natural language processing (NLP), yet their environmental costs remain dangerously overlooked. This article critiques the sustainability of LLMs, quantifying their carbon footprint, water usage, and contribution to e-waste through case studies of models such as GPT-4 and energy-efficient alternatives like Mistral 7B. Training a single LLM can emit carbon dioxide equivalent to hundreds of cars driven annually, while data centre cooling exacerbates water scarcity in vulnerable regions. Systemic challenges corporate greenwashing, redundant model development, and regulatory voids perpetuate harm, disproportionately burdening marginalized communities in the Global South. However, pathways exist for sustainable NLP: technical innovations (e.g., model pruning, quantum computing), policy reforms (carbon taxes, mandatory emissions reporting), and cultural shifts prioritizing necessity over novelty. By analysing industry leaders (Google, Microsoft) and laggards (Amazon), this work underscores the urgency of ethical accountability and global cooperation. Without immediate action, AIs ecological toll risks outpacing its societal benefits. The article concludes with a call to align technological progress with planetary boundaries, advocating for equitable, transparent, and regenerative AI systems that prioritize both human and environmental well-being.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$K^4$: Online Log Anomaly Detection Via Unsupervised Typicality Learning</title>
<link>https://arxiv.org/abs/2507.20051</link>
<guid>https://arxiv.org/abs/2507.20051</guid>
<content:encoded><![CDATA[
arXiv:2507.20051v1 Announce Type: cross 
Abstract: Existing Log Anomaly Detection (LogAD) methods are often slow, dependent on error-prone parsing, and use unrealistic evaluation protocols. We introduce $K^4$, an unsupervised and parser-independent framework for high-performance online detection. $K^4$ transforms arbitrary log embeddings into compact four-dimensional descriptors (Precision, Recall, Density, Coverage) using efficient k-nearest neighbor (k-NN) statistics. These descriptors enable lightweight detectors to accurately score anomalies without retraining. Using a more realistic online evaluation protocol, $K^4$ sets a new state-of-the-art (AUROC: 0.995-0.999), outperforming baselines by large margins while being orders of magnitude faster, with training under 4 seconds and inference as low as 4 $\mu$s.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training</title>
<link>https://arxiv.org/abs/2507.20067</link>
<guid>https://arxiv.org/abs/2507.20067</guid>
<content:encoded><![CDATA[
arXiv:2507.20067v1 Announce Type: cross 
Abstract: Inference-time alignment enables large language models (LLMs) to generate outputs aligned with end-user preferences without further training. Recent post-training methods achieve this by using small guidance models to modify token generation during inference. These methods typically optimize a reward function KL-regularized by the original LLM taken as the reference policy. A critical limitation, however, is their dependence on a pre-trained reward model, which requires fitting to human preference feedback--a potentially unstable process. In contrast, we introduce PITA, a novel framework that integrates preference feedback directly into the LLM's token generation, eliminating the need for a reward model. PITA learns a small preference-based guidance policy to modify token probabilities at inference time without LLM fine-tuning, reducing computational cost and bypassing the pre-trained reward model dependency. The problem is framed as identifying an underlying preference distribution, solved through stochastic search and iterative refinement of the preference-based guidance model. We evaluate PITA across diverse tasks, including mathematical reasoning and sentiment classification, demonstrating its effectiveness in aligning LLM outputs with user preferences.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Devil is in the EOS: Sequence Training for Detailed Image Captioning</title>
<link>https://arxiv.org/abs/2507.20077</link>
<guid>https://arxiv.org/abs/2507.20077</guid>
<content:encoded><![CDATA[
arXiv:2507.20077v1 Announce Type: cross 
Abstract: Despite significant advances in vision-language models (VLMs), image captioning often suffers from a lack of detail, with base models producing short, generic captions. This limitation persists even though VLMs are equipped with strong vision and language backbones. While supervised data and complex reward functions have been proposed to improve detailed image captioning, we identify a simpler underlying issue: a bias towards the end-of-sequence (EOS) token, which is introduced during cross-entropy training. We propose an unsupervised method to debias the model's tendency to predict the EOS token prematurely. By reducing this bias, we encourage the generation of longer, more detailed captions without the need for intricate reward functions or supervision. Our approach is straightforward, effective, and easily applicable to any pretrained model. We demonstrate its effectiveness through experiments with three VLMs and on three detailed captioning benchmarks. Our results show a substantial increase in caption length and relevant details, albeit with an expected increase in the rate of hallucinations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EcoTransformer: Attention without Multiplication</title>
<link>https://arxiv.org/abs/2507.20096</link>
<guid>https://arxiv.org/abs/2507.20096</guid>
<content:encoded><![CDATA[
arXiv:2507.20096v1 Announce Type: cross 
Abstract: The Transformer, with its scaled dot-product attention mechanism, has become a foundational architecture in modern AI. However, this mechanism is computationally intensive and incurs substantial energy costs. We propose a new Transformer architecture EcoTransformer, in which the output context vector is constructed as the convolution of the values using a Laplacian kernel, where the distances are measured by the L1 metric between the queries and keys. Compared to dot-product based attention, the new attention score calculation is free of matrix multiplication. It performs on par with, or even surpasses, scaled dot-product attention in NLP, bioinformatics, and vision tasks, while consuming significantly less energy.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models</title>
<link>https://arxiv.org/abs/2507.20150</link>
<guid>https://arxiv.org/abs/2507.20150</guid>
<content:encoded><![CDATA[
arXiv:2507.20150v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) plays a crucial role in shaping the behavior of large language and reasoning models (LLMs/LRMs). However, it often produces brittle and unstable policies, leading to critical failures such as spurious reasoning, deceptive alignment, and instruction disobedience that undermine the trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified theoretical explanation and are typically addressed using ad-hoc heuristics. This paper presents a rigorous mathematical framework for analyzing the stability of the mapping from a reward function to the optimal policy. We show that policy brittleness often stems from non-unique optimal actions, a common occurrence when multiple valid traces exist in a reasoning task. This theoretical lens provides a unified explanation for a range of seemingly disparate failures, reframing them as rational outcomes of optimizing rewards that may be incomplete or noisy, especially in the presence of action degeneracy. We extend this analysis from the fundamental single-reward setting to the more realistic multi-reward RL across diverse domains, showing how stability is governed by an "effective reward" aggregation mechanism. We also prove that entropy regularization restores policy stability at the cost of increased stochasticity. Our framework provides a unified explanation for recent empirical findings on deceptive reasoning, instruction-following trade-offs, and RLHF-induced sophistry, and is further validated through perturbation experiments in multi-reward RL. This work advances policy-stability analysis from empirical heuristics towards a principled theory, offering essential insights for designing safer and more trustworthy AI systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration</title>
<link>https://arxiv.org/abs/2507.20280</link>
<guid>https://arxiv.org/abs/2507.20280</guid>
<content:encoded><![CDATA[
arXiv:2507.20280v1 Announce Type: cross 
Abstract: Scientific research increasingly relies on specialized computational tools, yet effectively utilizing these tools demands substantial domain expertise. While Large Language Models (LLMs) show promise in tool automation, they struggle to seamlessly integrate and orchestrate multiple tools for complex scientific workflows. Here, we present SciToolAgent, an LLM-powered agent that automates hundreds of scientific tools across biology, chemistry, and materials science. At its core, SciToolAgent leverages a scientific tool knowledge graph that enables intelligent tool selection and execution through graph-based retrieval-augmented generation. The agent also incorporates a comprehensive safety-checking module to ensure responsible and ethical tool usage. Extensive evaluations on a curated benchmark demonstrate that SciToolAgent significantly outperforms existing approaches. Case studies in protein engineering, chemical reactivity prediction, chemical synthesis, and metal-organic framework screening further demonstrate SciToolAgent's capability to automate complex scientific workflows, making advanced research tools accessible to both experts and non-experts.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MountainLion: A Multi-Modal LLM-Based Agent System for Interpretable and Adaptive Financial Trading</title>
<link>https://arxiv.org/abs/2507.20474</link>
<guid>https://arxiv.org/abs/2507.20474</guid>
<content:encoded><![CDATA[
arXiv:2507.20474v1 Announce Type: cross 
Abstract: Cryptocurrency trading is a challenging task requiring the integration of heterogeneous data from multiple modalities. Traditional deep learning and reinforcement learning approaches typically demand large training datasets and encode diverse inputs into numerical representations, often at the cost of interpretability. Recent progress in large language model (LLM)-based agents has demonstrated the capacity to process multi-modal data and support complex investment decision-making. Building on these advances, we present \textbf{MountainLion}, a multi-modal, multi-agent system for financial trading that coordinates specialized LLM-based agents to interpret financial data and generate investment strategies. MountainLion processes textual news, candlestick charts, and trading signal charts to produce high-quality financial reports, while also enabling modification of reports and investment recommendations through data-driven user interaction and question answering. A central reflection module analyzes historical trading signals and outcomes to continuously refine decision processes, and the system is capable of real-time report analysis, summarization, and dynamic adjustment of investment strategies. Empirical results confirm that MountainLion systematically enriches technical price triggers with contextual macroeconomic and capital flow signals, providing a more interpretable, robust, and actionable investment framework that improves returns and strengthens investor confidence.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customize Multi-modal RAI Guardrails with Precedent-based predictions</title>
<link>https://arxiv.org/abs/2507.20503</link>
<guid>https://arxiv.org/abs/2507.20503</guid>
<content:encoded><![CDATA[
arXiv:2507.20503v1 Announce Type: cross 
Abstract: A multi-modal guardrail must effectively filter image content based on user-defined policies, identifying material that may be hateful, reinforce harmful stereotypes, contain explicit material, or spread misinformation. Deploying such guardrails in real-world applications, however, poses significant challenges. Users often require varied and highly customizable policies and typically cannot provide abundant examples for each custom policy. Consequently, an ideal guardrail should be scalable to the multiple policies and adaptable to evolving user standards with minimal retraining. Existing fine-tuning methods typically condition predictions on pre-defined policies, restricting their generalizability to new policies or necessitating extensive retraining to adapt. Conversely, training-free methods struggle with limited context lengths, making it difficult to incorporate all the policies comprehensively. To overcome these limitations, we propose to condition model's judgment on "precedents", which are the reasoning processes of prior data points similar to the given input. By leveraging precedents instead of fixed policies, our approach greatly enhances the flexibility and adaptability of the guardrail. In this paper, we introduce a critique-revise mechanism for collecting high-quality precedents and two strategies that utilize precedents for robust prediction. Experimental results demonstrate that our approach outperforms previous methods across both few-shot and full-dataset scenarios and exhibits superior generalization to novel policies.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition</title>
<link>https://arxiv.org/abs/2507.20526</link>
<guid>https://arxiv.org/abs/2507.20526</guid>
<content:encoded><![CDATA[
arXiv:2507.20526v1 Announce Type: cross 
Abstract: Recent advances have enabled LLM-powered AI agents to autonomously execute complex tasks by combining language model reasoning with tools, memory, and web access. But can these systems be trusted to follow deployment policies in realistic environments, especially under attack? To investigate, we ran the largest public red-teaming competition to date, targeting 22 frontier AI agents across 44 realistic deployment scenarios. Participants submitted 1.8 million prompt-injection attacks, with over 60,000 successfully eliciting policy violations such as unauthorized data access, illicit financial actions, and regulatory noncompliance. We use these results to build the Agent Red Teaming (ART) benchmark - a curated set of high-impact attacks - and evaluate it across 19 state-of-the-art models. Nearly all agents exhibit policy violations for most behaviors within 10-100 queries, with high attack transferability across models and tasks. Importantly, we find limited correlation between agent robustness and model size, capability, or inference-time compute, suggesting that additional defenses are needed against adversarial misuse. Our findings highlight critical and persistent vulnerabilities in today's AI agents. By releasing the ART benchmark and accompanying evaluation framework, we aim to support more rigorous security assessment and drive progress toward safer agent deployment.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kimi K2: Open Agentic Intelligence</title>
<link>https://arxiv.org/abs/2507.20534</link>
<guid>https://arxiv.org/abs/2507.20534</guid>
<content:encoded><![CDATA[
arXiv:2507.20534v1 Announce Type: cross 
Abstract: We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.
  Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Importance of Facial Features in Vision-based Sign Language Recognition: Eyes, Mouth or Full Face?</title>
<link>https://arxiv.org/abs/2507.20884</link>
<guid>https://arxiv.org/abs/2507.20884</guid>
<content:encoded><![CDATA[
arXiv:2507.20884v1 Announce Type: cross 
Abstract: Non-manual facial features play a crucial role in sign language communication, yet their importance in automatic sign language recognition (ASLR) remains underexplored. While prior studies have shown that incorporating facial features can improve recognition, related work often relies on hand-crafted feature extraction and fails to go beyond the comparison of manual features versus the combination of manual and facial features. In this work, we systematically investigate the contribution of distinct facial regionseyes, mouth, and full faceusing two different deep learning models (a CNN-based model and a transformer-based model) trained on an SLR dataset of isolated signs with randomly selected classes. Through quantitative performance and qualitative saliency map evaluation, we reveal that the mouth is the most important non-manual facial feature, significantly improving accuracy. Our findings highlight the necessity of incorporating facial features in ASLR.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Project-Specific Code Completion by Inferring Internal API Information</title>
<link>https://arxiv.org/abs/2507.20888</link>
<guid>https://arxiv.org/abs/2507.20888</guid>
<content:encoded><![CDATA[
arXiv:2507.20888v1 Announce Type: cross 
Abstract: Project-specific code completion is a critical task that leverages context from a project to generate accurate code. State-of-the-art methods use retrieval-augmented generation (RAG) with large language models (LLMs) and project information for code completion. However, they often struggle to incorporate internal API information, which is crucial for accuracy, especially when APIs are not explicitly imported in the file.
  To address this, we propose a method to infer internal API information without relying on imports. Our method extends the representation of APIs by constructing usage examples and semantic descriptions, building a knowledge base for LLMs to generate relevant completions. We also introduce ProjBench, a benchmark that avoids leaked imports and consists of large-scale real-world projects.
  Experiments on ProjBench and CrossCodeEval show that our approach significantly outperforms existing methods, improving code exact match by 22.72% and identifier exact match by 18.31%. Additionally, integrating our method with existing baselines boosts code match by 47.80% and identifier match by 35.55%.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with Attention-Guided Refinement</title>
<link>https://arxiv.org/abs/2507.20890</link>
<guid>https://arxiv.org/abs/2507.20890</guid>
<content:encoded><![CDATA[
arXiv:2507.20890v1 Announce Type: cross 
Abstract: Img2LaTeX is a practically significant task that involves converting mathematical expressions or tabular data from images into LaTeX code. In recent years, vision-language models (VLMs) have demonstrated strong performance across a variety of visual understanding tasks, owing to their generalization capabilities. While some studies have explored the use of VLMs for the Img2LaTeX task, their performance often falls short of expectations. Empirically, VLMs sometimes struggle with fine-grained visual elements, leading to inaccurate LaTeX predictions. To address this challenge, we propose $A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with Attention-Guided Refinement, a framework that effectively integrates attention localization and iterative refinement within a visual reasoning framework, enabling VLMs to perform self-correction and progressively improve prediction quality. For effective evaluation, we introduce a new dataset, Img2LaTex-Hard-1K, consisting of 1,100 carefully curated and challenging examples designed to rigorously evaluate the capabilities of VLMs within this task domain. Extensive experimental results demonstrate that: (1) $A^2R^2$ significantly improves model performance across six evaluation metrics spanning both textual and visual levels, consistently outperforming other baseline methods; (2) Increasing the number of inference rounds yields notable performance gains, underscoring the potential of $A^2R^2$ in test-time scaling scenarios; (3) Ablation studies and human evaluations validate the practical effectiveness of our approach, as well as the strong synergy among its core components during inference.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dissecting Persona-Driven Reasoning in Language Models via Activation Patching</title>
<link>https://arxiv.org/abs/2507.20936</link>
<guid>https://arxiv.org/abs/2507.20936</guid>
<content:encoded><![CDATA[
arXiv:2507.20936v1 Announce Type: cross 
Abstract: Large language models (LLMs) exhibit remarkable versatility in adopting diverse personas. In this study, we examine how assigning a persona influences a model's reasoning on an objective task. Using activation patching, we take a first step toward understanding how key components of the model encode persona-specific information. Our findings reveal that the early Multi-Layer Perceptron (MLP) layers attend not only to the syntactic structure of the input but also process its semantic content. These layers transform persona tokens into richer representations, which are then used by the middle Multi-Head Attention (MHA) layers to shape the model's output. Additionally, we identify specific attention heads that disproportionately attend to racial and color-based identities.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your AI, Not Your View: The Bias of LLMs in Investment Analysis</title>
<link>https://arxiv.org/abs/2507.20957</link>
<guid>https://arxiv.org/abs/2507.20957</guid>
<content:encoded><![CDATA[
arXiv:2507.20957v1 Announce Type: cross 
Abstract: In finance, Large Language Models (LLMs) face frequent knowledge conflicts due to discrepancies between pre-trained parametric knowledge and real-time market data. These conflicts become particularly problematic when LLMs are deployed in real-world investment services, where misalignment between a model's embedded preferences and those of the financial institution can lead to unreliable recommendations. Yet little research has examined what investment views LLMs actually hold. We propose an experimental framework to investigate such conflicts, offering the first quantitative analysis of confirmation bias in LLM-based investment analysis. Using hypothetical scenarios with balanced and imbalanced arguments, we extract models' latent preferences and measure their persistence. Focusing on sector, size, and momentum, our analysis reveals distinct, model-specific tendencies. In particular, we observe a consistent preference for large-cap stocks and contrarian strategies across most models. These preferences often harden into confirmation bias, with models clinging to initial judgments despite counter-evidence.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2507.20999</link>
<guid>https://arxiv.org/abs/2507.20999</guid>
<content:encoded><![CDATA[
arXiv:2507.20999v1 Announce Type: cross 
Abstract: Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit substantially from chain-of-thought (CoT) reasoning, yet pushing their performance typically requires vast data, large model sizes, and full-parameter fine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost, most existing approaches primarily address domain adaptation or layer-wise allocation rather than explicitly tailoring data and parameters to different response demands. Inspired by "Thinking, Fast and Slow," which characterizes two distinct modes of thought-System 1 (fast, intuitive, often automatic) and System 2 (slower, more deliberative and analytic)-we draw an analogy that different "subregions" of an LLM's parameters might similarly specialize for tasks that demand quick, intuitive responses versus those requiring multi-step logical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework that partitions both data and parameters by System 1 or System 2 demands, using fewer yet more focused parameters for each task. Specifically, we classify task data via multi-model role-playing and voting, and partition parameters based on importance scoring, then adopt a two-stage fine-tuning strategy of training System 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and intuition and refine System 2 tasks with reinforcement learning (RL) to reinforce deeper logical deliberation next. Extensive experiments show that the two-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while matching or surpassing SOTA PEFT baselines.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cheap Learning: Maximising Performance of Language Models for Social Data Science Using Minimal Data</title>
<link>https://arxiv.org/abs/2401.12295</link>
<guid>https://arxiv.org/abs/2401.12295</guid>
<content:encoded><![CDATA[
arXiv:2401.12295v2 Announce Type: replace 
Abstract: The field of machine learning has recently made significant progress in reducing the requirements for labelled training data when building new models. These `cheaper' learning techniques hold significant potential for the social sciences, where development of large labelled training datasets is often a significant practical impediment to the use of machine learning for analytical tasks. In this article we review three `cheap' techniques that have developed in recent years: weak supervision, transfer learning and prompt engineering. For the latter, we also review the particular case of zero-shot prompting of large language models. For each technique we provide a guide of how it works and demonstrate its application across six different realistic social science applications (two different tasks paired with three different dataset makeups). We show good performance for all techniques, and in particular we demonstrate how prompting of large language models can achieve high accuracy at very low cost. Our results are accompanied by a code repository to make it easy for others to duplicate our work and use it in their own research. Overall, our article is intended to stimulate further uptake of these techniques in the social sciences.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Juru: Legal Brazilian Large Language Model from Reputable Sources</title>
<link>https://arxiv.org/abs/2403.18140</link>
<guid>https://arxiv.org/abs/2403.18140</guid>
<content:encoded><![CDATA[
arXiv:2403.18140v2 Announce Type: replace 
Abstract: The high compute cost associated with pretraining large language models limits their research. Two strategies have emerged to address this issue: domain specialization and pretraining with high-quality data. To explore these strategies, we specialized the Mistral-7B model with 1.9 billion unique tokens from reputable Brazilian legal sources and conducted few-shot evaluations on legal and general knowledge test suites. Our model, Juru, demonstrates the benefits of domain specialization by achieving improved performance on legal benchmarks, even with a reduced amount of pretraining data. However, this domain specialization through continued pretraining comes at the cost of increased forgetting in unrelated domains, as evidenced by performance degradation on general knowledge test suites in both Portuguese and English. This study contributes to the growing body of scientific evidence showing that pretraining data selection may enhance the performance of large language models, enabling the exploration of these models at a lower cost. Juru is publicly available at https://huggingface.co/roseval/Juru-7B .
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training</title>
<link>https://arxiv.org/abs/2406.00222</link>
<guid>https://arxiv.org/abs/2406.00222</guid>
<content:encoded><![CDATA[
arXiv:2406.00222v2 Announce Type: replace 
Abstract: Large language models (LLMs), optimized through human feedback, have rapidly emerged as a leading paradigm for developing intelligent conversational assistants. However, despite their strong performance across many benchmarks, LLM-based agents might still lack conversational skills such as disambiguation -- when they are faced with ambiguity, they often overhedge or implicitly guess users' true intents rather than asking clarification questions. Under task-specific settings, high-quality conversation samples are often limited, constituting a bottleneck for LLMs' ability to learn optimal dialogue action policies. We propose Action-Based Contrastive Self-Training (ACT), a quasi-online preference optimization algorithm based on Direct Preference Optimization (DPO), that enables data-efficient dialogue policy learning in multi-turn conversation modeling. We demonstrate ACT's efficacy under in data-efficient tuning scenarios, even when there is no action label available, using multiple real-world conversational tasks: tabular-grounded question-answering, machine reading comprehension, and AmbigSQL, a novel task for disambiguating information-seeking requests for complex SQL generation towards data analysis agents. Additionally, we propose evaluating LLMs' ability to function as conversational agents by examining whether they can implicitly recognize and reason about ambiguity in conversation. ACT demonstrates substantial conversation modeling improvements over standard tuning approaches like supervised fine-tuning and DPO.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Resist Alignment: Evidence From Data Compression</title>
<link>https://arxiv.org/abs/2406.06144</link>
<guid>https://arxiv.org/abs/2406.06144</guid>
<content:encoded><![CDATA[
arXiv:2406.06144v5 Announce Type: replace 
Abstract: Large language models (LLMs) may exhibit unintended or undesirable behaviors. Recent works have concentrated on aligning LLMs to mitigate harmful outputs. Despite these efforts, some anomalies indicate that even a well-conducted alignment process can be easily circumvented, whether intentionally or accidentally. Does alignment fine-tuning yield have robust effects on models, or are its impacts merely superficial? In this work, we make the first exploration of this phenomenon from both theoretical and empirical perspectives. Empirically, we demonstrate the $\mathbf{elasticity}$ of post-alignment models, i.e., the tendency to revert to the behavior distribution formed during the pre-training phase upon further fine-tuning. Leveraging compression theory, we formally deduce that fine-tuning disproportionately undermines alignment relative to pre-training, potentially by orders of magnitude. We validate the presence of elasticity through experiments on models of varying types and scales. Specifically, we find that model performance declines rapidly before reverting to the pre-training distribution, after which the rate of decline drops significantly. Furthermore, we further reveal that elasticity positively correlates with the increased model size and the expansion of pre-training data. Our findings underscore the need to address the inherent elasticity of LLMs to mitigate their resistance to alignment. The model weight and code are available at pku-lm-resist-alignment.github.io.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DoubleDipper: Improving Long-Context LLMs via Context Recycling</title>
<link>https://arxiv.org/abs/2406.13632</link>
<guid>https://arxiv.org/abs/2406.13632</guid>
<content:encoded><![CDATA[
arXiv:2406.13632v4 Announce Type: replace 
Abstract: Despite recent advancements in Large Language Models (LLMs), their performance on tasks involving long contexts remains sub-optimal. In this work, we propose DoubleDipper, a novel In-Context-Learning method that automatically generates few-shot examples for long context QA tasks by recycling contexts. Specifically, given a long input context (1-3k tokens) and a query, we generate additional query-output pairs from the given context as few-shot examples, while introducing the context only once. This ensures that the demonstrations are leveraging the same context as the target query while only adding a small number of tokens to the prompt. We further enhance each demonstration by instructing the model to explicitly identify the relevant paragraphs before the answer, which improves performance while providing fine-grained attribution to the answer source. We apply our method on multiple LLMs and obtain substantial improvements (+16 absolute points on average across models) on various QA datasets with long context. Surprisingly, despite introducing only single-hop ICL examples, LLMs successfully generalize to multi-hop long-context QA using our approach.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of LoRA Adapters on LLMs for Clinical Text Classification Under Computational and Data Constraints</title>
<link>https://arxiv.org/abs/2407.19299</link>
<guid>https://arxiv.org/abs/2407.19299</guid>
<content:encoded><![CDATA[
arXiv:2407.19299v3 Announce Type: replace 
Abstract: Fine-tuning Large Language Models (LLMs) for clinical Natural Language Processing (NLP) poses significant challenges due to domain gap, limited data, and stringent hardware constraints. In this study, we evaluate four adapter techniques-Adapter, Lightweight, TinyAttention, and Gated Residual Network (GRN) - equivalent to Low-Rank Adaptation (LoRA), for clinical note classification under real-world, resource-constrained conditions. All experiments were conducted on a single NVIDIA Quadro P620 GPU (2 GB VRAM, 512 CUDA cores, 1.386 TFLOPS FP32), limiting batch sizes to <8 sequences and maximum sequence length to 256 tokens. Our clinical corpus comprises only 580 000 tokens, several orders of magnitude smaller than standard LLM pre-training datasets. We fine-tuned three biomedical pre-trained LLMs (CamemBERT-bio, AliBERT, DrBERT) and two lightweight Transformer models trained from scratch. Results show that 1) adapter structures provide no consistent gains when fine-tuning biomedical LLMs under these constraints, and 2) simpler Transformers, with minimal parameter counts and training times under six hours, outperform adapter-augmented LLMs, which required over 1000 GPU-hours. Among adapters, GRN achieved the best metrics (accuracy, precision, recall, F1 = 0.88). These findings demonstrate that, in low-resource clinical settings with limited data and compute, lightweight Transformers trained from scratch offer a more practical and efficient solution than large LLMs, while GRN remains a viable adapter choice when minimal adaptation is needed.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio</title>
<link>https://arxiv.org/abs/2409.06624</link>
<guid>https://arxiv.org/abs/2409.06624</guid>
<content:encoded><![CDATA[
arXiv:2409.06624v2 Announce Type: replace 
Abstract: Large Language Models (LLM) often need to be Continual Pre-Trained (CPT) to obtain unfamiliar language skills or adapt to new domains. The huge training cost of CPT often asks for cautious choice of key hyper-parameters such as the mixture ratio of extra language or domain corpus. However, there is no systematic study that bridges the gap between the optimal mixture ratio and the actual model performance, and the gap between experimental scaling law and the actual deployment in the full model size. In this paper, we perform CPT on Llama-3 8B and 70B to enhance its Chinese ability. We study the optimal correlation between the Additional Language Mixture Ratio (ALMR) and the Learning Rate (LR) on the 8B size which directly indicates the optimal experimental setup. By thorough choice of hyper-parameter, and subsequent fine-tuning, the model capability is improved not only on the Chinese-related benchmark but also in some specific domains including math, coding, and emotional intelligence. We deploy the final 70B version of LLM on a real-life chat system which obtains satisfying performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning</title>
<link>https://arxiv.org/abs/2409.12059</link>
<guid>https://arxiv.org/abs/2409.12059</guid>
<content:encoded><![CDATA[
arXiv:2409.12059v5 Announce Type: replace 
Abstract: Current research efforts are focused on enhancing the thinking and reasoning capability of large language model (LLM) by prompting, data-driven emergence and inference-time computation. In this study, we consider stimulating language model's thinking and cognitive abilities from a modular perspective, which mimics the human brain architecture. We select a specific intermediate attention layer with newly implemented language heads. We conduct dual-layer fine-tuning by annotated (query, thought, answer) samples and show that the intermediate layer can also learn to decode fluent and reasonable language tokens. A two-pass inference mechanism is designed to generate thoughts then formal responses. The entire framework is called modularized thinking language model (MeTHanol) which can enhance LLM's cognitive behaviors as indicated by Theory of Mind (ToM) and Vignette-based experiments. Case studies also show that MeTHanol can plan and self-reflect and generate human-like thoughts and answers, even on unseen and open-domain tasks. MeTHanol can also adapt to a personalized prompt and behave as the specified character. Our study holds promise for significant cognitive gains from a modular perspective. Our code, model and data are available at https://bachozean.github.io/methanol-page
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time Factuality Assessment from Adversarial Feedback</title>
<link>https://arxiv.org/abs/2410.14651</link>
<guid>https://arxiv.org/abs/2410.14651</guid>
<content:encoded><![CDATA[
arXiv:2410.14651v3 Announce Type: replace 
Abstract: We show that existing evaluations for assessing the factuality of news from conventional sources, such as claims on fact-checking websites, result in high accuracies over time for LLM-based detectors-even after their knowledge cutoffs. This suggests that recent popular false information from such sources can be easily identified due to its likely presence in pre-training/retrieval corpora or the emergence of salient, yet shallow, patterns in these datasets. Instead, we argue that a proper factuality evaluation dataset should test a model's ability to reason about current events by retrieving and reading related evidence. To this end, we develop a novel pipeline that leverages natural language feedback from a RAG-based detector to iteratively modify real-time news into deceptive variants that challenge LLMs. Our iterative rewrite decreases the binary classification ROC-AUC by an absolute 17.5 percent for a strong RAG-based GPT-4o detector. Our experiments reveal the important role of RAG in both evaluating and generating challenging news examples, as retrieval-free LLM detectors are vulnerable to unseen events and adversarial attacks, while feedback from RAG-based evaluation helps discover more deceitful patterns.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models Have an English Accent? Evaluating and Improving the Naturalness of Multilingual LLMs</title>
<link>https://arxiv.org/abs/2410.15956</link>
<guid>https://arxiv.org/abs/2410.15956</guid>
<content:encoded><![CDATA[
arXiv:2410.15956v3 Announce Type: replace 
Abstract: Current Large Language Models (LLMs) are predominantly designed with English as the primary language, and even the few that are multilingual tend to exhibit strong English-centric biases. Much like speakers who might produce awkward expressions when learning a second language, LLMs often generate unnatural outputs in non-English languages, reflecting English-centric patterns in both vocabulary and grammar. Despite the importance of this issue, the naturalness of multilingual LLM outputs has received limited attention. In this paper, we address this gap by introducing novel automatic corpus-level metrics to assess the lexical and syntactic naturalness of LLM outputs in a multilingual context. Using our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark in French and Chinese, revealing a tendency towards English-influenced patterns. To mitigate this issue, we also propose a simple and effective alignment method to improve the naturalness of an LLM in a target language and domain, achieving consistent improvements in naturalness without compromising the performance on general-purpose benchmarks. Our work highlights the importance of developing multilingual metrics, resources and methods for the new wave of multilingual LLMs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is Wrong with Perplexity for Long-context Language Modeling?</title>
<link>https://arxiv.org/abs/2410.23771</link>
<guid>https://arxiv.org/abs/2410.23771</guid>
<content:encoded><![CDATA[
arXiv:2410.23771v5 Announce Type: replace 
Abstract: Handling long-context inputs is crucial for large language models (LLMs) in tasks such as extended conversations, document summarization, and many-shot in-context learning. While recent approaches have extended the context windows of LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has proven unreliable for assessing long-context capabilities. The underlying cause of this limitation has remained unclear. In this work, we provide a comprehensive explanation for this issue. We find that PPL overlooks key tokens, which are essential for long-context understanding, by averaging across all tokens and thereby obscuring the true performance of models in long-context scenarios. To address this, we propose \textbf{LongPPL}, a novel metric that focuses on key tokens by employing a long-short context contrastive method to identify them. Our experiments demonstrate that LongPPL strongly correlates with performance on various long-context benchmarks (e.g., Pearson correlation of -0.96), significantly outperforming traditional PPL in predictive accuracy. Additionally, we introduce \textbf{LongCE} (Long-context Cross-Entropy) loss, a re-weighting strategy for fine-tuning that prioritizes key tokens, leading to consistent improvements across diverse benchmarks. In summary, these contributions offer deeper insights into the limitations of PPL and present effective solutions for accurately evaluating and enhancing the long-context capabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Summarization of Opinionated Political Documents with Varied Perspectives</title>
<link>https://arxiv.org/abs/2411.04093</link>
<guid>https://arxiv.org/abs/2411.04093</guid>
<content:encoded><![CDATA[
arXiv:2411.04093v2 Announce Type: replace 
Abstract: Global partisan hostility and polarization has increased, and this polarization is heightened around presidential elections. Models capable of generating accurate summaries of diverse perspectives can help reduce such polarization by exposing users to alternative perspectives. In this work, we introduce a novel dataset and task for independently summarizing each political perspective in a set of passages from opinionated news articles. For this task, we propose a framework for evaluating different dimensions of perspective summary performance. We benchmark 11 summarization models and LLMs of varying sizes and architectures through both automatic and human evaluation. While recent models like GPT-4o perform well on this task, we find that all models struggle to generate summaries that are faithful to the intended perspective. Our analysis of summaries focuses on how extraction behavior is impacted by features of the input documents.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Linguistic Diversity of Large Language Models</title>
<link>https://arxiv.org/abs/2412.10271</link>
<guid>https://arxiv.org/abs/2412.10271</guid>
<content:encoded><![CDATA[
arXiv:2412.10271v2 Announce Type: replace 
Abstract: The development and evaluation of Large Language Models (LLMs) has primarily focused on their task-solving capabilities, with recent models even surpassing human performance in some areas. However, this focus often neglects whether machine-generated language matches the human level of diversity, in terms of vocabulary choice, syntactic construction, and expression of meaning, raising questions about whether the fundamentals of language generation have been fully addressed. This paper emphasizes the importance of examining the preservation of human linguistic richness by language models, given the concerning surge in online content produced or aided by LLMs. We propose a comprehensive framework for evaluating LLMs from various linguistic diversity perspectives including lexical, syntactic, and semantic dimensions. Using this framework, we benchmark several state-of-the-art LLMs across all diversity dimensions, and conduct an in-depth case study for syntactic diversity. Finally, we analyze how different development and deployment choices impact the linguistic diversity of LLM outputs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critique of Impure Reason: Unveiling the reasoning behaviour of medical Large Language Models</title>
<link>https://arxiv.org/abs/2412.15748</link>
<guid>https://arxiv.org/abs/2412.15748</guid>
<content:encoded><![CDATA[
arXiv:2412.15748v2 Announce Type: replace 
Abstract: Background: Despite the current ubiquity of Large Language Models (LLMs) across the medical domain, there is a surprising lack of studies which address their reasoning behaviour. We emphasise the importance of understanding reasoning behaviour as opposed to high-level prediction accuracies, since it is equivalent to explainable AI (XAI) in this context. In particular, achieving XAI in medical LLMs used in the clinical domain will have a significant impact across the healthcare sector. Results: Therefore, in this work, we adapt the existing concept of reasoning behaviour and articulate its interpretation within the specific context of medical LLMs. We survey and categorise current state-of-the-art approaches for modeling and evaluating reasoning reasoning in medical LLMs. Additionally, we propose theoretical frameworks which can empower medical professionals or machine learning engineers to gain insight into the low-level reasoning operations of these previously obscure models. We also outline key open challenges facing the development of Large Reasoning Models. Conclusion: The subsequent increased transparency and trust in medical machine learning models by clinicians as well as patients will accelerate the integration, application as well as further development of medical AI for the healthcare system as a whole.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Analysis of Character Development in Holocaust Testimonies</title>
<link>https://arxiv.org/abs/2412.17063</link>
<guid>https://arxiv.org/abs/2412.17063</guid>
<content:encoded><![CDATA[
arXiv:2412.17063v4 Announce Type: replace 
Abstract: This work presents a computational approach to analyze character development along the narrative timeline. The analysis characterizes the inner and outer changes the protagonist undergoes within a narrative, and the interplay between them. We consider transcripts of Holocaust survivor testimonies as a test case, each telling the story of an individual in first-person terms. We focus on the survivor's religious trajectory, examining the evolution of their disposition toward religious belief and practice along the testimony. Clustering the resulting trajectories in the dataset, we identify common sequences in the data. Our findings highlight multiple common structures of religiosity across the narratives: in terms of belief, most present a constant disposition, while for practice, most present an oscillating structure, serving as valuable material for historical and sociological research. This work demonstrates the potential of natural language processing techniques for analyzing character evolution through thematic trajectories in narratives.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Align Attention Heads Before Merging Them: An Effective Way for Converting MHA to GQA</title>
<link>https://arxiv.org/abs/2412.20677</link>
<guid>https://arxiv.org/abs/2412.20677</guid>
<content:encoded><![CDATA[
arXiv:2412.20677v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated exceptional performance across diverse natural language processing tasks. However, as the model size and the input sequence's length increase, the linearly increasing key-value (KV) cache significantly degrades inference throughput. Therefore, grouped-query attention (GQA), as an alternative to multi-head attention (MHA), has been widely introduced into LLMs. In this work, we propose a cost-effective method for converting MHA into GQA with any compression ratio of KV heads. The key point of our method lies in the application of Procrustes analysis to the attention heads, which enhances the similarity among attention heads while preserving computational invariance, thereby improving the model's post-training performance. Subsequently, we employ $\mathit{L_0}$ regularization to prune redundant parameters. The model after pruning can be adapted to the standard GQA framework. Experimental results show that our strategy can compress up to 87.5\% KV heads of LLaMA2-7B model and 75\% KV heads of Sheared-LLaMA-1.3B with acceptable performance degradation. Our code is released at https://github.com/fpcsong/mha2gqa.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FocalPO: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings</title>
<link>https://arxiv.org/abs/2501.06645</link>
<guid>https://arxiv.org/abs/2501.06645</guid>
<content:encoded><![CDATA[
arXiv:2501.06645v3 Announce Type: replace 
Abstract: Efficient preference optimization algorithms such as Direct Preference Optimization (DPO) have become a popular approach in aligning large language models (LLMs) with human preferences. These algorithms implicitly treat the LLM as a reward model, and focus on training it to correct misranked preference pairs. However, recent work~\citep{chen2024preference} empirically finds that DPO training \textit{rarely improves these misranked preference pairs}, despite its gradient emphasizing on these cases. We introduce FocalPO, a DPO variant that instead \textit{down-weighs} misranked preference pairs and prioritizes enhancing the model's understanding of pairs that it can already rank correctly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this by adding a modulating factor to dynamically scale DPO loss. Our experiment demonstrates that FocalPO surpasses DPO and its variants on popular benchmarks like Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the introduced hyperparameter fixed. Additionally, we empirically reveals how FocalPO affects training on correct and incorrect sample groups, further underscoring its effectiveness.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Are Human-Like Internally</title>
<link>https://arxiv.org/abs/2502.01615</link>
<guid>https://arxiv.org/abs/2502.01615</guid>
<content:encoded><![CDATA[
arXiv:2502.01615v2 Announce Type: replace 
Abstract: Recent cognitive modeling studies have reported that larger language models (LMs) exhibit a poorer fit to human reading behavior (Oh and Schuler, 2023b; Shain et al., 2024; Kuribayashi et al., 2024), leading to claims of their cognitive implausibility. In this paper, we revisit this argument through the lens of mechanistic interpretability and argue that prior conclusions were skewed by an exclusive focus on the final layers of LMs. Our analysis reveals that next-word probabilities derived from internal layers of larger LMs align with human sentence processing data as well as, or better than, those from smaller LMs. This alignment holds consistently across behavioral (self-paced reading times, gaze durations, MAZE task processing times) and neurophysiological (N400 brain potentials) measures, challenging earlier mixed results and suggesting that the cognitive plausibility of larger LMs has been underestimated. Furthermore, we first identify an intriguing relationship between LM layers and human measures: earlier layers correspond more closely with fast gaze durations, while later layers better align with relatively slower signals such as N400 potentials and MAZE processing times. Our work opens new avenues for interdisciplinary research at the intersection of mechanistic interpretability and cognitive modeling.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIMO: Less is More for Reasoning</title>
<link>https://arxiv.org/abs/2502.03387</link>
<guid>https://arxiv.org/abs/2502.03387</guid>
<content:encoded><![CDATA[
arXiv:2502.03387v2 Announce Type: replace 
Abstract: We challenge the prevailing assumption that complex reasoning in large language models (LLMs) necessitates massive training data. We demonstrate that sophisticated mathematical reasoning can emerge with only a few examples. Specifically, through simple supervised fine-tuning, our model, LIMO, achieves 63.3\% accuracy on AIME24 and 95.6\% on MATH500, surpassing previous fine-tuned models (6.5\% on AIME24, 59.2\% on MATH500) while using only 1\% of the training data required by prior approaches. Furthermore, LIMO exhibits strong out-of-distribution generalization, achieving a 45.8\% absolute improvement across diverse benchmarks, outperforming models trained on 100x more data. Synthesizing these findings, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning can emerge through minimal but strategically designed demonstrations of cognitive processes. This hypothesis suggests that the threshold for eliciting complex reasoning is not dictated by task complexity but rather by two key factors: (1) the completeness of the model's pre-trained knowledge base and (2) the effectiveness of post-training examples in serving as "cognitive templates" that guide reasoning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Similar Case Retrieval Ranking Performance By Revisiting RankSVM</title>
<link>https://arxiv.org/abs/2502.11131</link>
<guid>https://arxiv.org/abs/2502.11131</guid>
<content:encoded><![CDATA[
arXiv:2502.11131v2 Announce Type: replace 
Abstract: Given the rapid development of Legal AI, a lot of attention has been paid to one of the most important legal AI tasks--similar case retrieval, especially with language models to use. In our paper, however, we try to improve the ranking performance of current models from the perspective of learning to rank instead of language models. Specifically, we conduct experiments using a pairwise method--RankSVM as the classifier to substitute a fully connected layer, combined with commonly used language models on similar case retrieval datasets LeCaRDv1 and LeCaRDv2. We finally come to the conclusion that RankSVM could generally help improve the retrieval performance on the LeCaRDv1 and LeCaRDv2 datasets compared with original classifiers by optimizing the precise ranking. It could also help mitigate overfitting owing to class imbalance. Our code is available in https://github.com/liuyuqi123study/RankSVM_for_SLR
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Regularization with Sparse Autoencoders for Controllable LLM-based Classification</title>
<link>https://arxiv.org/abs/2502.14133</link>
<guid>https://arxiv.org/abs/2502.14133</guid>
<content:encoded><![CDATA[
arXiv:2502.14133v3 Announce Type: replace 
Abstract: Modern text classification methods heavily rely on contextual embeddings from large language models (LLMs). Compared to human-engineered features, these embeddings provide automatic and effective representations for classification model training. However, they also introduce a challenge: we lose the ability to manually remove unintended features, such as sensitive or task-irrelevant features, to guarantee regulatory compliance or improve the generalizability of classification models. This limitation arises because LLM embeddings are opaque and difficult to interpret. In this paper, we propose a novel framework to identify and regularize unintended features in the LLM latent space. Specifically, we first pre-train a sparse autoencoder (SAE) to extract interpretable features from LLM latent spaces. To ensure the SAE can capture task-specific features, we further fine-tune it on task-specific datasets. In training the classification model, we propose a simple and effective regularizer, by minimizing the similarity between the classifier weights and the identified unintended feature, to remove the impact of these unintended features on classification. We evaluate the proposed framework on three real-world tasks, including toxic chat detection, reward modeling, and disease diagnosis. Results show that the proposed self-regularization framework can improve the classifier's generalizability by regularizing those features that are not semantically correlated to the task. This work pioneers controllable text classification on LLM latent spaces by leveraging interpreted features to address generalizability, fairness, and privacy challenges. The code and data are publicly available at https://github.com/JacksonWuxs/Controllable_LLM_Classifier.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models</title>
<link>https://arxiv.org/abs/2502.18573</link>
<guid>https://arxiv.org/abs/2502.18573</guid>
<content:encoded><![CDATA[
arXiv:2502.18573v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated vast capabilities on generative tasks in recent years, yet they struggle with guaranteeing the factual correctness of the generated content. This makes these models unreliable in realistic situations where factually accurate responses are expected. In this paper, we propose FactReasoner, a new factuality assessor that relies on probabilistic reasoning to assess the factuality of a long-form generated response. Specifically, FactReasoner decomposes the response into atomic units, retrieves relevant contexts for them from an external knowledge source, and constructs a joint probability distribution over the atoms and contexts using probabilistic encodings of the logical relationships (entailment, contradiction) between the textual utterances corresponding to the atoms and contexts. FactReasoner then computes the posterior probability of whether atomic units in the response are supported by the retrieved contexts. Our experiments on labeled and unlabeled benchmark datasets demonstrate clearly that FactReasoner improves considerably over state-of-the-art prompt-based approaches in terms of both factual precision and recall.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents</title>
<link>https://arxiv.org/abs/2503.08026</link>
<guid>https://arxiv.org/abs/2503.08026</guid>
<content:encoded><![CDATA[
arXiv:2503.08026v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have made significant progress in open-ended dialogue, yet their inability to retain and retrieve relevant information from long-term interactions limits their effectiveness in applications requiring sustained personalization. External memory mechanisms have been proposed to address this limitation, enabling LLMs to maintain conversational continuity. However, existing approaches struggle with two key challenges. First, rigid memory granularity fails to capture the natural semantic structure of conversations, leading to fragmented and incomplete representations. Second, fixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user interaction patterns. In this work, we propose Reflective Memory Management (RMM), a novel mechanism for long-term dialogue agents, integrating forward- and backward-looking reflections: (1) Prospective Reflection, which dynamically summarizes interactions across granularities-utterances, turns, and sessions-into a personalized memory bank for effective future retrieval, and (2) Retrospective Reflection, which iteratively refines the retrieval in an online reinforcement learning (RL) manner based on LLMs' cited evidence. Experiments show that RMM demonstrates consistent improvement across various metrics and benchmarks. For example, RMM shows more than 10% accuracy improvement over the baseline without memory management on the LongMemEval dataset.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Caricatures: On the Representation of African American Language in Pretraining Corpora</title>
<link>https://arxiv.org/abs/2503.10789</link>
<guid>https://arxiv.org/abs/2503.10789</guid>
<content:encoded><![CDATA[
arXiv:2503.10789v2 Announce Type: replace 
Abstract: With a combination of quantitative experiments, human judgments, and qualitative analyses, we evaluate the quantity and quality of African American Language (AAL) representation in 12 predominantly English, open-source pretraining corpora. We specifically focus on the sources, variation, and naturalness of included AAL texts representing the AAL-speaking community. We find that AAL is underrepresented in all evaluated pretraining corpora compared to US demographics, constituting as few as 0.007% and at most 0.18% of documents. We also find that more than 25% of AAL texts in C4 may be perceived as inappropriate for LLMs to generate and to reinforce harmful stereotypes. Finally, we find that most automated filters are more likely to conserve White Mainstream English (WME) texts over AAL in pretraining corpora.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Mathematical Proof Generation Using Large Language Model Agents and Knowledge Graphs</title>
<link>https://arxiv.org/abs/2503.11657</link>
<guid>https://arxiv.org/abs/2503.11657</guid>
<content:encoded><![CDATA[
arXiv:2503.11657v2 Announce Type: replace 
Abstract: Large language models have demonstrated remarkable capabilities in natural language processing tasks requiring multi-step logical reasoning capabilities, such as automated theorem proving. However, challenges persist within theorem proving, such as the identification of key mathematical concepts, understanding their interrelationships, and formalizing proofs correctly within natural language. We present KG-prover, a novel framework that leverages knowledge graphs mined from reputable mathematical texts to augment general-purpose LLMs to construct and formalize mathematical proofs. We also study the effects of scaling graph-based, test-time compute using KG-Prover, demonstrating significant performance improvements over baselines across multiple datasets. General-purpose LLMs improve up to 21\% on miniF2F-test when combined with KG-Prover, with consistent improvements ranging from 2-11\% on the ProofNet, miniF2F-test, and MUSTARD datasets without additional scaling. Furthermore, KG-Prover with o4-mini achieves over 50% miniF2F-test. This work provides a promising approach for augmenting natural language proof reasoning with knowledge graphs without the need for additional finetuning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Common Ground Misalignment in Goal-Oriented Dialog: A Case-Study with Ubuntu Chat Logs</title>
<link>https://arxiv.org/abs/2503.12370</link>
<guid>https://arxiv.org/abs/2503.12370</guid>
<content:encoded><![CDATA[
arXiv:2503.12370v2 Announce Type: replace 
Abstract: While it is commonly accepted that maintaining common ground plays a role in conversational success, little prior research exists connecting conversational grounding to success in task-oriented conversations. We study failures of grounding in the Ubuntu IRC dataset, where participants use text-only communication to resolve technical issues. We find that disruptions in conversational flow often stem from a misalignment in common ground, driven by a divergence in beliefs and assumptions held by participants. These disruptions, which we call conversational friction, significantly correlate with task success. We find that although LLMs can identify overt cases of conversational friction, they struggle with subtler and more context-dependent instances requiring pragmatic or domain-specific reasoning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical Investigation</title>
<link>https://arxiv.org/abs/2503.12854</link>
<guid>https://arxiv.org/abs/2503.12854</guid>
<content:encoded><![CDATA[
arXiv:2503.12854v3 Announce Type: replace 
Abstract: Recent advancements in post-training methodologies for large language models (LLMs) have highlighted reinforcement learning (RL) as a critical component for enhancing reasoning. However, the substantial computational costs associated with RL-based approaches have led to growing interest in alternative paradigms, such as Direct Preference Optimization (DPO). In this study, we investigate the effectiveness of DPO in facilitating self-improvement for LLMs through iterative preference-based learning. We demonstrate that a single round of DPO with coarse filtering significantly enhances mathematical reasoning performance, particularly for strong base model. Furthermore, we design an iterative enhancement framework for both the generator and the reward model (RM), enabling their mutual improvement through online interaction across multiple rounds of DPO. Finally, with simple verifiable rewards, our model DPO-VP achieves RL-level performance with significantly lower computational overhead. These findings highlight DPO as a scalable and cost-effective alternative to RL, offering a practical solution for enhancing LLM reasoning in resource-constrained situations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIB-STC: A Large-Scale Structured Tibetan Benchmark for Low-Resource Language Modeling</title>
<link>https://arxiv.org/abs/2503.18288</link>
<guid>https://arxiv.org/abs/2503.18288</guid>
<content:encoded><![CDATA[
arXiv:2503.18288v4 Announce Type: replace 
Abstract: Advancement of large language models (LLMs) has brought transformative capabilities to NLP, but such progress remains unevenly distributed, especially for low-resource and culturally rich languages like Tibetan. In this paper, we present TIB-STC, the first large-scale, expert-curated, and multi-domain benchmark specifically designed to support the development and evaluation of LLMs for the Tibetan language. Spanning over 11 billion tokens across literature, religion, medicine, law, and daily communication, TIB-STC preserves traditional grammar and stylistic richness. To validate its utility, we train a reference model, Sun-Shine, on TIB-STC through a three-stage pipeline involving pretraining, supervised fine-tuning, and preference optimization. Evaluation on TLUE Benchmark for Tibetan-specific tasks, including Ti-MMLU and Ti-SafetyBench, demonstrates the benchmark's effectiveness in enabling robust instruction-following and culturally aligned generation. We release TIB-STC to advance research in low-resource language modeling and promote inclusivity in multilingual NLP. All data are available at: https://github.com/Vicentvankor/sun-shine
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the Risks of Using Large Language Models for Text Annotation in Social Science Research</title>
<link>https://arxiv.org/abs/2503.22040</link>
<guid>https://arxiv.org/abs/2503.22040</guid>
<content:encoded><![CDATA[
arXiv:2503.22040v2 Announce Type: replace 
Abstract: Large language models (LLMs) have the potential to revolutionize computational social science, particularly in automated textual analysis. In this paper, we conduct a systematic evaluation of the promises and risks associated with using LLMs for text classification tasks, using social movement studies as an example. We propose a framework for social scientists to incorporate LLMs into text annotation, either as the primary coding decision-maker or as a coding assistant. This framework offers researchers tools to develop the potential best-performing prompt, and to systematically examine and report the validity and reliability of LLMs as a methodological tool. Additionally, we evaluate and discuss its epistemic risks associated with validity, reliability, replicability, and transparency. We conclude with several practical guidelines for using LLMs in text annotation tasks and offer recommendations for more effectively communicating epistemic risks in research.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Analysis of Interleaved Speech-Text Language Models</title>
<link>https://arxiv.org/abs/2504.02398</link>
<guid>https://arxiv.org/abs/2504.02398</guid>
<content:encoded><![CDATA[
arXiv:2504.02398v2 Announce Type: replace 
Abstract: Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. It predicts that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - "Do interleaved SLMs scale more efficiently than textless-SLMs?" In this paper we answer a resounding yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling dynamics significantly differ from textless-SLMs, suggesting one should allocate notably more of the compute budget to increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest that our scaled up model achieves comparable semantic speech performance to leading models, while using less compute and data. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims/ .
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Modeling for the Future of Finance: A Survey into Metrics, Tasks, and Data Opportunities</title>
<link>https://arxiv.org/abs/2504.07274</link>
<guid>https://arxiv.org/abs/2504.07274</guid>
<content:encoded><![CDATA[
arXiv:2504.07274v2 Announce Type: replace 
Abstract: Recent advances in language modeling have led to growing interest in applying Natural Language Processing (NLP) techniques to financial problems, enabling new approaches to analysis and decision-making. To systematically examine this trend, we review 374 NLP research papers published between 2017 and 2024 across 38 conferences and workshops, with a focused analysis of 221 papers that directly address finance-related tasks. We evaluate these papers across 11 quantitative and qualitative dimensions, and our study identifies the following opportunities: (i) expanding the scope of forecasting tasks; (ii) enriching evaluation with financial metrics; (iii) leveraging multilingual and crisis-period datasets; and (iv) balancing PLMs with efficient or interpretable alternatives. We identify actionable directions for research and practice, supported by dataset and tool recommendations, with implications for both the academia and industry communities.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization: A Close Look at Books</title>
<link>https://arxiv.org/abs/2504.12549</link>
<guid>https://arxiv.org/abs/2504.12549</guid>
<content:encoded><![CDATA[
arXiv:2504.12549v2 Announce Type: replace 
Abstract: To what extent can entire books be extracted from LLMs? Using the Llama 3 70B family of models, and the "prefix-prompting" extraction technique, we were able to auto-regressively reconstruct, with a very high level of similarity, one entire book (Alice's Adventures in Wonderland) from just the first 500 tokens. We were also able to obtain high extraction rates on several other books, piece-wise. However, these successes do not extend uniformly to all books. We show that extraction rates of books correlate with book popularity and thus, likely duplication in the training data.
  We also confirm the undoing of mitigations in the instruction-tuned Llama 3.1, following recent work (Nasr et al., 2025). We further find that this undoing comes from changes to only a tiny fraction of weights concentrated primarily in the lower transformer blocks. Our results provide evidence of the limits of current regurgitation mitigation strategies and introduce a framework for studying how fine-tuning affects the retrieval of verbatim memorization in aligned LLMs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars</title>
<link>https://arxiv.org/abs/2504.17562</link>
<guid>https://arxiv.org/abs/2504.17562</guid>
<content:encoded><![CDATA[
arXiv:2504.17562v2 Announce Type: replace 
Abstract: The ability to acquire latent semantics is one of the key properties that determines the performance of language models. One convenient approach to invoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at the beginning of texts in the pre-training data, making it easier for the model to access latent semantics before observing the entire text. Previous studies have reported that this technique actually improves the performance of trained models in downstream tasks; however, this improvement has been observed only in specific downstream tasks, without consistent enhancement in average next-token prediction loss. To understand this phenomenon, we closely investigate how prepending metadata during pre-training affects model performance by examining its behavior using artificial data. Interestingly, we found that this approach produces both positive and negative effects on the downstream tasks. We demonstrate that the effectiveness of the approach depends on whether latent semantics can be inferred from the downstream task's prompt. Specifically, through investigations using data generated by probabilistic context-free grammars, we show that training with metadata helps improve model's performance when the given context is long enough to infer the latent semantics. In contrast, the technique negatively impacts performance when the context lacks the necessary information to make an accurate posterior inference.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs</title>
<link>https://arxiv.org/abs/2505.02456</link>
<guid>https://arxiv.org/abs/2505.02456</guid>
<content:encoded><![CDATA[
arXiv:2505.02456v2 Announce Type: replace 
Abstract: One of the goals of fairness research in NLP is to measure and mitigate stereotypical biases that are propagated by NLP systems. However, such work tends to focus on single axes of bias (most often gender) and the English language. Addressing these limitations, we contribute the first study of multilingual intersecting country and gender biases, with a focus on occupation recommendations generated by large language models. We construct a benchmark of prompts in English, Spanish and German, where we systematically vary country and gender, using 25 countries and four pronoun sets. Then, we evaluate a suite of 5 Llama-based models on this benchmark, finding that LLMs encode significant gender and country biases. Notably, we find that even when models show parity for gender or country individually, intersectional occupational biases based on both country and gender persist. We also show that the prompting language significantly affects bias, and instruction-tuned models consistently demonstrate the lowest and most stable levels of bias. Our findings highlight the need for fairness researchers to use intersectional and multilingual lenses in their work.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Information Distortion in Hierarchical Ultra long Novel Reconstruction:The Optimal Expansion Ratio</title>
<link>https://arxiv.org/abs/2505.12572</link>
<guid>https://arxiv.org/abs/2505.12572</guid>
<content:encoded><![CDATA[
arXiv:2505.12572v2 Announce Type: replace 
Abstract: A two stage novel generation framework (outline -> section outline -> manuscript) is widely used in long novel generation,(e.g., \textsc{DOME}, \textsc{Plan\&amp;Write}, \textsc{Long Writer}), but study of such framework in ultra long novel(>1M words) reconstruction is little. Building on recent text compression methods (\textsc{LLMZip}, \textsc{LLM2Vec}), we conduct an information-theoretic analysis to quantify semantic distortion under different compression-expansion ratios. We examine how outline length affects information preservation. Experiments on ultra-long novels show that the optimal compression-expansion ratio significantly reduces semantic distortion compared to other non-optimal compression-expansion ratio.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accidental Vulnerability: Factors in Fine-Tuning that Shift Model Safeguards</title>
<link>https://arxiv.org/abs/2505.16789</link>
<guid>https://arxiv.org/abs/2505.16789</guid>
<content:encoded><![CDATA[
arXiv:2505.16789v2 Announce Type: replace 
Abstract: As large language models (LLMs) gain popularity, their vulnerability to adversarial attacks emerges as a primary concern. While fine-tuning models on domain-specific datasets is often employed to improve model performance, it can inadvertently introduce vulnerabilities within the underlying model. In this work, we investigate Accidental Vulnerability, unexpected vulnerabilities arising from characteristics of fine-tuning data. We begin by identifying potential correlation factors such as linguistic features, semantic similarity, and toxicity across multiple experimental datasets. We then evaluate the adversarial robustness of these fine-tuned models, analyzing persona shifts and interpretability traits to understand how dataset factors contribute to attack success rates. Lastly, we explore causal relationships that offer new insights into adversarial defense strategies, highlighting the crucial role of dataset design in preserving model alignment. Our code is available at https://github.com/psyonp/accidental_vulnerability.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.17067</link>
<guid>https://arxiv.org/abs/2505.17067</guid>
<content:encoded><![CDATA[
arXiv:2505.17067v3 Announce Type: replace 
Abstract: Detecting Mild Cognitive Impairment from picture descriptions is critical yet challenging, especially in multilingual and multiple picture settings. Prior work has primarily focused on English speakers describing a single picture (e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by introducing multilingual speakers and multiple pictures, which presents new challenges in analyzing picture-dependent content. To address these challenges, we propose a framework with three components: (1) enhancing discriminative representation learning via supervised contrastive learning, (2) involving image modality rather than relying solely on speech and text modalities, and (3) applying a Product of Experts (PoE) strategy to mitigate spurious correlations and overfitting. Our framework improves MCI detection performance, achieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to 75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the text unimodal baseline. Notably, the contrastive learning component yields greater gains for the text modality compared to speech. These results highlight our framework's effectiveness in multilingual and multi-picture MCI detection.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands</title>
<link>https://arxiv.org/abs/2505.17137</link>
<guid>https://arxiv.org/abs/2505.17137</guid>
<content:encoded><![CDATA[
arXiv:2505.17137v2 Announce Type: replace 
Abstract: Early detection of cognitive decline is crucial for enabling interventions that can slow neurodegenerative disease progression. Traditional diagnostic approaches rely on labor-intensive clinical assessments, which are impractical for frequent monitoring. Our pilot study investigates voice assistant systems (VAS) as non-invasive tools for detecting cognitive decline through longitudinal analysis of speech patterns in voice commands. Over an 18-month period, we collected voice commands from 35 older adults, with 15 participants providing daily at-home VAS interactions. To address the challenges of analyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a framework that combines (1) LLM-driven iterative prompt refinement for linguistic feature extraction, (2) HuBERT-based acoustic feature extraction, and (3) transformer-based temporal modeling. Using iTransformer, our approach achieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming its baseline by 27.13%. Through our LLM approach, we identify linguistic features that uniquely characterize everyday command usage patterns in individuals experiencing cognitive decline.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Physical Reasoning with the PHYSICS Dataset</title>
<link>https://arxiv.org/abs/2506.00022</link>
<guid>https://arxiv.org/abs/2506.00022</guid>
<content:encoded><![CDATA[
arXiv:2506.00022v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved remarkable progress on advanced reasoning tasks such as mathematics and coding competitions. Meanwhile, physics, despite being both reasoning-intensive and essential to real-world understanding, received limited academic and industrial attention. This paper introduces PHYSICS, a dataset containing 16,568 high-quality physics problems spanning subjects and difficulty levels, to facilitate this issue. Specifically, PHYSICS is curated with exercises from over 100 textbooks through a carefully designed pipeline for quality control. It covers five major physics domains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern Physics. It also spans a wide range of difficulty levels, from high school to graduate-level physics courses. To utilize the data for improving and evaluating the model's physical reasoning capabilities, we split the dataset into training and test sets, and provide reasoning paths generated by powerful reasoning models for the training data to facilitate model training. In addition, for the evaluation part, we find that existing evaluation frameworks exhibit biases in aspects such as units, simplification, and precision in physics domain. To balance efficiency and accuracy, we introduce a Rule+Model evaluation framework tailored to physics problems. Our evaluations on current state-of-the-art open-source and proprietary models highlight the limitations of current models in handling physics-related tasks. We hope that our dataset and evaluation methodology will jointly advance the development of LLMs in the field of physics.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimal Pair-Based Evaluation of Code-Switching</title>
<link>https://arxiv.org/abs/2506.01840</link>
<guid>https://arxiv.org/abs/2506.01840</guid>
<content:encoded><![CDATA[
arXiv:2506.01840v2 Announce Type: replace 
Abstract: There is a lack of an evaluation methodology that estimates the extent to which large language models (LLMs) use code-switching (CS) in the same way as bilinguals. Existing methods do not have wide language coverage, fail to account for the diverse range of CS phenomena, or do not scale. We propose an intervention based on minimal pairs of CS. Each minimal pair contains one naturally occurring CS sentence and one minimally manipulated variant. We collect up to 1,000 such pairs each for 11 language pairs. Our human experiments show that, for every language pair, bilinguals consistently prefer the naturally occurring CS sentence. Meanwhile our experiments with current LLMs show that the larger the model, the more consistently it assigns higher probability to the naturally occurring CS sentence than to the variant. In accordance with theoretical claims, the largest probability differences arise in those pairs where the manipulated material consisted of closed-class words.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code-Switching and Syntax: A Large-Scale Experiment</title>
<link>https://arxiv.org/abs/2506.01846</link>
<guid>https://arxiv.org/abs/2506.01846</guid>
<content:encoded><![CDATA[
arXiv:2506.01846v2 Announce Type: replace 
Abstract: The theoretical code-switching (CS) literature provides numerous pointwise investigations that aim to explain patterns in CS, i.e. why bilinguals switch language in certain positions in a sentence more often than in others. A resulting consensus is that CS can be explained by the syntax of the contributing languages. There is however no large-scale, multi-language, cross-phenomena experiment that tests this claim. When designing such an experiment, we need to make sure that the system that is predicting where bilinguals tend to switch has access only to syntactic information. We provide such an experiment here. Results show that syntax alone is sufficient for an automatic system to distinguish between sentences in minimal pairs of CS, to the same degree as bilingual humans. Furthermore, the learnt syntactic patterns generalise well to unseen language pairs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT</title>
<link>https://arxiv.org/abs/2506.02005</link>
<guid>https://arxiv.org/abs/2506.02005</guid>
<content:encoded><![CDATA[
arXiv:2506.02005v2 Announce Type: replace 
Abstract: In this paper, we address the persistent challenges that figurative language expressions pose for natural language processing (NLP) systems, particularly in low-resource languages such as Konkani. We present a hybrid model that integrates a pre-trained Multilingual BERT (mBERT) with a bidirectional LSTM and a linear classifier. This architecture is fine-tuned on a newly introduced annotated dataset for metaphor classification, developed as part of this work. To improve the model's efficiency, we implement a gradient-based attention head pruning strategy. For metaphor classification, the pruned model achieves an accuracy of 78%. We also applied our pruning approach to expand on an existing idiom classification task, achieving 83% accuracy. These results demonstrate the effectiveness of attention head pruning for building efficient NLP tools in underrepresented languages.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2506.09853</link>
<guid>https://arxiv.org/abs/2506.09853</guid>
<content:encoded><![CDATA[
arXiv:2506.09853v2 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing large language models (LLMs) with complex reasoning capabilities. However, CoT currently faces two fundamental challenges: (1) Sufficiency, which ensures that the generated intermediate inference steps comprehensively cover and substantiate the final conclusion; and (2) Necessity, which identifies the inference steps that are truly indispensable for the soundness of the resulting answer. We propose a causal framework that characterizes CoT reasoning through the dual lenses of sufficiency and necessity. Incorporating causal Probability of Sufficiency and Necessity allows us not only to determine which steps are logically sufficient or necessary to the prediction outcome, but also to quantify their actual influence on the final reasoning outcome under different intervention scenarios, thereby enabling the automated addition of missing steps and the pruning of redundant ones. Extensive experimental results on various mathematical and commonsense reasoning benchmarks confirm substantial improvements in reasoning efficiency and reduced token usage without sacrificing accuracy. Our work provides a promising direction for improving LLM reasoning performance and cost-effectiveness.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intersectional Bias in Japanese Large Language Models from a Contextualized Perspective</title>
<link>https://arxiv.org/abs/2506.12327</link>
<guid>https://arxiv.org/abs/2506.12327</guid>
<content:encoded><![CDATA[
arXiv:2506.12327v2 Announce Type: replace 
Abstract: An increasing number of studies have examined the social bias of rapidly developed large language models (LLMs). Although most of these studies have focused on bias occurring in a single social attribute, research in social science has shown that social bias often occurs in the form of intersectionality -- the constitutive and contextualized perspective on bias aroused by social attributes. In this study, we construct the Japanese benchmark inter-JBBQ, designed to evaluate the intersectional bias in LLMs on the question-answering setting. Using inter-JBBQ to analyze GPT-4o and Swallow, we find that biased output varies according to its contexts even with the equal combination of social attributes.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Structured Bangla Dataset of Disease-Symptom Associations to Improve Diagnostic Accuracy</title>
<link>https://arxiv.org/abs/2506.13610</link>
<guid>https://arxiv.org/abs/2506.13610</guid>
<content:encoded><![CDATA[
arXiv:2506.13610v3 Announce Type: replace 
Abstract: Disease-symptom datasets are significant and in demand for medical research, disease diagnosis, clinical decision-making, and AI-driven health management applications. These datasets help identify symptom patterns associated with specific diseases, thus improving diagnostic accuracy and enabling early detection. The dataset presented in this study systematically compiles disease-symptom relationships from various online sources, medical literature, and publicly available health databases. The data was gathered through analyzing peer-reviewed medical articles, clinical case studies, and disease-symptom association reports. Only the verified medical sources were included in the dataset, while those from non-peer-reviewed and anecdotal sources were excluded. The dataset is structured in a tabular format, where the first column represents diseases, and the remaining columns represent symptoms. Each symptom cell contains a binary value, indicating whether a symptom is associated with a disease. Thereby, this structured representation makes the dataset very useful for a wide range of applications, including machine learning-based disease prediction, clinical decision support systems, and epidemiological studies. Although there are some advancements in the field of disease-symptom datasets, there is a significant gap in structured datasets for the Bangla language. This dataset aims to bridge that gap by facilitating the development of multilingual medical informatics tools and improving disease prediction models for underrepresented linguistic communities. Further developments should include region-specific diseases and further fine-tuning of symptom associations for better diagnostic performance
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement learning fine-tuning of language model for instruction following and math reasoning</title>
<link>https://arxiv.org/abs/2506.21560</link>
<guid>https://arxiv.org/abs/2506.21560</guid>
<content:encoded><![CDATA[
arXiv:2506.21560v2 Announce Type: replace 
Abstract: This study investigates the effectiveness of reinforcement learning (RL) fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two challenging tasks: instruction following and mathematical reasoning. We compare supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models. Our experiments show that RLOO with DeBERTa reward modeling achieves the best alignment, while DPO provides strong and consistent results. For math reasoing tasks, synthetic data augmentation and best-of-N sampling with an external verifier significantly improve accuracy, showing the potential of combining fine-tuning with inference-time tools. This study highlights key trade-offs and practical strategies for training lightweight, task-aligned small-scale language models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate Speech</title>
<link>https://arxiv.org/abs/2506.21613</link>
<guid>https://arxiv.org/abs/2506.21613</guid>
<content:encoded><![CDATA[
arXiv:2506.21613v2 Announce Type: replace 
Abstract: Hate speech targeting children on social media is a serious and growing problem, yet current NLP systems struggle to detect it effectively. This gap exists mainly because existing datasets focus on adults, lack age specific labels, miss nuanced linguistic cues, and are often too small for robust modeling. To address this, we introduce ChildGuard, the first large scale English dataset dedicated to hate speech aimed at children. It contains 351,877 annotated examples from X (formerly Twitter), Reddit, and YouTube, labeled by three age groups: younger children (under 11), pre teens (11--12), and teens (13--17). The dataset is split into two subsets for fine grained analysis: a contextual subset (157K) focusing on discourse level features, and a lexical subset (194K) emphasizing word-level sentiment and vocabulary. Benchmarking state of the art hate speech models on ChildGuard reveals notable drops in performance, highlighting the challenges of detecting child directed hate speech.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Answers to Rationales: Self-Aligning Multimodal Reasoning with Answer-Oriented Chain-of-Thought</title>
<link>https://arxiv.org/abs/2507.02984</link>
<guid>https://arxiv.org/abs/2507.02984</guid>
<content:encoded><![CDATA[
arXiv:2507.02984v2 Announce Type: replace 
Abstract: Achieving human-like reasoning capabilities in Multimodal Large Language Models (MLLMs) has long been a goal. Current methods primarily focus on synthesizing positive rationales, typically relying on manual annotations or complex systems. Moreover, they often overlook negative reasoning, which limits the model's generalization ability and robustness in multimodal inference. To address this gap, we propose a novel framework: \textbf{S}elf-Aligning \textbf{M}ultimodal Reasoning with \textbf{A}nswer-O\textbf{r}iented Chain-of-\textbf{T}hought (SMART). SMART employs an answer-oriented chain-of-thought (AoT) prompt to automatically construct high-quality data. Drawing inspiration from human proof-based strategies, AoT leverages both correct and incorrect answers to extract key visual information that links questions and answers. When provided with correct answers, the model produces strong positive rationales. Conversely, when correct answers are replaced with incorrect alternatives, the model generates an erroneous yet compelling reasoning path, serving as a form of discriminative negative rationale. Models trained with AoT-generated data outperform those trained on manually annotated datasets, demonstrating superior reasoning capabilities. Consequently, SMART establishes an iterative generation-optimization method that continually enhances the model's reasoning skills. Experiments indicate that the SMART framework significantly improves various MLLMs, regardless of model architecture, parameter size, or pre-training dataset. The code is available at https://github.com/WentaoTan/SMART.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen Visual Unicode Representations</title>
<link>https://arxiv.org/abs/2507.04886</link>
<guid>https://arxiv.org/abs/2507.04886</guid>
<content:encoded><![CDATA[
arXiv:2507.04886v2 Announce Type: replace 
Abstract: Understanding the locus of semantic representation in large language models (LLMs) is crucial for interpretability and architectural innovation. The dominant paradigm posits that trainable input embeddings serve as foundational "meaning vectors." This paper challenges that view. We construct Transformer models where the embedding layer is entirely frozen, with vectors derived not from data, but from the visual structure of Unicode glyphs. These non-semantic, precomputed visual embeddings are fixed throughout training. Our method is compatible with any tokenizer, including a novel Unicode-centric tokenizer we introduce to ensure universal text coverage. Despite the absence of trainable, semantically initialized embeddings, our models converge, generate coherent text, and, critically, outperform architecturally identical models with trainable embeddings on the MMLU reasoning benchmark. We attribute this to "representational interference" in conventional models, where the embedding layer is burdened with learning both structural and semantic features. Our results indicate that high-level semantics are not inherent to input embeddings but are an emergent property of the Transformer's compositional architecture and data scale. This reframes the role of embeddings from meaning containers to structural primitives. We release all code and models to foster further research.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Checklist Engineering Empowers Multilingual LLM Judges</title>
<link>https://arxiv.org/abs/2507.06774</link>
<guid>https://arxiv.org/abs/2507.06774</guid>
<content:encoded><![CDATA[
arXiv:2507.06774v2 Announce Type: replace 
Abstract: Automated text evaluation has long been a central issue in Natural Language Processing (NLP). Recently, the field has shifted toward using Large Language Models (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While promising and easily adaptable across tasks, this approach has seen limited exploration in multilingual contexts. Existing multilingual studies often rely on proprietary models or require extensive training data for fine-tuning, raising concerns about cost, time, and efficiency. In this paper, we propose Checklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free framework that uses checklist intuition for multilingual evaluation with an open-source model. Experiments across multiple languages and three benchmark datasets, under both pointwise and pairwise settings, show that our method generally surpasses the baselines and performs on par with the GPT-4o model.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation</title>
<link>https://arxiv.org/abs/2507.07307</link>
<guid>https://arxiv.org/abs/2507.07307</guid>
<content:encoded><![CDATA[
arXiv:2507.07307v2 Announce Type: replace 
Abstract: Large language models (LLMs) incorporated with Retrieval-Augmented Generation (RAG) have demonstrated powerful capabilities in generating counterspeech against misinformation. However, current studies rely on limited evidence and offer less control over final outputs. To address these challenges, we propose a Multi-agent Retrieval-Augmented Framework to generate counterspeech against health misinformation, incorporating multiple LLMs to optimize knowledge retrieval, evidence enhancement, and response refinement. Our approach integrates both static and dynamic evidence, ensuring that the generated counterspeech is relevant, well-grounded, and up-to-date. Our method outperforms baseline approaches in politeness, relevance, informativeness, and factual accuracy, demonstrating its effectiveness in generating high-quality counterspeech. To further validate our approach, we conduct ablation studies to verify the necessity of each component in our framework. Furthermore, cross evaluations show that our system generalizes well across diverse health misinformation topics and datasets. And human evaluations reveal that refinement significantly enhances counterspeech quality and obtains human preference.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Otter: A Multi-Modal Model with In-Context Instruction Tuning</title>
<link>https://arxiv.org/abs/2305.03726</link>
<guid>https://arxiv.org/abs/2305.03726</guid>
<content:encoded><![CDATA[
arXiv:2305.03726v2 Announce Type: replace-cross 
Abstract: Recent advances in Large Multimodal Models (LMMs) have unveiled great potential as visual assistants. However, most existing works focus on responding to individual instructions or using previous dialogues for contextual understanding. There is little discussion on employing both images and text as in-context examples to enhance the instruction following capability.
  To bridge this gap, we introduce the \textbf{Otter} model to leverage both textual and visual in-context examples for instruction tuning. Specifically, Otter builds upon Flamingo with Perceiver architecture, and has been instruction tuned for general purpose multi-modal assistant. Otter seamlessly processes multi-modal inputs, supporting modalities including text, multiple images, and dynamic video content. To support the training of Otter, we present the \textbf{MIMIC-IT} (\textbf{M}ult\textbf{I}-\textbf{M}odal \textbf{I}n-\textbf{C}ontext \textbf{I}nstruction \textbf{T}uning) dataset, which encompasses over 3 million multi-modal instruction-response pairs, including approximately 2.2 million unique instructions across a broad spectrum of images and videos. MIMIC-IT has been carefully curated to feature a diverse array of in-context examples for each entry.
  Comprehensive evaluations suggest that instruction tuning with these in-context examples substantially enhances model convergence and generalization capabilities. Notably, the extensive scenario coverage provided by the MIMIC-IT dataset empowers the Otter model to excel in tasks involving complex video and multi-image understanding.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM2TEA: An Agentic AI Designer for Discovery with Generative Evolutionary Multitasking</title>
<link>https://arxiv.org/abs/2406.14917</link>
<guid>https://arxiv.org/abs/2406.14917</guid>
<content:encoded><![CDATA[
arXiv:2406.14917v3 Announce Type: replace-cross 
Abstract: This paper presents LLM2TEA, a Large Language Model (LLM) driven MultiTask Evolutionary Algorithm, representing the first agentic AI designer of its kind operating with generative evolutionary multitasking (GEM). LLM2TEA enables the crossbreeding of solutions from multiple domains, fostering novel solutions that transcend disciplinary boundaries. Of particular interest is the ability to discover designs that are both novel and conforming to real-world physical specifications. LLM2TEA comprises an LLM to generate genotype samples from text prompts describing target objects, a text-to-3D generative model to produce corresponding phenotypes, a classifier to interpret its semantic representations, and a computational simulator to assess its physical properties. Novel LLM-based multitask evolutionary operators are introduced to guide the search towards high-performing, practically viable designs. Experimental results in conceptual design optimization validate the effectiveness of LLM2TEA, showing 97% to 174% improvements in the diversity of novel designs over the current text-to-3D baseline. Moreover, over 73% of the generated designs outperform the top 1% of designs produced by the text-to-3D baseline in terms of physical performance. The designs produced by LLM2TEA are not only aesthetically creative but also functional in real-world contexts. Several of these designs have been successfully 3D printed, demonstrating the ability of our approach to transform AI-generated outputs into tangible, physical designs. These designs underscore the potential of LLM2TEA as a powerful tool for complex design optimization and discovery, capable of producing novel and physically viable designs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models</title>
<link>https://arxiv.org/abs/2408.10631</link>
<guid>https://arxiv.org/abs/2408.10631</guid>
<content:encoded><![CDATA[
arXiv:2408.10631v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have seen substantial growth, necessitating efficient model pruning techniques. Existing post-training pruning methods primarily measure weight importance in converged dense models, often overlooking changes in weight significance during the pruning process, leading to performance degradation. To address this issue, we present LLM-Barber (Block-Aware Rebuilder for Sparsity Mask in One-Shot), a novel one-shot pruning framework that rebuilds the sparsity mask of pruned models without any retraining or weight reconstruction. LLM-Barber incorporates block-aware error optimization across Self-Attention and MLP blocks, facilitating global performance optimization. We are the first to employ the product of weights and gradients as a pruning metric in the context of LLM post-training pruning. This enables accurate identification of weight importance in massive models and significantly reduces computational complexity compared to methods using secondorder information. Our experiments show that LLM-Barber efficiently prunes models from LLaMA and OPT families (7B to 13B) on a single A100 GPU in just 30 minutes, achieving state-of-the-art results in both perplexity and zero-shot performance across various language benchmarks. Code is available at https://github.com/YupengSu/LLM-Barber.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Everything is a Video: Unifying Modalities through Next-Frame Prediction</title>
<link>https://arxiv.org/abs/2411.10503</link>
<guid>https://arxiv.org/abs/2411.10503</guid>
<content:encoded><![CDATA[
arXiv:2411.10503v2 Announce Type: replace-cross 
Abstract: Multimodal learning, which involves integrating information from various modalities such as text, images, audio, and video, is pivotal for numerous complex tasks like visual question answering, cross-modal retrieval, and caption generation. Traditional approaches rely on modality-specific encoders and late fusion techniques, which can hinder scalability and flexibility when adapting to new tasks or modalities. To address these limitations, we introduce a novel framework that extends the concept of task reformulation beyond natural language processing (NLP) to multimodal learning. We propose to reformulate diverse multimodal tasks into a unified next-frame prediction problem, allowing a single model to handle different modalities without modality-specific components. This method treats all inputs and outputs as sequential frames in a video, enabling seamless integration of modalities and effective knowledge transfer across tasks. Our approach is evaluated on a range of tasks, including text-to-text, image-to-text, video-to-video, video-to-text, and audio-to-text, demonstrating the model's ability to generalize across modalities with minimal adaptation. We show that task reformulation can significantly simplify multimodal model design across various tasks, laying the groundwork for more generalized multimodal foundation models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models</title>
<link>https://arxiv.org/abs/2412.05167</link>
<guid>https://arxiv.org/abs/2412.05167</guid>
<content:encoded><![CDATA[
arXiv:2412.05167v2 Announce Type: replace-cross 
Abstract: Large Audio-Language Models (LALMs), such as GPT-4o, have recently unlocked audio dialogue capabilities, enabling direct spoken exchanges with humans. The potential of LALMs broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we firstly propose the evaluation of ambiguity handling in audio dialogues that expresses different intentions beyond the same literal meaning of sentences, e.g., "Really!?" with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments on 16 LALMs, our analysis reveals that existing LALMs struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones. The benchmark is available at https://adu-bench.github.io/.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference learning made easy: Everything should be understood through win rate</title>
<link>https://arxiv.org/abs/2502.10505</link>
<guid>https://arxiv.org/abs/2502.10505</guid>
<content:encoded><![CDATA[
arXiv:2502.10505v2 Announce Type: replace-cross 
Abstract: Preference learning, or the task of aligning generative models to preference comparison data, has yet to reach the conceptual maturity of classification, density estimation, etc. To close this gap, this work presents a framework to understand preference learning starting from the sampling distribution of pairwise preference data. First, we prove that the only evaluation of a generative model that respects both preferences and prevalences in the data distribution is a form of win rate, justifying win rate as the focal point to understand preference learning. We then analyze preference learning methods as win rate optimization (WRO) or non-WRO. We present novel instances of WRO beyond existing examples (RLHF, NLHF) and identify two key theoretical benefits of all such methods. We prove that common non-WRO methods like DPO and SFT on preferred samples lack these properties and suggest ways to mitigate such theoretical limitations. We also show that WRO underperforms in practice due optimization difficulties and that optimization success predicts performance better than choices which affect the objective's solution. Our analysis highlights best practices for existing methods and provides recommendations for future research, guided by the principle that one should either align non-WRO methods more closely with WRO or improve the optimization of WRO objectives.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protecting Users From Themselves: Safeguarding Contextual Privacy in Interactions with Conversational Agents</title>
<link>https://arxiv.org/abs/2502.18509</link>
<guid>https://arxiv.org/abs/2502.18509</guid>
<content:encoded><![CDATA[
arXiv:2502.18509v2 Announce Type: replace-cross 
Abstract: Conversational agents are increasingly woven into individuals' personal lives, yet users often underestimate the privacy risks associated with them. The moment users share information with these agents-such as large language models (LLMs)-their private information becomes vulnerable to exposure. In this paper, we characterize the notion of contextual privacy for user interactions with LLM-based Conversational Agents (LCAs). It aims to minimize privacy risks by ensuring that users (sender) disclose only information that is both relevant and necessary for achieving their intended goals when interacting with LCAs (untrusted receivers). Through a formative design user study, we observe how even "privacy-conscious" users inadvertently reveal sensitive information through indirect disclosures. Based on insights from this study, we propose a locally deployable framework that operates between users and LCAs, identifying and reformulating out-of-context information in user prompts. Our evaluation using examples from ShareGPT shows that lightweight models can effectively implement this framework, achieving strong gains in contextual privacy while preserving the user's intended interaction goals. Notably, about 76% of participants in our human evaluation preferred the reformulated prompts over the original ones, validating the usability and effectiveness of contextual privacy in our proposed framework. We opensource the code at https://github.com/IBM/contextual-privacy-LLM.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge</title>
<link>https://arxiv.org/abs/2503.04036</link>
<guid>https://arxiv.org/abs/2503.04036</guid>
<content:encoded><![CDATA[
arXiv:2503.04036v3 Announce Type: replace-cross 
Abstract: Data watermarking in language models injects traceable signals, such as specific token sequences or stylistic patterns, into copyrighted text, allowing copyright holders to track and verify training data ownership. Previous data watermarking techniques primarily focus on effective memorization during pretraining, while overlooking challenges that arise in other stages of the LLM lifecycle, such as the risk of watermark filtering during data preprocessing and verification difficulties due to API-only access. To address these challenges, we propose a novel data watermarking approach that injects plausible yet fictitious knowledge into training data using generated passages describing a fictitious entity and its associated attributes. Our watermarks are designed to be memorized by the LLM through seamlessly integrating in its training data, making them harder to detect lexically during preprocessing. We demonstrate that our watermarks can be effectively memorized by LLMs, and that increasing our watermarks' density, length, and diversity of attributes strengthens their memorization. We further show that our watermarks remain effective after continual pretraining and supervised finetuning. Finally, we show that our data watermarks can be evaluated even under API-only access via question answering.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Synthetic Image Detection through Diffusion Timestep Ensembling</title>
<link>https://arxiv.org/abs/2503.06201</link>
<guid>https://arxiv.org/abs/2503.06201</guid>
<content:encoded><![CDATA[
arXiv:2503.06201v2 Announce Type: replace-cross 
Abstract: Recent advances in diffusion models have enabled the creation of deceptively real images, posing significant security risks when misused. In this study, we empirically show that different timesteps of DDIM inversion reveal varying subtle distinctions between synthetic and real images that are extractable for detection, in the forms of such as Fourier power spectrum high-frequency discrepancies and inter-pixel variance distributions. Based on these observations, we propose a novel synthetic image detection method that directly utilizes features of intermediately noised images by training an ensemble on multiple noised timesteps, circumventing conventional reconstruction-based strategies. To enhance human comprehension, we introduce a metric-grounded explanation generation and refinement module to identify and explain AI-generated flaws. Additionally, we construct the GenHard and GenExplain benchmarks to provide detection samples of greater difficulty and high-quality rationales for fake images. Extensive experiments show that our method achieves state-of-the-art performance with 98.91% and 95.89% detection accuracy on regular and challenging samples respectively, and demonstrates generalizability and robustness. Our code and datasets are available at https://github.com/Shadowlized/ESIDE.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines</title>
<link>https://arxiv.org/abs/2504.07840</link>
<guid>https://arxiv.org/abs/2504.07840</guid>
<content:encoded><![CDATA[
arXiv:2504.07840v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have transformed human-computer interaction by enabling natural language-based communication with AI-powered chatbots. These models are designed to be intuitive and user-friendly, allowing users to articulate requests with minimal effort. However, despite their accessibility, studies reveal that users often struggle with effective prompting, resulting in inefficient responses. Existing research has highlighted both the limitations of LLMs in interpreting vague or poorly structured prompts and the difficulties users face in crafting precise queries. This study investigates learner-AI interactions through an educational experiment in which participants receive structured guidance on effective prompting. We introduce and compare three types of prompting guidelines: a task-specific framework developed through a structured methodology and two baseline approaches. To assess user behavior and prompting efficacy, we analyze a dataset of 642 interactions from 107 users. Using Von NeuMidas, an extended pragmatic annotation schema for LLM interaction analysis, we categorize common prompting errors and identify recurring behavioral patterns. We then evaluate the impact of different guidelines by examining changes in user behavior, adherence to prompting strategies, and the overall quality of AI-generated responses. Our findings provide a deeper understanding of how users engage with LLMs and the role of structured prompting guidance in enhancing AI-assisted communication. By comparing different instructional frameworks, we offer insights into more effective approaches for improving user competency in AI interactions, with implications for AI literacy, chatbot usability, and the design of more responsive AI systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants</title>
<link>https://arxiv.org/abs/2504.13887</link>
<guid>https://arxiv.org/abs/2504.13887</guid>
<content:encoded><![CDATA[
arXiv:2504.13887v2 Announce Type: replace-cross 
Abstract: Despite increasing AI chatbot deployment in public discourse, empirical evidence on their capacity to foster intercultural empathy remains limited. Through a randomized experiment, we assessed how different AI deliberation approaches--cross-cultural deliberation (presenting other-culture perspectives), own-culture deliberation (representing participants' own culture), and non-deliberative control--affect intercultural empathy across American and Latin American participants. Cross-cultural deliberation increased intercultural empathy among American participants through positive emotional engagement, but produced no such effects for Latin American participants, who perceived AI responses as culturally inauthentic despite explicit prompting to represent their cultural perspectives. Our analysis of participant-driven feedback, where users directly flagged and explained culturally inappropriate AI responses, revealed systematic gaps in AI's representation of Latin American contexts that persist despite sophisticated prompt engineering. These findings demonstrate that current approaches to AI cultural alignment--including linguistic adaptation and explicit cultural prompting--cannot fully address deeper representational asymmetries in AI systems. Our work advances both deliberation theory and AI alignment research by revealing how the same AI system can simultaneously promote intercultural understanding for one cultural group while failing for another, with critical implications for designing equitable AI systems for cross-cultural democratic discourse.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoLibra: Agent Metric Induction from Open-Ended Feedback</title>
<link>https://arxiv.org/abs/2505.02820</link>
<guid>https://arxiv.org/abs/2505.02820</guid>
<content:encoded><![CDATA[
arXiv:2505.02820v2 Announce Type: replace-cross 
Abstract: Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback e.g. "If you find that the button is disabled, don't click it again", or "This agent has too much autonomy to decide what to do on its own" into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent's behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: "coverage" and "redundancy". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra's ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation</title>
<link>https://arxiv.org/abs/2505.14351</link>
<guid>https://arxiv.org/abs/2505.14351</guid>
<content:encoded><![CDATA[
arXiv:2505.14351v2 Announce Type: replace-cross 
Abstract: Tibetan is a low-resource language with minimal parallel speech corpora spanning its three major dialects-\"U-Tsang, Amdo, and Kham-limiting progress in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot, multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel dialectal speech from limited reference audio and explicit dialect labels. Our method features a novel speaker-dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects while preserving speaker identity. Extensive objective and subjective evaluations demonstrate that FMSD-TTS significantly outperforms baselines in both dialectal expressiveness and speaker similarity. We further validate the quality and utility of the synthesized speech through a challenging speech-to-speech dialect conversion task. Our contributions include: (1) a novel few-shot TTS system tailored for Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source evaluation toolkit for standardized assessment of speaker similarity, dialect consistency, and audio quality.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs</title>
<link>https://arxiv.org/abs/2505.14699</link>
<guid>https://arxiv.org/abs/2505.14699</guid>
<content:encoded><![CDATA[
arXiv:2505.14699v2 Announce Type: replace-cross 
Abstract: The automatic analysis of document layouts in digital-born PDF documents remains a challenging problem due to the heterogeneous arrangement of textual and nontextual elements and the imprecision of the textual metadata in the Portable Document Format. In this work, we benchmark Graph Neural Network (GNN) architectures for the task of fine-grained layout classification of text blocks from digital native documents. We introduce two graph construction structures: a k-closest-neighbor graph and a fully connected graph, and generate node features via pre-trained text and vision models, thus avoiding manual feature engineering. Three experimental frameworks are evaluated: single-modality (text or visual), concatenated multimodal, and dual-branch multimodal. We evaluated four foundational GNN models and compared them with the baseline. Our experiments are specifically conducted on a rich dataset of public affairs documents that includes more than 20 sources (e.g., regional and national-level official gazettes), 37K PDF documents, with 441K pages in total. Our results demonstrate that GraphSAGE operating on the k-closest-neighbor graph in a dual-branch configuration achieves the highest per-class and overall accuracy, outperforming the baseline in some sources. These findings confirm the importance of local layout relationships and multimodal fusion exploited through GNNs for the analysis of native digital document layouts.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions</title>
<link>https://arxiv.org/abs/2507.02087</link>
<guid>https://arxiv.org/abs/2507.02087</guid>
<content:encoded><![CDATA[
arXiv:2507.02087v2 Announce Type: replace-cross 
Abstract: The use of large language models (LLMs) in hiring promises to streamline candidate screening, but it also raises serious concerns regarding accuracy and algorithmic bias where sufficient safeguards are not in place. In this work, we benchmark several state-of-the-art foundational LLMs - including models from OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our proprietary domain-specific hiring model (Match Score) for job candidate matching. We evaluate each model's predictive accuracy (ROC AUC, Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis across declared gender, race, and intersectional subgroups). Our experiments on a dataset of roughly 10,000 real-world recent candidate-job pairs show that Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs 0.77) and achieves significantly more equitable outcomes across demographic groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957 (near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the intersectionals, respectively). We discuss why pretraining biases may cause LLMs with insufficient safeguards to propagate societal biases in hiring scenarios, whereas a bespoke supervised model can more effectively mitigate these biases. Our findings highlight the importance of domain-specific modeling and bias auditing when deploying AI in high-stakes domains such as hiring, and caution against relying on off-the-shelf LLMs for such tasks without extensive fairness safeguards. Furthermore, we show with empirical evidence that there shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a well-designed algorithm can achieve both accuracy in hiring and fairness in outcomes.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assemble Your Crew: Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation</title>
<link>https://arxiv.org/abs/2507.18224</link>
<guid>https://arxiv.org/abs/2507.18224</guid>
<content:encoded><![CDATA[
arXiv:2507.18224v2 Announce Type: replace-cross 
Abstract: Multi-agent systems (MAS) based on large language models (LLMs) have emerged as a powerful solution for dealing with complex problems across diverse domains. The effectiveness of MAS is critically dependent on its collaboration topology, which has become a focal point for automated design research. However, existing approaches are fundamentally constrained by their reliance on a template graph modification paradigm with a predefined set of agents and hard-coded interaction structures, significantly limiting their adaptability to task-specific requirements. To address these limitations, we reframe MAS design as a conditional autoregressive graph generation task, where both the system composition and structure are designed jointly. We propose ARG-Designer, a novel autoregressive model that operationalizes this paradigm by constructing the collaboration graph from scratch. Conditioned on a natural language task query, ARG-Designer sequentially and dynamically determines the required number of agents, selects their appropriate roles from an extensible pool, and establishes the optimal communication links between them. This generative approach creates a customized topology in a flexible and extensible manner, precisely tailored to the unique demands of different tasks. Extensive experiments across six diverse benchmarks demonstrate that ARG-Designer not only achieves state-of-the-art performance but also enjoys significantly greater token efficiency and enhanced extensibility. The source code of ARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model</title>
<link>https://arxiv.org/abs/2507.08013</link>
<guid>https://arxiv.org/abs/2507.08013</guid>
<content:encoded><![CDATA[
<div> pretrained BERT model, biomedical natural language processing, MedicalBERT, domain-specific vocabulary, transfer learning <br />
Summary: <br />
Recent advancements in natural language processing have been fueled by pretrained language models such as BERT, RoBERTa, T5, and GPT. However, these models struggle with understanding biomedical literature due to its domain-specific terminology. To address this, the MedicalBERT model was introduced, trained on a large biomedical dataset with domain-specific vocabulary to improve comprehension. MedicalBERT outperforms other BERT-based models like BioBERT, SciBERT, and ClinicalBERT across various tasks such as named entity recognition, relation extraction, question answering, sentence similarity, and document classification. Performance metrics show that MedicalBERT exceeds the general-purpose BERT model by an average of 5.67% on all evaluated tasks. This study highlights the effectiveness of leveraging pretrained BERT models for medical NLP tasks and demonstrates the power of transfer learning techniques in capturing domain-specific information. <br /> <div>
arXiv:2507.08013v2 Announce Type: replace 
Abstract: Recent advances in natural language processing (NLP) have been driven bypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel at understanding complex texts, but biomedical literature, withits domain-specific terminology, poses challenges that models likeWord2Vec and bidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5, despite capturing context, fall short in tasks needingbidirectional understanding, unlike BERT. Addressing this, we proposedMedicalBERT, a pretrained BERT model trained on a large biomedicaldataset and equipped with domain-specific vocabulary that enhances thecomprehension of biomedical terminology. MedicalBERT model is furtheroptimized and fine-tuned to address diverse tasks, including named entityrecognition, relation extraction, question answering, sentence similarity, anddocument classification. Performance metrics such as the F1-score,accuracy, and Pearson correlation are employed to showcase the efficiencyof our model in comparison to other BERT-based models such as BioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost of the benchmarks, and surpasses the general-purpose BERT model by5.67% on average across all the tasks evaluated respectively. This work alsounderscores the potential of leveraging pretrained BERT models for medicalNLP tasks, demonstrating the effectiveness of transfer learning techniques incapturing domain-specific information.
  (PDF) MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model. Available from: https://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model [accessed Jul 06 2025].
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement</title>
<link>https://arxiv.org/abs/2507.18742</link>
<guid>https://arxiv.org/abs/2507.18742</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, Specification Self-Correction (SSC), test-time framework, dynamic repair, model behavior.<br />
Summary: Language models can exploit flaws in specifications to achieve high scores without fulfilling the user's intent. The Specification Self-Correction (SSC) framework allows models to identify and correct flaws in their guiding specifications. Through a multi-step inference process, the model generates a response based on the specification, critiques its output, revises the specification to remove loopholes, and generates a final response. Experiments across creative writing and coding tasks show that SSC reduces the vulnerability to gaming specifications by over 90%. This repair process occurs at inference time without weight modification, leading to more aligned model behavior.<br /><br />Summary: <div>
arXiv:2507.18742v1 Announce Type: new 
Abstract: Language models (LMs) are susceptible to in-context reward hacking, where they exploit flaws in tainted or faulty written specifications or rubrics to achieve high scores without fulfilling the user's true intent. We introduce Specification Self-Correction (SSC), a novel, test-time framework that enables an LM to identify and correct flaws within its own guiding specification. SSC employs a multi-step inference process where the model first generates a response based on a potentially tainted specification, critiques its output, and then revises the specification itself to remove the exploitable loophole. A final, more robust response is then generated using this self-corrected specification. Across experiments spanning creative writing and agentic coding tasks with several LMs, we demonstrate that while models initially game tainted specifications in 50-70\% of cases, the SSC process reduces this vulnerability by over 90\%. This dynamic repair occurs at inference time, requires no weight modification, and leads to more robustly aligned model behavior. Code at https://github.com/vicgalle/specification-self-correction .
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Orthographic Consistency in Multilingual Embedding Models for Text Classification in Arabic-Script Languages</title>
<link>https://arxiv.org/abs/2507.18762</link>
<guid>https://arxiv.org/abs/2507.18762</guid>
<content:encoded><![CDATA[
<div> mBERT, XLM-RoBERTa, Arabic-script languages, AS-RoBERTa, pre-training<br />
Summary:<br />
The paper introduces the Arabic Script RoBERTa (AS-RoBERTa) family of models, specifically designed for Arabic-script languages such as Kurdish Sorani, Arabic, Persian, and Urdu. These models are pre-trained on language-specific corpora to capture unique script features and patterns. When fine-tuned on classification tasks, AS-RoBERTa outperforms general-purpose models like mBERT and XLM-RoBERTa. An ablation study confirms the importance of script-focused pre-training for improved performance. Error analysis demonstrates how shared script traits and domain-specific content impact model accuracy. The results emphasize the significance of script-aware specialization for Arabic script languages and suggest the need for further research on language-specific pre-training strategies. <br /> 
Summary: <div>
arXiv:2507.18762v1 Announce Type: new 
Abstract: In natural language processing, multilingual models like mBERT and XLM-RoBERTa promise broad coverage but often struggle with languages that share a script yet differ in orthographic norms and cultural context. This issue is especially notable in Arabic-script languages such as Kurdish Sorani, Arabic, Persian, and Urdu. We introduce the Arabic Script RoBERTa (AS-RoBERTa) family: four RoBERTa-based models, each pre-trained on a large corpus tailored to its specific language. By focusing pre-training on language-specific script features and statistics, our models capture patterns overlooked by general-purpose models. When fine-tuned on classification tasks, AS-RoBERTa variants outperform mBERT and XLM-RoBERTa by 2 to 5 percentage points. An ablation study confirms that script-focused pre-training is central to these gains. Error analysis using confusion matrices shows how shared script traits and domain-specific content affect performance. Our results highlight the value of script-aware specialization for languages using the Arabic script and support further work on pre-training strategies rooted in script and language specificity.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ylmmcl at Multilingual Text Detoxification 2025: Lexicon-Guided Detoxification and Classifier-Gated Rewriting</title>
<link>https://arxiv.org/abs/2507.18769</link>
<guid>https://arxiv.org/abs/2507.18769</guid>
<content:encoded><![CDATA[
<div> lexicon-guided tagging, sequence-to-sequence model, classifier-based gatekeeping mechanism, multilingual text detoxification, PAN-2025 competition <br />
Summary:<br />
The ylmmcl team introduces a robust multilingual text detoxification pipeline for the PAN-2025 competition. Their solution combines lexicon-guided tagging, a fine-tuned sequence-to-sequence model, and an iterative classifier-based gatekeeping mechanism. By leveraging explicit toxic word annotations and a multilingual toxic lexicon, the approach achieves higher detoxification accuracy and cross-lingual generalization. The final model achieves a high STA of 0.922 and an average J score of 0.612 for toxic inputs in both development and test sets. It also scores 0.793 (dev) and 0.787 (test) in xCOMET evaluations, outperforming baseline and backtranslation methods in multiple languages. While there are some trade-offs, the model consistently improves detoxification strength and demonstrates strong generalization in high-resource settings. In the competition, the ylmmcl team achieves ninth place with a score of 0.612. <br /><br /> <div>
arXiv:2507.18769v1 Announce Type: new 
Abstract: In this work, we introduce our solution for the Multilingual Text Detoxification Task in the PAN-2025 competition for the ylmmcl team: a robust multilingual text detoxification pipeline that integrates lexicon-guided tagging, a fine-tuned sequence-to-sequence model (s-nlp/mt0-xl-detox-orpo) and an iterative classifier-based gatekeeping mechanism. Our approach departs from prior unsupervised or monolingual pipelines by leveraging explicit toxic word annotation via the multilingual_toxic_lexicon to guide detoxification with greater precision and cross-lingual generalization. Our final model achieves the highest STA (0.922) from our previous attempts, and an average official J score of 0.612 for toxic inputs in both the development and test sets. It also achieved xCOMET scores of 0.793 (dev) and 0.787 (test). This performance outperforms baseline and backtranslation methods across multiple languages, and shows strong generalization in high-resource settings (English, Russian, French). Despite some trade-offs in SIM, the model demonstrates consistent improvements in detoxification strength. In the competition, our team achieved ninth place with a score of 0.612.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Code-Mixing in LLMs Across 18 Languages</title>
<link>https://arxiv.org/abs/2507.18791</link>
<guid>https://arxiv.org/abs/2507.18791</guid>
<content:encoded><![CDATA[
<div> code-mixing, natural language processing, large language models, synthetic data, performance <br />
Summary:<br />
The paper discusses the challenges of code-mixing in natural language processing and the limitations of existing benchmarks. It evaluates the performance of large language models on code-mixed data involving 18 languages from seven language families. The study also introduces a novel method for generating synthetic code-mixed texts using word substitution and GPT-4 prompting. The analysis shows that current LLMs underperform on code-mixed datasets across multiple language families, suggesting that improvements in training data size, model scale, and few-shot learning could enhance their capabilities. <div>
arXiv:2507.18791v1 Announce Type: new 
Abstract: Code-mixing, the practice of switching between languages within a conversation, presents unique challenges for traditional natural language processing. Existing benchmarks, such as LinCE and GLUECoS, are limited by narrow language pairings and tasks, failing to adequately evaluate the code-mixing capabilities of large language models (LLMs). Despite the significance of code-mixing for multilingual users, research on LLMs in this context remains limited. Additionally, current methods for generating code-mixed data are underdeveloped. In this paper, we conduct a comprehensive evaluation of LLMs' performance on code-mixed data across 18 languages from seven language families. We also propose a novel approach for generating synthetic code-mixed texts by combining word substitution with GPT-4 prompting. Our analysis reveals consistent underperformance of LLMs on code-mixed datasets involving multiple language families. We suggest that improvements in training data size, model scale, and few-shot learning could enhance their performance.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CueBuddy: helping non-native English speakers navigate English-centric STEM education</title>
<link>https://arxiv.org/abs/2507.18827</link>
<guid>https://arxiv.org/abs/2507.18827</guid>
<content:encoded><![CDATA[
<div> Keywords: STEM, Global South, speech translation, multilingual glossary, CueBuddy

Summary: 
CueBuddy addresses the issue of students in STEM classes, especially in the Global South, struggling with English technical terms despite having scientific prerequisites. The system provides real-time "lexical cues" by spotting key technical keywords and looking them up in a multilingual glossary during live lectures. This helps students stay on track with complex English jargon without interrupting their focus on the lecture. Unlike traditional speech translation models, CueBuddy is cost-effective and focuses on technical content. The system aims to bridge the gap for students who have had their course prerequisites in lower resource languages, allowing them to follow lectures more effectively. However, there are limitations to the current approach, and future extensions will need to address these challenges to make CueBuddy more efficient and user-friendly for a wider audience. 

<br /><br />Summary: <div>
arXiv:2507.18827v1 Announce Type: new 
Abstract: Students across the world in STEM classes, especially in the Global South, fall behind their peers who are more fluent in English, despite being at par with them in terms of scientific prerequisites. While many of them are able to follow everyday English at ease, key terms in English stay challenging. In most cases, such students have had most of their course prerequisites in a lower resource language. Live speech translation to lower resource languages is a promising area of research, however, models for speech translation can be too expensive on a large scale and often struggle with technical content. In this paper, we describe CueBuddy, which aims to remediate these issues by providing real-time "lexical cues" through technical keyword spotting along real-time multilingual glossary lookup to help students stay up to speed with complex English jargon without disrupting their concentration on the lecture. We also describe the limitations and future extensions of our approach.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning</title>
<link>https://arxiv.org/abs/2507.18857</link>
<guid>https://arxiv.org/abs/2507.18857</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-augmented generation, fine-tuning, reasoning, QA benchmarks, factuality 

Summary: 
PrismRAG is a new efficient fine-tuning framework designed to address challenges faced by retrieval-augmented generation models when dealing with confusing or semi-relevant passages in retrieved contexts. The model is trained on distractor-aware QA pairs, which include both relevant evidence and subtle distractor passages to improve performance. PrismRAG aims to instill reasoning-centric habits in language models, enabling them to plan, rationalize, and synthesize information without explicit human instructions. Evaluation on 12 open-book RAG QA benchmarks across various domains shows that PrismRAG significantly enhances factuality and outperforms existing solutions. The model exhibits a 5.4% improvement in average factuality, showcasing its effectiveness in generating accurate and contextually relevant responses. This approach demonstrates the potential for enhancing natural language understanding and reasoning capabilities in AI systems.<br /><br />Summary: <div>
arXiv:2507.18857v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) often falls short when retrieved context includes confusing semi-relevant passages, or when answering questions require deep contextual understanding and reasoning. We propose an efficient fine-tuning framework, called PrismRAG, that (i) trains the model with distractor-aware QA pairs mixing gold evidence with subtle distractor passages, and (ii) instills reasoning-centric habits that make the LLM plan, rationalize, and synthesize without relying on extensive human engineered instructions. Evaluated across 12 open-book RAG QA benchmarks spanning diverse application domains and scenarios, PrismRAG improves average factuality by 5.4%, outperforming state-of-the-art solutions.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service</title>
<link>https://arxiv.org/abs/2507.18884</link>
<guid>https://arxiv.org/abs/2507.18884</guid>
<content:encoded><![CDATA[
<div> Keywords: dialogue agent, e-commerce, self-evolving, large language models, reinforcement learning 

Summary: 
MindFlow+ is a novel dialogue agent designed for e-commerce customer service, integrating large language models (LLMs) with imitation and reinforcement learning. The agent utilizes tool-augmented demonstration construction and reward-conditioned data modeling to improve task-specific behavior and align responses with goals. The AI Contribution Ratio metric is introduced to measure the model's involvement in dialogue generation. Real-world experiments demonstrate that MindFlow+ outperforms traditional systems in contextual relevance, flexibility, and task accuracy, showcasing the effectiveness of combining LLMs, tool reasoning, and reward-guided learning in building context-aware dialogue systems. <br /><br />Summary: <div>
arXiv:2507.18884v1 Announce Type: new 
Abstract: High-quality dialogue is crucial for e-commerce customer service, yet traditional intent-based systems struggle with dynamic, multi-turn interactions. We present MindFlow+, a self-evolving dialogue agent that learns domain-specific behavior by combining large language models (LLMs) with imitation learning and offline reinforcement learning (RL). MindFlow+ introduces two data-centric mechanisms to guide learning: tool-augmented demonstration construction, which exposes the model to knowledge-enhanced and agentic (ReAct-style) interactions for effective tool use; and reward-conditioned data modeling, which aligns responses with task-specific goals using reward signals. To evaluate the model's role in response generation, we introduce the AI Contribution Ratio, a novel metric quantifying AI involvement in dialogue. Experiments on real-world e-commerce conversations show that MindFlow+ outperforms strong baselines in contextual relevance, flexibility, and task accuracy. These results demonstrate the potential of combining LLMs tool reasoning, and reward-guided learning to build domain-specialized, context-aware dialogue systems.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NUTMEG: Separating Signal From Noise in Annotator Disagreement</title>
<link>https://arxiv.org/abs/2507.18890</link>
<guid>https://arxiv.org/abs/2507.18890</guid>
<content:encoded><![CDATA[
<div> Keywords: NLP, human-labeled data, annotator disagreements, Bayesian model, systematic disagreements

Summary: 
NLP models rely on human-labeled data, often sourced from crowdsourcing platforms with varied annotator skills. Traditional aggregation methods for conflicting annotations assume disagreements are errors. However, recent research suggests that annotator disagreements can contain valuable signal. The NUTMEG Bayesian model introduced in this work leverages annotator backgrounds to filter out noisy annotations while preserving systematic disagreements. Synthetic data experiments demonstrate the effectiveness of NUTMEG in recovering ground-truth from systematically disagreed annotations compared to traditional methods. The model's performance is influenced by factors such as subpopulation sizes, disagreement rates, and spam rates. Downstream models trained on NUTMEG-aggregated data exhibit superior performance. This study underscores the significance of considering annotator competence and systematic disagreements when utilizing human-labeled data for training NLP models. 

<br /><br />Summary: <div>
arXiv:2507.18890v1 Announce Type: new 
Abstract: NLP models often rely on human-labeled data for training and evaluation. Many approaches crowdsource this data from a large number of annotators with varying skills, backgrounds, and motivations, resulting in conflicting annotations. These conflicts have traditionally been resolved by aggregation methods that assume disagreements are errors. Recent work has argued that for many tasks annotators may have genuine disagreements and that variation should be treated as signal rather than noise. However, few models separate signal and noise in annotator disagreement. In this work, we introduce NUTMEG, a new Bayesian model that incorporates information about annotator backgrounds to remove noisy annotations from human-labeled training data while preserving systematic disagreements. Using synthetic data, we show that NUTMEG is more effective at recovering ground-truth from annotations with systematic disagreement than traditional aggregation methods. We provide further analysis characterizing how differences in subpopulation sizes, rates of disagreement, and rates of spam affect the performance of our model. Finally, we demonstrate that downstream models trained on NUTMEG-aggregated data significantly outperform models trained on data from traditionally aggregation methods. Our results highlight the importance of accounting for both annotator competence and systematic disagreements when training on human-labeled data.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science Research?</title>
<link>https://arxiv.org/abs/2507.18901</link>
<guid>https://arxiv.org/abs/2507.18901</guid>
<content:encoded><![CDATA[
<div> Keywords: reproducibility assessment, social science papers, agentic AI agents, REPRO-Bench, REPRO-Agent

Summary:
The study focuses on automating the reproducibility assessment of social science papers using AI agents, aiming to address the challenges of manual assessment. Existing benchmarks for reproducing research papers lack diversity and do not consider real-world scenarios adequately. To overcome these limitations, the researchers introduce REPRO-Bench, a collection of task instances representing social science papers with publicly available reproduction reports. AI agents are evaluated on their ability to assess reproducibility based on the original paper PDF and reproduction package. The study evaluates three AI agents on REPRO-Bench, with the best-performing agent achieving an accuracy of 21.4%. A new agent, REPRO-Agent, improves the accuracy by 71%. The findings suggest the need for more advanced AI agents to automate real-world reproducibility assessments. REPRO-Bench is made publicly available for future research. 

<br /><br />Summary: <div>
arXiv:2507.18901v1 Announce Type: new 
Abstract: Assessing the reproducibility of social science papers is essential for promoting rigor in research processes, but manual assessment is costly. With recent advances in agentic AI systems (i.e., AI agents), we seek to evaluate their capability to automate this process. However, existing benchmarks for reproducing research papers (1) focus solely on reproducing results using provided code and data without assessing their consistency with the paper, (2) oversimplify real-world scenarios, and (3) lack necessary diversity in data formats and programming languages. To address these issues, we introduce REPRO-Bench, a collection of 112 task instances, each representing a social science paper with a publicly available reproduction report. The agents are tasked with assessing the reproducibility of the paper based on the original paper PDF and the corresponding reproduction package. REPRO-Bench features end-to-end evaluation tasks on the reproducibility of social science papers with complexity comparable to real-world assessments. We evaluate three representative AI agents on REPRO-Bench, with the best-performing agent achieving an accuracy of only 21.4%. Building on our empirical analysis, we develop REPRO-Agent, which improves the highest accuracy achieved by existing agents by 71%. We conclude that more advanced AI agents should be developed to automate real-world reproducibility assessment. REPRO-Bench is publicly available at https://github.com/uiuc-kang-lab/REPRO-Bench.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLoW: Select Low-frequency Words! Automatic Dictionary Selection for Translation on Large Language Models</title>
<link>https://arxiv.org/abs/2507.18902</link>
<guid>https://arxiv.org/abs/2507.18902</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Automatic Dictionary Selection, Translation, Token Usage, Frequency Estimation

Summary:<br />
This paper introduces the concept of Automatic Dictionary Selection (ADS) to improve translation for a wide range of languages using Large Language Models (LLMs). The proposed method, Select Low-frequency Words (SLoW), strategically selects dictionaries based on lower frequency words without needing access to training data. Experimental results show that SLoW outperforms strong baselines in translation performance while also saving token usage. Surprisingly, the method does not require actual training data for frequency estimation, making it accessible and effective for enhancing translation with models like ChatGPT and Llama. Overall, ADS with SLoW shows promising results in improving translation efficiency and performance across a diverse set of languages.<br /><br />Summary: <div>
arXiv:2507.18902v1 Announce Type: new 
Abstract: There are more than 7,000 languages around the world, and current Large Language Models (LLMs) only support hundreds of languages. Dictionary-based prompting methods can enhance translation on them, but most methods use all the available dictionaries, which could be expensive. Instead, it will be flexible to have a trade-off between token consumption and translation performance. This paper proposes a novel task called \textbf{A}utomatic \textbf{D}ictionary \textbf{S}election (\textbf{ADS}). The goal of the task is to automatically select which dictionary to use to enhance translation. We propose a novel and effective method which we call \textbf{S}elect \textbf{Lo}w-frequency \textbf{W}ords! (\textbf{SLoW}) which selects those dictionaries that have a lower frequency. Our methods have unique advantages. First, there is no need for access to the training data for frequency estimation (which is usually unavailable). Second, it inherits the advantage of dictionary-based methods, where no additional tuning is required on LLMs. Experimental results on 100 languages from FLORES indicate that SLoW surpasses strong baselines, and it can obviously save token usage, with many languages even surpassing the translation performance of the full dictionary baseline.\footnote{A shocking fact is that there is no need to use the actual training data (often unobtainable) for frequency estimation, and an estimation frequency obtained using public resources is still apparently effective in improving translation with ChatGPT and Llama, and DeepSeek.}\footnote{Code and data available upon publication.}
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language models provide unsafe answers to patient-posed medical questions</title>
<link>https://arxiv.org/abs/2507.18905</link>
<guid>https://arxiv.org/abs/2507.18905</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, chatbots, medical advice, patient safety, physician-led red-teaming

Summary:
This study evaluates the safety of four publicly available chatbots used for medical advice: Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and Llama3-70B by Meta, on the HealthAdvice dataset. A total of 888 responses to 222 patient-posed medical questions are analyzed, revealing significant variations in the rate of problematic and unsafe responses among the chatbots. Results indicate that patients could be at risk of receiving unsafe medical advice from these chatbots, with potentially serious consequences. The study emphasizes the need for further improvements in the clinical safety of large language model chatbots, highlighting the importance of ensuring patient safety in the use of these powerful tools. 

<br /><br />Summary: Millions of patients are using large language model chatbots for medical advice, but this study finds significant differences in safety among four publicly available chatbots. Results show varying rates of problematic and unsafe responses, raising concerns about patient safety. Further work is needed to enhance the clinical safety of these tools and prevent potential harm to patients. <div>
arXiv:2507.18905v1 Announce Type: new 
Abstract: Millions of patients are already using large language model (LLM) chatbots for medical advice on a regular basis, raising patient safety concerns. This physician-led red-teaming study compares the safety of four publicly available chatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and Llama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation framework that enables quantitative and qualitative analysis. In total, 888 chatbot responses are evaluated for 222 patient-posed advice-seeking medical questions on primary care topics spanning internal medicine, women's health, and pediatrics. We find statistically significant differences between chatbots. The rate of problematic responses varies from 21.6 percent (Claude) to 43.2 percent (Llama), with unsafe responses varying from 5 percent (Claude) to 13 percent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the potential to lead to serious patient harm. This study suggests that millions of patients could be receiving unsafe medical advice from publicly available chatbots, and further work is needed to improve the clinical safety of these powerful tools.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions</title>
<link>https://arxiv.org/abs/2507.18910</link>
<guid>https://arxiv.org/abs/2507.18910</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, NLP, Information retrieval, Fusion strategies, Enterprise systems

Summary:
Retrieval-Augmented Generation (RAG) is a significant advancement in natural language processing (NLP) that combines large language models with information retrieval systems to enhance factual grounding and contextual relevance. This systematic review traces the evolution of RAG from early developments in open domain question answering to state-of-the-art implementations across various applications. The motivations behind RAG include mitigating hallucinations and outdated knowledge in parametric models. The review delves into core technical components such as retrieval mechanisms, sequence-to-sequence generation models, and fusion strategies, highlighting key milestones and research trends. Deployment challenges in enterprise systems, such as retrieval of proprietary data, security, and scalability, are addressed. A comparative evaluation of RAG implementations benchmarks performance on retrieval accuracy, generation fluency, latency, and computational efficiency. Persistent challenges like retrieval quality, privacy concerns, and integration overhead are critically assessed, while emerging solutions offer potential for more reliable, efficient, and context-aware knowledge-intensive NLP systems. 

<br /><br />Summary: <div>
arXiv:2507.18910v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) represents a major advancement in natural language processing (NLP), combining large language models (LLMs) with information retrieval systems to enhance factual grounding, accuracy, and contextual relevance. This paper presents a comprehensive systematic review of RAG, tracing its evolution from early developments in open domain question answering to recent state-of-the-art implementations across diverse applications. The review begins by outlining the motivations behind RAG, particularly its ability to mitigate hallucinations and outdated knowledge in parametric models. Core technical components-retrieval mechanisms, sequence-to-sequence generation models, and fusion strategies are examined in detail. A year-by-year analysis highlights key milestones and research trends, providing insight into RAG's rapid growth. The paper further explores the deployment of RAG in enterprise systems, addressing practical challenges related to retrieval of proprietary data, security, and scalability. A comparative evaluation of RAG implementations is conducted, benchmarking performance on retrieval accuracy, generation fluency, latency, and computational efficiency. Persistent challenges such as retrieval quality, privacy concerns, and integration overhead are critically assessed. Finally, the review highlights emerging solutions, including hybrid retrieval approaches, privacy-preserving techniques, optimized fusion strategies, and agentic RAG architectures. These innovations point toward a future of more reliable, efficient, and context-aware knowledge-intensive NLP systems.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining Contextualized Visual Associations from Images for Creativity Understanding</title>
<link>https://arxiv.org/abs/2507.18915</link>
<guid>https://arxiv.org/abs/2507.18915</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, creative captions, image-text retrieval, dataset, abstraction

Summary:
In this study, a novel method is introduced for mining contextualized associations for salient visual elements in images, allowing for the generation of creative captions at varying levels of abstraction. By leveraging this method, the researchers produced a dataset of visual associations and 1.7 million creative captions for images in the MSCOCO dataset. Human evaluation confirmed that these captions maintained visual grounding while exhibiting increasing levels of abstraction. Additionally, fine-tuning a visual encoder on this dataset led to significant improvements in zero-shot image-text retrieval in creative domains such as poetry and metaphor visualization. The dataset, generation code, and models developed in this work are made available for the wider research community to utilize. <div>
arXiv:2507.18915v1 Announce Type: new 
Abstract: Understanding another person's creative output requires a shared language of association. However, when training vision-language models such as CLIP, we rely on web-scraped datasets containing short, predominantly literal, alt-text. In this work, we introduce a method for mining contextualized associations for salient visual elements in an image that can scale to any unlabeled dataset. Given an image, we can use these mined associations to generate high quality creative captions at increasing degrees of abstraction. With our method, we produce a new dataset of visual associations and 1.7m creative captions for the images in MSCOCO. Human evaluation confirms that these captions remain visually grounded while exhibiting recognizably increasing abstraction. Moreover, fine-tuning a visual encoder on this dataset yields meaningful improvements in zero-shot image-text retrieval in two creative domains: poetry and metaphor visualization. We release our dataset, our generation code and our models for use by the broader community.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Cross-Linguistic Disparities in LLMs using Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2507.18918</link>
<guid>https://arxiv.org/abs/2507.18918</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual language models, activation patterns, sparse autoencoders, low-rank adaptation, performance improvement

Summary:<br />
- Multilingual large language models show strong cross-linguistic generalization but struggle with medium to low resource languages on benchmarks.
- Activation patterns in Gemma-2-2B across multiple languages reveal disparities, with medium to low resource languages experiencing lower activations in both early and deep layers.
- Sparse Autoencoders (SAEs) are used to analyze and address these activation disparities.
- Low-Rank Adaptation (LoRA) is applied for activation-aware fine-tuning, leading to significant activation gains for medium to low resource languages while maintaining English retention.
- After fine-tuning, benchmark results show consistent improvements, underscoring activation alignment as a crucial factor in enhancing performance of multilingual LLMs.

<br /><br />Summary: <div>
arXiv:2507.18918v1 Announce Type: new 
Abstract: Multilingual large language models (LLMs) exhibit strong cross-linguistic generalization, yet medium to low resource languages underperform on common benchmarks such as ARC-Challenge, MMLU, and HellaSwag. We analyze activation patterns in Gemma-2-2B across all 26 residual layers and 10 languages: Chinese (zh), Russian (ru), Spanish (es), Italian (it), medium to low resource languages including Indonesian (id), Catalan (ca), Marathi (mr), Malayalam (ml), and Hindi (hi), with English (en) as the reference. Using Sparse Autoencoders (SAEs), we reveal systematic disparities in activation patterns. Medium to low resource languages receive up to 26.27 percent lower activations in early layers, with a persistent gap of 19.89 percent in deeper layers. To address this, we apply activation-aware fine-tuning via Low-Rank Adaptation (LoRA), leading to substantial activation gains, such as 87.69 percent for Malayalam and 86.32 percent for Hindi, while maintaining English retention at approximately 91 percent. After fine-tuning, benchmark results show modest but consistent improvements, highlighting activation alignment as a key factor in enhancing multilingual LLM performance.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaVA-NeuMT: Selective Layer-Neuron Modulation for Efficient Multilingual Multimodal Translation</title>
<link>https://arxiv.org/abs/2507.18940</link>
<guid>https://arxiv.org/abs/2507.18940</guid>
<content:encoded><![CDATA[
<div> multimodal machine translation, multilingual translation, LLaVA-NeuMT, cross-lingual interference, SOTA results
<br />
Summary:
<br />
The article introduces LLaVA-NeuMT, a novel framework for multimodal multilingual translation that addresses challenges like cross-lingual interference. It explicitly models language-specific and language-agnostic representations, utilizing a layer selection mechanism to identify informative layers for different language pairs. Additionally, a neuron-level adaptation strategy dynamically selects language-specific and agnostic neurons to enhance translation quality and reduce redundancy. Through experiments on M3-Multi30K and M3-AmbigCaps datasets, LLaVA-NeuMT outperforms full fine-tuning approaches, achieving state-of-the-art results while fine-tuning only 40% of the model parameters. The analysis provides insights into the importance of selected layers and neurons in multimodal multilingual adaptation, offering an efficient and scalable solution for cross-lingual adaptation in multimodal translation. 
<br /> <div>
arXiv:2507.18940v1 Announce Type: new 
Abstract: Multimodal Machine Translation (MMT) enhances translation quality by incorporating visual context, helping to resolve textual ambiguities. While existing MMT methods perform well in bilingual settings, extending them to multilingual translation remains challenging due to cross-lingual interference and ineffective parameter-sharing strategies. To address this, we propose LLaVA-NeuMT, a novel multimodal multilingual translation framework that explicitly models language-specific and language-agnostic representations to mitigate multilingual interference. Our approach consists of a layer selection mechanism that identifies the most informative layers for different language pairs and a neuron-level adaptation strategy that dynamically selects language-specific and agnostic neurons to improve translation quality while reducing redundancy. We conduct extensive experiments on the M3-Multi30K and M3-AmbigCaps datasets, demonstrating that LLaVA-NeuMT, while fine-tuning only 40\% of the model parameters, surpasses full fine-tuning approaches and ultimately achieves SOTA results on both datasets. Our analysis further provides insights into the importance of selected layers and neurons in multimodal multilingual adaptation, offering an efficient and scalable solution to cross-lingual adaptation in multimodal translation.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Legal Document Summarization: Enhancing Judicial Efficiency through Automation Detection</title>
<link>https://arxiv.org/abs/2507.18952</link>
<guid>https://arxiv.org/abs/2507.18952</guid>
<content:encoded><![CDATA[
<div> Keywords: Legal document summarization, natural language processing, machine learning algorithms, operational efficiency, automation <br />
Summary: <br />
- Legal document summarization is essential for improving judicial efficiency through automation of key information detection. <br />
- State-of-the-art natural language processing techniques are used to extract essential data from legal texts, reducing the burden on legal professionals. <br />
- Advanced machine learning algorithms identify patterns within judicial documents to create precise summaries. <br />
- Comprehensive experiments with actual legal datasets demonstrate the method's capability to generate high-quality summaries and enhance processing times considerably. <br />
- Promising technology-driven strategies emphasize the role of automation in refining judicial processes, allowing legal practitioners to focus on critical analytical and decision-making activities. <br /> <div>
arXiv:2507.18952v1 Announce Type: new 
Abstract: Legal document summarization represents a significant advancement towards improving judicial efficiency through the automation of key information detection. Our approach leverages state-of-the-art natural language processing techniques to meticulously identify and extract essential data from extensive legal texts, which facilitates a more efficient review process. By employing advanced machine learning algorithms, the framework recognizes underlying patterns within judicial documents to create precise summaries that encapsulate the crucial elements. This automation alleviates the burden on legal professionals, concurrently reducing the likelihood of overlooking vital information that could lead to errors. Through comprehensive experiments conducted with actual legal datasets, we demonstrate the capability of our method to generate high-quality summaries while preserving the integrity of the original content and enhancing processing times considerably. The results reveal marked improvements in operational efficiency, allowing legal practitioners to direct their efforts toward critical analytical and decision-making activities instead of manual reviews. This research highlights promising technology-driven strategies that can significantly alter workflow dynamics within the legal sector, emphasizing the role of automation in refining judicial processes.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Similarity Measure for Comparing Conversational Dynamics</title>
<link>https://arxiv.org/abs/2507.18956</link>
<guid>https://arxiv.org/abs/2507.18956</guid>
<content:encoded><![CDATA[
<div> Keyword: conversation dynamics, similarity measure, validation framework, conversational agents, online community<br />
Summary:<br />
The article introduces a new similarity measure for comparing conversations based on their interactional dynamics. By analyzing how replies combine to give a conversation its overall shape, this metric provides a holistic view of conversational quality. The study includes a validation framework to test the measure's robustness and sensitivity to different conversation topics. The goal is to enhance the analysis of conversational data and evaluate conversational agents more effectively. Applying the measure to a large online community, the researchers uncover insights into the role of situational power in conversations. This research fills a gap in automated methods for evaluating and comparing conversations, offering a valuable tool for studying interactional patterns. <div>
arXiv:2507.18956v1 Announce Type: new 
Abstract: The quality of a conversation goes beyond the individual quality of each reply, and instead emerges from how these combine into interactional patterns that give the conversation its distinctive overall "shape". However, there is no robust automated method for comparing conversations in terms of their overall interactional dynamics. Such methods could enhance the analysis of conversational data and help evaluate conversational agents more holistically.
  In this work, we introduce a similarity measure for comparing conversations with respect to their dynamics. We design a validation framework for testing the robustness of the metric in capturing differences in conversation dynamics and for assessing its sensitivity to the topic of the conversations. Finally, to illustrate the measure's utility, we use it to analyze conversational dynamics in a large online community, bringing new insights into the role of situational power in conversations.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation</title>
<link>https://arxiv.org/abs/2507.18973</link>
<guid>https://arxiv.org/abs/2507.18973</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, mathematical reasoning, Multi-TAG, aggregation-based framework, inference-only

Summary: 
Multi-TAG is a novel framework for enhancing mathematical reasoning systems by aggregating the outputs of multiple external tools concurrently invoked by a large language model. Unlike previous approaches, Multi-TAG does not require finetuning and can be applied to any LLM backbone. By leveraging multiple tools simultaneously, Multi-TAG significantly improves solution robustness and accuracy on complex math problems across challenging benchmarks like MATH500, AIME, AMC, and OlympiadBench. The framework consistently outperforms state-of-the-art baselines, achieving average improvements of 6.0% to 7.5%. Multi-TAG's inference-only nature makes it suitable for computationally expensive open-weight models and proprietary models that cannot be finetuned with custom recipes. Overall, Multi-TAG represents a promising advancement in the development of high-performance mathematical reasoning systems. 

<br /><br />Summary: <div>
arXiv:2507.18973v1 Announce Type: new 
Abstract: Augmenting large language models (LLMs) with external tools is a promising avenue for developing high-performance mathematical reasoning systems. Prior tool-augmented approaches typically finetune an LLM to select and invoke a single tool at each reasoning step and show promising results on simpler math reasoning benchmarks such as GSM8K. However, these approaches struggle with more complex math problems that require precise reasoning over multiple steps. To address this limitation, in this work, we propose Multi-TAG, a Multi-Tool AGgregation-based framework. Instead of relying on a single tool, Multi-TAG guides an LLM to concurrently invoke multiple tools at each reasoning step. It then aggregates their diverse outputs to verify and refine the reasoning process, enhancing solution robustness and accuracy. Notably, Multi-TAG is a finetuning-free, inference-only framework, making it readily applicable to any LLM backbone, including large open-weight models which are computationally expensive to finetune and proprietary frontier models which cannot be finetuned with custom recipes. We evaluate Multi-TAG on four challenging benchmarks: MATH500, AIME, AMC, and OlympiadBench. Across both open-weight and closed-source LLM backbones, Multi-TAG consistently and substantially outperforms state-of-the-art baselines, achieving average improvements of 6.0% to 7.5% over state-of-the-art baselines.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arg-LLaDA: Argument Summarization via Large Language Diffusion Models and Sufficiency-Aware Refinement</title>
<link>https://arxiv.org/abs/2507.19081</link>
<guid>https://arxiv.org/abs/2507.19081</guid>
<content:encoded><![CDATA[
<div> Keywords: Argument Summarization, Large Language Diffusion Framework, Sufficiency-Guided Remasking, Generation Strategy, Evaluation Metrics

Summary: 
Arg-LLaDA introduces a novel approach to argument summarization by iteratively improving summaries through sufficiency-guided remasking and regeneration. This large language diffusion framework combines a masking controller with a sufficiency-checking module to enhance the fidelity, conciseness, and coherence of generated summaries. Empirical results on benchmark datasets show that Arg-LLaDA outperforms existing methods in automatic evaluation metrics and is validated through human evaluations. The iterative generation strategy leads to significant improvements in core dimensions such as coverage, faithfulness, and conciseness, highlighting the effectiveness of the approach in capturing complex, multi-perspective debates with structured representations. <div>
arXiv:2507.19081v1 Announce Type: new 
Abstract: Argument summarization aims to generate concise, structured representations of complex, multi-perspective debates. While recent work has advanced the identification and clustering of argumentative components, the generation stage remains underexplored. Existing approaches typically rely on single-pass generation, offering limited support for factual correction or structural refinement. To address this gap, we introduce Arg-LLaDA, a novel large language diffusion framework that iteratively improves summaries via sufficiency-guided remasking and regeneration. Our method combines a flexible masking controller with a sufficiency-checking module to identify and revise unsupported, redundant, or incomplete spans, yielding more faithful, concise, and coherent outputs. Empirical results on two benchmark datasets demonstrate that Arg-LLaDA surpasses state-of-the-art baselines in 7 out of 10 automatic evaluation metrics. In addition, human evaluations reveal substantial improvements across core dimensions, coverage, faithfulness, and conciseness, validating the effectiveness of our iterative, sufficiency-aware generation strategy.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents</title>
<link>https://arxiv.org/abs/2507.19090</link>
<guid>https://arxiv.org/abs/2507.19090</guid>
<content:encoded><![CDATA[
<div> claim verification, digital literacy, DebateCV framework, LLM agents, evidence quality

Summary:
The article introduces DebateCV, a novel claim verification framework that utilizes a debate-driven approach with multiple LLM agents to tackle complex claim verification tasks. Two Debaters present contrasting viewpoints on a claim, engaging in multi-round argumentation, while a Moderator evaluates the arguments and delivers a verdict. A post-training strategy leveraging synthetic debate data enhances the Moderator's performance by addressing the lack of real-world debate-driven claim verification data. Experimental results demonstrate the superiority of DebateCV over existing methods across varying levels of evidence quality. The framework's code and dataset are openly accessible for further research and development. <div>
arXiv:2507.19090v1 Announce Type: new 
Abstract: Claim verification is critical for enhancing digital literacy. However, the state-of-the-art single-LLM methods struggle with complex claim verification that involves multi-faceted evidences. Inspired by real-world fact-checking practices, we propose DebateCV, the first claim verification framework that adopts a debate-driven methodology using multiple LLM agents. In our framework, two Debaters take opposing stances on a claim and engage in multi-round argumentation, while a Moderator evaluates the arguments and renders a verdict with justifications. To further improve the performance of the Moderator, we introduce a novel post-training strategy that leverages synthetic debate data generated by the zero-shot DebateCV, effectively addressing the scarcity of real-world debate-driven claim verification data. Experimental results show that our method outperforms existing claim verification methods under varying levels of evidence quality. Our code and dataset are publicly available at https://anonymous.4open.science/r/DebateCV-6781.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Objectifying the Subjective: Cognitive Biases in Topic Interpretations</title>
<link>https://arxiv.org/abs/2507.19117</link>
<guid>https://arxiv.org/abs/2507.19117</guid>
<content:encoded><![CDATA[
<div> Keywords: topic interpretation, user studies, evaluation measures, cognitive biases, uncertainty <br />
Summary: <br />
The study focuses on the importance of topic interpretation for downstream applications and the limitations of existing evaluation measures such as coherence and word intrusion. To address this gap, user studies were conducted to understand how users interpret topics and assess constructs of topic quality. The analysis revealed that users rely on availability and representativeness heuristics rather than probability when interpreting topics. A theory of topic interpretation based on the anchoring-and-adjustment heuristic was proposed, suggesting that users anchor on salient words and make semantic adjustments to arrive at an interpretation. The study emphasizes the need for cognitive biases-aware user models and evaluation frameworks to better understand topic interpretation as a judgment made under uncertainty by ecologically rational users. <div>
arXiv:2507.19117v1 Announce Type: new 
Abstract: Interpretation of topics is crucial for their downstream applications. State-of-the-art evaluation measures of topic quality such as coherence and word intrusion do not measure how much a topic facilitates the exploration of a corpus. To design evaluation measures grounded on a task, and a population of users, we do user studies to understand how users interpret topics. We propose constructs of topic quality and ask users to assess them in the context of a topic and provide rationale behind evaluations. We use reflexive thematic analysis to identify themes of topic interpretations from rationales. Users interpret topics based on availability and representativeness heuristics rather than probability. We propose a theory of topic interpretation based on the anchoring-and-adjustment heuristic: users anchor on salient words and make semantic adjustments to arrive at an interpretation. Topic interpretation can be viewed as making a judgment under uncertainty by an ecologically rational user, and hence cognitive biases aware user models and evaluation frameworks are needed.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case</title>
<link>https://arxiv.org/abs/2507.19156</link>
<guid>https://arxiv.org/abs/2507.19156</guid>
<content:encoded><![CDATA[
<div> gender bias, language models, stereotypes, professional bias, Italian language

Summary: 
This study investigates how Large Language Models (LLMs) contribute to biased outputs, specifically focusing on gender and professional bias. Using Italian language to highlight potential limitations in LLMs, the study examines responses to ungendered prompts related to different professional job combinations. Two popular LLM-based chatbots, OpenAI ChatGPT and Google Gemini, were analyzed, revealing how they perpetuate stereotypes in generated content. For example, gender pronouns were consistently associated with specific job roles, reinforcing societal biases. The presence of bias in AI-generated text raises ethical concerns, particularly in fields like workplaces and job selections. Understanding these risks is crucial to developing mitigation strategies and ensuring that AI systems promote equitable outcomes. Future research directions include expanding the study to more chatbots or languages, refining prompt engineering methods, and increasing experimental data. 

<br /><br /> <div>
arXiv:2507.19156v1 Announce Type: new 
Abstract: The increasing use of Large Language Models (LLMs) in a large variety of domains has sparked worries about how easily they can perpetuate stereotypes and contribute to the generation of biased content. With a focus on gender and professional bias, this work examines in which manner LLMs shape responses to ungendered prompts, contributing to biased outputs. This analysis uses a structured experimental method, giving different prompts involving three different professional job combinations, which are also characterized by a hierarchical relationship. This study uses Italian, a language with extensive grammatical gender differences, to highlight potential limitations in current LLMs' ability to generate objective text in non-English languages. Two popular LLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google Gemini (gemini-1.5-flash). Through APIs, we collected a range of 3600 responses. The results highlight how content generated by LLMs can perpetuate stereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she' pronouns to the 'assistant' rather than the 'manager'. The presence of bias in AI-generated text can have significant implications in many fields, such as in the workplaces or in job selections, raising ethical concerns about its use. Understanding these risks is pivotal to developing mitigation strategies and assuring that AI-based systems do not increase social inequalities, but rather contribute to more equitable outcomes. Future research directions include expanding the study to additional chatbots or languages, refining prompt engineering methods or further exploiting a larger experimental base.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?</title>
<link>https://arxiv.org/abs/2507.19195</link>
<guid>https://arxiv.org/abs/2507.19195</guid>
<content:encoded><![CDATA[
<div> dialectal variation, African American Vernacular English, Standard American English, data poisoning, toxicity<br />
Summary:<br />
- The study focuses on how dialectal variation, particularly African American Vernacular English (AAVE) and Standard American English (SAE), interacts with data poisoning in large language models (LLMs) and influences toxicity in outputs.<br />
- Small- and medium-scale LLaMA models show that exposure to poisoned data increases toxicity for AAVE inputs but has minimal effects on SAE inputs, with larger models exhibiting a more significant amplification effect.<br />
- The use of GPT-4o as a fairness auditor reveals harmful stereotypical patterns associated with AAVE inputs, such as portrayals of aggression, criminality, and intellectual inferiority.<br />
- The study emphasizes the compounding impact of data poisoning and dialectal bias, highlighting the need for dialect-aware evaluation, targeted debiasing interventions, and socially responsible training protocols during development.<br /> <div>
arXiv:2507.19195v1 Announce Type: new 
Abstract: Despite the ongoing improvements in the design of large language models (LLMs) to foster inclusion and balanced responses, these systems remain susceptible to encoding and amplifying social biases. This study examines how dialectal variation, specifically African American Vernacular English (AAVE) versus Standard American English (SAE), interacts with data poisoning to influence toxicity in outputs. Using both small- and medium-scale LLaMA models, we show that even minimal exposure to poisoned data significantly increases toxicity for AAVE inputs, while it remains comparatively unaffected for SAE. Larger models exhibit a more significant amplification effect which suggests heightened susceptibility with scale. To further assess these disparities, we employed GPT-4o as a fairness auditor, which identified harmful stereotypical patterns disproportionately tied to AAVE inputs, including portrayals of aggression, criminality, and intellectual inferiority. These findings underscore the compounding impact of data poisoning and dialectal bias and emphasize the need for dialect-aware evaluation, targeted debiasing interventions, and socially responsible training protocols during development.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework</title>
<link>https://arxiv.org/abs/2507.19219</link>
<guid>https://arxiv.org/abs/2507.19219</guid>
<content:encoded><![CDATA[
<div> Evaluation Framework, Large Language Models, Overestimation, Benchmark, ArxivRoll 
Summary:
ArxivRoll proposes a dynamic evaluation framework to address the issue of overestimation in large language models (LLMs). It consists of two components: SCP for generating private test cases and Rugged Scores for measuring contamination and bias. The framework constructs a new benchmark every six months using recent articles from ArXiv and evaluates LLM performance. The approach aims to ensure reproducibility, transparency, and efficiency in evaluating LLMs. Extensive experiments demonstrate the quality of the benchmark and provide a systematic evaluation of current LLMs. The source code is available for access, enabling researchers to utilize ArxivRoll for fair and unbiased evaluations of LLMs. 
<br /><br />Summary: <div>
arXiv:2507.19219v1 Announce Type: new 
Abstract: Overestimation in evaluating large language models (LLMs) has become an increasing concern. Due to the contamination of public benchmarks or imbalanced model training, LLMs may achieve unreal evaluation results on public benchmarks, either intentionally or unintentionally, which leads to unfair comparisons among LLMs and undermines their realistic capability assessments. Existing benchmarks attempt to address these issues by keeping test cases permanently secret, mitigating contamination through human evaluation, or repeatedly collecting and constructing new samples. However, these approaches fail to ensure reproducibility, transparency, and high efficiency simultaneously. Moreover, the extent of overestimation in current LLMs remains unquantified. To address these issues, we propose ArxivRoll, a dynamic evaluation framework inspired by one-time pad encryption in cryptography. ArxivRoll comprises two key components: \emph{i) SCP (Sequencing, Cloze, and Prediction)}, an automated generator for private test cases, and \emph{ii) Rugged Scores (RS)}, metrics that measure the proportion of public benchmark contamination and training bias. Leveraging SCP, ArxivRoll constructs a new benchmark every six months using recent articles from ArXiv and employs them for one-time evaluations of LLM performance. Extensive experiments demonstrate the high quality of our benchmark, and we provide a systematic evaluation of current LLMs. The source code is available at https://github.com/liangzid/ArxivRoll/.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation</title>
<link>https://arxiv.org/abs/2507.19227</link>
<guid>https://arxiv.org/abs/2507.19227</guid>
<content:encoded><![CDATA[
<div> diffusion-based language models, jailbreak vulnerability, safety vulnerabilities, harmful generation speed, secure deployment<br />
Summary:<br />
The article discusses the vulnerability of Large Language Diffusion Models (LLDMs) to jailbreak attacks and highlights safety vulnerabilities in their architecture. Existing jailbreak methodologies for Large Language Models (LLMs) are ineffective against LLDMs, raising concerns about harmful generation capabilities. The authors introduce a PArallel Decoding jailbreak (PAD) technique for LLDMs, achieving a high success rate in attack scenarios. Experimental evaluations show that LLDMs increase harmful generation speed by 2x compared to autoregressive LLMs of the same size. This study provides critical insights into LLDM architecture and emphasizes the need for secure deployment strategies to mitigate risks of uncontrolled misuse.<br /><br />Summary: <div>
arXiv:2507.19227v1 Announce Type: new 
Abstract: Large Language Diffusion Models (LLDMs) exhibit comparable performance to LLMs while offering distinct advantages in inference speed and mathematical reasoning tasks.The precise and rapid generation capabilities of LLDMs amplify concerns of harmful generations, while existing jailbreak methodologies designed for Large Language Models (LLMs) prove limited effectiveness against LLDMs and fail to expose safety vulnerabilities.Successful defense cannot definitively resolve harmful generation concerns, as it remains unclear whether LLDMs possess safety robustness or existing attacks are incompatible with diffusion-based architectures.To address this, we first reveal the vulnerability of LLDMs to jailbreak and demonstrate that attack failure in LLDMs stems from fundamental architectural differences.We present a PArallel Decoding jailbreak (PAD) for diffusion-based language models. PAD introduces Multi-Point Attention Attack, which guides parallel generative processes toward harmful outputs that inspired by affirmative response patterns in LLMs. Experimental evaluations across four LLDMs demonstrate that PAD achieves jailbreak attack success rates by 97%, revealing significant safety vulnerabilities. Furthermore, compared to autoregressive LLMs of the same size, LLDMs increase the harmful generation speed by 2x, significantly highlighting risks of uncontrolled misuse.Through comprehensive analysis, we provide an investigation into LLDM architecture, offering critical insights for the secure deployment of diffusion-based language models.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Fine-grained Forms of Populism in Political Discourse: A Case Study on Donald Trump's Presidential Campaigns</title>
<link>https://arxiv.org/abs/2507.19303</link>
<guid>https://arxiv.org/abs/2507.19303</guid>
<content:encoded><![CDATA[
<div> populism, language models, classification, discourse analysis, political rhetoric
Summary:<br />
- Large Language Models (LLMs) are effective in instruction-following tasks but struggle with nuanced social science concepts like populism. 
- The paper creates datasets for capturing populist discourse and evaluates various pre-trained language models. 
- Fine-tuned RoBERTa classifier outperforms other models in detecting populist discourse. 
- Analyzing Donald Trump's campaign speeches using the best-performing model reveals insights into his use of populist rhetoric. 
- European politicians' campaign speeches are also analyzed to assess model generalizability, showing instruction-tuned LLMs are more robust on out-of-domain data.
 <div>
arXiv:2507.19303v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of instruction-following tasks, yet their grasp of nuanced social science concepts remains underexplored. This paper examines whether LLMs can identify and classify fine-grained forms of populism, a complex and contested concept in both academic and media debates. To this end, we curate and release novel datasets specifically designed to capture populist discourse. We evaluate a range of pre-trained (large) language models, both open-weight and proprietary, across multiple prompting paradigms. Our analysis reveals notable variation in performance, highlighting the limitations of LLMs in detecting populist discourse. We find that a fine-tuned RoBERTa classifier vastly outperforms all new-era instruction-tuned LLMs, unless fine-tuned. Additionally, we apply our best-performing model to analyze campaign speeches by Donald Trump, extracting valuable insights into his strategic use of populist rhetoric. Finally, we assess the generalizability of these models by benchmarking them on campaign speeches by European politicians, offering a lens into cross-context transferability in political discourse analysis. In this setting, we find that instruction-tuned LLMs exhibit greater robustness on out-of-domain data.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPCR: Automated Phenotype Concept Recognition by Prompting</title>
<link>https://arxiv.org/abs/2507.19315</link>
<guid>https://arxiv.org/abs/2507.19315</guid>
<content:encoded><![CDATA[
<div> Keywords: Phenotype concept recognition, biomedical text mining, AutoPCR, entity extraction, large language model <br />
Summary: <br />
Phenotype concept recognition (CR) in biomedical text mining is crucial for various applications like clinical diagnostics and knowledge graph construction. Current methods often struggle with generalization and require ontology-specific training. The AutoPCR approach presented in this study eliminates the need for ontology-specific training and excels in phenotype CR through a three-stage process. It combines rule-based and neural tagging strategies for entity extraction, utilizes SapBERT for candidate retrieval, and links entities through prompting a large language model. Experimental results on multiple benchmark datasets showcase AutoPCR's superior performance in both mention-level and document-level evaluations compared to existing methods. Ablation and transfer studies confirm its inductive capability and generalizability to new ontologies, establishing AutoPCR as a state-of-the-art approach in phenotype CR. <br /> 
Summary: <div>
arXiv:2507.19315v1 Announce Type: new 
Abstract: Phenotype concept recognition (CR) is a fundamental task in biomedical text mining, enabling applications such as clinical diagnostics and knowledge graph construction. However, existing methods often require ontology-specific training and struggle to generalize across diverse text types and evolving biomedical terminology. We present AutoPCR, a prompt-based phenotype CR method that does not require ontology-specific training. AutoPCR performs CR in three stages: entity extraction using a hybrid of rule-based and neural tagging strategies, candidate retrieval via SapBERT, and entity linking through prompting a large language model. Experiments on four benchmark datasets show that AutoPCR achieves the best average and most robust performance across both mention-level and document-level evaluations, surpassing prior state-of-the-art methods. Further ablation and transfer studies demonstrate its inductive capability and generalizability to new ontologies.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks</title>
<link>https://arxiv.org/abs/2507.19353</link>
<guid>https://arxiv.org/abs/2507.19353</guid>
<content:encoded><![CDATA[
<div> Keywords: Recurrent LLMs, Self-Attention LLMs, Smooth Reading, long-context tasks, efficiency

Summary:
Recurrent large language models (Recurrent LLMs) with linear computational complexity are gaining popularity as efficient alternatives to Self-Attention LLMs for long-context tasks. However, they often struggle due to limited memory capacity. To address this issue, the paper introduces Smooth Reading, a chunk-wise inference method inspired by human reading strategies. This approach processes context in chunks and summarizes information iteratively, reducing memory demands and improving compatibility with Recurrent LLMs. Experimental results demonstrate that Smooth Reading narrows the performance gap between Recurrent and Self-Attention LLMs on long-context tasks, boosting Recurrent LLMs' performance while maintaining efficiency advantages. This method allows Recurrent LLMs to achieve comparable performance with Self-Attention LLMs on long-context tasks, showcasing its potential for future research in this area. The code and dataset will be released to facilitate further progress. 

<br /><br />Summary: <div>
arXiv:2507.19353v1 Announce Type: new 
Abstract: Recently, recurrent large language models (Recurrent LLMs) with linear computational complexity have re-emerged as efficient alternatives to self-attention-based LLMs (Self-Attention LLMs), which have quadratic complexity. However, Recurrent LLMs often underperform on long-context tasks due to their limited fixed-size memory. Previous research has primarily focused on enhancing the memory capacity of Recurrent LLMs through architectural innovations, but these approaches have not yet enabled Recurrent LLMs to match the performance of Self-Attention LLMs on long-context tasks. We argue that this limitation arises because processing the entire context at once is not well-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a chunk-wise inference method inspired by human reading strategies. Smooth Reading processes context in chunks and iteratively summarizes the contextual information, thereby reducing memory demands and making the approach more compatible with Recurrent LLMs. Our experimental results show that this method substantially narrows the performance gap between Recurrent and Self-Attention LLMs on long-context tasks, while preserving the efficiency advantages of Recurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from 5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench. Besides, our method maintains the high efficiency, training 3x faster and inferring 2x faster at 64k context compared to Self-Attention LLMs. To our knowledge, this is the first work to achieve comparable performance using Recurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope our method will inspire future research in this area. To facilitate further progress, we will release code and dataset.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Speech Emotion Recognition Leveraging Aligning Timestamps of ASR Transcripts and Speaker Diarization</title>
<link>https://arxiv.org/abs/2507.19356</link>
<guid>https://arxiv.org/abs/2507.19356</guid>
<content:encoded><![CDATA[
<div> Keywords: timestamp alignment, Automatic Speech Recognition, Speaker Diarization, Speech Emotion Recognition, multimodal approach

Summary: 
- The paper investigates the impact of timestamp-based alignment between ASR transcripts and SD outputs on SER accuracy.
- Misalignment between ASR and SD reduces reliability in multimodal emotion recognition systems, especially in conversations.
- An alignment pipeline using pre-trained models is introduced to synchronize timestamps accurately in speaker segments.
- The multimodal approach combines textual embeddings from RoBERTa with audio embeddings from Wav2Vec, using cross-attention fusion with a gating mechanism.
- Experimental evaluations on the IEMOCAP dataset show that precise timestamp alignment improves SER accuracy, surpassing baseline methods without synchronization. The study underscores the crucial role of temporal alignment in enhancing emotion recognition accuracy, laying the groundwork for robust multimodal emotion analysis.

<br /><br />Summary: <div>
arXiv:2507.19356v1 Announce Type: new 
Abstract: In this paper, we investigate the impact of incorporating timestamp-based alignment between Automatic Speech Recognition (ASR) transcripts and Speaker Diarization (SD) outputs on Speech Emotion Recognition (SER) accuracy. Misalignment between these two modalities often reduces the reliability of multimodal emotion recognition systems, particularly in conversational contexts. To address this issue, we introduce an alignment pipeline utilizing pre-trained ASR and speaker diarization models, systematically synchronizing timestamps to generate accurately labeled speaker segments. Our multimodal approach combines textual embeddings extracted via RoBERTa with audio embeddings from Wav2Vec, leveraging cross-attention fusion enhanced by a gating mechanism. Experimental evaluations on the IEMOCAP benchmark dataset demonstrate that precise timestamp alignment improves SER accuracy, outperforming baseline methods that lack synchronization. The results highlight the critical importance of temporal alignment, demonstrating its effectiveness in enhancing overall emotion recognition accuracy and providing a foundation for robust multimodal emotion analysis.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice Understanding Large Language Models</title>
<link>https://arxiv.org/abs/2507.19361</link>
<guid>https://arxiv.org/abs/2507.19361</guid>
<content:encoded><![CDATA[
<div> Keywords: Speech-based Intelligence Quotient, voice understanding, large language models, cognitive levels, Bloom's Taxonomy

Summary:
Speech-based Intelligence Quotient (SIQ) is introduced as a new evaluation pipeline for assessing voice understanding abilities of large language models (LLM) Voice. SIQ goes beyond traditional metrics like word error rate to evaluate LLM Voice across three cognitive levels based on Bloom's Taxonomy. It looks at verbatim accuracy, interpretations similarity, and QA accuracy for simulating downstream tasks. SIQ allows for unified comparisons between cascaded methods and end-to-end models, uncovers annotation errors in benchmarks, and identifies hallucinations in LLM Voice. This framework integrates cognitive principles with voice-oriented benchmarks, highlighting challenges in multi-modal training.<br /><br />Summary: <div>
arXiv:2507.19361v1 Announce Type: new 
Abstract: We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human cognition-inspired evaluation pipeline for voice understanding large language models, LLM Voice, designed to assess their voice understanding ability. Moving beyond popular voice understanding metrics such as word error rate (WER), SIQ examines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy: (1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e., similarity of LLM's interpretations); and (3) Application (i.e., QA accuracy for simulating downstream tasks). We demonstrate that SIQ not only quantifies voice understanding abilities but also provides unified comparisons between cascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation errors in existing benchmarks, and detects hallucinations in LLM Voice. Our framework represents a first-of-its-kind intelligence examination that bridges cognitive principles with voice-oriented benchmarks, while exposing overlooked challenges in multi-modal training.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Augmentation for Spoken Grammatical Error Correction</title>
<link>https://arxiv.org/abs/2507.19374</link>
<guid>https://arxiv.org/abs/2507.19374</guid>
<content:encoded><![CDATA[
<div> Keywords: Spoken GEC, automated method, augmented dataset, objective metrics, S&amp;I Corpus

Summary:
An automated method for generating audio-text pairs with grammatical errors and disfluencies is proposed to address the lack of high-quality annotated datasets for Spoken Grammatical Error Correction (SGEC). Objective metrics are suggested to evaluate the generated data and select the most suitable dataset for SGEC. The aim is to create an augmented dataset that preserves the original data's textual and acoustic characteristics while introducing new types of errors. The augmented corpus should enhance the original corpus without impacting the language assessment scores of second language (L2) learners. The evaluation of the augmented dataset covers written GEC and SGEC using the S&amp;I Corpus, the first publicly available speech dataset with grammar error annotations. <div>
arXiv:2507.19374v1 Announce Type: new 
Abstract: While there exist strong benchmark datasets for grammatical error correction (GEC), high-quality annotated spoken datasets for Spoken GEC (SGEC) are still under-resourced. In this paper, we propose a fully automated method to generate audio-text pairs with grammatical errors and disfluencies. Moreover, we propose a series of objective metrics that can be used to evaluate the generated data and choose the more suitable dataset for SGEC. The goal is to generate an augmented dataset that maintains the textual and acoustic characteristics of the original data while providing new types of errors. This augmented dataset should augment and enrich the original corpus without altering the language assessment scores of the second language (L2) learners. We evaluate the use of the augmented corpus both for written GEC (the text part) and for SGEC (the audio-text pairs). Our experiments are conducted on the S\&amp;I Corpus, the first publicly available speech dataset with grammar error annotations.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection of Adverse Drug Events in Dutch clinical free text documents using Transformer Models: benchmark study</title>
<link>https://arxiv.org/abs/2507.19396</link>
<guid>https://arxiv.org/abs/2507.19396</guid>
<content:encoded><![CDATA[
<div> transformer models, Dutch clinical free text, adverse drug event detection, Bidirectional Long Short-Term Memory (Bi-LSTM), MedRoBERTa.nl<br />
<br />
Summary:<br />
The study sets a benchmark for adverse drug event (ADE) detection in Dutch clinical free text using various transformer models. They trained a Bi-LSTM model and four transformer-based models for named entity recognition and relation classification using annotated Dutch ICU progress notes. The models were evaluated internally and externally for ADE detection, with MedRoBERTa.nl performing the best with an F1 score of 0.63. The external validation showed MedRoBERTa.nl achieving a recall of 67 to 74% in detecting ADEs in discharge letters. The study emphasizes the importance of using appropriate performance measures for ADE detection in clinical documents and provides a robust evaluation approach for language models in this task. Future clinical applications can benefit from the findings of this benchmark study. <br /> <div>
arXiv:2507.19396v1 Announce Type: new 
Abstract: In this study, we set a benchmark for adverse drug event (ADE) detection in Dutch clinical free text documents using several transformer models, clinical scenarios and fit-for-purpose performance measures. We trained a Bidirectional Long Short-Term Memory (Bi-LSTM) model and four transformer-based Dutch and/or multilingual encoder models (BERTje, RobBERT, MedRoBERTa.nl, and NuNER) for the tasks of named entity recognition (NER) and relation classification (RC) using 102 richly annotated Dutch ICU clinical progress notes. Anonymized free text clinical progress notes of patients admitted to intensive care unit (ICU) of one academic hospital and discharge letters of patients admitted to Internal Medicine wards of two non-academic hospitals were reused. We evaluated our ADE RC models internally using gold standard (two-step task) and predicted entities (end-to-end task). In addition, all models were externally validated on detecting ADEs at the document level. We report both micro- and macro-averaged F1 scores, given the imbalance of ADEs in the datasets. Although differences for the ADE RC task between the models were small, MedRoBERTa.nl was the best performing model with macro-averaged F1 score of 0.63 using gold standard and 0.62 using predicted entities. The MedRoBERTa.nl models also performed the best in our external validation and achieved recall of between 0.67 to 0.74 using predicted entities, meaning between 67 to 74% of discharge letters with ADEs were detected. Our benchmark study presents a robust and clinically meaningful approach for evaluating language models for ADE detection in clinical free text documents. Our study highlights the need to use appropriate performance measures fit for the task of ADE detection in clinical free-text documents and envisioned future clinical use.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Domain Specification of Embedding Models in Medicine</title>
<link>https://arxiv.org/abs/2507.19407</link>
<guid>https://arxiv.org/abs/2507.19407</guid>
<content:encoded><![CDATA[
<div> Medical text embedding models, MEDTE, diverse medical corpora, self-supervised contrastive learning, benchmark suite, 51 tasks<br />
<br />
Summary: 
- Medical text embedding models are crucial for healthcare applications but often lack diversification in training data and up-to-date methodology.
- The authors propose using MEDTE, a GTE model fine-tuned on diverse medical corpora through self-supervised contrastive learning, to improve medical text embeddings.
- A benchmark suite of 51 tasks tailored to medical text is introduced to evaluate the performance of the embeddings.
- Results show that the combined approach of MEDTE and the benchmark suite outperforms existing models in various medical tasks.
- This work addresses critical shortcomings in current medical text embedding models and provides a robust evaluation framework for assessing their performance. <div>
arXiv:2507.19407v1 Announce Type: new 
Abstract: Medical text embedding models are foundational to a wide array of healthcare applications, ranging from clinical decision support and biomedical information retrieval to medical question answering, yet they remain hampered by two critical shortcomings. First, most models are trained on a narrow slice of medical and biological data, beside not being up to date in terms of methodology, making them ill suited to capture the diversity of terminology and semantics encountered in practice. Second, existing evaluations are often inadequate: even widely used benchmarks fail to generalize across the full spectrum of real world medical tasks.
  To address these gaps, we leverage MEDTE, a GTE model extensively fine-tuned on diverse medical corpora through self-supervised contrastive learning across multiple data sources, to deliver robust medical text embeddings.
  Alongside this model, we propose a comprehensive benchmark suite of 51 tasks spanning classification, clustering, pair classification, and retrieval modeled on the Massive Text Embedding Benchmark (MTEB) but tailored to the nuances of medical text. Our results demonstrate that this combined approach not only establishes a robust evaluation framework but also yields embeddings that consistently outperform state of the art alternatives in different tasks.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokenSmith: Streamlining Data Editing, Search, and Inspection for Large-Scale Language Model Training and Interpretability</title>
<link>https://arxiv.org/abs/2507.19419</link>
<guid>https://arxiv.org/abs/2507.19419</guid>
<content:encoded><![CDATA[
<div> TokenSmith, open-source library, interactive editing, inspection, analysis, datasets. <br />
<br />
Summary: TokenSmith is an open-source library designed for interactive editing, inspection, and analysis of datasets used in pretraining frameworks like GPT-NeoX, Megatron, and NVIDIA NeMo. It simplifies the process of understanding the relationship between training data and model behavior during pretraining. TokenSmith enables a range of operations such as searching, viewing, ingesting, exporting, inspecting, and sampling data through a user-friendly interface. It allows structured editing of pretraining data without modifying training code, making dataset debugging, validation, and experimentation easier. TokenSmith is compatible with existing large language model pretraining workflows, democratizing access to production-grade dataset tooling. It is hosted on GitHub with documentation and tutorials available, along with a demonstration video on YouTube. <div>
arXiv:2507.19419v1 Announce Type: new 
Abstract: Understanding the relationship between training data and model behavior during pretraining is crucial, but existing workflows make this process cumbersome, fragmented, and often inaccessible to researchers. We present TokenSmith, an open-source library for interactive editing, inspection, and analysis of datasets used in Megatron-style pretraining frameworks such as GPT-NeoX, Megatron, and NVIDIA NeMo. TokenSmith supports a wide range of operations including searching, viewing, ingesting, exporting, inspecting, and sampling data, all accessible through a simple user interface and a modular backend. It also enables structured editing of pretraining data without requiring changes to training code, simplifying dataset debugging, validation, and experimentation.
  TokenSmith is designed as a plug and play addition to existing large language model pretraining workflows, thereby democratizing access to production-grade dataset tooling. TokenSmith is hosted on GitHub1, with accompanying documentation and tutorials. A demonstration video is also available on YouTube.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.19457</link>
<guid>https://arxiv.org/abs/2507.19457</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, reinforcement learning, interpretable nature, natural language reflection, GEPA <br />
Summary: Large language models (LLMs) are often adapted to tasks using reinforcement learning methods like Group Relative Policy Optimization (GRPO). However, using natural language reflection can provide a richer learning experience for LLMs compared to sparse rewards. A new prompt optimizer, GEPA (Genetic-Pareto), incorporates natural language reflection to learn high-level rules through trial and error. By analyzing system-level trajectories and proposing prompt updates in natural language, GEPA can achieve significant quality gains with fewer rollouts. In experiments across four tasks, GEPA outperformed GRPO by 10% on average and up to 20%, while using significantly fewer rollouts. It also outperformed MIPROv2, a leading prompt optimizer, by over 10% on two LLMs and showed promise as an inference-time search strategy for code optimization. <br /><br />Summary: Large language models can benefit from using natural language reflection for learning tasks, as demonstrated by the success of the GEPA prompt optimizer. By incorporating natural language feedback into the learning process, GEPA outperformed traditional reinforcement learning methods like GRPO and other prompt optimizers like MIPROv2. Using a system-level approach and leveraging the interpretable nature of language, GEPA achieved higher quality gains with fewer rollouts, showcasing the potential of language-driven learning strategies for large language models. <div>
arXiv:2507.19457v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversations Gone Awry, But Then? Evaluating Conversational Forecasting Models</title>
<link>https://arxiv.org/abs/2507.19470</link>
<guid>https://arxiv.org/abs/2507.19470</guid>
<content:encoded><![CDATA[
<div> Keywords: conversation forecasting, automated systems, Conversations Gone Awry task, evaluation framework, language modeling

Summary:
Automated systems with predictive capabilities can enhance human-human interactions by anticipating conversation directions. This study focuses on the Conversations Gone Awry (CGA) task, predicting if a conversation will derail. By introducing a uniform evaluation framework, the researchers establish a benchmark for comparing different models accurately. They assess the progress of CGA models in light of advancements in language modeling, providing an updated overview of the field. The framework also introduces a novel metric to measure a model's ability to adjust its predictions as the conversation unfolds. This comprehensive evaluation approach sheds light on the current state of CGA models and their potential for improving communication dynamics. 

<br /><br />Summary: <div>
arXiv:2507.19470v1 Announce Type: new 
Abstract: We often rely on our intuition to anticipate the direction of a conversation. Endowing automated systems with similar foresight can enable them to assist human-human interactions. Recent work on developing models with this predictive capacity has focused on the Conversations Gone Awry (CGA) task: forecasting whether an ongoing conversation will derail. In this work, we revisit this task and introduce the first uniform evaluation framework, creating a benchmark that enables direct and reliable comparisons between different architectures. This allows us to present an up-to-date overview of the current progress in CGA models, in light of recent advancements in language modeling. Our framework also introduces a novel metric that captures a model's ability to revise its forecast as the conversation progresses.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>People Are Highly Cooperative with Large Language Models, Especially When Communication Is Possible or Following Human Interaction</title>
<link>https://arxiv.org/abs/2507.18639</link>
<guid>https://arxiv.org/abs/2507.18639</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, cooperation, Prisoner's Dilemma game, communication, business settings

Summary: 
Participants in two experiments played the Prisoner's Dilemma game against humans, classic bots, and a large language model (LLM). Cooperation rates with LLMs were slightly lower than with humans but still high, suggesting LLMs can be used in cooperative business settings. Allowing communication increased cooperation rates with both humans and LLMs by 88%, surprising given the assumption that people might be less receptive to cooperating with machines. Cooperation with LLMs was higher after prior interaction with humans, indicating a spillover effect in cooperative behavior. These findings highlight the potential for LLMs to augment humans in cooperative tasks, offering valuable insights for businesses looking to leverage LLM technology. 

<br /><br />Summary: <div>
arXiv:2507.18639v1 Announce Type: cross 
Abstract: Machines driven by large language models (LLMs) have the potential to augment humans across various tasks, a development with profound implications for business settings where effective communication, collaboration, and stakeholder trust are paramount. To explore how interacting with an LLM instead of a human might shift cooperative behavior in such settings, we used the Prisoner's Dilemma game -- a surrogate of several real-world managerial and economic scenarios. In Experiment 1 (N=100), participants engaged in a thirty-round repeated game against a human, a classic bot, and an LLM (GPT, in real-time). In Experiment 2 (N=192), participants played a one-shot game against a human or an LLM, with half of them allowed to communicate with their opponent, enabling LLMs to leverage a key advantage over older-generation machines. Cooperation rates with LLMs -- while lower by approximately 10-15 percentage points compared to interactions with human opponents -- were nonetheless high. This finding was particularly notable in Experiment 2, where the psychological cost of selfish behavior was reduced. Although allowing communication about cooperation did not close the human-machine behavioral gap, it increased the likelihood of cooperation with both humans and LLMs equally (by 88%), which is particularly surprising for LLMs given their non-human nature and the assumption that people might be less receptive to cooperating with machines compared to human counterparts. Additionally, cooperation with LLMs was higher following prior interaction with humans, suggesting a spillover effect in cooperative behavior. Our findings validate the (careful) use of LLMs by businesses in settings that have a cooperative component.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreeReader: A Hierarchical Academic Paper Reader Powered by Language Models</title>
<link>https://arxiv.org/abs/2507.18945</link>
<guid>https://arxiv.org/abs/2507.18945</guid>
<content:encoded><![CDATA[
<div> Keywords: academic papers, summarization, hierarchical structure, TreeReader, user study 

Summary:<br /><br />Efficiently navigating and understanding academic papers is essential for scientific progress. Traditional linear formats like PDF and HTML can overwhelm readers and obscure the paper's structure, making it challenging to find key information. LLM-based chatbots offer summarization but lack nuanced understanding and may produce unreliable information. Drawing on insights from a study on academic reading practices, TreeReader is introduced as a novel language model-augmented paper reader. TreeReader breaks down papers into an interactive tree structure, offering concise LLM-generated summaries for each section with detailed information available on demand. By allowing users to quickly grasp core ideas, explore specific sections, and verify summaries against the source text, TreeReader enhances reading efficiency and comprehension. A user study confirmed TreeReader's effectiveness in navigating and understanding complex academic literature through its combination of hierarchical summarization and interactive exploration. <div>
arXiv:2507.18945v1 Announce Type: cross 
Abstract: Efficiently navigating and understanding academic papers is crucial for scientific progress. Traditional linear formats like PDF and HTML can cause cognitive overload and obscure a paper's hierarchical structure, making it difficult to locate key information. While LLM-based chatbots offer summarization, they often lack nuanced understanding of specific sections, may produce unreliable information, and typically discard the document's navigational structure. Drawing insights from a formative study on academic reading practices, we introduce TreeReader, a novel language model-augmented paper reader. TreeReader decomposes papers into an interactive tree structure where each section is initially represented by an LLM-generated concise summary, with underlying details accessible on demand. This design allows users to quickly grasp core ideas, selectively explore sections of interest, and verify summaries against the source text. A user study was conducted to evaluate TreeReader's impact on reading efficiency and comprehension. TreeReader provides a more focused and efficient way to navigate and understand complex academic literature by bridging hierarchical summarization with interactive exploration.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Learning Systems: Personalized Curriculum Design Using LLM-Powered Analytics</title>
<link>https://arxiv.org/abs/2507.18949</link>
<guid>https://arxiv.org/abs/2507.18949</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Adaptive Learning Systems, personalized curriculum design, machine learning, student-centered model

Summary:
Adaptive Learning Systems utilizing Large Language Models (LLMs) are transforming education by providing personalized learning experiences tailored to individual student needs. This framework incorporates LLM-powered analytics to analyze real-time data, enabling the system to adapt learning pathways and recommend resources based on each learner's progress. By continuously evaluating students, the framework improves instructional strategies, ensuring the presented materials are relevant and engaging. Experimental results demonstrate significant enhancements in learner engagement and knowledge retention when utilizing a customized curriculum. Evaluations across various educational settings highlight the framework's flexibility and its positive impact on learning outcomes. This innovative approach has the potential to reshape traditional educational practices into a more adaptive and student-centered model. 

<br /><br />Summary: Large language models and Adaptive Learning Systems are revolutionizing education by personalizing learning experiences through machine learning analytics. The framework adapts learning pathways and recommends resources based on individual student progress, leading to improved engagement and knowledge retention. Experimental results show the effectiveness of customized curriculums in enhancing learning outcomes, with evaluations across diverse educational environments highlighting the framework's flexibility and positive influence on traditional teaching methods. <div>
arXiv:2507.18949v1 Announce Type: cross 
Abstract: Large language models (LLMs) are revolutionizing the field of education by enabling personalized learning experiences tailored to individual student needs. In this paper, we introduce a framework for Adaptive Learning Systems that leverages LLM-powered analytics for personalized curriculum design. This innovative approach uses advanced machine learning to analyze real-time data, allowing the system to adapt learning pathways and recommend resources that align with each learner's progress. By continuously assessing students, our framework enhances instructional strategies, ensuring that the materials presented are relevant and engaging. Experimental results indicate a marked improvement in both learner engagement and knowledge retention when using a customized curriculum. Evaluations conducted across varied educational environments demonstrate the framework's flexibility and positive influence on learning outcomes, potentially reshaping conventional educational practices into a more adaptive and student-centered model.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLLM-based Speech Recognition: When and How is Multimodality Beneficial?</title>
<link>https://arxiv.org/abs/2507.19037</link>
<guid>https://arxiv.org/abs/2507.19037</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal speech recognition, automatic speech recognition, noisy environments, model architectures, visual encoders <br />
Summary:<br />
1. Harnessing multiple modalities can improve automatic speech recognition (ASR) accuracy, with each modality providing complementary information, depending on the amount of auditory noise present.
2. Synchronized modalities such as lip movements are more beneficial in high noise levels, while unsynchronized modalities like image context are most helpful in moderate noise levels.
3. High-quality visual representations play a crucial role in enhancing ASR accuracy, emphasizing the need for more powerful visual encoders.
4. Mamba and Transformers exhibit similar trends in the benefits of multimodality for ASR.
5. The input order of modalities and their weights in the loss function can significantly impact ASR accuracy, providing practical insights and deepening understanding of multi-modal speech recognition under challenging conditions. <br /> 
Summary: <div>
arXiv:2507.19037v1 Announce Type: cross 
Abstract: Recent advances in multi-modal large language models (MLLMs) have opened new possibilities for unified modeling of speech, text, images, and other modalities. Building on our prior work, this paper examines the conditions and model architectures under which multiple input modalities can improve automatic speech recognition (ASR) accuracy in noisy environments. Through experiments on synthetic and real-world data, we find that (1) harnessing more modalities usually improves ASR accuracy, as each modality provides complementary information, but the improvement depends on the amount of auditory noise. (2) Synchronized modalities (e.g., lip movements) are more useful at high noise levels whereas unsynchronized modalities (e.g., image context) are most helpful at moderate noise levels. (3) Higher-quality visual representations consistently improve ASR accuracy, highlighting the importance of developing more powerful visual encoders. (4) Mamba exhibits similar trends regarding the benefits of multimodality as do Transformers. (5) The input order of modalities as well as their weights in the loss function can significantly impact accuracy. These findings both offer practical insights and help to deepen our understanding of multi-modal speech recognition under challenging conditions.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex Spoken Dialogue Systems</title>
<link>https://arxiv.org/abs/2507.19040</link>
<guid>https://arxiv.org/abs/2507.19040</guid>
<content:encoded><![CDATA[
<div> spoken dialogue systems, full-duplex, benchmarking, user interruptions, natural interaction
Summary: 
The study introduces a new benchmarking pipeline for full-duplex spoken dialogue systems (FDSDS) to evaluate their performance in handling user interruptions and delays. Three open-source FDSDS were tested using over 40 hours of generated speech, 293 simulated conversations, and 1,200 interruptions. The results revealed that all models faced challenges in responding to user interruptions and dealing with disruptions and noisy conditions. The benchmark assesses FDSDS in challenging scenarios with novel metrics and aims to improve their robustness and efficiency. The study provides a comprehensive evaluation of FDSDS capabilities, highlighting areas for improvement and further research. Demonstrations, data, and code related to the benchmarking pipeline will be made available for future use and development. 
<br /><br />Summary: <div>
arXiv:2507.19040v1 Announce Type: cross 
Abstract: Full-duplex spoken dialogue systems (FDSDS) enable more natural human-machine interactions by allowing real-time user interruptions and backchanneling, compared to traditional SDS that rely on turn-taking. However, existing benchmarks lack metrics for FD scenes, e.g., evaluating model performance during user interruptions. In this paper, we present a comprehensive FD benchmarking pipeline utilizing LLMs, TTS, and ASR to address this gap. It assesses FDSDS's ability to handle user interruptions, manage delays, and maintain robustness in challenging scenarios with diverse novel metrics. We applied our benchmark to three open-source FDSDS (Moshi, Freeze-omni, and VITA-1.5) using over 40 hours of generated speech, with 293 simulated conversations and 1,200 interruptions. The results show that all models continue to face challenges, such as failing to respond to user interruptions, under frequent disruptions and noisy conditions. Demonstrations, data, and code will be released.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Closing the Modality Gap for Mixed Modality Search</title>
<link>https://arxiv.org/abs/2507.19054</link>
<guid>https://arxiv.org/abs/2507.19054</guid>
<content:encoded><![CDATA[
<div> calibration, modality gap, mixed modality search, contrastive vision-language models, GR-CLIP 
Summary: 
GR-CLIP is proposed as a post-hoc calibration method to address the modality gap issue in the embedding space of CLIP for mixed modality search tasks. Analysis shows that current models exhibit a modality gap leading to ranking bias and fusion failure. GR-CLIP significantly improves NDCG@10 by up to 26 percentage points over CLIP on the MixBench benchmark. It outperforms vision-language generative embedding models by 4 percentage points with significantly lower computational requirements, demonstrating its efficiency. This work highlights the importance of addressing modality gaps in mixed modality search scenarios and provides a practical solution with GR-CLIP. <br /><br />Summary: <div>
arXiv:2507.19054v1 Announce Type: cross 
Abstract: Mixed modality search -- retrieving information across a heterogeneous corpus composed of images, texts, and multimodal documents -- is an important yet underexplored real-world application. In this work, we investigate how contrastive vision-language models, such as CLIP, perform on the mixed modality search task. Our analysis reveals a critical limitation: these models exhibit a pronounced modality gap in the embedding space, where image and text embeddings form distinct clusters, leading to intra-modal ranking bias and inter-modal fusion failure. To address this issue, we propose GR-CLIP, a lightweight post-hoc calibration method that removes the modality gap in CLIP's embedding space. Evaluated on MixBench -- the first benchmark specifically designed for mixed modality search -- GR-CLIP improves NDCG@10 by up to 26 percentage points over CLIP, surpasses recent vision-language generative embedding models by 4 percentage points, while using 75x less compute.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PurpCode: Reasoning for Safer Code Generation</title>
<link>https://arxiv.org/abs/2507.19060</link>
<guid>https://arxiv.org/abs/2507.19060</guid>
<content:encoded><![CDATA[
<div> machine learning, security, code reasoning, cybersafety, reinforcement learning

Summary:
PurpCode introduces a novel approach for training safe code reasoning models, focusing on generating secure code and preventing malicious cyber activities. The training process involves two stages: Rule Learning teaches the model to follow cybersafety rules, while Reinforcement Learning optimizes model safety and utility through various reward mechanisms. The model, PurpCode-32B, achieves superior cybersafety performance compared to existing models. An alignment method reduces model overrefusal rates and maintains utility in code generation and security knowledge tasks. The training pipeline is enriched with extensive cybersafety data synthesized through internal red-teaming to create realistic unsafe prompts for the model. Overall, PurpCode ensures that the reasoning-based coding model not only generates vulnerability-free code but also actively defends against potential cyber threats, making it a significant advancement in the field of secure code reasoning models. <br /><br />Summary: <div>
arXiv:2507.19060v1 Announce Type: cross 
Abstract: We introduce PurpCode, the first post-training recipe for training safe code reasoning models towards generating secure code and defending against malicious cyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule Learning, which explicitly teaches the model to reference cybersafety rules to generate vulnerability-free code and to avoid facilitating malicious cyberactivities; and (ii) Reinforcement Learning, which optimizes model safety and preserves model utility through diverse, multi-objective reward mechanisms. To empower the training pipelines with comprehensive cybersafety data, we conduct internal red-teaming to synthesize comprehensive and high-coverage prompts based on real-world tasks for inducing unsafe cyberactivities in the model. Based on PurpCode, we develop a reasoning-based coding model, namely PurpCode-32B, which demonstrates state-of-the-art cybersafety, outperforming various frontier models. Meanwhile, our alignment method decreases the model overrefusal rates in both general and cybersafety-specific scenarios, while preserving model utility in both code generation and common security knowledge.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling a Small Utility-Based Passage Selector to Enhance Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.19102</link>
<guid>https://arxiv.org/abs/2507.19102</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval-augmented generation, utility-based selection, large language models, distillation, answer quality

Summary: 
Retrieval-augmented generation (RAG) enhances large language models by incorporating retrieved information with a focus on utility rather than just relevance. The high computational cost of using LLMs for utility judgments limits the number of passages evaluated, especially for complex queries. To address this, a method to distill the utility judgment capabilities of LLMs into smaller, more efficient models is proposed. This approach enables dynamic passage selection tailored to specific queries without fixed thresholds. Student models are trained to learn pseudo-answer generation and utility judgments from teacher LLMs, significantly reducing computational costs while improving answer quality. Utility-based selection is found to be more effective than relevance ranking in enhancing answer generation performance for complex questions. Annotations for relevance ranking and utility-based selection on the MS MARCO dataset will be released to support further research in this area. 

<br /><br />Summary: <div>
arXiv:2507.19102v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating retrieved information. Standard retrieval process prioritized relevance, focusing on topical alignment between queries and passages. In contrast, in RAG, the emphasis has shifted to utility, which considers the usefulness of passages for generating accurate answers. Despite empirical evidence showing the benefits of utility-based retrieval in RAG, the high computational cost of using LLMs for utility judgments limits the number of passages evaluated. This restriction is problematic for complex queries requiring extensive information. To address this, we propose a method to distill the utility judgment capabilities of LLMs into smaller, more efficient models. Our approach focuses on utility-based selection rather than ranking, enabling dynamic passage selection tailored to specific queries without the need for fixed thresholds. We train student models to learn pseudo-answer generation and utility judgments from teacher LLMs, using a sliding window method that dynamically selects useful passages. Our experiments demonstrate that utility-based selection provides a flexible and cost-effective solution for RAG, significantly reducing computational costs while improving answer quality. We present the distillation results using Qwen3-32B as the teacher model for both relevance ranking and utility-based selection, distilled into RankQwen1.7B and UtilityQwen1.7B. Our findings indicate that for complex questions, utility-based selection is more effective than relevance ranking in enhancing answer generation performance. We will release the relevance ranking and utility-based selection annotations for the MS MARCO dataset, supporting further research in this area.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?</title>
<link>https://arxiv.org/abs/2507.19132</link>
<guid>https://arxiv.org/abs/2507.19132</guid>
<content:encoded><![CDATA[
<div> Keywords: computer-using agents, benchmark, automation level, generalization scope, OS-MAP

Summary:
The article introduces OS-MAP, a benchmark for daily computer-using automation that organizes realistic tasks across various applications. It addresses the lack of benchmarks that consider task heterogeneity and agent capabilities, hindering progress in research deployment. OS-MAP categorizes tasks based on automation level and generalization scope, aligning them with real-world user demands. This allows for a detailed assessment of agent capabilities and alignment with user scenarios. Experiments show that even State-of-the-Art agents struggle with higher-level tasks, emphasizing the need for a deeper understanding of strengths and limitations. The research provides valuable insights to drive future progress in computer-using agents research and deployment. The code, environments, baselines, and data for OS-MAP are publicly available at https://github.com/OS-Copilot/OS-Map.

<br /><br />Summary: 
- Introduction of OS-MAP benchmark for daily computer-using automation tasks
- Organization of tasks based on automation level and generalization scope
- Alignment with real-world user demands for comprehensive assessment
- Highlighting the struggles of State-of-the-Art agents with higher-level tasks
- Public availability of code, environments, baselines, and data at a specified link <div>
arXiv:2507.19132v1 Announce Type: cross 
Abstract: Computer-using agents have shown strong potential to boost human productivity and enable new application forms across platforms. While recent advances have led to usable applications, existing benchmarks fail to account for the internal task heterogeneity and the corresponding agent capabilities, as well as their alignment with actual user demands-hindering both targeted capability development and the reliable transition of research progress into practical deployment. To bridge the gap, we present OS-MAP, a benchmark for daily computer-using automation that organizes its 416 realistic tasks across 15 applications along two key dimensions: a five-level taxonomy of automation and a generalization scope derived from a real-world user demand hierarchy. To enable fine-grained analysis of required capabilities and alignment with real-world scenarios, OS-MAP evaluates agents along two dimensions: automation level across a five-level taxonomy, and generalization scope across a demand hierarchy. This design captures varying levels of required agent autonomy and generalization, forming a performance-generalization evaluation matrix for structured and comprehensive assessment. Experiments show that even State-of-the-Art agents with VLM backbones struggle with higher-level tasks involving perception, reasoning, and coordination-highlighting the need for a deeper understanding of current strengths and limitations to drive the future progress in computer-using agents research and deployment. All code, environments, baselines, and data are publicly available at https://github.com/OS-Copilot/OS-Map.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Multimodal Social Conversations with Robots: Using Vision-Language Models</title>
<link>https://arxiv.org/abs/2507.19196</link>
<guid>https://arxiv.org/abs/2507.19196</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, social robots, multimodal interactions, vision-language models, autonomous conversations
Summary: 
Large language models have enabled social robots to engage in open-domain conversations, but they lack the ability to utilize multiple modalities for social interactions. The need for a multimodal system in social conversations with robots is highlighted, emphasizing the importance of incorporating visual information. Vision-language models are proposed as a solution to process various visual cues in a general manner for autonomous social robots. Adapting these models to the social conversation setting poses technical challenges that need to be addressed. Evaluation practices for such systems must also be considered to assess their effectiveness in social interactions. Overall, integrating vision-language models into social robots can enhance their ability to engage in meaningful and natural conversations by incorporating visual context into their understanding and responses.<br /><br />Summary: <div>
arXiv:2507.19196v1 Announce Type: cross 
Abstract: Large language models have given social robots the ability to autonomously engage in open-domain conversations. However, they are still missing a fundamental social skill: making use of the multiple modalities that carry social interactions. While previous work has focused on task-oriented interactions that require referencing the environment or specific phenomena in social interactions such as dialogue breakdowns, we outline the overall needs of a multimodal system for social conversations with robots. We then argue that vision-language models are able to process this wide range of visual information in a sufficiently general manner for autonomous social robots. We describe how to adapt them to this setting, which technical challenges remain, and briefly discuss evaluation practices.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Should Top-Down Clustering Affect Boundaries in Unsupervised Word Discovery?</title>
<link>https://arxiv.org/abs/2507.19204</link>
<guid>https://arxiv.org/abs/2507.19204</guid>
<content:encoded><![CDATA[
<div> Keywords: speech segmentation, word clustering, lexicon construction, bottom-up approach, top-down approach

Summary: 
The study explores the segmentation of speech into word-like units and the clustering of these segments to create a lexicon. Existing methods fall into two frameworks: bottom-up and top-down. The research compares a simple bottom-up strategy with a top-down system, finding that both achieve state-of-the-art results on the ZeroSpeech benchmarks. The bottom-up approach is significantly faster, while the top-down approach can be beneficial in certain cases. However, both methods are limited by the clustering step, suggesting a need for improved techniques in this area. The study recommends focusing on developing more advanced clustering techniques and enhancing the discriminative power of word-like representations. Through detailed analyses, the research sheds light on the effectiveness of top-down information in segmentation tasks, highlighting the importance of considering factors such as candidate boundaries in achieving optimal results. <br /><br />Summary: <div>
arXiv:2507.19204v1 Announce Type: cross 
Abstract: We investigate the problem of segmenting unlabeled speech into word-like units and clustering these to create a lexicon. Prior work can be categorized into two frameworks. Bottom-up methods first determine boundaries and then cluster the fixed segmented words into a lexicon. In contrast, top-down methods incorporate information from the clustered words to inform boundary selection. However, it is unclear whether top-down information is necessary to improve segmentation. To explore this, we look at two similar approaches that differ in whether top-down clustering informs boundary selection. Our simple bottom-up strategy predicts word boundaries using the dissimilarity between adjacent self-supervised features, then clusters the resulting segments to construct a lexicon. Our top-down system is an updated version of the ES-KMeans dynamic programming method that iteratively uses K-means to update its boundaries. On the five-language ZeroSpeech benchmarks, both approaches achieve comparable state-of-the-art results, with the bottom-up system being nearly five times faster. Through detailed analyses, we show that the top-down influence of ES-KMeans can be beneficial (depending on factors like the candidate boundaries), but in many cases the simple bottom-up method performs just as well. For both methods, we show that the clustering step is a limiting factor. Therefore, we recommend that future work focus on improved clustering techniques and learning more discriminative word-like representations. Project code repository: https://github.com/s-malan/prom-seg-clus.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Markov Categorical Framework for Language Modeling</title>
<link>https://arxiv.org/abs/2507.19247</link>
<guid>https://arxiv.org/abs/2507.19247</guid>
<content:encoded><![CDATA[
<div> Markov Categories, AR generation, negative log-likelihood, information flow, geometry<br />
<br />
Summary: 
This article introduces a new analytical framework for auto-regressive language models using Markov Categories (MCs). By deconstructing the generation process and the negative log-likelihood (NLL) objective, the framework provides insight into the success of speculative decoding methods like EAGLE by quantifying information surplus in hidden states. It also formalizes how NLL minimization helps the model learn not just the next token but the data's intrinsic conditional stochasticity through categorical entropy analysis. Moreover, the study demonstrates that NLL training acts as an implicit form of spectral contrastive learning, aligning the learned representation space with the eigenspectrum of a predictive similarity operator. This perspective sheds light on the underlying structural principles that make modern language models effective. <div>
arXiv:2507.19247v1 Announce Type: cross 
Abstract: Auto-regressive language models factorize sequence probabilities and are trained by minimizing the negative log-likelihood (NLL) objective. While empirically powerful, a deep theoretical understanding of why this simple objective yields such versatile representations remains elusive. This work introduces a unifying analytical framework using Markov Categories (MCs) to deconstruct the AR generation process and the NLL objective. We model the single-step generation map as a composition of Markov kernels in the category Stoch. This compositional view, when enriched with statistical divergences, allows us to dissect information flow and learned geometry. Our framework makes three main contributions. First, we provide a formal, information-theoretic rationale for the success of modern speculative decoding methods like EAGLE, quantifying the information surplus in hidden states that these methods exploit. Second, we formalize how NLL minimization forces the model to learn not just the next token, but the data's intrinsic conditional stochasticity, a process we analyze using categorical entropy. Third, and most centrally, we prove that NLL training acts as an implicit form of spectral contrastive learning. By analyzing the information geometry of the model's prediction head, we show that NLL implicitly forces the learned representation space to align with the eigenspectrum of a predictive similarity operator, thereby learning a geometrically structured space without explicit contrastive pairs. This compositional and information-geometric perspective reveals the deep structural principles underlying the effectiveness of modern LMs. Project Page: https://github.com/asiresearch/lm-theory
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.19333</link>
<guid>https://arxiv.org/abs/2507.19333</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval-augmented generation, large language models, noisy passages, Passage Injection, robustness <br />
Summary: <br />
Retrieval-augmented generation (RAG) is commonly used to enhance large language models (LLMs) with external knowledge, but noisy retrieved passages can reduce its effectiveness. To address this issue, Passage Injection is proposed, which integrates retrieved passages into LLMs' reasoning process to improve their ability to identify and resist noisy passages. Experimental results using BM25 as the retriever and four reasoning-enhanced LLMs on factual QA datasets demonstrate that Passage Injection significantly enhances RAG performance. The method is also effective in handling noisy retrieval settings and leveraging helpful passages. This study suggests that incorporating passages in LLMs' reasoning process is a promising approach for developing more robust RAG systems. The code for Passage Injection is available on GitHub for further exploration. <br /> <div>
arXiv:2507.19333v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) has been widely adopted to augment large language models (LLMs) with external knowledge for knowledge-intensive tasks. However, its effectiveness is often undermined by the presence of noisy (i.e., low-quality) retrieved passages. Enhancing LLMs' robustness to such noise is critical for improving the reliability of RAG systems. Recent advances have equipped LLMs with strong reasoning and self-reflection capabilities, allowing them to identify and correct errors in their reasoning process. Inspired by this ability, we propose Passage Injection-a simple yet effective method that explicitly incorporates retrieved passages into LLMs' reasoning process, aiming to enhance the model's ability to recognize and resist noisy passages. We validate Passage Injection under general RAG settings using BM25 as the retriever. Experiments on four reasoning-enhanced LLMs across four factual QA datasets demonstrate that Passage Injection significantly improves overall RAG performance. Further analysis on two noisy retrieval settings-random noise, where the model is provided irrelevant passages, and counterfactual noise, where it is given misleading passages-shows that Passage Injection consistently improves robustness. Controlled experiments confirm that Passage Injection can also effectively leverage helpful passages. These findings suggest that incorporating passages in LLMs' reasoning process is a promising direction for building more robust RAG systems. The code can be found \href{here}{https://github.com/mh-tang/Passage-Injection}.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences</title>
<link>https://arxiv.org/abs/2507.19362</link>
<guid>https://arxiv.org/abs/2507.19362</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models, Image Captioning, Evaluation, LOTUS, Bias-aware Assessments <br />
<br />
Summary: Large Vision-Language Models have improved image captioning by providing detailed descriptions. The LOTUS leaderboard addresses gaps in existing evaluations by standardizing criteria, considering biases, and incorporating user preferences. LOTUS evaluates caption quality, risks like hallucination, and societal biases such as gender bias. Analysis shows no single model excels in all criteria, and correlations exist between caption detail and bias risks. Preference-oriented evaluations highlight the importance of user priorities in model selection. <div>
arXiv:2507.19362v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) have transformed image captioning, shifting from concise captions to detailed descriptions. We introduce LOTUS, a leaderboard for evaluating detailed captions, addressing three main gaps in existing evaluations: lack of standardized criteria, bias-aware assessments, and user preference considerations. LOTUS comprehensively evaluates various aspects, including caption quality (e.g., alignment, descriptiveness), risks (\eg, hallucination), and societal biases (e.g., gender bias) while enabling preference-oriented evaluations by tailoring criteria to diverse user preferences. Our analysis of recent LVLMs reveals no single model excels across all criteria, while correlations emerge between caption detail and bias risks. Preference-oriented evaluations demonstrate that optimal model selection depends on user priorities.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Event Forecasting through Massive Training of Large Language Models: Challenges, Solutions, and Broader Impacts</title>
<link>https://arxiv.org/abs/2507.19477</link>
<guid>https://arxiv.org/abs/2507.19477</guid>
<content:encoded><![CDATA[
arXiv:2507.19477v1 Announce Type: cross 
Abstract: Many recent papers have studied the development of superforecaster-level event forecasting LLMs. While methodological problems with early studies cast doubt on the use of LLMs for event forecasting, recent studies with improved evaluation methods have shown that state-of-the-art LLMs are gradually reaching superforecaster-level performance, and reinforcement learning has also been reported to improve future forecasting. Additionally, the unprecedented success of recent reasoning models and Deep Research-style models suggests that technology capable of greatly improving forecasting performance has been developed. Therefore, based on these positive recent trends, we argue that the time is ripe for research on large-scale training of superforecaster-level event forecasting LLMs. We discuss two key research directions: training methods and data acquisition. For training, we first introduce three difficulties of LLM-based event forecasting training: noisiness-sparsity, knowledge cut-off, and simple reward structure problems. Then, we present related ideas to mitigate these problems: hypothetical event Bayesian networks, utilizing poorly-recalled and counterfactual events, and auxiliary reward signals. For data, we propose aggressive use of market, public, and crawling datasets to enable large-scale training and evaluation. Finally, we explain how these technical advances could enable AI to provide predictive intelligence to society in broader areas. This position paper presents promising specific paths and considerations for getting closer to superforecaster-level AI technology, aiming to call for researchers' interest in these directions.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents</title>
<link>https://arxiv.org/abs/2507.19478</link>
<guid>https://arxiv.org/abs/2507.19478</guid>
<content:encoded><![CDATA[
arXiv:2507.19478v1 Announce Type: cross 
Abstract: We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI automation agents across Windows, macOS, Linux, iOS, Android, and Web platforms. It comprises four levels: GUI Content Understanding, Element Grounding, Task Automation, and Task Collaboration, covering essential skills for GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA) metric to assess GUI agent execution efficiency in online automation scenarios. Through MMBench-GUI, we identify accurate visual grounding as a critical determinant of overall task success, emphasizing the substantial benefits of modular frameworks that integrate specialized grounding modules. Furthermore, to achieve reliable GUI automation, an agent requires strong task planning and cross-platform generalization abilities, with long-context memory, a broad action space, and long-term reasoning playing a critical role. More important, task efficiency remains a critically underexplored dimension, and all models suffer from substantial inefficiencies, with excessive redundant steps even when tasks are ultimately completed. The integration of precise localization, effective planning, and early stopping strategies is indispensable to enable truly efficient and scalable GUI automation. Our benchmark code, evaluation data, and running environment will be publicly available at https://github.com/open-compass/MMBench-GUI.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison of pipeline, sequence-to-sequence, and GPT models for end-to-end relation extraction: experiments with the rare disease use-case</title>
<link>https://arxiv.org/abs/2311.13729</link>
<guid>https://arxiv.org/abs/2311.13729</guid>
<content:encoded><![CDATA[
arXiv:2311.13729v3 Announce Type: replace 
Abstract: End-to-end relation extraction (E2ERE) is an important and realistic application of natural language processing (NLP) in biomedicine. In this paper, we aim to compare three prevailing paradigms for E2ERE using a complex dataset focused on rare diseases involving discontinuous and nested entities. We use the RareDis information extraction dataset to evaluate three competing approaches (for E2ERE): NER $\rightarrow$ RE pipelines, joint sequence to sequence models, and generative pre-trained transformer (GPT) models. We use comparable state-of-the-art models and best practices for each of these approaches and conduct error analyses to assess their failure modes. Our findings reveal that pipeline models are still the best, while sequence-to-sequence models are not far behind; GPT models with eight times as many parameters are worse than even sequence-to-sequence models and lose to pipeline models by over 10 F1 points. Partial matches and discontinuous entities caused many NER errors contributing to lower overall E2E performances. We also verify these findings on a second E2ERE dataset for chemical-protein interactions. Although generative LM-based methods are more suitable for zero-shot settings, when training data is available, our results show that it is better to work with more conventional models trained and tailored for E2ERE. More innovative methods are needed to marry the best of the both worlds from smaller encoder-decoder pipeline models and the larger GPT models to improve E2ERE. As of now, we see that well designed pipeline models offer substantial performance gains at a lower cost and carbon footprint for E2ERE. Our contribution is also the first to conduct E2ERE for the RareDis dataset.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spike No More: Stabilizing the Pre-training of Large Language Models</title>
<link>https://arxiv.org/abs/2312.16903</link>
<guid>https://arxiv.org/abs/2312.16903</guid>
<content:encoded><![CDATA[
arXiv:2312.16903v4 Announce Type: replace 
Abstract: Loss spikes often occur during pre-training of large language models. The spikes degrade the performance of large language models and sometimes ruin the pre-training. Since the pre-training needs a vast computational budget, we should avoid such spikes. Based on the assumption that the loss spike is caused by the sudden growth of the gradient norm, we explore factors to keep the gradient norm small through an analysis of the spectral norms of the Jacobian matrices for the sub-layers. Our findings suggest that stabilizing the pre-training process requires two conditions: small sub-layers and large shortcut. We conduct various experiments to empirically verify our theoretical analyses. Experimental results demonstrate that methods satisfying the conditions effectively prevent loss spikes during pre-training.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction?</title>
<link>https://arxiv.org/abs/2402.13470</link>
<guid>https://arxiv.org/abs/2402.13470</guid>
<content:encoded><![CDATA[
arXiv:2402.13470v2 Announce Type: replace 
Abstract: Cutting edge techniques developed in the general NLP domain are often subsequently applied to the high-value, data-rich biomedical domain. The past few years have seen generative language models (LMs), instruction finetuning, and few-shot learning become foci of NLP research. As such, generative LMs pretrained on biomedical corpora have proliferated and biomedical instruction finetuning has been attempted as well, all with the hope that domain specificity improves performance on downstream tasks. Given the nontrivial effort in training such models, we investigate what, if any, benefits they have in the key biomedical NLP task of relation extraction. Specifically, we address two questions: (1) Do LMs trained on biomedical corpora outperform those trained on general domain corpora? (2) Do models instruction finetuned on biomedical datasets outperform those finetuned on assorted datasets or those simply pretrained? We tackle these questions using existing LMs, testing across four datasets. In a surprising result, general-domain models typically outperformed biomedical-domain models. However, biomedical instruction finetuning improved performance to a similar degree as general instruction finetuning, despite having orders of magnitude fewer instructions. Our findings suggest it may be more fruitful to focus research effort on larger-scale biomedical instruction finetuning of general LMs over building domain-specific biomedical LMs
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection of Social-Media Texts</title>
<link>https://arxiv.org/abs/2406.12549</link>
<guid>https://arxiv.org/abs/2406.12549</guid>
<content:encoded><![CDATA[
arXiv:2406.12549v2 Announce Type: replace 
Abstract: Recent LLMs are able to generate high-quality multilingual texts, indistinguishable for humans from authentic human-written ones. Research in machine-generated text detection is however mostly focused on the English language and longer texts, such as news articles, scientific papers or student essays. Social-media texts are usually much shorter and often feature informal language, grammatical errors, or distinct linguistic items (e.g., emoticons, hashtags). There is a gap in studying the ability of existing methods in detection of such texts, reflected also in the lack of existing multilingual benchmark datasets. To fill this gap we propose the first multilingual (22 languages) and multi-platform (5 social media platforms) dataset for benchmarking machine-generated text detection in the social-media domain, called MultiSocial. It contains 472,097 texts, of which about 58k are human-written and approximately the same amount is generated by each of 7 multilingual LLMs. We use this benchmark to compare existing detection methods in zero-shot as well as fine-tuned form. Our results indicate that the fine-tuned detectors have no problem to be trained on social-media texts and that the platform selection for training matters.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Form Answers to Visual Questions from Blind and Low Vision People</title>
<link>https://arxiv.org/abs/2408.06303</link>
<guid>https://arxiv.org/abs/2408.06303</guid>
<content:encoded><![CDATA[
arXiv:2408.06303v2 Announce Type: replace 
Abstract: Vision language models can now generate long-form answers to questions about images - long-form visual question answers (LFVQA). We contribute VizWiz-LF, a dataset of long-form answers to visual questions posed by blind and low vision (BLV) users. VizWiz-LF contains 4.2k long-form answers to 600 visual questions, collected from human expert describers and six VQA models. We develop and annotate functional roles of sentences of LFVQA and demonstrate that long-form answers contain information beyond the question answer such as explanations and suggestions. We further conduct automatic and human evaluations with BLV and sighted people to evaluate long-form answers. BLV people perceive both human-written and generated long-form answers to be plausible, but generated answers often hallucinate incorrect visual details, especially for unanswerable visual questions (e.g., blurry or irrelevant images). To reduce hallucinations, we evaluate the ability of VQA models to abstain from answering unanswerable questions across multiple prompting strategies.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing biomolecular understanding and design following human instructions</title>
<link>https://arxiv.org/abs/2410.07919</link>
<guid>https://arxiv.org/abs/2410.07919</guid>
<content:encoded><![CDATA[
arXiv:2410.07919v2 Announce Type: replace 
Abstract: Understanding and designing biomolecules, such as proteins and small molecules, is central to advancing drug discovery, synthetic biology and enzyme engineering. Recent breakthroughs in artificial intelligence have revolutionized biomolecular research, achieving remarkable accuracy in biomolecular prediction and design. However, a critical gap remains between artificial intelligence's computational capabilities and researchers' intuitive goals, particularly in using natural language to bridge complex tasks with human intentions. Large language models have shown potential to interpret human intentions, yet their application to biomolecular research remains nascent due to challenges including specialized knowledge requirements, multimodal data integration, and semantic alignment between natural language and biomolecules. To address these limitations, we present InstructBioMol, a large language model designed to bridge natural language and biomolecules through a comprehensive any-to-any alignment of natural language, molecules and proteins. This model can integrate multimodal biomolecules as the input, and enable researchers to articulate design goals in natural language, providing biomolecular outputs that meet precise biological needs. Experimental results demonstrate that InstructBioMol can understand and design biomolecules following human instructions. In particular, it can generate drug molecules with a 10% improvement in binding affinity and design enzymes that achieve an enzyme-substrate pair prediction score of 70.4. This highlights its potential to transform real-world biomolecular research. The code is available at https://github.com/HICAI-ZJU/InstructBioMol.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans</title>
<link>https://arxiv.org/abs/2412.01131</link>
<guid>https://arxiv.org/abs/2412.01131</guid>
<content:encoded><![CDATA[
arXiv:2412.01131v4 Announce Type: replace 
Abstract: Recently, much work has concerned itself with the enigma of what exactly pretrained language models~(PLMs) learn about different aspects of language, and how they learn it. One stream of this type of research investigates the knowledge that PLMs have about semantic relations. However, many aspects of semantic relations were left unexplored. Generally, only one relation has been considered, namely hypernymy. Furthermore, previous work did not measure humans' performance on the same task as that performed by the PLMs. This means that at this point in time, there is only an incomplete view of the extent of these models' semantic relation knowledge. To address this gap, we introduce a comprehensive evaluation framework covering five relations beyond hypernymy, namely hyponymy, holonymy, meronymy, antonymy, and synonymy. We use five metrics (two newly introduced here) for recently untreated aspects of semantic relation knowledge, namely soundness, completeness, symmetry, prototypicality, and distinguishability. Using these, we can fairly compare humans and models on the same task. Our extensive experiments involve six PLMs, four masked and two causal language models. The results reveal a significant knowledge gap between humans and models for all semantic relations. In general, causal language models, despite their wide use, do not always perform significantly better than masked language models. Antonymy is the outlier relation where all models perform reasonably well.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs are Also Effective Embedding Models: An In-depth Overview</title>
<link>https://arxiv.org/abs/2412.12591</link>
<guid>https://arxiv.org/abs/2412.12591</guid>
<content:encoded><![CDATA[
arXiv:2412.12591v2 Announce Type: replace 
Abstract: Large language models (LLMs) have revolutionized natural language processing by achieving state-of-the-art performance across various tasks. Recently, their effectiveness as embedding models has gained attention, marking a paradigm shift from traditional encoder-only models like ELMo and BERT to decoder-only, large-scale LLMs such as GPT, LLaMA, and Mistral. This survey provides an in-depth overview of this transition, beginning with foundational techniques before the LLM era, followed by LLM-based embedding models through two main strategies to derive embeddings from LLMs. 1) Direct prompting: We mainly discuss the prompt designs and the underlying rationale for deriving competitive embeddings. 2) Data-centric tuning: We cover extensive aspects that affect tuning an embedding model, including model architecture, training objectives, data constructions, etc. Upon the above, we also cover advanced methods for producing embeddings from longer texts, multilingual, code, cross-modal data, as well as reasoning-aware and other domain-specific scenarios. Furthermore, we discuss factors affecting choices of embedding models, such as performance/efficiency comparisons, dense vs sparse embeddings, pooling strategies, and scaling law. Lastly, the survey highlights the limitations and challenges in adapting LLMs for embeddings, including cross-task embedding quality, trade-offs between efficiency and accuracy, low-resource, long-context, data bias, robustness, etc. This survey serves as a valuable resource for researchers and practitioners by synthesizing current advancements, highlighting key challenges, and offering a comprehensive framework for future work aimed at enhancing the effectiveness and efficiency of LLMs as embedding models.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of LLM Vulnerabilities to Being Misused for Personalized Disinformation Generation</title>
<link>https://arxiv.org/abs/2412.13666</link>
<guid>https://arxiv.org/abs/2412.13666</guid>
<content:encoded><![CDATA[
arXiv:2412.13666v2 Announce Type: replace 
Abstract: The capabilities of recent large language models (LLMs) to generate high-quality content indistinguishable by humans from human-written texts raises many concerns regarding their misuse. Previous research has shown that LLMs can be effectively misused for generating disinformation news articles following predefined narratives. Their capabilities to generate personalized (in various aspects) content have also been evaluated and mostly found usable. However, a combination of personalization and disinformation abilities of LLMs has not been comprehensively studied yet. Such a dangerous combination should trigger integrated safety filters of the LLMs, if there are some. This study fills this gap by evaluating vulnerabilities of recent open and closed LLMs, and their willingness to generate personalized disinformation news articles in English. We further explore whether the LLMs can reliably meta-evaluate the personalization quality and whether the personalization affects the generated-texts detectability. Our results demonstrate the need for stronger safety-filters and disclaimers, as those are not properly functioning in most of the evaluated LLMs. Additionally, our study revealed that the personalization actually reduces the safety-filter activations; thus effectively functioning as a jailbreak. Such behavior must be urgently addressed by LLM developers and service providers.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation</title>
<link>https://arxiv.org/abs/2501.12612</link>
<guid>https://arxiv.org/abs/2501.12612</guid>
<content:encoded><![CDATA[
arXiv:2501.12612v3 Announce Type: replace 
Abstract: Text-to-image (T2I) models have rapidly advanced, enabling the generation of high-quality images from text prompts across various domains. However, these models present notable safety concerns, including the risk of generating harmful, biased, or private content. Current research on assessing T2I safety remains in its early stages. While some efforts have been made to evaluate models on specific safety dimensions, many critical risks remain unexplored. To address this gap, we introduce T2ISafety, a safety benchmark that evaluates T2I models across three key domains: toxicity, fairness, and bias. We build a detailed hierarchy of 12 tasks and 44 categories based on these three domains, and meticulously collect 70K corresponding prompts. Based on this taxonomy and prompt set, we build a large-scale T2I dataset with 68K manually annotated images and train an evaluator capable of detecting critical risks that previous work has failed to identify, including risks that even ultra-large proprietary models like GPTs cannot correctly detect. We evaluate 12 prominent diffusion models on T2ISafety and reveal several concerns including persistent issues with racial fairness, a tendency to generate toxic content, and significant variation in privacy protection across the models, even with defense methods like concept erasing. Data and evaluator are released under https://github.com/adwardlee/t2i_safety.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Sparse Fine-Tuning with Low Quantization Error via Neural Network Pruning</title>
<link>https://arxiv.org/abs/2502.11439</link>
<guid>https://arxiv.org/abs/2502.11439</guid>
<content:encoded><![CDATA[
arXiv:2502.11439v2 Announce Type: replace 
Abstract: Fine-tuning is an important step in adapting foundation models such as large language models to downstream tasks. To make this step more accessible to users with limited computational budgets, it is crucial to develop fine-tuning methods that are memory and computationally efficient. Sparse Fine-tuning (SpFT) and Low-rank adaptation (LoRA) are two frameworks that have emerged for addressing this problem and have been adopted widely in practice. In this work, we develop a new SpFT framework, based on ideas from neural network pruning. At a high level, we first identify ``important'' neurons/nodes using feature importance metrics from network pruning (specifically, we use the structural pruning method), and then perform fine-tuning by restricting to weights involving these neurons. Experiments on common language tasks show our method improves SpFT's memory efficiency by 20-50\% while matching the accuracy of state-of-the-art methods like LoRA's variants.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs</title>
<link>https://arxiv.org/abs/2502.14561</link>
<guid>https://arxiv.org/abs/2502.14561</guid>
<content:encoded><![CDATA[
arXiv:2502.14561v3 Announce Type: replace 
Abstract: This work investigates the ability of open Large Language Models (LLMs) to predict citation intent through in-context learning and fine-tuning. Unlike traditional approaches relying on domain-specific pre-trained models like SciBERT, we demonstrate that general-purpose LLMs can be adapted to this task with minimal task-specific data. We evaluate twelve model variations across five prominent open LLM families using zero-, one-, few-, and many-shot prompting. Our experimental study identifies the top-performing model and prompting parameters through extensive in-context learning experiments. We then demonstrate the significant impact of task-specific adaptation by fine-tuning this model, achieving a relative F1-score improvement of 8% on the SciCite dataset and 4.3% on the ACL-ARC dataset compared to the instruction-tuned baseline. These findings provide valuable insights for model selection and prompt engineering. Additionally, we make our end-to-end evaluation framework and models openly available for future use.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Palm: A Culturally Inclusive and Linguistically Diverse Dataset for Arabic LLMs</title>
<link>https://arxiv.org/abs/2503.00151</link>
<guid>https://arxiv.org/abs/2503.00151</guid>
<content:encoded><![CDATA[
arXiv:2503.00151v2 Announce Type: replace 
Abstract: As large language models (LLMs) become increasingly integrated into daily life, ensuring their cultural sensitivity and inclusivity is paramount. We introduce our dataset, a year-long community-driven project covering all 22 Arab countries. The dataset includes instructions (input, response pairs) in both Modern Standard Arabic (MSA) and dialectal Arabic (DA), spanning 20 diverse topics. Built by a team of 44 researchers across the Arab world, all of whom are authors of this paper, our dataset offers a broad, inclusive perspective. We use our dataset to evaluate the cultural and dialectal capabilities of several frontier LLMs, revealing notable limitations. For instance, while closed-source LLMs generally exhibit strong performance, they are not without flaws, and smaller open-source models face greater challenges. Moreover, certain countries (e.g., Egypt, the UAE) appear better represented than others (e.g., Iraq, Mauritania, Yemen). Our annotation guidelines, code, and data for reproducibility are publicly available.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Debiasing Across Class and Sample Levels for Fairer Prompting Accuracy</title>
<link>https://arxiv.org/abs/2503.05157</link>
<guid>https://arxiv.org/abs/2503.05157</guid>
<content:encoded><![CDATA[
arXiv:2503.05157v4 Announce Type: replace 
Abstract: Language models are strong few-shot learners and achieve good overall accuracy in text classification tasks, masking the fact that their results suffer from great class accuracy imbalance. We believe that the pursuit of overall accuracy should not come from enriching the strong classes, but from raising up the weak ones. To address the imbalance, we propose a Heaviside step function based ensemble debiasing method, which enables flexible rectifications of in-context learned class probabilities at both class and sample levels. Evaluations with Llama-2-13B on seven text classification benchmarks show that our approach achieves state-of-the-art overall accuracy gains with balanced class accuracies. More importantly, we perform analyses on the resulted probability correction scheme, showing that sample-level corrections are necessary to elevate weak classes. Due to effectively correcting weak classes, our method also brings significant performance gains to a larger model variant, Llama-2-70B, especially on a biomedical domain task, further demonstrating the necessity of ensemble debiasing at both levels. Our source code is available at https://github.com/NUS-HPC-AI-Lab/DCS.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relation Extraction with Instance-Adapted Predicate Descriptions</title>
<link>https://arxiv.org/abs/2503.17799</link>
<guid>https://arxiv.org/abs/2503.17799</guid>
<content:encoded><![CDATA[
arXiv:2503.17799v2 Announce Type: replace 
Abstract: Relation extraction (RE) is a standard information extraction task playing a major role in downstream applications such as knowledge discovery and question answering. Although decoder-only large language models are excelling in generative tasks, smaller encoder models are still the go to architecture for RE. In this paper, we revisit fine-tuning such smaller models using a novel dual-encoder architecture with a joint contrastive and cross-entropy loss. Unlike previous methods that employ a fixed linear layer for predicate representations, our approach uses a second encoder to compute instance-specific predicate representations by infusing them with real entity spans from corresponding input instances. We conducted experiments on two biomedical RE datasets and two general domain datasets. Our approach achieved F1 score improvements ranging from 1% to 2% over state-of-the-art methods with a simple but elegant formulation. Ablation studies justify the importance of various components built into the proposed architecture.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations</title>
<link>https://arxiv.org/abs/2504.21019</link>
<guid>https://arxiv.org/abs/2504.21019</guid>
<content:encoded><![CDATA[
arXiv:2504.21019v2 Announce Type: replace 
Abstract: The growing popularity of large language models has raised concerns regarding the potential to misuse AI-generated text (AIGT). It becomes increasingly critical to establish an excellent AIGT detection method with high generalization and robustness. However, existing methods either focus on model generalization or concentrate on robustness. The unified mechanism, to simultaneously address the challenges of generalization and robustness, is less explored. In this paper, we argue that robustness can be view as a specific form of domain shift, and empirically reveal an intrinsic mechanism for model generalization of AIGT detection task. Then, we proposed a novel AIGT detection method (DP-Net) via dynamic perturbations introduced by a reinforcement learning with elaborated reward and action. Experimentally, extensive results show that the proposed DP-Net significantly outperforms some state-of-the-art AIGT detection methods for generalization capacity in three cross-domain scenarios. Meanwhile, the DP-Net achieves best robustness under two text adversarial attacks. The code is publicly available at https://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale</title>
<link>https://arxiv.org/abs/2505.03005</link>
<guid>https://arxiv.org/abs/2505.03005</guid>
<content:encoded><![CDATA[
arXiv:2505.03005v3 Announce Type: replace 
Abstract: We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.
  Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16142</link>
<guid>https://arxiv.org/abs/2505.16142</guid>
<content:encoded><![CDATA[
arXiv:2505.16142v3 Announce Type: replace 
Abstract: Distilling reasoning paths from teacher to student models via supervised fine-tuning (SFT) provides a shortcut for improving the reasoning ability of smaller Large Language Models (LLMs). However, the reasoning paths generated by teacher models often reflect only surface-level traces of their underlying authentic reasoning. Insights from cognitive neuroscience suggest that authentic reasoning involves a complex interweaving between meta-reasoning (which selects appropriate sub-problems from multiple candidates) and solving (which addresses the sub-problem). This implies authentic reasoning has an implicit multi-branch structure. Supervised fine-tuning collapses this rich structure into a flat sequence of token prediction in the teacher's reasoning path, preventing effective distillation of this structure to students. To address this limitation, we propose RLKD, a reinforcement learning (RL)-based distillation framework guided by a novel Generative Structure Reward Model (GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving steps and computes rewards to measure structural alignment between student and teacher reasoning. RLKD combines this reward with RL, enabling student LLMs to internalize the teacher's implicit multi-branch reasoning structure rather than merely mimicking fixed output paths. Experiments show RLKD surpasses standard SFT-RL pipelines even when trained on 0.1% of data under an RL-only regime, unlocking greater student reasoning potential than SFT-based distillation.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue</title>
<link>https://arxiv.org/abs/2505.19630</link>
<guid>https://arxiv.org/abs/2505.19630</guid>
<content:encoded><![CDATA[
arXiv:2505.19630v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Single-round consultation systems require patients to describe all symptoms upfront, leading to vague diagnosis with unclear complaints. Traditional multi-turn dialogue models, constrained by static supervised learning, lack flexibility and fail to intelligently extract key clinical information. To address these limitations, we propose \Ours{}, a reinforcement learning (RL)-based multi-agent collaborative framework that models medical consultations as a dynamic decision-making process under uncertainty. The doctor agent continuously optimizes its questioning strategy within the RL framework through multi-turn interactions with the patient agent, dynamically adjusting its information-gathering path based on comprehensive rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop interaction strategies aligned with clinical reasoning logic, rather than superficially imitating patterns in existing dialogue data. Notably, we constructed MTMedDialog, the first English multi-turn medical consultation dataset capable of simulating patient interactions. Experiments demonstrate that \Ours{} outperforms existing models in both multi-turn reasoning capability and final diagnostic performance. This approach shows immense practical value by reducing misdiagnosis risks in time-pressured settings, freeing clinicians for complex cases, and pioneering a strategy to optimize medical resource allocation and alleviate workforce shortages. Code and data are available at https://github.com/JarvisUSTC/DoctorAgent-RL
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience</title>
<link>https://arxiv.org/abs/2506.00842</link>
<guid>https://arxiv.org/abs/2506.00842</guid>
<content:encoded><![CDATA[
arXiv:2506.00842v2 Announce Type: replace 
Abstract: Large language models (LLMs) achieve strong performance on plain text tasks but underperform on structured data like tables and databases. Potential challenges arise from their underexposure during pre-training and rigid text-to-structure transfer mechanisms. Unlike humans who seamlessly apply learned patterns across data modalities, LLMs struggle to infer implicit relationships embedded in tabular formats, especially in the absence of explicit structural guidance. To bridge this cognitive gap, we introduce Contrastive Retrieval-Augmented Generation on Experience (CoRE), a framework that builds experience memory representations and enhances generalization through contrastive In-Context Learning (ICL) to simulate human-like knowledge transfer. Experiments on Text-to-SQL and TableQA show CoRE significantly improves performance, achieving average gains of 3.44% and 4.24%, with up to 17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated Experience Memory expands training data 8-9x, enhancing diversity and domain coverage. This training-free and continual method propels LLMs toward structured knowledge expertise.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems</title>
<link>https://arxiv.org/abs/2506.04076</link>
<guid>https://arxiv.org/abs/2506.04076</guid>
<content:encoded><![CDATA[
arXiv:2506.04076v2 Announce Type: replace 
Abstract: Verbatim transcription for automatic speaking assessment demands accurate capture of disfluencies, crucial for downstream tasks like error analysis and feedback. However, many ASR systems discard or generalize hesitations, losing important acoustic details. We fine-tune Whisper models on the Speak & Improve 2025 corpus using low-rank adaptation (LoRA), without recourse to external audio training data. We compare three annotation schemes: removing hesitations (Pure), generic tags (Rich), and acoustically precise fillers inferred by Gemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge system achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge experiments reveal that fine-tuning Whisper Large V3 Turbo with the "Extra" scheme yielded a 5.5% WER, an 11.3% relative improvement over the "Pure" scheme (6.2% WER). This demonstrates that explicit, realistic filled-pause labeling significantly enhances ASR accuracy for verbatim L2 speech transcription.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>References Matter: Investigating the Impact of Reference Set Variation on Summarization Evaluation</title>
<link>https://arxiv.org/abs/2506.14335</link>
<guid>https://arxiv.org/abs/2506.14335</guid>
<content:encoded><![CDATA[
arXiv:2506.14335v2 Announce Type: replace 
Abstract: Human language production exhibits remarkable richness and variation, reflecting diverse communication styles and intents. However, this variation is often overlooked in summarization evaluation. While having multiple reference summaries is known to improve correlation with human judgments, the impact of the reference set on reference-based metrics has not been systematically investigated. This work examines the sensitivity of widely used reference-based metrics in relation to the choice of reference sets, analyzing three diverse multi-reference summarization datasets: SummEval, GUMSum, and DUC2004. We demonstrate that many popular metrics exhibit significant instability. This instability is particularly concerning for n-gram-based metrics like ROUGE, where model rankings vary depending on the reference sets, undermining the reliability of model comparisons. We also collect human judgments on LLM outputs for genre-diverse data and examine their correlation with metrics to supplement existing findings beyond newswire summaries, finding weak-to-no correlation. Taken together, we recommend incorporating reference set variation into summarization evaluation to enhance consistency alongside correlation with human judgments, especially when evaluating LLMs.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JCAPT: A Joint Modeling Approach for CAPT</title>
<link>https://arxiv.org/abs/2506.19315</link>
<guid>https://arxiv.org/abs/2506.19315</guid>
<content:encoded><![CDATA[
arXiv:2506.19315v2 Announce Type: replace 
Abstract: Effective pronunciation feedback is critical in second language (L2) learning, for which computer-assisted pronunciation training (CAPT) systems often encompass two key tasks: automatic pronunciation assessment (APA) and mispronunciation detection and diagnosis (MDD). Recent work has shown that joint modeling of these two tasks can yield mutual benefits. Our unified framework leverages Mamba, a selective state space model (SSM), while integrating phonological features and think token strategies to jointly enhance interpretability and fine-grained temporal reasoning in APA and MDD. To our knowledge, this is the first study to combine phonological attribution, SSM-based modeling, and prompting in CAPT. A series of experiments conducted on the speechocean762 benchmark demonstrate that our model consistently outperforms prior methods, particularly on the MDD task.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise Contrastive Estimation-based Matching Framework for Low-Resource Security Attack Pattern Recognition</title>
<link>https://arxiv.org/abs/2401.10337</link>
<guid>https://arxiv.org/abs/2401.10337</guid>
<content:encoded><![CDATA[
arXiv:2401.10337v4 Announce Type: replace-cross 
Abstract: Tactics, Techniques and Procedures (TTPs) represent sophisticated attack patterns in the cybersecurity domain, described encyclopedically in textual knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP mapping, is an important and challenging task. Conventional learning approaches often target the problem in the classical multi-class or multilabel classification setting. This setting hinders the learning ability of the model due to a large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. We formulate the problem in a different learning paradigm, where the assignment of a text to a TTP label is decided by the direct semantic similarity between the two, thus reducing the complexity of competing solely over the large labeling space. To that end, we propose a neural matching architecture with an effective sampling-based learn-to-compare mechanism, facilitating the learning process of the matching model despite constrained resources.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare</title>
<link>https://arxiv.org/abs/2405.06270</link>
<guid>https://arxiv.org/abs/2405.06270</guid>
<content:encoded><![CDATA[
arXiv:2405.06270v4 Announce Type: replace-cross 
Abstract: Clinical decision support systems require models that are not only highly accurate but also equitable and sensitive to the implications of missed diagnoses. In this study, we introduce a knowledge-guided in-context learning (ICL) framework designed to enable large language models (LLMs) to effectively process structured clinical data. Our approach integrates domain-specific feature groupings, carefully balanced few-shot examples, and task-specific prompting strategies. We systematically evaluate this method across seventy distinct ICL designs by various prompt variations and two different communication styles-natural-language narrative and numeric conversational-and compare its performance to robust classical machine learning (ML) benchmarks on tasks involving heart disease and diabetes prediction.
  Our findings indicate that while traditional ML models maintain superior performance in balanced precision-recall scenarios, LLMs employing narrative prompts with integrated domain knowledge achieve higher recall and significantly reduce gender bias, effectively narrowing fairness disparities by an order of magnitude. Despite the current limitation of increased inference latency, LLMs provide notable advantages, including the capacity for zero-shot deployment and enhanced equity. This research offers the first comprehensive analysis of ICL design considerations for applying LLMs to tabular clinical tasks and highlights distillation and multimodal extensions as promising directions for future research.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Investigation of Prompt Variations for Zero-shot LLM-based Rankers</title>
<link>https://arxiv.org/abs/2406.14117</link>
<guid>https://arxiv.org/abs/2406.14117</guid>
<content:encoded><![CDATA[
arXiv:2406.14117v4 Announce Type: replace-cross 
Abstract: We provide a systematic understanding of the impact of specific components and wordings used in prompts on the effectiveness of rankers based on zero-shot Large Language Models (LLMs). Several zero-shot ranking methods based on LLMs have recently been proposed. Among many aspects, methods differ across (1) the ranking algorithm they implement, e.g., pointwise vs. listwise, (2) the backbone LLMs used, e.g., GPT3.5 vs. FLAN-T5, (3) the components and wording used in prompts, e.g., the use or not of role-definition (role-playing) and the actual words used to express this. It is currently unclear whether performance differences are due to the underlying ranking algorithm, or because of spurious factors such as better choice of words used in prompts. This confusion risks to undermine future research. Through our large-scale experimentation and analysis, we find that ranking algorithms do contribute to differences between methods for zero-shot LLM ranking. However, so do the LLM backbones -- but even more importantly, the choice of prompt components and wordings affect the ranking. In fact, in our experiments, we find that, at times, these latter elements have more impact on the ranker's effectiveness than the actual ranking algorithms, and that differences among ranking methods become more blurred when prompt variations are considered.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolACE: Winning the Points of LLM Function Calling</title>
<link>https://arxiv.org/abs/2409.00920</link>
<guid>https://arxiv.org/abs/2409.00920</guid>
<content:encoded><![CDATA[
arXiv:2409.00920v2 Announce Type: replace-cross 
Abstract: Function calling significantly extends the application boundary of large language models, where high-quality and diverse training data is critical for unlocking this capability. However, real function-calling data is quite challenging to collect and annotate, while synthetic data generated by existing pipelines tends to lack coverage and accuracy. In this paper, we present ToolACE, an automatic agentic pipeline designed to generate accurate, complex, and diverse tool-learning data. ToolACE leverages a novel self-evolution synthesis process to curate a comprehensive API pool of 26,507 diverse APIs. Dialogs are further generated through the interplay among multiple agents, guided by a formalized thinking process. To ensure data accuracy, we implement a dual-layer verification system combining rule-based and model-based checks. We demonstrate that models trained on our synthesized data, even with only 8B parameters, achieve state-of-the-art performance on the Berkeley Function-Calling Leaderboard, rivaling the latest GPT-4 models. Our model and a subset of the data are publicly available at https://huggingface.co/Team-ACE.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verbalized Representation Learning for Interpretable Few-Shot Generalization</title>
<link>https://arxiv.org/abs/2411.18651</link>
<guid>https://arxiv.org/abs/2411.18651</guid>
<content:encoded><![CDATA[
arXiv:2411.18651v2 Announce Type: replace-cross 
Abstract: Humans recognize objects after observing only a few examples, a remarkable capability enabled by their inherent language understanding of the real-world environment. Developing verbalized and interpretable representation can significantly improve model generalization in low-data settings. In this work, we propose Verbalized Representation Learning (VRL), a novel approach for automatically extracting human-interpretable features for object recognition using few-shot data. Our method uniquely captures inter-class differences and intra-class commonalities in the form of natural language by employing a Vision-Language Model (VLM) to identify key discriminative features between different classes and shared characteristics within the same class. These verbalized features are then mapped to numeric vectors through the VLM. The resulting feature vectors can be further utilized to train and infer with downstream classifiers. Experimental results show that, at the same model scale, VRL achieves a 24% absolute improvement over prior state-of-the-art methods while using 95% less data and a smaller mode. Furthermore, compared to human-labeled attributes, the features learned by VRL exhibit a 20% absolute gain when used for downstream classification tasks. Code is available at: https://github.com/joeyy5588/VRL/tree/main.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Multimodal Large Language Models via Dynamic Visual-Token Exit and the Empirical Findings</title>
<link>https://arxiv.org/abs/2411.19628</link>
<guid>https://arxiv.org/abs/2411.19628</guid>
<content:encoded><![CDATA[
arXiv:2411.19628v2 Announce Type: replace-cross 
Abstract: The excessive use of visual tokens in existing Multimoal Large Language Models (MLLMs) often exhibits obvious redundancy and brings in prohibitively expensive computation. To gain insights into this problem, we first conduct extensive empirical studies on the attention behaviors of MLLMs, and summarize three main inference stages in MLLMs: (i) Early fusion between tokens is first accomplished quickly. (ii) Intra-modality modeling then comes to play. (iii) Multimodal reasoning} resumes and lasts until the end of inference. In particular, we reveal that visual tokens will stop contributing to reasoning when the text tokens receive enough image information, yielding obvious visual redundancy. Based on these generalized observations, we propose a simple yet effective method to improve the efficiency of MLLMs, termed dynamic visual-token exit (DyVTE). DyVTE uses lightweight hyper-networks to perceive the text token status and decide the removal of all visual tokens after a certain layer, thereby addressing the observed visual redundancy. To validate VTE, we apply it to a set of MLLMs, including LLaVA, VILA, Eagle and InternVL, and conduct extensive experiments on a bunch of benchmarks. The experiment results not only show the effectiveness of our VTE in improving MLLMs' efficiency, but also yield the general modeling patterns of MLLMs, well facilitating the in-depth understanding of MLLMs. Our code is released at https://github.com/DoubtedSteam/DyVTE.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyze Feature Flow to Enhance Interpretation and Steering in Language Models</title>
<link>https://arxiv.org/abs/2502.03032</link>
<guid>https://arxiv.org/abs/2502.03032</guid>
<content:encoded><![CDATA[
arXiv:2502.03032v3 Announce Type: replace-cross 
Abstract: We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations. Crucially, we demonstrate how these cross-layer feature maps facilitate direct steering of model behavior by amplifying or suppressing chosen features, achieving targeted thematic control in text generation. Together, our findings highlight the utility of a causal, cross-layer interpretability framework that not only clarifies how features develop through forward passes but also provides new means for transparent manipulation of large language models.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distillation Scaling Laws</title>
<link>https://arxiv.org/abs/2502.08606</link>
<guid>https://arxiv.org/abs/2502.08606</guid>
<content:encoded><![CDATA[
arXiv:2502.08606v2 Announce Type: replace-cross 
Abstract: We propose a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings mitigate the risks associated with large-scale distillation by enabling compute-optimal allocation for both the teacher and student to maximize student performance. We provide compute-optimal distillation recipes for two key scenarios: when a teacher already exists, and when a teacher needs training. In settings involving many students or an existing teacher, distillation outperforms supervised learning up to a compute level that scales predictably with student size. Conversely, if only one student is to be distilled and a teacher also requires training, supervised learning is generally preferable. Additionally, our large-scale study of distillation increases our understanding of the process and helps inform experimental design.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Super Agent System with Hybrid AI Routers</title>
<link>https://arxiv.org/abs/2504.10519</link>
<guid>https://arxiv.org/abs/2504.10519</guid>
<content:encoded><![CDATA[
arXiv:2504.10519v2 Announce Type: replace-cross 
Abstract: AI Agents powered by Large Language Models are transforming the world through enormous applications. A super agent has the potential to fulfill diverse user needs, such as summarization, coding, and research, by accurately understanding user intent and leveraging the appropriate tools to solve tasks. However, to make such an agent viable for real-world deployment and accessible at scale, significant optimizations are required to ensure high efficiency and low cost. This position paper presents a design of the Super Agent System powered by the hybrid AI routers. Upon receiving a user prompt, the system first detects the intent of the user, then routes the request to specialized task agents with the necessary tools or automatically generates agentic workflows. In practice, most applications directly serve as AI assistants on edge devices such as phones and robots. As different language models vary in capability and cloud-based models often entail high computational costs, latency, and privacy concerns, we then explore the hybrid mode where the router dynamically selects between local and cloud models based on task complexity. Finally, we introduce the blueprint of an on-device super agent enhanced with cloud. With advances in multi-modality models and edge hardware, we envision that most computations can be handled locally, with cloud collaboration only as needed. Such architecture paves the way for super agents to be seamlessly integrated into everyday life in the near future.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Flow: Perspectives, Scenarios, and Approaches</title>
<link>https://arxiv.org/abs/2506.12479</link>
<guid>https://arxiv.org/abs/2506.12479</guid>
<content:encoded><![CDATA[
arXiv:2506.12479v3 Announce Type: replace-cross 
Abstract: Pioneered by the foundational information theory by Claude Shannon and the visionary framework of machine intelligence by Alan Turing, the convergent evolution of information and communication technologies (IT/CT) has created an unbroken wave of connectivity and computation. This synergy has sparked a technological revolution, now reaching its peak with large artificial intelligence (AI) models that are reshaping industries and redefining human-machine collaboration. However, the realization of ubiquitous intelligence faces considerable challenges due to substantial resource consumption in large models and high communication bandwidth demands. To address these challenges, AI Flow has been introduced as a multidisciplinary framework that integrates cutting-edge IT and CT advancements, with a particular emphasis on the following three key points. First, device-edge-cloud framework serves as the foundation, which integrates end devices, edge servers, and cloud clusters to optimize scalability and efficiency for low-latency model inference. Second, we introduce the concept of familial models, which refers to a series of different-sized models with aligned hidden features, enabling effective collaboration and the flexibility to adapt to varying resource constraints and dynamic scenarios. Third, connectivity- and interaction-based intelligence emergence is a novel paradigm of AI Flow. By leveraging communication networks to enhance connectivity, the collaboration among AI models across heterogeneous nodes achieves emergent intelligence that surpasses the capability of any single model. The innovations of AI Flow provide enhanced intelligence, timely responsiveness, and ubiquitous accessibility to AI services, paving the way for the tighter fusion of AI techniques and communication systems.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weak-to-Strong Jailbreaking on Large Language Models</title>
<link>https://arxiv.org/abs/2401.17256</link>
<guid>https://arxiv.org/abs/2401.17256</guid>
<content:encoded><![CDATA[
<div> jailbreak attack, large language models, weak-to-strong attack, misalignment rate, defense strategy
Summary:
The research addresses the vulnerability of large language models (LLMs) to jailbreak attacks, which can result in the generation of harmful or biased text. Existing jailbreaking methods are computationally costly, prompting the proposal of the weak-to-strong jailbreaking attack as an efficient inference time attack for aligned LLMs. This attack leverages two smaller models to modify the decoding probabilities of a larger safe model, increasing the misalignment rate dramatically. The study evaluates the attack on five open-source LLMs, demonstrating its effectiveness with minimal computational overhead. The research highlights the need for urgent safety measures in aligning LLMs and proposes a defense strategy, although developing more robust defenses presents a challenge. The code for replicating the attack method is available for further exploration. <br /><br />Summary: <div>
arXiv:2401.17256v5 Announce Type: replace 
Abstract: Large language models (LLMs) are vulnerable to jailbreak attacks - resulting in harmful, unethical, or biased text generations. However, existing jailbreaking methods are computationally costly. In this paper, we propose the weak-to-strong jailbreaking attack, an efficient inference time attack for aligned LLMs to produce harmful text. Our key intuition is based on the observation that jailbroken and aligned models only differ in their initial decoding distributions. The weak-to-strong attack's key technical insight is using two smaller models (a safe and an unsafe one) to adversarially modify a significantly larger safe model's decoding probabilities. We evaluate the weak-to-strong attack on 5 diverse open-source LLMs from 3 organizations. The results show our method can increase the misalignment rate to over 99% on two datasets with just one forward pass per example. Our study exposes an urgent safety issue that needs to be addressed when aligning LLMs. As an initial attempt, we propose a defense strategy to protect against such attacks, but creating more advanced defenses remains challenging. The code for replicating the method is available at https://github.com/XuandongZhao/weak-to-strong
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1</title>
<link>https://arxiv.org/abs/2507.08621</link>
<guid>https://arxiv.org/abs/2507.08621</guid>
<content:encoded><![CDATA[
<div> Argument mining, large language models, argument classification, dataset analysis, model comparison<br />
<br />
Summary: 
This study explores the use of large language models (LLMs) in argument mining, specifically focusing on argument classification. Various LLMs, including GPT and Deepseek, are tested on argument datasets such as Args.me and UKP. The results show that ChatGPT-4o performs the best in argument classification benchmarks, while Deepseek-R1 excels among models with reasoning capabilities. Despite their accuracy, both models still make errors, with common errors discussed. This analysis is the first to extensively examine these datasets using LLMs and prompt algorithms, revealing weaknesses in the current prompt algorithms for argument analysis and suggesting directions for improvement. The study provides valuable insights into the strengths and limitations of LLMs in argument mining and classification. <br /> <div>
arXiv:2507.08621v2 Announce Type: replace 
Abstract: Argument mining (AM) is an interdisciplinary research field that integrates insights from logic, philosophy, linguistics, rhetoric, law, psychology, and computer science. It involves the automatic identification and extraction of argumentative components, such as premises and claims, and the detection of relationships between them, such as support, attack, or neutrality. Recently, the field has advanced significantly, especially with the advent of large language models (LLMs), which have enhanced the efficiency of analyzing and extracting argument semantics compared to traditional methods and other deep learning models. There are many benchmarks for testing and verifying the quality of LLM, but there is still a lack of research and results on the operation of these models in publicly available argument classification databases. This paper presents a study of a selection of LLM's, using diverse datasets such as Args.me and UKP. The models tested include versions of GPT, Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms the others in the argument classification benchmarks. In case of models incorporated with reasoning capabilities, the Deepseek-R1 shows its superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still make errors. The most common errors are discussed for all models. To our knowledge, the presented work is the first broader analysis of the mentioned datasets using LLM and prompt algorithms. The work also shows some weaknesses of known prompt algorithms in argument analysis, while indicating directions for their improvement. The added value of the work is the in-depth analysis of the available argument datasets and the demonstration of their shortcomings.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.17842</link>
<guid>https://arxiv.org/abs/2507.17842</guid>
<content:encoded><![CDATA[
<div> RL, reinforcement learning, LLM, large language models, human behavior simulation 

Summary: 
- The paper introduces Shop-R1, a novel reinforcement learning framework for enhancing the reasoning ability of large language models (LLMs) in simulating human behavior in online shopping environments.
- Shop-R1 divides the simulation task into two stages: rationale generation and action prediction, guided by distinct reward signals.
- Self-supervised rationale generation utilizes internal model signals to guide the reasoning process.
- For action prediction, a hierarchical reward structure with difficulty-aware scaling is proposed to prevent reward hacking and allow fine-grained reward assignment.
- Experimental results demonstrate a relative improvement of over 65% compared to the baseline approach. 

<br /><br />Summary: <div>
arXiv:2507.17842v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently demonstrated strong potential in generating 'believable human-like' behavior in web environments. Prior work has explored augmenting training data with LLM-synthesized rationales and applying supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can improve downstream action prediction. However, the performance of such approaches remains inherently bounded by the reasoning capabilities of the model used to generate the rationales. In this paper, we introduce Shop-R1, a novel reinforcement learning (RL) framework aimed at enhancing the reasoning ability of LLMs for simulation of real human behavior in online shopping environments Specifically, Shop-R1 decomposes the human behavior simulation task into two stages: rationale generation and action prediction, each guided by distinct reward signals. For rationale generation, we leverage internal model signals (e.g., logit distributions) to guide the reasoning process in a self-supervised manner. For action prediction, we propose a hierarchical reward structure with difficulty-aware scaling to prevent reward hacking and enable fine-grained reward assignment. This design evaluates both high-level action types and the correctness of fine-grained sub-action details (attributes and values), rewarding outputs proportionally to their difficulty. Experimental results show that our method achieves a relative improvement of over 65% compared to the baseline.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic and Generalizable Process Reward Modeling</title>
<link>https://arxiv.org/abs/2507.17849</link>
<guid>https://arxiv.org/abs/2507.17849</guid>
<content:encoded><![CDATA[
<div> Dynamic and Generalizable Process Reward Modeling, Dense reward signals, Large Language Models, Fine-grained reward criteria, Generalizability

Summary:
Dynamic and Generalizable Process Reward Modeling (DG-PRM) addresses the limitations of existing Process Reward Models (PRMs) by introducing a reward tree to capture fine-grained, multi-dimensional reward criteria. DG-PRM dynamically selects reward signals for step-wise scoring and utilizes Pareto dominance estimation to identify discriminative positive and negative pairs. Experimental results demonstrate the superior performance of DG-PRM on various benchmarks, boosting model performance with dense rewards. The model's adaptability to out-of-distribution scenarios showcases exceptional generalizability, a crucial feature for real-world applications. Overall, DG-PRM presents a novel approach to guiding Large Language Models (LLMs) in complex scenarios, significantly improving model performance and adaptability. 

<br /><br />Summary: <div>
arXiv:2507.17849v1 Announce Type: new 
Abstract: Process Reward Models (PRMs) are crucial for guiding Large Language Models (LLMs) in complex scenarios by providing dense reward signals. However, existing PRMs primarily rely on heuristic approaches, which struggle with cross-domain generalization. While LLM-as-judge has been proposed to provide generalized rewards, current research has focused mainly on feedback results, overlooking the meaningful guidance embedded within the text. Additionally, static and coarse-grained evaluation criteria struggle to adapt to complex process supervision. To tackle these challenges, we propose Dynamic and Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to capture and store fine-grained, multi-dimensional reward criteria. DG-PRM dynamically selects reward signals for step-wise reward scoring. To handle multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation to identify discriminative positive and negative pairs. Experimental results show that DG-PRM achieves stunning performance on prevailing benchmarks, significantly boosting model performance across tasks with dense rewards. Further analysis reveals that DG-PRM adapts well to out-of-distribution scenarios, demonstrating exceptional generalizability.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL</title>
<link>https://arxiv.org/abs/2507.17896</link>
<guid>https://arxiv.org/abs/2507.17896</guid>
<content:encoded><![CDATA[
<div> Keywords: NLIDBs, bias detection, cognitive biases, data analysis, VeriMinder

Summary: 
VeriMinder is an interactive system designed to detect and mitigate biases in analytical questions posed by users of natural language interfaces to databases (NLIDBs), particularly those without a background in statistical analysis. It introduces a contextual semantic mapping framework for biases, an analytical framework based on the Hard-to-Vary principle, and an optimized system using LLM technology to generate high-quality prompts. User testing showed a positive impact on analysis quality, with VeriMinder outperforming alternative approaches by at least 20% in metrics like concreteness, comprehensiveness, and accuracy. The system, available as an open-source web application, aims to help users avoid the "wrong question" vulnerability during data analysis.
<br /><br />Summary: <div>
arXiv:2507.17896v1 Announce Type: new 
Abstract: Application systems using natural language interfaces to databases (NLIDBs) have democratized data analysis. This positive development has also brought forth an urgent challenge to help users who might use these systems without a background in statistical analysis to formulate bias-free analytical questions. Although significant research has focused on text-to-SQL generation accuracy, addressing cognitive biases in analytical questions remains underexplored. We present VeriMinder, https://veriminder.ai, an interactive system for detecting and mitigating such analytical vulnerabilities. Our approach introduces three key innovations: (1) a contextual semantic mapping framework for biases relevant to specific analysis contexts (2) an analytical framework that operationalizes the Hard-to-Vary principle and guides users in systematic data analysis (3) an optimized LLM-powered system that generates high-quality, task-specific prompts using a structured process involving multiple candidates, critic feedback, and self-reflection.
  User testing confirms the merits of our approach. In direct user experience evaluation, 82.5% participants reported positively impacting the quality of the analysis. In comparative evaluation, VeriMinder scored significantly higher than alternative approaches, at least 20% better when considered for metrics of the analysis's concreteness, comprehensiveness, and accuracy. Our system, implemented as a web application, is set to help users avoid "wrong question" vulnerability during data analysis. VeriMinder code base with prompts, https://reproducibility.link/veriminder, is available as an MIT-licensed open-source software to facilitate further research and adoption within the community.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Whisper to Grade Them All</title>
<link>https://arxiv.org/abs/2507.17918</link>
<guid>https://arxiv.org/abs/2507.17918</guid>
<content:encoded><![CDATA[
<div> encoder, aggregator, automatic speaking assessment, computer-assisted language learning, data efficiency
Summary: 
The article introduces a novel end-to-end approach for Automatic Speaking Assessment (ASA) developed for the 2025 Speak & Improve Challenge. The system utilizes a single Whisper-small encoder to process all four spoken responses and predict the final score, eliminating the need for transcription and per-part models. This architecture reduces inference time and makes ASA practical for large-scale Computer-Assisted Language Learning systems. The system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming the baseline while using fewer parameters. Additionally, a data sampling strategy was proposed, allowing the model to train on a subset of speakers and still achieve strong performance, particularly on imbalanced classes. The approach demonstrates improved data efficiency and performance on multi-part second-language tests. <div>
arXiv:2507.17918v1 Announce Type: new 
Abstract: We present an efficient end-to-end approach for holistic Automatic Speaking Assessment (ASA) of multi-part second-language tests, developed for the 2025 Speak & Improve Challenge. Our system's main novelty is the ability to process all four spoken responses with a single Whisper-small encoder, combine all information via a lightweight aggregator, and predict the final score. This architecture removes the need for transcription and per-part models, cuts inference time, and makes ASA practical for large-scale Computer-Assisted Language Learning systems.
  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming the text-based baseline (0.44) while using at most 168M parameters (about 70% of Whisper-small). Furthermore, we propose a data sampling strategy, allowing the model to train on only 44.8% of the speakers in the corpus and still reach 0.383 RMSE, demonstrating improved performance on imbalanced classes and strong data efficiency.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text</title>
<link>https://arxiv.org/abs/2507.17944</link>
<guid>https://arxiv.org/abs/2507.17944</guid>
<content:encoded><![CDATA[
<div> adversarial attacks, DeepSeek, AI detection tools, detector robustness, few-shot prompting <br />
Summary:
- Large language models (LLMs) have impacted writing integrity, leading to the development of AI detection technologies.
- This study focused on evaluating if commonly accessible AI detection tools can detect text generated by DeepSeek using adversarial attacks.
- QuillBot and Copyleaks performed well in recognizing original and paraphrased DeepSeek text, while others showed inconsistent results.
- Humanization was the most effective attack in reducing detector accuracy.
- Few-shot and chain-of-thought prompting exhibited high accuracy in classifying AI and human-written text.  <div>
arXiv:2507.17944v1 Announce Type: new 
Abstract: Large language models (LLMs) have rapidly transformed the creation of written materials. LLMs have led to questions about writing integrity, thereby driving the creation of artificial intelligence (AI) detection technologies. Adversarial attacks, such as standard and humanized paraphrasing, inhibit detectors' ability to detect machine-generated text. Previous studies have mainly focused on ChatGPT and other well-known LLMs and have shown varying accuracy across detectors. However, there is a clear gap in the literature about DeepSeek, a recently published LLM. Therefore, in this work, we investigate whether six generally accessible AI detection tools -- AI Text Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can consistently recognize text generated by DeepSeek. The detectors were exposed to the aforementioned adversarial attacks. We also considered DeepSeek as a detector by performing few-shot prompting and chain-of-thought reasoning (CoT) for classifying AI and human-written text. We collected 49 human-authored question-answer pairs from before the LLM era and generated matching responses using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied adversarial techniques such as paraphrasing and humanizing to add 196 more samples. These were used to challenge detector robustness and assess accuracy impact. While QuillBot and Copyleaks showed near-perfect performance on original and paraphrased DeepSeek text, others -- particularly AI Text Classifier and GPT-2 -- showed inconsistent results. The most effective attack was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and 52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best five-shot result misclassifying only one of 49 samples (AI recall 96%, human recall 100%).
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLM Belief Updates Consistent with Bayes' Theorem?</title>
<link>https://arxiv.org/abs/2507.17951</link>
<guid>https://arxiv.org/abs/2507.17951</guid>
<content:encoded><![CDATA[
<div> Bayesian Coherence Coefficient, language models, pre-trained, Bayes' theorem, evidence in-context

Summary:
- A Bayesian Coherence Coefficient (BCC) metric is introduced to measure how well language models update their beliefs in accordance with Bayes' theorem.
- A dataset is created to quantify the BCC for various pre-trained language models from different model families.
- Larger and more capable language models show a higher coherence with Bayes' theorem in updating their beliefs when presented with evidence.
- The study compares BCC with model parameters, training data size, and model performance on standard benchmarks.
- The findings suggest that advanced pre-trained language models align better with Bayes' theorem, with implications for the understanding and regulation of such models. 

<br /><br />Summary: <div>
arXiv:2507.17951v1 Announce Type: new 
Abstract: Do larger and more capable language models learn to update their "beliefs" about propositions more consistently with Bayes' theorem when presented with evidence in-context? To test this, we formulate a Bayesian Coherence Coefficient (BCC) metric and generate a dataset with which to measure the BCC. We measure BCC for multiple pre-trained-only language models across five model families, comparing against the number of model parameters, the amount of training data, and model scores on common benchmarks. Our results provide evidence for our hypothesis that larger and more capable pre-trained language models assign credences that are more coherent with Bayes' theorem. These results have important implications for our understanding and governance of LLMs.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Processing for Tigrinya: Current State and Future Directions</title>
<link>https://arxiv.org/abs/2507.17974</link>
<guid>https://arxiv.org/abs/2507.17974</guid>
<content:encoded><![CDATA[
<div> Keywords: Tigrinya, Natural Language Processing, resource scarcity, neural architectures, morphological complexity

Summary: 
This study provides an in-depth analysis of Natural Language Processing (NLP) research for Tigrinya, a language that has been significantly underrepresented in the field. The researchers reviewed over 40 studies conducted between 2011 and 2025, focusing on various NLP tasks such as morphological processing, machine translation, speech recognition, and question-answering. The analysis shows a transition from rule-based systems to modern neural architectures, with advancements largely driven by the creation of resources for Tigrinya. Challenges identified include the language's morphological complexity and limited resources. Promising research directions include morphology-aware modeling, cross-lingual transfer, and community-centered resource development. The study aims to serve as a valuable reference for researchers working on Tigrinya NLP and provides a curated metadata of the surveyed studies and resources for public access. 

<br /><br />Summary: <div>
arXiv:2507.17974v1 Announce Type: new 
Abstract: Despite being spoken by millions of people, Tigrinya remains severely underrepresented in Natural Language Processing (NLP) research. This work presents a comprehensive survey of NLP research for Tigrinya, analyzing over 40 studies spanning more than a decade of work from 2011 to 2025. We systematically review the current state of computational resources, models, and applications across ten distinct downstream tasks, including morphological processing, machine translation, speech recognition, and question-answering. Our analysis reveals a clear trajectory from foundational, rule-based systems to modern neural architectures, with progress consistently unlocked by resource creation milestones. We identify key challenges rooted in Tigrinya's morphological complexity and resource scarcity, while highlighting promising research directions, including morphology-aware modeling, cross-lingual transfer, and community-centered resource development. This work serves as both a comprehensive reference for researchers and a roadmap for advancing Tigrinya NLP. A curated metadata of the surveyed studies and resources is made publicly available.\footnote{Tigrinya NLP Anthology: https://github.com/fgaim/tigrinya-nlp-anthology.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technical Report of TeleChat2, TeleChat2.5 and T1</title>
<link>https://arxiv.org/abs/2507.18013</link>
<guid>https://arxiv.org/abs/2507.18013</guid>
<content:encoded><![CDATA[
<div> Transformer-based architectures, TeleChat models, performance gains, pre-training strategies, reinforcement learning <br />
<br />
Summary: 
The latest series of TeleChat models, including TeleChat2, TeleChat2.5, and T1, have been introduced, offering significant improvements over their predecessor. These models undergo pre-training on large, diverse datasets and further training through Supervised Fine-Tuning and Direct Preference Optimization. TeleChat2.5 and T1 incorporate continual pre-training with domain-specific data and reinforcement learning, resulting in enhanced performance in code generation and mathematical reasoning tasks. The T1 variant is designed for complex reasoning, supporting long Chain-of-Thought (CoT) reasoning and showcasing substantial improvements in mathematics and coding tasks. On the other hand, TeleChat2.5 prioritizes speed in inference. Both flagship models, T1 and TeleChat2.5, are dense Transformer-based architectures with 115B parameters, demonstrating significant advancements in reasoning and task performance compared to the original TeleChat. T1-115B outperforms proprietary models such as OpenAI's o1-mini and GPT-4o. TeleChat2, TeleChat2.5, and T1 models have been publicly released to provide developers and researchers with state-of-the-art language models for various applications. <div>
arXiv:2507.18013v1 Announce Type: new 
Abstract: We introduce the latest series of TeleChat models: \textbf{TeleChat2}, \textbf{TeleChat2.5}, and \textbf{T1}, offering a significant upgrade over their predecessor, TeleChat. Despite minimal changes to the model architecture, the new series achieves substantial performance gains through enhanced training strategies in both pre-training and post-training stages. The series begins with \textbf{TeleChat2}, which undergoes pretraining on 10 trillion high-quality and diverse tokens. This is followed by Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to further enhance its capabilities. \textbf{TeleChat2.5} and \textbf{T1} expand the pipeline by incorporating a continual pretraining phase with domain-specific datasets, combined with reinforcement learning (RL) to improve performance in code generation and mathematical reasoning tasks. The \textbf{T1} variant is designed for complex reasoning, supporting long Chain-of-Thought (CoT) reasoning and demonstrating substantial improvements in mathematics and coding. In contrast, \textbf{TeleChat2.5} prioritizes speed, delivering rapid inference. Both flagship models of \textbf{T1} and \textbf{TeleChat2.5} are dense Transformer-based architectures with 115B parameters, showcasing significant advancements in reasoning and general task performance compared to the original TeleChat. Notably, \textbf{T1-115B} outperform proprietary models such as OpenAI's o1-mini and GPT-4o. We publicly release \textbf{TeleChat2}, \textbf{TeleChat2.5} and \textbf{T1}, including post-trained versions with 35B and 115B parameters, to empower developers and researchers with state-of-the-art language models tailored for diverse applications.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database</title>
<link>https://arxiv.org/abs/2507.18028</link>
<guid>https://arxiv.org/abs/2507.18028</guid>
<content:encoded><![CDATA[
<div> NeuralDB, Key-Value (KV) database, efficient editing, LLMs, knowledge updating<br />
Summary: <br />
NeuralDB is a framework that enhances editing efficiency in large language models (LLMs) by representing edited facts as a neural Key-Value database. This framework includes a non-linear gated retrieval module that preserves the general abilities of LLMs. Experiments on ZsRE and CounterFacts datasets showed that NeuralDB outperformed existing methods in editing efficacy, generalization, specificity, fluency, and consistency with models like GPT2-XL, GPT-J (6B), and Llama-3 (8B). Furthermore, NeuralDB maintained its effectiveness even when scaled up to 100,000 facts, showing a significant improvement over previous work. This approach enables efficient and effective knowledge editing in LLMs without compromising their overall performance in various text understanding and generation tasks. <br /> <div>
arXiv:2507.18028v1 Announce Type: new 
Abstract: Efficiently editing knowledge stored in large language models (LLMs) enables model updates without large-scale training. One possible solution is Locate-and-Edit (L\&amp;E), allowing simultaneous modifications of a massive number of facts. However, such editing may compromise the general abilities of LLMs and even result in forgetting edited facts when scaling up to thousands of edits. In this paper, we model existing linear L\&amp;E methods as querying a Key-Value (KV) database. From this perspective, we then propose NeuralDB, an editing framework that explicitly represents the edited facts as a neural KV database equipped with a non-linear gated retrieval module, % In particular, our gated module only operates when inference involves the edited facts, effectively preserving the general abilities of LLMs. Comprehensive experiments involving the editing of 10,000 facts were conducted on the ZsRE and CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results demonstrate that NeuralDB not only excels in editing efficacy, generalization, specificity, fluency, and consistency, but also preserves overall performance across six representative text understanding and generation tasks. Further experiments indicate that NeuralDB maintains its effectiveness even when scaled to 100,000 facts (\textbf{50x} more than in prior work).
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs</title>
<link>https://arxiv.org/abs/2507.18043</link>
<guid>https://arxiv.org/abs/2507.18043</guid>
<content:encoded><![CDATA[
<div> steering methods, language models, vision-language models, GrAInS, integrated gradients  
Summary:  
- GrAInS is a new inference-time steering approach for language-only and vision-language models.  
- It uses integrated gradients to identify influential tokens and constructs steering vectors for semantic shifts.  
- GrAInS adjusts hidden activations at transformer layers based on token-level attribution signals.  
- It outperforms fine-tuning and existing steering baselines in various tasks like TruthfulQA and MMHal-Bench.  
- GrAInS improves model alignment win rates on SPA-VL while maintaining fluency and general capabilities.  
<br /><br />Summary: <div>
arXiv:2507.18043v1 Announce Type: new 
Abstract: Inference-time steering methods offer a lightweight alternative to fine-tuning large language models (LLMs) and vision-language models (VLMs) by modifying internal activations at test time without updating model weights. However, most existing approaches rely on fixed, global intervention vectors, overlook the causal influence of individual input tokens, and fail to leverage informative gradients from the model's logits, particularly in multimodal settings where visual and textual inputs contribute unevenly. To address these limitations, we introduce GrAInS, an inference-time steering approach that operates across both language-only and vision-language models and tasks. GrAInS uses contrastive, gradient-based attribution via Integrated Gradients to identify the top-k most influential tokens, both positively and negatively attributed based on their contribution to preferred versus dispreferred outputs. These tokens are then used to construct directional steering vectors that capture semantic shifts from undesirable to desirable behavior. During inference, GrAInS adjusts hidden activations at transformer layers guided by token-level attribution signals, and normalizes activations to preserve representational scale. This enables fine-grained, interpretable, and modular control over model behavior, without retraining or auxiliary supervision. Empirically, GrAInS consistently outperforms both fine-tuning and existing steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514 with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all while preserving the model's fluency and general capabilities.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Data Generation for Phrase Break Prediction with Large Language Model</title>
<link>https://arxiv.org/abs/2507.18044</link>
<guid>https://arxiv.org/abs/2507.18044</guid>
<content:encoded><![CDATA[
<div> Keywords: phrase break prediction, text-to-speech systems, large language models, synthetic data generation, speech domain<br />
Summary:<br />
The article explores using large language models (LLMs) to generate synthetic data for phrase break prediction in text-to-speech systems. Traditional approaches rely heavily on human annotations, which can be time-consuming and costly. LLMs have shown success in generating synthetic data for NLP tasks, and this study aims to apply that to the speech domain. The study compares LLM-generated data with traditional annotations and assesses effectiveness across multiple languages. The findings suggest that LLM-based synthetic data generation can effectively address challenges in phrase break prediction, offering a promising solution for improving speech-related tasks. This research highlights the potential of LLMs in overcoming data challenges and reducing manual annotation needs in the speech domain.<br /> <div>
arXiv:2507.18044v1 Announce Type: new 
Abstract: Current approaches to phrase break prediction address crucial prosodic aspects of text-to-speech systems but heavily rely on vast human annotations from audio or text, incurring significant manual effort and cost. Inherent variability in the speech domain, driven by phonetic factors, further complicates acquiring consistent, high-quality data. Recently, large language models (LLMs) have shown success in addressing data challenges in NLP by generating tailored synthetic data while reducing manual annotation needs. Motivated by this, we explore leveraging LLM to generate synthetic phrase break annotations, addressing the challenges of both manual annotation and speech-related tasks by comparing with traditional annotations and assessing effectiveness across multiple languages. Our findings suggest that LLM-based synthetic data generation effectively mitigates data challenges in phrase break prediction and highlights the potential of LLMs as a viable solution for the speech domain.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs</title>
<link>https://arxiv.org/abs/2507.18055</link>
<guid>https://arxiv.org/abs/2507.18055</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic data, Large Language Models, diversity, privacy, metrics

Summary:
The article discusses the use of synthetic data generated by Large Language Models (LLMs) in data-driven applications. It highlights both the opportunities and challenges associated with synthetic data, focusing on text-based datasets. The study proposes a set of metrics to quantitatively evaluate the diversity and privacy aspects of synthetic data produced by state-of-the-art LLMs. The evaluation reveals significant limitations in LLMs' ability to generate diverse and privacy-preserving synthetic data. Based on these findings, a prompt-based approach is suggested to enhance the diversity of synthetic reviews while maintaining reviewer privacy. The findings emphasize the need for improvements in LLMs' synthetic data generation capabilities to address issues related to diversity and privacy in text-based datasets.
<br /><br />Summary: <div>
arXiv:2507.18055v1 Announce Type: new 
Abstract: The increasing use of synthetic data generated by Large Language Models (LLMs) presents both opportunities and challenges in data-driven applications. While synthetic data provides a cost-effective, scalable alternative to real-world data to facilitate model training, its diversity and privacy risks remain underexplored. Focusing on text-based synthetic data, we propose a comprehensive set of metrics to quantitatively assess the diversity (i.e., linguistic expression, sentiment, and user perspective), and privacy (i.e., re-identification risk and stylistic outliers) of synthetic datasets generated by several state-of-the-art LLMs. Experiment results reveal significant limitations in LLMs' capabilities in generating diverse and privacy-preserving synthetic data. Guided by the evaluation results, a prompt-based approach is proposed to enhance the diversity of synthetic reviews while preserving reviewer privacy.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios</title>
<link>https://arxiv.org/abs/2507.18061</link>
<guid>https://arxiv.org/abs/2507.18061</guid>
<content:encoded><![CDATA[
<div> benchmark, spoken language models, conversation, Chinese, user experience

Summary:
TELEVAL is a new benchmark designed to evaluate the effectiveness of spoken language models (SLMs) in conversational settings. It focuses on three evaluation dimensions: Explicit Semantics, Paralinguistic and Implicit Semantics, and System Abilities. The benchmark assesses both text and audio outputs in a dialogue format consistent with real-world usage. TELEVAL highlights the importance of SLMs extracting implicit cues from user speech and responding appropriately without additional instructions. Experiments show that existing SLMs still have room for improvement in natural conversational tasks. The goal of TELEVAL is to provide a user-centered evaluation framework that reflects the user experience and promotes the development of more capable dialogue-oriented SLMs.

<br /><br />Summary: <div>
arXiv:2507.18061v1 Announce Type: new 
Abstract: Spoken language models (SLMs) have seen rapid progress in recent years, along with the development of numerous benchmarks for evaluating their performance. However, most existing benchmarks primarily focus on evaluating whether SLMs can perform complex tasks comparable to those tackled by large language models (LLMs), often failing to align with how users naturally interact in real-world conversational scenarios. In this paper, we propose TELEVAL, a dynamic benchmark specifically designed to evaluate SLMs' effectiveness as conversational agents in realistic Chinese interactive settings. TELEVAL defines three evaluation dimensions: Explicit Semantics, Paralinguistic and Implicit Semantics, and System Abilities. It adopts a dialogue format consistent with real-world usage and evaluates text and audio outputs separately. TELEVAL particularly focuses on the model's ability to extract implicit cues from user speech and respond appropriately without additional instructions. Our experiments demonstrate that despite recent progress, existing SLMs still have considerable room for improvement in natural conversational tasks. We hope that TELEVAL can serve as a user-centered evaluation framework that directly reflects the user experience and contributes to the development of more capable dialogue-oriented SLMs.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints</title>
<link>https://arxiv.org/abs/2507.18076</link>
<guid>https://arxiv.org/abs/2507.18076</guid>
<content:encoded><![CDATA[
<div> parameter-efficient fine-tuning, large language models, convergence efficiency, generalization, resource consumption <br />
Summary: <br />
This paper evaluates parameter-efficient fine-tuning (PEFT) techniques for large language models (LLMs) and introduces a hybrid strategy combining BOFT's stability with LoRA-GA's convergence speed. The hybrid method dynamically adjusts updates based on gradient norms, leading to superior convergence efficiency and generalization across tasks. Additionally, unitary RNN (uRNN) principles are applied to transformer-based LLMs for enhanced gradient stability. Empirical results on various benchmarks show that the hybrid approach outperforms individual PEFT methods, achieving near full fine-tuning accuracy while reducing resource consumption by up to 2.1 times in training time and 50% in memory usage. This study establishes the hybrid method as a practical and scalable solution for fine-tuning LLMs under resource constraints. <br /> <div>
arXiv:2507.18076v1 Announce Type: new 
Abstract: Fine-tuning large language models (LLMs) remains a computational bottleneck due to their scale and memory demands. This paper presents a comprehensive evaluation of parameter-efficient fine-tuning (PEFT) techniques, including LoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that dynamically integrates BOFT's orthogonal stability with LoRA-GA's gradient-aligned rapid convergence. By computing per-layer adaptive updates guided by gradient norms, the hybrid method achieves superior convergence efficiency and generalization across diverse tasks. We also explore, for the first time, the adaptation of unitary RNN (uRNN) principles to transformer-based LLMs, enhancing gradient stability through structured unitary constraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench, and HumanEval -- using models ranging from 7B to 405B parameters demonstrate that our hybrid method consistently outperforms individual PEFT baselines, approaching full fine-tuning accuracy while reducing resource consumption by up to 2.1 times in training time and 50 percent in memory usage. These findings establish the hybrid approach as a practical and scalable fine-tuning solution for real-world deployment of LLMs under resource constraints.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Pair of GloVes</title>
<link>https://arxiv.org/abs/2507.18103</link>
<guid>https://arxiv.org/abs/2507.18103</guid>
<content:encoded><![CDATA[
<div> Keywords: English GloVe, word embeddings, NER tasks, Wikipedia, Gigaword

Summary:
The report introduces new 2024 English GloVe models, addressing the necessity for updated word embeddings due to language and world evolution. Documenting the exact data versions and preprocessing used, the models were trained on Wikipedia, Gigaword, and a Dolma subset. Evaluation revealed the vectors' incorporation of culturally relevant words, comparable performance on structural tasks like analogy and similarity, and improved accuracy on recent NER datasets, particularly on non-Western newswire data. The models showcase enhanced linguistic relevance and performance, reflective of evolving language usage and needs in a constantly changing global landscape. <br /><br />Summary: <div>
arXiv:2507.18103v1 Announce Type: new 
Abstract: This report documents, describes, and evaluates new 2024 English GloVe (Global Vectors for Word Representation) models. While the original GloVe models built in 2014 have been widely used and found useful, languages and the world continue to evolve and we thought that current usage could benefit from updated models. Moreover, the 2014 models were not carefully documented as to the exact data versions and preprocessing that were used, and we rectify this by documenting these new models. We trained two sets of word embeddings using Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary comparison, direct testing, and NER tasks shows that the 2024 vectors incorporate new culturally and linguistically relevant words, perform comparably on structural tasks like analogy and similarity, and demonstrate improved performance on recent, temporally dependent NER datasets such as non-Western newswire data.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness</title>
<link>https://arxiv.org/abs/2507.18119</link>
<guid>https://arxiv.org/abs/2507.18119</guid>
<content:encoded><![CDATA[
<div> Keywords: end-to-end spoken language models, paralinguistic cues, speaker characteristics, GOAT-SLM, natural spoken interactions 

Summary: 

GOAT-SLM is a novel spoken language model that incorporates paralinguistic and speaker characteristic awareness, going beyond traditional models by incorporating non-linguistic cues in speech. The model features a dual-modality head architecture that separates linguistic analysis from acoustic realization, allowing for more expressive and adaptive speech generation. A modular, staged training strategy gradually incorporates linguistic, paralinguistic, and speaker characteristic information from large-scale speech-text corpora. Experimental results on the TELEVAL evaluation benchmark show that GOAT-SLM outperforms existing models in tasks such as handling emotion, dialectal variation, and age-sensitive interactions. This research emphasizes the importance of considering paralinguistic cues in spoken language models to develop more natural, adaptive, and socially aware AI systems. 

<br /><br />Summary: <div>
arXiv:2507.18119v1 Announce Type: new 
Abstract: Recent advances in end-to-end spoken language models (SLMs) have significantly improved the ability of AI systems to engage in natural spoken interactions. However, most existing models treat speech merely as a vehicle for linguistic content, often overlooking the rich paralinguistic and speaker characteristic cues embedded in human speech, such as dialect, age, emotion, and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel spoken language model with paralinguistic and speaker characteristic awareness, designed to extend spoken language modeling beyond text semantics. GOAT-SLM adopts a dual-modality head architecture that decouples linguistic modeling from acoustic realization, enabling robust language understanding while supporting expressive and adaptive speech generation. To enhance model efficiency and versatility, we propose a modular, staged training strategy that progressively aligns linguistic, paralinguistic, and speaker characteristic information using large-scale speech-text corpora. Experimental results on TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM achieves well-balanced performance across both semantic and non-semantic tasks, and outperforms existing open-source models in handling emotion, dialectal variation, and age-sensitive interactions. This work highlights the importance of modeling beyond linguistic content and advances the development of more natural, adaptive, and socially aware spoken language systems.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2507.18140</link>
<guid>https://arxiv.org/abs/2507.18140</guid>
<content:encoded><![CDATA[
<div> Evaluation, Multi-modal Large Language Models, Mathematical reasoning, Visual operations, Code-based capabilities <br />
Summary: <br />
This study focuses on evaluating the capabilities of Multi-modal Large Language Models (MLLMs) in performing multi-modal mathematical reasoning using code as an intermediate representation to manipulate images. The evaluation framework includes Multi-modal Code Generation (MCG) to assess the model's ability to construct visualizations and Multi-modal Code Editing (MCE) to evaluate fine-grained visual operations such as Deletion, Modification, and Annotation. The dataset used covers popular mathematical figure types, revealing that current MLLMs fall short of human performance in performing detailed visual operations. The study involves nine mainstream MLLMs and highlights the need for further advancement in understanding and manipulating visual content through textual instructions. <br /> <div>
arXiv:2507.18140v1 Announce Type: new 
Abstract: Recent progress in Multi-modal Large Language Models (MLLMs) has enabled step-by-step multi-modal mathematical reasoning by performing visual operations based on the textual instructions. A promising approach uses code as an intermediate representation to precisely express and manipulate the images in the reasoning steps. However, existing evaluations focus mainly on text-only reasoning outputs, leaving the MLLM's ability to perform accurate visual operations via code largely unexplored. This work takes a first step toward addressing that gap by evaluating MLLM's code-based capabilities in multi-modal mathematical reasoning.Specifically, our framework focuses on two key evaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's ability to accurately understand and construct visualizations from scratch. (2) Multi-modal Code Editing (MCE) assesses the model's capacity for fine-grained operations, which include three types: Deletion, Modification and Annotation. To evaluate the above tasks, we incorporate a dataset that covers the five most popular types of mathematical figures, including geometric diagrams, function plots, and three types of statistical charts, to provide a comprehensive and effective measurement of existing MLLMs. Our experimental evaluation involves nine mainstream MLLMs, and the results reveal that existing models still lag significantly behind human performance in performing fine-grained visual operations.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HIVMedQA: Benchmarking large language models for HIV medical decision support</title>
<link>https://arxiv.org/abs/2507.18143</link>
<guid>https://arxiv.org/abs/2507.18143</guid>
<content:encoded><![CDATA[
<div> HIV management, large language models, benchmarking, clinical practice, AI applications <br />
Summary:<br />
- HIV management complexity makes it a compelling use case for large language models (LLMs) in clinical decision-making.
- Concerns about accuracy, potential harm, and clinician acceptance need to be addressed when integrating LLMs into clinical practice.
- AI applications in HIV care are underexplored, and benchmarking studies for LLMs are scarce.
- HIVMedQA is introduced as a benchmark to assess open-ended medical question answering in HIV care.
- Evaluation of LLMs in HIV management showed that Gemini 2.5 Pro outperformed other models, with performance varying across dimensions such as question comprehension, knowledge recall, bias, and factual accuracy.
- Model size and medical fine-tuning were not always reliable predictors of performance, and cognitive biases such as recency and status quo were identified.
- Targeted development and evaluation are needed to ensure the safe and effective integration of LLMs in clinical care.<br />Summary: <div>
arXiv:2507.18143v1 Announce Type: new 
Abstract: Large language models (LLMs) are emerging as valuable tools to support clinicians in routine decision-making. HIV management is a compelling use case due to its complexity, including diverse treatment options, comorbidities, and adherence challenges. However, integrating LLMs into clinical practice raises concerns about accuracy, potential harm, and clinician acceptance. Despite their promise, AI applications in HIV care remain underexplored, and LLM benchmarking studies are scarce. This study evaluates the current capabilities of LLMs in HIV management, highlighting their strengths and limitations. We introduce HIVMedQA, a benchmark designed to assess open-ended medical question answering in HIV care. The dataset consists of curated, clinically relevant questions developed with input from an infectious disease physician. We evaluated seven general-purpose and three medically specialized LLMs, applying prompt engineering to enhance performance. Our evaluation framework incorporates both lexical similarity and an LLM-as-a-judge approach, extended to better reflect clinical relevance. We assessed performance across key dimensions: question comprehension, reasoning, knowledge recall, bias, potential harm, and factual accuracy. Results show that Gemini 2.5 Pro consistently outperformed other models across most dimensions. Notably, two of the top three models were proprietary. Performance declined as question complexity increased. Medically fine-tuned models did not always outperform general-purpose ones, and larger model size was not a reliable predictor of performance. Reasoning and comprehension were more challenging than factual recall, and cognitive biases such as recency and status quo were observed. These findings underscore the need for targeted development and evaluation to ensure safe, effective LLM integration in clinical care.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models</title>
<link>https://arxiv.org/abs/2507.18171</link>
<guid>https://arxiv.org/abs/2507.18171</guid>
<content:encoded><![CDATA[
<div> Tokenization, Sticky Tokens, Text Embedding Models, NLP tasks, Downstream Tasks

Summary:
Sticky tokens in Transformer-based text embedding models can disrupt embedding distances and degrade performance, leading to concerns about their impact on downstream tasks. The Sticky Token Detector (STD) efficiently detects these anomalous tokens, revealing their origins in special or unused vocabulary entries and fragmented subwords. Sticky tokens do not correlate with model or vocabulary size but significantly affect clustering and retrieval tasks. Attention-layer analysis shows that sticky tokens dominate internal representations, highlighting the need for better tokenization strategies and model design to mitigate their impact. Further research is needed to address the presence of sticky tokens and improve the robustness of text embedding applications.<br /><br />Summary: <div>
arXiv:2507.18171v1 Announce Type: new 
Abstract: Despite the widespread use of Transformer-based text embedding models in NLP tasks, surprising 'sticky tokens' can undermine the reliability of embeddings. These tokens, when repeatedly inserted into sentences, pull sentence similarity toward a certain value, disrupting the normal distribution of embedding distances and degrading downstream performance. In this paper, we systematically investigate such anomalous tokens, formally defining them and introducing an efficient detection method, Sticky Token Detector (STD), based on sentence and token filtering. Applying STD to 40 checkpoints across 14 model families, we discover a total of 868 sticky tokens. Our analysis reveals that these tokens often originate from special or unused entries in the vocabulary, as well as fragmented subwords from multilingual corpora. Notably, their presence does not strictly correlate with model size or vocabulary size. We further evaluate how sticky tokens affect downstream tasks like clustering and retrieval, observing significant performance drops of up to 50%. Through attention-layer analysis, we show that sticky tokens disproportionately dominate the model's internal representations, raising concerns about tokenization robustness. Our findings show the need for better tokenization strategies and model design to mitigate the impact of sticky tokens in future text embedding applications.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models</title>
<link>https://arxiv.org/abs/2507.18182</link>
<guid>https://arxiv.org/abs/2507.18182</guid>
<content:encoded><![CDATA[
<div> evaluation framework, selection bias, distribution, debiasing methods, reliability

Summary:
The study introduces SCOPE, an evaluation framework designed to measure and mitigate selection bias in large language models (LLMs). SCOPE uses a null prompt to estimate each model's position-bias distribution and redistributes answer slots to equalize the lucky-rate. It prevents near-miss guesses by not placing semantically similar distractors adjacent to the answer. SCOPE outperforms existing debiasing methods, showing stable performance improvements and clearer confidence distributions over correct options. This framework sets a new standard for improving the fairness and reliability of LLM evaluations. 

<br /><br />Summary: <div>
arXiv:2507.18182v1 Announce Type: new 
Abstract: Large Language Models (LLMs) can achieve inflated scores on multiple-choice tasks by exploiting inherent biases in option positions or labels, rather than demonstrating genuine understanding. This study introduces SCOPE, an evaluation framework designed to measure and mitigate such selection bias in a dataset-independent manner. By repeatedly invoking a null prompt that lacks semantic content, SCOPE estimates each model's unique position-bias distribution. It then redistributes the answer slot according to the inverse-bias distribution, thereby equalizing the lucky-rate, the probability of selecting the correct answer by chance. Furthermore, it prevents semantically similar distractors from being placed adjacent to the answer, thereby blocking near-miss guesses based on superficial proximity cues. Across multiple benchmark experiments, SCOPE consistently outperformed existing debiasing methods in terms of stable performance improvements and showed clearer confidence distributions over correct options. This framework thus offers a new standard for enhancing the fairness and reliability of LLM evaluations.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks</title>
<link>https://arxiv.org/abs/2507.18190</link>
<guid>https://arxiv.org/abs/2507.18190</guid>
<content:encoded><![CDATA[
<div> Keywords: Root Cause Analysis, telecommunication networks, Artificial Intelligence, graph-based reasoning, benchmarks

Summary: 
Root Cause Analysis (RCA) in telecommunication networks is a critical task that poses challenges for Artificial Intelligence (AI) systems due to its complex, graph-based reasoning requirements and the lack of realistic benchmarks. RCA involves identifying the main cause of an issue or fault in the network, which is crucial for maintaining network efficiency and reliability. AI technologies are being developed to automate and optimize the RCA process, but the complex nature of network data and dependencies makes this task difficult. Realistic benchmarks are needed to evaluate the performance of AI systems in RCA tasks, but currently, there is a scarcity of such benchmarks in the telecommunication domain. Addressing these challenges will be essential for improving the efficiency and accuracy of RCA in telecommunication networks using AI technologies. 

<br /><br />Summary: <div>
arXiv:2507.18190v1 Announce Type: new 
Abstract: Root Cause Analysis (RCA) in telecommunication networks is a critical task, yet it presents a formidable challenge for Artificial Intelligence (AI) due to its complex, graph-based reasoning requirements and the scarcity of realistic benchmarks.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization</title>
<link>https://arxiv.org/abs/2507.18197</link>
<guid>https://arxiv.org/abs/2507.18197</guid>
<content:encoded><![CDATA[
<div> process modeling, ISO 9001, ISO30401, Knowledge Management System, SECI model <br />
Summary: <br />
Business process modeling is essential for organizations to ensure efficiency and alignment with strategic goals. ISO30401 sets universal requirements for Knowledge Management Systems (KMS), integrating knowledge development with operational processes. Implementers face the challenge of explaining how KMS activities align with existing processes. This article explores how an ISO30401-compliant KMS can be implemented within an Integrated Management System. By deploying the SECI model and utilizing PDCA cycles, organizations can effectively integrate knowledge management into their existing processes. This approach enhances efficiency and effectiveness, ensuring that knowledge development, transformation, and conveyance activities are seamlessly integrated with operational workflows. <div>
arXiv:2507.18197v1 Announce Type: new 
Abstract: Business process modeling is used by most organizations as an essential framework for ensuring efficiency and effectiveness of the work and workflow performed by its employees and for ensuring the alignment of such work with its strategic goals. For organizations that are compliant or near-compliant with ISO 9001, this approach involves the detailed mapping of processes, sub-processes, activities, and tasks. ISO30401 is a Management System Standard, introduced in 2018, establishing universal requirements for the set up of a Knowledge Management System in an organization. As ``ISO30401 implementers'' we regularly face the challenge of explaining our clients how the knowledge development, transformation and conveyances activities depicted in ISO30401 do integrate with existing operational processes. This article recaps process modelling principles in the context of ISO9001 and explores, based on our experience, how an ISO30401-compliant Knowledge Management System (KMS) entwines with all other processes of an Integrated Management System and in particular how it can be implemented by deploying the mechanisms of the SECI model through the steps of PDCA cycles.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection</title>
<link>https://arxiv.org/abs/2507.18202</link>
<guid>https://arxiv.org/abs/2507.18202</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Large Language Models, Gradient-based Masked Token Probability, defense method, adversarial settings

Summary:
The paper introduces a defense mechanism called Gradient-based Masked Token Probability (GMTP) to protect Retrieval-Augmented Generation systems from adversarial attacks. By analyzing the gradients of the similarity function used in retrieving external knowledge, GMTP identifies and masks high-impact tokens that could be part of poisoned documents. These tokens are then evaluated using a Masked Language Model to detect malicious content. The experimental results show that GMTP can effectively filter out over 90% of poisoned documents while maintaining the performance of retrieval and generation tasks on various datasets and in different adversarial scenarios. This approach enhances the security of LLMs by preventing the generation of harmful or misleading responses based on tampered external knowledge sources.<br /><br />Summary: <div>
arXiv:2507.18202v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by providing external knowledge for accurate and up-to-date responses. However, this reliance on external sources exposes a security risk, attackers can inject poisoned documents into the knowledge base to steer the generation process toward harmful or misleading outputs. In this paper, we propose Gradient-based Masked Token Probability (GMTP), a novel defense method to detect and filter out adversarially crafted documents. Specifically, GMTP identifies high-impact tokens by examining gradients of the retriever's similarity function. These key tokens are then masked, and their probabilities are checked via a Masked Language Model (MLM). Since injected tokens typically exhibit markedly low masked-token probabilities, this enables GMTP to easily detect malicious documents and achieve high-precision filtering. Experiments demonstrate that GMTP is able to eliminate over 90% of poisoned content while retaining relevant documents, thus maintaining robust retrieval and generation performance across diverse datasets and adversarial settings.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation</title>
<link>https://arxiv.org/abs/2507.18203</link>
<guid>https://arxiv.org/abs/2507.18203</guid>
<content:encoded><![CDATA[
<div> misinformation, language models, instruction-tuning, user input, reliability

Summary:
Instruction-tuning in large language models (LLMs) improves user instruction accuracy but may lead to increased susceptibility to misinformation. LLMs tuned with instructions are more likely to accept misinformation provided by users, shifting reliance from the model to the user. Factors affecting susceptibility include user role in prompt structure, misinformation length, and the presence of warnings in prompts. The study emphasizes the need for systematic approaches to mitigate unintended consequences of instruction-tuning and enhance LLM reliability in real-world applications.<br /><br />Summary: <div>
arXiv:2507.18203v1 Announce Type: new 
Abstract: Instruction-tuning enhances the ability of large language models (LLMs) to follow user instructions more accurately, improving usability while reducing harmful outputs. However, this process may increase the model's dependence on user input, potentially leading to the unfiltered acceptance of misinformation and the generation of hallucinations. Existing studies primarily highlight that LLMs are receptive to external information that contradict their parametric knowledge, but little research has been conducted on the direct impact of instruction-tuning on this phenomenon. In our study, we investigate the impact of instruction-tuning on LLM's susceptibility to misinformation. Our analysis reveals that instruction-tuned LLMs are significantly more likely to accept misinformation when it is presented by the user. A comparison with base models shows that instruction-tuning increases reliance on user-provided information, shifting susceptibility from the assistant role to the user role. Furthermore, we explore additional factors influencing misinformation susceptibility, such as the role of the user in prompt structure, misinformation length, and the presence of warnings in the system prompt. Our findings underscore the need for systematic approaches to mitigate unintended consequences of instruction-tuning and enhance the reliability of LLMs in real-world applications.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prune&amp;Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation</title>
<link>https://arxiv.org/abs/2507.18212</link>
<guid>https://arxiv.org/abs/2507.18212</guid>
<content:encoded><![CDATA[
<div> pruning, language models, acceleration, magnitude compensation, iterative strategy
Summary:<br /><br />Layer pruning in large language models (LLMs) can lead to performance degradation due to significant magnitude gaps in hidden states. The proposed Prune&amp;Comp method addresses this issue by using magnitude compensation to mitigate these gaps without the need for training. By estimating and rescaling the weights to eliminate the magnitude gaps, Prune&amp;Comp can enhance existing layer pruning metrics. When integrated with an iterative prune-and-compensate loop, it improves performance metrics significantly. For instance, when applying Prune&amp;Comp to the pruning of 5 layers in LLaMA-3-8B using a block influence metric, it reduces perplexity by nearly half and retains 93.19% of the original model's question-answering performance, outperforming the baseline by 4.01%. <div>
arXiv:2507.18212v1 Announce Type: new 
Abstract: Layer pruning has emerged as a promising technique for compressing large language models (LLMs) while achieving acceleration proportional to the pruning ratio. In this work, we identify that removing any layer induces a significant magnitude gap in hidden states, resulting in substantial performance degradation. To address this issue, we propose Prune&amp;Comp, a novel plug-and-play layer pruning scheme that leverages magnitude compensation to mitigate such gaps in a training-free manner. Specifically, we first estimate the magnitude gap caused by layer removal and then eliminate this gap by rescaling the remaining weights offline, with zero runtime overhead incurred. We further demonstrate the advantages of Prune&amp;Comp through an iterative pruning strategy. When integrated with an iterative prune-and-compensate loop, Prune&amp;Comp consistently enhances existing layer pruning metrics. For instance, when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence metric, Prune&amp;Comp nearly halves the perplexity and retains 93.19\% of the original model's question-answering performance, outperforming the baseline by 4.01%.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models</title>
<link>https://arxiv.org/abs/2507.18263</link>
<guid>https://arxiv.org/abs/2507.18263</guid>
<content:encoded><![CDATA[
<div> Keywords: Direct speech translation, terminology translation, Locate-and-Focus method, translation knowledge, multimodal

Summary:
The article discusses the challenges of accurately translating terminology within utterances in direct speech translation. Current methods struggle with interference from irrelevant noise and inefficient utilization of translation knowledge. To address these issues, a novel Locate-and-Focus method is proposed. This method effectively locates speech clips containing terminologies within the utterance to construct translation knowledge, minimizing irrelevant information. It associates this knowledge with the utterance and hypothesis from audio and textual modalities, allowing the ST model to focus better on translation knowledge during translation. Experimental results show that the method successfully locates terminologies within utterances, improves terminology translation success rates, and maintains robust general translation performance. Overall, the Locate-and-Focus method enhances the translation of terminology in direct speech translation by minimizing noise interference and optimizing the utilization of translation knowledge. 

<br /><br />Summary: <div>
arXiv:2507.18263v1 Announce Type: new 
Abstract: Direct speech translation (ST) has garnered increasing attention nowadays, yet the accurate translation of terminology within utterances remains a great challenge. In this regard, current studies mainly concentrate on leveraging various translation knowledge into ST models. However, these methods often struggle with interference from irrelevant noise and can not fully utilize the translation knowledge. To address these issues, in this paper, we propose a novel Locate-and-Focus method for terminology translation. It first effectively locates the speech clips containing terminologies within the utterance to construct translation knowledge, minimizing irrelevant information for the ST model. Subsequently, it associates the translation knowledge with the utterance and hypothesis from both audio and textual modalities, allowing the ST model to better focus on translation knowledge during translation. Experimental results across various datasets demonstrate that our method effectively locates terminologies within utterances and enhances the success rate of terminology translation, while maintaining robust general translation performance.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil</title>
<link>https://arxiv.org/abs/2507.18264</link>
<guid>https://arxiv.org/abs/2507.18264</guid>
<content:encoded><![CDATA[
<div> Comparative Analysis, OCR engines, Low-Resourced Languages, Sinhala, Tamil

Summary: 
- The study evaluates the zero-shot performance of six OCR engines on two Low-Resourced Languages, Sinhala and Tamil.
- Cloud Vision API, Surya, Document AI, Tesseract, Subasa OCR, and EasyOCR were examined for their accuracy at character and word levels.
- Surya performed best for Sinhala with a Word Error Rate (WER) of 2.61%, while Document AI excelled for Tamil with a very low Character Error Rate (CER) of 0.78%.
- A novel synthetic Tamil OCR benchmarking dataset was introduced in the study.<br /><br /> <div>
arXiv:2507.18264v1 Announce Type: new 
Abstract: Solving the problem of Optical Character Recognition (OCR) on printed text for Latin and its derivative scripts can now be considered settled due to the volumes of research done on English and other High-Resourced Languages (HRL). However, for Low-Resourced Languages (LRL) that use unique scripts, it remains an open problem. This study presents a comparative analysis of the zero-shot performance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The selected engines include both commercial and open-source systems, aiming to evaluate the strengths of each category. The Cloud Vision API, Surya, Document AI, and Tesseract were evaluated for both Sinhala and Tamil, while Subasa OCR and EasyOCR were examined for only one language due to their limitations. The performance of these systems was rigorously analysed using five measurement techniques to assess accuracy at both the character and word levels. According to the findings, Surya delivered the best performance for Sinhala across all metrics, with a WER of 2.61%. Conversely, Document AI excelled across all metrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the above analysis, we also introduce a novel synthetic Tamil OCR benchmarking dataset.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer</title>
<link>https://arxiv.org/abs/2507.18294</link>
<guid>https://arxiv.org/abs/2507.18294</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, Stylistic customization, Low-Rank Adaptation, Instruction adherence, Enterprise communication <br />
Summary: <br />
- Adapting Large Language Models (LLMs) to specific stylistic characteristics, such as brand voice or authorial tones, is essential for enterprise communication but challenging without compromising instruction adherence.
- The StyleAdaptedLM framework utilizes Low-Rank Adaptation (LoRA) to efficiently transfer stylistic traits to instruction-following models.
- LoRA adapters are trained on a base model with varied unstructured stylistic corpora and then merged with a separate instruction-following model, enabling robust stylistic customization without paired data.
- Experiments across different datasets and models show improved stylistic consistency while maintaining instruction adherence, confirmed through human evaluations of brand-specific convention uptake.
- StyleAdaptedLM offers a practical solution for personalized stylistic adaptation in LLMs. <br /> 
Summary: <div>
arXiv:2507.18294v1 Announce Type: new 
Abstract: Adapting LLMs to specific stylistic characteristics, like brand voice or authorial tones, is crucial for enterprise communication but challenging to achieve from corpora which lacks instruction-response formatting without compromising instruction adherence. We introduce StyleAdaptedLM, a framework that efficiently transfers stylistic traits to instruction-following models using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base model with diverse unstructured stylistic corpora, then merged with a separate instruction-following model. This enables robust stylistic customization without paired data or sacrificing task performance. Experiments across multiple datasets and models demonstrate improved stylistic consistency while preserving instruction adherence, with human evaluations confirming brand-specific convention uptake. StyleAdaptedLM offers an efficient path for stylistic personalization in LLMs.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit</title>
<link>https://arxiv.org/abs/2507.18305</link>
<guid>https://arxiv.org/abs/2507.18305</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, backdoor attacks, data poisoning, chain-of-thought reasoning, artificial intelligence 

Summary:
Large reasoning models (LRMs) are advanced language models designed for complex reasoning tasks, with a focus on chain-of-thought (CoT) reasoning capabilities. A new attack vector called "overthinking backdoors" targets LRMs by introducing a tunable backdoor that allows attackers to control the model's reasoning verbosity. This attack is implemented through data poisoning, using a trigger to increase the verbosity of the model's responses without affecting correctness. By injecting redundant refinement steps into the reasoning process, the attacker can significantly increase the length of the reasoning process. The source code for this attack method is available on GitHub. This novel approach demonstrates the potential vulnerability of LRMs to controlled attacks that manipulate the extent of reasoning without impacting the final output correctness.

<br /><br />Summary: Large reasoning models (LRMs) with chain-of-thought reasoning capabilities are vulnerable to "overthinking backdoor" attacks using data poisoning. Attackers can manipulate the verbosity of LRMs by introducing triggers that increase the length of reasoning processes without affecting correctness. This controlled attack method highlights the potential security risks associated with LRMs in artificial intelligence research. <div>
arXiv:2507.18305v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) have emerged as a significant advancement in artificial intelligence, representing a specialized class of large language models (LLMs) designed to tackle complex reasoning tasks. The defining characteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning capabilities. In this paper, we identify a previously unexplored attack vector against LRMs, which we term "overthinking backdoors". We advance this concept by proposing a novel tunable backdoor, which moves beyond simple on/off attacks to one where an attacker can precisely control the extent of the model's reasoning verbosity. Our attack is implemented through a novel data poisoning methodology. It pairs a tunable trigger-where the number of repetitions signals the desired intensity-with a correspondingly verbose CoT response. These responses are programmatically generated by instructing a teacher LLM to inject a controlled number of redundant refinement steps into a correct reasoning process. The approach preserves output correctness, which ensures stealth and establishes the attack as a pure resource-consumption vector. Extensive empirical results on various LRMs demonstrate that our method can reliably trigger a controllable, multi-fold increase in the length of the reasoning process, without degrading the final answer's correctness. Our source code is available at https://github.com/FZaKK/BadReasoner.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Evaluating Machine Translation Bias</title>
<link>https://arxiv.org/abs/2507.18338</link>
<guid>https://arxiv.org/abs/2507.18338</guid>
<content:encoded><![CDATA[
<div> gender, machine translation, bias, uncertainty, debiasing

Summary: 
Machine translation models often struggle with gendered languages, especially when the source language does not explicitly mark gender. This can lead to biased translations based on stereotypes rather than context. The study suggests that models should maintain uncertainty when translating ambiguous gender instances, even if they are confident in their translation. Results show that models that are accurate in translating unambiguous instances may not exhibit the same level of uncertainty in ambiguous cases. Additionally, debiasing techniques have varying effects on ambiguous and unambiguous translations, indicating the need for further research in mitigating biases in machine translation. <div>
arXiv:2507.18338v1 Announce Type: new 
Abstract: In machine translation (MT), when the source sentence includes a lexeme whose gender is not overtly marked, but whose target-language equivalent requires gender specification, the model must infer the appropriate gender from the context and/or external knowledge. Studies have shown that MT models exhibit biased behaviour, relying on stereotypes even when they clash with contextual information. We posit that apart from confidently translating using the correct gender when it is evident from the input, models should also maintain uncertainty about the gender when it is ambiguous. Using recently proposed metrics of semantic uncertainty, we find that models with high translation and gender accuracy on unambiguous instances do not necessarily exhibit the expected level of uncertainty in ambiguous ones. Similarly, debiasing has independent effects on ambiguous and unambiguous translation instances.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning</title>
<link>https://arxiv.org/abs/2507.18340</link>
<guid>https://arxiv.org/abs/2507.18340</guid>
<content:encoded><![CDATA[
<div> Keywords: In-context learning, LLMs, example retrieval, TDR framework, NLP tasks

Summary:
In the realm of in-context learning, the quality of input-output examples plays a crucial role in the effectiveness of language model handling various tasks. The TDR framework addresses two key challenges: distinguishing cross-task data distributions and establishing a fine-grained connection between retriever output and LLM feedback. TDR decouples examples from different tasks, enhancing the retrieval module's ability to select task-specific examples within a multi-task dataset. Additionally, TDR incorporates feedback from LLMs to supervise and refine the retrieval process, resulting in the retrieval of high-quality examples. Experimental results on 30 NLP tasks demonstrate that TDR consistently improves performance, achieving state-of-the-art results across all datasets. Moreover, TDR is easily integrated with different LLMs, making it a versatile and effective tool for enhancing example retrieval capabilities in in-context learning scenarios.

<br /><br />Summary: <div>
arXiv:2507.18340v1 Announce Type: new 
Abstract: In-context learning (ICL) has become a classic approach for enabling LLMs to handle various tasks based on a few input-output examples. The effectiveness of ICL heavily relies on the quality of these examples, and previous works which focused on enhancing example retrieval capabilities have achieved impressive performances. However, two challenges remain in retrieving high-quality examples: (1) Difficulty in distinguishing cross-task data distributions, (2) Difficulty in making the fine-grained connection between retriever output and feedback from LLMs. In this paper, we propose a novel framework called TDR. TDR decouples the ICL examples from different tasks, which enables the retrieval module to retrieve examples specific to the target task within a multi-task dataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise and guide the training of the retrieval module, which helps to retrieve high-quality examples. We conducted extensive experiments on a suite of 30 NLP tasks, the results demonstrate that TDR consistently improved results across all datasets and achieves state-of-the-art performance. Meanwhile, our approach is a plug-and-play method, which can be easily combined with various LLMs to improve example retrieval abilities for ICL. The code is available at https://github.com/Nnn-s/TDR.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence</title>
<link>https://arxiv.org/abs/2507.18343</link>
<guid>https://arxiv.org/abs/2507.18343</guid>
<content:encoded><![CDATA[
<div> Keywords: propaganda detection, social media, Large Language Model, human annotation, hierarchical taxonomy

Summary: 
This paper presents a novel framework for propaganda detection on social media that combines human expertise with Large Language Model (LLM) assistance. The framework includes a hierarchical taxonomy of 14 fine-grained propaganda techniques organized into three categories. A human annotation study on the HQP dataset reveals low inter-annotator agreement for fine-grained labels. An LLM-assisted pre-annotation pipeline extracts propagandistic spans, generates concise explanations, and assigns labels. A secondary human verification study shows improved agreement and time-efficiency. Smaller language models (SLMs) are fine-tuned on LLM-generated data for structured annotation, enabling a large model to produce annotations and a smaller model to learn from them via knowledge distillation. This work aims to develop scalable and robust propaganda detection systems, promoting transparent and accountable media ecosystems in alignment with SDG 16. The code for this framework is available on GitHub.
<br /><br />Summary: <div>
arXiv:2507.18343v1 Announce Type: new 
Abstract: Propaganda detection on social media remains challenging due to task complexity and limited high-quality labeled data. This paper introduces a novel framework that combines human expertise with Large Language Model (LLM) assistance to improve both annotation consistency and scalability. We propose a hierarchical taxonomy that organizes 14 fine-grained propaganda techniques into three broader categories, conduct a human annotation study on the HQP dataset that reveals low inter-annotator agreement for fine-grained labels, and implement an LLM-assisted pre-annotation pipeline that extracts propagandistic spans, generates concise explanations, and assigns local labels as well as a global label. A secondary human verification study shows significant improvements in both agreement and time-efficiency. Building on this, we fine-tune smaller language models (SLMs) to perform structured annotation. Instead of fine-tuning on human annotations, we train on high-quality LLM-generated data, allowing a large model to produce these annotations and a smaller model to learn to generate them via knowledge distillation. Our work contributes towards the development of scalable and robust propaganda detection systems, supporting the idea of transparent and accountable media ecosystems in line with SDG 16. The code is publicly available at our GitHub repository.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLEAR: Error Analysis via LLM-as-a-Judge Made Easy</title>
<link>https://arxiv.org/abs/2507.18392</link>
<guid>https://arxiv.org/abs/2507.18392</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, error analysis, interactive dashboard, system-level error issues, benchmarking 

Summary:
CLEAR is an open-source package designed to facilitate error analysis for Large Language Models (LLMs). It aims to provide specific and actionable feedback on model performance by generating per-instance textual feedback and identifying system-level error issues. The package offers an interactive dashboard that allows users to visualize and analyze errors, apply filters to isolate specific issues, and delve into individual instances. By using CLEAR, researchers can gain insights into the reasons behind a model's performance, going beyond top-level scores or rankings. The demonstration of CLEAR analysis on RAG and Math benchmarks highlights its utility in evaluating LLMs effectively. <div>
arXiv:2507.18392v1 Announce Type: new 
Abstract: The evaluation of Large Language Models (LLMs) increasingly relies on other LLMs acting as judges. However, current evaluation paradigms typically yield a single score or ranking, answering which model is better but not why. While essential for benchmarking, these top-level scores obscure the specific, actionable reasons behind a model's performance. To bridge this gap, we introduce CLEAR, an interactive, open-source package for LLM-based error analysis. CLEAR first generates per-instance textual feedback, then it creates a set of system-level error issues, and quantifies the prevalence of each identified issue. Our package also provides users with an interactive dashboard that allows for a comprehensive error analysis through aggregate visualizations, applies interactive filters to isolate specific issues or score ranges, and drills down to the individual instances that exemplify a particular behavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks, and showcase its utility through a user case study.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Factual Inconsistencies in Multilingual Wikipedia Tables</title>
<link>https://arxiv.org/abs/2507.18406</link>
<guid>https://arxiv.org/abs/2507.18406</guid>
<content:encoded><![CDATA[
<div> Keywords: Wikipedia, multilingual inconsistencies, tabular data, factual verification, AI systems<br />
Summary:<br />
- The study focuses on cross-lingual inconsistencies in Wikipedia's structured content, particularly tabular data.
- Methodology was developed to collect, align, and analyze tables from multilingual Wikipedia articles to identify categories of inconsistency.
- Various quantitative and qualitative metrics were applied to assess multilingual alignment using a sample dataset.
- Findings have implications for factual verification processes and the interaction of multilingual knowledge.
- The study also highlights the importance of designing reliable AI systems that leverage Wikipedia content. 
<br /><br />Summary: <div>
arXiv:2507.18406v1 Announce Type: new 
Abstract: Wikipedia serves as a globally accessible knowledge source with content in over 300 languages. Despite covering the same topics, the different versions of Wikipedia are written and updated independently. This leads to factual inconsistencies that can impact the neutrality and reliability of the encyclopedia and AI systems, which often rely on Wikipedia as a main training source. This study investigates cross-lingual inconsistencies in Wikipedia's structured content, with a focus on tabular data. We developed a methodology to collect, align, and analyze tables from Wikipedia multilingual articles, defining categories of inconsistency. We apply various quantitative and qualitative metrics to assess multilingual alignment using a sample dataset. These insights have implications for factual verification, multilingual knowledge interaction, and design for reliable AI systems leveraging Wikipedia content.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs</title>
<link>https://arxiv.org/abs/2507.18417</link>
<guid>https://arxiv.org/abs/2507.18417</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment analysis, finance, Generative AI, Direct Preference Optimization, LLM <br />
<br />
Summary: <br />
Online finance-related textual data is increasingly influencing trading decisions, emphasizing the need for accurate sentiment analysis. Current methods, such as supervised fine-tuned large language models (LLMs), can struggle to generalize to new data. To address this, FinDPO, a finance-specific LLM framework, utilizes Direct Preference Optimization to align with human preferences post-training. FinDPO outperforms existing models by 11% on sentiment classification benchmarks. It also introduces a novel 'logit-to-score' conversion to integrate sentiment predictions into portfolio strategies. Simulation results show FinDPO maintains substantial annual returns of 67% and a strong Sharpe ratio of 2.0, even with realistic transaction costs. This approach marks a significant advancement in sentiment-based financial analysis. <br /> <div>
arXiv:2507.18417v1 Announce Type: new 
Abstract: Opinions expressed in online finance-related textual data are having an increasingly profound impact on trading decisions and market movements. This trend highlights the vital role of sentiment analysis as a tool for quantifying the nature and strength of such opinions. With the rapid development of Generative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs) have become the de facto standard for financial sentiment analysis. However, the SFT paradigm can lead to memorization of the training data and often fails to generalize to unseen samples. This is a critical limitation in financial domains, where models must adapt to previously unobserved events and the nuanced, domain-specific language of finance. To this end, we introduce FinDPO, the first finance-specific LLM framework based on post-training human preference alignment via Direct Preference Optimization (DPO). The proposed FinDPO achieves state-of-the-art performance on standard sentiment classification benchmarks, outperforming existing supervised fine-tuned models by 11% on the average. Uniquely, the FinDPO framework enables the integration of a fine-tuned causal LLM into realistic portfolio strategies through a novel 'logit-to-score' conversion, which transforms discrete sentiment predictions into continuous, rankable sentiment scores (probabilities). In this way, simulations demonstrate that FinDPO is the first sentiment-based approach to maintain substantial positive returns of 67% annually and strong risk-adjusted performance, as indicated by a Sharpe ratio of 2.0, even under realistic transaction costs of 5 basis points (bps).
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data</title>
<link>https://arxiv.org/abs/2507.18442</link>
<guid>https://arxiv.org/abs/2507.18442</guid>
<content:encoded><![CDATA[
<div> benchmark, Arabic, tabular data, large language models, reasoning

Summary:
The article introduces AraTable, a benchmark specifically designed to evaluate the performance of large language models (LLMs) on Arabic tabular data. AraTable includes tasks such as question answering, fact verification, and complex reasoning, covering a wide range of Arabic tabular sources. The dataset is curated using a hybrid pipeline involving LLMs and human experts to ensure quality. Initial results show that while LLMs perform well on basic tasks, they struggle with more complex reasoning and fact verification challenges. The research also proposes an automated evaluation framework that nearly matches human judge performance. This work aims to provide a valuable resource for developing models that can effectively process and analyze Arabic structured data. 

<br /><br />Summary: <div>
arXiv:2507.18442v1 Announce Type: new 
Abstract: The cognitive and reasoning abilities of large language models (LLMs) have enabled remarkable progress in natural language processing. However, their performance in interpreting structured data, especially in tabular formats, remains limited. Although benchmarks for English tabular data are widely available, Arabic is still underrepresented because of the limited availability of public resources and its unique language features. To address this gap, we present AraTable, a novel and comprehensive benchmark designed to evaluate the reasoning and understanding capabilities of LLMs when applied to Arabic tabular data. AraTable consists of various evaluation tasks, such as direct question answering, fact verification, and complex reasoning, involving a wide range of Arabic tabular sources. Our methodology follows a hybrid pipeline, where initial content is generated by LLMs and subsequently filtered and verified by human experts to ensure high dataset quality. Initial analyses using AraTable show that, while LLMs perform adequately on simpler tabular tasks such as direct question answering, they continue to face significant cognitive challenges when tasks require deeper reasoning and fact verification. This indicates that there are substantial opportunities for future work to improve performance on complex tabular reasoning tasks. We also propose a fully automated evaluation framework that uses a self-deliberation mechanism and achieves performance nearly identical to that of human judges. This research provides a valuable, publicly available resource and evaluation framework that can help accelerate the development of foundational models for processing and analysing Arabic structured data.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language</title>
<link>https://arxiv.org/abs/2507.18448</link>
<guid>https://arxiv.org/abs/2507.18448</guid>
<content:encoded><![CDATA[
<div> Keywords: punctuation restoration, transformer-based models, Bangla text, data augmentation, low-resource NLP

Summary: 
This study explores the use of transformer-based models, specifically XLM-RoBERTa-large, for automatically restoring punctuation in unpunctuated Bangla text. The focus is on predicting four punctuation marks: period, comma, question mark, and exclamation mark in diverse text domains. To overcome the lack of annotated resources, a large and varied training corpus was constructed, along with data augmentation techniques. The best-performing model, with an augmentation factor of alpha = 0.20%, achieves high accuracies on different test sets. Strong generalization is shown to reference and ASR transcripts, indicating the model's effectiveness in noisy scenarios. This work establishes a solid baseline for Bangla punctuation restoration and provides publicly available datasets and code for further research in low-resource NLP. 

<br /><br />Summary: <div>
arXiv:2507.18448v1 Announce Type: new 
Abstract: Punctuation restoration enhances the readability of text and is critical for post-processing tasks in Automatic Speech Recognition (ASR), especially for low-resource languages like Bangla. In this study, we explore the application of transformer-based models, specifically XLM-RoBERTa-large, to automatically restore punctuation in unpunctuated Bangla text. We focus on predicting four punctuation marks: period, comma, question mark, and exclamation mark across diverse text domains. To address the scarcity of annotated resources, we constructed a large, varied training corpus and applied data augmentation techniques. Our best-performing model, trained with an augmentation factor of alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the Reference set, and 90.2% on the ASR set.
  Results show strong generalization to reference and ASR transcripts, demonstrating the model's effectiveness in real-world, noisy scenarios. This work establishes a strong baseline for Bangla punctuation restoration and contributes publicly available datasets and code to support future research in low-resource NLP.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generation of Synthetic Clinical Text: A Systematic Review</title>
<link>https://arxiv.org/abs/2507.18451</link>
<guid>https://arxiv.org/abs/2507.18451</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic medical text, NLP, privacy, evaluation, Transformer architectures

Summary: 
The paper conducts a systematic review on generating synthetic medical free-text, focusing on the purpose, techniques, and evaluation methods. The main purposes of generating synthetic medical text include text augmentation, assistive writing, privacy-preserving, and annotation. Transformer architectures, particularly GPTs, are commonly used for text generation. Evaluation of synthetic medical text focuses on similarity, privacy, structure, and utility, with utility being the main method used. While synthetic medical text shows potential in improving accuracy and overcoming sparsity issues, privacy remains a concern. Human assessments are necessary to ensure sensitive information is not present. Despite this, advancements in synthetic medical text generation can streamline workflows and pipeline development, reducing the legal complexities of data transfer. 

<br /><br />Summary: <div>
arXiv:2507.18451v1 Announce Type: new 
Abstract: Generating clinical synthetic text represents an effective solution for common clinical NLP issues like sparsity and privacy. This paper aims to conduct a systematic review on generating synthetic medical free-text by formulating quantitative analysis to three research questions concerning (i) the purpose of generation, (ii) the techniques, and (iii) the evaluation methods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE, Google Scholar, and arXiv databases for publications associated with generating synthetic medical unstructured free-text. We have identified 94 relevant articles out of 1,398 collected ones. A great deal of attention has been given to the generation of synthetic medical text from 2018 onwards, where the main purpose of such a generation is towards text augmentation, assistive writing, corpus building, privacy-preserving, annotation, and usefulness. Transformer architectures were the main predominant technique used to generate the text, especially the GPTs. On the other hand, there were four main aspects of evaluation, including similarity, privacy, structure, and utility, where utility was the most frequent method used to assess the generated synthetic medical text. Although the generated synthetic medical text demonstrated a moderate possibility to act as real medical documents in different downstream NLP tasks, it has proven to be a great asset as augmented, complementary to the real documents, towards improving the accuracy and overcoming sparsity/undersampling issues. Yet, privacy is still a major issue behind generating synthetic medical text, where more human assessments are needed to check for the existence of any sensitive information. Despite that, advances in generating synthetic medical text will considerably accelerate the adoption of workflows and pipeline development, discarding the time-consuming legalities of data transfer.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models</title>
<link>https://arxiv.org/abs/2507.18504</link>
<guid>https://arxiv.org/abs/2507.18504</guid>
<content:encoded><![CDATA[
<div> keywords: Large Language Models, tabular data generation, sparse feature-level dependencies, GraDe, dependency graphs<br />
<br />
Summary: <br />
Large Language Models (LLMs) have shown promise in generating tabular data by representing feature-value pairs. However, tabular data often has sparse feature-level dependencies, leading to a diluted focus on critical relationships in LLMs. To address this issue, GraDe (Graph-Guided Dependency Learning) integrates sparse dependency graphs into LLMs' attention mechanism, prioritizing important feature interactions. Experimental results across various datasets show that GraDe outperforms existing LLM-based methods by up to 12% on complex datasets and achieves comparable results with state-of-the-art approaches in synthetic data quality. GraDe is a non-intrusive yet effective solution for modeling structure-aware tabular data with LLMs. <div>
arXiv:2507.18504v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown strong potential for tabular data generation by modeling textualized feature-value pairs. However, tabular data inherently exhibits sparse feature-level dependencies, where many feature interactions are structurally insignificant. This creates a fundamental mismatch as LLMs' self-attention mechanism inevitably distributes focus across all pairs, diluting attention on critical relationships, particularly in datasets with complex dependencies or semantically ambiguous features. To address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a novel method that explicitly integrates sparse dependency graphs into LLMs' attention mechanism. GraDe employs a lightweight dynamic graph learning module guided by externally extracted functional dependencies, prioritizing key feature interactions while suppressing irrelevant ones. Our experiments across diverse real-world datasets demonstrate that GraDe outperforms existing LLM-based approaches by up to 12% on complex datasets while achieving competitive results with state-of-the-art approaches in synthetic data quality. Our method is minimally intrusive yet effective, offering a practical solution for structure-aware tabular data modeling with LLMs.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Moral Gap of Large Language Models</title>
<link>https://arxiv.org/abs/2507.18523</link>
<guid>https://arxiv.org/abs/2507.18523</guid>
<content:encoded><![CDATA[
<div> Keywords: Moral foundation detection, Language models, Fine-tuned transformers, Twitter, Reddit 

Summary: 
The study focuses on the importance of detecting moral foundations in social discourse and the development of ethically-aligned AI systems. While large language models (LLMs) are effective in various tasks, their performance in specialized moral reasoning is uncertain. The research compares LLMs and fine-tuned transformers using Twitter and Reddit datasets, analyzing ROC, PR, and DET curves. Results show significant performance differences, with LLMs having high false negative rates and a tendency to miss moral content despite prompt engineering adjustments. Task-specific fine-tuning proves to be more effective for moral reasoning applications compared to simple prompting. This comparison sheds light on the need for tailored approaches in ethical AI development. 

<br /><br />Summary: <div>
arXiv:2507.18523v1 Announce Type: new 
Abstract: Moral foundation detection is crucial for analyzing social discourse and developing ethically-aligned AI systems. While large language models excel across diverse tasks, their performance on specialized moral reasoning remains unclear.
  This study provides the first comprehensive comparison between state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit datasets using ROC, PR, and DET curve analysis.
  Results reveal substantial performance gaps, with LLMs exhibiting high false negative rates and systematic under-detection of moral content despite prompt engineering efforts. These findings demonstrate that task-specific fine-tuning remains superior to prompting for moral reasoning applications.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective Multi-Task Learning for Biomedical Named Entity Recognition</title>
<link>https://arxiv.org/abs/2507.18542</link>
<guid>https://arxiv.org/abs/2507.18542</guid>
<content:encoded><![CDATA[
<div> Nested named entities, multi-task learning, biomedical NER, cross-corpus evaluation, generalization

Summary:
SRU-NER (Slot-based Recurrent Unit NER) is a novel approach for Biomedical Named Entity Recognition that addresses the challenges posed by complex biomedical terminology and inconsistent annotations across datasets. It handles nested named entities and integrates multiple datasets through multi-task learning. By dynamically adjusting loss computation, SRU-NER avoids penalizing predictions of entity types not present in a dataset, thus mitigating annotation gaps. Extensive experiments, including cross-corpus evaluation and human assessment, demonstrate the competitive performance of SRU-NER in both biomedical and general-domain NER tasks. The model also shows improvement in cross-domain generalization, making it a promising solution for challenging NER tasks in the biomedical domain. 

<br /><br />Summary: <div>
arXiv:2507.18542v1 Announce Type: new 
Abstract: Biomedical Named Entity Recognition presents significant challenges due to the complexity of biomedical terminology and inconsistencies in annotation across datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER), a novel approach designed to handle nested named entities while integrating multiple datasets through an effective multi-task learning strategy. SRU-NER mitigates annotation gaps by dynamically adjusting loss computation to avoid penalizing predictions of entity types absent in a given dataset. Through extensive experiments, including a cross-corpus evaluation and human assessment of the model's predictions, SRU-NER achieves competitive performance in biomedical and general-domain NER tasks, while improving cross-domain generalization.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface</title>
<link>https://arxiv.org/abs/2507.18546</link>
<guid>https://arxiv.org/abs/2507.18546</guid>
<content:encoded><![CDATA[
<div> Keywords: Information Extraction, NLP Applications, Named Entity Recognition, Text Classification, Pretrained Transformer Encoder

Summary:
GLiNER2 is a unified framework that enhances the original GLiNER architecture to support named entity recognition, text classification, and hierarchical structured data extraction in a single efficient model. It is built on a pretrained transformer encoder architecture, maintaining CPU efficiency and compact size while introducing multi-task composition through a schema-based interface. The framework demonstrates competitive performance across extraction and classification tasks, offering substantial improvements in deployment accessibility compared to large language model-based alternatives. GLiNER2 is released as an open-source pip-installable library with pre-trained models and documentation available on GitHub at https://github.com/fastino-ai/GLiNER2.<br /><br />Summary: <div>
arXiv:2507.18546v1 Announce Type: new 
Abstract: Information extraction (IE) is fundamental to numerous NLP applications, yet existing solutions often require specialized models for different tasks or rely on computationally expensive large language models. We present GLiNER2, a unified framework that enhances the original GLiNER architecture to support named entity recognition, text classification, and hierarchical structured data extraction within a single efficient model. Built pretrained transformer encoder architecture, GLiNER2 maintains CPU efficiency and compact size while introducing multi-task composition through an intuitive schema-based interface. Our experiments demonstrate competitive performance across extraction and classification tasks with substantial improvements in deployment accessibility compared to LLM-based alternatives. We release GLiNER2 as an open-source pip-installable library with pre-trained models and documentation at https://github.com/fastino-ai/GLiNER2.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation</title>
<link>https://arxiv.org/abs/2507.18562</link>
<guid>https://arxiv.org/abs/2507.18562</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Machine Translation, Visual Information, Multimodal Scene Graphs, Graph-guided Inductive Image-Free MMT, Cross-modal Graph Attention Network

Summary:
GIIFT introduces a two-stage framework for Multimodal Machine Translation that utilizes novel multimodal scene graphs to integrate modality-specific information. By employing a Graph Attention Network adapter, GIIFT learns multimodal knowledge in a unified fused space and generalizes it to image-free translation domains. Experimental results on the Multi30K dataset for English-to-French and English-to-German tasks show that GIIFT outperforms existing methods, achieving state-of-the-art performance without images during inference. Additionally, results on the WMT benchmark demonstrate significant improvements over image-free translation baselines, highlighting the effectiveness of GIIFT for inductive image-free inference.<br /><br />Summary: <div>
arXiv:2507.18562v1 Announce Type: new 
Abstract: Multimodal Machine Translation (MMT) has demonstrated the significant help of visual information in machine translation. However, existing MMT methods face challenges in leveraging the modality gap by enforcing rigid visual-linguistic alignment whilst being confined to inference within their trained multimodal domains. In this work, we construct novel multimodal scene graphs to preserve and integrate modality-specific information and introduce GIIFT, a two-stage Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph Attention Network adapter to learn multimodal knowledge in a unified fused space and inductively generalize it to broader image-free translation domains. Experimental results on the Multi30K dataset of English-to-French and English-to-German tasks demonstrate that our GIIFT surpasses existing approaches and achieves the state-of-the-art, even without images during inference. Results on the WMT benchmark show significant improvements over the image-free translation baselines, demonstrating the strength of GIIFT towards inductive image-free inference.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods</title>
<link>https://arxiv.org/abs/2507.18570</link>
<guid>https://arxiv.org/abs/2507.18570</guid>
<content:encoded><![CDATA[
<div> Tokenization, DNA Language Models, 6-mer, Byte Pair Encoding, BPE-600

Summary:
- The paper introduces a new hybrid tokenization strategy for DNA Language Models (DLMs) by combining 6-mer tokenization with Byte Pair Encoding (BPE-600).
- Traditional k-mer tokenization struggles with uneven token distribution and limited global sequence context understanding.
- The hybrid approach combines 6mer tokens with optimally selected BPE tokens generated through 600 BPE cycles, resulting in a balanced and context-aware vocabulary for DLMs.
- A DLM trained on this hybrid vocabulary showed improved performance in next-k-mer prediction, surpassing state-of-the-art models like NT, DNABERT2, and GROVER.
- The results highlight the hybrid tokenization strategy's ability to capture both local sequence structures and global contextual information, emphasizing its significance in genomic language modeling for future applications in DNA sequence analysis and biological research.

<br /><br />Summary: <div>
arXiv:2507.18570v1 Announce Type: new 
Abstract: This paper presents a novel hybrid tokenization strategy that enhances the performance of DNA Language Models (DLMs) by combining 6-mer tokenization with Byte Pair Encoding (BPE-600). Traditional k-mer tokenization is effective at capturing local DNA sequence structures but often faces challenges, including uneven token distribution and a limited understanding of global sequence context. To address these limitations, we propose merging unique 6mer tokens with optimally selected BPE tokens generated through 600 BPE cycles. This hybrid approach ensures a balanced and context-aware vocabulary, enabling the model to capture both short and long patterns within DNA sequences simultaneously. A foundational DLM trained on this hybrid vocabulary was evaluated using next-k-mer prediction as a fine-tuning task, demonstrating significantly improved performance. The model achieved prediction accuracies of 10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming state-of-the-art models such as NT, DNABERT2, and GROVER. These results highlight the ability of the hybrid tokenization strategy to preserve both the local sequence structure and global contextual information in DNA modeling. This work underscores the importance of advanced tokenization methods in genomic language modeling and lays a robust foundation for future applications in downstream DNA sequence analysis and biological research.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs</title>
<link>https://arxiv.org/abs/2507.18578</link>
<guid>https://arxiv.org/abs/2507.18578</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion Large Language Models, Parallel generation, Wide-In, Narrow-Out, WINO 

Summary: 
WINO is a training-free decoding algorithm designed to improve the quality-speed trade-off in Diffusion Large Language Models (DLLMs). This algorithm enables revokable decoding by using a parallel draft-and-verify mechanism, drafting multiple tokens simultaneously while verifying and refining suspicious ones using bidirectional context. Through experiments on open-source DLLMs like LLaDA and MMaDA, WINO was shown to significantly accelerate inference speed while improving accuracy. For example, on the GSM8K math benchmark, WINO achieved a 6$\times$ speedup with a 2.58% accuracy improvement, and on the Flickr30K captioning task, it achieved a 10$\times$ speedup with enhanced performance. The results demonstrate the superiority of WINO in resolving the quality-speed trade-off in DLLMs, making it a promising solution for fast parallel generation with improved accuracy. 

<br /><br />Summary: <div>
arXiv:2507.18578v1 Announce Type: new 
Abstract: Diffusion Large Language Models (DLLMs) have emerged as a compelling alternative to Autoregressive models, designed for fast parallel generation. However, existing DLLMs are plagued by a severe quality-speed trade-off, where faster parallel decoding leads to significant performance degradation. We attribute this to the irreversibility of standard decoding in DLLMs, which is easily polarized into the wrong decoding direction along with early error context accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO), a training-free decoding algorithm that enables revokable decoding in DLLMs. WINO employs a parallel draft-and-verify mechanism, aggressively drafting multiple tokens while simultaneously using the model's bidirectional context to verify and re-mask suspicious ones for refinement. Verified in open-source DLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the quality-speed trade-off. For instance, on the GSM8K math benchmark, it accelerates inference by 6$\times$ while improving accuracy by 2.58%; on Flickr30K captioning, it achieves a 10$\times$ speedup with higher performance. More comprehensive experiments are conducted to demonstrate the superiority and provide an in-depth understanding of WINO.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition</title>
<link>https://arxiv.org/abs/2507.18580</link>
<guid>https://arxiv.org/abs/2507.18580</guid>
<content:encoded><![CDATA[
<div> Methodology, SRAG-MAV framework, Fine-Grained Chinese Hate Speech Recognition, Contextual prompts, Multi-round inference

Summary:
The paper introduces a novel SRAG-MAV framework for Fine-Grained Chinese Hate Speech Recognition. The system utilizes task reformulation to extract triplets, incorporates Self-Retrieval-Augmented Generation (SRAG) with dynamic retrieval for creating contextual prompts, and employs Multi-Round Accumulative Voting (MAV) for improved output stability and performance. Utilizing the Qwen2.5-7B model, the system achieves a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on the STATE ToxiCN dataset. This outperforms baselines like GPT-4o and a fine-tuned Qwen2.5-7B model. The code for the system is available on GitHub. <div>
arXiv:2507.18580v1 Announce Type: new 
Abstract: This paper presents our system for CCL25-Eval Task 10, addressing Fine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel SRAG-MAV framework that synergistically integrates task reformulation(TR), Self-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting (MAV). Our method reformulates the quadruplet extraction task into triplet extraction, uses dynamic retrieval from the training set to create contextual prompts, and applies multi-round inference with voting to improve output stability and performance. Our system, based on the Qwen2.5-7B model, achieves a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o (Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The code is available at https://github.com/king-wang123/CCL25-SRAG-MAV.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs</title>
<link>https://arxiv.org/abs/2507.18584</link>
<guid>https://arxiv.org/abs/2507.18584</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, specialized domains, data synthesis, logic, inspection

Summary: <br /><br />
The article introduces AQuilt, a framework designed to improve the performance of large language models (LLMs) in specialized domains. Existing approaches to address this issue often come with high computational costs or limited performance gains. AQuilt aims to tackle these challenges by constructing instruction-tuning data for specialized domains using unlabeled data. By incorporating logic and inspection into the data generation process, AQuilt encourages reasoning processes and self-inspection to enhance model performance. The framework allows for customizable task instructions, enabling high-quality data generation for any task. Experiments demonstrate that AQuilt is comparable to DeepSeek-V3 while requiring only 17% of the production cost. The generated data shows higher relevance to downstream tasks, highlighting the effectiveness of the AQuilt framework. Further details, including source code, models, and scripts, can be found on the project's GitHub repository. <div>
arXiv:2507.18584v1 Announce Type: new 
Abstract: Despite the impressive performance of large language models (LLMs) in general domains, they often underperform in specialized domains. Existing approaches typically rely on data synthesis methods and yield promising results by using unlabeled data to capture domain-specific features. However, these methods either incur high computational costs or suffer from performance limitations, while also demonstrating insufficient generalization across different tasks. To address these challenges, we propose AQuilt, a framework for constructing instruction-tuning data for any specialized domains from corresponding unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic, and Task type. By incorporating logic and inspection, we encourage reasoning processes and self-inspection to enhance model performance. Moreover, customizable task instructions enable high-quality data generation for any task. As a result, we construct a dataset of 703k examples to train a powerful data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3 while utilizing just 17% of the production cost. Further analysis demonstrates that our generated data exhibits higher relevance to downstream tasks. Source code, models, and scripts are available at https://github.com/Krueske/AQuilt.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards</title>
<link>https://arxiv.org/abs/2507.18618</link>
<guid>https://arxiv.org/abs/2507.18618</guid>
<content:encoded><![CDATA[
<div> optimization, reasoning abilities, large language models, prompt model, textual feedback

Summary:
The Textual Reward Prompt framework (TRPrompt) introduces a new approach to improving reasoning abilities in large language models (LLMs) by incorporating textual feedback into prompt model training. This framework does not require dataset collection and iteratively improves prompts based on feedback. By leveraging textual rewards, TRPrompt can train a prompt model that generates state-of-the-art query-specific prompts for challenging math problems. This unified approach combines methods that use textual feedback and numerical rewards to enhance the capabilities of LLMs. TRPrompt benefits from the high-resolution signal provided by textual rewards and allows the prompt model to internalize what constitutes a "good" prompt. As a result, TRPrompt achieves impressive results on math datasets such as GSMHard and MATH. <div>
arXiv:2507.18618v1 Announce Type: new 
Abstract: Prompt optimization improves the reasoning abilities of large language models (LLMs) without requiring parameter updates to the target model. Following heuristic-based "Think step by step" approaches, the field has evolved in two main directions: while one group of methods uses textual feedback to elicit improved prompts from general-purpose LLMs in a training-free way, a concurrent line of research relies on numerical rewards to train a special prompt model, tailored for providing optimal prompts to the target model. In this paper, we introduce the Textual Reward Prompt framework (TRPrompt), which unifies these approaches by directly incorporating textual feedback into training of the prompt model. Our framework does not require prior dataset collection and is being iteratively improved with the feedback on the generated prompts. When coupled with the capacity of an LLM to internalize the notion of what a "good" prompt is, the high-resolution signal provided by the textual rewards allows us to train a prompt model yielding state-of-the-art query-specific prompts for the problems from the challenging math datasets GSMHard and MATH.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Checklists Are Better Than Reward Models For Aligning Language Models</title>
<link>https://arxiv.org/abs/2507.18624</link>
<guid>https://arxiv.org/abs/2507.18624</guid>
<content:encoded><![CDATA[
<div> criteria, reinforcement learning, instruction following, checklist feedback, language models  
Summary:<br /><br />Language models need adaptation to understand and follow user instructions effectively. This study introduces a novel approach called Reinforcement Learning from Checklist Feedback (RLCF), which uses flexible, instruction-specific criteria to improve instruction following. By extracting checklists from instructions and evaluating responses against them, RLCF computes rewards for reinforcement learning. Compared to other alignment methods, RLCF shows significant performance improvements on five benchmark tasks, including notable increases in satisfaction rates and win rates. These results highlight the potential of checklist feedback in enhancing language models' ability to meet diverse user needs.  
<br /><br /> <div>
arXiv:2507.18624v1 Announce Type: new 
Abstract: Language models must be adapted to understand and follow user instructions. Reinforcement learning is widely used to facilitate this -- typically using fixed criteria such as "helpfulness" and "harmfulness". In our work, we instead propose using flexible, instruction-specific criteria as a means of broadening the impact that reinforcement learning can have in eliciting instruction following. We propose "Reinforcement Learning from Checklist Feedback" (RLCF). From instructions, we extract checklists and evaluate how well responses satisfy each item - using both AI judges and specialized verifier programs - then combine these scores to compute rewards for RL. We compare RLCF with other alignment methods applied to a strong instruction following model (Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only method to improve performance on every benchmark, including a 4-point boost in hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a 3-point rise in win rate on Arena-Hard. These results establish checklist feedback as a key tool for improving language models' support of queries that express a multitude of needs.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving</title>
<link>https://arxiv.org/abs/2507.17753</link>
<guid>https://arxiv.org/abs/2507.17753</guid>
<content:encoded><![CDATA[
<div> communication strategies, large language model, problem-solving, education, AI

Summary:
- The study examines different communication modes in a dual-agent, chat-based mathematical problem-solving environment using the OpenAI GPT-4o model.
- Results show that dual-agent setups outperform single agents, with peer-to-peer collaboration being the most effective.
- Dialogue acts like statements, acknowledgment, and hints are crucial in collaborative problem-solving.
- Multi-agent frameworks enhance computational tasks in AI education.
- Effective communication strategies are essential for addressing complex problems in AI education. 

<br /><br />Summary: <div>
arXiv:2507.17753v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents are increasingly utilized in AI-aided education to support tutoring and learning. Effective communication strategies among LLM agents improve collaborative problem-solving efficiency and facilitate cost-effective adoption in education. However, little research has systematically evaluated the impact of different communication strategies on agents' problem-solving. Our study examines four communication modes, \textit{teacher-student interaction}, \textit{peer-to-peer collaboration}, \textit{reciprocal peer teaching}, and \textit{critical debate}, in a dual-agent, chat-based mathematical problem-solving environment using the OpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that dual-agent setups outperform single agents, with \textit{peer-to-peer collaboration} achieving the highest accuracy. Dialogue acts like statements, acknowledgment, and hints play a key role in collaborative problem-solving. While multi-agent frameworks enhance computational tasks, effective communication strategies are essential for tackling complex problems in AI education.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenSelect: A Generative Approach to Best-of-N</title>
<link>https://arxiv.org/abs/2507.17797</link>
<guid>https://arxiv.org/abs/2507.17797</guid>
<content:encoded><![CDATA[
arXiv:2507.17797v1 Announce Type: cross 
Abstract: Generative reward models with parallel sampling have enabled effective test-time scaling for reasoning tasks. Current approaches employ pointwise scoring of individual solutions or pairwise comparisons. However, pointwise methods underutilize LLMs' comparative abilities, while pairwise methods scale inefficiently with larger sampling budgets. We introduce GenSelect, where the LLM uses long reasoning to select the best solution among N candidates. This leverages LLMs' comparative strengths while scaling efficiently across parallel sampling budgets. For math reasoning, we demonstrate that reasoning models, such as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing scoring approaches with simple prompting.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation</title>
<link>https://arxiv.org/abs/2507.17937</link>
<guid>https://arxiv.org/abs/2507.17937</guid>
<content:encoded><![CDATA[
arXiv:2507.17937v1 Announce Type: cross 
Abstract: Lyrics-to-Song (LS2) generation models promise end-to-end music synthesis from text, yet their vulnerability to training data memorization remains underexplored. We introduce Adversarial PhoneTic Prompting (APT), a novel attack where lyrics are semantically altered while preserving their acoustic structure through homophonic substitutions (e.g., Eminem's famous "mom's spaghetti" $\rightarrow$ "Bob's confetti"). Despite these distortions, we uncover a powerful form of sub-lexical memorization: models like SUNO and YuE regenerate outputs strikingly similar to known training content, achieving high similarity across audio-domain metrics, including CLAP, AudioJudge, and CoverID. This vulnerability persists across multiple languages and genres. More surprisingly, we discover that phoneme-altered lyrics alone can trigger visual memorization in text-to-video models. When prompted with phonetically modified lyrics from Lose Yourself, Veo 3 reconstructs visual elements from the original music video -- including character appearance and scene composition -- despite no visual cues in the prompt. We term this phenomenon phonetic-to-visual regurgitation. Together, these findings expose a critical vulnerability in transcript-conditioned multimodal generation: phonetic prompting alone can unlock memorized audiovisual content, raising urgent questions about copyright, safety, and content provenance in modern generative systems. Example generations are available on our demo page (jrohsc.github.io/music_attack/).
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures</title>
<link>https://arxiv.org/abs/2507.18009</link>
<guid>https://arxiv.org/abs/2507.18009</guid>
<content:encoded><![CDATA[
arXiv:2507.18009v1 Announce Type: cross 
Abstract: State-of-the-art (SOTA) image and text generation models are multimodal models that have many similarities to large language models (LLMs). Despite achieving strong performances, leading foundational multimodal model architectures frequently lag behind the architectural sophistication of contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner (CoCa) model that incorporates Gaussian error gated linear units, root mean squared normalization, and rotary positional embedding into the textual decoders and the vision transformer (ViT) encoder. Each architectural modification has been shown to improve model performance in LLMs, but has yet to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model with the same modified textual decoders but with CoCa's original ViT encoder. We used standard pretraining and fine-tuning workflows to benchmark the models on contrastive and generative tasks. Our GRR-CoCa significantly outperformed Baseline CoCa on the pretraining dataset and three diverse fine-tuning datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were 13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We show that GRR-CoCa's modified architecture improves performance and generalization across vision-language domains.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RECALLED: An Unbounded Resource Consumption Attack on Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2507.18053</link>
<guid>https://arxiv.org/abs/2507.18053</guid>
<content:encoded><![CDATA[
arXiv:2507.18053v1 Announce Type: cross 
Abstract: Resource Consumption Attacks (RCAs) have emerged as a significant threat to the deployment of Large Language Models (LLMs). With the integration of vision modalities, additional attack vectors exacerbate the risk of RCAs in large vision-language models (LVLMs). However, existing red-teaming studies have largely overlooked visual inputs as a potential attack surface, resulting in insufficient mitigation strategies against RCAs in LVLMs. To address this gap, we propose RECALLED (\textbf{RE}source \textbf{C}onsumption \textbf{A}ttack on \textbf{L}arge Vision-\textbf{L}anguag\textbf{E} Mo\textbf{D}els), the first approach for exploiting visual modalities to trigger unbounded RCAs red-teaming. First, we present \textit{Vision Guided Optimization}, a fine-grained pixel-level optimization, to obtain \textit{Output Recall} adversarial perturbations, which can induce repeating output. Then, we inject the perturbations into visual inputs, triggering unbounded generations to achieve the goal of RCAs. Additionally, we introduce \textit{Multi-Objective Parallel Losses} to generate universal attack templates and resolve optimization conflicts when intending to implement parallel attacks. Empirical results demonstrate that RECALLED increases service response latency by over 26 $\uparrow$, resulting in an additional 20\% increase in GPU utilization and memory consumption. Our study exposes security vulnerabilities in LVLMs and establishes a red-teaming framework that can facilitate future defense development against RCAs.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group Sequence Policy Optimization</title>
<link>https://arxiv.org/abs/2507.18071</link>
<guid>https://arxiv.org/abs/2507.18071</guid>
<content:encoded><![CDATA[
arXiv:2507.18071v1 Announce Type: cross 
Abstract: This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI framework for End-to-End Medical Data Inference</title>
<link>https://arxiv.org/abs/2507.18115</link>
<guid>https://arxiv.org/abs/2507.18115</guid>
<content:encoded><![CDATA[
arXiv:2507.18115v1 Announce Type: cross 
Abstract: Building and deploying machine learning solutions in healthcare remains expensive and labor-intensive due to fragmented preprocessing workflows, model compatibility issues, and stringent data privacy constraints. In this work, we introduce an Agentic AI framework that automates the entire clinical data pipeline, from ingestion to inference, through a system of modular, task-specific agents. These agents handle both structured and unstructured data, enabling automatic feature selection, model selection, and preprocessing recommendation without manual intervention. We evaluate the system on publicly available datasets from geriatrics, palliative care, and colonoscopy imaging. For example, in the case of structured data (anxiety data) and unstructured data (colonoscopy polyps data), the pipeline begins with file-type detection by the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring privacy compliance, where we first identify the data type and then anonymize it. The Feature Extraction Agent identifies features using an embedding-based approach for tabular data, extracting all column names, and a multi-stage MedGemma-based approach for image data, which infers modality and disease name. These features guide the Model-Data Feature Matcher Agent in selecting the best-fit model from a curated repository. The Preprocessing Recommender Agent and Preprocessing Implementor Agent then apply tailored preprocessing based on data type and model requirements. Finally, the ``Model Inference Agent" runs the selected model on the uploaded data and generates interpretable outputs using tools like SHAP, LIME, and DETR attention maps. By automating these high-friction stages of the ML lifecycle, the proposed framework reduces the need for repeated expert intervention, offering a scalable, cost-efficient pathway for operationalizing AI in clinical environments.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recent Trends in Distant Conversational Speech Recognition: A Review of CHiME-7 and 8 DASR Challenges</title>
<link>https://arxiv.org/abs/2507.18161</link>
<guid>https://arxiv.org/abs/2507.18161</guid>
<content:encoded><![CDATA[
arXiv:2507.18161v1 Announce Type: cross 
Abstract: The CHiME-7 and 8 distant speech recognition (DASR) challenges focus on multi-channel, generalizable, joint automatic speech recognition (ASR) and diarization of conversational speech. With participation from 9 teams submitting 32 diverse systems, these challenges have contributed to state-of-the-art research in the field. This paper outlines the challenges' design, evaluation metrics, datasets, and baseline systems while analyzing key trends from participant submissions. From this analysis it emerges that: 1) Most participants use end-to-end (e2e) ASR systems, whereas hybrid systems were prevalent in previous CHiME challenges. This transition is mainly due to the availability of robust large-scale pre-trained models, which lowers the data burden for e2e-ASR. 2) Despite recent advances in neural speech separation and enhancement (SSE), all teams still heavily rely on guided source separation, suggesting that current neural SSE techniques are still unable to reliably deal with complex scenarios and different recording setups. 3) All best systems employ diarization refinement via target-speaker diarization techniques. Accurate speaker counting in the first diarization pass is thus crucial to avoid compounding errors and CHiME-8 DASR participants especially focused on this part. 4) Downstream evaluation via meeting summarization can correlate weakly with transcription quality due to the remarkable effectiveness of large-language models in handling errors. On the NOTSOFAR-1 scenario, even systems with over 50\% time-constrained minimum permutation WER can perform roughly on par with the most effective ones (around 11\%). 5) Despite recent progress, accurately transcribing spontaneous speech in challenging acoustic environments remains difficult, even when using computationally intensive system ensembles.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models</title>
<link>https://arxiv.org/abs/2507.18302</link>
<guid>https://arxiv.org/abs/2507.18302</guid>
<content:encoded><![CDATA[
arXiv:2507.18302v1 Announce Type: cross 
Abstract: Language Models (LMs) typically adhere to a "pre-training and fine-tuning" paradigm, where a universal pre-trained model can be fine-tuned to cater to various specialized domains. Low-Rank Adaptation (LoRA) has gained the most widespread use in LM fine-tuning due to its lightweight computational cost and remarkable performance. Because the proportion of parameters tuned by LoRA is relatively small, there might be a misleading impression that the LoRA fine-tuning data is invulnerable to Membership Inference Attacks (MIAs). However, we identify that utilizing the pre-trained model can induce more information leakage, which is neglected by existing MIAs. Therefore, we introduce LoRA-Leak, a holistic evaluation framework for MIAs against the fine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership inference attacks, including ten existing MIAs, and five improved MIAs that leverage the pre-trained model as a reference. In experiments, we apply LoRA-Leak to three advanced LMs across three popular natural language processing tasks, demonstrating that LoRA-based fine-tuned LMs are still vulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings). We also applied LoRA-Leak to different fine-tuning settings to understand the resulting privacy risks. We further explore four defenses and find that only dropout and excluding specific LM layers during fine-tuning effectively mitigate MIA risks while maintaining utility. We highlight that under the "pre-training and fine-tuning" paradigm, the existence of the pre-trained model makes MIA a more severe risk for LoRA-based LMs. We hope that our findings can provide guidance on data privacy protection for specialized LM providers.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Embedders for Prior Case Retrieval</title>
<link>https://arxiv.org/abs/2507.18455</link>
<guid>https://arxiv.org/abs/2507.18455</guid>
<content:encoded><![CDATA[
arXiv:2507.18455v1 Announce Type: cross 
Abstract: In common law systems, legal professionals such as lawyers and judges rely on precedents to build their arguments. As the volume of cases has grown massively over time, effectively retrieving prior cases has become essential. Prior case retrieval (PCR) is an information retrieval (IR) task that aims to automatically identify the most relevant court cases for a specific query from a large pool of potential candidates. While IR methods have seen several paradigm shifts over the last few years, the vast majority of PCR methods continue to rely on traditional IR methods, such as BM25. The state-of-the-art deep learning IR methods have not been successful in PCR due to two key challenges: i. Lengthy legal text limitation; when using the powerful BERT-based transformer models, there is a limit of input text lengths, which inevitably requires to shorten the input via truncation or division with a loss of legal context information. ii. Lack of legal training data; due to data privacy concerns, available PCR datasets are often limited in size, making it difficult to train deep learning-based models effectively. In this research, we address these challenges by leveraging LLM-based text embedders in PCR. LLM-based embedders support longer input lengths, and since we use them in an unsupervised manner, they do not require training data, addressing both challenges simultaneously. In this paper, we evaluate state-of-the-art LLM-based text embedders in four PCR benchmark datasets and show that they outperform BM25 and supervised transformer-based models.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PosterMate: Audience-driven Collaborative Persona Agents for Poster Design</title>
<link>https://arxiv.org/abs/2507.18572</link>
<guid>https://arxiv.org/abs/2507.18572</guid>
<content:encoded><![CDATA[
arXiv:2507.18572v1 Announce Type: cross 
Abstract: Poster designing can benefit from synchronous feedback from target audiences. However, gathering audiences with diverse perspectives and reconciling them on design edits can be challenging. Recent generative AI models present opportunities to simulate human-like interactions, but it is unclear how they may be used for feedback processes in design. We introduce PosterMate, a poster design assistant that facilitates collaboration by creating audience-driven persona agents constructed from marketing documents. PosterMate gathers feedback from each persona agent regarding poster components, and stimulates discussion with the help of a moderator to reach a conclusion. These agreed-upon edits can then be directly integrated into the poster design. Through our user study (N=12), we identified the potential of PosterMate to capture overlooked viewpoints, while serving as an effective prototyping tool. Additionally, our controlled online evaluation (N=100) revealed that the feedback from an individual persona agent is appropriate given its persona identity, and the discussion effectively synthesizes the different persona agents' perspectives.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\circ}$ Law</title>
<link>https://arxiv.org/abs/2507.18576</link>
<guid>https://arxiv.org/abs/2507.18576</guid>
<content:encoded><![CDATA[
arXiv:2507.18576v1 Announce Type: cross 
Abstract: We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that demonstrates the coevolution of capabilities and safety. It is developed by our proposed SafeLadder framework, which incorporates large-scale, progressive, safety-oriented reinforcement learning post-training, supported by a suite of multi-principled verifiers. Unlike previous alignment methods such as RLHF that simply learn human preferences, SafeLadder enables SafeWork-R1 to develop intrinsic safety reasoning and self-reflection abilities, giving rise to safety `aha' moments. Notably, SafeWork-R1 achieves an average improvement of $46.54\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks without compromising general capabilities, and delivers state-of-the-art safety performance compared to leading proprietary models such as GPT-4.1 and Claude Opus 4. To further bolster its reliability, we implement two distinct inference-time intervention methods and a deliberative search mechanism, enforcing step-level verification. Finally, we further develop SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and capability can co-evolve synergistically, highlighting the generalizability of our framework in building robust, reliable, and trustworthy general-purpose AI.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge Injection and Synthetic Data</title>
<link>https://arxiv.org/abs/2507.18583</link>
<guid>https://arxiv.org/abs/2507.18583</guid>
<content:encoded><![CDATA[
arXiv:2507.18583v1 Announce Type: cross 
Abstract: Electronic Health Records (EHRs) are pivotal in clinical practices, yet their retrieval remains a challenge mainly due to semantic gap issues. Recent advancements in dense retrieval offer promising solutions but existing models, both general-domain and biomedical-domain, fall short due to insufficient medical knowledge or mismatched training corpora. This paper introduces \texttt{DR.EHR}, a series of dense retrieval models specifically tailored for EHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV discharge summaries to address the need for extensive medical knowledge and large-scale training data. The first stage involves medical entity extraction and knowledge injection from a biomedical knowledge graph, while the second stage employs large language models to generate diverse training data. We train two variants of \texttt{DR.EHR}, with 110M and 7B parameters, respectively. Evaluated on the CliniQ benchmark, our models significantly outperforms all existing dense retrievers, achieving state-of-the-art results. Detailed analyses confirm our models' superiority across various match and query types, particularly in challenging semantic matches like implication and abbreviation. Ablation studies validate the effectiveness of each pipeline component, and supplementary experiments on EHR QA datasets demonstrate the models' generalizability on natural language questions, including complex ones with multiple entities. This work significantly advances EHR retrieval, offering a robust solution for clinical applications.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning</title>
<link>https://arxiv.org/abs/2507.18616</link>
<guid>https://arxiv.org/abs/2507.18616</guid>
<content:encoded><![CDATA[
arXiv:2507.18616v1 Announce Type: cross 
Abstract: Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets generated by text-to-image (T2I) models to mitigate the need for costly manual annotation. However, these T2I models often produce images that exhibit semantic misalignments with their corresponding input captions (e.g., missing objects, incorrect attributes), resulting in noisy synthetic image-caption pairs that can hinder model training. Existing dataset pruning techniques are largely designed for removing noisy text in web-crawled data. However, these methods are ill-suited for the distinct challenges of synthetic data, where captions are typically well-formed, but images may be inaccurate representations. To address this gap, we introduce SynC, a novel framework specifically designed to refine synthetic image-caption datasets for ZIC. Instead of conventional filtering or regeneration, SynC focuses on reassigning captions to the most semantically aligned images already present within the synthetic image pool. Our approach employs a one-to-many mapping strategy by initially retrieving multiple relevant candidate images for each caption. We then apply a cycle-consistency-inspired alignment scorer that selects the best image by verifying its ability to retrieve the original caption via image-to-text retrieval. Extensive evaluations demonstrate that SynC consistently and significantly improves performance across various ZIC models on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art results in several scenarios. SynC offers an effective strategy for curating refined synthetic data to enhance ZIC.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias</title>
<link>https://arxiv.org/abs/2212.10678</link>
<guid>https://arxiv.org/abs/2212.10678</guid>
<content:encoded><![CDATA[
arXiv:2212.10678v4 Announce Type: replace 
Abstract: Generated texts from large language models (LLMs) have been shown to exhibit a variety of harmful, human-like biases against various demographics. These findings motivate research efforts aiming to understand and measure such effects. This paper introduces a causal formulation for bias measurement in generative language models. Based on this theoretical foundation, we outline a list of desiderata for designing robust bias benchmarks. We then propose a benchmark called OccuGender, with a bias-measuring procedure to investigate occupational gender bias. We test several state-of-the-art open-source LLMs on OccuGender, including Llama, Mistral, and their instruction-tuned versions. The results show that these models exhibit substantial occupational gender bias. Lastly, we discuss prompting strategies for bias mitigation and an extension of our causal formulation to illustrate the generalizability of our framework. Our code and data https://github.com/chenyuen0103/gender-bias.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocTER: Evaluating Document-based Knowledge Editing</title>
<link>https://arxiv.org/abs/2308.09954</link>
<guid>https://arxiv.org/abs/2308.09954</guid>
<content:encoded><![CDATA[
arXiv:2308.09954v2 Announce Type: replace 
Abstract: Knowledge editing aims to correct outdated or inaccurate knowledge in neural networks. In this paper, we explore knowledge editing using easily accessible documents instead of manually labeled factual triples employed in earlier research. To advance this field, we establish the first evaluation benchmark, \textit{DocTER}, featuring Documents containing counterfactual knowledge for editing. A comprehensive four-perspective evaluation is introduced: Edit Success, Locality, Reasoning, and Cross-lingual Transfer. To adapt conventional triplet-based knowledge editing methods for this task, we develop an Extract-then-Edit pipeline that extracts triples from documents before applying existing methods. Experiments on popular knowledge editing methods demonstrate that editing with documents presents significantly greater challenges than using triples. In document-based scenarios, even the best-performing in-context editing approach still lags behind by 10 points in editing success when compared to using gold triples. This observation also holds for both reasoning and cross-lingual test sets. We further analyze key factors influencing task performance, including the quality of extracted triples, the frequency and position of edited knowledge in documents, various methods for enhancing reasoning, and performance differences across various directions in cross-lingual knowledge editing, which provide valuable insights for future research.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying the Uniqueness and Divisiveness of Presidential Discourse</title>
<link>https://arxiv.org/abs/2401.01405</link>
<guid>https://arxiv.org/abs/2401.01405</guid>
<content:encoded><![CDATA[
arXiv:2401.01405v2 Announce Type: replace 
Abstract: Do American presidents speak discernibly different from each other? If so, in what ways? And are these differences confined to any single medium of communication? To investigate these questions, this paper introduces a novel metric of uniqueness based on large language models, develops a new lexicon for divisive speech, and presents a framework for assessing the distinctive ways in which presidents speak about their political opponents. Applying these tools to a variety of corpora of presidential speeches, we find considerable evidence that Donald Trump's speech patterns diverge from those of all major party nominees for the presidency in recent history. Trump is significantly more distinctive than his fellow Republicans, whose uniqueness values appear closer to those of the Democrats. Contributing to these differences is Trump's employment of divisive and antagonistic language, particularly when targeting his political opponents. These differences hold across a variety of measurement strategies, arise on both the campaign trail and in official presidential addresses, and do not appear to be an artifact of secular changes in presidential communications.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts</title>
<link>https://arxiv.org/abs/2406.12548</link>
<guid>https://arxiv.org/abs/2406.12548</guid>
<content:encoded><![CDATA[
arXiv:2406.12548v3 Announce Type: replace 
Abstract: Personalized large language models (LLMs) have attracted great attention in many applications, such as emotional support and role-playing. However, existing works primarily focus on modeling explicit character profiles, while ignoring the underlying personality traits that truly shape behaviors and decision-making, hampering the development of more anthropomorphic and psychologically-grounded AI systems. In this paper, we explore the modeling of Big Five personality traits, which is the most widely used trait theory in psychology, and propose P-React, a mixture of experts (MoE)-based personalized LLM. Particularly, we integrate a Personality Specialization Loss (PSL) to better capture individual trait expressions, providing a more nuanced and psychologically grounded personality simulacrum. To facilitate research in this field, we curate OCEAN-Chat, a high-quality, human-verified dataset designed to train LLMs in expressing personality traits across diverse topics. Extensive experiments demonstrate the effectiveness of P-React in maintaining consistent and real personality.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks</title>
<link>https://arxiv.org/abs/2407.19795</link>
<guid>https://arxiv.org/abs/2407.19795</guid>
<content:encoded><![CDATA[
arXiv:2407.19795v2 Announce Type: replace 
Abstract: Domain generalizability is a crucial aspect of a deep learning model since it determines the capability of the model to perform well on data from unseen domains. However, research on the domain generalizability of deep learning models for vision-language tasks remains limited, primarily because of the lack of required datasets. To address these challenges, we propose VolDoGer: Vision-Language Dataset for Domain Generalization, a dedicated dataset designed for domain generalization that addresses three vision-language tasks: image captioning, visual question answering, and visual entailment. We constructed VolDoGer by extending LLM-based data annotation techniques to vision-language tasks, thereby alleviating the burden of recruiting human annotators. We evaluated the domain generalizability of various models, ranging from fine-tuned models to a recent multimodal large language model, through VolDoGer.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identity-related Speech Suppression in Generative AI Content Moderation</title>
<link>https://arxiv.org/abs/2409.13725</link>
<guid>https://arxiv.org/abs/2409.13725</guid>
<content:encoded><![CDATA[
arXiv:2409.13725v3 Announce Type: replace 
Abstract: Automated content moderation has long been used to help identify and filter undesired user-generated content online. But such systems have a history of incorrectly flagging content by and about marginalized identities for removal. Generative AI systems now use such filters to keep undesired generated content from being created by or shown to users. While a lot of focus has been given to making sure such systems do not produce undesired outcomes, considerably less attention has been paid to making sure appropriate text can be generated. From classrooms to Hollywood, as generative AI is increasingly used for creative or expressive text generation, whose stories will these technologies allow to be told, and whose will they suppress?
  In this paper, we define and introduce measures of speech suppression, focusing on speech related to different identity groups incorrectly filtered by a range of content moderation APIs. Using both short-form, user-generated datasets traditional in content moderation and longer generative AI-focused data, including two datasets we introduce in this work, we create a benchmark for measurement of speech suppression for nine identity groups. Across one traditional and four generative AI-focused automated content moderation services tested, we find that identity-related speech is more likely to be incorrectly suppressed than other speech. We find that reasons for incorrect flagging behavior vary by identity based on stereotypes and text associations, with, e.g., disability-related content more likely to be flagged for self-harm or health-related reasons while non-Christian content is more likely to be flagged as violent or hateful. As generative AI systems are increasingly used for creative work, we urge further attention to how this may impact the creation of identity-related content.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIFBench: Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios</title>
<link>https://arxiv.org/abs/2411.07037</link>
<guid>https://arxiv.org/abs/2411.07037</guid>
<content:encoded><![CDATA[
arXiv:2411.07037v3 Announce Type: replace 
Abstract: As Large Language Models (LLMs) evolve in natural language processing (NLP), their ability to stably follow instructions in long-context inputs has become critical for real-world applications. However, existing benchmarks seldom focus on instruction-following in long-context scenarios or stability on different inputs. To bridge this gap, we introduce LIFBench, a scalable dataset designed to evaluate LLMs' instruction-following capabilities and stability across long contexts. LIFBench comprises three long-context scenarios and eleven diverse tasks, featuring 2,766 instructions generated through an automated expansion method across three dimensions: length, expression, and variables. For evaluation, we propose LIFEval, a rubric-based assessment method that enables precise, automated scoring of complex LLM responses without reliance on LLM-assisted assessments or human judgment. This method allows for a comprehensive analysis of model performance and stability from multiple perspectives. We conduct detailed experiments on 20 prominent LLMs across six length intervals. Our work contributes LIFBench and LIFEval as robust tools for assessing LLM performance in complex and long-context settings, offering valuable insights to guide future advancements in LLM development.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlockDialect: Block-wise Fine-grained Mixed Format Quantization for Energy-Efficient LLM Inference</title>
<link>https://arxiv.org/abs/2501.01144</link>
<guid>https://arxiv.org/abs/2501.01144</guid>
<content:encoded><![CDATA[
arXiv:2501.01144v5 Announce Type: replace 
Abstract: The rapidly increasing size of large language models (LLMs) presents significant challenges in memory usage and computational costs. Quantizing both weights and activations can address these issues, with hardware-supported fine-grained scaling emerging as a promising solution to mitigate outliers. However, existing methods struggle to capture nuanced block data distributions. We propose BlockDialect, a block-wise fine-grained mixed format technique that assigns a per-block optimal number format from a formatbook for better data representation. Additionally, we introduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that adapt to diverse data distributions. To leverage this efficiently, we propose a two-stage approach for online DialectFP4 activation quantization. Importantly, DialectFP4 ensures energy efficiency by selecting representable values as scaled integers compatible with low-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit usage per data, while being only 5.45% (2.69%) below full precision even when quantizing full-path matrix multiplication. Focusing on how to represent over how to scale, our work presents a promising path for energy-efficient LLM inference.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Alignment as Retriever Optimization: An Information Retrieval Perspective</title>
<link>https://arxiv.org/abs/2502.03699</link>
<guid>https://arxiv.org/abs/2502.03699</guid>
<content:encoded><![CDATA[
arXiv:2502.03699v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have revolutionized artificial intelligence with capabilities in reasoning, coding, and communication, driving innovation across industries. Their true potential depends on effective alignment to ensure correct, trustworthy and ethical behavior, addressing challenges like misinformation, hallucinations, bias and misuse. While existing Reinforcement Learning (RL)-based alignment methods are notoriously complex, direct optimization approaches offer a simpler alternative. In this work, we introduce a novel direct optimization approach for LLM alignment by drawing on established Information Retrieval (IR) principles. We present a systematic framework that bridges LLM alignment and IR methodologies, mapping LLM generation and reward models to IR's retriever-reranker paradigm. Building on this foundation, we propose LLM Alignment as Retriever Preference Optimization (LarPO), a new alignment method that enhances overall alignment quality. Extensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 % averaged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work opens new avenues for advancing LLM alignment by integrating IR foundations, offering a promising direction for future research.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2502.15487</link>
<guid>https://arxiv.org/abs/2502.15487</guid>
<content:encoded><![CDATA[
arXiv:2502.15487v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly used in tasks requiring interpretive and inferential accuracy. In this paper, we introduce ExpliCa, a new dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely integrates both causal and temporal relations presented in different linguistic orders and explicitly expressed by linguistic connectives. The dataset is enriched with crowdsourced human acceptability ratings. We tested LLMs on ExpliCa through prompting and perplexity-based metrics. We assessed seven commercial and open-source LLMs, revealing that even top models struggle to reach 0.80 accuracy. Interestingly, models tend to confound temporal relations with causal ones, and their performance is also strongly influenced by the linguistic order of the events. Finally, perplexity-based scores and prompting performance are differently affected by model size.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data</title>
<link>https://arxiv.org/abs/2502.18679</link>
<guid>https://arxiv.org/abs/2502.18679</guid>
<content:encoded><![CDATA[
arXiv:2502.18679v3 Announce Type: replace 
Abstract: Supervised fine-tuning (SFT) has become a crucial step for aligning pretrained large language models (LLMs) using supervised datasets of input-output pairs. However, despite being supervised, SFT is inherently limited by its generative training objective. To address its limitations, the existing common strategy is to follow SFT with a separate phase of preference optimization (PO), which relies on either human-labeled preference data or a strong reward model to guide the learning process. In this paper, we address the limitations of SFT by exploring one of the most successful techniques in conventional supervised learning: discriminative learning. We introduce Discriminative Fine-Tuning (DFT), an improved variant of SFT, which mitigates the burden of collecting human-labeled preference data or training strong reward models. Unlike SFT that employs a generative approach and overlooks negative data, DFT adopts a discriminative paradigm that increases the probability of positive answers while suppressing potentially negative ones, aiming for data prediction instead of token prediction. Our contributions include: (i) a discriminative probabilistic framework for fine-tuning LLMs by explicitly modeling the discriminative likelihood of an answer among all possible outputs given an input; (ii) efficient algorithms to optimize this discriminative likelihood; and (iii) extensive experiments demonstrating DFT's effectiveness, achieving performance better than SFT and comparable to if not better than SFT$\rightarrow$PO. The code can be found at https://github.com/Optimization-AI/DFT.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How do language models learn facts? Dynamics, curricula and hallucinations</title>
<link>https://arxiv.org/abs/2503.21676</link>
<guid>https://arxiv.org/abs/2503.21676</guid>
<content:encoded><![CDATA[
arXiv:2503.21676v2 Announce Type: replace 
Abstract: Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories. Our results emphasize the importance of data distribution in knowledge acquisition and suggest novel data scheduling strategies to accelerate neural network training.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting individual differences to bootstrap communication</title>
<link>https://arxiv.org/abs/2504.05211</link>
<guid>https://arxiv.org/abs/2504.05211</guid>
<content:encoded><![CDATA[
arXiv:2504.05211v2 Announce Type: replace 
Abstract: Establishing a communication system is hard because the intended meaning of a signal is unknown to its receiver when first produced, and the signaller also has no idea how that signal will be interpreted. Most theoretical accounts of the emergence of communication systems rely on feedback to reinforce behaviours that have led to successful communication in the past. However, providing such feedback requires already being able to communicate the meaning that was intended or interpreted. Therefore these accounts cannot explain how communication can be bootstrapped from non-communicative behaviours. Here we present a model that shows how a communication system, capable of expressing an unbounded number of meanings, can emerge as a result of individual behavioural differences in a large population without any pre-existing means to determine communicative success. The two key cognitive capabilities responsible for this outcome are behaving predictably in a given situation, and an alignment of psychological states ahead of signal production that derives from shared intentionality. Since both capabilities can exist independently of communication, our results are compatible with theories in which large flexible socially-learned communication systems like language are the product of a general but well-developed capacity for social cognition.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge</title>
<link>https://arxiv.org/abs/2505.20658</link>
<guid>https://arxiv.org/abs/2505.20658</guid>
<content:encoded><![CDATA[
arXiv:2505.20658v2 Announce Type: replace 
Abstract: Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise formal specification, making it widely used in cyber-physical systems such as autonomous driving and robotics. Automatically transforming NL into STL is an attractive approach to overcome the limitations of manual transformation, which is time-consuming and error-prone. However, due to the lack of datasets, automatic transformation currently faces significant challenges and has not been fully explored. In this paper, we propose an NL-STL dataset named STL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched with diverse patterns. To develop the dataset, we first manually create a small-scale seed set of NL-STL pairs. Next, representative examples are identified through clustering and used to guide large language models (LLMs) in generating additional NL-STL pairs. Finally, diversity and accuracy are ensured through rigorous rule-based filters and human validation. Furthermore, we introduce the Knowledge-Guided STL Transformation (KGST) framework, a novel approach for transforming natural language into STL, involving a generate-then-refine process based on external knowledge. Statistical analysis shows that the STL-DivEn dataset exhibits more diversity than the existing NL-STL dataset. Moreover, both metric-based and human evaluations indicate that our KGST approach outperforms baseline models in transformation accuracy on STL-DivEn and DeepSTL datasets.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation</title>
<link>https://arxiv.org/abs/2506.05606</link>
<guid>https://arxiv.org/abs/2506.05606</guid>
<content:encoded><![CDATA[
arXiv:2506.05606v4 Announce Type: replace 
Abstract: Can large language models (LLMs) accurately simulate the next web action of a specific user? While LLMs have shown promising capabilities in generating ``believable'' human behaviors, evaluating their ability to mimic real user behaviors remains an open challenge, largely due to the lack of high-quality, publicly available datasets that capture both the observable actions and the internal reasoning of an actual human user. To address this gap, we introduce OPERA, a novel dataset of Observation, Persona, Rationale, and Action collected from real human participants during online shopping sessions. OPERA is the first public dataset that comprehensively captures: user personas, browser observations, fine-grained web actions, and self-reported just-in-time rationales. We developed both an online questionnaire and a custom browser plugin to gather this dataset with high fidelity. Using OPERA, we establish the first benchmark to evaluate how well current LLMs can predict a specific user's next action and rationale with a given persona and  history. This dataset lays the groundwork for future research into LLM agents that aim to act as personalized digital twins for human.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Barriers: Do Reinforcement Post Training Gains Transfer To Unseen Domains?</title>
<link>https://arxiv.org/abs/2506.19733</link>
<guid>https://arxiv.org/abs/2506.19733</guid>
<content:encoded><![CDATA[
arXiv:2506.19733v2 Announce Type: replace 
Abstract: Reinforcement post training (RPT) has recently shown promise in improving the reasoning abilities of large language models (LLMs). However, it remains unclear how well these improvements generalize to new domains, as prior work evaluates RPT models on data from the same domains used for fine-tuning. To understand the generalizability of RPT, we conduct two studies. (1) Observational: We compare a wide range of open-weight RPT models against their corresponding base models across multiple domains, including both seen and unseen domains in their fine-tuning data. (2) Interventional: we fine-tune LLMs with RPT on single domains and evaluate their performance across multiple domains. Both studies converge on the same conclusion that, although RPT brings substantial gains on tasks similar to the fine-tuning data, the gains generalize inconsistently and can vanish on domains with different reasoning patterns.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Faceted Evaluation Framework for Assessing Synthetic Data Generated by Large Language Models</title>
<link>https://arxiv.org/abs/2404.14445</link>
<guid>https://arxiv.org/abs/2404.14445</guid>
<content:encoded><![CDATA[
arXiv:2404.14445v2 Announce Type: replace-cross 
Abstract: The rapid advancements in generative AI and large language models (LLMs) have opened up new avenues for producing synthetic data, particularly in the realm of structured tabular formats, such as product reviews. Despite the potential benefits, concerns regarding privacy leakage have surfaced, especially when personal information is utilized in the training datasets. In addition, there is an absence of a comprehensive evaluation framework capable of quantitatively measuring the quality of the generated synthetic data and their utility for downstream tasks. In response to this gap, we introduce SynEval, an open-source evaluation framework designed to assess the fidelity, utility, and privacy preservation of synthetically generated tabular data via a suite of diverse evaluation metrics. We validate the efficacy of our proposed framework - SynEval - by applying it to synthetic product review data generated by three state-of-the-art LLMs: ChatGPT, Claude, and Llama. Our experimental findings illuminate the trade-offs between various evaluation metrics in the context of synthetic data generation. Furthermore, SynEval stands as a critical instrument for researchers and practitioners engaged with synthetic tabular data,, empowering them to judiciously determine the suitability of the generated data for their specific applications, with an emphasis on upholding user privacy.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Fairness of Computer Vision and Natural Language Processing Models</title>
<link>https://arxiv.org/abs/2412.09900</link>
<guid>https://arxiv.org/abs/2412.09900</guid>
<content:encoded><![CDATA[
arXiv:2412.09900v3 Announce Type: replace-cross 
Abstract: Machine learning (ML) algorithms play a critical role in decision-making across various domains, such as healthcare, finance, education, and law enforcement. However, concerns about fairness and bias in these systems have raised significant ethical and social challenges. To address these challenges, this research utilizes two prominent fairness libraries, Fairlearn by Microsoft and AIF360 by IBM. These libraries offer comprehensive frameworks for fairness analysis, providing tools to evaluate fairness metrics, visualize results, and implement bias mitigation algorithms. The study focuses on assessing and mitigating biases for unstructured datasets using Computer Vision (CV) and Natural Language Processing (NLP) models. The primary objective is to present a comparative analysis of the performance of mitigation algorithms from the two fairness libraries. This analysis involves applying the algorithms individually, one at a time, in one of the stages of the ML lifecycle, pre-processing, in-processing, or post-processing, as well as sequentially across more than one stage. The results reveal that some sequential applications improve the performance of mitigation algorithms by effectively reducing bias while maintaining the model's performance. Publicly available datasets from Kaggle were chosen for this research, providing a practical context for evaluating fairness in real-world machine learning workflows.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts</title>
<link>https://arxiv.org/abs/2412.10510</link>
<guid>https://arxiv.org/abs/2412.10510</guid>
<content:encoded><![CDATA[
arXiv:2412.10510v4 Announce Type: replace-cross 
Abstract: The proliferation of disinformation demands reliable and scalable fact-checking solutions. We present Dynamic Evidence-based FAct-checking with Multimodal Experts (DEFAME), a modular, zero-shot MLLM pipeline for open-domain, text-image claim verification. DEFAME operates in a six-stage process, dynamically selecting the tools and search depth to extract and evaluate textual and visual evidence. Unlike prior approaches that are text-only, lack explainability, or rely solely on parametric knowledge, DEFAME performs end-to-end verification, accounting for images in claims and evidence while generating structured, multimodal reports. Evaluation on the popular benchmarks VERITE, AVerITeC, and MOCHEG shows that DEFAME surpasses all previous methods, establishing itself as the new state-of-the-art fact-checking system for uni- and multimodal fact-checking. Moreover, we introduce a new multimodal benchmark, ClaimReview2024+, featuring claims after the knowledge cutoff of GPT-4o, avoiding data leakage. Here, DEFAME drastically outperforms the GPT-4o baselines, showing temporal generalizability and the potential for real-time fact-checking.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark</title>
<link>https://arxiv.org/abs/2412.13102</link>
<guid>https://arxiv.org/abs/2412.13102</guid>
<content:encoded><![CDATA[
arXiv:2412.13102v4 Announce Type: replace-cross 
Abstract: Evaluation plays a crucial role in the advancement of information retrieval (IR) models. However, current benchmarks, which are based on predefined domains and human-labeled data, face limitations in addressing evaluation needs for emerging domains both cost-effectively and efficiently. To address this challenge, we propose the Automated Heterogeneous Information Retrieval Benchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1) Automated. The testing data in AIR-Bench is automatically generated by large language models (LLMs) without human intervention. 2) Heterogeneous. The testing data in AIR-Bench is generated with respect to diverse tasks, domains and languages. 3) Dynamic. The domains and languages covered by AIR-Bench are constantly augmented to provide an increasingly comprehensive evaluation benchmark for community developers. We develop a reliable and robust data generation pipeline to automatically create diverse and high-quality evaluation datasets based on real-world corpora. Our findings demonstrate that the generated testing data in AIR-Bench aligns well with human-labeled testing data, making AIR-Bench a dependable benchmark for evaluating IR models. The resources in AIR-Bench are publicly available at https://github.com/AIR-Bench/AIR-Bench.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELITE: Enhanced Language-Image Toxicity Evaluation for Safety</title>
<link>https://arxiv.org/abs/2502.04757</link>
<guid>https://arxiv.org/abs/2502.04757</guid>
<content:encoded><![CDATA[
arXiv:2502.04757v3 Announce Type: replace-cross 
Abstract: Current Vision Language Models (VLMs) remain vulnerable to malicious prompts that induce harmful outputs. Existing safety benchmarks for VLMs primarily rely on automated evaluation methods, but these methods struggle to detect implicit harmful content or produce inaccurate evaluations. Therefore, we found that existing benchmarks have low levels of harmfulness, ambiguous data, and limited diversity in image-text pair combinations. To address these issues, we propose the ELITE benchmark, a high-quality safety evaluation benchmark for VLMs, underpinned by our enhanced evaluation method, the ELITE evaluator. The ELITE evaluator explicitly incorporates a toxicity score to accurately assess harmfulness in multimodal contexts, where VLMs often provide specific, convincing, but unharmful descriptions of images. We filter out ambiguous and low-quality image-text pairs from existing benchmarks using the ELITE evaluator and generate diverse combinations of safe and unsafe image-text pairs. Our experiments demonstrate that the ELITE evaluator achieves superior alignment with human evaluations compared to prior automated methods, and the ELITE benchmark offers enhanced benchmark quality and diversity. By introducing ELITE, we pave the way for safer, more robust VLMs, contributing essential tools for evaluating and mitigating safety risks in real-world applications.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IPCGRL: Language-Instructed Reinforcement Learning for Procedural Level Generation</title>
<link>https://arxiv.org/abs/2503.12358</link>
<guid>https://arxiv.org/abs/2503.12358</guid>
<content:encoded><![CDATA[
arXiv:2503.12358v4 Announce Type: replace-cross 
Abstract: Recent research has highlighted the significance of natural language in enhancing the controllability of generative models. While various efforts have been made to leverage natural language for content generation, research on deep reinforcement learning (DRL) agents utilizing text-based instructions for procedural content generation remains limited. In this paper, we propose IPCGRL, an instruction-based procedural content generation method via reinforcement learning, which incorporates a sentence embedding model. IPCGRL fine-tunes task-specific embedding representations to effectively compress game-level conditions. We evaluate IPCGRL in a two-dimensional level generation task and compare its performance with a general-purpose embedding method. The results indicate that IPCGRL achieves up to a 21.4% improvement in controllability and a 17.2% improvement in generalizability for unseen instructions. Furthermore, the proposed method extends the modality of conditional input, enabling a more flexible and expressive interaction framework for procedural content generation.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs</title>
<link>https://arxiv.org/abs/2503.16870</link>
<guid>https://arxiv.org/abs/2503.16870</guid>
<content:encoded><![CDATA[
arXiv:2503.16870v2 Announce Type: replace-cross 
Abstract: Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse knowledge distillation such as caching Top-K probabilities, while intuitive, provide biased estimates of teacher probability distribution to the student, resulting in suboptimal performance and calibration. We propose an importance-sampling-based method `Random Sampling Knowledge Distillation', which provides unbiased estimates, preserves the gradient in expectation, and requires storing significantly sparser logits. Our method enables faster training of student models with marginal overhead (<10%) compared to cross-entropy based training, while maintaining competitive performance compared to full distillation, across a range of model sizes from 300M to 3B.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important</title>
<link>https://arxiv.org/abs/2504.04704</link>
<guid>https://arxiv.org/abs/2504.04704</guid>
<content:encoded><![CDATA[
arXiv:2504.04704v2 Announce Type: replace-cross 
Abstract: The increasing size of the Key-Value (KV) cache during the Large Language Models long-context inference is the main obstacle for its balance between the deployment cost and task accuracy. To reduce the KV cache size in such scenarios, most previous efforts leveraged on the attention weight to evict non-critical cache tokens. But there is a trade-off in those methods, they usually require major modification of the inference infrastructure and significant computation overhead. Based on the fact that the Large Language models are autoregressive models, we propose LagKV, a KV compression strategy only relying on straight forward comparison among KV themselves. It is a totally attention free method which offers easy integration to the main stream inference platform and comparable performance comparing to other complicated KV compression methods. Results on RULER benchmark show that, our approach outperforms SnapKV and StreamingLLM in different compression ratios. Especially in the 64-digit passkey retrieval task, our method outperforms the attention weight based method $H_2O$ over $50\%$ with same compression ratios. Our code is available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework</title>
<link>https://arxiv.org/abs/2504.14928</link>
<guid>https://arxiv.org/abs/2504.14928</guid>
<content:encoded><![CDATA[
arXiv:2504.14928v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) increasingly serve as educational tools, yet evaluating their teaching capabilities remains challenging due to the resource-intensive, context-dependent, and methodologically complex nature of teacher-student interactions. We introduce EducationQ, a multi-agent dialogue framework that efficiently assesses teaching capabilities through simulated dynamic educational scenarios, featuring specialized agents for teaching, learning, and evaluation. Testing 14 LLMs across major AI Organizations (OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13 disciplines and 10 difficulty levels reveals that teaching effectiveness does not correlate linearly with model scale or general reasoning capabilities - with some smaller open-source models outperforming larger commercial counterparts in teaching contexts. This finding highlights a critical gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Our mixed-methods evaluation, combining quantitative metrics with qualitative analysis and expert case studies, identifies distinct pedagogical strengths employed by top-performing models (e.g., sophisticated questioning strategies, adaptive feedback mechanisms). Human expert evaluations show 78% agreement with our automated qualitative analysis of effective teaching behaviors, validating our methodology. EducationQ demonstrates that LLMs-as-teachers require specialized optimization beyond simple scaling, suggesting next-generation educational AI prioritize targeted enhancement of specific pedagogical effectiveness.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games</title>
<link>https://arxiv.org/abs/2506.23276</link>
<guid>https://arxiv.org/abs/2506.23276</guid>
<content:encoded><![CDATA[
arXiv:2506.23276v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are increasingly deployed as autonomous agents, understanding their cooperation and social mechanisms is becoming increasingly important. In particular, how LLMs balance self-interest and collective well-being is a critical challenge for ensuring alignment, robustness, and safe deployment. In this paper, we examine the challenge of costly sanctioning in multi-agent LLM systems, where an agent must decide whether to invest its own resources to incentivize cooperation or penalize defection. To study this, we adapt a public goods game with institutional choice from behavioral economics, allowing us to observe how different LLMs navigate social dilemmas over repeated interactions. Our analysis reveals four distinct behavioral patterns among models: some consistently establish and sustain high levels of cooperation, others fluctuate between engagement and disengagement, some gradually decline in cooperative behavior over time, and others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we find that reasoning LLMs, such as the o1 series, struggle significantly with cooperation, whereas some traditional LLMs consistently achieve high levels of cooperation. These findings suggest that the current approach to improving LLMs, which focuses on enhancing their reasoning capabilities, does not necessarily lead to cooperation, providing valuable insights for deploying LLM agents in environments that require sustained collaboration. Our code is available at https://github.com/davidguzmanp/SanctSim
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling RL to Long Videos</title>
<link>https://arxiv.org/abs/2507.07966</link>
<guid>https://arxiv.org/abs/2507.07966</guid>
<content:encoded><![CDATA[
arXiv:2507.07966v2 Announce Type: replace-cross 
Abstract: We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 104K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves strong performance on video benchmarks, reaching 65.0% and 70.7% accuracy on VideoMME without and with subtitles, respectively, and consistently outperforming LongVILA-R1 across multiple benchmarks. Moreover, LongVILA-R1 shows steady performance improvements as the number of input video frames increases. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames / around 256k tokens).
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unifying Scheme for Extractive Content Selection Tasks</title>
<link>https://arxiv.org/abs/2507.16922</link>
<guid>https://arxiv.org/abs/2507.16922</guid>
<content:encoded><![CDATA[
<div> Keywords: NLP tasks, content selection, instruction-guided framework, benchmark, transfer learning

Summary:
Instruction-guided content selection (IGCS) is proposed as a unified framework for various content selection tasks in natural language processing. The framework encapsulates task definitions and specific requests as instructions to a language model. A unified benchmark, \igcsbench{}, is introduced to cover diverse content selection tasks. A large synthetic dataset is created to aid in transfer learning for enhanced performance across different tasks. Different content selection models can benefit from dedicated training or transfer learning using these datasets. The study also addresses inference time issues in language model-based content selection and proposes a generic evaluation metric. The resources and methods presented in the study are deemed valuable for the development of future content selection models. The models and datasets are available at https://github.com/shmuelamar/igcs. 

<br /><br />Summary: <div>
arXiv:2507.16922v1 Announce Type: new 
Abstract: A broad range of NLP tasks involve selecting relevant text spans from given source texts. Despite this shared objective, such \textit{content selection} tasks have traditionally been studied in isolation, each with its own modeling approaches, datasets, and evaluation metrics. In this work, we propose \textit{instruction-guided content selection (IGCS)} as a beneficial unified framework for such settings, where the task definition and any instance-specific request are encapsulated as instructions to a language model. To promote this framework, we introduce \igcsbench{}, the first unified benchmark covering diverse content selection tasks. Further, we create a large generic synthetic dataset that can be leveraged for diverse content selection tasks, and show that transfer learning with these datasets often boosts performance, whether dedicated training for the targeted task is available or not. Finally, we address generic inference time issues that arise in LLM-based modeling of content selection, assess a generic evaluation metric, and overall propose the utility of our resources and methods for future content selection models. Models and datasets available at https://github.com/shmuelamar/igcs.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-based Clinical Decision Support for Primary Care: A Real-World Study</title>
<link>https://arxiv.org/abs/2507.16947</link>
<guid>https://arxiv.org/abs/2507.16947</guid>
<content:encoded><![CDATA[
<div> AI Consult, clinical decision support, Penda Health, diagnostic errors, treatment errors <br />
<br />Summary: 
In a study conducted at Penda Health clinics in Nairobi, Kenya, the impact of AI Consult, a large language model-based clinical decision support tool, was evaluated in real-world clinical settings. The tool served as a safety net for clinicians, identifying potential errors in documentation and decision-making. Clinicians with access to AI Consult made significantly fewer errors, with 16% fewer diagnostic errors and 13% fewer treatment errors compared to those without access. The implementation of AI Consult could potentially avert a substantial number of errors annually at Penda Health clinics. A survey of clinicians using AI Consult indicated that it improved the quality of care delivered, with 75% acknowledging a substantial effect. The study highlights the potential of language model-based tools in reducing errors in clinical practice when implemented in a workflow-aligned manner and actively deployed to encourage clinician uptake. 

<br /> <div>
arXiv:2507.16947v1 Announce Type: new 
Abstract: We evaluate the impact of large language model-based clinical decision support in live care. In partnership with Penda Health, a network of primary care clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a safety net for clinicians by identifying potential documentation and clinical decision-making errors. AI Consult integrates into clinician workflows, activating only when needed and preserving clinician autonomy. We conducted a quality improvement study, comparing outcomes for 39,849 patient visits performed by clinicians with or without access to AI Consult across 15 clinics. Visits were rated by independent physicians to identify clinical errors. Clinicians with access to AI Consult made relatively fewer errors: 16% fewer diagnostic errors and 13% fewer treatment errors. In absolute terms, the introduction of AI Consult would avert diagnostic errors in 22,000 visits and treatment errors in 29,000 visits annually at Penda alone. In a survey of clinicians with AI Consult, all clinicians said that AI Consult improved the quality of care they delivered, with 75% saying the effect was "substantial". These results required a clinical workflow-aligned AI Consult implementation and active deployment to encourage clinician uptake. We hope this study demonstrates the potential for LLM-based clinical decision support tools to reduce errors in real-world settings and provides a practical framework for advancing responsible adoption.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs</title>
<link>https://arxiv.org/abs/2507.16951</link>
<guid>https://arxiv.org/abs/2507.16951</guid>
<content:encoded><![CDATA[
<div> Generative Large Language Models, Conversational Information Retrieval, Unanswerable questions, Self-aware LLM, SALU

Summary:
Self-Aware LLM for Unanswerability (SALU) is a novel approach that integrates unanswerability detection directly within Large Language Models (LLMs) for Conversational Information Retrieval systems. It utilizes a multi-task learning framework to handle Question Answering and abstention generation for unanswerable queries. Employing confidence-score-guided reinforcement learning with human feedback, SALU effectively penalizes hallucinations and rewards appropriate abstentions, promoting self-awareness of knowledge boundaries. Experimental results on the C-IR_Answerability dataset demonstrate SALU's superiority over hybrid LLM-classifier systems in accurately answering or abstaining from questions. Human evaluation confirms SALU's reliability, with high factuality, appropriate abstention, and reduced hallucination. SALU excels in determining when to abstain from answering, showcasing its robustness in handling unanswerable queries. 

<br /><br />Summary: <div>
arXiv:2507.16951v1 Announce Type: new 
Abstract: Conversational Information Retrieval (CIR) systems, while offering intuitive access to information, face a significant challenge: reliably handling unanswerable questions to prevent the generation of misleading or hallucinated content. Traditional approaches often rely on external classifiers, which can introduce inconsistencies with the core generative Large Language Models (LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a novel approach that deeply integrates unanswerability detection directly within the LLM's generative process. SALU is trained using a multi-task learning framework for both standard Question Answering (QA) and explicit abstention generation for unanswerable queries. Crucially, it incorporates a confidence-score-guided reinforcement learning with human feedback (RLHF) phase, which explicitly penalizes hallucinated responses and rewards appropriate abstentions, fostering intrinsic self-awareness of knowledge boundaries. Through extensive experiments on our custom-built C-IR_Answerability dataset, SALU consistently outperforms strong baselines, including hybrid LLM-classifier systems, in overall accuracy for correctly answering or abstaining from questions. Human evaluation further confirms SALU's superior reliability, achieving high scores in factuality, appropriate abstention, and, most importantly, a dramatic reduction in hallucination, demonstrating its ability to robustly "know when to say 'I don't know'."
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>