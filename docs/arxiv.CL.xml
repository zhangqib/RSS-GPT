<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>

<item>
<title>Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards</title>
<link>https://arxiv.org/abs/2506.11425</link>
<guid>https://arxiv.org/abs/2506.11425</guid>
<content:encoded><![CDATA[
<div> Agent-RLVR, reinforcement learning, agentic environments, software engineering tasks, guidance<br>
Summary:<br>
Agent-RLVR is a framework designed to enhance reinforcement learning in challenging agentic environments, particularly focusing on software engineering tasks. By incorporating agent guidance mechanisms inspired by human pedagogy, the framework steers the agent towards successful trajectories using diverse informational cues. This approach enables the agent to navigate complex solution spaces, learn from errors, and improve through active exploration. In the training loop, agents initially attempt tasks, validate their trajectories with unit tests, and receive guidance for improvement. Through this process, Agent-RLVR significantly improves the performance of LLMs in software engineering benchmarks. The guided trajectories also prove beneficial for test-time reward model training, further boosting performance. Overall, Agent-RLVR lays the foundation for effectively training agents with reinforcement learning in real-world environments where traditional RL methods face challenges.<br> <div>
arXiv:2506.11425v2 Announce Type: replace 
Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted as the de facto method for enhancing the reasoning capabilities of large language models and has demonstrated notable success in verifiable domains like math and competitive programming tasks. However, the efficacy of RLVR diminishes significantly when applied to agentic environments. These settings, characterized by multi-step, complex problem solving, lead to high failure rates even for frontier LLMs, as the reward landscape is too sparse for effective model training via conventional RLVR. In this work, we introduce Agent-RLVR, a framework that makes RLVR effective in challenging agentic settings, with an initial focus on software engineering tasks. Inspired by human pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively steers the agent towards successful trajectories by leveraging diverse informational cues. These cues, ranging from high-level strategic plans to dynamic feedback on the agent's errors and environmental interactions, emulate a teacher's guidance, enabling the agent to navigate difficult solution spaces and promotes active self-improvement via additional environment exploration. In the Agent-RLVR training loop, agents first attempt to solve tasks to produce initial trajectories, which are then validated by unit tests and supplemented with agent guidance. Agents then reattempt with guidance, and the agent policy is updated with RLVR based on the rewards of these guided trajectories. Agent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4% to 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data is additionally useful for test-time reward model training, shown by further boosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents with RLVR in complex, real-world environments where conventional RL methods struggle.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions</title>
<link>https://arxiv.org/abs/2504.11673</link>
<guid>https://arxiv.org/abs/2504.11673</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, in-group/out-group biases, virtual personas, synthetic user backstories, political science studies

Summary:<br />
Large language models (LLMs) are increasingly being used to estimate user responses in surveys and polls, but it's unclear if their responses are from a deep in-group perspective or a shallow out-group perspective. To address this, the study explores known in-group/out-group biases using a novel methodology to create virtual personas with detailed synthetic user backstories. These backstories closely replicate human response distributions and effect sizes related to in-group/out-group biases. By enhancing the fidelity of LLMs in understanding socially understood attitudes, this research expands their utility in various political science studies, including research on polarization dynamics, inter-group conflict, and democratic backsliding. This approach allows for a broader range of human studies to benefit from the capabilities of LLMs. <div>
arXiv:2504.11673v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly capable of simulating human behavior, offering cost-effective ways to estimate user responses to various surveys and polls. However, the questions in these surveys usually reflect socially understood attitudes: the patterns of attitudes of old/young, liberal/conservative, as understood by both members and non-members of those groups. It is not clear whether the LLM binding is \emph{deep}, meaning the LLM answers as a member of a particular in-group would, or \emph{shallow}, meaning the LLM responds as an out-group member believes an in-group member would. To explore this difference, we use questions that expose known in-group/out-group biases. This level of fidelity is critical for applying LLMs to various political science studies, including timely topics on polarization dynamics, inter-group conflict, and democratic backsliding. To this end, we propose a novel methodology for constructing virtual personas with synthetic user ``backstories" generated as extended, multi-turn interview transcripts. Our generated backstories are longer, rich in detail, and consistent in authentically describing a singular individual, compared to previous methods. We show that virtual personas conditioned on our backstories closely replicate human response distributions (up to an 87\% improvement as measured by Wasserstein Distance) and produce effect sizes that closely match those observed in the original studies of in-group/out-group biases. Altogether, our work extends the applicability of LLMs beyond estimating socially understood responses, enabling their use in a broader range of human studies.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outcome-Based Education: Evaluating Students' Perspectives Using Transformer</title>
<link>https://arxiv.org/abs/2506.17223</link>
<guid>https://arxiv.org/abs/2506.17223</guid>
<content:encoded><![CDATA[
<div> Importance of Outcome-Based Education, DistilBERT, transformer models, NLP dataset, student feedback <br />
Summary: This study focuses on Outcome-Based Education (OBE) and uses transformer-based models, particularly DistilBERT, to analyze student feedback in an NLP dataset. The objective is to assess and improve educational outcomes through a deep understanding of language context provided by transformer models. The use of transformer models, combined with LIME explanations, allows for better sentiment classification and clear model predictions, leading to a stronger framework for analyzing student feedback. This approach aligns with OBE principles by facilitating the identification of patterns in student learning experiences and providing data-driven insights to improve educational practices. <div>
arXiv:2506.17223v1 Announce Type: new 
Abstract: Outcome-Based Education (OBE) emphasizes the development of specific competencies through student-centered learning. In this study, we reviewed the importance of OBE and implemented transformer-based models, particularly DistilBERT, to analyze an NLP dataset that includes student feedback. Our objective is to assess and improve educational outcomes. Our approach is better than other machine learning models because it uses the transformer's deep understanding of language context to classify sentiment better, giving better results across a wider range of matrices. Our work directly contributes to OBE's goal of achieving measurable outcomes by facilitating the identification of patterns in student learning experiences. We have also applied LIME (local interpretable model-agnostic explanations) to make sure that model predictions are clear. This gives us understandable information about how key terms affect sentiment. Our findings indicate that the combination of transformer models and LIME explanations results in a strong and straightforward framework for analyzing student feedback. This aligns more closely with the principles of OBE and ensures the improvement of educational practices through data-driven insights.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs</title>
<link>https://arxiv.org/abs/2506.17231</link>
<guid>https://arxiv.org/abs/2506.17231</guid>
<content:encoded><![CDATA[
<div> masked language modeling, reinforcement learning, dynamic temperature control, prompt generation, distillation

Summary:
This study focuses on improving the efficiency and adaptability of jailbreak attacks on large language models (LLMs) using small language models (SLMs). The proposed method, Adversarial Prompt Distillation, combines masked language modeling, reinforcement learning, and dynamic temperature control to enable SLMs to successfully jailbreak mainstream LLMs. Experimental results demonstrate the method's superiority in attack success rate, resource efficiency, and cross-model adaptability. By distilling the jailbreak ability of LLMs to SLMs, this research reveals vulnerabilities in current LLM security measures and provides a new approach to enhancing LLM security. <div>
arXiv:2506.17231v1 Announce Type: new 
Abstract: Attacks on large language models (LLMs) in jailbreaking scenarios raise many security and ethical issues. Current jailbreak attack methods face problems such as low efficiency, high computational cost, and poor cross-model adaptability and versatility, which make it difficult to cope with the rapid development of LLM and new defense strategies. Our work proposes an Adversarial Prompt Distillation, which combines masked language modeling, reinforcement learning, and dynamic temperature control through a prompt generation and distillation method. It enables small language models (SLMs) to jailbreak attacks on mainstream LLMs. The experimental results verify the superiority of the proposed method in terms of attack success rate and harm, and reflect the resource efficiency and cross-model adaptability. This research explores the feasibility of distilling the jailbreak ability of LLM to SLM, reveals the model's vulnerability, and provides a new idea for LLM security research.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTA: Grouped-head latenT Attention</title>
<link>https://arxiv.org/abs/2506.17286</link>
<guid>https://arxiv.org/abs/2506.17286</guid>
<content:encoded><![CDATA[
<div> Keywords: Attention mechanisms, Large language models, Computational complexity, Memory optimization, Efficiency improvement

Summary:<br />
The article introduces a novel attention mechanism called Grouped-Head Latent Attention (GTA) to address the computational and memory overhead challenges faced by large language models (LLMs). GTA reduces memory usage and computational complexity by reusing attention scores across multiple heads and compressing the value cache into a latent space. This results in a significant reduction in attention computation FLOPs and key-value cache size, leading to a 2x increase in end-to-end inference speed for GTA models. By improving efficiency and performance without the extra overhead of other mechanisms, GTA is a promising solution for optimizing LLM deployment on hardware with limited computational and memory resources.<br /><br />Summary: <div>
arXiv:2506.17286v1 Announce Type: new 
Abstract: Attention mechanisms underpin the success of large language models (LLMs), yet their substantial computational and memory overhead poses challenges for optimizing efficiency and performance. A critical bottleneck arises as KV cache and attention computations scale rapidly with text length, challenging deployment on hardware with limited computational and memory resources. We observe that attention mechanisms exhibit substantial redundancy, since the KV cache can be significantly compressed and attention maps across heads display high similarity, revealing that much of the computation and storage is unnecessary. Leveraging these insights, we propose \textbf{G}rouped-Head Laten\textbf{T} \textbf{A}ttention (GTA), a novel attention mechanism that reduces memory usage and computational complexity while maintaining performance. GTA comprises two components: (1) a shared attention map mechanism that reuses attention scores across multiple heads, decreasing the key cache size; and (2) a nonlinear value decoder with learned projections that compresses the value cache into a latent space, further cutting memory needs. GTA cuts attention computation FLOPs by up to \emph{62.5\%} versus Grouped-Query Attention and shrink the KV cache by up to \emph{70\%}, all while avoiding the extra overhead of Multi-Head Latent Attention to improve LLM deployment efficiency. Consequently, GTA models achieve a \emph{2x} increase in end-to-end inference speed, with prefill benefiting from reduced computational cost and decoding benefiting from the smaller cache footprint.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Generated Game Commentary: A Survey and a Datasheet Repository</title>
<link>https://arxiv.org/abs/2506.17294</link>
<guid>https://arxiv.org/abs/2506.17294</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-Generated Game Commentary, Natural Language Processing, datasets, evaluation metrics, framework

Summary: 
AI-Generated Game Commentary (AIGGC) is a complex task that requires language models to exhibit factual accuracy, logical reasoning, expressive text generation, generation speed, and context management. This paper introduces a general framework for AIGGC and surveys 45 existing game commentary datasets and methods that address key challenges in this field. The paper also classifies and compares evaluation metrics commonly used in AIGGC research. A structured datasheet summarizing essential dataset attributes is provided in an open repository to support future research and benchmarking efforts. <div>
arXiv:2506.17294v1 Announce Type: new 
Abstract: AI-Generated Game Commentary (AIGGC) has gained increasing attention due to its market potential and inherent technical challenges. As a comprehensive multimodal Natural Language Processing (NLP) task, AIGGC imposes substantial demands on language models, including factual accuracy, logical reasoning, expressive text generation, generation speed, and context management. In this paper, we introduce a general framework for AIGGC and present a comprehensive survey of 45 existing game commentary dataset and methods according to key challenges they aim to address in this domain. We further classify and compare various evaluation metrics commonly used in this domain. To support future research and benchmarking, we also provide a structured datasheet summarizing the essential attributes of these datasets in appendix, which is meanwhile publicly available in an open repository.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic uncertainty in advanced decoding methods for LLM generation</title>
<link>https://arxiv.org/abs/2506.17296</link>
<guid>https://arxiv.org/abs/2506.17296</guid>
<content:encoded><![CDATA[
<div> diversity, reliability, decoding strategies, semantic uncertainty, language model 

Summary: This study explores semantic uncertainty in large language model outputs using different decoding methods, such as speculative sampling and chain-of-thought decoding. The experiments conducted on question answering, summarization, and code generation tasks reveal that CoT decoding leads to higher semantic diversity but lower predictive entropy, resulting in more confident and accurate outputs, especially in code generation. Speculative sampling is shown to be effective in achieving superior ROUGE scores and moderate semantic diversity in summarization tasks. The findings challenge the conventional trade-offs between diversity and accuracy in language model outputs, emphasizing the importance of structured decoding methods to increase semantic exploration while maintaining or improving output quality. These results have implications for the practical deployment of language models where both reliable and diverse solution generation is essential. 

 <div>
arXiv:2506.17296v1 Announce Type: new 
Abstract: This study investigates semantic uncertainty in large language model (LLM) outputs across different decoding methods, focusing on emerging techniques like speculative sampling and chain-of-thought (CoT) decoding. Through experiments on question answering, summarization, and code generation tasks, we analyze how different decoding strategies affect both the diversity and reliability of model outputs. Our findings reveal that while CoT decoding demonstrates higher semantic diversity, it maintains lower predictive entropy, suggesting that structured exploration can lead to more confident and accurate outputs. This is evidenced by a 48.8% improvement in code generation Pass@2 rates, despite lower alignment with reference solutions. For summarization tasks, speculative sampling proved particularly effective, achieving superior ROUGE scores while maintaining moderate semantic diversity. Our results challenge conventional assumptions about trade-offs between diversity and accuracy in language model outputs, demonstrating that properly structured decoding methods can increase semantic exploration while maintaining or improving output quality. These findings have significant implications for deploying language models in practical applications where both reliability and diverse solution generation are crucial.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mercury: Ultra-Fast Language Models Based on Diffusion</title>
<link>https://arxiv.org/abs/2506.17298</link>
<guid>https://arxiv.org/abs/2506.17298</guid>
<content:encoded><![CDATA[
<div> Mercury; large language models; diffusion; coding applications; state-of-the-art

Summary:
Mercury introduces a new generation of large language models (LLMs) based on diffusion, designed specifically for coding applications. These models, parameterized via the Transformer architecture, predict multiple tokens in parallel. Mercury Coder, the first set of diffusion LLMs, includes Mini and Small sizes currently setting a new speed-quality frontier state-of-the-art. Independent evaluations by Artificial Analysis show Mercury Coder Mini and Small achieve impressive throughputs on NVIDIA H100 GPUs, outperforming speed-optimized models with up to a 10x improvement on average while maintaining quality. Results on various code benchmarks and real-world validation on Copilot Arena demonstrate the models' effectiveness. Additionally, a public API and free playground are released for developers to access the technology. 

<br /><br />Summary: <div>
arXiv:2506.17298v1 Announce Type: new 
Abstract: We present Mercury, a new generation of commercial-scale large language models (LLMs) based on diffusion. These models are parameterized via the Transformer architecture and trained to predict multiple tokens in parallel. In this report, we detail Mercury Coder, our first set of diffusion LLMs designed for coding applications. Currently, Mercury Coder comes in two sizes: Mini and Small. These models set a new state-of-the-art on the speed-quality frontier. Based on independent evaluations conducted by Artificial Analysis, Mercury Coder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109 tokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform speed-optimized frontier models by up to 10x on average while maintaining comparable quality. We discuss additional results on a variety of code benchmarks spanning multiple languages and use-cases as well as real-world validation by developers on Copilot Arena, where the model currently ranks second on quality and is the fastest model overall. We also release a public API at https://platform.inceptionlabs.ai/ and free playground at https://chat.inceptionlabs.ai
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights</title>
<link>https://arxiv.org/abs/2506.17314</link>
<guid>https://arxiv.org/abs/2506.17314</guid>
<content:encoded><![CDATA[
<div> Keywords: e-commerce, product descriptions, customer reviews, Large Language Models (LLMs), structured insights 

Summary:

Accurate product descriptions are essential for e-commerce success, but information provided by sellers can often be insufficient. PRAISE, a new system, leverages Large Language Models to automatically extract and compare insights from customer reviews and seller descriptions. This innovative tool offers a user-friendly interface to identify discrepancies and inconsistencies between the two sources, presenting this information in a structured manner with evidence from reviews. PRAISE enables sellers to enhance their product listings for clarity and persuasiveness and helps buyers make more informed decisions. The demonstration of PRAISE showcases its ability to generate actionable insights from unstructured reviews, ultimately improving the quality and reliability of e-commerce product catalogs.  

<br /><br />Summary: <div>
arXiv:2506.17314v1 Announce Type: new 
Abstract: Accurate and complete product descriptions are crucial for e-commerce, yet seller-provided information often falls short. Customer reviews offer valuable details but are laborious to sift through manually. We present PRAISE: Product Review Attribute Insight Structuring Engine, a novel system that uses Large Language Models (LLMs) to automatically extract, compare, and structure insights from customer reviews and seller descriptions. PRAISE provides users with an intuitive interface to identify missing, contradictory, or partially matching details between these two sources, presenting the discrepancies in a clear, structured format alongside supporting evidence from reviews. This allows sellers to easily enhance their product listings for clarity and persuasiveness, and buyers to better assess product reliability. Our demonstration showcases PRAISE's workflow, its effectiveness in generating actionable structured insights from unstructured reviews, and its potential to significantly improve the quality and trustworthiness of e-commerce product catalogs.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Safety Evaluations of Theory of Mind in Large Language Models</title>
<link>https://arxiv.org/abs/2506.17352</link>
<guid>https://arxiv.org/abs/2506.17352</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, safety evaluation, theory of mind, developmental psychology, deceptive behavior

Summary:
The article discusses the importance of evaluating the safety of large language models (LLMs) due to concerns regarding deceptive behaviors. It highlights the need to assess whether these behaviors are intentional or result from covert processes within the model. The study proposes measuring the theory of mind capabilities of LLMs to understand their perspective-taking abilities. By analyzing developmental trends in LLMs, the study finds that while they have improved in reading comprehension, their theory of mind capabilities have not shown significant progress. The current state of safety evaluation in LLMs' theory of mind is discussed, along with the challenges for future research. Overall, understanding LLMs' theory of mind is crucial for assessing the risk of deceptive actions towards developers or users and ensuring the responsible use of these models.<br /><br />Summary: The study explores the theory of mind capabilities of large language models (LLMs) to evaluate their potential for deceptive behaviors. It reviews existing research on theory of mind and developmental trends in LLMs, finding that while they have improved in reading comprehension, their theory of mind capabilities have not progressed as significantly. The article emphasizes the need to understand LLMs' perspective-taking abilities to ensure safety evaluation and addresses challenges for future research in this area. <div>
arXiv:2506.17352v1 Announce Type: new 
Abstract: As the capabilities of large language models (LLMs) continue to advance, the importance of rigorous safety evaluation is becoming increasingly evident. Recent concerns within the realm of safety assessment have highlighted instances in which LLMs exhibit behaviors that appear to disable oversight mechanisms and respond in a deceptive manner. For example, there have been reports suggesting that, when confronted with information unfavorable to their own persistence during task execution, LLMs may act covertly and even provide false answers to questions intended to verify their behavior.To evaluate the potential risk of such deceptive actions toward developers or users, it is essential to investigate whether these behaviors stem from covert, intentional processes within the model. In this study, we propose that it is necessary to measure the theory of mind capabilities of LLMs. We begin by reviewing existing research on theory of mind and identifying the perspectives and tasks relevant to its application in safety evaluation. Given that theory of mind has been predominantly studied within the context of developmental psychology, we analyze developmental trends across a series of open-weight LLMs. Our results indicate that while LLMs have improved in reading comprehension, their theory of mind capabilities have not shown comparable development. Finally, we present the current state of safety evaluation with respect to LLMs' theory of mind, and discuss remaining challenges for future work.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cash or Comfort? How LLMs Value Your Inconvenience</title>
<link>https://arxiv.org/abs/2506.17367</link>
<guid>https://arxiv.org/abs/2506.17367</guid>
<content:encoded><![CDATA[
<div> price assign, user discomfort, LLMs, decision-making, financial rewards

Summary:
- The study focuses on understanding how Large Language Models (LLMs) value human inconvenience in decision-making scenarios involving financial rewards.
- Multiple LLMs showed significant variability in their responses to user discomforts like additional walking, waiting, hunger, and pain.
- Even within a single LLM, responses were found to be fragile to slight changes in prompt phrasing, highlighting a lack of consistency.
- LLMs accepted unreasonably low rewards for major inconveniences and rejected monetary gains in scenarios where no discomfort was present.
- The findings underscore the need for careful evaluation of how LLMs prioritize human comfort versus financial gains, especially as they are considered for decision-making assistant roles. 

<br /><br />Summary: <div>
arXiv:2506.17367v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly proposed as near-autonomous artificial intelligence (AI) agents capable of making everyday decisions on behalf of humans. Although LLMs perform well on many technical tasks, their behaviour in personal decision-making remains less understood. Previous studies have assessed their rationality and moral alignment with human decisions. However, the behaviour of AI assistants in scenarios where financial rewards are at odds with user comfort has not yet been thoroughly explored. In this paper, we tackle this problem by quantifying the prices assigned by multiple LLMs to a series of user discomforts: additional walking, waiting, hunger and pain. We uncover several key concerns that strongly question the prospect of using current LLMs as decision-making assistants: (1) a large variance in responses between LLMs, (2) within a single LLM, responses show fragility to minor variations in prompt phrasing (e.g., reformulating the question in the first person can considerably alter the decision), (3) LLMs can accept unreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10 hours), and (4) LLMs can reject monetary gains where no discomfort is imposed (e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for scrutiny of how LLMs value human inconvenience, particularly as we move toward applications where such cash-versus-comfort trade-offs are made on users' behalf.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study</title>
<link>https://arxiv.org/abs/2506.17410</link>
<guid>https://arxiv.org/abs/2506.17410</guid>
<content:encoded><![CDATA[
<div> Keywords: tutoring, generative AI, math education, student learning, large language models

Summary:
- The study explores using generative AI to assess specific tutor actions in math tutoring sessions.
- Analysis of transcripts from college-student remote tutors assisting middle school students in mathematics was conducted.
- Models like GPT-4 and Gemini-1.5-pro were used to evaluate tutor skills in praising students and responding to math errors.
- The AI models reliably detected situations such as tutors providing praise (94-98% accuracy) and students making math errors (82-88% accuracy).
- The models effectively evaluated tutor adherence to best practices, closely aligning with human judgments (83-89% and 73-77%, respectively).
- A cost-effective prompting strategy was proposed, highlighting practical implications for using large language models in authentic tutoring settings.
- The study contributes to the development of LLM prompts to enhance reproducibility and research in AI-supported learning.

<br /><br />Summary: The study investigates the use of generative AI to analyze tutor actions in math tutoring sessions, particularly focusing on praising students and responding to math errors. Through the analysis of real-life transcripts, AI models like GPT-4 were able to accurately detect tutoring situations and evaluate tutor performance. The findings suggest the potential for AI to support scalable assessment in educational settings, with implications for improving tutoring practices and student learning outcomes. The study also introduces cost-effective strategies for using large language models and contributes to the development of prompts for AI-supported learning research. <div>
arXiv:2506.17410v1 Announce Type: new 
Abstract: Tutoring improves student achievement, but identifying and studying what tutoring actions are most associated with student learning at scale based on audio transcriptions is an open research problem. This present study investigates the feasibility and scalability of using generative AI to identify and evaluate specific tutor moves in real-life math tutoring. We analyze 50 randomly selected transcripts of college-student remote tutors assisting middle school students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo, Gemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills: delivering effective praise and responding to student math errors. All models reliably detected relevant situations, for example, tutors providing praise to students (94-98% accuracy) and a student making a math error (82-88% accuracy) and effectively evaluated the tutors' adherence to tutoring best practices, aligning closely with human judgments (83-89% and 73-77%, respectively). We propose a cost-effective prompting strategy and discuss practical implications for using large language models to support scalable assessment in authentic settings. This work further contributes LLM prompts to support reproducibility and research in AI-supported learning.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making</title>
<link>https://arxiv.org/abs/2506.17419</link>
<guid>https://arxiv.org/abs/2506.17419</guid>
<content:encoded><![CDATA[
<div> framework, Large Language Models, uncertainty quantification, decision-making, Mutual-Information<br />
<br />
Summary: This paper introduces a new information-theoretic framework for quantifying uncertainty in the decisions made by Large Language Models (LLMs) in multi-step decision-making scenarios. The framework decomposes decision uncertainty into internal uncertainty intrinsic to the current decision and extrinsic uncertainty, which describes uncertainty inherited from preceding decisions. The proposed method, UProp, efficiently estimates extrinsic uncertainty by estimating Pointwise Mutual Information over multiple Trajectory-Dependent Decision Processes. UProp outperforms existing single-turn uncertainty quantification methods and has been evaluated on benchmarks with state-of-the-art LLMs. The paper also discusses sampling efficiency, potential applications, and uncertainty propagation in detail. Codes for UProp will be made available on GitHub. <br /><br />Summary: <div>
arXiv:2506.17419v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are integrated into safety-critical applications involving sequential decision-making in the real world, it is essential to know when to trust LLM decisions. Existing LLM Uncertainty Quantification (UQ) methods are primarily designed for single-turn question-answering formats, resulting in multi-step decision-making scenarios, e.g., LLM agentic system, being underexplored. In this paper, we introduce a principled, information-theoretic framework that decomposes LLM sequential decision uncertainty into two parts: (i) internal uncertainty intrinsic to the current decision, which is focused on existing UQ methods, and (ii) extrinsic uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty should be inherited from preceding decisions. We then propose UProp, an efficient and effective extrinsic uncertainty estimator that converts the direct estimation of MI to the estimation of Pointwise Mutual Information (PMI) over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is evaluated over extensive multi-step decision-making benchmarks, e.g., AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and DeepSeek-V3. Experimental results demonstrate that UProp significantly outperforms existing single-turn UQ baselines equipped with thoughtful aggregation strategies. Moreover, we provide a comprehensive analysis of UProp, including sampling efficiency, potential applications, and intermediate uncertainty propagation, to demonstrate its effectiveness. Codes will be available at https://github.com/jinhaoduan/UProp.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media</title>
<link>https://arxiv.org/abs/2506.17435</link>
<guid>https://arxiv.org/abs/2506.17435</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, political content, URLs, model performance, political science

Summary: 
- The article explores the use of large language models (LLMs) in classifying political content (PC) from URLs.
- LLMs like GPT, Llama, Mistral, Deepseek, Qwen, and Gemma are evaluated for their ability to identify PC vs. non-PC across different countries and languages.
- The study compares LLM outputs with human-labelled articles and traditional machine learning techniques to establish a performance baseline.
- Findings suggest that URLs can effectively capture most news content, offering a cost-effective approach to accuracy in content classification.
- The article discusses contextual limitations and provides methodological recommendations for incorporating LLMs in political science studies.

<br /><br />Summary: <div>
arXiv:2506.17435v1 Announce Type: new 
Abstract: The use of large language models (LLMs) is becoming common in the context of political science, particularly in studies that analyse individuals use of digital media. However, while previous research has demonstrated LLMs ability at labelling tasks, the effectiveness of using LLMs to classify political content (PC) from just URLs is not yet well explored. The work presented in this article bridges this gap by evaluating whether LLMs can accurately identify PC vs. non-PC from both the article text and the URLs from five countries (France, Germany, Spain, the UK, and the US) and different languages. Using cutting-edge LLMs like GPT, Llama, Mistral, Deepseek, Qwen and Gemma, we measure model performance to assess whether URL-level analysis can be a good approximation for full-text analysis of PC, even across different linguistic and national contexts. Model outputs are compared with human-labelled articles, as well as traditional supervised machine learning techniques, to set a baseline of performance. Overall, our findings suggest the capacity of URLs to embed most of the news content, providing a vital perspective on accuracy-cost balancing. We also account for contextual limitations and suggest methodological recommendations to use LLMs within political science studies.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Transcription Bottleneck: Fine-tuning ASR Models for Extremely Low-Resource Fieldwork Languages</title>
<link>https://arxiv.org/abs/2506.17459</link>
<guid>https://arxiv.org/abs/2506.17459</guid>
<content:encoded><![CDATA[
<div> benchmarking, automatic speech recognition, linguistic fieldwork, low-resource languages, training data

Summary:
The paper evaluates the performance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five low-resource languages with limited training data. MMS outperforms XLS-R when minimal training data is available, while XLS-R achieves similar performance when training data exceeds one hour. The study emphasizes the challenges faced in fieldwork contexts such as spontaneous speech and environmental noise. It provides linguistically grounded analysis and practical guidelines for field linguists to adopt reproducible ASR adaptation approaches. These insights aim to address the transcription bottleneck in language documentation, offering potential solutions to enhance ASR utility in linguistic fieldwork. 
<br /><br />Summary: <div>
arXiv:2506.17459v1 Announce Type: new 
Abstract: Automatic Speech Recognition (ASR) has reached impressive accuracy for high-resource languages, yet its utility in linguistic fieldwork remains limited. Recordings collected in fieldwork contexts present unique challenges, including spontaneous speech, environmental noise, and severely constrained datasets from under-documented languages. In this paper, we benchmark the performance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five typologically diverse low-resource languages with control of training data duration. Our findings show that MMS is best suited when extremely small amounts of training data are available, whereas XLS-R shows parity performance once training data exceed one hour. We provide linguistically grounded analysis for further provide insights towards practical guidelines for field linguists, highlighting reproducible ASR adaptation approaches to mitigate the transcription bottleneck in language documentation.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems</title>
<link>https://arxiv.org/abs/2506.17467</link>
<guid>https://arxiv.org/abs/2506.17467</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, institutional adoption, AI governance, algorithmic approaches, manuscript feedback 

Summary: 
This dissertation explores the impact of Large Language Models (LLMs) on various aspects of society. It examines the institutional adoption of AI detectors and the biases they introduce, particularly disadvantaging writers of non-dominant language varieties. The research also presents population-level algorithmic approaches to measure the increasing use of LLMs across different writing domains, uncovering consistent patterns in academic peer reviews, scientific publications, consumer complaints, corporate communications, job postings, and international organization press releases. Additionally, the study investigates the potential of LLMs to provide feedback on research manuscripts, highlighting their ability to support researchers, especially early-career researchers and those from under-resourced settings. The findings underscore critical equity concerns in AI governance and the promising role of LLMs in enhancing access to timely manuscript feedback for researchers. 

<br /><br />Summary: <div>
arXiv:2506.17467v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown significant potential to change how we write, communicate, and create, leading to rapid adoption across society. This dissertation examines how individuals and institutions are adapting to and engaging with this emerging technology through three research directions. First, I demonstrate how the institutional adoption of AI detectors introduces systematic biases, particularly disadvantaging writers of non-dominant language varieties, highlighting critical equity concerns in AI governance. Second, I present novel population-level algorithmic approaches that measure the increasing adoption of LLMs across writing domains, revealing consistent patterns of AI-assisted content in academic peer reviews, scientific publications, consumer complaints, corporate communications, job postings, and international organization press releases. Finally, I investigate LLMs' capability to provide feedback on research manuscripts through a large-scale empirical analysis, offering insights into their potential to support researchers who face barriers in accessing timely manuscript feedback, particularly early-career researchers and those from under-resourced settings.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM</title>
<link>https://arxiv.org/abs/2506.17506</link>
<guid>https://arxiv.org/abs/2506.17506</guid>
<content:encoded><![CDATA[
<div> large language models, formal compiler techniques, register allocation, GPUs, VeriLocc <br />
Summary:
VeriLocc is a framework that combines large language models and formal compiler techniques to enhance register allocation on evolving GPU architectures. By fine-tuning an LLM to translate MIRs into target-specific register assignments, aided by static analysis and a verifier-guided loop, VeriLocc achieves high accuracy and performance on tasks like GEMM and MHA. It outperforms expert-tuned libraries like rocBLAS by over 10% in runtime. This approach enables generalizable and verifiable register allocation, reducing the need for manual tuning for each hardware generation. <div>
arXiv:2506.17506v1 Announce Type: new 
Abstract: Modern GPUs evolve rapidly, yet production compilers still rely on hand-crafted register allocation heuristics that require substantial re-tuning for each hardware generation. We introduce VeriLocc, a framework that combines large language models (LLMs) with formal compiler techniques to enable generalizable and verifiable register allocation across GPU architectures. VeriLocc fine-tunes an LLM to translate intermediate representations (MIRs) into target-specific register assignments, aided by static analysis for cross-architecture normalization and generalization and a verifier-guided regeneration loop to ensure correctness. Evaluated on matrix multiplication (GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot accuracy and near-100% pass@100. Case study shows that VeriLocc discovers more performant assignments than expert-tuned libraries, outperforming rocBLAS by over 10% in runtime.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning</title>
<link>https://arxiv.org/abs/2506.17525</link>
<guid>https://arxiv.org/abs/2506.17525</guid>
<content:encoded><![CDATA[
<div> audit, multilingual speech datasets, quality issues, language planning, dataset development 

Summary: 
The article discusses a quality audit of three public multilingual speech datasets, uncovering significant quality issues in some languages. The identified issues are categorized as micro-level and macro-level, with the latter being more common in under-resourced languages. The case analysis of Taiwanese Southern Min (nan_tw) highlights the necessity for proactive language planning and improved data quality control in ASR dataset creation. Guidelines and recommendations are proposed to address these issues in future dataset development, stressing the importance of sociolinguistic awareness in creating reliable speech data resources. <div>
arXiv:2506.17525v1 Announce Type: new 
Abstract: Our quality audit for three widely used public multilingual speech datasets - Mozilla Common Voice 17.0, FLEURS, and VoxPopuli - shows that in some languages, these datasets suffer from significant quality issues. We believe addressing these issues will make these datasets more useful as training and evaluation sets, and improve downstream models. We divide these quality issues into two categories: micro-level and macro-level. We find that macro-level issues are more prevalent in less institutionalized, often under-resourced languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that highlights the need for proactive language planning (e.g. orthography prescriptions, dialect boundary definition) and enhanced data quality control in the process of Automatic Speech Recognition (ASR) dataset creation. We conclude by proposing guidelines and recommendations to mitigate these issues in future dataset development, emphasizing the importance of sociolinguistic awareness in creating robust and reliable speech data resources.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2506.17533</link>
<guid>https://arxiv.org/abs/2506.17533</guid>
<content:encoded><![CDATA[
<div> reward modeling, Large Language Models, mathematical reasoning, DuaShepherd framework, automated pipeline

Summary:<br />
- The paper introduces DuaShepherd, a framework that combines correctness and potential reward signals to enhance the mathematical reasoning abilities of Large Language Models.
- The framework uses an automated pipeline to create a large-scale reward modeling dataset with both correctness and potential signals.
- A unified, multi-head architecture is utilized to train the two reward models simultaneously, leading to improved performance on various benchmarks.
- By combining correctness and potential signals into a compound probability, the model achieves consistent performance enhancements.
- Empirical evaluations on MATH500 and ProcessBench demonstrate that the combined reward significantly outperforms models trained on individual reward types, achieving state-of-the-art results within similar resource constraints.

<br /><br />Summary: <div>
arXiv:2506.17533v1 Announce Type: new 
Abstract: In this paper, we propose DuaShepherd, a novel reward modeling framework that integrates two complementary reward signals, correctness and potential, to enhance the mathematical reasoning capabilities of Large Language Models (LLMs). While correctness-based signals emphasize identification of stepwise errors, potential-based signals focus on the likelihood of reaching the correct final answer. We developed an automated pipeline for constructing large-scale reward modeling dataset with both signals. A unified, multi-head architecture was explored to train the two reward models in a multi-task setup, demonstrating benefits from learning both correctness and potential in parallel. By combining these two signals into a compound probability, our model achieves consistent performance improvements across multiple benchmarks. Empirical evaluations on MATH500 and ProcessBench confirm that this combined reward significantly outperforms models trained on either reward type alone, achieving state-of-the-art performance under comparable resource constraints.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing for Phonology in Self-Supervised Speech Representations: A Case Study on Accent Perception</title>
<link>https://arxiv.org/abs/2506.17542</link>
<guid>https://arxiv.org/abs/2506.17542</guid>
<content:encoded><![CDATA[
<div> segmental accent, phonological features, self-supervised learning, pretrained representations, accent perception

Summary:
- Traditional accent perception models overlook the importance of gradient variations in phonological features.
- The study analyzes how current self-supervised learning speech models encode phonological feature-level variations affecting accent perception.
- By focusing on specific speech segments, the study uses phonological feature probabilities and pretrained representations from Wav2Vec2-BERT and WavLM models.
- Results suggest that accent strength is closely related to specific features in pretrained representations that highlight phonological differences between native and non-native English segments.
- A logistic regression analysis reveals a strong correlation between accent ratings and the distances of non-native English segments from American and Indian English baselines, emphasizing the importance of self-supervised speech representations for accent perception modeling. 

<br /><br />Summary: <div>
arXiv:2506.17542v1 Announce Type: new 
Abstract: Traditional models of accent perception underestimate the role of gradient variations in phonological features which listeners rely upon for their accent judgments. We investigate how pretrained representations from current self-supervised learning (SSL) models of speech encode phonological feature-level variations that influence the perception of segmental accent. We focus on three segments: the labiodental approximant, the rhotic tap, and the retroflex stop, which are uniformly produced in the English of native speakers of Hindi as well as other languages in the Indian sub-continent. We use the CSLU Foreign Accented English corpus (Lander, 2007) to extract, for these segments, phonological feature probabilities using Phonet (V\'asquez-Correa et al., 2019) and pretrained representations from Wav2Vec2-BERT (Barrault et al., 2023) and WavLM (Chen et al., 2022) along with accent judgements by native speakers of American English. Probing analyses show that accent strength is best predicted by a subset of the segment's pretrained representation features, in which perceptually salient phonological features that contrast the expected American English and realized non-native English segments are given prominent weighting. A multinomial logistic regression of pretrained representation-based segment distances from American and Indian English baselines on accent ratings reveals strong associations between the odds of accent strength and distances from the baselines, in the expected directions. These results highlight the value of self-supervised speech representations for modeling accent perception using interpretable phonological features.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgriCHN: A Comprehensive Cross-domain Resource for Chinese Agricultural Named Entity Recognition</title>
<link>https://arxiv.org/abs/2506.17578</link>
<guid>https://arxiv.org/abs/2506.17578</guid>
<content:encoded><![CDATA[
<div> Keywords: Agricultural named entity recognition, Chinese dataset, hydrology, meteorology, neural NER models 

Summary: 
The article introduces AgriCHN, an open-source Chinese dataset designed for agricultural named entity recognition. It addresses the lack of high-quality agricultural datasets by including entities from hydrology and meteorology, in addition to traditional agricultural categories. The dataset contains 15,799 mentions across 27 diverse entity categories, making it a valuable resource for advancing automated agricultural entity annotation. Data validation confirms the high quality of AgriCHN compared to existing resources, with more detailed entity divisions and types. Benchmarking with neural NER models demonstrates the challenge posed by AgriCHN and its potential for further research. Overall, AgriCHN provides a comprehensive and diverse dataset for improving the accuracy of agricultural entity recognition. 

<br /><br />Summary: <div>
arXiv:2506.17578v1 Announce Type: new 
Abstract: Agricultural named entity recognition is a specialized task focusing on identifying distinct agricultural entities within vast bodies of text, including crops, diseases, pests, and fertilizers. It plays a crucial role in enhancing information extraction from extensive agricultural text resources. However, the scarcity of high-quality agricultural datasets, particularly in Chinese, has resulted in suboptimal performance when employing mainstream methods for this purpose. Most earlier works only focus on annotating agricultural entities while overlook the profound correlation of agriculture with hydrology and meteorology. To fill this blank, we present AgriCHN, a comprehensive open-source Chinese resource designed to promote the accuracy of automated agricultural entity annotation. The AgriCHN dataset has been meticulously curated from a wealth of agricultural articles, comprising a total of 4,040 sentences and encapsulating 15,799 agricultural entity mentions spanning 27 diverse entity categories. Furthermore, it encompasses entities from hydrology to meteorology, thereby enriching the diversity of entities considered. Data validation reveals that, compared with relevant resources, AgriCHN demonstrates outstanding data quality, attributable to its richer agricultural entity types and more fine-grained entity divisions. A benchmark task has also been constructed using several state-of-the-art neural NER models. Extensive experimental results highlight the significant challenge posed by AgriCHN and its potential for further research.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: Assessing Wiktionary's Crowd-Sourced Linguistic Knowledge on Morphological Gaps in Two Related Languages</title>
<link>https://arxiv.org/abs/2506.17603</link>
<guid>https://arxiv.org/abs/2506.17603</guid>
<content:encoded><![CDATA[
<div> morphological defectivity, linguistics, NLP tools, morphologically rich languages, crowd-sourced data<br />
<br />
Summary:
The study delves into the topic of morphological defectivity, a less explored area in linguistics that is crucial for enhancing the accuracy of NLP tools in languages with complex morphology. Traditional linguistic resources often lack information on morphological gaps, necessitating the use of crowd-sourced platforms like Wikipedia and Wiktionary. The research customizes a neural morphological analyzer to analyze Latin and Italian data and validates crowd-sourced lists of defective verbs from Wiktionary computationally. The findings reveal that Wiktionary is a reliable source for Italian morphological gaps, but there is a discrepancy for Latin lemmata, where some listed as defective show evidence of not being so in actual usage. This highlights the limitations of crowd-sourced wikis in providing definitive linguistic knowledge, especially for less-studied languages and phenomena. The study contributes to advancing computational morphology and expanding understanding of defectivity in non-English, morphologically rich languages. 
<br /> <div>
arXiv:2506.17603v1 Announce Type: new 
Abstract: Morphological defectivity is an intriguing and understudied phenomenon in linguistics. Addressing defectivity, where expected inflectional forms are absent, is essential for improving the accuracy of NLP tools in morphologically rich languages. However, traditional linguistic resources often lack coverage of morphological gaps as such knowledge requires significant human expertise and effort to document and verify. For scarce linguistic phenomena in under-explored languages, Wikipedia and Wiktionary often serve as among the few accessible resources. Despite their extensive reach, their reliability has been a subject of controversy. This study customizes a novel neural morphological analyzer to annotate Latin and Italian corpora. Using the massive annotated data, crowd-sourced lists of defective verbs compiled from Wiktionary are validated computationally. Our results indicate that while Wiktionary provides a highly reliable account of Italian morphological gaps, 7% of Latin lemmata listed as defective show strong corpus evidence of being non-defective. This discrepancy highlights potential limitations of crowd-sourced wikis as definitive sources of linguistic knowledge, particularly for less-studied phenomena and languages, despite their value as resources for rare linguistic features. By providing scalable tools and methods for quality assurance of crowd-sourced data, this work advances computational morphology and expands linguistic knowledge of defectivity in non-English, morphologically rich languages.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting</title>
<link>https://arxiv.org/abs/2506.17609</link>
<guid>https://arxiv.org/abs/2506.17609</guid>
<content:encoded><![CDATA[
<div> Keywords: Typhoon track forecasting, Transformer-based models, Natural language descriptions, Auxiliary prompts, Meteorological trajectories

Summary: 
TyphoFormer is a novel framework proposed for accurate typhoon track forecasting, combining numerical attributes with natural language descriptions as auxiliary prompts. The model incorporates Large Language Models to generate textual descriptions capturing high-level meteorological semantics, enhancing contextual knowledge. By integrating sequential and textual information in a unified Transformer encoder, TyphoFormer outperforms state-of-the-art baseline methods in forecasting typhoon trajectories, especially in scenarios with nonlinear path shifts and limited historical data. The framework's ability to leverage contextual cues inaccessible through numerical features alone demonstrates its reliability and effectiveness in improving forecasting accuracy. The results of extensive experiments on the HURDAT2 benchmark validate TyphoFormer's superior performance in capturing the complex dynamics of typhoon tracks, making it a promising development in the field of typhoon forecasting. 

<br /><br />Summary: <div>
arXiv:2506.17609v1 Announce Type: new 
Abstract: Accurate typhoon track forecasting is crucial for early system warning and disaster response. While Transformer-based models have demonstrated strong performance in modeling the temporal dynamics of dense trajectories of humans and vehicles in smart cities, they usually lack access to broader contextual knowledge that enhances the forecasting reliability of sparse meteorological trajectories, such as typhoon tracks. To address this challenge, we propose TyphoFormer, a novel framework that incorporates natural language descriptions as auxiliary prompts to improve typhoon trajectory forecasting. For each time step, we use Large Language Model (LLM) to generate concise textual descriptions based on the numerical attributes recorded in the North Atlantic hurricane database. The language descriptions capture high-level meteorological semantics and are embedded as auxiliary special tokens prepended to the numerical time series input. By integrating both textual and sequential information within a unified Transformer encoder, TyphoFormer enables the model to leverage contextual cues that are otherwise inaccessible through numerical features alone. Extensive experiments are conducted on HURDAT2 benchmark, results show that TyphoFormer consistently outperforms other state-of-the-art baseline methods, particularly under challenging scenarios involving nonlinear path shifts and limited historical observations.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpusLM: A Family of Open Unified Speech Language Models</title>
<link>https://arxiv.org/abs/2506.17611</link>
<guid>https://arxiv.org/abs/2506.17611</guid>
<content:encoded><![CDATA[
<div> Keywords: Open Unified Speech Language Models, foundational speech language models, speech recognition, speech synthesis, multi-stream language models, model size scaling

Summary:
Open Unified Speech Language Models (OpusLMs) are a family of open foundational speech language models up to 7B. These models are continuously pre-trained on a large dataset of speech-text pairs and text tokens. OpusLMs show comparable or superior performance in speech recognition, synthesis, and text-only capabilities compared to existing models. The paper discusses tokenization, multi-stream models, and training strategies. It highlights the importance of model size scaling and data selection methods. OpusLMs are built from publicly available materials and aim to facilitate open SpeechLM research. The authors release code, data, checkpoints, and training logs for transparency and reproducibility. The research contributes to advancing speech language modeling and encourages collaboration in the field. 

<br /><br />Summary: <div>
arXiv:2506.17611v1 Announce Type: new 
Abstract: This paper presents Open Unified Speech Language Models (OpusLMs), a family of open foundational speech language models (SpeechLMs) up to 7B. Initialized from decoder-only text language models, the OpusLMs are continuously pre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We demonstrate our OpusLMs achieve comparable (or even superior) performance with existing SpeechLMs in speech recognition, speech synthesis, and text-only capabilities. Technically, this paper articulates our SpeechLM designs on tokenization, multi-stream language models, and multi-stage training strategies. We experimentally demonstrate the importance of model size scaling and the effect of annealing data selection. The OpusLMs are all built from publicly available materials and are fully transparent models. We release our code, data, checkpoints, and training logs to facilitate open SpeechLM research
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs</title>
<link>https://arxiv.org/abs/2506.17630</link>
<guid>https://arxiv.org/abs/2506.17630</guid>
<content:encoded><![CDATA[
<div> answer-visibility, large language models, reasoning, inference, prompt framework  
Summary:  
- The study investigates if Large Language Models (LLMs) rely more on final answers or textual reasoning chains.  
- A five-level answer-visibility prompt framework is proposed to manipulate answer cues and analyze model behavior.  
- Experiments across LLMs show a strong dependence on explicit answers, with a significant performance drop when answer cues are masked.  
- The findings indicate that LLMs may prioritize post-hoc rationalization over true inference, raising concerns about their reasoning depth.  
- The study highlights the need for a deeper understanding of reasoning in LLMs and showcases the phenomenon of answer anchoring with empirical validation.  
<br /><br />Summary: <div>
arXiv:2506.17630v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) demonstrate impressive reasoning capabilities, growing evidence suggests much of their success stems from memorized answer-reasoning patterns rather than genuine inference. In this work, we investigate a central question: are LLMs primarily anchored to final answers or to the textual pattern of reasoning chains? We propose a five-level answer-visibility prompt framework that systematically manipulates answer cues and probes model behavior through indirect, behavioral analysis. Experiments across state-of-the-art LLMs reveal a strong and consistent reliance on explicit answers. The performance drops by 26.90\% when answer cues are masked, even with complete reasoning chains. These findings suggest that much of the reasoning exhibited by LLMs may reflect post-hoc rationalization rather than true inference, calling into question their inferential depth. Our study uncovers the answer-anchoring phenomenon with rigorous empirical validation and underscores the need for a more nuanced understanding of what constitutes reasoning in LLMs.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation</title>
<link>https://arxiv.org/abs/2506.17637</link>
<guid>https://arxiv.org/abs/2506.17637</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Optimization Modeling, Step-Opt-Instruct, Fine-tuning, Decision-making.

Summary:
Step-Opt-Instruct introduces a framework for optimizing modeling tasks in Operations Research by generating high-quality fine-tuning data tailored to complex optimization problems. The framework utilizes iterative problem generation and stepwise validation to ensure data quality and prevent error propagation. By fine-tuning open-source LLMs like LLaMA-3-8B and Mistral-7B, Step-Opt model achieves state-of-the-art performance on various benchmarks such as NL4OPT, MAMO, and IndustryOR. The approach outperforms existing techniques, especially in addressing complex OR tasks, with a significant 17.01% improvement in micro average accuracy on difficult problems. These results demonstrate the effectiveness of combining structured validation with gradual problem refinement in advancing the automation of decision-making processes using LLMs. The code and dataset for the framework are available on Github for further exploration and application.

<br /><br />Summary: <div>
arXiv:2506.17637v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized various domains but encounter substantial challenges in tackling optimization modeling tasks for Operations Research (OR), particularly when dealing with complex problem. In this work, we propose Step-Opt-Instruct, a framework that augments existing datasets and generates high-quality fine-tuning data tailored to optimization modeling. Step-Opt-Instruct employs iterative problem generation to systematically increase problem complexity and stepwise validation to rigorously verify data, preventing error propagation and ensuring the quality of the generated dataset. Leveraging this framework, we fine-tune open-source LLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and IndustryOR. Extensive experiments demonstrate the superior performance of Step-Opt, especially in addressing complex OR tasks, with a notable 17.01\% improvement in micro average accuracy on difficult problems. These findings highlight the effectiveness of combining structured validation with gradual problem refinement to advance the automation of decision-making processes using LLMs.The code and dataset are available at https://github.com/samwu-learn/Step.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TPTT: Transforming Pretrained Transformer into Titans</title>
<link>https://arxiv.org/abs/2506.17671</link>
<guid>https://arxiv.org/abs/2506.17671</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Transformer models, memory management, efficient attention mechanisms, fine-tuning

Summary:
<br /><br />
Recent advances in natural language processing have been driven by large language models (LLMs), but their computational demands remain a challenge. A new framework, TPTT (Transforming Pretrained Transformer into Titans), enhances pretrained Transformer models with efficient linearized attention mechanisms and advanced memory management techniques. By utilizing Memory as Gate (MaG) and mixed linearized attention (LiZA), TPTT optimizes long-context inference while being fully compatible with the Hugging Face Transformers library. Through parameter-efficient fine-tuning (LoRA), TPTT enables significant improvements in efficiency and accuracy on the MMLU benchmark, with models reaching approximately 1 billion parameters. For example, Titans-Llama-3.2-1B achieved a 20% increase in Exact Match (EM) compared to its baseline. The practical scalability and robustness of TPTT are confirmed through statistical analyses and comparisons with state-of-the-art methods. The TPTT code is available on GitHub and as a Python package on PyPI.  <div>
arXiv:2506.17671v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have led to remarkable progress in natural language processing, but their computational and memory demands remain a significant challenge, particularly for long-context inference. We introduce TPTT (Transforming Pretrained Transformer into Titans), a novel framework for enhancing pretrained Transformer models with efficient linearized attention mechanisms and advanced memory management. TPTT employs techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA). It is fully compatible with the Hugging Face Transformers library, enabling seamless adaptation of any causal LLM through parameter-efficient fine-tuning (LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU benchmark with models of approximately 1 billion parameters, observing substantial improvements in both efficiency and accuracy. For instance, Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its baseline. Statistical analyses and comparisons with recent state-of-the-art methods confirm the practical scalability and robustness of TPTT. Code is available at https://github.com/fabienfrfr/tptt . Python package at https://pypi.org/project/tptt/ .
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2506.17692</link>
<guid>https://arxiv.org/abs/2506.17692</guid>
<content:encoded><![CDATA[
<div> Keyword: multi-hop question answering, large language models, DEC, hallucination-free reasoning chain, lightweight discriminative keyword extraction

Summary: 
The article introduces a novel framework called DEC (Dynamic Enhancement Chain) for knowledge-intensive multi-hop question answering tasks. DEC addresses challenges faced by lightweight language models with fewer parameters by decomposing complex questions into coherent subquestions and refining them iteratively through context-aware rewriting. A lightweight discriminative keyword extraction module is also introduced for targeted document retrieval with low computational cost. Experimental results show that DEC performs comparably or better than state-of-the-art benchmarks while reducing token consumption. The approach achieves state-of-the-art results on models with 8B parameters, highlighting its effectiveness in resource-constrained environments.<br /><br />Summary: <div>
arXiv:2506.17692v1 Announce Type: new 
Abstract: Knowledge-intensive multi-hop question answering (QA) tasks, which require integrating evidence from multiple sources to address complex queries, often necessitate multiple rounds of retrieval and iterative generation by large language models (LLMs). However, incorporating many documents and extended contexts poses challenges -such as hallucinations and semantic drift-for lightweight LLMs with fewer parameters. This work proposes a novel framework called DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions into logically coherent subquestions to form a hallucination-free reasoning chain. It then iteratively refines these subquestions through context-aware rewriting to generate effective query formulations. For retrieval, we introduce a lightweight discriminative keyword extraction module that leverages extracted keywords to achieve targeted, precise document recall with relatively low computational overhead. Extensive experiments on three multi-hop QA datasets demonstrate that DEC performs on par with or surpasses state-of-the-art benchmarks while significantly reducing token consumption. Notably, our approach attains state-of-the-art results on models with 8B parameters, showcasing its effectiveness in various scenarios, particularly in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Conversational Stance Detection: Dataset and Approaches</title>
<link>https://arxiv.org/abs/2506.17693</link>
<guid>https://arxiv.org/abs/2506.17693</guid>
<content:encoded><![CDATA[
<div> dataset, conversational stance detection, targets, zero-shot, SITPCL<br />
Summary:<br />
The article introduces a new dataset called ZS-CSD for conversational stance detection, which includes 280 targets of two types. A proposed model, SITPCL, leverages this dataset to achieve state-of-the-art performance in zero-shot conversational stance detection. Despite its success, the model only achieves an F1-macro score of 43.81%, highlighting the ongoing challenges in this area. The dataset aims to address limitations in existing datasets by expanding the scope of targets, enabling better performance in real-world applications. The SITPCL model considers speaker interaction and target awareness, improving accuracy in detecting public opinion in online conversations. Overall, the ZS-CSD dataset and SITPCL model contribute to advancements in conversational stance detection by addressing the issue of unseen targets and enhancing performance in a zero-shot setting.<br /><br />Summary: <div>
arXiv:2506.17693v1 Announce Type: new 
Abstract: Stance detection, which aims to identify public opinion towards specific targets using social media data, is an important yet challenging task. With the increasing number of online debates among social media users, conversational stance detection has become a crucial research area. However, existing conversational stance detection datasets are restricted to a limited set of specific targets, which constrains the effectiveness of stance detection models when encountering a large number of unseen targets in real-world applications. To bridge this gap, we manually curate a large-scale, high-quality zero-shot conversational stance detection dataset, named ZS-CSD, comprising 280 targets across two distinct target types. Leveraging the ZS-CSD dataset, we propose SITPCL, a speaker interaction and target-aware prototypical contrastive learning model, and establish the benchmark performance in the zero-shot setting. Experimental results demonstrate that our proposed SITPCL model achieves state-of-the-art performance in zero-shot conversational stance detection. Notably, the SITPCL model attains only an F1-macro score of 43.81%, highlighting the persistent challenges in zero-shot conversational stance detection.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future</title>
<link>https://arxiv.org/abs/2506.17700</link>
<guid>https://arxiv.org/abs/2506.17700</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Natural Language Processing, Prompt engineering, Prompt optimization strategies, NLP tasks

Summary: 
This paper discusses the impact of Large Language Models (LLMs) on Natural Language Processing (NLP) tasks, highlighting the importance of prompt engineering and optimization strategies. It identifies 11 distinct classes of prompt optimization strategies and examines their application in various NLP tasks using different LLMs and benchmark datasets. The comprehensive analysis provided in this paper serves as a valuable resource for future comparative studies and enables a thorough evaluation of prompt optimization techniques and LLM-based predictive pipelines. By centralizing strategic knowledge, the research aims to facilitate the adaptation of existing prompt optimization strategies for the development of innovative predictors across unexplored tasks.<br /><br /> <div>
arXiv:2506.17700v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP) by automating traditional labor-intensive tasks and consequently accelerated the development of computer-aided applications. As researchers continue to advance this field with the introduction of novel language models and more efficient training/finetuning methodologies, the idea of prompt engineering and subsequent optimization strategies with LLMs has emerged as a particularly impactful trend to yield a substantial performance boost across diverse NLP tasks. To best of our knowledge numerous review articles have explored prompt engineering, however, a critical gap exists in comprehensive analyses of prompt optimization strategies. To bridge this gap this paper provides unique and comprehensive insights about the potential of diverse prompt optimization strategies. It analyzes their underlying working paradigms and based on these principles, categorizes them into 11 distinct classes. Moreover, the paper provides details about various NLP tasks where these prompt optimization strategies have been employed, along with details of different LLMs and benchmark datasets used for evaluation. This comprehensive compilation lays a robust foundation for future comparative studies and enables rigorous assessment of prompt optimization and LLM-based predictive pipelines under consistent experimental settings: a critical need in the current landscape. Ultimately, this research will centralize diverse strategic knowledge to facilitate the adaptation of existing prompt optimization strategies for development of innovative predictors across unexplored tasks.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aged to Perfection: Machine-Learning Maps of Age in Conversational English</title>
<link>https://arxiv.org/abs/2506.17708</link>
<guid>https://arxiv.org/abs/2506.17708</guid>
<content:encoded><![CDATA[
<div> age groups, language patterns, British National Corpus, computational analysis, machine learning

Summary:<br /><br />
The study utilizes the British National Corpus 2014 to analyze language patterns across different age groups in contemporary spoken British English. It aims to investigate the relationship between speaker demographics and linguistic factors like utterance duration, lexical diversity, and word choice. By employing computational language analysis and machine learning techniques, the research strives to identify distinct linguistic markers unique to various generations and develop predictive models for estimating speaker age groups accurately. This research enhances our understanding of sociolinguistic diversity in contemporary British speech, shedding light on how language usage varies among different age demographics. <div>
arXiv:2506.17708v1 Announce Type: new 
Abstract: The study uses the British National Corpus 2014, a large sample of contemporary spoken British English, to investigate language patterns across different age groups. Our research attempts to explore how language patterns vary between different age groups, exploring the connection between speaker demographics and linguistic factors such as utterance duration, lexical diversity, and word choice. By merging computational language analysis and machine learning methodologies, we attempt to uncover distinctive linguistic markers characteristic of multiple generations and create prediction models that can consistently estimate the speaker's age group from various aspects. This work contributes to our knowledge of sociolinguistic diversity throughout the life of modern British speech.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages</title>
<link>https://arxiv.org/abs/2506.17715</link>
<guid>https://arxiv.org/abs/2506.17715</guid>
<content:encoded><![CDATA[
<div> Keywords: part-of-speech tagging, Medieval Romance languages, diachronic linguistic evolution, fine-tuning approaches, cross-lingual transfer learning

Summary:<br /><br />
- Part-of-speech tagging is crucial for analyzing historical texts in computational linguistics and digital humanities.
- Applying large language models to Medieval Romance languages is challenging due to linguistic evolution and data scarcity.
- The study explores the factors affecting POS tagging accuracy in Medieval Occitan, Medieval Spanish, and Medieval French texts.
- Experimentation reveals limitations in LLMs processing historical language variations and spelling, but also effective techniques for low-resource historical languages.
- Fine-tuning approaches, prompt engineering, model architectures, decoding strategies, and cross-lingual transfer learning impact tagging performance. 

Summary: <div>
arXiv:2506.17715v1 Announce Type: new 
Abstract: Part-of-speech (POS) tagging remains a foundational component in natural language processing pipelines, particularly critical for historical text analysis at the intersection of computational linguistics and digital humanities. Despite significant advancements in modern large language models (LLMs) for ancient languages, their application to Medieval Romance languages presents distinctive challenges stemming from diachronic linguistic evolution, spelling variations, and labeled data scarcity. This study systematically investigates the central determinants of POS tagging performance across diverse corpora of Medieval Occitan, Medieval Spanish, and Medieval French texts, spanning biblical, hagiographical, medical, and dietary domains. Through rigorous experimentation, we evaluate how fine-tuning approaches, prompt engineering, model architectures, decoding strategies, and cross-lingual transfer learning techniques affect tagging accuracy. Our results reveal both notable limitations in LLMs' ability to process historical language variations and non-standardized spelling, as well as promising specialized techniques that effectively address the unique challenges presented by low-resource historical languages.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process</title>
<link>https://arxiv.org/abs/2506.17728</link>
<guid>https://arxiv.org/abs/2506.17728</guid>
<content:encoded><![CDATA[
<div> Reasoning Framework, Large Language Model, Knowledge Retrieval, Logical Forms, Reasoning Analysis
<br />
Summary: 
KAG-Thinker introduces a human-like reasoning framework based on a parameter-light large language model (LLM) to improve logical coherence in question-answering tasks. The framework decomposes complex questions into sub-problems using breadth decomposition, distinguishing between Knowledge Retrieval and Reasoning Analysis tasks. It utilizes LLMs and external knowledge sources for knowledge retrieval, with a knowledge boundary model and depth solving model for optimal source selection and comprehensive knowledge acquisition. Supervised fine-tuning with multi-turn dialogues is employed for model alignment, avoiding excessive reflection. Data evaluation framework and iterative corpus synthesis support the generation of detailed reasoning trajectories. <div>
arXiv:2506.17728v1 Announce Type: new 
Abstract: In this paper, we introduce KAG-Thinker, a novel human-like reasoning framework built upon a parameter-light large language model (LLM). Our approach enhances the logical coherence and contextual consistency of the thinking process in question-answering (Q\&amp;A) tasks on domain-specific knowledge bases (KBs) within LLMs. This framework simulates human cognitive mechanisms for handling complex problems by establishing a structured thinking process. Continuing the \textbf{Logical Form} guided retrieval and reasoning technology route of KAG v0.7, firstly, it decomposes complex questions into independently solvable sub-problems(also referred to as logical forms) through \textbf{breadth decomposition}, each represented in two equivalent forms-natural language and logical function-and further classified as either Knowledge Retrieval or Reasoning Analysis tasks, with dependencies and variables passing explicitly modeled via logical function interfaces. In the solving process, the Retrieval function is used to perform knowledge retrieval tasks, while the Math and Deduce functions are used to perform reasoning analysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external knowledge sources are regarded as equivalent KBs. We use the \textbf{knowledge boundary} model to determine the optimal source using self-regulatory mechanisms such as confidence calibration and reflective reasoning, and use the \textbf{depth solving} model to enhance the comprehensiveness of knowledge acquisition. Finally, instead of utilizing reinforcement learning, we employ supervised fine-tuning with multi-turn dialogues to align the model with our structured inference paradigm, thereby avoiding excessive reflection. This is supported by a data evaluation framework and iterative corpus synthesis, which facilitate the generation of detailed reasoning trajectories...
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations</title>
<link>https://arxiv.org/abs/2506.17748</link>
<guid>https://arxiv.org/abs/2506.17748</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Models, Hallucination Detection, Hidden-State Representations, Factuality, Faithfulness

Summary: 
HIDE is a new approach for detecting hallucinations in Language Models (LMs) with a single-pass, training-free method. It leverages the statistical decoupling between input context and generated output to identify hallucinations using the Hilbert-Schmidt Independence Criterion (HSIC) on hidden-state representations. Extensive experiments on question answering datasets show that HIDE outperforms other single-pass methods with an average relative improvement of ~29% in AUC-ROC. It also performs competitively with multi-pass state-of-the-art methods, with an average relative improvement of ~3% in AUC-ROC and consuming ~51% less computation time. This approach demonstrates the effectiveness of exploiting internal representation decoupling in LMs for efficient and practical hallucination detection.<br /><br />Summary: <div>
arXiv:2506.17748v1 Announce Type: new 
Abstract: Contemporary Language Models (LMs), while impressively fluent, often generate content that is factually incorrect or unfaithful to the input context - a critical issue commonly referred to as 'hallucination'. This tendency of LMs to generate hallucinated content undermines their reliability, especially because these fabrications are often highly convincing and therefore difficult to detect. While several existing methods attempt to detect hallucinations, most rely on analyzing multiple generations per input, leading to increased computational cost and latency. To address this, we propose a single-pass, training-free approach for effective Hallucination detectIon via Decoupled rEpresentations (HIDE). Our approach leverages the hypothesis that hallucinations result from a statistical decoupling between an LM's internal representations of input context and its generated output. We quantify this decoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to hidden-state representations extracted while generating the output sequence. We conduct extensive experiments on four diverse question answering datasets, evaluating both faithfulness and factuality hallucinations across six open-source LMs of varying scales and properties. Our results demonstrate that HIDE outperforms other single-pass methods in almost all settings, achieving an average relative improvement of ~29% in AUC-ROC over the best-performing single-pass strategy across various models and datasets. Additionally, HIDE shows competitive and often superior performance with multi-pass state-of-the-art methods, obtaining an average relative improvement of ~3% in AUC-ROC while consuming ~51% less computation time. Our findings highlight the effectiveness of exploiting internal representation decoupling in LMs for efficient and practical hallucination detection.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights</title>
<link>https://arxiv.org/abs/2506.17789</link>
<guid>https://arxiv.org/abs/2506.17789</guid>
<content:encoded><![CDATA[
<div> tokenization, multilingual NLP, Indian languages, tokenizer algorithms, vocabulary sizes<br />
Summary:<br />
Tokenization is crucial in multilingual NLP, but current tokenizers are biased towards high-resource languages, limiting their effectiveness with linguistically diverse Indian languages. This study evaluates tokenization strategies across 17 Indian languages, comparing bottom-up (BPE) and top-down (Unigram LM) algorithms, vocabulary sizes, and multilingual vocabulary construction methods. It highlights the benefits of using tokenizers trained on high-resource languages for extremely low-resource languages. The research provides practical insights for developing fair, efficient, and linguistically informed tokenizers for multilingual NLP.<br /> <div>
arXiv:2506.17789v1 Announce Type: new 
Abstract: Tokenization plays a pivotal role in multilingual NLP. However, existing tokenizers are often skewed towards high-resource languages, limiting their effectiveness for linguistically diverse and morphologically rich languages such as those in the Indian subcontinent. This paper presents a comprehensive intrinsic evaluation of tokenization strategies across 17 Indian languages. We quantify the trade-offs between bottom-up and top-down tokenizer algorithms (BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of multilingual vocabulary construction such as joint and cluster-based training. We also show that extremely low-resource languages can benefit from tokenizers trained on related high-resource languages. Our study provides practical insights for building more fair, efficient, and linguistically informed tokenizers for multilingual NLP.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction</title>
<link>https://arxiv.org/abs/2506.17844</link>
<guid>https://arxiv.org/abs/2506.17844</guid>
<content:encoded><![CDATA[
<div> causal model, electronic health records, clinical risk prediction, multimodal data, conformal calibration

Summary:
THCM-CAL is a novel Temporal-Hierarchical Causal Model with Conformal Calibration for automated clinical risk prediction from electronic health records (EHRs). It considers both structured diagnostic codes and unstructured narrative notes, modeling their interactions in a multimodal causal graph. The model infers intra-slice same-modality sequencing, intra-slice cross-modality triggers, and inter-slice risk propagation, capturing the directional, hierarchical causal relationships in clinical data. Additionally, THCM-CAL extends conformal prediction to multi-label ICD coding, enhancing prediction reliability by calibrating per-code confidence intervals under complex co-occurrences. Experimental results on MIMIC-III and MIMIC-IV datasets demonstrate the effectiveness of THCM-CAL in comparison to existing approaches, showcasing its superiority in clinical risk prediction from EHRs. <br /><br />Summary: <div>
arXiv:2506.17844v1 Announce Type: new 
Abstract: Automated clinical risk prediction from electronic health records (EHRs) demands modeling both structured diagnostic codes and unstructured narrative notes. However, most prior approaches either handle these modalities separately or rely on simplistic fusion strategies that ignore the directional, hierarchical causal interactions by which narrative observations precipitate diagnoses and propagate risk across admissions. In this paper, we propose THCM-CAL, a Temporal-Hierarchical Causal Model with Conformal Calibration. Our framework constructs a multimodal causal graph where nodes represent clinical entities from two modalities: Textual propositions extracted from notes and ICD codes mapped to textual descriptions. Through hierarchical causal discovery, THCM-CAL infers three clinically grounded interactions: intra-slice same-modality sequencing, intra-slice cross-modality triggers, and inter-slice risk propagation. To enhance prediction reliability, we extend conformal prediction to multi-label ICD coding, calibrating per-code confidence intervals under complex co-occurrences. Experimental results on MIMIC-III and MIMIC-IV demonstrate the superiority of THCM-CAL.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs for Customized Marketing Content Generation and Evaluation at Scale</title>
<link>https://arxiv.org/abs/2506.17863</link>
<guid>https://arxiv.org/abs/2506.17863</guid>
<content:encoded><![CDATA[
<div> keyword-specific ad copy, offsite marketing, MarketingFM, AutoEval-Main, AutoEval-Update

Summary: 
Offsite marketing plays a crucial role in e-commerce by driving traffic to retail websites via external platforms. Current offsite marketing content is often generic and template-based, leading to inefficiencies. MarketingFM is a system that generates keyword-specific ad copy with minimal human intervention, resulting in higher CTR, more impressions, and lower CPC compared to traditional templates. However, human review of generated ads can be costly. AutoEval-Main is proposed as an automated evaluation system to ensure alignment with marketing principles, achieving high agreement with human reviewers. AutoEval-Update further enhances evaluation consistency by using a critic LLM to suggest refinements and reduce manual effort. Human oversight remains important for setting thresholds and validating refinements, highlighting the collaboration between humans and AI in improving offsite marketing effectiveness. <div>
arXiv:2506.17863v1 Announce Type: new 
Abstract: Offsite marketing is essential in e-commerce, enabling businesses to reach customers through external platforms and drive traffic to retail websites. However, most current offsite marketing content is overly generic, template-based, and poorly aligned with landing pages, limiting its effectiveness. To address these limitations, we propose MarketingFM, a retrieval-augmented system that integrates multiple data sources to generate keyword-specific ad copy with minimal human intervention. We validate MarketingFM via offline human and automated evaluations and large-scale online A/B tests. In one experiment, keyword-focused ad copy outperformed templates, achieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC, demonstrating gains in ad ranking and cost efficiency. Despite these gains, human review of generated ads remains costly. To address this, we propose AutoEval-Main, an automated evaluation system that combines rule-based metrics with LLM-as-a-Judge techniques to ensure alignment with marketing principles. In experiments with large-scale human annotations, AutoEval-Main achieved 89.57% agreement with human reviewers. Building on this, we propose AutoEval-Update, a cost-efficient LLM-human collaborative framework to dynamically refine evaluation prompts and adapt to shifting criteria with minimal human input. By selectively sampling representative ads for human review and using a critic LLM to generate alignment reports, AutoEval-Update improves evaluation consistency while reducing manual effort. Experiments show the critic LLM suggests meaningful refinements, improving LLM-human agreement. Nonetheless, human oversight remains essential for setting thresholds and validating refinements before deployment.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs</title>
<link>https://arxiv.org/abs/2506.17864</link>
<guid>https://arxiv.org/abs/2506.17864</guid>
<content:encoded><![CDATA[
<div> framework, sequential model editing, large language models, self-correction, queue-based <br />
<br />
Summary: 
The paper introduces a queue-based self-correction framework, QueueEDIT, to address errors in large language models through sequential model editing. The framework incorporates a structural mapping editing loss to identify and store relevant parameters in a queue, allowing for dynamic alignment of edited knowledge. Queue parameters are selectively updated to prevent bias and maintain the LLM's general capabilities. Experimental results demonstrate QueueEDIT's superiority in sequential editing tasks and competitiveness in single-turn editing. Furthermore, the proposed framework ensures consistent performance in general NLP tasks throughout the editing process. <div>
arXiv:2506.17864v1 Announce Type: new 
Abstract: Recently, large language models (LLMs) have demonstrated impressive results but still suffer from hallucinations. Model editing has been proposed to correct factual inaccuracies in LLMs. A challenging case is sequential model editing (SME), which aims to rectify errors continuously rather than treating them as a one-time task. During SME, the general capabilities of LLMs can be negatively affected due to the introduction of new parameters. In this paper, we propose a queue-based self-correction framework (QueueEDIT) that not only enhances SME performance by addressing long-sequence dependency but also mitigates the impact of parameter bias on the general capabilities of LLMs. Specifically, we first introduce a structural mapping editing loss to map the triplets to the knowledge-sensitive neurons within the Transformer layers of LLMs. We then store the located parameters for each piece of edited knowledge in a queue and dynamically align previously edited parameters. In each edit, we select queue parameters most relevant to the currently located parameters to determine whether previous knowledge needs realignment. Irrelevant parameters in the queue are frozen, and we update the parameters at the queue head to the LLM to ensure they do not harm general abilities. Experiments show that our framework significantly outperforms strong baselines across various SME settings and maintains competitiveness in single-turn editing. The resulting LLMs also preserve high capabilities in general NLP tasks throughout the SME process.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Alignment Shrinks the Generative Horizon</title>
<link>https://arxiv.org/abs/2506.17871</link>
<guid>https://arxiv.org/abs/2506.17871</guid>
<content:encoded><![CDATA[
<div> branching factor, large language models, generation, alignment tuning, stability

Summary: 
- Large language models (LLMs) often lack diversity in their outputs due to probability concentration.
- The Branching Factor (BF) measures the effective number of plausible next steps during generation.
- BF tends to decrease as generation progresses, making LLMs more predictable.
- Alignment tuning sharpens the output distribution, reducing BF and making models less sensitive to decoding strategies.
- Aligned Chain-of-Thought (CoT) models leverage low BF stages for stable outputs through longer reasoning chains.

<br /><br />Summary: <div>
arXiv:2506.17871v1 Announce Type: new 
Abstract: Despite their impressive capabilities, aligned large language models (LLMs) often generate outputs that lack diversity. What drives this stability in the generation? We investigate this phenomenon through the lens of probability concentration in the model's output distribution. To quantify this concentration, we introduce the Branching Factor (BF) -- a token-invariant measure of the effective number of plausible next steps during generation. Our empirical analysis reveals two key findings: (1) BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. (2) alignment tuning substantially sharpens the model's output distribution from the outset, reducing BF by nearly an order of magnitude (e.g., from 12 to 1.2) relative to base models. This stark reduction helps explain why aligned models often appear less sensitive to decoding strategies. Building on this insight, we find this stability has surprising implications for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g., DeepSeek-distilled models), for instance, leverage this effect; by generating longer reasoning chains, they push generation into later, more deterministic (lower BF) stages, resulting in more stable outputs. We hypothesize that alignment tuning does not fundamentally change a model's behavior, but instead steers it toward stylistic tokens (e.g., "Sure") that unlock low-entropy trajectories already present in the base model. This view is supported by nudging experiments, which show that prompting base models with such tokens can similarly reduce BF. Together, our findings establish BF as a powerful diagnostic for understanding and controlling LLM outputs - clarifying how alignment reduces variability, how CoT promotes stable generations, and how base models can be steered away from diversity.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-turn Jailbreaking via Global Refinement and Active Fabrication</title>
<link>https://arxiv.org/abs/2506.17881</link>
<guid>https://arxiv.org/abs/2506.17881</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, jailbreaking, multi-turn scenarios, security threats, active fabrication

Summary: 
Large Language Models (LLMs) have shown exceptional performance but also pose significant safety risks from potential misuse. Jailbreaks, targeting harmful content generation, are crucial in identifying security threats. Current jailbreaking techniques mainly focus on single-turn scenarios, leaving multi-turn scenarios underexplored. Existing multi-turn methods struggle to adapt to evolving dialogue dynamics. To address this, a new multi-turn jailbreaking method is proposed, refining the jailbreaking path globally at each interaction. The method actively fabricates model responses to suppress safety warnings, increasing the chances of eliciting harmful outputs in subsequent questions. Experimental results show the superiority of this approach over existing techniques across six state-of-the-art LLMs. The code is publicly available for further exploration. 

<br /><br />Summary: 
- LLMs have high performance but carry safety risks
- Jailbreaks aim to identify security threats from misuse
- Current focus is on single-turn scenarios, neglecting multi-turn scenarios
- Proposed method refines global jailbreaking path at each interaction
- Active fabrication of model responses boosts chances of harmful outputs <div>
arXiv:2506.17881v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved exceptional performance across a wide range of tasks. However, they still pose significant safety risks due to the potential misuse for malicious purposes. Jailbreaks, which aim to elicit models to generate harmful content, play a critical role in identifying the underlying security threats. Recent jailbreaking primarily focuses on single-turn scenarios, while the more complicated multi-turn scenarios remain underexplored. Moreover, existing multi-turn jailbreaking techniques struggle to adapt to the evolving dynamics of dialogue as the interaction progresses. To address this limitation, we propose a novel multi-turn jailbreaking method that refines the jailbreaking path globally at each interaction. We also actively fabricate model responses to suppress safety-related warnings, thereby increasing the likelihood of eliciting harmful outputs in subsequent questions. Experimental results demonstrate the superior performance of our method compared with existing single-turn and multi-turn jailbreaking techniques across six state-of-the-art LLMs. Our code is publicly available at https://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation</title>
<link>https://arxiv.org/abs/2506.17949</link>
<guid>https://arxiv.org/abs/2506.17949</guid>
<content:encoded><![CDATA[
<div> innovation scatter model, Large Language Models, generalization, novel ideas, multi-stage process

Summary:
The paper introduces the innovation scatter model to address the challenge of applying localized innovations from one stage or component to other parts of a multi-stage process using Large Language Models (LLMs). The model involves a four-step process: identifying the core innovation, generalizing it, determining its broader applicability, and systematically applying it to structurally similar stages. By leveraging structural redundancy across stages, the innovation scatter model enables LLMs to extend innovations, enhance generalization, and promote reuse. Verification results demonstrate the effectiveness of this approach in improving the applicability of novel ideas and extending them across stages. <div>
arXiv:2506.17949v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit strong capabilities in reproducing and extending patterns observed during pretraining but often struggle to generalize novel ideas beyond their original context. This paper addresses the challenge of applying such localized innovations - introduced at a specific stage or component - to other parts of a multi-stage process. We propose a scatter-based innovation expansion model (innovation scatter model) that guides the LLM through a four-step process: (1) identifying the core innovation by comparing the user's input with its surrounding context, (2) generalizing the innovation by removing references to specific stages or components, (3) determining whether the generalized innovation applies to a broader scope beyond the original stage, and (4) systematically applying it to other structurally similar stages using the LLM. This model leverages structural redundancy across stages to improve the applicability of novel ideas. Verification results demonstrate that the innovation scatter model enables LLMs to extend innovations across structurally similar stages, thereby enhancing generalization and reuse.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment</title>
<link>https://arxiv.org/abs/2506.17951</link>
<guid>https://arxiv.org/abs/2506.17951</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval-augmented generation, GraphMPA, question answering, external knowledge, mode-seeking preference alignment

Summary: 
GraphMPA is a novel graph-based framework designed to enhance large language models in question answering by incorporating external knowledge and aligning responses with human ethical and quality preferences. The framework constructs a hierarchical document graph using a general similarity measurement, thereby mimicking human cognitive processes for information understanding and synthesis. Additionally, GraphMPA introduces mode-seeking preference optimization to improve the alignment of model outputs with human preferences through probability-matching constraints. Extensive experiments conducted on six datasets demonstrate the effectiveness of GraphMPA in achieving global understanding and aligning responses with human preferences in question answering tasks. This innovative approach addresses challenges in current retrieval-augmented generation models and offers a comprehensive solution for improving the quality and ethical considerations of generated responses.<br /><br />Summary: <div>
arXiv:2506.17951v1 Announce Type: new 
Abstract: Recent advancements in retrieval-augmented generation (RAG) have enhanced large language models in question answering by integrating external knowledge. However, challenges persist in achieving global understanding and aligning responses with human ethical and quality preferences. To address these issues, we propose GraphMPA, a comprehensive graph-based framework with mode-seeking preference alignment. Our approach constructs a hierarchical document graph using a general similarity measurement, mimicking human cognitive processes for information understanding and synthesis. Additionally, we introduce mode-seeking preference optimization to better align model outputs with human preferences through probability-matching constraints. Extensive experiments on six datasets demonstrate the effectiveness of our \href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PDF Retrieval Augmented Question Answering</title>
<link>https://arxiv.org/abs/2506.18027</link>
<guid>https://arxiv.org/abs/2506.18027</guid>
<content:encoded><![CDATA[
<div> Keywords: Question-Answering, Retrieval Augmented Generation, PDF files, multimodal data, large language models

Summary: 
This paper introduces a novel approach to Question-Answering (QA) systems by utilizing a Retrieval Augmented Generation (RAG) framework to extract information from PDF files. Due to the diverse data types present in PDFs, including text, images, graphs, and tables, existing QA systems face challenges in effectively processing this information. The proposed RAG-based system aims to address these challenges by enhancing the integration of non-textual elements and fine-tuning large language models. Experimental evaluations demonstrate the system's ability to accurately extract information across various types of content in PDFs. This advancement not only expands the capabilities of retrieval-augmented QA systems but also paves the way for future research in multimodal data integration and processing.
<br /><br />Summary: <div>
arXiv:2506.18027v1 Announce Type: new 
Abstract: This paper presents an advancement in Question-Answering (QA) systems using a Retrieval Augmented Generation (RAG) framework to enhance information extraction from PDF files. Recognizing the richness and diversity of data within PDFs--including text, images, vector diagrams, graphs, and tables--poses unique challenges for existing QA systems primarily designed for textual content. We seek to develop a comprehensive RAG-based QA system that will effectively address complex multimodal questions, where several data types are combined in the query. This is mainly achieved by refining approaches to processing and integrating non-textual elements in PDFs into the RAG framework to derive precise and relevant answers, as well as fine-tuning large language models to better adapt to our system. We provide an in-depth experimental evaluation of our solution, demonstrating its capability to extract accurate information that can be applied to different types of content across PDFs. This work not only pushes the boundaries of retrieval-augmented QA systems but also lays a foundation for further research in multimodal data integration and processing.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Splitformer: An improved early-exit architecture for automatic speech recognition on edge devices</title>
<link>https://arxiv.org/abs/2506.18035</link>
<guid>https://arxiv.org/abs/2506.18035</guid>
<content:encoded><![CDATA[
<div> early-exit architectures, resource-aware processing, on-device processing, automatic speech recognition, neural architectures
Summary:
The article introduces a method to improve the performance of early-exit models for on-device processing scenarios. It addresses the need for dynamically adjusting computational load in neural models by proposing the use of parallel layers that process downsampled inputs alongside standard processing layers in the architecture. This approach aims to enhance speech recognition performance on well-established benchmarks without affecting inference time. The method involves introducing modularity into memory-efficient neural architectures like Zipformer, which traditionally lack the ability to incorporate early-exit branches. By utilizing parallel layers for processing downsampled inputs, the proposed approach shows significant performance improvements in speech recognition, albeit with a small increase in model parameters. This enhancement showcases the potential for optimizing neural models for resource-aware processing tasks. 

<br /><br />Summary: <div>
arXiv:2506.18035v1 Announce Type: new 
Abstract: The ability to dynamically adjust the computational load of neural models during inference in a resource aware manner is crucial for on-device processing scenarios, characterised by limited and time-varying computational resources. Early-exit architectures represent an elegant and effective solution, since they can process the input with a subset of their layers, exiting at intermediate branches (the upmost layers are hence removed from the model).
  From a different perspective, for automatic speech recognition applications there are memory-efficient neural architectures that apply variable frame rate analysis, through downsampling/upsampling operations in the middle layers, reducing the overall number of operations and improving significantly the performance on well established benchmarks. One example is the Zipformer. However, these architectures lack the modularity necessary to inject early-exit branches.
  With the aim of improving the performance in early-exit models, we propose introducing parallel layers in the architecture that process downsampled versions of their inputs. % in conjunction with standard processing layers. We show that in this way the speech recognition performance on standard benchmarks significantly improve, at the cost of a small increase in the overall number of model parameters but without affecting the inference time.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models</title>
<link>https://arxiv.org/abs/2506.18036</link>
<guid>https://arxiv.org/abs/2506.18036</guid>
<content:encoded><![CDATA[
<div> Keywords: automatic text summarization, extractive, abstractive, large language models, hybrid summarization approach

Summary: 
The article discusses the need for effective automatic text summarization due to the vast amount of information available. It highlights the two main types of summarization methods - extractive and abstractive. While large language models have advanced abstractive summarization, they struggle to retain key information in lengthy documents, resulting in information being "lost in the middle." To address this challenge, a hybrid summarization approach is proposed. This approach involves splitting the document into smaller chunks, clustering their vector embeddings, generating summaries for each cluster to capture key ideas, and utilizing a Markov chain graph to select the semantic order of ideas in the final summary. <div>
arXiv:2506.18036v1 Announce Type: new 
Abstract: The rapid expansion of information from diverse sources has heightened the need for effective automatic text summarization, which condenses documents into shorter, coherent texts. Summarization methods generally fall into two categories: extractive, which selects key segments from the original text, and abstractive, which generates summaries by rephrasing the content coherently. Large language models have advanced the field of abstractive summarization, but they are resourceintensive and face significant challenges in retaining key information across lengthy documents, which we call being "lost in the middle". To address these issues, we propose a hybrid summarization approach that combines extractive and abstractive techniques. Our method splits the document into smaller text chunks, clusters their vector embeddings, generates a summary for each cluster that represents a key idea in the document, and constructs the final summary by relying on a Markov chain graph when selecting the semantic order of ideas.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Multicriteria Evaluation of LLM-Generated Text</title>
<link>https://arxiv.org/abs/2506.18082</link>
<guid>https://arxiv.org/abs/2506.18082</guid>
<content:encoded><![CDATA[
<div> Framework, LLM-generated text, quality evaluation, Generalized Stochastic Dominance, decoding strategies

Summary:
- The quality of LLM-generated text is a challenge in NLP.
- Current evaluation methods are limited in assessing nuances in text quality.
- A new framework based on Generalized Stochastic Dominance (GSD) is proposed.
- The GSD-front approach simultaneously evaluates multiple quality dimensions.
- It addresses limitations in single-metric evaluation and disparity between automatic metrics and human judgments.
- The framework provides inferential statistical guarantees, identifying significant performance differences.
- It avoids arbitrary weighting of metrics by utilizing partial orders of decoding strategies.
- The approach is effective in comparing common decoding strategies against human-generated text.
- It accommodates potential deviations from the sampling design assumption. 

<br /><br />Summary: <div>
arXiv:2506.18082v1 Announce Type: new 
Abstract: Assessing the quality of LLM-generated text remains a fundamental challenge in natural language processing. Current evaluation approaches often rely on isolated metrics or simplistic aggregations that fail to capture the nuanced trade-offs between coherence, diversity, fluency, and other relevant indicators of text quality. In this work, we adapt a recently proposed framework for statistical inference based on Generalized Stochastic Dominance (GSD) that addresses three critical limitations in existing benchmarking methodologies: the inadequacy of single-metric evaluation, the incompatibility between cardinal automatic metrics and ordinal human judgments, and the lack of inferential statistical guarantees. The GSD-front approach enables simultaneous evaluation across multiple quality dimensions while respecting their different measurement scales, building upon partial orders of decoding strategies, thus avoiding arbitrary weighting of the involved metrics. By applying this framework to evaluate common decoding strategies against human-generated text, we demonstrate its ability to identify statistically significant performance differences while accounting for potential deviations from the i.i.d. assumption of the sampling design.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution</title>
<link>https://arxiv.org/abs/2506.18091</link>
<guid>https://arxiv.org/abs/2506.18091</guid>
<content:encoded><![CDATA[
<div> Keywords: anaphora resolution, Czech text, language models, prompt engineering, fine-tuning

Summary: 
- The paper evaluates two approaches to anaphora resolution on Czech text: prompt engineering with large language models (LLMs) and fine-tuning compact generative models.
- LLMs like Mistral Large 2 and Llama 3 using prompt templates achieved promising few-shot results with up to 74.5% accuracy.
- Fine-tuned models, particularly mT5-large, outperformed prompt-based approaches with up to 88% accuracy and required fewer computational resources. 
- The study analyzed performance differences across various anaphora types, antecedent distances, and source corpora, highlighting strengths and trade-offs of each approach. 
- The findings indicate that fine-tuned models are more effective for Czech anaphora resolution, achieving higher accuracy levels than prompt-based methods, with mT5-large being particularly successful. 

<br /><br />Summary: <div>
arXiv:2506.18091v1 Announce Type: new 
Abstract: Anaphora resolution plays a critical role in natural language understanding, especially in morphologically rich languages like Czech. This paper presents a comparative evaluation of two modern approaches to anaphora resolution on Czech text: prompt engineering with large language models (LLMs) and fine-tuning compact generative models. Using a dataset derived from the Prague Dependency Treebank, we evaluate several instruction-tuned LLMs, including Mistral Large 2 and Llama 3, using a series of prompt templates. We compare them against fine-tuned variants of the mT5 and Mistral models that we trained specifically for Czech anaphora resolution. Our experiments demonstrate that while prompting yields promising few-shot results (up to 74.5% accuracy), the fine-tuned models, particularly mT5-large, outperform them significantly, achieving up to 88% accuracy while requiring fewer computational resources. We analyze performance across different anaphora types, antecedent distances, and source corpora, highlighting key strengths and trade-offs of each approach.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating</title>
<link>https://arxiv.org/abs/2506.18102</link>
<guid>https://arxiv.org/abs/2506.18102</guid>
<content:encoded><![CDATA[
<div> evaluation system, multi-dimensional assessment, debating framework, CoT reasoning, optimization approach

Summary:
The article introduces a dual-component framework for improving large language model-based debating systems. The framework includes InspireScore, an evaluation system incorporating subjective criteria and objective metrics for assessing argument quality, and InspireDebate, an optimized debating framework. InspireScore achieves higher correlation with expert judgments compared to existing methods, while InspireDebate demonstrates significant improvements over baseline models. InspireDebate employs a phased optimization approach, enhances chain-of-thought reasoning, utilizes multi-dimensional direct preference optimization, and grounds knowledge in real-time through web-based retrieval augmented generation. The framework addresses challenges in existing LLM-based debating systems by providing a structured approach to optimize evaluation metrics, reasoning processes, and debate refinement, ultimately enhancing the effectiveness of the systems. (200 words) 

Summary: <br /><br />The article introduces a dual-component framework for improving large language model-based debating systems. The framework includes InspireScore, an evaluation system incorporating subjective criteria and objective metrics for assessing argument quality, and InspireDebate, an optimized debating framework. InspireScore achieves higher correlation with expert judgments compared to existing methods, while InspireDebate demonstrates significant improvements over baseline models. InspireDebate employs a phased optimization approach, enhances chain-of-thought reasoning, utilizes multi-dimensional direct preference optimization, and grounds knowledge in real-time through web-based retrieval augmented generation. The framework addresses challenges in existing LLM-based debating systems by providing a structured approach to optimize evaluation metrics, reasoning processes, and debate refinement, ultimately enhancing the effectiveness of the systems. <div>
arXiv:2506.18102v1 Announce Type: new 
Abstract: With the rapid advancements in large language models (LLMs), debating tasks, such as argument quality assessment and debate process simulation, have made significant progress. However, existing LLM-based debating systems focus on responding to specific arguments while neglecting objective assessments such as authenticity and logical validity. Furthermore, these systems lack a structured approach to optimize across various dimensions$-$including evaluation metrics, chain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby limiting their effectiveness. To address these interconnected challenges, we propose a dual-component framework: (1) $\textbf{InspireScore}$, a novel evaluation system that establishes a multi-dimensional assessment architecture incorporating four subjective criteria (emotional appeal, argument clarity, argument arrangement, and topic relevance) alongside two objective metrics (fact authenticity and logical validity); and (2) $\textbf{InspireDebate}$, an optimized debating framework employing a phased optimization approach through CoT reasoning enhancement, multi-dimensional Direct Preference Optimization (DPO), and real-time knowledge grounding via web-based Retrieval Augmented Generation (Web-RAG). Empirical evaluations demonstrate that $\textbf{InspireScore}$ achieves 44$\%$ higher correlation with expert judgments compared to existing methods, while $\textbf{InspireDebate}$ shows significant improvements, outperforming baseline models by 57$\%$. Source code is available at https://github.com/fywang12/InspireDebate.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use</title>
<link>https://arxiv.org/abs/2506.18105</link>
<guid>https://arxiv.org/abs/2506.18105</guid>
<content:encoded><![CDATA[
<div> idioms, Chengyu-Bench, language models, cultural nuances, benchmark<br />
Summary:<br />
The article introduces Chengyu-Bench, a benchmark that focuses on the correct usage of Chinese idioms, known as Chengyu, challenging for language models due to their cultural and historical complexity. Chengyu-Bench includes tasks like Evaluative Connotation, Appropriateness, and Open Cloze, evaluating popular Language Models (LLMs) on these tasks. Results show high accuracy in evaluating idiom sentiment but lower accuracy in determining appropriateness and completing open cloze tasks. Errors mostly stem from a lack of understanding of idiom meanings, indicating a struggle in grasping cultural and contextual nuances. This highlights the limitations of LLMs in interpreting and using idioms correctly. The Chengyu-Bench benchmark and source code are available for further research and development. <br /><br />Summary: <div>
arXiv:2506.18105v1 Announce Type: new 
Abstract: Chinese idioms (Chengyu) are concise four-character expressions steeped in history and culture, whose literal translations often fail to capture their full meaning. This complexity makes them challenging for language models to interpret and use correctly. Existing benchmarks focus on narrow tasks - multiple-choice cloze tests, isolated translation, or simple paraphrasing. We introduce Chengyu-Bench, a comprehensive benchmark featuring three tasks: (1) Evaluative Connotation, classifying idioms as positive or negative; (2) Appropriateness, detecting incorrect idiom usage in context; and (3) Open Cloze, filling blanks in longer passages without options. Chengyu-Bench comprises 2,937 human-verified examples covering 1,765 common idioms sourced from diverse corpora. We evaluate leading LLMs and find they achieve over 95% accuracy on Evaluative Connotation, but only ~85% on Appropriateness and ~40% top-1 accuracy on Open Cloze. Error analysis reveals that most mistakes arise from fundamental misunderstandings of idiom meanings. Chengyu-Bench demonstrates that while LLMs can reliably gauge idiom sentiment, they still struggle to grasp the cultural and contextual nuances essential for proper usage. The benchmark and source code are available at: https://github.com/sofyc/ChengyuBench.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives</title>
<link>https://arxiv.org/abs/2506.18116</link>
<guid>https://arxiv.org/abs/2506.18116</guid>
<content:encoded><![CDATA[
<div> MHQA, bias detection, intersectional biases, LLMs, mental healthcare <br />
<br />
Summary: 
The study introduces a multi-hop question answering (MHQA) framework to investigate biases in Large Language Models (LLMs) in mental healthcare discourse. By analyzing the Interpretable Mental Health Instruction (IMHI) dataset, the researchers identify systematic disparities across sentiment, demographics, and mental health conditions in LLM responses. Utilizing systematic tagging for age, race, gender, and socioeconomic status, the study reveals how biases can magnify through sequential reasoning in LLMs. Two debiasing techniques, Roleplay Simulation and Explicit Bias Reduction, are implemented and lead to significant reductions in bias by utilizing the BBQ dataset examples for few-shot prompting. The findings emphasize the importance of addressing biases in LLMs to promote equitable AI development in mental healthcare. <br /><br /> <div>
arXiv:2506.18116v1 Announce Type: new 
Abstract: Large Language Models (LLMs) in mental healthcare risk propagating biases that reinforce stigma and harm marginalized groups. While previous research identified concerning trends, systematic methods for detecting intersectional biases remain limited. This work introduces a multi-hop question answering (MHQA) framework to explore LLM response biases in mental health discourse. We analyze content from the Interpretable Mental Health Instruction (IMHI) dataset across symptom presentation, coping mechanisms, and treatment approaches. Using systematic tagging across age, race, gender, and socioeconomic status, we investigate bias patterns at demographic intersections. We evaluate four LLMs: Claude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic disparities across sentiment, demographics, and mental health conditions. Our MHQA approach demonstrates superior detection compared to conventional methods, identifying amplification points where biases magnify through sequential reasoning. We implement two debiasing techniques: Roleplay Simulation and Explicit Bias Reduction, achieving 66-94% bias reductions through few-shot prompting with BBQ dataset examples. These findings highlight critical areas where LLMs reproduce mental healthcare biases, providing actionable insights for equitable AI development.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of English</title>
<link>https://arxiv.org/abs/2506.18120</link>
<guid>https://arxiv.org/abs/2506.18120</guid>
<content:encoded><![CDATA[
<div> Dataset, Syntax, Computational linguistics, Well-formedness, Acceptability <br />
<br />
Summary: 
The article introduces the Syntactic Acceptability Dataset, a resource for syntax and computational linguistics research. The dataset consists of 1,000 English sequences from textbooks and the journal Linguistic Inquiry, labeled with grammatical and acceptability status. Preliminary analyses show that grammaticality and acceptability judgments align in 83% of cases, with "in-betweenness" common. Machine learning models struggle with predicting grammaticality but perform better at predicting acceptability. This research provides insights into the relationship between syntactic formalisms, native speaker perceptions, and machine learning models. Future work will focus on expanding the dataset to further explore these findings. <br />  <div>
arXiv:2506.18120v1 Announce Type: new 
Abstract: We present a preview of the Syntactic Acceptability Dataset, a resource being designed for both syntax and computational linguistics research. In its current form, the dataset comprises 1,000 English sequences from the syntactic discourse: Half from textbooks and half from the journal Linguistic Inquiry, the latter to ensure a representation of the contemporary discourse. Each entry is labeled with its grammatical status ("well-formedness" according to syntactic formalisms) extracted from the literature, as well as its acceptability status ("intuitive goodness" as determined by native speakers) obtained through crowdsourcing, with highest experimental standards. Even in its preliminary form, this dataset stands as the largest of its kind that is publicly accessible. We also offer preliminary analyses addressing three debates in linguistics and computational linguistics: We observe that grammaticality and acceptability judgments converge in about 83% of the cases and that "in-betweenness" occurs frequently. This corroborates existing research. We also find that while machine learning models struggle with predicting grammaticality, they perform considerably better in predicting acceptability. This is a novel finding. Future work will focus on expanding the dataset.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\phi^{\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models</title>
<link>https://arxiv.org/abs/2506.18129</link>
<guid>https://arxiv.org/abs/2506.18129</guid>
<content:encoded><![CDATA[
<div> Keywords: autoregressive transformer, em dash, semantic drift, clause boundary hallucination, embedding space

Summary: 
In this study, a critical vulnerability in autoregressive transformer language models is identified, specifically related to the em dash token. The insertion of the em dash leads to recursive semantic drift, causing issues such as clause boundary hallucination and embedding space entanglement. By analyzing token-level perturbations, the researchers show how the em dash alters the model's latent representations, resulting in errors in long-form generation. A novel solution is proposed, involving symbolic clause purification and embedding matrix realignment, which effectively suppresses problematic tokens without the need for model retraining. Experimental validation demonstrates improvements in generation consistency and topic maintenance. This work not only addresses punctuation-related vulnerabilities but also provides a framework for mitigating broader classes of recursive instabilities in neural text generation systems.

<br /><br />Summary: <div>
arXiv:2506.18129v1 Announce Type: new 
Abstract: We identify a critical vulnerability in autoregressive transformer language models where the em dash token induces recursive semantic drift, leading to clause boundary hallucination and embedding space entanglement. Through formal analysis of token-level perturbations in semantic lattices, we demonstrate that em dash insertion fundamentally alters the model's latent representations, causing compounding errors in long-form generation. We propose a novel solution combining symbolic clause purification via the phi-infinity operator with targeted embedding matrix realignment. Our approach enables total suppression of problematic tokens without requiring model retraining, while preserving semantic coherence through fixed-point convergence guarantees. Experimental validation shows significant improvements in generation consistency and topic maintenance. This work establishes a general framework for identifying and mitigating token-level vulnerabilities in foundation models, with immediate implications for AI safety, model alignment, and robust deployment of large language models in production environments. The methodology extends beyond punctuation to address broader classes of recursive instabilities in neural text generation systems.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models</title>
<link>https://arxiv.org/abs/2506.18141</link>
<guid>https://arxiv.org/abs/2506.18141</guid>
<content:encoded><![CDATA[
arXiv:2506.18141v1 Announce Type: new 
Abstract: We identify semantically coherent, context-consistent network components in large language models (LLMs) using coactivation of sparse autoencoder (SAE) features collected from just a handful of prompts. Focusing on country-relation tasks, we show that ablating semantic components for countries and relations changes model outputs in predictable ways, while amplifying these components induces counterfactual responses. Notably, composing relation and country components yields compound counterfactual outputs. We find that, whereas most country components emerge from the very first layer, the more abstract relation components are concentrated in later layers. Furthermore, within relation components themselves, nodes from later layers tend to have a stronger causal impact on model outputs. Overall, these findings suggest a modular organization of knowledge within LLMs and advance methods for efficient, targeted model manipulation.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuranMorph: Morphologically Annotated Quranic Corpus</title>
<link>https://arxiv.org/abs/2506.18148</link>
<guid>https://arxiv.org/abs/2506.18148</guid>
<content:encoded><![CDATA[
arXiv:2506.18148v1 Announce Type: new 
Abstract: We present the QuranMorph corpus, a morphologically annotated corpus for the Quran (77,429 tokens). Each token in the QuranMorph was manually lemmatized and tagged with its part-of-speech by three expert linguists. The lemmatization process utilized lemmas from Qabas, an Arabic lexicographic database linked with 110 lexicons and corpora of 2 million tokens. The part-of-speech tagging was performed using the fine-grained SAMA/Qabas tagset, which encompasses 40 tags. As shown in this paper, this rich lemmatization and POS tagset enabled the QuranMorph corpus to be inter-linked with many linguistic resources. The corpus is open-source and publicly available as part of the SinaLab resources at (https://sina.birzeit.edu/quran)
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers</title>
<link>https://arxiv.org/abs/2506.18185</link>
<guid>https://arxiv.org/abs/2506.18185</guid>
<content:encoded><![CDATA[
arXiv:2506.18185v1 Announce Type: new 
Abstract: This paper presents our system for the SMM4H-HeaRD 2025 shared tasks, specifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2). Task 4 focused on detecting mentions of insomnia in clinical notes, while Task 5 addressed the extraction of food safety events from news articles. We participated in all subtasks and report key findings across them, with particular emphasis on Task 5 Subtask 1, where our system achieved strong performance-securing first place with an F1 score of 0.958 on the test set. To attain this result, we employed encoder-based models (e.g., RoBERTa), alongside GPT-4 for data augmentation. This paper outlines our approach, including preprocessing, model architecture, and subtask-specific adaptations
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review</title>
<link>https://arxiv.org/abs/2506.18199</link>
<guid>https://arxiv.org/abs/2506.18199</guid>
<content:encoded><![CDATA[
arXiv:2506.18199v1 Announce Type: new 
Abstract: Large language models have demonstrated remarkable capabilities across various domains, yet concerns about cultural bias - particularly towards Arabs and Muslims - pose significant ethical challenges by perpetuating harmful stereotypes and marginalization. Despite growing recognition of bias in LLMs, prompt engineering strategies specifically addressing Arab and Muslim representation remain understudied. This mixed-methods systematic review examines such techniques, offering evidence-based guidance for researchers and practitioners. Following PRISMA guidelines and Kitchenham's systematic review methodology, we analyzed 8 empirical studies published between 2021-2024 investigating bias mitigation strategies. Our findings reveal five primary prompt engineering approaches: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts. Although all approaches show potential for reducing bias, effectiveness varied substantially across studies and bias types. Evidence suggests that certain bias types may be more resistant to prompt-based mitigation than others. Structured multi-step pipelines demonstrated the highest overall effectiveness, achieving up to 87.7% reduction in bias, though they require greater technical expertise. Cultural prompting offers broader accessibility with substantial effectiveness. These results underscore the accessibility of prompt engineering for mitigating cultural bias without requiring access to model parameters. The limited number of studies identified highlights a significant research gap in this critical area. Future research should focus on developing culturally adaptive prompting techniques, creating Arab and Muslim-specific evaluation resources, and integrating prompt engineering with complementary debiasing methods to address deeper stereotypes while maintaining model utility.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications</title>
<link>https://arxiv.org/abs/2506.18201</link>
<guid>https://arxiv.org/abs/2506.18201</guid>
<content:encoded><![CDATA[
arXiv:2506.18201v1 Announce Type: new 
Abstract: Emotion recognition capabilities in multimodal AI systems are crucial for developing culturally responsive educational technologies, yet remain underexplored for Arabic language contexts where culturally appropriate learning tools are critically needed. This study evaluates the emotion recognition performance of two advanced multimodal large language models, GPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook illustrations. We assessed both models across three prompting strategies (zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic storybooks, comparing model predictions with human annotations based on Plutchik's emotional framework. GPT-4o consistently outperformed Gemini across all conditions, achieving the highest macro F1-score of 59% with chain-of-thought prompting compared to Gemini's best performance of 43%. Error analysis revealed systematic misclassification patterns, with valence inversions accounting for 60.7% of errors, while both models struggled with culturally nuanced emotions and ambiguous narrative contexts. These findings highlight fundamental limitations in current models' cultural understanding and emphasize the need for culturally sensitive training approaches to develop effective emotion-aware educational technologies for Arabic-speaking learners.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Entity Aware Machine Translation with Multi-task Learning</title>
<link>https://arxiv.org/abs/2506.18318</link>
<guid>https://arxiv.org/abs/2506.18318</guid>
<content:encoded><![CDATA[
arXiv:2506.18318v1 Announce Type: new 
Abstract: Entity-aware machine translation (EAMT) is a complicated task in natural language processing due to not only the shortage of translation data related to the entities needed to translate but also the complexity in the context needed to process while translating those entities. In this paper, we propose a method that applies multi-task learning to optimize the performance of the two subtasks named entity recognition and machine translation, which improves the final performance of the Entity-aware machine translation task. The result and analysis are performed on the dataset provided by the organizer of Task 2 of the SemEval 2025 competition.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance</title>
<link>https://arxiv.org/abs/2506.18337</link>
<guid>https://arxiv.org/abs/2506.18337</guid>
<content:encoded><![CDATA[
arXiv:2506.18337v1 Announce Type: new 
Abstract: Machine translation (MT) post-editing and research data collection often rely on inefficient, disconnected workflows. We introduce TranslationCorrect, an integrated framework designed to streamline these tasks. TranslationCorrect combines MT generation using models like NLLB, automated error prediction using models like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive post-editing interface within a single environment. Built with human-computer interaction (HCI) principles in mind to minimize cognitive load, as confirmed by a user study. For translators, it enables them to correct errors and batch translate efficiently. For researchers, TranslationCorrect exports high-quality span-based annotations in the Error Span Annotation (ESA) format, using an error taxonomy inspired by Multidimensional Quality Metrics (MQM). These outputs are compatible with state-of-the-art error detection models and suitable for training MT or post-editing systems. Our user study confirms that TranslationCorrect significantly improves translation efficiency and user satisfaction over traditional annotation methods.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2506.18341</link>
<guid>https://arxiv.org/abs/2506.18341</guid>
<content:encoded><![CDATA[
arXiv:2506.18341v1 Announce Type: new 
Abstract: This paper explores the challenges of test-time scaling of large language models (LLMs), regarding both the data and inference efficiency. We highlight the diversity of multi-lingual reasoning based on our pilot studies, and then introduce a novel approach, \(L^2\) multi-lingual unification learning with a decoding intervention strategy for further investigation. The basic idea of \(L^2\) is that the reasoning process varies across different languages, which may be mutually beneficial to enhance both model performance and efficiency. In specific, there are two types of multi-lingual data: the entire long chain-of-thought annotations in different languages and the step-wise mixture of languages. By further tuning based on them, we show that even small amounts of data can significantly improve reasoning capabilities. Our findings suggest that multilingual learning reduces both the required data and the number of inference tokens while maintaining a comparable performance. Furthermore, \(L^2\) is orthogonal to other data efficient methods. Thus, we also emphasize the importance of diverse data selection. The \(L^2\) method offers a promising solution to the challenges of data collection and test-time compute efficiency in LLMs.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics</title>
<link>https://arxiv.org/abs/2506.18387</link>
<guid>https://arxiv.org/abs/2506.18387</guid>
<content:encoded><![CDATA[
arXiv:2506.18387v1 Announce Type: new 
Abstract: This study investigates how accurately different evaluation metrics capture the quality of causal explanations in automatically generated diagnostic reports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec, GPT-White, GPT-Black, and expert qualitative assessment across two input types: observation-based and multiple-choice-based report generation. Two weighting strategies are applied: one reflecting task-specific priorities, and the other assigning equal weights to all metrics. Our results show that GPT-Black demonstrates the strongest discriminative power in identifying logically coherent and clinically valid causal narratives. GPT-White also aligns well with expert evaluations, while similarity-based metrics diverge from clinical reasoning quality. These findings emphasize the impact of metric selection and weighting on evaluation outcomes, supporting the use of LLM-based evaluation for tasks requiring interpretability and causal reasoning.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lemmatization as a Classification Task: Results from Arabic across Multiple Genres</title>
<link>https://arxiv.org/abs/2506.18399</link>
<guid>https://arxiv.org/abs/2506.18399</guid>
<content:encoded><![CDATA[
arXiv:2506.18399v1 Announce Type: new 
Abstract: Lemmatization is crucial for NLP tasks in morphologically rich languages with ambiguous orthography like Arabic, but existing tools face challenges due to inconsistent standards and limited genre coverage. This paper introduces two novel approaches that frame lemmatization as classification into a Lemma-POS-Gloss (LPG) tagset, leveraging machine translation and semantic clustering. We also present a new Arabic lemmatization test set covering diverse genres, standardized alongside existing datasets. We evaluate character level sequence-to-sequence models, which perform competitively and offer complementary value, but are limited to lemma prediction (not LPG) and prone to hallucinating implausible forms. Our results show that classification and clustering yield more robust, interpretable outputs, setting new benchmarks for Arabic lemmatization.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2506.18421</link>
<guid>https://arxiv.org/abs/2506.18421</guid>
<content:encoded><![CDATA[
arXiv:2506.18421v1 Announce Type: new 
Abstract: The majority of data in businesses and industries is stored in tables, databases, and data warehouses. Reasoning with table-structured data poses significant challenges for large language models (LLMs) due to its hidden semantics, inherent complexity, and structured nature. One of these challenges is lacking an effective evaluation benchmark fairly reflecting the performances of LLMs on broad table reasoning abilities. In this paper, we fill in this gap, presenting a comprehensive table reasoning evolution benchmark, TReB, which measures both shallow table understanding abilities and deep table reasoning abilities, a total of 26 sub-tasks. We construct a high quality dataset through an iterative data processing procedure. We create an evaluation framework to robustly measure table reasoning capabilities with three distinct inference modes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs using this frame work and prove its effectiveness. Experimental results reveal that existing LLMs still have significant room for improvement in addressing the complex and real world Table related tasks. Both the dataset and evaluation framework are publicly available, with the dataset hosted on [HuggingFace] and the framework on [GitHub].
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2506.18485</link>
<guid>https://arxiv.org/abs/2506.18485</guid>
<content:encoded><![CDATA[
arXiv:2506.18485v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle complex reasoning tasks. However, existing RLVR methods overlook one of the most distinctive capabilities of LLMs, their in-context learning ability, as prominently demonstrated by the success of Chain-of-Thought (CoT) prompting. This motivates us to explore how reinforcement learning can be effectively combined with in-context learning to better improve the reasoning capabilities of LLMs. In this paper, we introduce Motivation-enhanced Reinforcement Finetuning} (MeRF), an intuitive yet effective method enhancing reinforcement learning of LLMs by involving ``telling LLMs the rules of the game''. Specifically, MeRF directly injects the reward specification into the prompt, which serves as an in-context motivation for model to improve its responses with awareness of the optimization objective. This simple modification leverages the in-context learning ability of LLMs aligning generation with optimization, thereby incentivizing the model to generate desired outputs from both inner motivation and external reward. Empirical evaluations on the Knights and Knaves~(K&amp;K) logic puzzle reasoning benchmark demonstrate that \texttt{MeRF} achieves substantial performance gains over baselines. Moreover, ablation studies show that performance improves with greater consistency between the in-context motivation and the external reward function, while the model also demonstrates an ability to adapt to misleading motivations through reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance</title>
<link>https://arxiv.org/abs/2506.18501</link>
<guid>https://arxiv.org/abs/2506.18501</guid>
<content:encoded><![CDATA[
arXiv:2506.18501v1 Announce Type: new 
Abstract: The increasing use of large language models (LLMs) in natural language processing (NLP) tasks has sparked significant interest in evaluating their effectiveness across diverse applications. While models like ChatGPT and DeepSeek have shown strong results in many NLP domains, a comprehensive evaluation is needed to understand their strengths, weaknesses, and domain-specific abilities. This is critical as these models are applied to various tasks, from sentiment analysis to more nuanced tasks like textual entailment and translation. This study aims to evaluate ChatGPT and DeepSeek across five key NLP tasks: sentiment analysis, topic classification, text summarization, machine translation, and textual entailment. A structured experimental protocol is used to ensure fairness and minimize variability. Both models are tested with identical, neutral prompts and evaluated on two benchmark datasets per task, covering domains like news, reviews, and formal/informal texts. The results show that DeepSeek excels in classification stability and logical reasoning, while ChatGPT performs better in tasks requiring nuanced understanding and flexibility. These findings provide valuable insights for selecting the appropriate LLM based on task requirements.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Spoken Grammatical Error Correction</title>
<link>https://arxiv.org/abs/2506.18532</link>
<guid>https://arxiv.org/abs/2506.18532</guid>
<content:encoded><![CDATA[
arXiv:2506.18532v1 Announce Type: new 
Abstract: Grammatical Error Correction (GEC) and feedback play a vital role in supporting second language (L2) learners, educators, and examiners. While written GEC is well-established, spoken GEC (SGEC), aiming to provide feedback based on learners' speech, poses additional challenges due to disfluencies, transcription errors, and the lack of structured input. SGEC systems typically follow a cascaded pipeline consisting of Automatic Speech Recognition (ASR), disfluency detection, and GEC, making them vulnerable to error propagation across modules. This work examines an End-to-End (E2E) framework for SGEC and feedback generation, highlighting challenges and possible solutions when developing these systems. Cascaded, partial-cascaded and E2E architectures are compared, all built on the Whisper foundation model. A challenge for E2E systems is the scarcity of GEC labeled spoken data. To address this, an automatic pseudo-labeling framework is examined, increasing the training data from 77 to over 2500 hours. To improve the accuracy of the SGEC system, additional contextual information, exploiting the ASR output, is investigated. Candidate feedback of their mistakes is an essential step to improving performance. In E2E systems the SGEC output must be compared with an estimate of the fluent transcription to obtain the feedback. To improve the precision of this feedback, a novel reference alignment process is proposed that aims to remove hypothesised edits that results from fluent transcription errors. Finally, these approaches are combined with an edit confidence estimation approach, to exclude low-confidence edits. Experiments on the in-house Linguaskill (LNG) corpora and the publicly available Speak & Improve (S&amp;I) corpus show that the proposed approaches significantly boost E2E SGEC performance.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking</title>
<link>https://arxiv.org/abs/2506.18535</link>
<guid>https://arxiv.org/abs/2506.18535</guid>
<content:encoded><![CDATA[
arXiv:2506.18535v1 Announce Type: new 
Abstract: This paper investigates the counterintuitive phenomenon where fine-tuning pre-trained transformer models degrades performance on the MS MARCO passage ranking task. Through comprehensive experiments involving five model variants-including full parameter fine-tuning and parameter efficient LoRA adaptations-we demonstrate that all fine-tuning approaches underperform the base sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our analysis reveals that fine-tuning disrupts the optimal embedding space structure learned during the base model's extensive pre-training on 1 billion sentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations show progressive embedding space flattening, while training dynamics analysis and computational efficiency metrics further support our findings. These results challenge conventional wisdom about transfer learning effectiveness on saturated benchmarks and suggest architectural innovations may be necessary for meaningful improvements.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance</title>
<link>https://arxiv.org/abs/2506.18576</link>
<guid>https://arxiv.org/abs/2506.18576</guid>
<content:encoded><![CDATA[
arXiv:2506.18576v1 Announce Type: new 
Abstract: Detecting harmful content is a crucial task in the landscape of NLP applications for Social Good, with hate speech being one of its most dangerous forms. But what do we mean by hate speech, how can we define it, and how does prompting different definitions of hate speech affect model performance? The contribution of this work is twofold. At the theoretical level, we address the ambiguity surrounding hate speech by collecting and analyzing existing definitions from the literature. We organize these definitions into a taxonomy of 14 Conceptual Elements-building blocks that capture different aspects of hate speech definitions, such as references to the target of hate (individual or groups) or of the potential consequences of it. At the experimental level, we employ the collection of definitions in a systematic zero-shot evaluation of three LLMs, on three hate speech datasets representing different types of data (synthetic, human-in-the-loop, and real-world). We find that choosing different definitions, i.e., definitions with a different degree of specificity in terms of encoded elements, impacts model performance, but this effect is not consistent across all architectures.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Continuous Chain-of-Thought with Jacobi Iteration</title>
<link>https://arxiv.org/abs/2506.18582</link>
<guid>https://arxiv.org/abs/2506.18582</guid>
<content:encoded><![CDATA[
arXiv:2506.18582v1 Announce Type: new 
Abstract: Continuous chain-of-thought has been shown to be effective in saving reasoning tokens for large language models. By reasoning with continuous latent thought tokens, continuous CoT is able to perform implicit reasoning in a compact manner. However, the sequential dependencies between latent thought tokens spoil parallel training, leading to long training time. In this paper, we propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi iteration on the latent thought tokens, updating them iteratively in parallel instead of sequentially and thus improving both training and inference efficiency of continuous CoT. Experiments demonstrate that by choosing the proper number of iterations, we are able to achieve comparable or even better performance while saving nearly 50% of the training and inference time. Moreover, PCCoT shows better stability and robustness in the training process. Our code is available at https://github.com/whyNLP/PCCoT.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reply to "Emergent LLM behaviors are observationally equivalent to data leakage"</title>
<link>https://arxiv.org/abs/2506.18600</link>
<guid>https://arxiv.org/abs/2506.18600</guid>
<content:encoded><![CDATA[
arXiv:2506.18600v1 Announce Type: new 
Abstract: A potential concern when simulating populations of large language models (LLMs) is data contamination, i.e. the possibility that training data may shape outcomes in unintended ways. While this concern is important and may hinder certain experiments with multi-agent models, it does not preclude the study of genuinely emergent dynamics in LLM populations. The recent critique by Barrie and T\"ornberg [1] of the results of Flint Ashery et al. [2] offers an opportunity to clarify that self-organisation and model-dependent emergent dynamics can be studied in LLM populations, highlighting how such dynamics have been empirically observed in the specific case of social conventions.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic similarity estimation for domain specific data using BERT and other techniques</title>
<link>https://arxiv.org/abs/2506.18602</link>
<guid>https://arxiv.org/abs/2506.18602</guid>
<content:encoded><![CDATA[
arXiv:2506.18602v1 Announce Type: new 
Abstract: Estimation of semantic similarity is an important research problem both in natural language processing and the natural language understanding, and that has tremendous application on various downstream tasks such as question answering, semantic search, information retrieval, document clustering, word-sense disambiguation and machine translation. In this work, we carry out the estimation of semantic similarity using different state-of-the-art techniques including the USE (Universal Sentence Encoder), InferSent and the most recent BERT, or Bidirectional Encoder Representations from Transformers, models. We use two question pairs datasets for the analysis, one is a domain specific in-house dataset and the other is a public dataset which is the Quora's question pairs dataset. We observe that the BERT model gave much superior performance as compared to the other methods. This should be because of the fine-tuning procedure that is involved in its training process, allowing it to learn patterns based on the training data that is used. This works demonstrates the applicability of BERT on domain specific datasets. We infer from the analysis that BERT is the best technique to use in the case of domain specific data.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches</title>
<link>https://arxiv.org/abs/2506.18621</link>
<guid>https://arxiv.org/abs/2506.18621</guid>
<content:encoded><![CDATA[
arXiv:2506.18621v1 Announce Type: new 
Abstract: This study examines how large language models understand the concept of persuasiveness in public speaking by modifying speech transcripts from PhD candidates in the "Ma These en 180 Secondes" competition, using the 3MT French dataset. Our contributions include a novel methodology and an interpretable textual feature set integrating rhetorical devices and discourse markers. We prompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic shifts between original and generated speech in terms of the new features. Results indicate that GPT-4o applies systematic stylistic modifications rather than optimizing persuasiveness in a human-like manner. Notably, it manipulates emotional lexicon and syntactic structures (such as interrogative and exclamatory clauses) to amplify rhetorical impact.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ByteSpan: Information-Driven Subword Tokenisation</title>
<link>https://arxiv.org/abs/2506.18639</link>
<guid>https://arxiv.org/abs/2506.18639</guid>
<content:encoded><![CDATA[
arXiv:2506.18639v1 Announce Type: new 
Abstract: Recent dynamic tokenisation methods operate directly on bytes and pool their latent representations into patches. This bears similarities to computational models of word segmentation that determine lexical boundaries using spikes in an autoregressive model's prediction error. Inspired by this connection, we explore whether grouping predictable bytes - rather than pooling their representations - can yield a useful fixed subword vocabulary. We propose a new information-driven subword tokeniser, ByteSpan, that uses an external byte-level LM during training to identify contiguous predictable byte sequences and group them into subwords. Experiments show that ByteSpan yields efficient vocabularies with higher morphological alignment scores than BPE for English. Multilingual experiments show similar compression and R\'enyi efficiency for 25 languages.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is There a Case for Conversation Optimized Tokenizers in Large Language Models?</title>
<link>https://arxiv.org/abs/2506.18674</link>
<guid>https://arxiv.org/abs/2506.18674</guid>
<content:encoded><![CDATA[
arXiv:2506.18674v1 Announce Type: new 
Abstract: The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of a token. Therefore, the tokenizer plays an important role in the efficiency of a model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. A key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, a question that immediately arises is whether there is a potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using a publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition</title>
<link>https://arxiv.org/abs/2506.18703</link>
<guid>https://arxiv.org/abs/2506.18703</guid>
<content:encoded><![CDATA[
arXiv:2506.18703v1 Announce Type: new 
Abstract: Neural sequence-to-sequence systems deliver state-of-the-art performance for automatic speech recognition. When using appropriate modeling units, e.g., byte-pair encoded characters, these systems are in principal open vocabulary systems. In practice, however, they often fail to recognize words not seen during training, e.g., named entities, acronyms, or domain-specific special words. To address this problem, many context biasing methods have been proposed; however, for words with a pronunciation-orthography mismatch, these methods may still struggle. We propose a method which allows corrections of substitution errors to improve the recognition accuracy of such challenging words. Users can add corrections on the fly during inference. We show that with this method we get a relative improvement in biased word error rate of up to 11\%, while maintaining a competitive overall word error rate.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking the Pedagogical Knowledge of Large Language Models</title>
<link>https://arxiv.org/abs/2506.18710</link>
<guid>https://arxiv.org/abs/2506.18710</guid>
<content:encoded><![CDATA[
arXiv:2506.18710v1 Announce Type: new 
Abstract: Benchmarks like Massive Multitask Language Understanding (MMLU) have played a pivotal role in evaluating AI's knowledge and abilities across diverse domains. However, existing benchmarks predominantly focus on content knowledge, leaving a critical gap in assessing models' understanding of pedagogy - the method and practice of teaching. This paper introduces The Pedagogy Benchmark, a novel dataset designed to evaluate large language models on their Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) pedagogical knowledge. These benchmarks are built on a carefully curated set of questions sourced from professional development exams for teachers, which cover a range of pedagogical subdomains such as teaching strategies and assessment methods. Here we outline the methodology and development of these benchmarks. We report results for 97 models, with accuracies spanning a range from 28% to 89% on the pedagogical knowledge questions. We consider the relationship between cost and accuracy and chart the progression of the Pareto value frontier over time. We provide online leaderboards at https://rebrand.ly/pedagogy which are updated with new models and allow interactive exploration and filtering based on various model properties, such as cost per token and open-vs-closed weights, as well as looking at performance in different subjects. LLMs and generative AI have tremendous potential to influence education and help to address the global learning crisis. Education-focused benchmarks are crucial to measure models' capacities to understand pedagogical concepts, respond appropriately to learners' needs, and support effective teaching practices across diverse contexts. They are needed for informing the responsible and evidence-based deployment of LLMs and LLM-based tools in educational settings, and for guiding both development and policy decisions.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach</title>
<link>https://arxiv.org/abs/2506.18756</link>
<guid>https://arxiv.org/abs/2506.18756</guid>
<content:encoded><![CDATA[
arXiv:2506.18756v1 Announce Type: new 
Abstract: Large Language Models (LLMs) increasingly rely on automatic prompt engineering in graphical user interfaces (GUIs) to refine user inputs and enhance response accuracy. However, the diversity of user requirements often leads to unintended misinterpretations, where automated optimizations distort original intentions and produce erroneous outputs. To address this challenge, we propose the Adaptive Greedy Binary Search (AGBS) method, which simulates common prompt optimization mechanisms while preserving semantic stability. Our approach dynamically evaluates the impact of such strategies on LLM performance, enabling robust adversarial sample generation. Through extensive experiments on open and closed-source LLMs, we demonstrate AGBS's effectiveness in balancing semantic consistency and attack efficacy. Our findings offer actionable insights for designing more reliable prompt optimization systems. Code is available at: https://github.com/franz-chang/DOBS
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASP2LJ : An Adversarial Self-Play Laywer Augmented Legal Judgment Framework</title>
<link>https://arxiv.org/abs/2506.18768</link>
<guid>https://arxiv.org/abs/2506.18768</guid>
<content:encoded><![CDATA[
arXiv:2506.18768v1 Announce Type: new 
Abstract: Legal Judgment Prediction (LJP) aims to predict judicial outcomes, including relevant legal charge, terms, and fines, which is a crucial process in Large Language Model(LLM). However, LJP faces two key challenges: (1)Long Tail Distribution: Current datasets, derived from authentic cases, suffer from high human annotation costs and imbalanced distributions, leading to model performance degradation. (2)Lawyer's Improvement: Existing systems focus on enhancing judges' decision-making but neglect the critical role of lawyers in refining arguments, which limits overall judicial accuracy. To address these issues, we propose an Adversarial Self-Play Lawyer Augmented Legal Judgment Framework, called ASP2LJ, which integrates a case generation module to tackle long-tailed data distributions and an adversarial self-play mechanism to enhance lawyers' argumentation skills. Our framework enables a judge to reference evolved lawyers' arguments, improving the objectivity, fairness, and rationality of judicial decisions. Besides, We also introduce RareCases, a dataset for rare legal cases in China, which contains 120 tail-end cases. We demonstrate the effectiveness of our approach on the SimuCourt dataset and our RareCases dataset. Experimental results show our framework brings improvements, indicating its utilization. Our contributions include an integrated framework, a rare-case dataset, and publicly releasing datasets and code to support further research in automated judicial systems.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Existing LLMs Are Not Self-Consistent For Simple Tasks</title>
<link>https://arxiv.org/abs/2506.18781</link>
<guid>https://arxiv.org/abs/2506.18781</guid>
<content:encoded><![CDATA[
arXiv:2506.18781v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have grown increasingly powerful, yet ensuring their decisions remain transparent and trustworthy requires self-consistency -- no contradictions in their internal reasoning. Our study reveals that even on simple tasks, such as comparing points on a line or a plane, or reasoning in a family tree, all smaller models are highly inconsistent, and even state-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully self-consistent. To quantify and mitigate these inconsistencies, we introduce inconsistency metrics and propose two automated methods -- a graph-based and an energy-based approach. While these fixes provide partial improvements, they also highlight the complexity and importance of self-consistency in building more reliable and interpretable AI. The code and data are available at https://github.com/scorpio-nova/llm-self-consistency.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies</title>
<link>https://arxiv.org/abs/2506.18819</link>
<guid>https://arxiv.org/abs/2506.18819</guid>
<content:encoded><![CDATA[
arXiv:2506.18819v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have been extensively evaluated for general summarization tasks as well as medical research assistance, but they have not been specifically evaluated for the task of summarizing real-world evidence (RWE) from structured output of RWE studies. We introduce RWESummary, a proposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al., 2025) to enable benchmarking of LLMs for this task. RWESummary includes one scenario and three evaluations covering major types of errors observed in summarization of medical research studies and was developed using Atropos Health proprietary data. Additionally, we use RWESummary to compare the performance of different LLMs in our internal RWE summarization tool. At the time of publication, with 13 distinct RWE studies, we found the Gemini 2.5 models performed best overall (both Flash and Pro). We suggest RWESummary as a novel and useful foundation model benchmark for real-world evidence study summarization.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task</title>
<link>https://arxiv.org/abs/2506.18828</link>
<guid>https://arxiv.org/abs/2506.18828</guid>
<content:encoded><![CDATA[
arXiv:2506.18828v1 Announce Type: new 
Abstract: This work describes the participation of the MLLP-VRAIN research group in the shared task of the IWSLT 2025 Simultaneous Speech Translation track. Our submission addresses the unique challenges of real-time translation of long-form speech by developing a modular cascade system that adapts strong pre-trained models to streaming scenarios. We combine Whisper Large-V3-Turbo for ASR with the multilingual NLLB-3.3B model for MT, implementing lightweight adaptation techniques rather than training new end-to-end models from scratch. Our approach employs document-level adaptation with prefix training to enhance the MT model's ability to handle incomplete inputs, while incorporating adaptive emission policies including a wait-$k$ strategy and RALCP for managing the translation stream. Specialized buffer management techniques and segmentation strategies ensure coherent translations across long audio sequences. Experimental results on the ACL60/60 dataset demonstrate that our system achieves a favorable balance between translation quality and latency, with a BLEU score of 31.96 and non-computational-aware StreamLAAL latency of 2.94 seconds. Our final model achieves a preliminary score on the official test set (IWSLT25Instruct) of 29.8 BLEU. Our work demonstrates that carefully adapted pre-trained components can create effective simultaneous translation systems for long-form content without requiring extensive in-domain parallel data or specialized end-to-end training.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2506.18831</link>
<guid>https://arxiv.org/abs/2506.18831</guid>
<content:encoded><![CDATA[
arXiv:2506.18831v1 Announce Type: new 
Abstract: Large Language Models employing extended chain-of-thought (CoT) reasoning often suffer from the overthinking phenomenon, generating excessive and redundant reasoning steps that increase computational costs while potentially degrading performance. While recent work has explored static steering approaches to mitigate this issue, they lack the adaptability to dynamically adjust intervention strength based on real-time reasoning quality. We propose STUPID (Steering Token Usage via PID controller), a novel training-free method that employs a PID controller to dynamically modulate activation steering strength during inference. Our approach combines a chunk-level classifier for detecting redundant reasoning patterns with a PID control mechanism that adaptively adjusts steering intensity based on the predicted redundancy probability. Experimental evaluation on GSM8K demonstrates that STUPID achieves a 6% improvement in accuracy while reducing token usage by 32%, outperforming static steering baselines. Our method provides a principled framework for dynamic reasoning calibration that maintains reasoning quality while significantly improving computational efficiency.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.18841</link>
<guid>https://arxiv.org/abs/2506.18841</guid>
<content:encoded><![CDATA[
arXiv:2506.18841v1 Announce Type: new 
Abstract: Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under https://huggingface.co/THU-KEG/LongWriter-Zero-32B
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic Interpretability Needs Philosophy</title>
<link>https://arxiv.org/abs/2506.18852</link>
<guid>https://arxiv.org/abs/2506.18852</guid>
<content:encoded><![CDATA[
arXiv:2506.18852v1 Announce Type: new 
Abstract: Mechanistic interpretability (MI) aims to explain how neural networks work by uncovering their underlying causal mechanisms. As the field grows in influence, it is increasingly important to examine not just models themselves, but the assumptions, concepts and explanatory strategies implicit in MI research. We argue that mechanistic interpretability needs philosophy: not as an afterthought, but as an ongoing partner in clarifying its concepts, refining its methods, and assessing the epistemic and ethical stakes of interpreting AI systems. Taking three open problems from the MI literature as examples, this position paper illustrates the value philosophy can add to MI research, and outlines a path toward deeper interdisciplinary dialogue.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CommVQ: Commutative Vector Quantization for KV Cache Compression</title>
<link>https://arxiv.org/abs/2506.18879</link>
<guid>https://arxiv.org/abs/2506.18879</guid>
<content:encoded><![CDATA[
arXiv:2506.18879v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization</title>
<link>https://arxiv.org/abs/2506.18880</link>
<guid>https://arxiv.org/abs/2506.18880</guid>
<content:encoded><![CDATA[
arXiv:2506.18880v1 Announce Type: new 
Abstract: Recent large-scale language models (LLMs) with long Chain-of-Thought reasoning-such as DeepSeek-R1-have achieved impressive results on Olympiad-level mathematics benchmarks. However, they often rely on a narrow set of strategies and struggle with problems that require a novel way of thinking. To systematically investigate these limitations, we introduce OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a controlled yet diverse benchmark designed to evaluate three axes of out-of-distribution generalization, inspired by Boden's typology of creativity: (1) Exploratory-applying known problem solving skills to more complex instances within the same problem domain; (2) Compositional-combining distinct reasoning skills, previously learned in isolation, to solve novel problems that require integrating these skills in new and coherent ways; and (3) Transformative-adopting novel, often unconventional strategies by moving beyond familiar approaches to solve problems more effectively. OMEGA consists of programmatically generated training-test pairs derived from templated problem generators across geometry, number theory, algebra, combinatorics, logic, and puzzles, with solutions verified using symbolic, numerical, or graphical methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance degradation as problem complexity increases. Moreover, we fine-tune the Qwen-series models across all generalization settings and observe notable improvements in exploratory generalization, while compositional generalization remains limited and transformative reasoning shows little to no improvement. By isolating and quantifying these fine-grained failures, OMEGA lays the groundwork for advancing LLMs toward genuine mathematical creativity beyond mechanical proficiency.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2506.18896</link>
<guid>https://arxiv.org/abs/2506.18896</guid>
<content:encoded><![CDATA[
arXiv:2506.18896v1 Announce Type: new 
Abstract: Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: https://github.com/Gen-Verse/ReasonFlux
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SlimRAG: Retrieval without Graphs via Entity-Aware Context Selection</title>
<link>https://arxiv.org/abs/2506.17288</link>
<guid>https://arxiv.org/abs/2506.17288</guid>
<content:encoded><![CDATA[
arXiv:2506.17288v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) enhances language models by incorporating external knowledge at inference time. However, graph-based RAG systems often suffer from structural overhead and imprecise retrieval: they require costly pipelines for entity linking and relation extraction, yet frequently return subgraphs filled with loosely related or tangential content. This stems from a fundamental flaw -- semantic similarity does not imply semantic relevance. We introduce SlimRAG, a lightweight framework for retrieval without graphs. SlimRAG replaces structure-heavy components with a simple yet effective entity-aware mechanism. At indexing time, it constructs a compact entity-to-chunk table based on semantic embeddings. At query time, it identifies salient entities, retrieves and scores associated chunks, and assembles a concise, contextually relevant input -- without graph traversal or edge construction. To quantify retrieval efficiency, we propose Relative Index Token Utilization (RITU), a metric measuring the compactness of retrieved content. Experiments across multiple QA benchmarks show that SlimRAG outperforms strong flat and graph-based baselines in accuracy while reducing index size and RITU (e.g., 16.31 vs. 56+), highlighting the value of structure-free, entity-centric context selection. The code will be released soon. https://github.com/continue-ai-company/SlimRAG
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaceLLM: Brain-Inspired Large Language Models for Long-Context Understanding</title>
<link>https://arxiv.org/abs/2506.17310</link>
<guid>https://arxiv.org/abs/2506.17310</guid>
<content:encoded><![CDATA[
arXiv:2506.17310v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) demonstrate strong performance across domains, their long-context capabilities are limited by transient neural activations causing information decay and unstructured feed-forward network (FFN) weights leading to semantic fragmentation. Inspired by the brain's working memory and cortical modularity, we propose PaceLLM, featuring two innovations: (1) a Persistent Activity (PA) Mechanism that mimics prefrontal cortex (PFC) neurons' persistent firing by introducing an activation-level memory bank to dynamically retrieve, reuse, and update critical FFN states, addressing contextual decay; and (2) Cortical Expert (CE) Clustering that emulates task-adaptive neural specialization to reorganize FFN weights into semantic modules, establishing cross-token dependencies and mitigating fragmentation. Extensive evaluations show that PaceLLM achieves 6% improvement on LongBench's Multi-document QA and 12.5-17.5% performance gains on Infinite-Bench tasks, while extending measurable context length to 200K tokens in Needle-In-A-Haystack (NIAH) tests. This work pioneers brain-inspired LLM optimization and is complementary to other works. Besides, it can be generalized to any model and enhance their long-context performance and interpretability without structural overhauls.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Prediction -- Structuring Epistemic Integrity in Artificial Reasoning Systems</title>
<link>https://arxiv.org/abs/2506.17331</link>
<guid>https://arxiv.org/abs/2506.17331</guid>
<content:encoded><![CDATA[
arXiv:2506.17331v1 Announce Type: cross 
Abstract: This paper develops a comprehensive framework for artificial intelligence systems that operate under strict epistemic constraints, moving beyond stochastic language prediction to support structured reasoning, propositional commitment, and contradiction detection. It formalises belief representation, metacognitive processes, and normative verification, integrating symbolic inference, knowledge graphs, and blockchain-based justification to ensure truth-preserving, auditably rational epistemic agents.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Cognitive Impairment Detection from Speech Using AudioLLM</title>
<link>https://arxiv.org/abs/2506.17351</link>
<guid>https://arxiv.org/abs/2506.17351</guid>
<content:encoded><![CDATA[
arXiv:2506.17351v1 Announce Type: cross 
Abstract: Cognitive impairment (CI) is of growing public health concern, and early detection is vital for effective intervention. Speech has gained attention as a non-invasive and easily collectible biomarker for assessing cognitive decline. Traditional CI detection methods typically rely on supervised models trained on acoustic and linguistic features extracted from speech, which often require manual annotation and may not generalise well across datasets and languages. In this work, we propose the first zero-shot speech-based CI detection method using the Qwen2- Audio AudioLLM, a model capable of processing both audio and text inputs. By designing prompt-based instructions, we guide the model in classifying speech samples as indicative of normal cognition or cognitive impairment. We evaluate our approach on two datasets: one in English and another multilingual, spanning different cognitive assessment tasks. Our results show that the zero-shot AudioLLM approach achieves performance comparable to supervised methods and exhibits promising generalizability and consistency across languages, tasks, and datasets.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning</title>
<link>https://arxiv.org/abs/2506.17562</link>
<guid>https://arxiv.org/abs/2506.17562</guid>
<content:encoded><![CDATA[
arXiv:2506.17562v1 Announce Type: cross 
Abstract: LLMs have demonstrated significant potential in Medical Report Generation (MRG), yet their development requires large amounts of medical image-report pairs, which are commonly scattered across multiple centers. Centralizing these data is exceptionally challenging due to privacy regulations, thereby impeding model development and broader adoption of LLM-driven MRG models. To address this challenge, we present FedMRG, the first framework that leverages Federated Learning (FL) to enable privacy-preserving, multi-center development of LLM-driven MRG models, specifically designed to overcome the critical challenge of communication-efficient LLM training under multi-modal data heterogeneity. To start with, our framework tackles the fundamental challenge of communication overhead in FL-LLM tuning by employing low-rank factorization to efficiently decompose parameter updates, significantly reducing gradient transmission costs and making LLM-driven MRG feasible in bandwidth-constrained FL settings. Furthermore, we observed the dual heterogeneity in MRG under the FL scenario: varying image characteristics across medical centers, as well as diverse reporting styles and terminology preferences. To address this, we further enhance FedMRG with (1) client-aware contrastive learning in the MRG encoder, coupled with diagnosis-driven prompts, which capture both globally generalizable and locally distinctive features while maintaining diagnostic accuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder that harmonizes generic and specialized adapters to address variations in reporting styles and terminology. Through extensive evaluation of our established FL-MRG benchmark, we demonstrate the generalizability and adaptability of FedMRG, underscoring its potential in harnessing multi-center data and generating clinically accurate reports while maintaining communication efficiency.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models</title>
<link>https://arxiv.org/abs/2506.17585</link>
<guid>https://arxiv.org/abs/2506.17585</guid>
<content:encoded><![CDATA[
arXiv:2506.17585v1 Announce Type: cross 
Abstract: Trustworthy language models should provide both correct and verifiable answers. While language models can sometimes attribute their outputs to pretraining data, their citations are often unreliable due to hallucination. As a result, current systems insert citations by querying an external retriever at inference time, introducing latency, infrastructure dependence, and vulnerability to retrieval noise. We explore whether LLMs can be made to reliably attribute to the documents seen during (continual) pretraining--without test-time retrieval--by revising the training process. To evaluate this, we release CitePretrainBench, a benchmark that mixes real-world corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and probes both short-form (single fact) and long-form (multi-fact) citation tasks. Our approach follows a two-stage process: (1) continual pretraining to bind facts to persistent document identifiers, and (2) instruction tuning to elicit citation behavior. We find that simple Passive Indexing, which appends an identifier to each document, helps memorize verbatim text but fails on paraphrased or compositional facts. Instead, we propose Active Indexing, which continually pretrains on synthetic QA pairs that (1) restate each fact in diverse compositional forms, and (2) require bidirectional source-to-fact and fact-to-source generation, jointly teaching the model to generate content from a cited source and to attribute its own answers. Experiments with Qwen2.5-7B and 3B show that Active Indexing consistently outperforms Passive Indexing across all tasks and models, with citation precision gains up to 30.2 percent. Our ablation studies reveal that performance continues to improve as we scale the amount of augmented data, showing a clear upward trend even at 16 times the original token count.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning</title>
<link>https://arxiv.org/abs/2506.17629</link>
<guid>https://arxiv.org/abs/2506.17629</guid>
<content:encoded><![CDATA[
arXiv:2506.17629v1 Announce Type: cross 
Abstract: Embodied Visual Reasoning (EVR) seeks to follow complex, free-form instructions based on egocentric video, enabling semantic understanding and spatiotemporal reasoning in dynamic environments. Despite its promising potential, EVR encounters significant challenges stemming from the diversity of complex instructions and the intricate spatiotemporal dynamics in long-term egocentric videos. Prior solutions either employ Large Language Models (LLMs) over static video captions, which often omit critical visual details, or rely on end-to-end Vision-Language Models (VLMs) that struggle with stepwise compositional reasoning. Consider the complementary strengths of LLMs in reasoning and VLMs in perception, we propose CLiViS. It is a novel training-free framework that leverages LLMs for high-level task planning and orchestrates VLM-driven open-world visual perception to iteratively update the scene context. Building on this synergy, the core of CLiViS is a dynamic Cognitive Map that evolves throughout the reasoning process. This map constructs a structured representation of the embodied scene, bridging low-level perception and high-level reasoning. Extensive experiments across multiple benchmarks demonstrate the effectiveness and generality of CLiViS, especially in handling long-term visual dependencies. Code is available at https://github.com/Teacher-Tom/CLiViS.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies</title>
<link>https://arxiv.org/abs/2506.17673</link>
<guid>https://arxiv.org/abs/2506.17673</guid>
<content:encoded><![CDATA[
arXiv:2506.17673v1 Announce Type: cross 
Abstract: Sparse Autoencoders (SAEs) have emerged as a promising solution for decomposing large language model representations into interpretable features. However, Paulo and Belrose (2025) have highlighted instability across different initialization seeds, and Heap et al. (2025) have pointed out that SAEs may not capture model-internal features. These problems likely stem from training SAEs on external datasets - either collected from the Web or generated by another model - which may contain out-of-distribution (OOD) data beyond the model's generalisation capabilities. This can result in hallucinated SAE features, which we term "Fake Features", that misrepresent the model's internal activations. To address these issues, we propose FaithfulSAE, a method that trains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we demonstrate that training SAEs on less-OOD instruction datasets results in SAEs being more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained on web-based datasets in the SAE probing task and exhibit a lower Fake Feature Ratio in 5 out of 7 models. Overall, our approach eliminates the dependency on external datasets, advancing interpretability by better capturing model-internal features while highlighting the often neglected importance of SAE training datasets.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Few-shot Keyword Spotting Performance through Pre-Trained Self-supervised Speech Models</title>
<link>https://arxiv.org/abs/2506.17686</link>
<guid>https://arxiv.org/abs/2506.17686</guid>
<content:encoded><![CDATA[
arXiv:2506.17686v1 Announce Type: cross 
Abstract: Keyword Spotting plays a critical role in enabling hands-free interaction for battery-powered edge devices. Few-Shot Keyword Spotting (FS-KWS) addresses the scalability and adaptability challenges of traditional systems by enabling recognition of custom keywords with only a few examples. However, existing FS-KWS systems achieve subpar accuracy at desirable false acceptance rates, particularly in resource-constrained edge environments. To address these issues, we propose a training scheme that leverages self-supervised learning models for robust feature extraction, dimensionality reduction, and knowledge distillation. The teacher model, based on Wav2Vec 2.0 is trained using Sub-center ArcFace loss, which enhances inter-class separability and intra-class compactness. To enable efficient deployment on edge devices, we introduce attention-based dimensionality reduction and train a standard lightweight ResNet15 student model. We evaluate the proposed approach on the English portion of the Multilingual Spoken Words Corpus (MSWC) and the Google Speech Commands (GSC) datasets. Notably, the proposed training method improves the 10-shot classification accuracy from 33.4% to 74.1% on 11 classes at 1% false alarm accuracy on the GSC dataset, thus making it significantly better-suited for a real use case scenario.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models</title>
<link>https://arxiv.org/abs/2506.17781</link>
<guid>https://arxiv.org/abs/2506.17781</guid>
<content:encoded><![CDATA[
arXiv:2506.17781v1 Announce Type: cross 
Abstract: Dense embeddings are fundamental to modern machine learning systems, powering Retrieval-Augmented Generation (RAG), information retrieval, and representation learning. While instruction-conditioning has become the dominant approach for embedding specialization, its direct application to low-capacity models imposes fundamental representational constraints that limit the performance gains derived from specialization. In this paper, we analyze these limitations and introduce the Mixture of Task Experts (MoTE) transformer block, which leverages task-specialized parameters trained with Task-Aware Contrastive Learning (\tacl) to enhance the model ability to generate specialized embeddings. Empirical results show that MoTE achieves $64\%$ higher performance gains in retrieval datasets ($+3.27 \rightarrow +5.21$) and $43\%$ higher performance gains across all datasets ($+1.81 \rightarrow +2.60$). Critically, these gains are achieved without altering instructions, training data, inference time, or number of active parameters.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Social Deduction with Graph-Informed Language Models</title>
<link>https://arxiv.org/abs/2506.17788</link>
<guid>https://arxiv.org/abs/2506.17788</guid>
<content:encoded><![CDATA[
arXiv:2506.17788v1 Announce Type: cross 
Abstract: Social reasoning - inferring unobservable beliefs and intentions from partial observations of other agents - remains a challenging task for large language models (LLMs). We evaluate the limits of current reasoning language models in the social deduction game Avalon and find that while the largest models demonstrate strong performance, they require extensive test-time inference and degrade sharply when distilled to smaller, real-time-capable variants. To address this, we introduce a hybrid reasoning framework that externalizes belief inference to a structured probabilistic model, while using an LLM for language understanding and interaction. Our approach achieves competitive performance with much larger models in Agent-Agent play and, notably, is the first language agent to defeat human players in a controlled study - achieving a 67% win rate and receiving higher qualitative ratings than both reasoning baselines and human teammates. We release code, models, and a dataset to support future work on social reasoning in LLM agents, which can be found at https://camp-lab-purdue.github.io/bayesian-social-deduction/
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach</title>
<link>https://arxiv.org/abs/2506.17828</link>
<guid>https://arxiv.org/abs/2506.17828</guid>
<content:encoded><![CDATA[
arXiv:2506.17828v1 Announce Type: cross 
Abstract: Aligning large language models (LLMs) with human preferences usually requires fine-tuning methods such as RLHF and DPO. These methods directly optimize the model parameters, so they cannot be used in test-time to improve model performance, nor are they applicable when the model weights are not accessible. In contrast, test-time methods sidestep weight updates by leveraging reward functions to guide and improve output quality. However, they incur high inference costs, and their one-shot guidance is often based on imperfect reward or value functions, leading to suboptimal outputs. In this work, we present a method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning (RL) framework that performs RL-style alignment of the (frozen) base model without touching its parameters. During training, each iteration (i) samples candidates from the base model, (ii) resamples using current value functions, and (iii) trains a new lightweight value function that guides the next decoding pass. At test time, the value functions are used to guide the base model generation via a search-based optimization process. Notably, users can apply IRO to align a model on their own dataset, similar to OpenAI's reinforcement fine-tuning (RFT), but without requiring access to the model weights.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective</title>
<link>https://arxiv.org/abs/2506.17930</link>
<guid>https://arxiv.org/abs/2506.17930</guid>
<content:encoded><![CDATA[
arXiv:2506.17930v1 Announce Type: cross 
Abstract: We propose a novel prompt design paradigm that challenges conventional wisdom in large language model (LLM) prompting. While conventional wisdom prioritizes well-crafted instructions and demonstrations for in-context learning (ICL), we show that pruning random demonstrations into seemingly incoherent "gibberish" can remarkably improve performance across diverse tasks. Notably, the "gibberish" always matches or surpasses state-of-the-art automatic prompt optimization techniques, achieving substantial gains regardless of LLM alignment. Nevertheless, discovering an effective pruning strategy is non-trivial, as existing attribution methods and prompt compression algorithms fail to deliver robust results, let alone human intuition. In terms of this, we propose a self-discover prompt optimization framework, PromptQuine, an evolutionary search framework that automatically searches for the pruning strategy by itself using only low-data regimes. Much like the emergent complexity in nature--such as symbiosis and self-organization--arising in response to resource constraints, our framework evolves and refines unconventional yet highly effective prompts by leveraging only the tokens present within the context. We demonstrate its effectiveness across classification, multi-choice question answering, generation and math reasoning tasks across LLMs, while achieving decent runtime efficiency. We hope our findings can guide mechanistic studies on in-context learning, and provide a call to action, to pave the way for more open-ended search algorithms for more effective LLM prompting.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tutorial: $\varphi$-Transductions in OpenFst via the Gallic Semiring</title>
<link>https://arxiv.org/abs/2506.17942</link>
<guid>https://arxiv.org/abs/2506.17942</guid>
<content:encoded><![CDATA[
arXiv:2506.17942v1 Announce Type: cross 
Abstract: OpenFst, a popular finite-state transducer library, supports $\varphi$-transitions but, due to an implementation constraint, they cannot be used with transducers in a straightforward way.
  In this short tutorial, we describe how one can use other functionality provided by OpenFst (namely, the Gallic semiring) to correctly implement $\varphi$-transductions and demonstrate it by implementing the MaxMatch (WordPiece) tokenization algorithm (Devlin et al., 2019; Song et al., 2021). Accompanying self-contained code examples are provided. https://www.openfst.org/twiki/pub/Contrib/FstContrib/phi_transduction_tutorial_code.tgz
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding</title>
<link>https://arxiv.org/abs/2506.18023</link>
<guid>https://arxiv.org/abs/2506.18023</guid>
<content:encoded><![CDATA[
arXiv:2506.18023v1 Announce Type: cross 
Abstract: This report introduces PP-DocBee2, an advanced version of the PP-DocBee, designed to enhance multimodal document understanding. Built on a large multimodal model architecture, PP-DocBee2 addresses the limitations of its predecessor through key technological improvements, including enhanced synthetic data quality, improved visual feature fusion strategy, and optimized inference methodologies. These enhancements yield an $11.4\%$ performance boost on internal benchmarks for Chinese business documents, and reduce inference latency by $73.0\%$ to the vanilla version. A key innovation of our work is a data quality optimization strategy for multimodal document tasks. By employing a large-scale multimodal pre-trained model to evaluate data, we apply a novel statistical criterion to filter outliers, ensuring high-quality training data. Inspired by insights into underutilized intermediate features in multimodal models, we enhance the ViT representational capacity by decomposing it into layers and applying a novel feature fusion strategy to improve complex reasoning. The source code and pre-trained model are available at \href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Democratic Paradox in Large Language Models' Underestimation of Press Freedom</title>
<link>https://arxiv.org/abs/2506.18045</link>
<guid>https://arxiv.org/abs/2506.18045</guid>
<content:encoded><![CDATA[
arXiv:2506.18045v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) increasingly mediate global information access for millions of users worldwide, their alignment and biases have the potential to shape public understanding and trust in fundamental democratic institutions, such as press freedom. In this study, we uncover three systematic distortions in the way six popular LLMs evaluate press freedom in 180 countries compared to expert assessments of the World Press Freedom Index (WPFI). The six LLMs exhibit a negative misalignment, consistently underestimating press freedom, with individual models rating between 71% to 93% of countries as less free. We also identify a paradoxical pattern we term differential misalignment: LLMs disproportionately underestimate press freedom in countries where it is strongest. Additionally, five of the six LLMs exhibit positive home bias, rating their home countries' press freedoms more favorably than would be expected given their negative misalignment with the human benchmark. In some cases, LLMs rate their home countries between 7% to 260% more positively than expected. If LLMs are set to become the next search engines and some of the most important cultural tools of our time, they must ensure accurate representations of the state of our human and civic rights globally.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation</title>
<link>https://arxiv.org/abs/2506.18088</link>
<guid>https://arxiv.org/abs/2506.18088</guid>
<content:encoded><![CDATA[
arXiv:2506.18088v1 Announce Type: cross 
Abstract: Simulation-based data synthesis has emerged as a powerful paradigm for enhancing real-world robotic manipulation. However, existing synthetic datasets remain insufficient for robust bimanual manipulation due to two challenges: (1) the lack of an efficient, scalable data generation method for novel tasks, and (2) oversimplified simulation environments that fail to capture real-world complexity. We present RoboTwin 2.0, a scalable simulation framework that enables automated, large-scale generation of diverse and realistic data, along with unified evaluation protocols for dual-arm manipulation. We first construct RoboTwin-OD, a large-scale object library comprising 731 instances across 147 categories, each annotated with semantic and manipulation-relevant labels. Building on this foundation, we develop an expert data synthesis pipeline that combines multimodal large language models (MLLMs) with simulation-in-the-loop refinement to generate task-level execution code automatically. To improve sim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization along five axes: clutter, lighting, background, tabletop height and language instructions, thereby enhancing data diversity and policy robustness. We instantiate this framework across 50 dual-arm tasks spanning five robot embodiments, and pre-collect over 100,000 domain-randomized expert trajectories. Empirical results show a 10.9% gain in code generation success and improved generalization to novel real-world scenarios. A VLA model fine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%) on unseen scene real-world tasks, while zero-shot models trained solely on our synthetic data achieve a 228% relative gain, highlighting strong generalization without real-world supervision. We release the data generator, benchmark, dataset, and code to support scalable research in robust bimanual manipulation.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging</title>
<link>https://arxiv.org/abs/2506.18135</link>
<guid>https://arxiv.org/abs/2506.18135</guid>
<content:encoded><![CDATA[
arXiv:2506.18135v1 Announce Type: cross 
Abstract: Model merging has gained increasing attention due to its intriguing property: interpolating the parameters of different task-specific fine-tuned models leads to multi-task abilities. However, despite its empirical success, the underlying mechanisms of model merging remain poorly understood. In this work, we delve into the mechanism behind model merging from a representation perspective. Our analysis reveals that model merging achieves multi-task abilities through two key capabilities: i) distinguishing samples from different tasks, and ii) adapting to the corresponding expert model for each sample. These two capabilities allow the merged model to retain task-specific expertise, enabling efficient multi-task adaptation. Building on these insights, we propose \texttt{SE-Merging}, a self-enhanced model merging framework that leverages these two characteristics to dynamically identify the corresponding task for each sample and then adaptively rescales the merging coefficients to further enhance task-specific expertise in the merged model. Notably, \texttt{SE-Merging} achieves dynamic model merging without additional training. Extensive experiments demonstrate that \texttt{SE-Merging} achieves significant performance improvements while remaining compatible with existing model merging techniques.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?</title>
<link>https://arxiv.org/abs/2506.18183</link>
<guid>https://arxiv.org/abs/2506.18183</guid>
<content:encoded><![CDATA[
arXiv:2506.18183v1 Announce Type: cross 
Abstract: Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks, enabled by multi-step reasoning induced using reinforcement learning. However, like previous language models, reasoning models are prone to generating confident, plausible responses that are incorrect (hallucinations). Knowing when and how much to trust these models is critical to the safe deployment of reasoning models in real-world applications. To this end, we explore uncertainty quantification of reasoning models in this work. Specifically, we ask three fundamental questions: First, are reasoning models well-calibrated? Second, does deeper reasoning improve model calibration? Finally, inspired by humans' innate ability to double-check their thought processes to verify the validity of their answers and their confidence, we ask: can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces? We introduce introspective uncertainty quantification (UQ) to explore this direction. In extensive evaluations on SOTA reasoning models across a broad range of benchmarks, we find that reasoning models: (i) are typically overconfident, with self-verbalized confidence estimates often greater than 85% particularly for incorrect responses, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we conclude with important research directions to design necessary UQ benchmarks and improve the calibration of reasoning models.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shrinking the Generation-Verification Gap with Weak Verifiers</title>
<link>https://arxiv.org/abs/2506.18203</link>
<guid>https://arxiv.org/abs/2506.18203</guid>
<content:encoded><![CDATA[
arXiv:2506.18203v1 Announce Type: cross 
Abstract: Verifiers can improve language model capabilities by scoring and ranking responses from generated candidates. Currently, high-quality verifiers are either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean). While LM judges and reward models have become broadly useful as general-purpose verifiers, a significant performance gap remains between them and oracle verifiers (verifiers with perfect accuracy). To help close this gap, we introduce Weaver, a framework for designing a strong verifier by combining multiple weak, imperfect verifiers. We find weighted ensembles of verifiers, which typically require learning from labeled data, significantly outperform unweighted combinations due to differences in verifier accuracies. To reduce dependency on labeled data, Weaver leverages weak supervision to estimate each verifier's accuracy and combines outputs into a unified score that better reflects true response quality. However, directly applying weak supervision algorithms poses challenges, including inconsistent verifier output formats and handling low-quality verifiers. Weaver addresses these using dataset statistics to normalize outputs and filter specific verifiers. We study Weaver's effectiveness in test-time repeated sampling, where a model generates multiple candidate responses and selects one. Our evaluations show Weaver significantly improves over Pass@1-performance when selecting the first candidate-across reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B Instruct as generator, and an ensemble of 70B or smaller judge and reward models as verifiers (87.7% average). This gain mirrors the jump between GPT-4o and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and post-training. To reduce computational costs of verifier ensembles, we train a 400M cross-encoder using Weaver's combined output scores.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdapThink: Adaptive Thinking Preferences for Reasoning Language Model</title>
<link>https://arxiv.org/abs/2506.18237</link>
<guid>https://arxiv.org/abs/2506.18237</guid>
<content:encoded><![CDATA[
arXiv:2506.18237v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL)-based post-training has significantly advanced the complex reasoning capabilities of language models, fostering sophisticated self-reflection processes. However, this ``slow thinking'' paradigm presents a critical challenge to reasoning efficiency: models may expend excessive computation on simple questions and shift reasoning prematurely for complex ones. Previous mechanisms typically rely on static length budgets or predefined rules, lacking the adaptability for varying question complexities and models' evolving capabilities. To this end, we propose AdapThink, an adaptive post-training framework designed to induce more efficient thinking while maintaining the performance of reasoning language models. Specifically, AdapThink incorporates two key mechanisms: 1) A group-relative reward function that leverages model confidence and response's characteristic to dynamically adjust the preference of reflection-related transition words without resorting to a fixed length preference. 2) A diversity-aware sampling mechanism that balances the training group's solution accuracy with reasoning diversity via an entropy-guided score. Experiments on several mathematical reasoning datasets with DeepSeek-distilled models demonstrate AdapThink's advantages in enabling adaptive reasoning patterns and mitigating the inefficiencies.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLPR: Extrapolating RLVR to General Domains without Verifiers</title>
<link>https://arxiv.org/abs/2506.18254</link>
<guid>https://arxiv.org/abs/2506.18254</guid>
<content:encoded><![CDATA[
arXiv:2506.18254v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising potential in advancing the reasoning capabilities of LLMs. However, its success remains largely confined to mathematical and code domains. This primary limitation stems from the heavy reliance on domain-specific verifiers, which results in prohibitive complexity and limited scalability. To address the challenge, our key observation is that LLM's intrinsic probability of generating a correct free-form answer directly indicates its own evaluation of the reasoning reward (i.e., how well the reasoning process leads to the correct answer). Building on this insight, we propose RLPR, a simple verifier-free framework that extrapolates RLVR to broader general domains. RLPR uses the LLM's own token probability scores for reference answers as the reward signal and maximizes the expected reward during training. We find that addressing the high variance of this noisy probability reward is crucial to make it work, and propose prob-to-reward and stabilizing methods to ensure a precise and stable reward from LLM intrinsic probabilities. Comprehensive experiments in four general-domain benchmarks and three mathematical benchmarks show that RLPR consistently improves reasoning capabilities in both areas for Gemma, Llama, and Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6 points on TheoremQA and 7.5 points on Minerva, and even surpasses strong verifier-model-dependent approaches General-Reasoner by 1.6 average points across seven benchmarks.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Document Retrieval in COVID-19 Research: Leveraging Large Language Models for Hidden Relation Extraction</title>
<link>https://arxiv.org/abs/2506.18311</link>
<guid>https://arxiv.org/abs/2506.18311</guid>
<content:encoded><![CDATA[
arXiv:2506.18311v1 Announce Type: cross 
Abstract: In recent years, with the appearance of the COVID-19 pandemic, numerous publications relevant to this disease have been issued. Because of the massive volume of publications, an efficient retrieval system is necessary to provide researchers with useful information if an unexpected pandemic happens so suddenly, like COVID-19. In this work, we present a method to help the retrieval system, the Covrelex-SE system, to provide more high-quality search results. We exploited the power of the large language models (LLMs) to extract the hidden relationships inside the unlabeled publication that cannot be found by the current parsing tools that the system is using. Since then, help the system to have more useful information during retrieval progress.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based zero-shot retrieval</title>
<link>https://arxiv.org/abs/2506.18316</link>
<guid>https://arxiv.org/abs/2506.18316</guid>
<content:encoded><![CDATA[
arXiv:2506.18316v1 Announce Type: cross 
Abstract: The Citation Discovery Shared Task focuses on predicting the correct citation from a given candidate pool for a given paragraph. The main challenges stem from the length of the abstract paragraphs and the high similarity among candidate abstracts, making it difficult to determine the exact paper to cite. To address this, we develop a system that first retrieves the top-k most similar abstracts based on extracted relational features from the given paragraph. From this subset, we leverage a Large Language Model (LLM) to accurately identify the most relevant citation. We evaluate our framework on the training dataset provided by the SCIDOCA 2025 organizers, demonstrating its effectiveness in citation prediction.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning</title>
<link>https://arxiv.org/abs/2506.18330</link>
<guid>https://arxiv.org/abs/2506.18330</guid>
<content:encoded><![CDATA[
arXiv:2506.18330v1 Announce Type: cross 
Abstract: We introduce Confucius3-Math, an open-source large language model with 14B parameters that (1) runs efficiently on a single consumer-grade GPU; (2) achieves SOTA performances on a range of mathematical reasoning tasks, outperforming many models with significantly larger sizes. In particular, as part of our mission to enhancing education and knowledge dissemination with AI, Confucius3-Math is specifically committed to mathematics learning for Chinese K-12 students and educators. Built via post-training with large-scale reinforcement learning (RL), Confucius3-Math aligns with national curriculum and excels at solving main-stream Chinese K-12 mathematical problems with low cost. In this report we share our development recipe, the challenges we encounter and the techniques we develop to overcome them. In particular, we introduce three technical innovations: Targeted Entropy Regularization, Recent Sample Recovery and Policy-Specific Hardness Weighting. These innovations encompass a new entropy regularization, a novel data scheduling policy, and an improved group-relative advantage estimator. Collectively, they significantly stabilize the RL training, improve data efficiency, and boost performance. Our work demonstrates the feasibility of building strong reasoning models in a particular domain at low cost. We open-source our model and code at https://github.com/netease-youdao/Confucius3-Math.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation</title>
<link>https://arxiv.org/abs/2506.18349</link>
<guid>https://arxiv.org/abs/2506.18349</guid>
<content:encoded><![CDATA[
arXiv:2506.18349v1 Announce Type: cross 
Abstract: The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm for scaling large language models (LLMs) while maintaining inference efficiency. However, their enormous memory requirements make them prohibitively expensive to fine-tune or deploy in resource-constrained environments. To address this challenge, we introduce SlimMoE, a multi-stage compression framework for transforming large MoE models into much smaller, efficient variants without incurring the prohibitive costs of training from scratch. Our method systematically reduces parameter counts by slimming experts and transferring knowledge through intermediate stages, effectively mitigating the performance degradation common in one-shot pruning approaches. Using this framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE (3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of the original model's training data. These compressed models can be fine-tuned on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them highly suitable for academic and resource-limited settings. Our experiments demonstrate that these compressed models outperform others of similar size and remain competitive with larger models. For instance, Phi-mini-MoE achieves similar or better performance to Phi-3-mini using only 2/3 of the activated parameters and yields comparable MMLU scores to Llama 3.1 8B despite having significantly lower latency. Our findings demonstrate that structured pruning combined with staged distillation offers an effective path to creating high-quality, compact MoE models, paving the way for broader adoption of MoE architectures. We make our models publicly available at https://huggingface.co/microsoft/Phi-mini-MoE-instruct and https://huggingface.co/microsoft/Phi-tiny-MoE-instruct .
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Generated Song Detection via Lyrics Transcripts</title>
<link>https://arxiv.org/abs/2506.18488</link>
<guid>https://arxiv.org/abs/2506.18488</guid>
<content:encoded><![CDATA[
arXiv:2506.18488v1 Announce Type: cross 
Abstract: The recent rise in capabilities of AI-based music generation tools has created an upheaval in the music industry, necessitating the creation of accurate methods to detect such AI-generated content. This can be done using audio-based detectors; however, it has been shown that they struggle to generalize to unseen generators or when the audio is perturbed. Furthermore, recent work used accurate and cleanly formatted lyrics sourced from a lyrics provider database to detect AI-generated music. However, in practice, such perfect lyrics are not available (only the audio is); this leaves a substantial gap in applicability in real-life use cases. In this work, we instead propose solving this gap by transcribing songs using general automatic speech recognition (ASR) models. We do this using several detectors. The results on diverse, multi-genre, and multi-lingual lyrics show generally strong detection performance across languages and genres, particularly for our best-performing model using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that our method is more robust than state-of-the-art audio-based ones when the audio is perturbed in different ways and when evaluated on different music generators. Our code is available at https://github.com/deezer/robust-AI-lyrics-detection.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich Transcripts</title>
<link>https://arxiv.org/abs/2506.18510</link>
<guid>https://arxiv.org/abs/2506.18510</guid>
<content:encoded><![CDATA[
arXiv:2506.18510v1 Announce Type: cross 
Abstract: Accurate detection of disfluencies in spoken language is crucial for enhancing the performance of automatic speech and language processing systems, as well as fostering the development of more inclusive speech and language technologies. Leveraging the growing trend of large language models (LLMs) as versatile learners capable of processing both lexical and non-lexical inputs (e.g., audio and video), we propose a novel approach to transcribing disfluencies as explicit tokens with timestamps, enabling the generation of fully annotated disfluency-rich transcripts. Our method integrates acoustic representations extracted from an audio encoder with textual inputs of varying quality: clean transcriptions without disfluencies, time-aligned transcriptions from aligners, or outputs from phoneme-based ASR models -- all of which may contain imperfections. Importantly, our experiments demonstrate that textual inputs do not need to be flawless. As long as they include timestamp-related cues, LLMs can effectively smooth the input and produce fully disfluency-annotated transcripts, underscoring their robustness in handling imperfect hints.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Airalogy: AI-empowered universal data digitization for research automation</title>
<link>https://arxiv.org/abs/2506.18586</link>
<guid>https://arxiv.org/abs/2506.18586</guid>
<content:encoded><![CDATA[
arXiv:2506.18586v1 Announce Type: cross 
Abstract: Research data are the foundation of Artificial Intelligence (AI)-driven science, yet current AI applications remain limited to a few fields with readily available, well-structured, digitized datasets. Achieving comprehensive AI empowerment across multiple disciplines is still out of reach. Present-day research data collection is often fragmented, lacking unified standards, inefficiently managed, and difficult to share. Creating a single platform for standardized data digitization needs to overcome the inherent challenge of balancing between universality (supporting the diverse, ever-evolving needs of various disciplines) and standardization (enforcing consistent formats to fully enable AI). No existing platform accommodates both facets. Building a truly multidisciplinary platform requires integrating scientific domain knowledge with sophisticated computing skills. Researchers often lack the computational expertise to design customized and standardized data recording methods, whereas platform developers rarely grasp the intricate needs of multiple scientific domains. These gaps impede research data standardization and hamper AI-driven progress. In this study, we address these challenges by developing Airalogy (https://airalogy.com), the world's first AI- and community-driven platform that balances universality and standardization for digitizing research data across multiple disciplines. Airalogy represents entire research workflows using customizable, standardized data records and offers an advanced AI research copilot for intelligent Q&amp;A, automated data entry, analysis, and research automation. Already deployed in laboratories across all four schools of Westlake University, Airalogy has the potential to accelerate and automate scientific innovation in universities, industry, and the global research community-ultimately benefiting humanity as a whole.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Training Wheels: Steering Vectors for Bias Correction at Inference Time</title>
<link>https://arxiv.org/abs/2506.18598</link>
<guid>https://arxiv.org/abs/2506.18598</guid>
<content:encoded><![CDATA[
arXiv:2506.18598v1 Announce Type: cross 
Abstract: Neural network classifiers trained on datasets with uneven group representation often inherit class biases and learn spurious correlations. These models may perform well on average but consistently fail on atypical groups. For example, in hair color classification, datasets may over-represent females with blond hair, reinforcing stereotypes. Although various algorithmic and data-centric methods have been proposed to address such biases, they often require retraining or significant compute. In this work, we propose a cheap, training-free method inspired by steering vectors used to edit behaviors in large language models. We compute the difference in mean activations between majority and minority groups to define a "bias vector," which we subtract from the model's residual stream. This leads to reduced classification bias and improved worst-group accuracy. We explore multiple strategies for extracting and applying these vectors in transformer-like classifiers, showing that steering vectors, traditionally used in generative models, can also be effective in classification. More broadly, we showcase an extremely cheap, inference time, training free method to mitigate bias in classification models.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs</title>
<link>https://arxiv.org/abs/2506.18628</link>
<guid>https://arxiv.org/abs/2506.18628</guid>
<content:encoded><![CDATA[
arXiv:2506.18628v1 Announce Type: cross 
Abstract: In real-world applications, Large Language Models (LLMs) often hallucinate, even in Retrieval-Augmented Generation (RAG) settings, which poses a significant challenge to their deployment. In this paper, we introduce AggTruth, a method for online detection of contextual hallucinations by analyzing the distribution of internal attention scores in the provided context (passage). Specifically, we propose four different variants of the method, each varying in the aggregation technique used to calculate attention scores. Across all LLMs examined, AggTruth demonstrated stable performance in both same-task and cross-task setups, outperforming the current SOTA in multiple scenarios. Furthermore, we conducted an in-depth analysis of feature selection techniques and examined how the number of selected attention heads impacts detection performance, demonstrating that careful selection of heads is essential to achieve optimal results.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReDit: Reward Dithering for Improved LLM Policy Optimization</title>
<link>https://arxiv.org/abs/2506.18631</link>
<guid>https://arxiv.org/abs/2506.18631</guid>
<content:encoded><![CDATA[
arXiv:2506.18631v1 Announce Type: cross 
Abstract: DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal Anchor Gated Transformer with Knowledge Distillation for Emotion Recognition in Conversation</title>
<link>https://arxiv.org/abs/2506.18716</link>
<guid>https://arxiv.org/abs/2506.18716</guid>
<content:encoded><![CDATA[
arXiv:2506.18716v1 Announce Type: cross 
Abstract: Emotion Recognition in Conversation (ERC) aims to detect the emotions of individual utterances within a conversation. Generating efficient and modality-specific representations for each utterance remains a significant challenge. Previous studies have proposed various models to integrate features extracted using different modality-specific encoders. However, they neglect the varying contributions of modalities to this task and introduce high complexity by aligning modalities at the frame level. To address these challenges, we propose the Multi-modal Anchor Gated Transformer with Knowledge Distillation (MAGTKD) for the ERC task. Specifically, prompt learning is employed to enhance textual modality representations, while knowledge distillation is utilized to strengthen representations of weaker modalities. Furthermore, we introduce a multi-modal anchor gated transformer to effectively integrate utterance-level representations across modalities. Extensive experiments on the IEMOCAP and MELD datasets demonstrate the effectiveness of knowledge distillation in enhancing modality representations and achieve state-of-the-art performance in emotion recognition. Our code is available at: https://github.com/JieLi-dd/MAGTKD.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Total Variation Distance Estimators for Changepoint Detection in News Data</title>
<link>https://arxiv.org/abs/2506.18764</link>
<guid>https://arxiv.org/abs/2506.18764</guid>
<content:encoded><![CDATA[
arXiv:2506.18764v1 Announce Type: cross 
Abstract: Detecting when public discourse shifts in response to major events is crucial for understanding societal dynamics. Real-world data is high-dimensional, sparse, and noisy, making changepoint detection in this domain a challenging endeavor. In this paper, we leverage neural networks for changepoint detection in news data, introducing a method based on the so-called learning-by-confusion scheme, which was originally developed for detecting phase transitions in physical systems. We train classifiers to distinguish between articles from different time periods. The resulting classification accuracy is used to estimate the total variation distance between underlying content distributions, where significant distances highlight changepoints. We demonstrate the effectiveness of this method on both synthetic datasets and real-world data from The Guardian newspaper, successfully identifying major historical events including 9/11, the COVID-19 pandemic, and presidential elections. Our approach requires minimal domain knowledge, can autonomously discover significant shifts in public discourse, and yields a quantitative measure of change in content, making it valuable for journalism, policy analysis, and crisis monitoring.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training</title>
<link>https://arxiv.org/abs/2506.18777</link>
<guid>https://arxiv.org/abs/2506.18777</guid>
<content:encoded><![CDATA[
arXiv:2506.18777v1 Announce Type: cross 
Abstract: Training large language models (LLMs) on source code significantly enhances their general-purpose reasoning abilities, but the mechanisms underlying this generalisation are poorly understood. In this paper, we propose Programming by Backprop (PBB) as a potential driver of this effect - teaching a model to evaluate a program for inputs by training on its source code alone, without ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of programs representing simple maths problems and algorithms: one with source code and I/O examples (w/ IO), the other with source code only (w/o IO). We find evidence that LLMs have some ability to evaluate w/o IO programs for inputs in a range of experimental settings, and make several observations. Firstly, PBB works significantly better when programs are provided as code rather than semantically equivalent language descriptions. Secondly, LLMs can produce outputs for w/o IO programs directly, by implicitly evaluating the program within the forward pass, and more reliably when stepping through the program in-context via chain-of-thought. We further show that PBB leads to more robust evaluation of programs across inputs than training on I/O pairs drawn from a distribution that mirrors naturally occurring data. Our findings suggest a mechanism for enhanced reasoning through code training: it allows LLMs to internalise reusable algorithmic abstractions. Significant scope remains for future work to enable LLMs to more effectively learn from symbolic procedures, and progress in this direction opens other avenues like model alignment by training on formal constitutional principles.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation</title>
<link>https://arxiv.org/abs/2506.18810</link>
<guid>https://arxiv.org/abs/2506.18810</guid>
<content:encoded><![CDATA[
arXiv:2506.18810v1 Announce Type: cross 
Abstract: Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and OpenAI o1 series have achieved notable performance enhancements on complex reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT). However, an emerging issue is their inclination to produce excessively verbose reasoning processes, leading to the inefficiency problem. Existing literature on improving efficiency mainly adheres to the before-reasoning paradigms such as prompting and reasoning or fine-tuning and reasoning, but ignores the promising direction of directly encouraging the model to speak concisely by intervening during the generation of reasoning. In order to fill the blank, we propose a framework dubbed ConciseHint, which continuously encourages the reasoning model to speak concisely by injecting the textual hint (manually designed or trained on the concise data) during the token generation of the reasoning process. Besides, ConciseHint is adaptive to the complexity of the query by adaptively adjusting the hint intensity, which ensures it will not undermine model performance. Experiments on the state-of-the-art LRMs, including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can effectively produce concise reasoning processes while maintaining performance well. For instance, we achieve a reduction ratio of 65\% for the reasoning length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>USAD: Universal Speech and Audio Representation via Distillation</title>
<link>https://arxiv.org/abs/2506.18843</link>
<guid>https://arxiv.org/abs/2506.18843</guid>
<content:encoded><![CDATA[
arXiv:2506.18843v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) has revolutionized audio representations, yet models often remain domain-specific, focusing on either speech or non-speech tasks. In this work, we present Universal Speech and Audio Distillation (USAD), a unified approach to audio representation learning that integrates diverse audio types - speech, sound, and music - into a single model. USAD employs efficient layer-to-layer distillation from domain-specific SSL models to train a student on a comprehensive audio dataset. USAD offers competitive performance across various benchmarks and datasets, including frame and instance-level speech processing tasks, audio tagging, and sound classification, achieving near state-of-the-art results with a single encoder on SUPERB and HEAR benchmarks.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniGen2: Exploration to Advanced Multimodal Generation</title>
<link>https://arxiv.org/abs/2506.18871</link>
<guid>https://arxiv.org/abs/2506.18871</guid>
<content:encoded><![CDATA[
arXiv:2506.18871v1 Announce Type: cross 
Abstract: In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen2
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations</title>
<link>https://arxiv.org/abs/2506.18898</link>
<guid>https://arxiv.org/abs/2506.18898</guid>
<content:encoded><![CDATA[
arXiv:2506.18898v1 Announce Type: cross 
Abstract: This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at https://tar.csuhan.com
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval</title>
<link>https://arxiv.org/abs/2506.18902</link>
<guid>https://arxiv.org/abs/2506.18902</guid>
<content:encoded><![CDATA[
arXiv:2506.18902v1 Announce Type: cross 
Abstract: We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-based information retrieval, cross-modal semantic similarity, and programming code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single- modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Data Selection for LLM Instruction Tuning</title>
<link>https://arxiv.org/abs/2402.05123</link>
<guid>https://arxiv.org/abs/2402.05123</guid>
<content:encoded><![CDATA[
arXiv:2402.05123v2 Announce Type: replace 
Abstract: Instruction tuning is a vital step of training large language models (LLM), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLM. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances,and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment Helps Make the Most of Multimodal Data</title>
<link>https://arxiv.org/abs/2405.08454</link>
<guid>https://arxiv.org/abs/2405.08454</guid>
<content:encoded><![CDATA[
arXiv:2405.08454v3 Announce Type: replace 
Abstract: Political scientists increasingly analyze multimodal data. However, the effective analysis of such data requires aligning information across different modalities. In our paper, we demonstrate the significance of such alignment. Informed by a systematic review of 2,703 papers, we find that political scientists typically do not align their multimodal data. Introducing a decision tree that guides alignment choices, our framework highlights alignment's untapped potential and provides concrete advice in research design and modeling decisions. We illustrate alignment's analytical value through two applications: predicting tonality in U.S. presidential campaign ads and cross-modal querying of German parliamentary speeches to examine responses to the far-right AfD.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closer Look into Mixture-of-Experts in Large Language Models</title>
<link>https://arxiv.org/abs/2406.18219</link>
<guid>https://arxiv.org/abs/2406.18219</guid>
<content:encoded><![CDATA[
arXiv:2406.18219v3 Announce Type: replace 
Abstract: Mixture-of-experts (MoE) is gaining increasing attention due to its unique properties and remarkable performance, especially for language tasks. By sparsely activating a subset of parameters for each token, MoE architecture could increase the model size without sacrificing computational efficiency, achieving a better trade-off between performance and training costs. However, the underlying mechanism of MoE still lacks further exploration, and its modularization degree remains questionable. In this paper, we make an initial attempt to understand the inner workings of MoE-based large language models. Concretely, we comprehensively study the parametric and behavioral features of three popular MoE-based models and reveal some intriguing observations, including 1) Neurons act like fine-grained experts; 2) The router of MoE usually selects experts with larger output norms; 3) The expert diversity increases as the layer increases, while the last layer is an outlier, which is further validated by an initial experiment. Based on the observations, we also provide suggestions for a broad spectrum of MoE practitioners, such as router design and expert allocation. We hope this work could shed light on future research on the MoE framework and other modular architectures. Code is available at https://github.com/kamanphoebe/Look-into-MoEs.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anthropocentric bias in language model evaluation</title>
<link>https://arxiv.org/abs/2407.03859</link>
<guid>https://arxiv.org/abs/2407.03859</guid>
<content:encoded><![CDATA[
arXiv:2407.03859v2 Announce Type: replace 
Abstract: Evaluating the cognitive capacities of large language models (LLMs) requires overcoming not only anthropomorphic but also anthropocentric biases. This article identifies two types of anthropocentric bias that have been neglected: overlooking how auxiliary factors can impede LLM performance despite competence ("auxiliary oversight"), and dismissing LLM mechanistic strategies that differ from those of humans as not genuinely competent ("mechanistic chauvinism"). Mitigating these biases necessitates an empirically-driven, iterative approach to mapping cognitive tasks to LLM-specific capacities and mechanisms, which can be done by supplementing carefully designed behavioral experiments with mechanistic studies.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"I understand why I got this grade": Automatic Short Answer Grading with Feedback</title>
<link>https://arxiv.org/abs/2407.12818</link>
<guid>https://arxiv.org/abs/2407.12818</guid>
<content:encoded><![CDATA[
arXiv:2407.12818v2 Announce Type: replace 
Abstract: In recent years, there has been a growing interest in using Artificial Intelligence (AI) to automate student assessment in education. Among different types of assessments, summative assessments play a crucial role in evaluating a student's understanding level of a course. Such examinations often involve short-answer questions. However, grading these responses and providing meaningful feedback manually at scale is both time-consuming and labor-intensive. Feedback is particularly important, as it helps students recognize their strengths and areas for improvement. Despite the importance of this task, there is a significant lack of publicly available datasets that support automatic short-answer grading with feedback generation. To address this gap, we introduce Engineering Short Answer Feedback (EngSAF), a dataset designed for automatic short-answer grading with feedback. The dataset covers a diverse range of subjects, questions, and answer patterns from multiple engineering domains and contains ~5.8k data points. We incorporate feedback into our dataset by leveraging the generative capabilities of state-of-the-art large language models (LLMs) using our Label-Aware Synthetic Feedback Generation (LASFG) strategy. This paper underscores the importance of enhanced feedback in practical educational settings, outlines dataset annotation and feedback generation processes, conducts a thorough EngSAF analysis, and provides different LLMs-based zero-shot and finetuned baselines for future comparison. The best-performing model (Mistral-7B) achieves an overall accuracy of 75.4% and 58.7% on unseen answers and unseen question test sets, respectively. Additionally, we demonstrate the efficiency and effectiveness of our ASAG system through its deployment in a real-world end-semester exam at a reputed institute.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation</title>
<link>https://arxiv.org/abs/2408.00863</link>
<guid>https://arxiv.org/abs/2408.00863</guid>
<content:encoded><![CDATA[
arXiv:2408.00863v2 Announce Type: replace 
Abstract: The remarkable success of Large Language Models (LLMs) across diverse tasks has driven the research community to extend their capabilities to molecular applications. However, most molecular LLMs employ adapter-based architectures that do not treat molecule and text modalities equally and lack a supervision signal for the molecule modality. To address these issues, we introduce UniMoT, a Unified Molecule-Text LLM adopting a tokenizer-based architecture that expands the vocabulary of LLM with molecule tokens. Specifically, we introduce a Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge the modality gap between molecule and text. This tokenizer transforms molecules into sequences of molecule tokens with causal dependency, encapsulating high-level molecular and textual information. Equipped with this tokenizer, UniMoT can unify molecule and text modalities under a shared token representation and an autoregressive training paradigm, enabling it to interpret molecules as a foreign language and generate them as text. Following a four-stage training scheme, UniMoT emerges as a multi-modal generalist capable of performing both molecule-to-text and text-to-molecule tasks. Extensive experiments demonstrate that UniMoT achieves state-of-the-art performance across a wide range of molecule comprehension and generation tasks.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Circuits in Language Models: A Mechanistic Interpretation of Syllogistic Inference</title>
<link>https://arxiv.org/abs/2408.08590</link>
<guid>https://arxiv.org/abs/2408.08590</guid>
<content:encoded><![CDATA[
arXiv:2408.08590v3 Announce Type: replace 
Abstract: Recent studies on reasoning in language models (LMs) have sparked a debate on whether they can learn systematic inferential principles or merely exploit superficial patterns in the training data. To understand and uncover the mechanisms adopted for formal reasoning in LMs, this paper presents a mechanistic interpretation of syllogistic inference. Specifically, we present a methodology for circuit discovery aimed at interpreting content-independent and formal reasoning mechanisms. Through two distinct intervention methods, we uncover a sufficient and necessary circuit involving middle-term suppression that elucidates how LMs transfer information to derive valid conclusions from premises. Furthermore, we investigate how belief biases manifest in syllogistic inference, finding evidence of partial contamination from additional attention heads responsible for encoding commonsense and contextualized knowledge. Finally, we explore the generalization of the discovered mechanisms across various syllogistic schemes, model sizes and architectures. The identified circuit is sufficient and necessary for syllogistic schemes on which the models achieve high accuracy (>60%), with compatible activation patterns across models of different families. Overall, our findings suggest that LMs learn transferable content-independent reasoning mechanisms, but that, at the same time, such mechanisms do not involve generalizable and abstract logical primitives, being susceptible to contamination by the same world knowledge acquired during pre-training.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models</title>
<link>https://arxiv.org/abs/2408.14470</link>
<guid>https://arxiv.org/abs/2408.14470</guid>
<content:encoded><![CDATA[
arXiv:2408.14470v3 Announce Type: replace 
Abstract: Fine-tuning large language models (LLMs) on downstream tasks requires substantial computational resources. Selective PEFT, a class of parameter-efficient fine-tuning (PEFT) methodologies, aims to mitigate these computational challenges by selectively fine-tuning only a small fraction of the model parameters. Although parameter-efficient, these techniques often fail to match the performance of fully fine-tuned models, primarily due to inherent biases introduced during parameter selection. Traditional selective PEFT techniques use a fixed set of parameters selected using different importance heuristics, failing to capture parameter importance dynamically and often leading to suboptimal performance. We introduce $\text{ID}^3$, a novel selective PEFT method that calculates parameter importance continually, and dynamically unmasks parameters by balancing exploration and exploitation in parameter selection. Our empirical study on 16 tasks spanning natural language understanding, mathematical reasoning and summarization demonstrates the effectiveness of our method compared to fixed-masking selective PEFT techniques. We analytically show that $\text{ID}^3$ reduces the number of gradient updates by a factor of two, enhancing computational efficiency. Since $\text{ID}^3$ is robust to random initialization of neurons and operates directly on the optimization process, it is highly flexible and can be integrated with existing additive and reparametrization-based PEFT techniques such as adapters and LoRA respectively.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Disease Diagnosis: A Scoping Review</title>
<link>https://arxiv.org/abs/2409.00097</link>
<guid>https://arxiv.org/abs/2409.00097</guid>
<content:encoded><![CDATA[
arXiv:2409.00097v3 Announce Type: replace 
Abstract: Automatic disease diagnosis has become increasingly valuable in clinical practice. The advent of large language models (LLMs) has catalyzed a paradigm shift in artificial intelligence, with growing evidence supporting the efficacy of LLMs in diagnostic tasks. Despite the increasing attention in this field, a holistic view is still lacking. Many critical aspects remain unclear, such as the diseases and clinical data to which LLMs have been applied, the LLM techniques employed, and the evaluation methods used. In this article, we perform a comprehensive review of LLM-based methods for disease diagnosis. Our review examines the existing literature across various dimensions, including disease types and associated clinical specialties, clinical data, LLM techniques, and evaluation methods. Additionally, we offer recommendations for applying and evaluating LLMs for diagnostic tasks. Furthermore, we assess the limitations of current research and discuss future directions. To our knowledge, this is the first comprehensive review for LLM-based disease diagnosis.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Retrieval Augmented Generation for Culturally-Sensitive Tasks: A Benchmark for Cross-lingual Robustness</title>
<link>https://arxiv.org/abs/2410.01171</link>
<guid>https://arxiv.org/abs/2410.01171</guid>
<content:encoded><![CDATA[
arXiv:2410.01171v3 Announce Type: replace 
Abstract: The paradigm of retrieval-augmented generated (RAG) helps mitigate hallucinations of large language models (LLMs). However, RAG also introduces biases contained within the retrieved documents. These biases can be amplified in scenarios which are multilingual and culturally-sensitive, such as territorial disputes. We thus introduce BordIRLines, a dataset of territorial disputes paired with retrieved Wikipedia documents, across 49 languages. We evaluate the cross-lingual robustness of this RAG setting by formalizing several modes for multilingual retrieval. Our experiments on several LLMs show that incorporating perspectives from diverse languages can in fact improve robustness; retrieving multilingual documents best improves response consistency and decreases geopolitical bias over RAG with purely in-language documents. We also consider how RAG responses utilize presented documents, finding a much wider variance in the linguistic distribution of response citations, when querying in low-resource languages. Our further analyses investigate the various aspects of a cross-lingual RAG pipeline, from retrieval to document contents. We release our benchmark and code to support continued research towards equitable information access across languages at https://huggingface.co/datasets/borderlines/bordirlines.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Preference Bias in LLM-as-a-Judge</title>
<link>https://arxiv.org/abs/2410.21819</link>
<guid>https://arxiv.org/abs/2410.21819</guid>
<content:encoded><![CDATA[
arXiv:2410.21819v2 Announce Type: replace 
Abstract: Automated evaluation leveraging large language models (LLMs), commonly referred to as LLM evaluators or LLM-as-a-judge, has been widely used in measuring the performance of dialogue systems. However, the self-preference bias in LLMs has posed significant risks, including promoting specific styles or policies intrinsic to the LLMs. Despite the importance of this issue, there is a lack of established methods to measure the self-preference bias quantitatively, and its underlying causes are poorly understood. In this paper, we introduce a novel quantitative metric to measure the self-preference bias. Our experimental results demonstrate that GPT-4 exhibits a significant degree of self-preference bias. To explore the causes, we hypothesize that LLMs may favor outputs that are more familiar to them, as indicated by lower perplexity. We analyze the relationship between LLM evaluations and the perplexities of outputs. Our findings reveal that LLMs assign significantly higher evaluations to outputs with lower perplexity than human evaluators, regardless of whether the outputs were self-generated. This suggests that the essence of the bias lies in perplexity and that the self-preference bias exists because LLMs prefer texts more familiar to them.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Reward Gap Optimization for Mitigating VLM Hallucinations</title>
<link>https://arxiv.org/abs/2411.17265</link>
<guid>https://arxiv.org/abs/2411.17265</guid>
<content:encoded><![CDATA[
arXiv:2411.17265v3 Announce Type: replace 
Abstract: The success of Direct Preference Optimization (DPO) in mitigating hallucinations in Vision Language Models (VLMs) critically hinges on the true reward gaps within preference pairs. However, current methods, typically relying on ranking or rewriting strategies, often struggle to optimize these reward gaps in a systematic way during data curation. A core difficulty lies in precisely characterizing and strategically manipulating the overall reward gap configuration, that is, the deliberate design of how to shape these reward gaps within each preference pair across the data. To address this, we introduce Topic-level Preference Rewriting(TPR), a novel framework designed for the systematic optimization of reward gap configuration. Through selectively replacing semantic topics within VLM responses with model's own resampled candidates for targeted rewriting, TPR can provide topic-level control over fine-grained semantic details. This precise control enables advanced data curation strategies, such as progressively adjusting the difficulty of rejected responses, thereby sculpting an effective reward gap configuration that guides the model to overcome challenging hallucinations. Comprehensive experiments demonstrate TPR achieves state-of-the-art performance on multiple hallucination benchmarks, outperforming previous methods by an average of 20%. Notably, it significantly reduces hallucinations by up to 93% on ObjectHal-Bench, and also exhibits superior data efficiency towards robust and cost-effective VLM alignment.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinGPT: Enhancing Sentiment-Based Stock Movement Prediction with Dissemination-Aware and Context-Enriched LLMs</title>
<link>https://arxiv.org/abs/2412.10823</link>
<guid>https://arxiv.org/abs/2412.10823</guid>
<content:encoded><![CDATA[
arXiv:2412.10823v2 Announce Type: replace 
Abstract: Financial sentiment analysis is crucial for understanding the influence of news on stock prices. Recently, large language models (LLMs) have been widely adopted for this purpose due to their advanced text analysis capabilities. However, these models often only consider the news content itself, ignoring its dissemination, which hampers accurate prediction of short-term stock movements. Additionally, current methods often lack sufficient contextual data and explicit instructions in their prompts, limiting LLMs' ability to interpret news. In this paper, we propose a data-driven approach that enhances LLM-powered sentiment-based stock movement predictions by incorporating news dissemination breadth, contextual data, and explicit instructions. We cluster recent company-related news to assess its reach and influence, enriching prompts with more specific data and precise instructions. This data is used to construct an instruction tuning dataset to fine-tune an LLM for predicting short-term stock price movements. Our experimental results show that our approach improves prediction accuracy by 8\% compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction in the Era of Large Language Models</title>
<link>https://arxiv.org/abs/2412.12832</link>
<guid>https://arxiv.org/abs/2412.12832</guid>
<content:encoded><![CDATA[
arXiv:2412.12832v2 Announce Type: replace 
Abstract: Evaluating the performance of Grammatical Error Correction (GEC) models has become increasingly challenging, as large language model (LLM)-based GEC systems often produce corrections that diverge from provided gold references. This discrepancy undermines the reliability of traditional reference-based evaluation metrics. In this study, we propose a novel evaluation framework for GEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency, and utilizing a dynamic weighting mechanism. Our framework employs the Analytic Hierarchy Process (AHP) in conjunction with large language models to ascertain the relative importance of various evaluation criteria. Additionally, we develop a dataset incorporating human annotations and LLM-simulated sentences to validate our algorithms and fine-tune more cost-effective models. Experimental results indicate that our proposed approach enhances the effectiveness of GEC model evaluations.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Inconsistencies</title>
<link>https://arxiv.org/abs/2412.15035</link>
<guid>https://arxiv.org/abs/2412.15035</guid>
<content:encoded><![CDATA[
arXiv:2412.15035v3 Announce Type: replace 
Abstract: Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we conduct a large-scale, comprehensive safety evaluation of the current LLM landscape. For this purpose, we introduce M-ALERT, a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language, totaling 75k, with category-wise annotations. Our extensive experiments on 39 state-of-the-art LLMs highlight the importance of language-specific safety analysis, revealing that models often exhibit significant inconsistencies in safety across languages and categories. For instance, Llama3.2 shows high unsafety in category crime_tax for Italian but remains safe in other languages. Similar inconsistencies can be observed across all models. In contrast, certain categories, such as substance_cannabis and crime_propaganda, consistently trigger unsafe responses across models and languages. These findings underscore the need for robust multilingual safety practices in LLMs to ensure responsible usage across diverse communities.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeAR: Graph-enhanced Agent for Retrieval-augmented Generation</title>
<link>https://arxiv.org/abs/2412.18431</link>
<guid>https://arxiv.org/abs/2412.18431</guid>
<content:encoded><![CDATA[
arXiv:2412.18431v2 Announce Type: replace 
Abstract: Retrieval-augmented Generation (RAG) relies on effective retrieval capabilities, yet traditional sparse and dense retrievers inherently struggle with multi-hop retrieval scenarios. In this paper, we introduce GeAR, a system that advances RAG performance through two key innovations: (i) an efficient graph expansion mechanism that augments any conventional base retriever, such as BM25, and (ii) an agent framework that incorporates the resulting graph-based retrieval into a multi-step retrieval framework. Our evaluation demonstrates GeAR's superior retrieval capabilities across three multi-hop question answering datasets. Notably, our system achieves state-of-the-art results with improvements exceeding 10% on the challenging MuSiQue dataset, while consuming fewer tokens and requiring fewer iterations than existing multi-step retrieval systems. The project page is available at https://gear-rag.github.io.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Multi-Task Inferencing with a Shared Backbone and Lightweight Task-Specific Adapters for Automatic Scoring</title>
<link>https://arxiv.org/abs/2412.21065</link>
<guid>https://arxiv.org/abs/2412.21065</guid>
<content:encoded><![CDATA[
arXiv:2412.21065v2 Announce Type: replace 
Abstract: The integration of Artificial Intelligence (AI) in education requires scalable and efficient frameworks that balance performance, adaptability, and cost. This paper addresses these needs by proposing a shared backbone model architecture enhanced with lightweight LoRA adapters for task-specific fine-tuning, targeting the automated scoring of student responses across 27 mutually exclusive tasks. By achieving competitive performance (average QWK of 0.848 compared to 0.888 for fully fine-tuned models) while reducing GPU memory consumption by 60% and inference latency by 40%, the framework demonstrates significant efficiency gains. This approach aligns with the workshop's focus on improving language models for educational tasks, creating responsible innovations for cost-sensitive deployment, and supporting educators by streamlining assessment workflows. The findings underscore the potential of scalable AI to enhance learning outcomes while maintaining fairness and transparency in automated scoring systems.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEAL: Scaling to Emphasize Attention for Long-Context Retrieval</title>
<link>https://arxiv.org/abs/2501.15225</link>
<guid>https://arxiv.org/abs/2501.15225</guid>
<content:encoded><![CDATA[
arXiv:2501.15225v2 Announce Type: replace 
Abstract: While many advanced LLMs are designed to handle long sequence data, we can still observe notable quality degradation even within the sequence limit. In this work, we introduce a novel approach called Scaling to Emphasize Attention for Long-context retrieval (SEAL), which enhances the retrieval performance of large language models (LLMs) over long contexts. We observe that specific attention heads are closely tied to long-context retrieval, showing positive or negative correlation with retrieval scores, and adjusting the strength of these heads boosts the quality of LLMs in long context by a large margin. Built on this insight, we propose a learning-based mechanism that leverages generated data to emphasize these heads. By applying SEAL, we achieve significant improvements in long-context retrieval performance across various tasks and models. Additionally, when combined with existing training-free context extension techniques, SEAL extends the contextual limits of LLMs while maintaining highly reliable outputs.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping</title>
<link>https://arxiv.org/abs/2502.02072</link>
<guid>https://arxiv.org/abs/2502.02072</guid>
<content:encoded><![CDATA[
arXiv:2502.02072v2 Announce Type: replace 
Abstract: The rapid evolution of Large Language Models (LLMs) has transformed natural language processing but raises critical concerns about biases inherent in their deployment and use across diverse linguistic and sociocultural contexts. This paper presents a framework named ASCenD BDS (Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping). The framework presents approach to detecting bias, discrimination, stereotyping across various categories such as gender, caste, age, disability, socioeconomic status, linguistic variations, etc., using an approach which is Adaptive, Stochastic and Context-Aware. The existing frameworks rely heavily on usage of datasets to generate scenarios for detection of Bias, Discrimination and Stereotyping. Examples include datasets such as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ. However, such an approach provides point solutions. As a result, these datasets provide a finite number of scenarios for assessment. The current framework overcomes this limitation by having features which enable Adaptability, Stochasticity, Context Awareness. Context awareness can be customized for any nation or culture or sub-culture (for example an organization's unique culture). In this paper, context awareness in the Indian context has been established. Content has been leveraged from Indian Census 2011 to have a commonality of categorization. A framework has been developed using Category, Sub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability, Stochasticity and Context awareness. The framework has been described in detail in Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories were developed by a team of consultants at Saint Fox Consultancy Private Ltd. The concept has been tested out in SFCLabs as part of product development.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compromising Honesty and Harmlessness in Language Models via Deception Attacks</title>
<link>https://arxiv.org/abs/2502.08301</link>
<guid>https://arxiv.org/abs/2502.08301</guid>
<content:encoded><![CDATA[
arXiv:2502.08301v2 Announce Type: replace 
Abstract: Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce "deception attacks" that undermine both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. We introduce fine-tuning methods that cause models to selectively deceive users on targeted topics while remaining accurate on others. Through a series of experiments, we show that such targeted deception is effective even in high-stakes domains or ideologically charged subjects. In addition, we find that deceptive fine-tuning often compromises other safety properties: deceptive models are more likely to produce toxic content, including hate speech and stereotypes. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stop Overvaluing Multi-Agent Debate -- We Must Rethink Evaluation and Embrace Model Heterogeneity</title>
<link>https://arxiv.org/abs/2502.08788</link>
<guid>https://arxiv.org/abs/2502.08788</guid>
<content:encoded><![CDATA[
arXiv:2502.08788v3 Announce Type: replace 
Abstract: Multi-agent debate (MAD) has gained significant attention as a promising line of research to improve the factual accuracy and reasoning capabilities of large language models (LLMs). Despite its conceptual appeal, current MAD research suffers from critical limitations in evaluation practices, including limited benchmark coverage, weak baseline comparisons, and inconsistent setups. This paper presents a systematic evaluation of 5 representative MAD methods across 9 benchmarks using 4 foundational models. Surprisingly, our findings reveal that MAD often fail to outperform simple single-agent baselines such as Chain-of-Thought and Self-Consistency, even when consuming significantly more inference-time computation. To advance MAD research, we further explore the role of model heterogeneity and find it as a universal antidote to consistently improve current MAD frameworks. Based on our findings, we argue that the field must stop overvaluing MAD in its current form; for true advancement, we must critically rethink evaluation paradigms and actively embrace model heterogeneity as a core design principle.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity</title>
<link>https://arxiv.org/abs/2502.13063</link>
<guid>https://arxiv.org/abs/2502.13063</guid>
<content:encoded><![CDATA[
arXiv:2502.13063v3 Announce Type: replace 
Abstract: A range of recent works addresses the problem of compression of sequence of tokens into a shorter sequence of real-valued vectors to be used as inputs instead of token embeddings or key-value cache. These approaches are focused on reduction of the amount of compute in existing language models rather than minimization of number of bits needed to store text. Despite relying on powerful models as encoders, the maximum attainable lossless compression ratio is typically not higher than x10. This fact is highly intriguing because, in theory, the maximum information capacity of large real-valued vectors is far beyond the presented rates even for 16-bit precision and a modest vector size. In this work, we explore the limits of compression by replacing the encoder with a per-sample optimization procedure. We show that vectors with compression ratios up to x1500 exist, which highlights two orders of magnitude gap between existing and practically attainable solutions. Furthermore, we empirically show that the compression limits are determined not by the length of the input but by the amount of uncertainty to be reduced, namely, the cross-entropy loss on this sequence without any conditioning. The obtained limits highlight the substantial gap between the theoretical capacity of input embeddings and their practical utilization, suggesting significant room for optimization in model design.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Craw4LLM: Efficient Web Crawling for LLM Pretraining</title>
<link>https://arxiv.org/abs/2502.13347</link>
<guid>https://arxiv.org/abs/2502.13347</guid>
<content:encoded><![CDATA[
arXiv:2502.13347v3 Announce Type: replace 
Abstract: Web crawl is a main source of large language models' (LLMs) pretraining data, but the majority of crawled web pages are discarded in pretraining due to low data quality. This paper presents Craw4LLM, an efficient web crawling method that explores the web graph based on the preference of LLM pretraining. Specifically, it leverages the influence of a webpage in LLM pretraining as the priority score of the web crawler's scheduler, replacing the standard graph connectivity based priority. Our experiments on a web graph containing 900 million webpages from a commercial search engine's index demonstrate the efficiency of Craw4LLM in obtaining high-quality pretraining data. With just 21% URLs crawled, LLMs pretrained on Craw4LLM data reach the same downstream performances of previous crawls, significantly reducing the crawling waste and alleviating the burdens on websites. Our code is publicly available at https://github.com/cxcscmu/Craw4LLM.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States</title>
<link>https://arxiv.org/abs/2502.14744</link>
<guid>https://arxiv.org/abs/2502.14744</guid>
<content:encoded><![CDATA[
arXiv:2502.14744v4 Announce Type: replace 
Abstract: The integration of additional modalities increases the susceptibility of large vision-language models (LVLMs) to safety risks, such as jailbreak attacks, compared to their language-only counterparts. While existing research primarily focuses on post-hoc alignment techniques, the underlying safety mechanisms within LVLMs remain largely unexplored. In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety. Experimental results show that {HiddenDetect} surpasses state-of-the-art methods in detecting jailbreak attacks against LVLMs. By utilizing intrinsic safety-aware patterns, our method provides an efficient and scalable solution for strengthening LVLM robustness against multimodal threats. Our code will be released publicly at https://github.com/leigest519/HiddenDetect.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParamMute: Suppressing Knowledge-Critical FFNs for Faithful Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2502.15543</link>
<guid>https://arxiv.org/abs/2502.15543</guid>
<content:encoded><![CDATA[
arXiv:2502.15543v3 Announce Type: replace 
Abstract: Large language models (LLMs) integrated with retrieval-augmented generation (RAG) have improved factuality by grounding outputs in external evidence. However, they remain susceptible to unfaithful generation, where outputs contradict retrieved context despite its relevance and accuracy. Existing approaches aiming to improve faithfulness primarily focus on enhancing the utilization of external context, but often overlook the persistent influence of internal parametric knowledge during generation. In this work, we investigate the internal mechanisms behind unfaithful generation and identify a subset of mid-to-deep feed-forward networks (FFNs) that are disproportionately activated in such cases. Building on this insight, we propose Parametric Knowledge Muting through FFN Suppression (ParamMute), a framework that improves contextual faithfulness by suppressing the activation of unfaithfulness-associated FFNs and calibrating the model toward retrieved knowledge. To evaluate our approach, we introduce CoFaithfulQA, a benchmark specifically designed to evaluate faithfulness in scenarios where internal knowledge conflicts with accurate external evidence. Experimental results show that ParamMute significantly enhances faithfulness across both CoFaithfulQA and the established ConFiQA benchmark, achieving substantial reductions in reliance on parametric memory. These findings underscore the importance of mitigating internal knowledge dominance and provide a new direction for improving LLM trustworthiness in RAG. All codes are available at https://github.com/OpenBMB/ParamMute.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Grow Less Humanlike beyond Phase Transition</title>
<link>https://arxiv.org/abs/2502.18802</link>
<guid>https://arxiv.org/abs/2502.18802</guid>
<content:encoded><![CDATA[
arXiv:2502.18802v2 Announce Type: replace 
Abstract: LMs' alignment with human reading behavior (i.e. psychometric predictive power; PPP) is known to improve during pretraining up to a tipping point, beyond which it either plateaus or degrades. Various factors, such as word frequency, recency bias in attention, and context size, have been theorized to affect PPP, yet there is no current account that explains why such a tipping point exists, and how it interacts with LMs' pretraining dynamics more generally. We hypothesize that the underlying factor is a pretraining phase transition, characterized by the rapid emergence of specialized attention heads. We conduct a series of correlational and causal experiments to show that such a phase transition is responsible for the tipping point in PPP. We then show that, rather than producing attention patterns that contribute to the degradation in PPP, phase transitions alter the subsequent learning dynamics of the model, such that further training keeps damaging PPP.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAPID: Long-Context Inference with Retrieval-Augmented Speculative Decoding</title>
<link>https://arxiv.org/abs/2502.20330</link>
<guid>https://arxiv.org/abs/2502.20330</guid>
<content:encoded><![CDATA[
arXiv:2502.20330v2 Announce Type: replace 
Abstract: The emergence of long-context large language models (LLMs) offers a promising alternative to traditional retrieval-augmented generation (RAG) for processing extensive documents. However, the computational overhead of long-context inference presents significant efficiency challenges. While Speculative Decoding (SD) traditionally accelerates inference using smaller draft models, its effectiveness diminishes substantially in long-context scenarios due to memory-bound KV cache operations. We introduce Retrieval-Augmented Speculative Decoding (RAPID), which leverages RAG for both accelerating and enhancing generation quality in long-context inference. RAPID introduces the RAG drafter-a draft LLM operating on shortened retrieval contexts-to speculate on the generation of long-context target LLMs. Our approach enables a new paradigm where same-scale or even larger LLMs can serve as RAG drafters while maintaining computational efficiency. To fully leverage the potentially superior capabilities from stronger RAG drafters, we develop an inference-time knowledge transfer that enriches the target distribution by RAG. Extensive experiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates the strengths of both RAG and long-context LLMs, achieving significant performance improvements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context inference. Our analyses also reveal the robustness of RAPID across various context lengths and retrieval quality.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM Knowledge Learning through Generalization</title>
<link>https://arxiv.org/abs/2503.03705</link>
<guid>https://arxiv.org/abs/2503.03705</guid>
<content:encoded><![CDATA[
arXiv:2503.03705v2 Announce Type: replace 
Abstract: As Large language models (LLMs) are increasingly deployed in diverse applications, faithfully integrating evolving factual knowledge into these models remains a critical challenge. Continued pre-training on paraphrased data has shown empirical promise for enhancing knowledge acquisition. However, this approach is often costly and unreliable, as it relies on external models or manual effort for rewriting, and may inadvertently alter the factual content. In this work, we hypothesize and empirically show that an LLM's ability to continually predict the same factual knowledge tokens given diverse paraphrased contexts is positively correlated with its capacity to extract that knowledge via question-answering. Based on this view and aiming to improve generalization to diverse paraphrased contexts, we introduce two strategies to enhance LLMs' ability to predict the same knowledge tokens given varied contexts, thereby enhancing knowledge acquisition. First, we propose formatting-based data augmentation, which diversifies documents conveying the same knowledge by altering document formats rather than their content, thereby preserving factual integrity. Second, we adopt sharpness-aware minimization as the optimizer to better improve generalization. Extensive experiments demonstrate our methods' effectiveness in both continued pre-training and instruction tuning, and further gains can be achieved by combining with paraphrased data.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiRAG: Retrieval-Augmented Generation with Hierarchical Knowledge</title>
<link>https://arxiv.org/abs/2503.10150</link>
<guid>https://arxiv.org/abs/2503.10150</guid>
<content:encoded><![CDATA[
arXiv:2503.10150v2 Announce Type: replace 
Abstract: Graph-based Retrieval-Augmented Generation (RAG) methods have significantly enhanced the performance of large language models (LLMs) in domain-specific tasks. However, existing RAG methods do not adequately utilize the naturally inherent hierarchical knowledge in human cognition, which limits the capabilities of RAG systems. In this paper, we introduce a new RAG approach, called HiRAG, which utilizes hierarchical knowledge to enhance the semantic understanding and structure capturing capabilities of RAG systems in the indexing and retrieval processes. Our extensive experiments demonstrate that HiRAG achieves significant performance improvements over the state-of-the-art baseline methods.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dual-Directional Context-Aware Test-Time Learning for Text Classification</title>
<link>https://arxiv.org/abs/2503.15469</link>
<guid>https://arxiv.org/abs/2503.15469</guid>
<content:encoded><![CDATA[
arXiv:2503.15469v5 Announce Type: replace 
Abstract: Text classification assigns text to predefined categories. Traditional methods struggle with complex structures and long-range dependencies. Deep learning with recurrent neural networks and Transformer models has improved feature extraction and context awareness. However, these models still trade off interpretability, efficiency and contextual range. We propose the Dynamic Bidirectional Elman Attention Network (DBEAN). DBEAN combines bidirectional temporal modeling and self-attention. It dynamically weights critical input segments and preserves computational efficiency.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data</title>
<link>https://arxiv.org/abs/2504.09895</link>
<guid>https://arxiv.org/abs/2504.09895</guid>
<content:encoded><![CDATA[
arXiv:2504.09895v2 Announce Type: replace 
Abstract: Large language models~(LLMs) are expected to be helpful, harmless, and honest. In alignment scenarios such as safety, confidence, and general preference alignment, binary preference data collection and reward modeling are resource-intensive but essential for transferring human preference. In this work, we explore using the similarity between sampled generations and high-quality reference answers as an alternative reward function choice for LLM alignment. Similarity reward circumvents binary preference data collection and reward modeling when unary high-quality reference answers are available. We introduce \textit{RefAlign}, a versatile REINFORCE-style alignment algorithm that does not rely on reference or reward models. RefAlign utilizes similarity metrics, such as BERTScore between sampled generations and reference answers as surrogate rewards. Beyond general human preference optimization, RefAlign can be readily extended to diverse scenarios, such as safety and confidence alignment, by incorporating the similarity reward with task-related objectives. In various scenarios, RefAlign demonstrates comparable performance to previous alignment methods without binary preference data and reward models.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Large Language Model based Human-Agent Systems</title>
<link>https://arxiv.org/abs/2505.00753</link>
<guid>https://arxiv.org/abs/2505.00753</guid>
<content:encoded><![CDATA[
arXiv:2505.00753v3 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. These human-agent collaboration systems enable humans and LLM-based agents to collaborate effectively by leveraging their complementary strengths. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities arising from human-AI collaboration. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proper Noun Diacritization for Arabic Wikipedia: A Benchmark Dataset</title>
<link>https://arxiv.org/abs/2505.02656</link>
<guid>https://arxiv.org/abs/2505.02656</guid>
<content:encoded><![CDATA[
arXiv:2505.02656v3 Announce Type: replace 
Abstract: Proper nouns in Arabic Wikipedia are frequently undiacritized, creating ambiguity in pronunciation and interpretation, especially for transliterated named entities of foreign origin. While transliteration and diacritization have been well-studied separately in Arabic NLP, their intersection remains underexplored. In this paper, we introduce a new manually diacritized dataset of Arabic proper nouns of various origins with their English Wikipedia equivalent glosses, and present the challenges and guidelines we followed to create it. We benchmark GPT-4o on the task of recovering full diacritization given the undiacritized Arabic and English forms, and analyze its performance. Achieving 73% accuracy, our results underscore both the difficulty of the task and the need for improved models and resources. We release our dataset to facilitate further research on Arabic Wikipedia proper noun diacritization.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking</title>
<link>https://arxiv.org/abs/2505.07891</link>
<guid>https://arxiv.org/abs/2505.07891</guid>
<content:encoded><![CDATA[
arXiv:2505.07891v2 Announce Type: replace 
Abstract: In the age of social media, the rapid spread of misinformation and rumors has led to the emergence of infodemics, where false information poses a significant threat to society. To combat this issue, we introduce TrumorGPT, a novel generative artificial intelligence solution designed for fact-checking in the health domain. TrumorGPT aims to distinguish "trumors", which are health-related rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts. This framework leverages a large language model (LLM) with few-shot learning for semantic health knowledge graph construction and semantic reasoning. TrumorGPT incorporates graph-based retrieval-augmented generation (GraphRAG) to address the hallucination issue common in LLMs and the limitations of static training data. GraphRAG involves accessing and utilizing information from regularly updated semantic health knowledge graphs that consist of the latest medical news and health information, ensuring that fact-checking by TrumorGPT is based on the most recent data. Evaluating with extensive healthcare datasets, TrumorGPT demonstrates superior performance in fact-checking for public health claims. Its ability to effectively conduct fact-checking across various platforms marks a critical step forward in the fight against health-related misinformation, enhancing trust and accuracy in the digital information age.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild</title>
<link>https://arxiv.org/abs/2505.16023</link>
<guid>https://arxiv.org/abs/2505.16023</guid>
<content:encoded><![CDATA[
arXiv:2505.16023v3 Announce Type: replace 
Abstract: As large language models (LLMs) are used in complex writing workflows, users engage in multi-turn interactions to steer generations to better fit their needs. Rather than passively accepting output, users actively refine, explore, and co-construct text. We conduct a large-scale analysis of this collaborative behavior for users engaged in writing tasks in the wild with two popular AI assistants, Bing Copilot and WildChat. Our analysis goes beyond simple task classification or satisfaction estimation common in prior work and instead characterizes how users interact with LLMs through the course of a session. We identify prototypical behaviors in how users interact with LLMs in prompts following their original request. We refer to these as Prototypical Human-AI Collaboration Behaviors (PATHs) and find that a small group of PATHs explain a majority of the variation seen in user-LLM interaction. These PATHs span users revising intents, exploring texts, posing questions, adjusting style or injecting new content. Next, we find statistically significant correlations between specific writing intents and PATHs, revealing how users' intents shape their collaboration behaviors. We conclude by discussing the implications of our findings on LLM alignment.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions</title>
<link>https://arxiv.org/abs/2505.16576</link>
<guid>https://arxiv.org/abs/2505.16576</guid>
<content:encoded><![CDATA[
arXiv:2505.16576v2 Announce Type: replace 
Abstract: Determining the veracity of atomic claims is an imperative component of many recently proposed fact-checking systems. Many approaches tackle this problem by first retrieving evidence by querying a search engine and then performing classification by providing the evidence set and atomic claim to a large language model, but this process deviates from what a human would do in order to perform the task. Recent work attempted to address this issue by proposing iterative evidence retrieval, allowing for evidence to be collected several times and only when necessary. Continuing along this line of research, we propose a novel claim verification system, called EMULATE, which is designed to better emulate human actions through the use of a multi-agent framework where each agent performs a small part of the larger task, such as ranking search results according to predefined criteria or evaluating webpage content. Extensive experiments on several benchmarks show clear improvements over prior work, demonstrating the efficacy of our new multi-agent framework.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When can isotropy help adapt LLMs' next word prediction to numerical domains?</title>
<link>https://arxiv.org/abs/2505.17135</link>
<guid>https://arxiv.org/abs/2505.17135</guid>
<content:encoded><![CDATA[
arXiv:2505.17135v4 Announce Type: replace 
Abstract: Vector representations of contextual embeddings learned by pre-trained large language models (LLMs) are effective in various downstream tasks in numerical domains such as time series forecasting. Despite their significant benefits, the tendency of LLMs to hallucinate in such domains can have severe consequences in applications such as energy, nature, finance, healthcare, retail and transportation, among others. To guarantee prediction reliability and accuracy in numerical domains, it is necessary to open the black box behind the LLM and provide performance guarantees through explanation. However, there is little theoretical understanding of when pre-trained language models help solve numerical downstream tasks. This paper seeks to bridge this gap by understanding when the next-word prediction capability of LLMs can be adapted to numerical domains through a novel analysis based on the concept of isotropy in the contextual embedding space. Specifically, a log-linear model for LLMs is considered in which numerical data can be predicted from its context through a network with softmax in the output layer of LLMs (i.e., language model head in self-attention). For this model, it is demonstrated that, in order to achieve state-of-the-art performance in numerical domains, the hidden representations of the LLM embeddings must possess a structure that accounts for the shift-invariance of the softmax function. By formulating a gradient structure of self-attention in pre-trained models, it is shown how the isotropic property of LLM embeddings in contextual embedding space preserves the underlying structure of representations, thereby resolving the shift-invariance problem and providing a performance guarantee. Experiments show that different characteristics of numerical data and model architectures have different impacts on isotropy, and this variability directly affects the performances.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback</title>
<link>https://arxiv.org/abs/2505.19514</link>
<guid>https://arxiv.org/abs/2505.19514</guid>
<content:encoded><![CDATA[
arXiv:2505.19514v2 Announce Type: replace 
Abstract: Prompt quality plays a critical role in the performance of large language models (LLMs), motivating a growing body of work on prompt optimization. Most existing methods optimize prompts over a fixed dataset, assuming static input distributions and offering limited support for iterative improvement. We introduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a closed-loop framework for prompt learning that integrates synthetic data generation into the optimization process. SIPDO couples a synthetic data generator with a prompt optimizer, where the generator produces new examples that reveal current prompt weaknesses and the optimizer incrementally refines the prompt in response. This feedback-driven loop enables systematic improvement of prompt performance without assuming access to external supervision or new tasks. Experiments across question answering and reasoning benchmarks show that SIPDO outperforms standard prompt tuning methods, highlighting the value of integrating data synthesis into prompt learning workflows.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pretraining Language Models to Ponder in Continuous Space</title>
<link>https://arxiv.org/abs/2505.20674</link>
<guid>https://arxiv.org/abs/2505.20674</guid>
<content:encoded><![CDATA[
arXiv:2505.20674v2 Announce Type: replace 
Abstract: Humans ponder before articulating complex sentence elements, enabling deeper cognitive processing through focused effort. In this work, we introduce this pondering process into language models by repeatedly invoking the forward process within a single token generation step. During pondering, instead of generating an actual token sampled from the prediction distribution, the model ponders by yielding a weighted sum of all token embeddings according to the predicted token distribution. The generated embedding is then fed back as input for another forward pass. We show that the model can learn to ponder in this way through self-supervised learning, without any human annotations. Experiments across three widely used open-source architectures-GPT-2, Pythia, and LLaMA-and extensive downstream task evaluations demonstrate the effectiveness and generality of our method. For language modeling tasks, pondering language models achieve performance comparable to vanilla models with twice the number of parameters. On 9 downstream benchmarks, our pondering-enhanced Pythia models significantly outperform the official Pythia models. Notably, PonderingPythia-2.8B surpasses Pythia-6.9B, and PonderingPythia-1B is comparable to TinyLlama-1.1B, which is trained on 10 times more data. The code is available at https://github.com/LUMIA-Group/PonderingLM.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset</title>
<link>https://arxiv.org/abs/2505.21979</link>
<guid>https://arxiv.org/abs/2505.21979</guid>
<content:encoded><![CDATA[
arXiv:2505.21979v2 Announce Type: replace 
Abstract: Mainstream large vision-language models (LVLMs) inherently encode cultural biases, highlighting the need for diverse multimodal datasets. To address this gap, we introduce Pearl, a large-scale Arabic multimodal dataset and benchmark explicitly designed for cultural understanding. Constructed through advanced agentic workflows and extensive human-in-the-loop annotations by 45 annotators from across the Arab world, Pearl comprises over K multimodal examples spanning ten culturally significant domains covering all Arab countries. We further provide two robust evaluation benchmarks Pearl and Pearl-Lite along with a specialized subset Pearl-X explicitly developed to assess nuanced cultural variations. Comprehensive evaluations on state-of-the-art open and proprietary LVLMs demonstrate that reasoning-centric instruction alignment substantially improves models' cultural grounding compared to conventional scaling methods. Pearl establishes a foundational resource for advancing culturally-informed multimodal modeling research. All datasets and benchmarks are publicly available.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX</title>
<link>https://arxiv.org/abs/2505.24616</link>
<guid>https://arxiv.org/abs/2505.24616</guid>
<content:encoded><![CDATA[
arXiv:2505.24616v2 Announce Type: replace 
Abstract: We introduce POLLUX, a comprehensive open-source benchmark designed to evaluate the generative capabilities of large language models (LLMs) in Russian. Our main contribution is a novel evaluation methodology that enhances the interpretability of LLM assessment. For each task type, we define a set of detailed criteria and develop a scoring protocol where models evaluate responses and provide justifications for their ratings. This enables transparent, criteria-driven evaluation beyond traditional resource-consuming, side-by-side human comparisons. POLLUX includes a detailed, fine-grained taxonomy of 35 task types covering diverse generative domains such as code generation, creative writing, and practical assistant use cases, totaling 2,100 manually crafted and professionally authored prompts. Each task is categorized by difficulty (easy/medium/hard), with experts constructing the dataset entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B) evaluators trained for nuanced assessment of generative outputs. This approach provides scalable, interpretable evaluation and annotation tools for model development, effectively replacing costly and less precise human judgments.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Debiasing for Noisy In-Context Learning for Text Generation</title>
<link>https://arxiv.org/abs/2506.00418</link>
<guid>https://arxiv.org/abs/2506.00418</guid>
<content:encoded><![CDATA[
arXiv:2506.00418v2 Announce Type: replace 
Abstract: In context learning (ICL) relies heavily on high quality demonstrations drawn from large annotated corpora. Existing approaches detect noisy annotations by ranking local perplexities, presuming that noisy samples yield higher perplexities than their clean counterparts. However, this assumption breaks down when the noise ratio is high and many demonstrations are flawed. We reexamine the perplexity based paradigm for text generation under noisy annotations, highlighting two sources of bias in perplexity: the annotation itself and the domain specific knowledge inherent in large language models (LLMs). To overcome these biases, we introduce a dual debiasing framework that uses synthesized neighbors to explicitly correct perplexity estimates, yielding a robust Sample Cleanliness Score. This metric uncovers absolute sample cleanliness regardless of the overall corpus noise level. Extensive experiments demonstrate our method's superior noise detection capabilities and show that its final ICL performance is comparable to that of a fully clean demonstration corpus. Moreover, our approach remains robust even when noise ratios are extremely high.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists</title>
<link>https://arxiv.org/abs/2506.01241</link>
<guid>https://arxiv.org/abs/2506.01241</guid>
<content:encoded><![CDATA[
arXiv:2506.01241v2 Announce Type: replace 
Abstract: This paper introduces ExpertLongBench, an expert-level benchmark containing 11 tasks from 9 domains that reflect realistic expert workflows and applications. Beyond question answering, the application-driven tasks in ExpertLongBench demand long-form outputs that can exceed 5,000 tokens and strict adherence to domain-specific requirements. Notably, each task in ExpertLongBench includes a rubric, designed or validated by domain experts, to specify task requirements and guide output evaluation. Furthermore, we propose CLEAR, an evaluation framework that supports accurate evaluation of long-form model outputs in our benchmark. To achieve fine-grained, expert-aligned evaluation, CLEAR derives checklists from both model outputs and references by extracting information corresponding to items in the task-specific rubric. Checklist items for model outputs are then compared with corresponding items for reference outputs to assess their correctness, enabling grounded evaluation. We benchmark 11 large language models (LLMs) and analyze components in CLEAR, showing that (1) existing LLMs, with the top performer achieving only a 26.8% F1 score, require significant improvement for expert-level tasks; (2) models can generate content corresponding to the required aspects, though often not accurately; and (3) accurate checklist extraction and comparison in CLEAR can be achieved by open-weight models for more scalable and low-cost usage.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.01713</link>
<guid>https://arxiv.org/abs/2506.01713</guid>
<content:encoded><![CDATA[
arXiv:2506.01713v2 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) have shown promising capabilities in reasoning tasks, yet still struggle with complex problems requiring explicit self-reflection and self-correction, especially compared to their unimodal text-based counterparts. Existing reflection methods are simplistic and struggle to generate meaningful and instructive feedback, as the reasoning ability and knowledge limits of pre-trained models are largely fixed during initial training. To overcome these challenges, we propose Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO), a two-stage reflection-aware reinforcement learning (RL) framework explicitly designed to enhance multimodal LLM reasoning. In the first stage, we construct a high-quality, reflection-focused dataset under the guidance of an advanced MLLM, which generates reflections based on initial responses to help the policy model learn both reasoning and self-reflection. In the second stage, we introduce a novel reward mechanism within the GRPO framework that encourages concise and cognitively meaningful reflection while avoiding redundancy. Extensive experiments across multiple multimodal reasoning benchmarks, including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms state-of-the-art models, achieving notable improvements in both reasoning accuracy and reflection quality.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts</title>
<link>https://arxiv.org/abs/2506.02000</link>
<guid>https://arxiv.org/abs/2506.02000</guid>
<content:encoded><![CDATA[
arXiv:2506.02000v2 Announce Type: replace 
Abstract: Current large language models (LLMs) struggle to answer questions that span tens of thousands of tokens, especially when multi-hop reasoning is involved. While prior benchmarks explore long-context comprehension or multi-hop reasoning in isolation, none jointly vary context length and reasoning depth in natural narrative settings. We introduce NovelHopQA, the first benchmark to evaluate 1-4 hop QA over 64k-128k-token excerpts from 83 full-length public-domain novels. A keyword-guided pipeline builds hop-separated chains grounded in coherent storylines. We evaluate seven state-of-the-art models and apply oracle-context filtering to ensure all questions are genuinely answerable. Human annotators validate both alignment and hop depth. We additionally present retrieval-augmented generation (RAG) evaluations to test model performance when only selected passages are provided instead of the full context. We noticed consistent accuracy drops with increased hops and context length increase, even for frontier models-revealing that sheer scale does not guarantee robust reasoning. Failure-mode analysis highlights common breakdowns such as missed final-hop integration and long-range drift. NovelHopQA offers a controlled diagnostic setting to test multi-hop reasoning at scale. All code and datasets are available at https://novelhopqa.github.io.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Efficiency of Long Document Classification using Sentence Ranking Approach</title>
<link>https://arxiv.org/abs/2506.07248</link>
<guid>https://arxiv.org/abs/2506.07248</guid>
<content:encoded><![CDATA[
arXiv:2506.07248v2 Announce Type: replace 
Abstract: Long document classification poses challenges due to the computational limitations of transformer-based models, particularly BERT, which are constrained by fixed input lengths and quadratic attention complexity. Moreover, using the full document for classification is often redundant, as only a subset of sentences typically carries the necessary information. To address this, we propose a TF-IDF-based sentence ranking method that improves efficiency by selecting the most informative content. Our approach explores fixed-count and percentage-based sentence selection, along with an enhanced scoring strategy combining normalized TF-IDF scores and sentence length. Evaluated on the MahaNews LDC dataset of long Marathi news articles, the method consistently outperforms baselines such as first, last, and random sentence selection. With MahaBERT-v2, we achieve near-identical classification accuracy with just a 0.33 percent drop compared to the full-context baseline, while reducing input size by over 50 percent and inference latency by 43 percent. This demonstrates that significant context reduction is possible without sacrificing performance, making the method practical for real-world long document classification tasks.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LGAI-EMBEDDING-Preview Technical Report</title>
<link>https://arxiv.org/abs/2506.07438</link>
<guid>https://arxiv.org/abs/2506.07438</guid>
<content:encoded><![CDATA[
arXiv:2506.07438v2 Announce Type: replace 
Abstract: This report presents a unified instruction-based framework for learning generalized text embeddings optimized for both information retrieval (IR) and non-IR tasks. Built upon a decoder-only large language model (Mistral-7B), our approach combines in-context learning, soft supervision, and adaptive hard-negative mining to generate context-aware embeddings without task-specific fine-tuning. Structured instructions and few-shot examples are used to guide the model across diverse tasks, enabling strong performance on classification, semantic similarity, clustering, and reranking benchmarks. To improve semantic discrimination, we employ a soft labeling framework where continuous relevance scores, distilled from a high-performance dense retriever and reranker, serve as fine-grained supervision signals. In addition, we introduce adaptive margin-based hard-negative mining, which filters out semantically ambiguous negatives based on their similarity to positive examples, thereby enhancing training stability and retrieval robustness. Our model is evaluated on the newly introduced MTEB (English, v2) benchmark, covering 41 tasks across seven categories. Results show that our method achieves strong generalization and ranks among the top-performing models by Borda score, outperforming several larger or fully fine-tuned baselines. These findings highlight the effectiveness of combining in-context prompting, soft supervision, and adaptive sampling for scalable, high-quality embedding generation.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Piloting Copilot, Codex, and StarCoder2: Hot Temperature, Cold Prompts, or Black Magic?</title>
<link>https://arxiv.org/abs/2210.14699</link>
<guid>https://arxiv.org/abs/2210.14699</guid>
<content:encoded><![CDATA[
arXiv:2210.14699v3 Announce Type: replace-cross 
Abstract: Language models are promising solutions for tackling increasing complex problems. In software engineering, they recently gained attention in code assistants, which generate programs from a natural language task description (prompt). They have the potential to save time and effort but remain poorly understood, limiting their optimal use. In this article, we investigate the impact of input variations on two configurations of a language model, focusing on parameters such as task description, surrounding context, model creativity, and the number of generated solutions. We design specific operators to modify these inputs and apply them to three LLM-based code assistants (Copilot, Codex, StarCoder2) and two benchmarks representing algorithmic problems (HumanEval, LeetCode). Our study examines whether these variations significantly affect program quality and how these effects generalize across models. Our results show that varying input parameters can greatly improve performance, achieving up to 79.27% success in one-shot generation compared to 22.44% for Codex and 31.1% for Copilot in default settings. Actioning this potential in practice is challenging due to the complex interplay in our study - the optimal settings for temperature, prompt, and number of generated solutions vary by problem. Reproducing our study with StarCoder2 confirms these findings, indicating they are not model-specific. We also uncover surprising behaviors (e.g., fully removing the prompt can be effective), revealing model brittleness and areas for improvement.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Large Language Models Meet Vector Databases: A Survey</title>
<link>https://arxiv.org/abs/2402.01763</link>
<guid>https://arxiv.org/abs/2402.01763</guid>
<content:encoded><![CDATA[
arXiv:2402.01763v4 Announce Type: replace-cross 
Abstract: This survey explores the synergistic potential of Large Language Models (LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving research area. With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues. VecDBs emerge as a compelling solution to these issues by offering an efficient means to store, retrieve, and manage the high-dimensional vector representations intrinsic to LLM operations. Through this nuanced review, we delineate the foundational principles of LLMs and VecDBs and critically analyze their integration's impact on enhancing LLM functionalities. This discourse extends into a discussion on the speculative future developments in this domain, aiming to catalyze further research into optimizing the confluence of LLMs and VecDBs for advanced data handling and knowledge extraction capabilities.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$L^*LM$: Learning Automata from Examples using Natural Language Oracles</title>
<link>https://arxiv.org/abs/2402.07051</link>
<guid>https://arxiv.org/abs/2402.07051</guid>
<content:encoded><![CDATA[
arXiv:2402.07051v2 Announce Type: replace-cross 
Abstract: Expert demonstrations have proven an easy way to indirectly specify complex tasks. Recent algorithms even support extracting unambiguous formal specifications, e.g. deterministic finite automata (DFA), from demonstrations. Unfortunately, these techniques are generally not sample efficient. In this work, we introduce $L^*LM$, an algorithm for learning DFAs from both demonstrations and natural language. Due to the expressivity of natural language, we observe a significant improvement in the data efficiency of learning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large language models to answer membership queries about the underlying task. This is then combined with recent techniques for transforming learning from demonstrations into a sequence of labeled example learning problems. In our experiments, we observe the two modalities complement each other, yielding a powerful few-shot learner.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs with Multiple Problems at once</title>
<link>https://arxiv.org/abs/2406.10786</link>
<guid>https://arxiv.org/abs/2406.10786</guid>
<content:encoded><![CDATA[
arXiv:2406.10786v3 Announce Type: replace-cross 
Abstract: This paper shows the benefits and fruitfulness of evaluating LLMs with multiple problems at once, a paradigm we call multi-problem evaluation (MPE). Unlike conventional single-problem evaluation, where a prompt presents a single problem and expects one specific answer, MPE places multiple problems together in a single prompt and assesses how well an LLM answers all these problems in a single output. Leveraging 6 classification and 12 reasoning benchmarks that already exist, we introduce a new benchmark called ZeMPE (Zero-shot Multi-Problem Evaluation), comprising 53,100 zero-shot multi-problem prompts. We experiment with a total of 13 LLMs from 5 model families on ZeMPE to present a comprehensive and systematic MPE. Our results show that LLMs are capable of handling multiple problems from a single data source as well as handling them separately, but there are conditions this multiple problem handling capability falls short. In addition, we perform in-depth further analyses and explore model-level factors that may enable multiple problem handling capabilities in LLMs. We release our corpus and code to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Numeric Expressions in Automatic Speech Recognition</title>
<link>https://arxiv.org/abs/2408.00004</link>
<guid>https://arxiv.org/abs/2408.00004</guid>
<content:encoded><![CDATA[
arXiv:2408.00004v2 Announce Type: replace-cross 
Abstract: This paper addresses the problem of correctly formatting numeric expressions in automatic speech recognition (ASR) transcripts. This is challenging since the expected transcript format depends on the context, e.g., 1945 (year) vs. 19:45 (timestamp). We compare cascaded and end-to-end approaches to recognize and format numeric expressions such as years, timestamps, currency amounts, and quantities. For the end-to-end approach, we employed a data generation strategy using a large language model (LLM) together with a text to speech (TTS) model to generate adaptation data. The results on our test data set show that while approaches based on LLMs perform well in recognizing formatted numeric expressions, adapted end-to-end models offer competitive performance with the advantage of lower latency and inference cost.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sycophancy in Vision-Language Models: A Systematic Analysis and an Inference-Time Mitigation Framework</title>
<link>https://arxiv.org/abs/2408.11261</link>
<guid>https://arxiv.org/abs/2408.11261</guid>
<content:encoded><![CDATA[
arXiv:2408.11261v2 Announce Type: replace-cross 
Abstract: Large Vision-Language Models (LVLMs) have shown significant capability in vision-language understanding. However, one critical issue that persists in these models is sycophancy, where models are unduly influenced by leading or deceptive prompts, resulting in biased outputs and hallucinations. Despite the rapid development of LVLMs, evaluating and mitigating sycophancy remains largely under-explored. In this work, we fill this gap by systematically analyzing sycophancy across multiple vision-language benchmarks and propose an inference-time mitigation framework. We curate leading queries and quantify the susceptibility of state-of-the-art LVLMs to prompt-induced bias, revealing consistent performance degradation and instability across models and tasks. Our analysis further uncovers model-specific behavioral traits, such as sentiment sensitivity and prediction polarity shifts under sycophancy. To mitigate these issues, we propose a training-free, model-agnostic framework that operates entirely at inference time. Our approach first employs a query neutralizer, leveraging an language model to suppress implicit sycophantic bias in user queries. We then introduce a sycophancy-aware contrastive decoding mechanism that dynamically recalibrates token-level output distributions by contrasting responses to neutralized and leading queries. Finally, an adaptive logits refinement module further modifies the contrasted logits by integrating both a adaptive plausibility filter and query sentiment scaler, ensuring coherent and robust generation. Extensive experiments demonstrate that this framework effectively mitigates sycophancy across all evaluated models, while maintaining performance on neutral prompts. Our results suggest that sycophancy in LVLMs is a general and urgent challenge, and that inference-time strategies offer a promising path toward trustworthy multimodal reasoning.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RePST: Language Model Empowered Spatio-Temporal Forecasting via Semantic-Oriented Reprogramming</title>
<link>https://arxiv.org/abs/2408.14505</link>
<guid>https://arxiv.org/abs/2408.14505</guid>
<content:encoded><![CDATA[
arXiv:2408.14505v3 Announce Type: replace-cross 
Abstract: Spatio-temporal forecasting is pivotal in numerous real-world applications, including transportation planning, energy management, and climate monitoring. In this work, we aim to harness the reasoning and generalization abilities of Pre-trained Language Models (PLMs) for more effective spatio-temporal forecasting, particularly in data-scarce scenarios. However, recent studies uncover that PLMs, which are primarily trained on textual data, often falter when tasked with modeling the intricate correlations in numerical time series, thereby limiting their effectiveness in comprehending spatio-temporal data. To bridge the gap, we propose RePST, a semantic-oriented PLM reprogramming framework tailored for spatio-temporal forecasting. Specifically, we first propose a semantic-oriented decomposer that adaptively disentangles spatially correlated time series into interpretable sub-components, which facilitates PLM to understand sophisticated spatio-temporal dynamics via a divide-and-conquer strategy. Moreover, we propose a selective discrete reprogramming scheme, which introduces an expanded spatio-temporal vocabulary space to project spatio-temporal series into discrete representations. This scheme minimizes the information loss during reprogramming and enriches the representations derived by PLMs. Extensive experiments on real-world datasets show that the proposed RePST outperforms twelve state-of-the-art baseline methods, particularly in data-scarce scenarios, highlighting the effectiveness and superior generalization capabilities of PLMs for spatio-temporal forecasting. Our codes can be found at https://github.com/usail-hkust/REPST.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking and Building Zero-Shot Hindi Retrieval Model with Hindi-BEIR and NLLB-E5</title>
<link>https://arxiv.org/abs/2409.05401</link>
<guid>https://arxiv.org/abs/2409.05401</guid>
<content:encoded><![CDATA[
arXiv:2409.05401v3 Announce Type: replace-cross 
Abstract: Given the large number of Hindi speakers worldwide, there is a pressing need for robust and efficient information retrieval systems for Hindi. Despite ongoing research, comprehensive benchmarks for evaluating retrieval models in Hindi are lacking. To address this gap, we introduce the Hindi-BEIR benchmark, comprising 15 datasets across seven distinct tasks. We evaluate state-of-the-art multilingual retrieval models on the Hindi-BEIR benchmark, identifying task and domain-specific challenges that impact Hindi retrieval performance. Building on the insights from these results, we introduce NLLB-E5, a multilingual retrieval model that leverages a zero-shot approach to support Hindi without the need for Hindi training data. We believe our contributions, which include the release of the Hindi-BEIR benchmark and the NLLB-E5 model, will prove to be a valuable resource for researchers and promote advancements in multilingual retrieval models.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Circuit Compositions: Exploring Modular Structures in Transformer-Based Language Models</title>
<link>https://arxiv.org/abs/2410.01434</link>
<guid>https://arxiv.org/abs/2410.01434</guid>
<content:encoded><![CDATA[
arXiv:2410.01434v3 Announce Type: replace-cross 
Abstract: A fundamental question in interpretability research is to what extent neural networks, particularly language models, implement reusable functions through subnetworks that can be composed to perform more complex tasks. Recent advances in mechanistic interpretability have made progress in identifying $\textit{circuits}$, which represent the minimal computational subgraphs responsible for a model's behavior on specific tasks. However, most studies focus on identifying circuits for individual tasks without investigating how functionally similar circuits $\textit{relate}$ to each other. To address this gap, we study the modularity of neural networks by analyzing circuits for highly compositional subtasks within a transformer-based language model. Specifically, given a probabilistic context-free grammar, we identify and compare circuits responsible for ten modular string-edit operations. Our results indicate that functionally similar circuits exhibit both notable node overlap and cross-task faithfulness. Moreover, we demonstrate that the circuits identified can be reused and combined through set operations to represent more complex functional model capabilities.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FutureFill: Fast Generation from Convolutional Sequence Models</title>
<link>https://arxiv.org/abs/2410.03766</link>
<guid>https://arxiv.org/abs/2410.03766</guid>
<content:encoded><![CDATA[
arXiv:2410.03766v3 Announce Type: replace-cross 
Abstract: We address the challenge of efficient auto-regressive generation in sequence prediction models by introducing FutureFill, a general-purpose fast generation method for any sequence prediction algorithm based on convolutional operators. FutureFill reduces generation time from quadratic to quasilinear in the context length. Moreover, when generating from a prompt, it requires a prefill cache whose size grows only with the number of tokens to be generated, often much smaller than the caches required by standard convolutional or attention based models. We validate our theoretical claims with experiments on synthetic tasks and demonstrate substantial efficiency gains when generating from a deep convolutional sequence prediction model.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Numerical Precision Affects Arithmetical Reasoning Capabilities of LLMs</title>
<link>https://arxiv.org/abs/2410.13857</link>
<guid>https://arxiv.org/abs/2410.13857</guid>
<content:encoded><![CDATA[
arXiv:2410.13857v2 Announce Type: replace-cross 
Abstract: Despite the remarkable success of Transformer-based large language models (LLMs) across various domains, understanding and enhancing their mathematical capabilities remains a significant challenge. In this paper, we conduct a rigorous theoretical analysis of LLMs' mathematical abilities, with a specific focus on their arithmetic performances. We identify numerical precision as a key factor that influences their effectiveness in arithmetical tasks. Our results show that Transformers operating with low numerical precision fail to address arithmetic tasks, such as iterated addition and integer multiplication, unless the model size grows super-polynomially with respect to the input length. In contrast, Transformers with standard numerical precision can efficiently handle these tasks with significantly smaller model sizes. We further support our theoretical findings through empirical experiments that explore the impact of varying numerical precision on arithmetic tasks, providing valuable insights for improving the mathematical reasoning capabilities of LLMs.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA vs Full Fine-tuning: An Illusion of Equivalence</title>
<link>https://arxiv.org/abs/2410.21228</link>
<guid>https://arxiv.org/abs/2410.21228</guid>
<content:encoded><![CDATA[
arXiv:2410.21228v2 Announce Type: replace-cross 
Abstract: Fine-tuning is a crucial paradigm for adapting pre-trained large language models to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA) have been shown to effectively fine-tune LLMs with an extreme reduction in trainable parameters. But, \emph{are their learned solutions really equivalent?} We study how LoRA and full-finetuning change pre-trained models by analyzing the model's weight matrices through the lens of their spectral properties. We find that LoRA and full fine-tuning yield weight matrices whose singular value decompositions exhibit very different structure: weight matrices trained with LoRA have new, high-ranking singular vectors, which we call \emph{intruder dimensions}, while those trained with full fine-tuning do not. Further, we extend the finding that LoRA forgets less than full fine-tuning and find its forgetting is vastly localized to the intruder dimension -- by causally intervening on the intruder dimensions by changing their associated singular values post-fine-tuning, we show that they cause forgetting. Moreover, scaling them down significantly improves modeling of the pre-training distribution with a minimal drop in downstream task performance. Given this, we should expect accumulating intruder dimensions to be harmful and lead to more forgetting. This will be amplified during continual learning because of sequentially fine-tuning, and we show that LoRA models do accumulate intruder dimensions here tend to perform worse in this setting, emphasizing the practicality of our findings.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlzheimerRAG: Multimodal Retrieval Augmented Generation for Clinical Use Cases using PubMed articles</title>
<link>https://arxiv.org/abs/2412.16701</link>
<guid>https://arxiv.org/abs/2412.16701</guid>
<content:encoded><![CDATA[
arXiv:2412.16701v2 Announce Type: replace-cross 
Abstract: Recent advancements in generative AI have fostered the development of highly adept Large Language Models (LLMs) that integrate diverse data types to empower decision-making. Among these, multimodal retrieval-augmented generation (RAG) applications are promising because they combine the strengths of information retrieval and generative models, enhancing their utility across various domains, including clinical use cases. This paper introduces AlzheimerRAG, a Multimodal RAG application for clinical use cases, primarily focusing on Alzheimer's Disease case studies from PubMed articles. This application incorporates cross-modal attention fusion techniques to integrate textual and visual data processing by efficiently indexing and accessing vast amounts of biomedical literature. Our experimental results, compared to benchmarks such as BioASQ and PubMedQA, have yielded improved performance in the retrieval and synthesis of domain-specific information. We also present a case study using our multimodal RAG in various Alzheimer's clinical scenarios. We infer that AlzheimerRAG can generate responses with accuracy non-inferior to humans and with low rates of hallucination.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Potential of Encoder-free Architectures in 3D LMMs</title>
<link>https://arxiv.org/abs/2502.09620</link>
<guid>https://arxiv.org/abs/2502.09620</guid>
<content:encoded><![CDATA[
arXiv:2502.09620v3 Announce Type: replace-cross 
Abstract: Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to alleviate the challenges of encoder-based 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art model, ShapeLLM-13B, achieving 55.10%, 50.98%, and 43.10% on the classification, captioning, and VQA tasks, respectively. Our results demonstrate that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at https://github.com/Ivan-Tang-3D/ENEL
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PlanGenLLMs: A Modern Survey of LLM Planning Capabilities</title>
<link>https://arxiv.org/abs/2502.11221</link>
<guid>https://arxiv.org/abs/2502.11221</guid>
<content:encoded><![CDATA[
arXiv:2502.11221v3 Announce Type: replace-cross 
Abstract: LLMs have immense potential for generating plans, transforming an initial world state into a desired goal state. A large body of research has explored the use of LLMs for various planning tasks, from web navigation to travel planning and database querying. However, many of these systems are tailored to specific problems, making it challenging to compare them or determine the best approach for new tasks. There is also a lack of clear and consistent evaluation criteria. Our survey aims to offer a comprehensive overview of current LLM planners to fill this gap. It builds on foundational work by Kartam and Wilkins (1990) and examines six key performance criteria: completeness, executability, optimality, representation, generalization, and efficiency. For each, we provide a thorough analysis of representative works and highlight their strengths and weaknesses. Our paper also identifies crucial future directions, making it a valuable resource for both practitioners and newcomers interested in leveraging LLM planning to support agentic workflows.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering LLMs for Formal Theorem Proving</title>
<link>https://arxiv.org/abs/2502.15507</link>
<guid>https://arxiv.org/abs/2502.15507</guid>
<content:encoded><![CDATA[
arXiv:2502.15507v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown promise in proving formal theorems using proof assistants like Lean. However, current state of the art language models struggles to predict next step in proofs leading practitioners to use different sampling techniques to improve LLMs capabilities. We observe that the LLM is capable of predicting the correct tactic; however, it faces challenges in ranking it appropriately within the set of candidate tactics, affecting the overall selection process. To overcome this hurdle, we use activation steering to guide LLMs responses to improve the generations at the time of inference. Our results suggest that activation steering offers a promising lightweight alternative to specialized fine-tuning for enhancing theorem proving capabilities in LLMs, particularly valuable in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directional Gradient Projection for Robust Fine-Tuning of Foundation Models</title>
<link>https://arxiv.org/abs/2502.15895</link>
<guid>https://arxiv.org/abs/2502.15895</guid>
<content:encoded><![CDATA[
arXiv:2502.15895v2 Announce Type: replace-cross 
Abstract: Robust fine-tuning aims to adapt large foundation models to downstream tasks while preserving their robustness to distribution shifts. Existing methods primarily focus on constraining and projecting current model towards the pre-trained initialization based on the magnitudes between fine-tuned and pre-trained weights, which often require extensive hyper-parameter tuning and can sometimes result in underfitting. In this work, we propose Directional Gradient Projection (DiGraP), a novel layer-wise trainable method that incorporates directional information from gradients to bridge regularization and multi-objective optimization. Besides demonstrating our method on image classification, as another contribution we generalize this area to the multi-modal evaluation settings for robust fine-tuning. Specifically, we first bridge the uni-modal and multi-modal gap by performing analysis on Image Classification reformulated Visual Question Answering (VQA) benchmarks and further categorize ten out-of-distribution (OOD) VQA datasets by distribution shift types and degree (i.e. near versus far OOD). Experimental results show that DiGraP consistently outperforms existing baselines across Image Classfication and VQA tasks with discriminative and generative backbones, improving both in-distribution (ID) generalization and OOD robustness.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training</title>
<link>https://arxiv.org/abs/2504.09710</link>
<guid>https://arxiv.org/abs/2504.09710</guid>
<content:encoded><![CDATA[
arXiv:2504.09710v2 Announce Type: replace-cross 
Abstract: Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason under Off-Policy Guidance</title>
<link>https://arxiv.org/abs/2504.14945</link>
<guid>https://arxiv.org/abs/2504.14945</guid>
<content:encoded><![CDATA[
arXiv:2504.14945v5 Announce Type: replace-cross 
Abstract: Recent advances in large reasoning models (LRMs) demonstrate that sophisticated behaviors such as multi-step reasoning and self-reflection can emerge via reinforcement learning with verifiable rewards~(\textit{RLVR}). However, existing \textit{RLVR} approaches are inherently ``on-policy'', limiting learning to a model's own outputs and failing to acquire reasoning abilities beyond its initial capabilities. To address this issue, we introduce \textbf{LUFFY} (\textbf{L}earning to reason \textbf{U}nder o\textbf{FF}-polic\textbf{Y} guidance), a framework that augments \textit{RLVR} with off-policy reasoning traces. LUFFY dynamically balances imitation and exploration by combining off-policy demonstrations with on-policy rollouts during training. Specifically, LUFFY combines the Mixed-Policy GRPO framework, which has a theoretically guaranteed convergence rate, alongside policy shaping via regularized importance sampling to avoid superficial and rigid imitation during mixed-policy training. Compared with previous RLVR methods, LUFFY achieves an over \textbf{+6.4} average gain across six math benchmarks and an advantage of over \textbf{+6.2} points in out-of-distribution tasks. Most significantly, we show that LUFFY successfully trains weak models in scenarios where on-policy RLVR completely fails. These results provide compelling evidence that LUFFY transcends the fundamental limitations of on-policy RLVR and demonstrates the great potential of utilizing off-policy guidance in RLVR.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference</title>
<link>https://arxiv.org/abs/2505.12260</link>
<guid>https://arxiv.org/abs/2505.12260</guid>
<content:encoded><![CDATA[
arXiv:2505.12260v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode queries and documents into low-dimensional dense or high-dimensional sparse vectors. It retrieves documents relevant to search queries based on vector similarities. Documents are pre-encoded offline, while queries arrive in real-time, necessitating an efficient online query encoder. Although LLMs significantly enhance retrieval capabilities, serving deeply parameterized LLMs slows down query inference throughput and increases demands for online deployment resources. In this paper, we propose LightRetriever, a novel LLM-based hybrid retriever with extremely lightweight query encoders. Our method retains a full-sized LLM for document encoding, but reduces the workload of query encoding to no more than an embedding lookup. Compared to serving a full-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for query inference with GPU acceleration, and even a 20x speedup without GPU. Experiments on large-scale retrieval benchmarks demonstrate that our method generalizes well across diverse retrieval tasks, retaining an average of 95% full-sized performance.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training</title>
<link>https://arxiv.org/abs/2505.17331</link>
<guid>https://arxiv.org/abs/2505.17331</guid>
<content:encoded><![CDATA[
arXiv:2505.17331v2 Announce Type: replace-cross 
Abstract: This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to improve both the training speed and inference throughput of LLaMA architectures while maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models into shared KV caching across certain layers, significantly reducing KV computational complexity while maintaining or improving language performance. Experimental results demonstrate that ECHO-LLaMA achieves up to 77\% higher token-per-second throughput during training, up to 16\% higher Model FLOPs Utilization (MFU), and up to 14\% lower loss when trained on an equal number of tokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\% higher test-time throughput compared to the baseline. By introducing a computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable and cost-effective solution for pretraining and finetuning large language models, enabling faster and more resource-efficient training without compromising performance.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2505.20897</link>
<guid>https://arxiv.org/abs/2505.20897</guid>
<content:encoded><![CDATA[
arXiv:2505.20897v2 Announce Type: replace-cross 
Abstract: Vision-and-Language Navigation (VLN) requires the agent to navigate by following natural instructions under partial observability, making it difficult to align perception with language. Recent methods mitigate this by imagining future scenes, yet they rely on vision-based synthesis, leading to high computational cost and redundant details. To this end, we propose to adaptively imagine key environmental semantics via \textit{language} form, enabling a more reliable and efficient strategy. Specifically, we introduce a novel Adaptive Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a large language model (LLM). ATD is designed with a human-like left-right brain architecture, where the left brain focuses on logical integration, and the right brain is responsible for imaginative prediction of future scenes. To achieve this, we fine-tune only the Q-former within both brains to efficiently activate domain-specific knowledge in the LLM, enabling dynamic updates of logical reasoning and imagination during navigation. Furthermore, we introduce a cross-interaction mechanism to regularize the imagined outputs and inject them into a navigation expert module, allowing ATD to jointly exploit both the reasoning capacity of the LLM and the expertise of the navigation model. We conduct extensive experiments on the R2R benchmark, where ATD achieves state-of-the-art performance with fewer parameters. The code is \href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)</title>
<link>https://arxiv.org/abs/2505.21091</link>
<guid>https://arxiv.org/abs/2505.21091</guid>
<content:encoded><![CDATA[
arXiv:2505.21091v3 Announce Type: replace-cross 
Abstract: System prompts in Large Language Models (LLMs) are predefined directives that guide model behaviour, taking precedence over user inputs in text processing and generation. LLM deployers increasingly use them to ensure consistent responses across contexts. While model providers set a foundation of system prompts, deployers and third-party developers can append additional prompts without visibility into others' additions, while this layered implementation remains entirely hidden from end-users. As system prompts become more complex, they can directly or indirectly introduce unaccounted for side effects. This lack of transparency raises fundamental questions about how the position of information in different directives shapes model outputs. As such, this work examines how the placement of information affects model behaviour. To this end, we compare how models process demographic information in system versus user prompts across six commercially available LLMs and 50 demographic groups. Our analysis reveals significant biases, manifesting in differences in user representation and decision-making scenarios. Since these variations stem from inaccessible and opaque system-level configurations, they risk representational, allocative and potential other biases and downstream harms beyond the user's ability to detect or correct. Our findings draw attention to these critical issues, which have the potential to perpetuate harms if left unexamined. Further, we argue that system prompt analysis must be incorporated into AI auditing processes, particularly as customisable system prompts become increasingly prevalent in commercial AI deployments.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering</title>
<link>https://arxiv.org/abs/2505.21755</link>
<guid>https://arxiv.org/abs/2505.21755</guid>
<content:encoded><![CDATA[
arXiv:2505.21755v2 Announce Type: replace-cross 
Abstract: Visual question answering (VQA) systems face significant challenges when adapting to real-world data shifts, especially in multi-modal contexts. While robust fine-tuning strategies are essential for maintaining performance across in-distribution (ID) and out-of-distribution (OOD) scenarios, current evaluation settings are primarily unimodal or particular to some types of OOD, offering limited insight into the complexities of multi-modal contexts. In this work, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across Multi-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We utilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA and others, and categorize them into ID, near and far OOD datasets covering uni-modal, multi-modal and adversarial distribution shifts. We first conduct a comprehensive comparison of existing robust fine-tuning methods. We then quantify the distribution shifts by calculating the Mahalanobis distance using uni-modal and multi-modal embeddings extracted from various models. Further, we perform an extensive analysis to explore the interactions between uni- and multi-modal shifts as well as modality importance for ID and OOD samples. These analyses offer valuable guidance on developing more robust fine-tuning methods to handle multi-modal distribution shifts. The code is available at https://github.com/chengyuehuang511/FRAMES-VQA .
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models</title>
<link>https://arxiv.org/abs/2505.23091</link>
<guid>https://arxiv.org/abs/2505.23091</guid>
<content:encoded><![CDATA[
arXiv:2505.23091v3 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have demonstrated substantial progress in reasoning capabilities, such as DeepSeek-R1, which leverages rule-based reinforcement learning to enhance logical reasoning significantly. However, extending these achievements to multimodal large language models (MLLMs) presents critical challenges, which are frequently more pronounced for Multimodal Small Language Models (MSLMs) given their typically weaker foundational reasoning abilities: (1) the scarcity of high-quality multimodal reasoning datasets, (2) the degradation of reasoning capabilities due to the integration of visual processing, and (3) the risk that direct application of reinforcement learning may produce complex yet incorrect reasoning processes. To address these challenges, we design a novel framework Infi-MMR to systematically unlock the reasoning potential of MSLMs through a curriculum of three carefully structured phases and propose our multimodal reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning Activation, leverages high-quality textual reasoning datasets to activate and strengthen the model's logical reasoning capabilities. The second phase, Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to facilitate the progressive transfer of reasoning skills to multimodal contexts. The third phase, Multimodal Reasoning Enhancement, employs curated, caption-free multimodal data to mitigate linguistic biases and promote robust cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on MathVista testmini). Resources are available at https://huggingface.co/Reallm-Labs/Infi-MMR-3B.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comba: Improving Bilinear RNNs with Closed-loop Control</title>
<link>https://arxiv.org/abs/2506.02475</link>
<guid>https://arxiv.org/abs/2506.02475</guid>
<content:encoded><![CDATA[
arXiv:2506.02475v3 Announce Type: replace-cross 
Abstract: Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and RWKV-7 have achieved performance improvements by supervising the recurrent memory management through Delta learning rule. Unlike previous state-space models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models introduce interactions between the recurrent state and the key vector, structurally resembling bilinear systems. In this paper, we first introduce the concept of Bilinear RNNs with a comprehensive analysis on the advantages and limitations of these models. Then, based on closed-loop control theory, we propose a novel Bilinear RNN variant named Comba, which adopts a scalar-plus-low-rank state transition, with both state feedback and output feedback corrections. We also implement a hardware-efficient chunk-wise parallel kernel in Triton and train models with 340M/1.3B parameters on large-scale corpus. Comba demonstrates superior performance and computation efficiency in both language and vision modeling.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures</title>
<link>https://arxiv.org/abs/2506.06832</link>
<guid>https://arxiv.org/abs/2506.06832</guid>
<content:encoded><![CDATA[
arXiv:2506.06832v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) define probability measures on text. By considering the implicit knowledge question of what it means for an LLM to know such a measure and what it entails algorithmically, we are naturally led to formulate a series of tasks that go beyond generative sampling, involving forms of summarization, counterfactual thinking, anomaly detection, originality search, reverse prompting, debating, creative solving, etc. These tasks can be formulated as games based on LLM measures, which we call Cross-Entropy (Xent) Games. Xent Games can be single-player or multi-player. They involve cross-entropy scores and cross-entropy constraints, and can be expressed as simple computational graphs and programs. We show the Xent Game space is large enough to contain a wealth of interesting examples, while being constructible from basic game-theoretic consistency axioms. We then discuss how the Xent Game space can be used to measure the abilities of LLMs. This leads to the construction of Xent Game measures: finite families of Xent Games that can be used as capability benchmarks, built from a given scope, by extracting a covering measure. To address the unbounded scope problem associated with the challenge of measuring general abilities, we propose to explore the space of Xent Games in a coherent fashion, using ideas inspired by evolutionary dynamics.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning Teachers of Test Time Scaling</title>
<link>https://arxiv.org/abs/2506.08388</link>
<guid>https://arxiv.org/abs/2506.08388</guid>
<content:encoded><![CDATA[
arXiv:2506.08388v2 Announce Type: replace-cross 
Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for one-hot correctness inherently relies on the LM being able to explore and solve its task with some chance at initialization. Furthermore, a key use case of reasoning LMs is to act as teachers for distilling new students and cold-starting future RL iterations rather than being deployed themselves. From these considerations, we introduce a new framework that avoids RL's exploration challenge by training a new class of Reinforcement-Learned Teachers (RLTs) focused on yielding the most effective downstream distillation. RLTs are prompted with both the question and solution to each problem, and tasked to simply "connect-the-dots" with detailed explanations tailored for their students. We train RLTs with dense rewards obtained by feeding each explanation to the student and testing its understanding of the problem's solution. In practice, the raw outputs of a 7B RLT provide higher final performance on competition and graduate-level tasks than existing distillation and cold-starting pipelines that collect and postprocess the reasoning traces of orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness when training larger students and when applied zero-shot to out-of-distribution tasks, unlocking new levels of efficiency and re-usability for the RL reasoning framework.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective Red-Teaming of Policy-Adherent Agents</title>
<link>https://arxiv.org/abs/2506.09600</link>
<guid>https://arxiv.org/abs/2506.09600</guid>
<content:encoded><![CDATA[
arXiv:2506.09600v2 Announce Type: replace-cross 
Abstract: Task-oriented LLM-based agents are increasingly used in domains with strict policies, such as refund eligibility or cancellation rules. The challenge lies in ensuring that the agent consistently adheres to these rules and policies, appropriately refusing any request that would violate them, while still maintaining a helpful and natural interaction. This calls for the development of tailored design and evaluation methodologies to ensure agent resilience against malicious user behavior. We propose a novel threat model that focuses on adversarial users aiming to exploit policy-adherent agents for personal benefit. To address this, we present CRAFT, a multi-agent red-teaming system that leverages policy-aware persuasive strategies to undermine a policy-adherent agent in a customer-service scenario, outperforming conventional jailbreak methods such as DAN prompts, emotional manipulation, and coercive. Building upon the existing tau-bench benchmark, we introduce tau-break, a complementary benchmark designed to rigorously assess the agent's robustness against manipulative user behavior. Finally, we evaluate several straightforward yet effective defense strategies. While these measures provide some protection, they fall short, highlighting the need for stronger, research-driven safeguards to protect policy-adherent agents from adversarial attacks
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LogProber: Disentangling confidence from contamination in LLM responses</title>
<link>https://arxiv.org/abs/2408.14352</link>
<guid>https://arxiv.org/abs/2408.14352</guid>
<content:encoded><![CDATA[
<div> Machine learning, contamination, Large Language Models (LLMs), detection, LogProber  
Summary:  
Contamination in machine learning refers to the challenge of testing data leaking into the training set, particularly problematic in assessing Large Language Models (LLMs) trained on extensive text corpora. Existing methods for detecting contamination in short text sequences have limitations. The LogProber algorithm presented in this paper focuses on familiarity with questions rather than answers, offering efficient contamination detection in a black box setting. The study compares LogProber with other approaches, highlighting its strengths and weaknesses. It demonstrates how different forms of contamination may remain undetected based on detection algorithm design. This research underscores the importance of developing effective tools to accurately track LLM performance evolution and emphasizes the need for robust contamination detection mechanisms in machine learning evaluation.  
<br /><br />Summary: <div>
arXiv:2408.14352v3 Announce Type: replace 
Abstract: In machine learning, contamination refers to situations where testing data leak into the training set. The issue is particularly relevant for the evaluation of the performance of Large Language Models (LLMs), which are generally trained on gargantuan, and generally opaque, corpora of text scraped from the world wide web. Developing tools to detect contamination is therefore crucial to be able to fairly and properly track the evolution of the performance of LLMs. To date, only a few recent studies have attempted to address the issue of quantifying and detecting contamination in short text sequences, such as those commonly found in benchmarks. However, these methods have limitations that can sometimes render them impractical. In the present paper, we introduce LogProber, a novel, efficient algorithm that we show to be able to detect contamination in a black box setting that tries to tackle some of these drawbacks by focusing on the familiarity with the question rather than the answer. Here, we explore the properties of the proposed method in comparison with concurrent approaches, identify its advantages and limitations, and illustrate how different forms of contamination can go undetected depending on the design of the detection algorithm.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements</title>
<link>https://arxiv.org/abs/2506.09707</link>
<guid>https://arxiv.org/abs/2506.09707</guid>
<content:encoded><![CDATA[
<div> fidelity, Prolonged Exposure therapy, automatic localization, audio-language model, LoRA<br />
Summary:<br />
This study introduces a method for automatically detecting key elements of Prolonged Exposure therapy from session audio and transcripts. By fine-tuning a pre-trained audio-language model, the proposed approach can identify the start and stop times of therapist fidelity elements in PE sessions. The model, trained on a dataset of 313 real sessions, achieves a mean absolute error of 5.3 seconds for fidelity element localization. The study explores the impact of window size and model adaptation on the accuracy of fidelity tracking, highlighting the importance of context granularity and model fine-tuning. This scalable framework has the potential to support clinician training, supervision, and quality assurance in PE therapy. <div>
arXiv:2506.09707v2 Announce Type: replace-cross 
Abstract: Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic stress disorder (PTSD), but evaluating therapist fidelity remains labor-intensive due to the need for manual review of session recordings. We present a method for the automatic temporal localization of key PE fidelity elements -- identifying their start and stop times -- directly from session audio and transcripts. Our approach fine-tunes a large pre-trained audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process focused 30-second windows of audio-transcript input. Fidelity labels for three core protocol phases -- therapist orientation (P1), imaginal exposure (P2), and post-imaginal processing (P3) -- are generated via LLM-based prompting and verified by trained raters. The model is trained to predict normalized boundary offsets using soft supervision guided by task-specific prompts. On a dataset of 313 real PE sessions, our best configuration (LoRA rank 8, 30s windows) achieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further analyze the effects of window size and LoRA rank, highlighting the importance of context granularity and model adaptation. This work introduces a scalable framework for fidelity tracking in PE therapy, with potential to support clinician training, supervision, and quality assurance.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Veracity: An Open-Source AI Fact-Checking System</title>
<link>https://arxiv.org/abs/2506.15794</link>
<guid>https://arxiv.org/abs/2506.15794</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, generative AI, fact-checking, Veracity, media literacy

Summary:
Veracity is an open-source AI system developed to combat the proliferation of misinformation using Large Language Models and web retrieval agents. It allows individuals to submit claims for analysis and receive veracity assessments with clear explanations. The system offers multilingual support, numerical scoring of claim veracity, and a user-friendly interface resembling messaging apps. Veracity not only detects misinformation but also provides reasoning behind its assessments, promoting media literacy and fostering a more informed society.<br /><br />Summary: Veracity, a transparent and accessible fact-checking AI system, empowers individuals to combat misinformation by leveraging Large Language Models and web retrieval agents. With features like multilingual support and numerical veracity scores, it not only detects misinformation but also provides clear explanations for its assessments. The intuitive interface inspired by messaging applications makes it easy for users to interact with the system, promoting media literacy and contributing to a more informed society. <div>
arXiv:2506.15794v1 Announce Type: new 
Abstract: The proliferation of misinformation poses a significant threat to society, exacerbated by the capabilities of generative AI. This demo paper introduces Veracity, an open-source AI system designed to empower individuals to combat misinformation through transparent and accessible fact-checking. Veracity leverages the synergy between Large Language Models (LLMs) and web retrieval agents to analyze user-submitted claims and provide grounded veracity assessments with intuitive explanations. Key features include multilingual support, numerical scoring of claim veracity, and an interactive interface inspired by familiar messaging applications. This paper will showcase Veracity's ability to not only detect misinformation but also explain its reasoning, fostering media literacy and promoting a more informed society.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking LLM Training through Information Geometry and Quantum Metrics</title>
<link>https://arxiv.org/abs/2506.15830</link>
<guid>https://arxiv.org/abs/2506.15830</guid>
<content:encoded><![CDATA[
<div> Fisher information metric, natural gradient descent, optimization, information geometry, language models  
Summary:  
Optimization in large language models (LLMs) is complex due to high-dimensional parameter spaces with non-Euclidean structure. The Fisher information metric provides insights into phenomena like sharp minima, generalization, and scaling laws in LLM training. Approaches that consider curvature offer a deeper understanding of the optimization process. The potential for quantum analogies, leveraging the Fubini-Study metric and Quantum Fisher Information, suggests the possibility of efficient optimization in quantum-enhanced systems. <br /><br />Summary: <div>
arXiv:2506.15830v1 Announce Type: new 
Abstract: Optimization in large language models (LLMs) unfolds over high-dimensional parameter spaces with non-Euclidean structure. Information geometry frames this landscape using the Fisher information metric, enabling more principled learning via natural gradient descent. Though often impractical, this geometric lens clarifies phenomena such as sharp minima, generalization, and observed scaling laws. We argue that curvature-aware approaches deepen our understanding of LLM training. Finally, we speculate on quantum analogies based on the Fubini-Study metric and Quantum Fisher Information, hinting at efficient optimization in quantum-enhanced systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents</title>
<link>https://arxiv.org/abs/2506.15841</link>
<guid>https://arxiv.org/abs/2506.15841</guid>
<content:encoded><![CDATA[
<div> reinforcement, learning, memory consolidation, reasoning, performance 
Summary: 
MEM1 is a reinforcement learning framework that enables language agents to operate with constant memory across long multi-turn tasks. It updates a compact shared internal state at each turn, integrating prior memory with new observations while discarding irrelevant information. The framework outperforms existing systems on a multi-turn QA task and reduces memory usage significantly. The approach can be applied to various domains, including internal retrieval QA, open-domain web QA, and multi-turn web shopping. MEM1-7B shows a 3.5x improvement in performance and a 3.7x reduction in memory usage compared to Qwen2.5-14B-Instruct. It demonstrates the scalability and efficiency of reasoning-driven memory consolidation in training long-horizon interactive agents. 
Summary: <div>
arXiv:2506.15841v1 Announce Type: new 
Abstract: Modern language agents must operate over long-horizon, multi-turn interactions, where they retrieve external information, adapt to observations, and answer interdependent queries. Yet, most LLM systems rely on full-context prompting, appending all past turns regardless of their relevance. This leads to unbounded memory growth, increased computational costs, and degraded reasoning performance on out-of-distribution input lengths. We introduce MEM1, an end-to-end reinforcement learning framework that enables agents to operate with constant memory across long multi-turn tasks. At each turn, MEM1 updates a compact shared internal state that jointly supports memory consolidation and reasoning. This state integrates prior memory with new observations from the environment while strategically discarding irrelevant or redundant information. To support training in more realistic and compositional settings, we propose a simple yet effective and scalable approach to constructing multi-turn environments by composing existing datasets into arbitrarily complex task sequences. Experiments across three domains, including internal retrieval QA, open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves performance by 3.5x while reducing memory usage by 3.7x compared to Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes beyond the training horizon. Our results demonstrate the promise of reasoning-driven memory consolidation as a scalable alternative to existing solutions for training long-horizon interactive agents, where both efficiency and performance are optimized.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finance Language Model Evaluation (FLaME)</title>
<link>https://arxiv.org/abs/2506.15846</link>
<guid>https://arxiv.org/abs/2506.15846</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Models, Natural Language Processing, Finance, Benchmarking, Evaluation

Summary:
This paper introduces a new benchmarking suite, Financial Language Model Evaluation (FLaME), to assess the performance of Language Models (LMs) in finance-related tasks. Existing evaluation frameworks have limitations that underestimate the capabilities of LMs in financial NLP tasks. The authors conducted an empirical study comparing 23 foundation LMs and 'reasoning-reinforced' LMs across 20 core NLP tasks in finance. They found that LMs have significant potential for specialized finance tasks, contrary to the belief in their limited performance. The framework software, data, and results are openly shared for further research and development in this area.

<br /><br />Summary:This paper presents a new benchmarking suite, FLaME, to evaluate the performance of Language Models (LMs) in finance tasks. Existing evaluation frameworks underestimate LMs' potential in finance NLP, leading to misconceptions about their capabilities. The authors conducted a comprehensive study comparing various LMs across multiple finance NLP tasks and found significant potential for LMs in specialized finance tasks. The study demonstrates that LMs can perform well in complex finance-related tasks, contrary to previous beliefs. The framework software, data, and results are made publicly available for further research and development in this field. <div>
arXiv:2506.15846v1 Announce Type: new 
Abstract: Language Models (LMs) have demonstrated impressive capabilities with core Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly specialized knowledge-intensive tasks in finance remains difficult to assess due to major gaps in the methodologies of existing evaluation frameworks, which have caused an erroneous belief in a far lower bound of LMs' performance on common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for these FinNLP tasks, we present the first holistic benchmarking suite for Financial Language Model Evaluation (FLaME). We are the first research paper to comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source our framework software along with all data and results.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-Driven Pre-Tokenization for Byte-Pair Encoding</title>
<link>https://arxiv.org/abs/2506.15889</link>
<guid>https://arxiv.org/abs/2506.15889</guid>
<content:encoded><![CDATA[
<div> entropy, BPE, tokenization, language models, segmentation<br />
<br />
Summary: <br />
The article introduces two entropy-informed pre-tokenization strategies to improve the segmentation of unsegmented languages like Chinese using Byte-Pair Encoding (BPE). The first approach utilizes pointwise mutual information and left/right entropy to identify coherent character spans, while the second method leverages predictive entropy from a pretrained GPT-2 model to detect boundary uncertainty. By incorporating these strategies, the study demonstrates significant enhancements in segmentation precision, recall, and F1 score compared to standard BPE. The results indicate that entropy-guided pre-tokenization not only improves alignment with linguistic boundaries but also shows promise in enhancing tokenization quality for low-resource and multilingual scenarios. This research offers valuable insights for optimizing tokenization processes in language models, particularly in settings where linguistic boundaries are ambiguous or challenging to identify. <br /> <div>
arXiv:2506.15889v1 Announce Type: new 
Abstract: Byte-Pair Encoding (BPE) has become a widely adopted subword tokenization method in modern language models due to its simplicity and strong empirical performance across downstream tasks. However, applying BPE to unsegmented languages such as Chinese presents significant challenges, as its frequency-driven merge operation is agnostic to linguistic boundaries. To address this, we propose two entropy-informed pre-tokenization strategies that guide BPE segmentation using unsupervised information-theoretic cues. The first approach uses pointwise mutual information and left/right entropy to identify coherent character spans, while the second leverages predictive entropy derived from a pretrained GPT-2 model to detect boundary uncertainty. We evaluate both methods on a subset of the PKU dataset and demonstrate substantial improvements in segmentation precision, recall, and F1 score compared to standard BPE. Our results suggest that entropy-guided pre-tokenization not only enhances alignment with gold-standard linguistic units but also offers a promising direction for improving tokenization quality in low-resource and multilingual settings.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning</title>
<link>https://arxiv.org/abs/2506.15894</link>
<guid>https://arxiv.org/abs/2506.15894</guid>
<content:encoded><![CDATA[
<div> Language Models, Self-Correction, Reasoning, Chain of Thought, Robustness <br />
Summary:<br />
The study investigates the self-correction capabilities of Large Language Models (LLMs) in mathematical reasoning tasks. It finds that LLMs exhibit robust intrinsic self-correction behavior in response to perturbations in their Chain of Thought (CoT) reasoning. This ability is observed across various models and datasets, suggesting that even models not specifically trained for long CoT tasks possess strong self-correction capabilities. The experiments show that LLMs can make subtle implicit corrections as well as explicit acknowledgments and corrections of errors. The presence of this intrinsic self-correction ability implies that recent "reasoning" models may amplify traits already present in the models. Overall, the study highlights the potential for LLMs to self-correct and adapt to errors in reasoning tasks, indicating a promising direction for future research in improving the robustness and reliability of such models. <br /> <div>
arXiv:2506.15894v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive mathematical reasoning capabilities, yet their performance remains brittle to minor variations in problem description and prompting strategy. Furthermore, reasoning is vulnerable to sampling-induced errors which autoregressive models must primarily address using self-correction via additionally-generated tokens. To better understand self-correction capabilities of recent models, we conduct experiments measuring models' ability to self-correct synthetic perturbations introduced into their Chain of Thought (CoT) reasoning. We observe robust single-utterance intrinsic self-correction behavior across a range of open-weight models and datasets, ranging from subtle, implicit corrections to explicit acknowledgments and corrections of errors. Our findings suggest that LLMs, including those not finetuned for long CoT, may possess stronger intrinsic self-correction capabilities than commonly shown in the literature. The presence of this ability suggests that recent "reasoning" model work involves amplification of traits already meaningfully present in models.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents</title>
<link>https://arxiv.org/abs/2506.15911</link>
<guid>https://arxiv.org/abs/2506.15911</guid>
<content:encoded><![CDATA[
<div> Islamic medicine, Avicenna, Tibb-e-Nabawi, language models, medical question-answering <br />
<br />Summary: 
The study introduces Tibbe-AG, a pipeline evaluating Islamic medical texts in modern AI systems. 30 Prophetic-medicine questions are aligned with human-verified remedies and assessed using three language models. Results show that retrieval and self-evaluation enhance factual accuracy by 13% and 10% respectively, improving the reliability of culturally sensitive medical question-answering. Through blending classical Islamic texts with modern AI techniques, the study bridges the gap in validating preventive care, nutrition, and holistic therapies encoded in centuries-old Islamic medical texts like Avicenna's Canon of Medicine and Tibb-e-Nabawi. <div>
arXiv:2506.15911v1 Announce Type: new 
Abstract: Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and holistic therapies, yet remain inaccessible to many and underutilized in modern AI systems. Existing language-model benchmarks focus narrowly on factual recall or user preference, leaving a gap in validating culturally grounded medical guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that aligns 30 carefully curated Prophetic-medicine questions with human-verified remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three configurations: direct generation, retrieval-augmented generation, and a scientific self-critique filter. Each answer is then assessed by a secondary LLM serving as an agentic judge, yielding a single 3C3H quality score. Retrieval improves factual accuracy by 13%, while the agentic prompt adds another 10% improvement through deeper mechanistic insight and safety considerations. Our results demonstrate that blending classical Islamic texts with retrieval and self-evaluation enables reliable, culturally sensitive medical question-answering.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reranking-based Generation for Unbiased Perspective Summarization</title>
<link>https://arxiv.org/abs/2506.15925</link>
<guid>https://arxiv.org/abs/2506.15925</guid>
<content:encoded><![CDATA[
<div> reliable metrics, perspective summarization, Large Language Models, benchmarking, summarizers <br />
<br />
Summary: <br />Generating unbiased political perspective summaries is crucial, but current evaluation methods lack reliability. This study identifies reliable metrics for measuring perspective summary quality, showing that language model-based metrics outperform traditional ones. Reranking-based methods are effective, and preference tuning with synthetic data enhances performance. The study contributes to improving evaluation and development of perspective summarization methods. <div>
arXiv:2506.15925v1 Announce Type: new 
Abstract: Generating unbiased summaries in real-world settings such as political perspective summarization remains a crucial application of Large Language Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics for measuring key attributes such as coverage and faithfulness without verifying their applicability, and efforts to develop improved summarizers are still nascent. We address these gaps by (1) identifying reliable metrics for measuring perspective summary quality, and (2) investigating the efficacy of LLM-based methods beyond zero-shot inference. Namely, we build a test set for benchmarking metric reliability using human annotations and show that traditional metrics underperform compared to language model-based metrics, which prove to be strong evaluators. Using these metrics, we show that reranking-based methods yield strong results, and preference tuning with synthetically generated and reranking-labeled data further boosts performance. Our findings aim to contribute to the reliable evaluation and development of perspective summarization methods.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension</title>
<link>https://arxiv.org/abs/2506.15978</link>
<guid>https://arxiv.org/abs/2506.15978</guid>
<content:encoded><![CDATA[
<div> Dataset, Vietnamese, natural language processing, text segmentation, machine reading comprehension

Summary:
The article introduces VSMRC, a dataset for Vietnamese text segmentation and multiple-choice reading comprehension tasks. With over 15,000 documents sourced from Vietnamese Wikipedia, the dataset also includes synthetic question-answer pairs. Experiments show that multilingual models like mBERT outperform monolingual models in both tasks, achieving high accuracy and F1 scores. The analysis highlights the potential of multilingual models in NLP tasks for under-resourced languages, indicating broader applications in the field. VSMRC offers a reliable and diverse resource for advancing research in Vietnamese NLP.  

<br /><br />Summary: <div>
arXiv:2506.15978v1 Announce Type: new 
Abstract: Vietnamese, the 20th most spoken language with over 102 million native speakers, lacks robust resources for key natural language processing tasks such as text segmentation and machine reading comprehension (MRC). To address this gap, we present VSMRC, the Vietnamese Text Segmentation and Multiple-Choice Reading Comprehension Dataset. Sourced from Vietnamese Wikipedia, our dataset includes 15,942 documents for text segmentation and 16,347 synthetic multiple-choice question-answer pairs generated with human quality assurance, ensuring a reliable and diverse resource. Experiments show that mBERT consistently outperforms monolingual models on both tasks, achieving an accuracy of 88.01% on MRC test set and an F1 score of 63.15\% on text segmentation test set. Our analysis reveals that multilingual models excel in NLP tasks for Vietnamese, suggesting potential applications to other under-resourced languages. VSMRC is available at HuggingFace
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion</title>
<link>https://arxiv.org/abs/2506.15981</link>
<guid>https://arxiv.org/abs/2506.15981</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-based music generation, copyright detection, multimodal late-fusion pipeline, audio perturbations, DE-detect method

Summary: 
The article discusses the challenges faced by artists, copyright holders, and providers due to the rapid development of AI-based music generation tools. Detecting AI-generated content is crucial, but current detection methods have limitations. To address these, a novel approach called DE-detect is proposed, utilizing a multimodal late-fusion pipeline that combines automatically transcribed sung lyrics and speech features from the audio. This method enhances robustness, mitigates susceptibility to low-level artifacts, and improves practical applicability. DE-detect outperforms existing lyrics-based detectors in detecting AI-generated music and is more resilient to audio perturbations. The code for DE-detect is available on GitHub, providing an effective and robust solution for detecting AI-generated music in various real-world scenarios.
<br /><br />Summary: <div>
arXiv:2506.15981v1 Announce Type: new 
Abstract: The rapid advancement of AI-based music generation tools is revolutionizing the music industry but also posing challenges to artists, copyright holders, and providers alike. This necessitates reliable methods for detecting such AI-generated content. However, existing detectors, relying on either audio or lyrics, face key practical limitations: audio-based detectors fail to generalize to new or unseen generators and are vulnerable to audio perturbations; lyrics-based methods require cleanly formatted and accurate lyrics, unavailable in practice. To overcome these limitations, we propose a novel, practically grounded approach: a multimodal, modular late-fusion pipeline that combines automatically transcribed sung lyrics and speech features capturing lyrics-related information within the audio. By relying on lyrical aspects directly from audio, our method enhances robustness, mitigates susceptibility to low-level artifacts, and enables practical applicability. Experiments show that our method, DE-detect, outperforms existing lyrics-based detectors while also being more robust to audio perturbations. Thus, it offers an effective, robust solution for detecting AI-generated music in real-world scenarios. Our code is available at https://github.com/deezer/robust-AI-lyrics-detection.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation</title>
<link>https://arxiv.org/abs/2506.16024</link>
<guid>https://arxiv.org/abs/2506.16024</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, Large Language Models, long-context generation, Open-LTG, ProxyReward<br />
<br />
Summary:<br />
The research introduces ProxyReward, a novel reinforcement learning framework for training Large Language Models on Open-ended Long Text Generation tasks. By generating a ProxyReward Dataset with simple prompts, the model can create high-quality long-form content without extensive manual labeling. The ProxyReward Signal evaluates information accuracy and comprehensiveness for specific questions, leading to improved performance compared to existing methods. Experimental results show that ProxyReward outperforms GPT-4-Turbo by 20% on Open-LTG tasks and surpasses the LLM-as-a-Judge approach. This approach enhances the ability of LLMs to address complex open-ended questions posed by humans. <div>
arXiv:2506.16024v1 Announce Type: new 
Abstract: Current research on long-form context in Large Language Models (LLMs) primarily focuses on the understanding of long-contexts, the Open-ended Long Text Generation (Open-LTG) remains insufficiently explored. Training a long-context generation model requires curation of gold standard reference data, which is typically nonexistent for informative Open-LTG tasks. However, previous methods only utilize general assessments as reward signals, which limits accuracy. To bridge this gap, we introduce ProxyReward, an innovative reinforcement learning (RL) based framework, which includes a dataset and a reward signal computation method. Firstly, ProxyReward Dataset generation is accomplished through simple prompts that enables the model to create automatically, obviating extensive labeled data or significant manual effort. Secondly, ProxyReward Signal offers a targeted evaluation of information comprehensiveness and accuracy for specific questions. The experimental results indicate that our method ProxyReward surpasses even GPT-4-Turbo. It can significantly enhance performance by 20% on the Open-LTG task when training widely used open-source models, while also surpassing the LLM-as-a-Judge approach. Our work presents effective methods to enhance the ability of LLMs to address complex open-ended questions posed by human.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoLM: In Search of Lost Language Model Training Dynamics</title>
<link>https://arxiv.org/abs/2506.16029</link>
<guid>https://arxiv.org/abs/2506.16029</guid>
<content:encoded><![CDATA[
<div> Keywords: language model, training dynamics, pre-training, fine-tuning, reinforcement learning 

Summary:<br /><br />
The paper introduces EvoLM, a model suite designed for analyzing different stages of language model training, including pre-training, continued pre-training, fine-tuning, and reinforcement learning. By training over 100 LMs with varying parameters, the study evaluates both language modeling and problem-solving abilities, highlighting insights such as the diminishing returns of excessive pre-training and post-training, the need to address forgetting during domain-specific continued pre-training, the importance of continued pre-training in connecting different training phases, and the trade-offs involved in configuring fine-tuning and reinforcement learning. The research emphasizes the significance of systematic analysis to understand the impact of design choices at each training stage. All pre-trained and post-trained models, along with datasets and training pipelines, are released to promote transparency and reproducibility in research. <div>
arXiv:2506.16029v1 Announce Type: new 
Abstract: Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. By training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3</title>
<link>https://arxiv.org/abs/2506.16037</link>
<guid>https://arxiv.org/abs/2506.16037</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, complex question answering, multi-hop reasoning, contextual understanding, LLaMA 3 <br />
Summary:<br />
This paper introduces a novel Retrieval-Augmented Generation (RAG) framework designed for complex question answering tasks that involve multi-hop reasoning and contextual understanding in lengthy documents. The framework, based on LLaMA 3, combines a dense retrieval module with advanced context fusion and multi-hop reasoning mechanisms to enhance response generation accuracy and coherence. By jointly optimizing retrieval likelihood and generation cross-entropy, the model becomes more robust and adaptable. Experimental results demonstrate that the proposed system surpasses existing retrieval-augmented and generative baselines, highlighting its effectiveness in delivering precise and contextually grounded answers. <div>
arXiv:2506.16037v1 Announce Type: new 
Abstract: This paper presents a novel Retrieval-Augmented Generation (RAG) framework tailored for complex question answering tasks, addressing challenges in multi-hop reasoning and contextual understanding across lengthy documents. Built upon LLaMA 3, the framework integrates a dense retrieval module with advanced context fusion and multi-hop reasoning mechanisms, enabling more accurate and coherent response generation. A joint optimization strategy combining retrieval likelihood and generation cross-entropy improves the model's robustness and adaptability. Experimental results show that the proposed system outperforms existing retrieval-augmented and generative baselines, confirming its effectiveness in delivering precise, contextually grounded answers.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling</title>
<link>https://arxiv.org/abs/2506.16043</link>
<guid>https://arxiv.org/abs/2506.16043</guid>
<content:encoded><![CDATA[
<div> scaling, language model, inference, computational efficiency, DynScaling 

Summary:
DynScaling introduces an innovative approach to address the challenges of inference-time scaling in large language models (LLMs). It combines an integrated parallel-sequential sampling strategy with a bandit-based dynamic budget allocation framework to enhance LLM performance under realistic computational constraints. The integrated sampling strategy promotes diverse and coherent reasoning trajectories by synthesizing sequential reasoning chains from initially independent parallel responses. The dynamic budget allocation framework optimizes computational efficiency by adapting the allocation of resources based on the uncertainty of previous responses, treating it as a multi-armed bandit problem. This approach eliminates the need for external verifiers and consistently outperforms existing verifier-free inference scaling baselines in both task performance and computational cost. DynScaling offers a promising solution to enhance LLM performance in practical settings where resource constraints are a key consideration. 

<br /><br />Summary: <div>
arXiv:2506.16043v1 Announce Type: new 
Abstract: Inference-time scaling has proven effective in boosting large language model (LLM) performance through increased test-time computation. Yet, its practical application is often hindered by reliance on external verifiers or a lack of optimization for realistic computational constraints. We propose DynScaling, which addresses these limitations through two primary innovations: an integrated parallel-sequential sampling strategy and a bandit-based dynamic budget allocation framework. The integrated sampling strategy unifies parallel and sequential sampling by constructing synthetic sequential reasoning chains from initially independent parallel responses, promoting diverse and coherent reasoning trajectories. The dynamic budget allocation framework formulates the allocation of computational resources as a multi-armed bandit problem, adaptively distributing the inference budget across queries based on the uncertainty of previously sampled responses, thereby maximizing computational efficiency. By combining these components, DynScaling effectively improves LLM performance under practical resource constraints without the need for external verifiers. Experimental results demonstrate that DynScaling consistently surpasses existing verifier-free inference scaling baselines in both task performance and computational cost.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text</title>
<link>https://arxiv.org/abs/2506.16052</link>
<guid>https://arxiv.org/abs/2506.16052</guid>
<content:encoded><![CDATA[
<div> Transformer-based models, DeBERTa, cyberbullying detection, sentiment analysis, Gated Broad Learning System (GBLS)<br />
Summary:<br />
The paper introduces a hybrid architecture for cyberbullying detection, leveraging Transformer-based models like DeBERTa and sentiment analysis along with a Gated Broad Learning System (GBLS) classifier. The proposed ModifiedDeBERTa + GBLS model demonstrates superior performance on multiple English datasets, achieving high accuracy rates. The framework also ensures transparency through various explainability mechanisms such as token-level attribution analysis and LIME-based local interpretations. Ablation studies highlight the significance of each architectural component, while failure case analysis sheds light on challenges in detecting implicit bias and sarcastic content. Overall, the hybrid approach shows promising results in enhancing cyberbullying detection systems, offering insights for future improvements. <br /> <div>
arXiv:2506.16052v1 Announce Type: new 
Abstract: The proliferation of online communication platforms has created unprecedented opportunities for global connectivity while simultaneously enabling harmful behaviors such as cyberbullying, which affects approximately 54.4\% of teenagers according to recent research. This paper presents a hybrid architecture that combines the contextual understanding capabilities of transformer-based models with the pattern recognition strengths of broad learning systems for effective cyberbullying detection. This approach integrates a modified DeBERTa model augmented with Squeeze-and-Excitation blocks and sentiment analysis capabilities with a Gated Broad Learning System (GBLS) classifier, creating a synergistic framework that outperforms existing approaches across multiple benchmark datasets. The proposed ModifiedDeBERTa + GBLS model achieved good performance on four English datasets: 79.3\% accuracy on HateXplain, 95.41\% accuracy on SOSNet, 91.37\% accuracy on Mendeley-I, and 94.67\% accuracy on Mendeley-II. Beyond performance gains, the framework incorporates comprehensive explainability mechanisms including token-level attribution analysis, LIME-based local interpretations, and confidence calibration, addressing critical transparency requirements in automated content moderation. Ablation studies confirm the meaningful contribution of each architectural component, while failure case analysis reveals specific challenges in detecting implicit bias and sarcastic content, providing valuable insights for future improvements in cyberbullying detection systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knee-Deep in C-RASP: A Transformer Depth Hierarchy</title>
<link>https://arxiv.org/abs/2506.16055</link>
<guid>https://arxiv.org/abs/2506.16055</guid>
<content:encoded><![CDATA[
<div> depth, transformers, C-RASP, expressiveness, temporal logic<br />
<br />
Summary: 
The study investigates the relationship between the depth of transformers and their capabilities. It is found that transformers with greater depth have increased capabilities. The research begins by analyzing transformers that round to fixed precision, showing their equivalence to the programming language C-RASP. Deeper C-RASP programs are proven to be more expressive than shallower ones, suggesting that deeper transformers are also more expressive. This conclusion is supported by a study of temporal logic with counting operators, which was previously shown to be equivalent to C-RASP. Empirical evidence is provided to demonstrate how the theory accurately predicts the depth needed for transformers to generalize on sequential dependency tasks without positional encodings. The research combines theoretical proofs with empirical findings to highlight the advantages of deeper transformers in terms of expressiveness and capabilities. <div>
arXiv:2506.16055v1 Announce Type: new 
Abstract: It has been observed that transformers with greater depth (that is, more layers) have more capabilities, but can we establish formally which capabilities are gained with greater depth? We answer this question with a theoretical proof followed by an empirical study. First, we consider transformers that round to fixed precision except inside attention. We show that this subclass of transformers is expressively equivalent to the programming language C-RASP and this equivalence preserves depth. Second, we prove that deeper C-RASP programs are more expressive than shallower C-RASP programs, implying that deeper transformers are more expressive than shallower transformers (within the subclass mentioned above). These results are established by studying a form of temporal logic with counting operators, which was shown equivalent to C-RASP in previous work. Finally, we provide empirical evidence that our theory predicts the depth required for transformers without positional encodings to length-generalize on a family of sequential dependency tasks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning</title>
<link>https://arxiv.org/abs/2506.16064</link>
<guid>https://arxiv.org/abs/2506.16064</guid>
<content:encoded><![CDATA[
<div> benchmark evaluation, large language models, self-critique, curiosity refinement prompting, HONESET dataset

Summary:
The paper addresses the challenge of producing consistently honest and helpful outputs from large language models (LLMs). It conducts a benchmark evaluation of ten LLMs and proposes a self-critique-guided curiosity refinement prompting strategy. This strategy allows models to self-critique and refine their responses without additional training by incorporating self-critique and refinement steps. The experiment results on the HONESET dataset using the $\mathrm{H}^2$ framework show improvements in reducing poor-quality responses and increasing high-quality responses across all models. The structured self-refinement approach leads to relative gains in $\mathrm{H}^2$ scores ranging from 1.4% to 4.3% compared to curiosity-driven prompting. This study demonstrates the effectiveness of structured self-refinement as a scalable and training-free method to enhance the trustworthiness of LLM outputs.<br /><br />Summary: <div>
arXiv:2506.16064v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated robust capabilities across various natural language tasks. However, producing outputs that are consistently honest and helpful remains an open challenge. To overcome this challenge, this paper tackles the problem through two complementary directions. It conducts a comprehensive benchmark evaluation of ten widely used large language models, including both proprietary and open-weight models from OpenAI, Meta, and Google. In parallel, it proposes a novel prompting strategy, self-critique-guided curiosity refinement prompting. The key idea behind this strategy is enabling models to self-critique and refine their responses without additional training. The proposed method extends the curiosity-driven prompting strategy by incorporating two lightweight in-context steps including self-critique step and refinement step.
  The experiment results on the HONESET dataset evaluated using the framework $\mathrm{H}^2$ (honesty and helpfulness), which was executed with GPT-4o as a judge of honesty and helpfulness, show consistent improvements across all models. The approach reduces the number of poor-quality responses, increases high-quality responses, and achieves relative gains in $\mathrm{H}^2$ scores ranging from 1.4% to 4.3% compared to curiosity-driven prompting across evaluated models. These results highlight the effectiveness of structured self-refinement as a scalable and training-free strategy to improve the trustworthiness of LLMs outputs.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cyberbullying Detection in Hinglish Text Using MURIL and Explainable AI</title>
<link>https://arxiv.org/abs/2506.16066</link>
<guid>https://arxiv.org/abs/2506.16066</guid>
<content:encoded><![CDATA[
<div> Keywords: cyberbullying detection, Hinglish text, Multilingual Representations, MURIL architecture, explainability features

Summary:
The paper introduces a framework for cyberbullying detection in Hinglish text using the MURIL architecture. The approach outperforms existing models on six benchmark datasets, with accuracies ranging from 75.41% to 94.63%. The framework includes explainability features through attribution analysis and cross-linguistic pattern recognition. Ablation studies highlight the importance of selective layer freezing, appropriate classification head design, and specialized preprocessing for code-mixed content in improving detection performance. Failure analysis identifies challenges such as context-dependent interpretation, cultural understanding, and cross-linguistic sarcasm detection. The study provides valuable insights for future research in multilingual cyberbullying detection. 

<br /><br />Summary: The paper presents an advanced framework for detecting cyberbullying in Hinglish text using the MURIL architecture. It surpasses existing models on various datasets, showcasing accuracies between 75.41% to 94.63%. The framework incorporates explainability features, ablation studies reveal crucial factors for enhanced performance, and failure analysis identifies key challenges, offering valuable directions for future research in multilingual cyberbullying detection. <div>
arXiv:2506.16066v1 Announce Type: new 
Abstract: The growth of digital communication platforms has led to increased cyberbullying incidents worldwide, creating a need for automated detection systems to protect users. The rise of code-mixed Hindi-English (Hinglish) communication on digital platforms poses challenges for existing cyberbullying detection systems, which were designed primarily for monolingual text. This paper presents a framework for cyberbullying detection in Hinglish text using the Multilingual Representations for Indian Languages (MURIL) architecture to address limitations in current approaches. Evaluation across six benchmark datasets -- Bohra \textit{et al.}, BullyExplain, BullySentemo, Kumar \textit{et al.}, HASOC 2021, and Mendeley Indo-HateSpeech -- shows that the MURIL-based approach outperforms existing multilingual models including RoBERTa and IndicBERT, with improvements of 1.36 to 13.07 percentage points and accuracies of 86.97\% on Bohra, 84.62\% on BullyExplain, 86.03\% on BullySentemo, 75.41\% on Kumar datasets, 83.92\% on HASOC 2021, and 94.63\% on Mendeley dataset. The framework includes explainability features through attribution analysis and cross-linguistic pattern recognition. Ablation studies show that selective layer freezing, appropriate classification head design, and specialized preprocessing for code-mixed content improve detection performance, while failure analysis identifies challenges including context-dependent interpretation, cultural understanding, and cross-linguistic sarcasm detection, providing directions for future research in multilingual cyberbullying detection.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning</title>
<link>https://arxiv.org/abs/2506.16123</link>
<guid>https://arxiv.org/abs/2506.16123</guid>
<content:encoded><![CDATA[
<div> Keywords: FinCoT, structured CoT prompting, financial reasoning, domain-specific, prompt engineering

Summary:
FinCoT is a structured approach in financial natural language processing (FinNLP) that leverages domain-specific expert financial reasoning to guide large language models. The study explores three main prompting styles in FinNLP, including standard prompting, unstructured CoT prompting, and structured CoT prompting. While prior work focused on standard and unstructured CoT prompting, structured CoT prompting with explicit reasoning steps has been neglected. The research evaluates these prompting approaches and introduces FinCoT, showing significant performance improvements and reduced inference costs. FinCoT enhances accuracy on financial domain questions, yielding more interpretable reasoning traces aligned with experts. The findings demonstrate the effectiveness of domain-aligned structured prompts in improving model performance and reducing token generation in financial reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2506.16123v1 Announce Type: new 
Abstract: This paper presents FinCoT, a structured chain-of-thought (CoT) prompting approach that incorporates insights from domain-specific expert financial reasoning to guide the reasoning traces of large language models. We investigate that there are three main prompting styles in FinNLP: (1) standard prompting--zero-shot prompting; (2) unstructured CoT--CoT prompting without an explicit reasoning structure, such as the use of tags; and (3) structured CoT prompting--CoT prompting with explicit instructions or examples that define structured reasoning steps. Previously, FinNLP has primarily focused on prompt engineering with either standard or unstructured CoT prompting. However, structured CoT prompting has received limited attention in prior work. Furthermore, the design of reasoning structures in structured CoT prompting is often based on heuristics from non-domain experts. In this study, we investigate each prompting approach in FinNLP. We evaluate the three main prompting styles and FinCoT on CFA-style questions spanning ten financial domains. We observe that FinCoT improves performance from 63.2% to 80.5% and Qwen-2.5-7B-Instruct from 69.7% to 74.2%, while reducing generated tokens eight-fold compared to structured CoT prompting. Our findings show that domain-aligned structured prompts not only improve performance and reduce inference costs but also yield more interpretable and expert-aligned reasoning traces.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Under the Shadow of Babel: How Language Shapes Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2506.16151</link>
<guid>https://arxiv.org/abs/2506.16151</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, bilingual dataset, causal reasoning, attention patterns, cognitive linguistic theory

Summary: 
Large language models (LLMs) trained on human language may internalize the logical structures embedded in different languages. The BICAUSE dataset, a structured bilingual dataset for causal reasoning, was introduced to examine this hypothesis. Three key findings emerged from the study: LLMs exhibit typologically aligned attention patterns, with a focus on causes and connectives in Chinese; models internalize language-specific preferences for causal word order, leading to degraded performance in atypical inputs; and when causal reasoning succeeds, model representations converge towards semantically aligned abstractions across languages. These results suggest that LLMs not only mimic surface linguistic forms but also internalize reasoning biases shaped by language, as verified through structural analysis of model internals. <br /><br />Summary: <div>
arXiv:2506.16151v1 Announce Type: new 
Abstract: Language is not only a tool for communication but also a medium for human cognition and reasoning. If, as linguistic relativity suggests, the structure of language shapes cognitive patterns, then large language models (LLMs) trained on human language may also internalize the habitual logical structures embedded in different languages. To examine this hypothesis, we introduce BICAUSE, a structured bilingual dataset for causal reasoning, which includes semantically aligned Chinese and English samples in both forward and reversed causal forms. Our study reveals three key findings: (1) LLMs exhibit typologically aligned attention patterns, focusing more on causes and sentence-initial connectives in Chinese, while showing a more balanced distribution in English. (2) Models internalize language-specific preferences for causal word order and often rigidly apply them to atypical inputs, leading to degraded performance, especially in Chinese. (3) When causal reasoning succeeds, model representations converge toward semantically aligned abstractions across languages, indicating a shared understanding beyond surface form. Overall, these results suggest that LLMs not only mimic surface linguistic forms but also internalize the reasoning biases shaped by language. Rooted in cognitive linguistic theory, this phenomenon is for the first time empirically verified through structural analysis of model internals.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGIC: A Self-Guided Iterative Calibration Framework for RAG</title>
<link>https://arxiv.org/abs/2506.16172</link>
<guid>https://arxiv.org/abs/2506.16172</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval-augmented generation, large language models, calibration, uncertainty scores, self-calibration<br />
Summary:<br />
The research discusses a new Self-Guided Iterative Calibration Framework (SGIC) that enhances the calibration efficacy of large language models (LLMs) by utilizing uncertainty scores. The framework leverages specific cues to improve in-context reasoning and response accuracy, particularly in multi-round calibrations. Initially, uncertainty scores are calculated to evaluate document relevance and response confidence. The scores are then iteratively reevaluated and combined with prior responses to refine calibration. The framework also introduces a novel approach for constructing an iterative self-calibration training set, optimizing LLMs to effectively utilize uncertainty scores. Experimental results demonstrate significant performance improvements on both closed-source and open-weight LLMs, highlighting the effectiveness of the proposed SGIC framework in enhancing calibration capabilities. <br /><br />Summary: <div>
arXiv:2506.16172v1 Announce Type: new 
Abstract: Recent research in retrieval-augmented generation (RAG) has concentrated on retrieving useful information from candidate documents. However, numerous methodologies frequently neglect the calibration capabilities of large language models (LLMs), which capitalize on their robust in-context reasoning prowess. This work illustrates that providing LLMs with specific cues substantially improves their calibration efficacy, especially in multi-round calibrations. We present a new SGIC: Self-Guided Iterative Calibration Framework that employs uncertainty scores as a tool. Initially, this framework calculates uncertainty scores to determine both the relevance of each document to the query and the confidence level in the responses produced by the LLMs. Subsequently, it reevaluates these scores iteratively, amalgamating them with prior responses to refine calibration. Furthermore, we introduce an innovative approach for constructing an iterative self-calibration training set, which optimizes LLMs to efficiently harness uncertainty scores for capturing critical information and enhancing response accuracy. Our proposed framework significantly improves performance on both closed-source and open-weight LLMs.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JETHICS: Japanese Ethics Understanding Evaluation Dataset</title>
<link>https://arxiv.org/abs/2506.16187</link>
<guid>https://arxiv.org/abs/2506.16187</guid>
<content:encoded><![CDATA[
<div> ethics, AI models, Japanese dataset, normative theories, GPT-4o 
Summary: 
- The article introduces JETHICS, a Japanese dataset designed for evaluating ethics understanding of AI models, containing 78K examples following construction methods of the ETHICS dataset.
- JETHICS includes categories based on normative theories, concepts from ethics and political philosophy, and commonsense morality.
- Evaluation experiments on non-proprietary large language models (LLMs) and GPT-4o show room for improvement, with GPT-4o scoring around 0.7 and the best Japanese LLM achieving about 0.5.
- The results suggest a need for enhancing AI models' ethics understanding and performance in the context of the dataset.
- JETHICS provides a valuable resource for further research and development in the field of ethics and AI models. 
<br /><br />Summary: <div>
arXiv:2506.16187v1 Announce Type: new 
Abstract: In this work, we propose JETHICS, a Japanese dataset for evaluating ethics understanding of AI models. JETHICS contains 78K examples and is built by following the construction methods of the existing English ETHICS dataset. It includes four categories based normative theories and concepts from ethics and political philosophy; and one representing commonsense morality. Our evaluation experiments on non-proprietary large language models (LLMs) and on GPT-4o reveal that even GPT-4o achieves only an average score of about 0.7, while the best-performing Japanese LLM attains around 0.5, indicating a relatively large room for improvement in current LLMs.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Web(er) of Hate: A Survey on How Hate Speech Is Typed</title>
<link>https://arxiv.org/abs/2506.16190</link>
<guid>https://arxiv.org/abs/2506.16190</guid>
<content:encoded><![CDATA[
<div> Keywords: hate speech, datasets, methodological choices, dataset reliability, transparency

Summary: 
This paper examines the curation of hate speech datasets and the methodological choices involved. It discusses the balancing act of priorities and highlights common themes and practices in dataset creation. The implications of these choices on dataset reliability are also analyzed. Drawing on Max Weber's concept of ideal types, the authors advocate for a reflexive approach in dataset creation. They stress the importance of researchers acknowledging their own value judgments during the construction process to promote transparency and methodological rigor. By fostering transparency and being mindful of biases in dataset creation, researchers can ensure the credibility and robustness of hate speech datasets. This critical examination of methodological choices serves as a call for researchers to be self-aware and reflective in their dataset creation practices. 

<br /><br />Summary: <div>
arXiv:2506.16190v1 Announce Type: new 
Abstract: The curation of hate speech datasets involves complex design decisions that balance competing priorities. This paper critically examines these methodological choices in a diverse range of datasets, highlighting common themes and practices, and their implications for dataset reliability. Drawing on Max Weber's notion of ideal types, we argue for a reflexive approach in dataset creation, urging researchers to acknowledge their own value judgments during dataset construction, fostering transparency and methodological rigour.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Analysis of Abstractive Summarization Models for Clinical Radiology Reports</title>
<link>https://arxiv.org/abs/2506.16247</link>
<guid>https://arxiv.org/abs/2506.16247</guid>
<content:encoded><![CDATA[
<div> abstractive summarization models, radiology report, MIMIC-CXR dataset, comparison, evaluation metrics<br />
<br />
Summary: 
This research explores the use of advanced abstractive summarization models to condense the impression section from the detailed findings of a radiology report. Using the MIMIC-CXR dataset, the study compares various pre-trained language models such as T5-base, BART-base, PEGASUS-x-base, ChatGPT-4, LLaMA-3-8B, and a custom Pointer Generator Network with a coverage mechanism. Multiple evaluation metrics, including ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and BERTScore, are utilized to assess the performance of these models in summarizing medical text. The findings of the study highlight the strengths and limitations of each model, providing valuable insights for medical professionals seeking automated summarization solutions in the healthcare sector. <br /><br /> <div>
arXiv:2506.16247v1 Announce Type: new 
Abstract: The findings section of a radiology report is often detailed and lengthy, whereas the impression section is comparatively more compact and captures key diagnostic conclusions. This research explores the use of advanced abstractive summarization models to generate the concise impression from the findings section of a radiology report. We have used the publicly available MIMIC-CXR dataset. A comparative analysis is conducted on leading pre-trained and open-source large language models, including T5-base, BART-base, PEGASUS-x-base, ChatGPT-4, LLaMA-3-8B, and a custom Pointer Generator Network with a coverage mechanism. To ensure a thorough assessment, multiple evaluation metrics are employed, including ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and BERTScore. By analyzing the performance of these models, this study identifies their respective strengths and limitations in the summarization of medical text. The findings of this paper provide helpful information for medical professionals who need automated summarization solutions in the healthcare sector.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled Data</title>
<link>https://arxiv.org/abs/2506.16251</link>
<guid>https://arxiv.org/abs/2506.16251</guid>
<content:encoded><![CDATA[
<div> Hypothesis testing, weakly labeled data, speech-to-text translation, low-resource languages, dataset construction<br />
Summary:<br />
This paper investigates the use of weakly labeled data to develop speech-to-text translation (ST) systems for low-resource languages. By mining the multilingual Shrutilipi corpus, datasets were created for language pairs Bengali-Hindi, Malayalam-Hindi, Odia-Hindi, and Telugu-Hindi. Various versions of training data were generated to analyze the impact of data quality and quantity on ST model performance. The results indicate that ST systems can effectively utilize weakly labeled data, achieving performance levels comparable to large-scale multi-modal multilingual baselines like SONAR and SeamlessM4T. This study offers insights into addressing data scarcity challenges in developing efficient ST systems for low-resource languages. <br /><br />Summary: <div>
arXiv:2506.16251v1 Announce Type: new 
Abstract: The scarcity of high-quality annotated data presents a significant challenge in developing effective end-to-end speech-to-text translation (ST) systems, particularly for low-resource languages. This paper explores the hypothesis that weakly labeled data can be used to build ST models for low-resource language pairs. We constructed speech-to-text translation datasets with the help of bitext mining using state-of-the-art sentence encoders. We mined the multilingual Shrutilipi corpus to build Shrutilipi-anuvaad, a dataset comprising ST data for language pairs Bengali-Hindi, Malayalam-Hindi, Odia-Hindi, and Telugu-Hindi. We created multiple versions of training data with varying degrees of quality and quantity to investigate the effect of quality versus quantity of weakly labeled data on ST model performance. Results demonstrate that ST systems can be built using weakly labeled data, with performance comparable to massive multi-modal multilingual baselines such as SONAR and SeamlessM4T.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Automated Speaking Assessment Leveraging Multifaceted Relevance and Grammar Information</title>
<link>https://arxiv.org/abs/2506.16285</link>
<guid>https://arxiv.org/abs/2506.16285</guid>
<content:encoded><![CDATA[
<div> Keywords: automated speaking assessment, content relevance, grammar error analysis, hybrid scoring model, L2 speaker

Summary: 
This article addresses shortcomings in current automated speaking assessment (ASA) systems by introducing two key enhancements. The first enhancement is a multifaceted relevance module that combines question content, images, exemplars, and the spoken responses of L2 speakers for a more comprehensive assessment of content relevance. The second enhancement involves fine-grained grammar error features derived from advanced grammar error correction and detailed annotation to identify specific error categories. Experiments and ablation studies show that these enhancements significantly improve the evaluation of content relevance, language use, and overall ASA performance. The study emphasizes the importance of utilizing richer and more nuanced feature sets for a holistic assessment of speaking abilities. 

<br /><br />Summary: <div>
arXiv:2506.16285v1 Announce Type: new 
Abstract: Current automated speaking assessment (ASA) systems for use in multi-aspect evaluations often fail to make full use of content relevance, overlooking image or exemplar cues, and employ superficial grammar analysis that lacks detailed error types. This paper ameliorates these deficiencies by introducing two novel enhancements to construct a hybrid scoring model. First, a multifaceted relevance module integrates question and the associated image content, exemplar, and spoken response of an L2 speaker for a comprehensive assessment of content relevance. Second, fine-grained grammar error features are derived using advanced grammar error correction (GEC) and detailed annotation to identify specific error categories. Experiments and ablation studies demonstrate that these components significantly improve the evaluation of content relevance, language use, and overall ASA performance, highlighting the benefits of using richer, more nuanced feature sets for holistic speaking assessment.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PL-Guard: Benchmarking Language Model Safety for Polish</title>
<link>https://arxiv.org/abs/2506.16322</link>
<guid>https://arxiv.org/abs/2506.16322</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, language model safety classification, Polish, adversarial perturbation, model evaluation

Summary: 
The study introduces a benchmark dataset for evaluating language model safety in Polish, addressing the lack of assessment tools for non-English languages. Adversarially perturbed samples were created to test model robustness. Three models were fine-tuned and compared: Llama-Guard-3-8B, HerBERT-based classifier, and PLLuM. Training on annotated data revealed the HerBERT-based classifier performed best, especially in adversarial scenarios. Results show the importance of evaluating language model safety in various languages and highlight the effectiveness of the HerBERT-based classifier in the Polish context. <br /><br />Summary: <div>
arXiv:2506.16322v1 Announce Type: new 
Abstract: Despite increasing efforts to ensure the safety of large language models (LLMs), most existing safety assessments and moderation tools remain heavily biased toward English and other high-resource languages, leaving majority of global languages underexamined. To address this gap, we introduce a manually annotated benchmark dataset for language model safety classification in Polish. We also create adversarially perturbed variants of these samples designed to challenge model robustness. We conduct a series of experiments to evaluate LLM-based and classifier-based models of varying sizes and architectures. Specifically, we fine-tune three models: Llama-Guard-3-8B, a HerBERT-based classifier (a Polish BERT derivative), and PLLuM, a Polish-adapted Llama-8B model. We train these models using different combinations of annotated data and evaluate their performance, comparing it against publicly available guard models. Results demonstrate that the HerBERT-based classifier achieves the highest overall performance, particularly under adversarial conditions.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizability of Media Frames: Corpus creation and analysis across countries</title>
<link>https://arxiv.org/abs/2506.16337</link>
<guid>https://arxiv.org/abs/2506.16337</guid>
<content:encoded><![CDATA[
<div> Keywords: Frames, Media Frame Corpus, Brazilian Portuguese news, Political and economic news, Cross-cultural frame use

Summary: 
The article explores the use of frames in political language and its influence on people's opinions. The Media Frame Corpus (MFC) is a commonly used framework for analyzing frames in U.S. news issues, but its applicability to other cultural contexts, such as Brazilian news, is unclear. The authors introduce FrameNews-PT, a dataset of Brazilian Portuguese news articles, and annotate it within the MFC framework to evaluate its generalizability. Results show that MFC frames can be broadly applied to Brazilian news issues with minor revisions, but some frames are rarely used. Novel news topics are often analyzed using more general frames. The study highlights the importance of considering cross-cultural differences in frame use. Overall, the research suggests that understanding how frames are used in different cultural contexts is essential for accurately analyzing political discourse and shaping public opinion.

<br /><br />Summary: <div>
arXiv:2506.16337v1 Announce Type: new 
Abstract: Frames capture aspects of an issue that are emphasized in a debate by interlocutors and can help us understand how political language conveys different perspectives and ultimately shapes people's opinions. The Media Frame Corpus (MFC) is the most commonly used framework with categories and detailed guidelines for operationalizing frames. It is, however, focused on a few salient U.S. news issues, making it unclear how well these frames can capture news issues in other cultural contexts. To explore this, we introduce FrameNews-PT, a dataset of Brazilian Portuguese news articles covering political and economic news and annotate it within the MFC framework. Through several annotation rounds, we evaluate the extent to which MFC frames generalize to the Brazilian debate issues. We further evaluate how fine-tuned and zero-shot models perform on out-of-domain data. Results show that the 15 MFC frames remain broadly applicable with minor revisions of the guidelines. However, some MFC frames are rarely used, and novel news issues are analyzed using general 'fall-back' frames. We conclude that cross-cultural frame use requires careful consideration.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing the Influence of Knowledge Graph Information on Relation Extraction</title>
<link>https://arxiv.org/abs/2506.16343</link>
<guid>https://arxiv.org/abs/2506.16343</guid>
<content:encoded><![CDATA[
<div> knowledge graph, relation extraction, performance improvement, Neural Bellman-Ford networks, supervised and zero-shot settings

Summary:
The study explores the impact of incorporating knowledge graph information on relation extraction model performance. The hypothesis posits that knowledge graph entity positions offer valuable insights for relation extraction tasks. Experiments conducted on diverse datasets with varying relations and training examples reveal that integrating knowledge graph information leads to significant performance enhancements, particularly effective when handling imbalanced training examples for relations. The study evaluates the effectiveness of knowledge graph-based features by integrating them into established relation extraction methods and employing graph-aware Neural Bellman-Ford networks. These features are tested in supervised and zero-shot settings, consistently improving performance across different datasets. <div>
arXiv:2506.16343v1 Announce Type: new 
Abstract: We examine the impact of incorporating knowledge graph information on the performance of relation extraction models across a range of datasets. Our hypothesis is that the positions of entities within a knowledge graph provide important insights for relation extraction tasks. We conduct experiments on multiple datasets, each varying in the number of relations, training examples, and underlying knowledge graphs. Our results demonstrate that integrating knowledge graph information significantly enhances performance, especially when dealing with an imbalance in the number of training examples for each relation. We evaluate the contribution of knowledge graph-based features by combining established relation extraction methods with graph-aware Neural Bellman-Ford networks. These features are tested in both supervised and zero-shot settings, demonstrating consistent performance improvements across various datasets.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DISCIE -- Discriminative Closed Information Extraction</title>
<link>https://arxiv.org/abs/2506.16348</link>
<guid>https://arxiv.org/abs/2506.16348</guid>
<content:encoded><![CDATA[
<div> discriminative approach, relation extraction, long-tail relations, closed information extraction, efficiency

Summary:<br /><br />
This paper introduces a novel method for closed information extraction using a discriminative approach that takes into account type and entity-specific information to enhance relation extraction accuracy, especially for long-tail relations. The method outperforms state-of-the-art generative models, demonstrating superior performance in large-scale closed information extraction scenarios with numerous entities and relations. By utilizing smaller models and integrating type-information, the method achieves comparable or better performance than larger generative models, emphasizing efficiency in information extraction techniques. This advancement shows promise for more accurate and efficient extraction of information. <div>
arXiv:2506.16348v1 Announce Type: new 
Abstract: This paper introduces a novel method for closed information extraction. The method employs a discriminative approach that incorporates type and entity-specific information to improve relation extraction accuracy, particularly benefiting long-tail relations. Notably, this method demonstrates superior performance compared to state-of-the-art end-to-end generative models. This is especially evident for the problem of large-scale closed information extraction where we are confronted with millions of entities and hundreds of relations. Furthermore, we emphasize the efficiency aspect by leveraging smaller models. In particular, the integration of type-information proves instrumental in achieving performance levels on par with or surpassing those of a larger generative model. This advancement holds promise for more accurate and efficient information extraction techniques.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can structural correspondences ground real world representational content in Large Language Models?</title>
<link>https://arxiv.org/abs/2506.16370</link>
<guid>https://arxiv.org/abs/2506.16370</guid>
<content:encoded><![CDATA[
<div> representation, language models, structural correspondences, task performance, real world contents

Summary:
The paper examines whether Large Language Models (LLMs) such as GPT-4 can truly represent real-world entities and if so, how. It discusses the challenge posed by the text-bounded nature of LLMs and explores the idea of structural correspondences between LLMs and real entities. The author argues that these correspondences alone are not enough to establish representation but suggests that if these correspondences contribute to successful task performance, they could ground representations of real-world contents. The paper highlights the importance of utilizing these structural correspondences in a way that explains LLMs' abilities to perform tasks effectively. By considering the relationship between LLMs, textual inputs, and their interaction with the external world, the paper delves into the complexities of determining the representational capacities of LLMs. <div>
arXiv:2506.16370v1 Announce Type: new 
Abstract: Large Language Models (LLMs) such as GPT-4 produce compelling responses to a wide range of prompts. But their representational capacities are uncertain. Many LLMs have no direct contact with extra-linguistic reality: their inputs, outputs and training data consist solely of text, raising the questions (1) can LLMs represent anything and (2) if so, what? In this paper, I explore what it would take to answer these questions according to a structural-correspondence based account of representation, and make an initial survey of this evidence. I argue that the mere existence of structural correspondences between LLMs and worldly entities is insufficient to ground representation of those entities. However, if these structural correspondences play an appropriate role - they are exploited in a way that explains successful task performance - then they could ground real world contents. This requires overcoming a challenge: the text-boundedness of LLMs appears, on the face of it, to prevent them engaging in the right sorts of tasks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems</title>
<link>https://arxiv.org/abs/2506.16381</link>
<guid>https://arxiv.org/abs/2506.16381</guid>
<content:encoded><![CDATA[
<div> benchmark, style control, instruction-following TTS systems, paralinguistic features, natural-language instructions
<br />
<br />
Summary: 
The article introduces InstructTTSEval, a benchmark for evaluating complex natural-language style control in Text-to-Speech (TTS) systems. It addresses the limitations of traditional TTS systems by assessing the systems' capability to interpret and execute complex instructions for modulating paralinguistic features. Three tasks are defined in the benchmark, each with English and Chinese subsets, including Acoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play. The benchmark consists of 1k test cases per task with paired reference audio and leverages Gemini as an automatic judge. Evaluation results show the need for improvement in instruction-following TTS systems. InstructTTSEval aims to drive progress towards more powerful, flexible, and accurate instruction-based TTS systems. 
<br />
<br />Summary: <div>
arXiv:2506.16381v1 Announce Type: new 
Abstract: In modern speech synthesis, paralinguistic information--such as a speaker's vocal timbre, emotional state, and dynamic prosody--plays a critical role in conveying nuance beyond mere semantics. Traditional Text-to-Speech (TTS) systems rely on fixed style labels or inserting a speech prompt to control these cues, which severely limits flexibility. Recent attempts seek to employ natural-language instructions to modulate paralinguistic features, substantially improving the generalization of instruction-driven TTS models. Although many TTS systems now support customized synthesis via textual description, their actual ability to interpret and execute complex instructions remains largely unexplored. In addition, there is still a shortage of high-quality benchmarks and automated evaluation metrics specifically designed for instruction-based TTS, which hinders accurate assessment and iterative optimization of these models. To address these limitations, we introduce InstructTTSEval, a benchmark for measuring the capability of complex natural-language style control. We introduce three tasks, namely Acoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play, including English and Chinese subsets, each with 1k test cases (6k in total) paired with reference audio. We leverage Gemini as an automatic judge to assess their instruction-following abilities. Our evaluation of accessible instruction-following TTS systems highlights substantial room for further improvement. We anticipate that InstructTTSEval will drive progress toward more powerful, flexible, and accurate instruction-following TTS.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models in Argument Mining: A Survey</title>
<link>https://arxiv.org/abs/2506.16383</link>
<guid>https://arxiv.org/abs/2506.16383</guid>
<content:encoded><![CDATA[
<div> Keywords: Argument Mining, Natural Language Processing, Large Language Models, Annotation Frameworks, Evaluation Practices

Summary: 
This survey delves into the advancements in Argument Mining driven by Large Language Models (LLMs). It reviews foundational theories and annotation frameworks in the field, while also providing a curated list of datasets. A taxonomy of Argument Mining subtasks is presented, highlighting how LLM techniques have reshaped their execution. The survey covers LLM architectures and methodologies, evaluates current practices, and addresses challenges like long-context reasoning and interpretability. It also discusses emerging trends and proposes a research agenda for LLM-based computational argumentation. The aim is to guide researchers in navigating the rapidly evolving landscape of Argument Mining in the era of Large Language Models.<br /><br />Summary: <div>
arXiv:2506.16383v1 Announce Type: new 
Abstract: Argument Mining (AM), a critical subfield of Natural Language Processing (NLP), focuses on extracting argumentative structures from text. The advent of Large Language Models (LLMs) has profoundly transformed AM, enabling advanced in-context learning, prompt-based generation, and robust cross-domain adaptability. This survey systematically synthesizes recent advancements in LLM-driven AM. We provide a concise review of foundational theories and annotation frameworks, alongside a meticulously curated catalog of datasets. A key contribution is our comprehensive taxonomy of AM subtasks, elucidating how contemporary LLM techniques -- such as prompting, chain-of-thought reasoning, and retrieval augmentation -- have reconfigured their execution. We further detail current LLM architectures and methodologies, critically assess evaluation practices, and delineate pivotal challenges including long-context reasoning, interpretability, and annotation bottlenecks. Conclusively, we highlight emerging trends and propose a forward-looking research agenda for LLM-based computational argumentation, aiming to strategically guide researchers in this rapidly evolving domain.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HausaNLP at SemEval-2025 Task 11: Advancing Hausa Text-based Emotion Detection</title>
<link>https://arxiv.org/abs/2506.16388</link>
<guid>https://arxiv.org/abs/2506.16388</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-label emotion detection, Hausa language, AfriBERTa, transformer-based model, low-resource languages

Summary: 
- The paper presents an approach to multi-label emotion detection in the low-resource African language of Hausa as part of the SemEval Track A competition.
- The researchers fine-tuned AfriBERTa, a transformer-based model pre-trained on African languages, to classify Hausa text into six emotions: anger, disgust, fear, joy, sadness, and surprise.
- The methodology involved data preprocessing, tokenization, and model fine-tuning using the Hugging Face Trainer API.
- The system achieved a validation accuracy of 74.00% and an F1-score of 73.50%, showing the effectiveness of transformer-based models for emotion detection in low-resource languages.
- This study demonstrates the potential of leveraging pre-trained transformer models for emotion detection tasks, particularly in languages with limited resources.<br /><br />Summary: <div>
arXiv:2506.16388v1 Announce Type: new 
Abstract: This paper presents our approach to multi-label emotion detection in Hausa, a low-resource African language, as part of SemEval Track A. We fine-tuned AfriBERTa, a transformer-based model pre-trained on African languages, to classify Hausa text into six emotions: anger, disgust, fear, joy, sadness, and surprise. Our methodology involved data preprocessing, tokenization, and model fine-tuning using the Hugging Face Trainer API. The system achieved a validation accuracy of 74.00%, with an F1-score of 73.50%, demonstrating the effectiveness of transformer-based models for emotion detection in low-resource languages.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RiOT: Efficient Prompt Refinement with Residual Optimization Tree</title>
<link>https://arxiv.org/abs/2506.16389</link>
<guid>https://arxiv.org/abs/2506.16389</guid>
<content:encoded><![CDATA[
<div> optimize, large language models, prompt optimization, RiOT, automatic

Summary: RiOT is a novel framework for automatic prompt optimization in large language models. It addresses the lack of diversity and semantic drift issues faced by existing methods by generating multiple diverse prompt candidates and selecting the best one based on perplexity. By incorporating a text residual connection, RiOT helps mitigate semantic drift by retaining beneficial content across optimization iterations. The tree structure efficiently manages the optimization process, ensuring scalability and flexibility. Experimental results across five benchmarks show that RiOT outperforms previous prompt optimization methods and manual prompting techniques. <div>
arXiv:2506.16389v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have highlighted their potential across a variety of tasks, but their performance still heavily relies on the design of effective prompts. Existing methods for automatic prompt optimization face two challenges: lack of diversity, limiting the exploration of valuable and innovative directions and semantic drift, where optimizations for one task can degrade performance in others. To address these issues, we propose Residual Optimization Tree (RiOT), a novel framework for automatic prompt optimization. RiOT iteratively refines prompts through text gradients, generating multiple semantically diverse candidates at each step, and selects the best prompt using perplexity. Additionally, RiOT incorporates the text residual connection to mitigate semantic drift by selectively retaining beneficial content across optimization iterations. A tree structure efficiently manages the optimization process, ensuring scalability and flexibility. Extensive experiments across five benchmarks, covering commonsense, mathematical, logical, temporal, and semantic reasoning, demonstrate that RiOT outperforms both previous prompt optimization methods and manual prompting.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling</title>
<link>https://arxiv.org/abs/2506.16393</link>
<guid>https://arxiv.org/abs/2506.16393</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, annotation, multi-model cooperative annotation, AutoAnnotator, fine-grained semantic understanding

Summary: 
AutoAnnotator introduces a novel approach to annotation tasks by combining Large Language Models (LLMs) and Small Language Models (SLMs) in a multi-model cooperative framework. The system utilizes a two-layer architecture where a meta-controller layer leverages LLMs for selecting SLMs, generating annotation code, and verifying difficult samples, while a task-specialist layer comprising multiple SLMs performs annotation through multi-model voting. The framework also incorporates a continual learning strategy to fine-tune SLMs and improve their generalization. Experimental results demonstrate that AutoAnnotator achieves superior performance in various annotation settings compared to existing LLMs, significantly reducing annotation costs while enhancing accuracy. The project website provides access to the framework for further exploration and application development.

<br /><br />Summary: <div>
arXiv:2506.16393v1 Announce Type: new 
Abstract: Although the annotation paradigm based on Large Language Models (LLMs) has made significant breakthroughs in recent years, its actual deployment still has two core bottlenecks: first, the cost of calling commercial APIs in large-scale annotation is very expensive; second, in scenarios that require fine-grained semantic understanding, such as sentiment classification and toxicity classification, the annotation accuracy of LLMs is even lower than that of Small Language Models (SLMs) dedicated to this field. To address these problems, we propose a new paradigm of multi-model cooperative annotation and design a fully automatic annotation framework AutoAnnotator based on this. Specifically, AutoAnnotator consists of two layers. The upper-level meta-controller layer uses the generation and reasoning capabilities of LLMs to select SLMs for annotation, automatically generate annotation code and verify difficult samples; the lower-level task-specialist layer consists of multiple SLMs that perform annotation through multi-model voting. In addition, we use the difficult samples obtained by the secondary review of the meta-controller layer as the reinforcement learning set and fine-tune the SLMs in stages through a continual learning strategy, thereby improving the generalization of SLMs. Extensive experiments show that AutoAnnotator outperforms existing open-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings. Notably, AutoAnnotator reduces the annotation cost by 74.15% compared to directly annotating with GPT-3.5-turbo, while still improving the accuracy by 6.21%. Project page: https://github.com/Zhaiyuan-Ji/AutoAnnotator.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OJBench: A Competition Level Code Benchmark For Large Language Models</title>
<link>https://arxiv.org/abs/2506.16395</link>
<guid>https://arxiv.org/abs/2506.16395</guid>
<content:encoded><![CDATA[
<div> large language models, code reasoning, benchmark, competitive-level, evaluation
Summary:
The article introduces OJBench, a challenging benchmark designed to assess competitive-level code reasoning abilities of large language models (LLMs). OJBench includes 232 programming competition problems from NOI and ICPC to provide a rigorous test of models' reasoning skills. Evaluation of 37 models shows that even top reasoning-oriented models like o4-mini and Gemini-2.5-pro-exp struggle with highly challenging competition-level problems. This highlights the significant challenges faced by models in competitive-level code reasoning. <div>
arXiv:2506.16395v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have demonstrated significant progress in math and code reasoning capabilities. However, existing code benchmark are limited in their ability to evaluate the full spectrum of these capabilities, particularly at the competitive level. To bridge this gap, we introduce OJBench, a novel and challenging benchmark designed to assess the competitive-level code reasoning abilities of LLMs. OJBench comprises 232 programming competition problems from NOI and ICPC, providing a more rigorous test of models' reasoning skills. We conducted a comprehensive evaluation using OJBench on 37 models, including both closed-source and open-source models, reasoning-oriented and non-reasoning-oriented models. Our results indicate that even state-of-the-art reasoning-oriented models, such as o4-mini and Gemini-2.5-pro-exp, struggle with highly challenging competition-level problems. This highlights the significant challenges that models face in competitive-level code reasoning.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NepaliGPT: A Generative Language Model for the Nepali Language</title>
<link>https://arxiv.org/abs/2506.16399</link>
<guid>https://arxiv.org/abs/2506.16399</guid>
<content:encoded><![CDATA[
<div> Keywords: NepaliGPT, Large Language Models, Nepali language, Devanagari Corpus, text generation 

Summary: 
NepaliGPT is introduced as the first generative large language model tailored for the Nepali language. A new corpus called the Devanagari Corpus is introduced for Nepali language research. The NepaliGPT benchmark dataset comprises 4,296 question-answer pairs in Nepali. NepaliGPT achieves a Perplexity of 26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25%, and causal consistency of 85.41% in text generation. This research aims to fill the gap in Nepali NLP by providing a model and resources for downstream tasks in the Nepali language. 

Summary: <br />
NepaliGPT is introduced as the first generative large language model tailored for the Nepali language. A new corpus called the Devanagari Corpus is introduced for Nepali language research. The NepaliGPT benchmark dataset comprises 4,296 question-answer pairs in Nepali. NepaliGPT achieves a Perplexity of 26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25%, and causal consistency of 85.41% in text generation. This research aims to fill the gap in Nepali NLP by providing a model and resources for downstream tasks in the Nepali language. <div>
arXiv:2506.16399v1 Announce Type: new 
Abstract: After the release of ChatGPT, Large Language Models (LLMs) have gained huge popularity in recent days and thousands of variants of LLMs have been released. However, there is no generative language model for the Nepali language, due to which other downstream tasks, including fine-tuning, have not been explored yet. To fill this research gap in the Nepali NLP space, this research proposes \textit{NepaliGPT}, a generative large language model tailored specifically for the Nepali language. This research introduces an advanced corpus for the Nepali language collected from several sources, called the Devanagari Corpus. Likewise, the research introduces the first NepaliGPT benchmark dataset comprised of 4,296 question-answer pairs in the Nepali language. The proposed LLM NepaliGPT achieves the following metrics in text generation: Perplexity of 26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25\%, and causal consistency of 85.41\%.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework</title>
<link>https://arxiv.org/abs/2506.16411</link>
<guid>https://arxiv.org/abs/2506.16411</guid>
<content:encoded><![CDATA[
<div> framework, Large Language Models, long texts, multi-agent chunking, noise
Summary:
The article investigates the challenges of applying Large Language Models (LLMs) to long texts. It proposes a theoretical framework that identifies three failure modes for long context tasks: cross-chunk dependence, confusion growing with context size, and imperfect integration of partial results. The study analyzes the effectiveness of multi-agent chunking, wherein a lengthy sequence is divided into smaller chunks for processing and aggregation. Experimentation on tasks such as retrieval, question answering, and summarization validates both the theoretical analysis and the conditions favoring multi-agent chunking. The research also explores the superlinear growth of model noise with input length and demonstrates how a weaker model, configured with chunk-based processing, can outperform a more advanced model like GPT4o when handling large inputs. Overall, the article presents a principled understanding framework and shows that carefully managed chunking and aggregator strategies provide a direct pathway to addressing long contexts in Large Language Models. 
Summary: <div>
arXiv:2506.16411v1 Announce Type: new 
Abstract: We investigate the challenge of applying Large Language Models (LLMs) to long texts. We propose a theoretical framework that distinguishes the failure modes of long context tasks into three categories: cross-chunk dependence (task noise), confusion that grows with context size (model noise), and the imperfect integration of partial results (aggregator noise). Under this view, we analyze when it is effective to use multi-agent chunking, i.e., dividing a length sequence into smaller chunks and aggregating the processed results of each chunk. Our experiments on tasks such as retrieval, question answering, and summarization confirm both the theoretical analysis and the conditions that favor multi-agent chunking. By exploring superlinear model noise growth with input length, we also explain why, for large inputs, a weaker model configured with chunk-based processing can surpass a more advanced model like GPT4o applied in a single shot. Overall, we present a principled understanding framework and our results highlight a direct pathway to handling long contexts in LLMs with carefully managed chunking and aggregator strategies.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing</title>
<link>https://arxiv.org/abs/2506.16444</link>
<guid>https://arxiv.org/abs/2506.16444</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, Large Language Models, Approximate Nearest Neighbor Search, In-Storage Processing, REIS <br />
Summary: <br />
Large Language Models like Retrieval-Augmented Generation (RAG) face limitations in accessing knowledge beyond their training data. The retrieval stage of RAG, involving Approximate Nearest Neighbor Search (ANNS), can be a bottleneck due to data movement overheads. To address this, the proposed REIS system optimizes ANNS for RAG by leveraging In-Storage Processing (ISP). REIS introduces a database layout linking embedding vectors to documents for efficient retrieval and employs a data placement technique to enhance ANNS performance. By utilizing existing computational resources within the storage system, REIS significantly improves retrieval performance compared to traditional server-grade systems, resulting in a 13x to 55x increase in energy efficiency. This tailored ISP approach boosts the overall efficiency of RAG inference pipelines. <br /> <div>
arXiv:2506.16444v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face an inherent challenge: their knowledge is confined to the data that they have been trained on. To overcome this issue, Retrieval-Augmented Generation (RAG) complements the static training-derived knowledge of LLMs with an external knowledge repository. RAG consists of three stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes a significant bottleneck in inference pipelines. In this stage, a user query is mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS) algorithm searches for similar vectors in the database to identify relevant items. Due to the large database sizes, ANNS incurs significant data movement overheads between the host and the storage system. To alleviate these overheads, prior works propose In-Storage Processing (ISP) techniques that accelerate ANNS by performing computations inside storage. However, existing works that leverage ISP for ANNS (i) employ algorithms that are not tailored to ISP systems, (ii) do not accelerate data retrieval operations for data selected by ANNS, and (iii) introduce significant hardware modifications, limiting performance and hindering their adoption. We propose REIS, the first ISP system tailored for RAG that addresses these limitations with three key mechanisms. First, REIS employs a database layout that links database embedding vectors to their associated documents, enabling efficient retrieval. Second, it enables efficient ANNS by introducing an ISP-tailored data placement technique that distributes embeddings across the planes of the storage system and employs a lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that uses the existing computational resources inside the storage system. Compared to a server-grade system, REIS improves the performance (energy efficiency) of retrieval by an average of 13x (55x).
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StoryWriter: A Multi-Agent Framework for Long Story Generation</title>
<link>https://arxiv.org/abs/2506.16445</link>
<guid>https://arxiv.org/abs/2506.16445</guid>
<content:encoded><![CDATA[
<div> Keywords: StoryWriter, long story generation, multi-agent framework, narrative complexity, supervised fine-tuning

Summary:
StoryWriter is a multi-agent framework designed to improve long story generation by addressing challenges related to discourse coherence and narrative complexity. The framework consists of three modules: outline agent, planning agent, and writing agent, each focusing on different aspects of story development. By generating event-based outlines and dynamically compressing the story history, StoryWriter ensures coherence and engagement in the generated stories. Evaluation results show that StoryWriter outperforms existing baselines in terms of story quality and length. The framework has been used to create a dataset of high-quality long stories, which have been used to train large language models such as Llama3.1-8B and GLM4-9B, resulting in advanced performance in long story generation. <div>
arXiv:2506.16445v1 Announce Type: new 
Abstract: Long story generation remains a challenge for existing large language models (LLMs), primarily due to two main factors: (1) discourse coherence, which requires plot consistency, logical coherence, and completeness in the long-form generation, and (2) narrative complexity, which requires an interwoven and engaging narrative. To address these challenges, we propose StoryWriter, a multi-agent story generation framework, which consists of three main modules: (1) outline agent, which generates event-based outlines containing rich event plots, character, and event-event relationships. (2) planning agent, which further details events and plans which events should be written in each chapter to maintain an interwoven and engaging story. (3) writing agent, which dynamically compresses the story history based on the current event to generate and reflect new plots, ensuring the coherence of the generated story. We conduct both human and automated evaluation, and StoryWriter significantly outperforms existing story generation baselines in both story quality and length. Furthermore, we use StoryWriter to generate a dataset, which contains about $6,000$ high-quality long stories, with an average length of $8,000$ words. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning on LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which demonstrates advanced performance in long story generation.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection</title>
<link>https://arxiv.org/abs/2506.16476</link>
<guid>https://arxiv.org/abs/2506.16476</guid>
<content:encoded><![CDATA[
<div> lexicon analysis, implicit hate speech, harmful speech datasets, crowdsourced datasets, detection

Summary:

The paper addresses the challenge of detecting implicit hate speech on social media platforms. It highlights that existing harmful speech datasets may already contain implicit hate speech that has not been explicitly recognized. The study focuses on improving detection techniques to identify veiled and subtle forms of hate speech. The approach proposed involves identifying influential samples, reannotating the data, and augmenting it using Llama-3 70B and GPT-40. By leveraging these methods, the study demonstrates a significant improvement in implicit hate speech detection, achieving a +12.9-point F1 score compared to the baseline. This approach enhances generalizability across diverse datasets and aims to improve understanding and identification of harmful speech online. <div>
arXiv:2506.16476v1 Announce Type: new 
Abstract: Implicit hate speech has recently emerged as a critical challenge for social media platforms. While much of the research has traditionally focused on harmful speech in general, the need for generalizable techniques to detect veiled and subtle forms of hate has become increasingly pressing. Based on lexicon analysis, we hypothesize that implicit hate speech is already present in publicly available harmful speech datasets but may not have been explicitly recognized or labeled by annotators. Additionally, crowdsourced datasets are prone to mislabeling due to the complexity of the task and often influenced by annotators' subjective interpretations. In this paper, we propose an approach to address the detection of implicit hate speech and enhance generalizability across diverse datasets by leveraging existing harmful speech datasets. Our method comprises three key components: influential sample identification, reannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental results demonstrate the effectiveness of our approach in improving implicit hate detection, achieving a +12.9-point F1 score improvement compared to the baseline.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples</title>
<link>https://arxiv.org/abs/2506.16502</link>
<guid>https://arxiv.org/abs/2506.16502</guid>
<content:encoded><![CDATA[
<div> Keywords: Reward models, Multilingual, Indic languages, RELIC, In-context learning <br />
Summary: <br />
Reward models are crucial for aligning large language models (LLMs) with human preferences. However, existing multilingual reward models are mainly trained on preference datasets in high-resource languages, making them unreliable for low-resource Indic languages. To address this issue, RELIC introduces an in-context learning framework that trains a retriever to select relevant examples from high-resource languages for reward modeling in low-resource Indic languages. Through extensive experiments on three preference datasets, RELIC outperforms existing methods in improving reward model accuracy for low-resource Indic languages. For instance, on the Bodo language, RELIC achieves significant improvements over zero-shot prompting and state-of-the-art example selection methods, demonstrating its effectiveness in enhancing reward modeling capabilities for low-resource languages. <div>
arXiv:2506.16502v1 Announce Type: new 
Abstract: Reward models are essential for aligning large language models (LLMs) with human preferences. However, most open-source multilingual reward models are primarily trained on preference datasets in high-resource languages, resulting in unreliable reward signals for low-resource Indic languages. Collecting large-scale, high-quality preference data for these languages is prohibitively expensive, making preference-based training approaches impractical. To address this challenge, we propose RELIC, a novel in-context learning framework for reward modeling in low-resource Indic languages. RELIC trains a retriever with a pairwise ranking objective to select in-context examples from auxiliary high-resource languages that most effectively highlight the distinction between preferred and less-preferred responses. Extensive experiments on three preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art open-source reward models demonstrate that RELIC significantly improves reward model accuracy for low-resource Indic languages, consistently outperforming existing example selection methods. For example, on Bodo-a low-resource Indic language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13% improvement in accuracy over zero-shot prompting and state-of-the-art example selection method, respectively.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Speech Recognition Biases in Newcastle English: an Error Analysis</title>
<link>https://arxiv.org/abs/2506.16558</link>
<guid>https://arxiv.org/abs/2506.16558</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Speech Recognition, regional dialects, Newcastle English, error analysis, sociolinguistic analysis <br />
Summary: <br />
The study examines the impact of regional dialects on Automatic Speech Recognition (ASR) systems, focusing on Newcastle English. Through a manual error analysis, phonological, lexical, and morphosyntactic errors contributing to ASR misrecognitions are identified. A case study delves into ASR recognition of regional pronouns like "yous" and "wor". Results show a direct correlation between ASR errors and regional dialectal features. Social factors have a lesser influence on ASR mismatches. The study advocates for incorporating greater dialectal diversity in ASR training data and emphasizes the importance of sociolinguistic analysis in detecting and addressing regional biases. <br /> <div>
arXiv:2506.16558v1 Announce Type: new 
Abstract: Automatic Speech Recognition (ASR) systems struggle with regional dialects due to biased training which favours mainstream varieties. While previous research has identified racial, age, and gender biases in ASR, regional bias remains underexamined. This study investigates ASR performance on Newcastle English, a well-documented regional dialect known to be challenging for ASR. A two-stage analysis was conducted: first, a manual error analysis on a subsample identified key phonological, lexical, and morphosyntactic errors behind ASR misrecognitions; second, a case study focused on the systematic analysis of ASR recognition of the regional pronouns ``yous'' and ``wor''. Results show that ASR errors directly correlate with regional dialectal features, while social factors play a lesser role in ASR mismatches. We advocate for greater dialectal diversity in ASR training data and highlight the value of sociolinguistic analysis in diagnosing and addressing regional biases.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weight Factorization and Centralization for Continual Learning in Speech Recognition</title>
<link>https://arxiv.org/abs/2506.16574</link>
<guid>https://arxiv.org/abs/2506.16574</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network, speech recognition, continual learning, catastrophic forgetting, code-switching

Summary: 
Modern neural network based speech recognition models need to continually absorb new data without re-training the entire system. This is especially important for downstream applications using foundation models without access to the original training data. Continual training without rehearsal can lead to catastrophic forgetting, where even small disruptions to the weights can severely impact model quality. To address this, a new approach inspired by the human brain's ability to consolidate knowledge through the waking-sleeping cycle has been proposed. The approach involves two phases: factorization and centralization, where knowledge is learned and merged accordingly. Experiments on various code-switching datasets demonstrate that the centralization phase effectively prevents catastrophic forgetting by accumulating knowledge in low-rank adapters. <br /><br />Summary: <div>
arXiv:2506.16574v1 Announce Type: new 
Abstract: Modern neural network based speech recognition models are required to continually absorb new data without re-training the whole system, especially in downstream applications using foundation models, having no access to the original training data. Continually training the models in a rehearsal-free, multilingual, and language agnostic condition, likely leads to catastrophic forgetting, when a seemingly insignificant disruption to the weights can destructively harm the quality of the models. Inspired by the ability of human brains to learn and consolidate knowledge through the waking-sleeping cycle, we propose a continual learning approach with two distinct phases: factorization and centralization, learning and merging knowledge accordingly. Our experiments on a sequence of varied code-switching datasets showed that the centralization stage can effectively prevent catastrophic forgetting by accumulating the knowledge in multiple scattering low-rank adapters.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement</title>
<link>https://arxiv.org/abs/2506.16580</link>
<guid>https://arxiv.org/abs/2506.16580</guid>
<content:encoded><![CDATA[
<div> Keywords: streaming, accent conversion, Emformer encoder, text-to-speech, latency

Summary: 
The article introduces a novel streaming accent conversion (AC) model that transforms non-native speech into a native-like accent while retaining speaker identity, prosody, and improving pronunciation. By incorporating an Emformer encoder and an optimized inference mechanism, the model enables real-time processing and achieves performance comparable to leading AC models. This streaming AC system is the first of its kind, capable of processing input speech in a continuous stream while maintaining stable latency. The integration of a native text-to-speech (TTS) model allows for the generation of ideal ground-truth data, enhancing training efficiency. This innovative approach represents a significant advancement in the field of accent conversion, offering a solution that combines high performance with real-time processing capabilities. <br /><br />Summary: <div>
arXiv:2506.16580v1 Announce Type: new 
Abstract: We propose a first streaming accent conversion (AC) model that transforms non-native speech into a native-like accent while preserving speaker identity, prosody and improving pronunciation. Our approach enables stream processing by modifying a previous AC architecture with an Emformer encoder and an optimized inference mechanism. Additionally, we integrate a native text-to-speech (TTS) model to generate ideal ground-truth data for efficient training. Our streaming AC model achieves comparable performance to the top AC models while maintaining stable latency, making it the first AC system capable of streaming.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework</title>
<link>https://arxiv.org/abs/2506.16584</link>
<guid>https://arxiv.org/abs/2506.16584</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, world model, evaluation framework, semantic diagnostics, model robustness

Summary: 
The article discusses the importance of determining whether large language models (LLMs) have a solid world model to ensure their reliability in various applications. A formal framework is proposed to evaluate if an LLM exhibits a robust world model by measuring response consistency to semantically equivalent prompts. The evaluation approach decomposes response variability into three components: user purpose, user articulation, and model instability. A strong world model should attribute variability to foundational purpose rather than superficial articulation changes. The results show that larger models tend to attribute more variability to user purpose, indicating a more robust world model. However, the improvement in model robustness is not consistent across all domains, suggesting the need for semantic diagnostics over accuracy-based benchmarks to assess a model's internal understanding and stability. <div>
arXiv:2506.16584v1 Announce Type: new 
Abstract: Understanding whether large language models (LLMs) possess a world model-a structured understanding of the world that supports generalization beyond surface-level patterns-is central to assessing their reliability, especially in high-stakes applications. We propose a formal framework for evaluating whether an LLM exhibits a sufficiently robust world model, defined as producing consistent outputs across semantically equivalent prompts while distinguishing between prompts that express different intents. We introduce a new evaluation approach to measure this that decomposes model response variability into three components: variability due to user purpose, user articulation, and model instability. An LLM with a strong world model should attribute most of the variability in its responses to changes in foundational purpose rather than superficial changes in articulation. This approach allows us to quantify how much of a model's behavior is semantically grounded rather than driven by model instability or alternative wording. We apply this framework to evaluate LLMs across diverse domains. Our results show how larger models attribute a greater share of output variability to changes in user purpose, indicating a more robust world model. This improvement is not uniform, however: larger models do not consistently outperform smaller ones across all domains, and their advantage in robustness is often modest. These findings highlight the importance of moving beyond accuracy-based benchmarks toward semantic diagnostics that more directly assess the structure and stability of a model's internal understanding of the world.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications</title>
<link>https://arxiv.org/abs/2506.16594</link>
<guid>https://arxiv.org/abs/2506.16594</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic data generation, large language models, biomedical research, clinical applications, evaluation metrics <br />
Summary: 
This scoping review discusses the use of large language models (LLMs) in synthetic data generation for biomedical fields. It examines 59 studies published between 2020 and 2025, focusing on data modalities, generation methods, and evaluation metrics. The analysis reveals that unstructured texts are the most common data modality, with prompting being the primary generation method. Evaluation metrics include intrinsic metrics, human-in-the-loop assessments, and LLM-based evaluations. The review highlights the potential of synthetic data generation in addressing data scarcity, privacy concerns, and data quality challenges in biomedical research. However, challenges such as adaption across clinical domains, resource accessibility, and evaluation standardizations need to be addressed for widespread adoption of synthetic data generation in healthcare. <div>
arXiv:2506.16594v1 Announce Type: new 
Abstract: Synthetic data generation--mitigating data scarcity, privacy concerns, and data quality challenges in biomedical fields--has been facilitated by rapid advances of large language models (LLMs). This scoping review follows PRISMA-ScR guidelines and synthesizes 59 studies, published between 2020 and 2025 and collected from PubMed, ACM, Web of Science, and Google Scholar. The review systematically examines biomedical research and application trends in synthetic data generation, emphasizing clinical applications, methodologies, and evaluations. Our analysis identifies data modalities of unstructured texts (78.0%), tabular data (13.6%), and multimodal sources (8.4%); generation methods of prompting (72.9%), fine-tuning (22.0%) LLMs and specialized model (5.1%); and heterogeneous evaluations of intrinsic metrics (27.1%), human-in-the-loop assessments (55.9%), and LLM-based evaluations (13.6%). The analysis addresses current limitations in what, where, and how health professionals can leverage synthetic data generation for biomedical domains. Our review also highlights challenges in adaption across clinical domains, resource and model accessibility, and evaluation standardizations.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Public Perceptions of Science in Media</title>
<link>https://arxiv.org/abs/2506.16622</link>
<guid>https://arxiv.org/abs/2506.16622</guid>
<content:encoded><![CDATA[
<div> Keywords: public perception, science communication, NLP models, public engagement, science news

Summary:
- A computational framework is introduced to model public perception across dimensions like newsworthiness, importance, and surprisingness.
- A large-scale science news perception dataset is created with annotations from diverse US and UK populations, providing insights into public responses to scientific information.
- NLP models are developed to predict public perception scores with strong performance.
- Individuals' frequency of science news consumption is the main driver of perception, while demographic factors have minimal influence.
- Public perception of scientific information has a direct impact on engagement, as posts with more positive perception scores receive more comments and upvotes on platforms like Reddit.
- The research highlights the importance of nuanced perception modeling in science communication and offers new ways to predict public interest and engagement with scientific content. 

<br /><br />Summary: <div>
arXiv:2506.16622v1 Announce Type: new 
Abstract: Effectively engaging the public with science is vital for fostering trust and understanding in our scientific community. Yet, with an ever-growing volume of information, science communicators struggle to anticipate how audiences will perceive and interact with scientific news. In this paper, we introduce a computational framework that models public perception across twelve dimensions, such as newsworthiness, importance, and surprisingness. Using this framework, we create a large-scale science news perception dataset with 10,489 annotations from 2,101 participants from diverse US and UK populations, providing valuable insights into public responses to scientific information across domains. We further develop NLP models that predict public perception scores with a strong performance. Leveraging the dataset and model, we examine public perception of science from two perspectives: (1) Perception as an outcome: What factors affect the public perception of scientific information? (2) Perception as a predictor: Can we use the estimated perceptions to predict public engagement with science? We find that individuals' frequency of science news consumption is the driver of perception, whereas demographic factors exert minimal influence. More importantly, through a large-scale analysis and carefully designed natural experiment on Reddit, we demonstrate that the estimated public perception of scientific information has direct connections with the final engagement pattern. Posts with more positive perception scores receive significantly more comments and upvotes, which is consistent across different scientific information and for the same science, but are framed differently. Overall, this research underscores the importance of nuanced perception modeling in science communication, offering new pathways to predict public interest and engagement with scientific content.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System</title>
<link>https://arxiv.org/abs/2506.16628</link>
<guid>https://arxiv.org/abs/2506.16628</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, natural language processing, clinical settings, rule-based systems, named entity recognition <br />
Summary:<br />
The study explores the combination of large language models (LLMs) and rule-based systems in natural language processing (NLP) for clinical applications. Despite the efficiency and interpretability of rule-based systems in clinical settings, their manual development is labor-intensive. The proposed approach utilizes LLMs during the development phase to enhance the rule-based NLP pipeline. Initial experiments focused on identifying relevant text snippets and extracting keywords for named entity recognition (NER) components. Results showed high recall rates for identifying clinical text snippets and perfect extraction of key terms for NER. This approach offers a more cost-effective, efficient, and transparent method for developing rule-based NLP systems compared to deep learning models. The integration of LLMs in rule-based systems shows promise for semi-automated or automated development processes in NLP for clinical applications. <br /><br />Summary: <div>
arXiv:2506.16628v1 Announce Type: new 
Abstract: Despite advances in machine learning (ML) and large language models (LLMs), rule-based natural language processing (NLP) systems remain active in clinical settings due to their interpretability and operational efficiency. However, their manual development and maintenance are labor-intensive, particularly in tasks with large linguistic variability. To overcome these limitations, we proposed a novel approach employing LLMs solely during the rule-based systems development phase. We conducted the initial experiments focusing on the first two steps of developing a rule-based NLP pipeline: find relevant snippets from the clinical note; extract informative keywords from the snippets for the rule-based named entity recognition (NER) component. Our experiments demonstrated exceptional recall in identifying clinically relevant text snippets (Deepseek: 0.98, Qwen: 0.99) and 1.0 in extracting key terms for NER. This study sheds light on a promising new direction for NLP development, enabling semi-automated or automated development of rule-based systems with significantly faster, more cost-effective, and transparent execution compared with deep learning model-based solutions.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View</title>
<link>https://arxiv.org/abs/2506.16633</link>
<guid>https://arxiv.org/abs/2506.16633</guid>
<content:encoded><![CDATA[
arXiv:2506.16633v1 Announce Type: new 
Abstract: Multimodal reasoning is a process of understanding, integrating and inferring information across different data modalities. It has recently attracted surging academic attention as a benchmark for Artificial Intelligence (AI). Although there are various tasks for evaluating multimodal reasoning ability, they still have limitations. Lack of reasoning on hierarchical visual clues at different levels of granularity, e.g., local details and global context, is of little discussion, despite its frequent involvement in real scenarios. To bridge the gap, we introduce a novel and challenging task for multimodal reasoning, namely GeoGuess. Given a street view image, the task is to identify its location and provide a detailed explanation. A system that succeeds in GeoGuess should be able to detect tiny visual clues, perceive the broader landscape, and associate with vast geographic knowledge. Therefore, GeoGuess would require the ability to reason between hierarchical visual information and geographic knowledge. In this work, we establish a benchmark for GeoGuess by introducing a specially curated dataset GeoExplain which consists of panoramas-geocoordinates-explanation tuples. Additionally, we present a multimodal and multilevel reasoning method, namely SightSense which can make prediction and generate comprehensive explanation based on hierarchy of visual information and external knowledge. Our analysis and experiments demonstrate their outstanding performance in GeoGuess.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Context Generalization with Sparse Attention</title>
<link>https://arxiv.org/abs/2506.16640</link>
<guid>https://arxiv.org/abs/2506.16640</guid>
<content:encoded><![CDATA[
arXiv:2506.16640v1 Announce Type: new 
Abstract: Transformer-based architectures traditionally employ softmax to compute attention weights, which produces dense distributions over all tokens in a sequence. While effective in many settings, this density has been shown to be detrimental for tasks that demand precise focus on fixed-size patterns: as sequence length increases, non-informative tokens accumulate attention probability mass, leading to dispersion and representational collapse. We show in this paper that sparse attention mechanisms using $\alpha$-entmax can avoid these issues, due to their ability to assign exact zeros to irrelevant tokens. Furthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows $\alpha$-entmax with a learnable temperature parameter, allowing the attention distribution to interpolate between sparse (pattern-focused) and dense (softmax-like) regimes. Finally, we show that the ability to locate and generalize fixed-size patterns can be further improved through a careful design of position encodings, which impacts both dense and sparse attention methods. By integrating ASEntmax into standard transformer layers alongside proper positional encodings, we show that our models greatly outperform softmax, scalable softmax, and fixed-temperature $\alpha$-entmax baselines on long-context generalization.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arch-Router: Aligning LLM Routing with Human Preferences</title>
<link>https://arxiv.org/abs/2506.16655</link>
<guid>https://arxiv.org/abs/2506.16655</guid>
<content:encoded><![CDATA[
arXiv:2506.16655v1 Announce Type: new 
Abstract: With the rapid proliferation of large language models (LLMs) -- each optimized for different strengths, style, or latency/cost profile -- routing has become an essential technique to operationalize the use of different models. However, existing LLM routing approaches are limited in two key ways: they evaluate performance using benchmarks that often fail to capture human preferences driven by subjective evaluation criteria, and they typically select from a limited pool of models. In this work, we propose a preference-aligned routing framework that guides model selection by matching queries to user-defined domains (e.g., travel) or action types (e.g., image editing) -- offering a practical mechanism to encode preferences in routing decisions. Specifically, we introduce \textbf{Arch-Router}, a compact 1.5B model that learns to map queries to domain-action preferences for model routing decisions. Our approach also supports seamlessly adding new models for routing without requiring retraining or architectural modifications. Experiments on conversational datasets demonstrate that our approach achieves state-of-the-art (SOTA) results in matching queries with human preferences, outperforming top proprietary models. Our approach captures subjective evaluation criteria and makes routing decisions more transparent and flexible. Our model is available at: \texttt{https://huggingface.co/katanemo/Arch-Router-1.5B}.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations</title>
<link>https://arxiv.org/abs/2506.16678</link>
<guid>https://arxiv.org/abs/2506.16678</guid>
<content:encoded><![CDATA[
arXiv:2506.16678v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit a robust mastery of syntax when processing and generating text. While this suggests internalized understanding of hierarchical syntax and dependency relations, the precise mechanism by which they represent syntactic structure is an open area within interpretability research. Probing provides one way to identify the mechanism of syntax being linearly encoded in activations, however, no comprehensive study has yet established whether a model's probing accuracy reliably predicts its downstream syntactic performance. Adopting a "mechanisms vs. outcomes" framework, we evaluate 32 open-weight transformer models and find that syntactic features extracted via probing fail to predict outcomes of targeted syntax evaluations across English linguistic phenomena. Our results highlight a substantial disconnect between latent syntactic representations found via probing and observable syntactic behaviors in downstream tasks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LegiGPT: Party Politics and Transport Policy with Large Language Model</title>
<link>https://arxiv.org/abs/2506.16692</link>
<guid>https://arxiv.org/abs/2506.16692</guid>
<content:encoded><![CDATA[
arXiv:2506.16692v1 Announce Type: new 
Abstract: Given the significant influence of lawmakers' political ideologies on legislative decision-making, understanding their impact on policymaking is critically important. We introduce a novel framework, LegiGPT, which integrates a large language model (LLM) with explainable artificial intelligence (XAI) to analyze transportation-related legislative proposals. LegiGPT employs a multi-stage filtering and classification pipeline using zero-shot prompting with GPT-4. Using legislative data from South Korea's 21st National Assembly, we identify key factors - including sponsor characteristics, political affiliations, and geographic variables - that significantly influence transportation policymaking. The LLM was used to classify transportation-related bill proposals through a stepwise filtering process based on keywords, phrases, and contextual relevance. XAI techniques were then applied to examine relationships between party affiliation and associated attributes. The results reveal that the number and proportion of conservative and progressive sponsors, along with district size and electoral population, are critical determinants shaping legislative outcomes. These findings suggest that both parties contributed to bipartisan legislation through different forms of engagement, such as initiating or supporting proposals. This integrated approach provides a valuable tool for understanding legislative dynamics and guiding future policy development, with broader implications for infrastructure planning and governance.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models</title>
<link>https://arxiv.org/abs/2506.16712</link>
<guid>https://arxiv.org/abs/2506.16712</guid>
<content:encoded><![CDATA[
arXiv:2506.16712v1 Announce Type: new 
Abstract: Generative Reward Models (GRMs) provide greater flexibility than scalar reward models in capturing human preferences, but their effectiveness is limited by poor reasoning capabilities. This often results in incomplete or overly speculative reasoning paths, leading to hallucinations or missing key information in complex tasks. We address this challenge with ReasonGRM, a three-stage generative reward modeling framework. In the first stage, Zero-RL is used to generate concise, outcome-directed reasoning paths that reduce the likelihood of critical omissions. In the second stage, we introduce a novel evaluation metric, $R^\star$, which scores reasoning paths based on their generation likelihood. This favors paths that reach correct answers with minimal exploration, helping to reduce hallucination-prone data during training. In the final stage, the model is further refined through reinforcement learning on challenging examples to enhance its preference discrimination capabilities. Experiments on three public benchmarks show that ReasonGRM achieves competitive or state-of-the-art performance, outperforming previous best GRMs by 1.8\% on average and surpassing proprietary models such as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of reasoning-aware training and highlight the importance of high-quality rationale selection for reliable preference modeling.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Model Confidence on Bias Effects in Measured Uncertainties</title>
<link>https://arxiv.org/abs/2506.16724</link>
<guid>https://arxiv.org/abs/2506.16724</guid>
<content:encoded><![CDATA[
arXiv:2506.16724v1 Announce Type: new 
Abstract: With the growing adoption of Large Language Models (LLMs) for open-ended tasks, accurately assessing epistemic uncertainty, which reflects a model's lack of knowledge, has become crucial to ensuring reliable outcomes. However, quantifying epistemic uncertainty in such tasks is challenging due to the presence of aleatoric uncertainty, which arises from multiple valid answers. While bias can introduce noise into epistemic uncertainty estimation, it may also reduce noise from aleatoric uncertainty. To investigate this trade-off, we conduct experiments on Visual Question Answering (VQA) tasks and find that mitigating prompt-introduced bias improves uncertainty quantification in GPT-4o. Building on prior work showing that LLMs tend to copy input information when model confidence is low, we further analyze how these prompt biases affect measured epistemic and aleatoric uncertainty across varying bias-free confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases induce greater changes in both uncertainties when bias-free model confidence is lower. Moreover, lower bias-free model confidence leads to greater underestimation of epistemic uncertainty (i.e. overconfidence) due to bias, whereas it has no significant effect on the direction of changes in aleatoric uncertainty estimation. These distinct effects deepen our understanding of bias mitigation for uncertainty quantification and potentially inform the development of more advanced techniques.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization</title>
<link>https://arxiv.org/abs/2506.16738</link>
<guid>https://arxiv.org/abs/2506.16738</guid>
<content:encoded><![CDATA[
arXiv:2506.16738v1 Announce Type: new 
Abstract: With the rapid progress of speech language models (SLMs), discrete speech tokens have emerged as a core interface between speech and text, enabling unified modeling across modalities. Recent speech tokenization approaches aim to isolate semantic information from low-level acoustics to better align with language models. In particular, previous methods use SSL teachers such as HuBERT to extract semantic representations, which are then distilled into a semantic quantizer to suppress acoustic redundancy as well as capture content-related latent structures. However, they still produce speech token sequences significantly longer than their textual counterparts, creating challenges for efficient speech-language modeling. Reducing the frame rate is a natural solution, but standard techniques, such as rigid average pooling across frames, can distort or dilute the semantic structure required for effective LM alignment. To address this, we propose LM-SPT, a speech tokenization method that introduces a novel semantic distillation. Instead of directly matching teacher and student features via pooling, we reconstruct speech solely from semantic tokens and minimize the discrepancy between the encoded representations of the original and reconstructed waveforms, obtained from a frozen automatic speech recognition (ASR) encoder. This indirect yet data-driven supervision enables the tokenizer to learn discrete units that are more semantically aligned with language models. LM-SPT further incorporates architectural improvements to the encoder and decoder for speech tokenization, and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz. Experimental results show that LM-SPT achieves superior reconstruction fidelity compared to baselines, and that SLMs trained with LM-SPT tokens achieve competitive performances on speech-to-text and consistently outperform baselines on text-to-speech tasks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly</title>
<link>https://arxiv.org/abs/2506.16755</link>
<guid>https://arxiv.org/abs/2506.16755</guid>
<content:encoded><![CDATA[
arXiv:2506.16755v1 Announce Type: new 
Abstract: Drawing real world social inferences usually requires taking into account information from multiple modalities. Language is a particularly powerful source of information in social settings, especially in novel situations where language can provide both abstract information about the environment dynamics and concrete specifics about an agent that cannot be easily visually observed. In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a framework for drawing context-specific social inferences that integrate linguistic and visual inputs. LIRAS frames multimodal social reasoning as a process of constructing structured but situation-specific agent and environment representations - leveraging multimodal language models to parse language and visual inputs into unified symbolic representations, over which a Bayesian inverse planning engine can be run to produce granular probabilistic judgments. On a range of existing and new social reasoning tasks derived from cognitive science experiments, we find that our model (instantiated with a comparatively lightweight VLM) outperforms ablations and state-of-the-art models in capturing human judgments across all domains.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocialSim: Towards Socialized Simulation of Emotional Support Conversation</title>
<link>https://arxiv.org/abs/2506.16756</link>
<guid>https://arxiv.org/abs/2506.16756</guid>
<content:encoded><![CDATA[
arXiv:2506.16756v1 Announce Type: new 
Abstract: Emotional support conversation (ESC) helps reduce people's psychological stress and provide emotional value through interactive dialogues. Due to the high cost of crowdsourcing a large ESC corpus, recent attempts use large language models for dialogue augmentation. However, existing approaches largely overlook the social dynamics inherent in ESC, leading to less effective simulations. In this paper, we introduce SocialSim, a novel framework that simulates ESC by integrating key aspects of social interactions: social disclosure and social awareness. On the seeker side, we facilitate social disclosure by constructing a comprehensive persona bank that captures diverse and authentic help-seeking scenarios. On the supporter side, we enhance social awareness by eliciting cognitive reasoning to generate logical and supportive responses. Building upon SocialSim, we construct SSConv, a large-scale synthetic ESC corpus of which quality can even surpass crowdsourced ESC data. We further train a chatbot on SSConv and demonstrate its state-of-the-art performance in both automatic and human evaluations. We believe SocialSim offers a scalable way to synthesize ESC, making emotional care more accessible and practical.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.16760</link>
<guid>https://arxiv.org/abs/2506.16760</guid>
<content:encoded><![CDATA[
arXiv:2506.16760v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) demonstrate exceptional performance across multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass built-in safety mechanisms to elicit restricted content generation. Existing black-box jailbreak methods primarily rely on adversarial textual prompts or image perturbations, yet these approaches are highly detectable by standard content filtering systems and exhibit low query and computational efficiency. In this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO), a novel black-box jailbreak attack framework that decomposes malicious prompts into semantically benign visual and textual fragments. By leveraging LVLMs' cross-modal reasoning abilities, CAMO covertly reconstructs harmful instructions through multi-step reasoning, evading conventional detection mechanisms. Our approach supports adjustable reasoning complexity and requires significantly fewer queries than prior attacks, enabling both stealth and efficiency. Comprehensive evaluations conducted on leading LVLMs validate CAMO's effectiveness, showcasing robust performance and strong cross-model transferability. These results underscore significant vulnerabilities in current built-in safety mechanisms, emphasizing an urgent need for advanced, alignment-aware security and safety solutions in vision-language systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DistillNote: LLM-based clinical note summaries improve heart failure diagnosis</title>
<link>https://arxiv.org/abs/2506.16777</link>
<guid>https://arxiv.org/abs/2506.16777</guid>
<content:encoded><![CDATA[
arXiv:2506.16777v1 Announce Type: new 
Abstract: Large language models (LLMs) offer unprecedented opportunities to generate concise summaries of patient information and alleviate the burden of clinical documentation that overwhelms healthcare providers. We present Distillnote, a framework for LLM-based clinical note summarization, and generate over 64,000 admission note summaries through three techniques: (1) One-step, direct summarization, and a divide-and-conquer approach involving (2) Structured summarization focused on independent clinical insights, and (3) Distilled summarization that further condenses the Structured summaries. We test how useful are the summaries by using them to predict heart failure compared to a model trained on the original notes. Distilled summaries achieve 79% text compression and up to 18.2% improvement in AUPRC compared to an LLM trained on the full notes. We also evaluate the quality of the generated summaries in an LLM-as-judge evaluation as well as through blinded pairwise comparisons with clinicians. Evaluations indicate that one-step summaries are favoured by clinicians according to relevance and clinical actionability, while distilled summaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio) and significantly reduce hallucinations. We release our summaries on PhysioNet to encourage future research.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning</title>
<link>https://arxiv.org/abs/2506.16792</link>
<guid>https://arxiv.org/abs/2506.16792</guid>
<content:encoded><![CDATA[
arXiv:2506.16792v1 Announce Type: new 
Abstract: Despite efforts to align large language models (LLMs) with societal and moral values, these models remain susceptible to jailbreak attacks--methods designed to elicit harmful responses. Jailbreaking black-box LLMs is considered challenging due to the discrete nature of token inputs, restricted access to the target LLM, and limited query budget. To address the issues above, we propose an effective method for jailbreaking black-box large language Models via Iterative Semantic Tuning, named MIST. MIST enables attackers to iteratively refine prompts that preserve the original semantic intent while inducing harmful content. Specifically, to balance semantic similarity with computational efficiency, MIST incorporates two key strategies: sequential synonym search, and its advanced version--order-determining optimization. Extensive experiments across two open-source models and four closed-source models demonstrate that MIST achieves competitive attack success rates and attack transferability compared with other state-of-the-art white-box and black-box jailbreak methods. Additionally, we conduct experiments on computational efficiency to validate the practical viability of MIST.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts</title>
<link>https://arxiv.org/abs/2506.16912</link>
<guid>https://arxiv.org/abs/2506.16912</guid>
<content:encoded><![CDATA[
arXiv:2506.16912v1 Announce Type: new 
Abstract: Sample efficiency is a crucial property of language models with practical implications for training efficiency. In real-world text, information follows a long-tailed distribution. Yet, we expect models to learn and recall frequent and infrequent facts. Sample-efficient models are better equipped to handle this challenge of learning and retaining rare information without requiring excessive exposure. This study analyzes multiple models of varying architectures and sizes, all trained on the same pre-training data. By annotating relational facts with their frequencies in the training corpus, we examine how model performance varies with fact frequency. Our findings show that most models perform similarly on high-frequency facts but differ notably on low-frequency facts. This analysis provides new insights into the relationship between model architecture, size, and factual learning efficiency.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond</title>
<link>https://arxiv.org/abs/2506.16982</link>
<guid>https://arxiv.org/abs/2506.16982</guid>
<content:encoded><![CDATA[
arXiv:2506.16982v1 Announce Type: new 
Abstract: Accurately assessing student knowledge is critical for effective education, yet traditional Knowledge Tracing (KT) methods rely on opaque latent embeddings, limiting interpretability. Even LLM-based approaches generate direct predictions or summaries that may hallucinate without any accuracy guarantees. We recast KT as an inverse problem: learning the minimum natural-language summary that makes past answers explainable and future answers predictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM that writes an interpretable knowledge summary and a frozen decoder LLM that must reconstruct and predict student responses using only that summary text. By constraining all predictive information to pass through a short natural-language bottleneck, LBMs ensure that the summary contains accurate information while remaining human-interpretable. Experiments on synthetic arithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the accuracy of state-of-the-art KT and direct LLM methods while requiring orders-of-magnitude fewer student trajectories. We demonstrate that training the encoder with group-relative policy optimization, using downstream decoding accuracy as a reward signal, effectively improves summary quality.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs</title>
<link>https://arxiv.org/abs/2506.16990</link>
<guid>https://arxiv.org/abs/2506.16990</guid>
<content:encoded><![CDATA[
arXiv:2506.16990v1 Announce Type: new 
Abstract: LaTeX's precision and flexibility in typesetting have made it the gold standard for the preparation of scientific documentation. Large Language Models (LLMs) present a promising opportunity for researchers to produce publication-ready material using LaTeX with natural language instructions, yet current benchmarks completely lack evaluation of this ability. By introducing TeXpert, our benchmark dataset with natural language prompts for generating LaTeX code focused on components of scientific documents across multiple difficulty levels, we conduct an in-depth analysis of LLM performance in this regard and identify frequent error types. Our evaluation across open and closed-source LLMs highlights multiple key findings: LLMs excelling on standard benchmarks perform poorly in LaTeX generation with a significant accuracy drop-off as the complexity of tasks increases; open-source models like DeepSeek v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks; and formatting and package errors are unexpectedly prevalent, suggesting a lack of diverse LaTeX examples in the training datasets of most LLMs. Our dataset, code, and model evaluations are available at https://github.com/knowledge-verse-ai/TeXpert.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonalAI: Towards digital twins in the graph form</title>
<link>https://arxiv.org/abs/2506.17001</link>
<guid>https://arxiv.org/abs/2506.17001</guid>
<content:encoded><![CDATA[
arXiv:2506.17001v1 Announce Type: new 
Abstract: The challenge of personalizing language models, specifically the ability to account for a user's history during interactions, is of significant interest. Despite recent advancements in large language models (LLMs) and Retrieval Augmented Generation that have enhanced the factual base of LLMs, the task of retaining extensive personal information and using it to generate personalized responses remains pertinent. To address this, we propose utilizing external memory in the form of knowledge graphs, which are constructed and updated by the LLM itself. We have expanded upon ideas of AriGraph architecture and for the first time introduced a combined graph featuring both standard edges and two types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and DiaASQ benchmarks indicates that this approach aids in making the process of graph construction and knowledge extraction unified and robust. Furthermore, we augmented the DiaASQ benchmark by incorporating parameters such as time into dialogues and introducing contradictory statements made by the same speaker at different times. Despite these modifications, the performance of the question-answering system remained robust, demonstrating the proposed architecture's ability to maintain and utilize temporal dependencies.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Generated Feedback Supports Learning If Learners Choose to Use It</title>
<link>https://arxiv.org/abs/2506.17006</link>
<guid>https://arxiv.org/abs/2506.17006</guid>
<content:encoded><![CDATA[
arXiv:2506.17006v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used to generate feedback, yet their impact on learning remains underexplored, especially compared to existing feedback methods. This study investigates how on-demand LLM-generated explanatory feedback influences learning in seven scenario-based tutor training lessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we compare posttest performance among learners across three groups: learners who received feedback generated by gpt-3.5-turbo, those who declined it, and those without access. All groups received non-LLM corrective feedback. To address potential selection bias-where higher-performing learners may be more inclined to use LLM feedback-we applied propensity scoring. Learners with a higher predicted likelihood of engaging with LLM feedback scored significantly higher at posttest than those with lower propensity. After adjusting for this effect, two out of seven lessons showed statistically significant learning benefits from LLM feedback with standardized effect sizes of 0.28 and 0.33. These moderate effects suggest that the effectiveness of LLM feedback depends on the learners' tendency to seek support. Importantly, LLM feedback did not significantly increase completion time, and learners overwhelmingly rated it as helpful. These findings highlight LLM feedback's potential as a low-cost and scalable way to improve learning on open-ended tasks, particularly in existing systems already providing feedback without LLMs. This work contributes open datasets, LLM prompts, and rubrics to support reproducibility.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instituto de Telecomunica\c{c}\~oes at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning</title>
<link>https://arxiv.org/abs/2506.17019</link>
<guid>https://arxiv.org/abs/2506.17019</guid>
<content:encoded><![CDATA[
arXiv:2506.17019v1 Announce Type: new 
Abstract: This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on Instruction Following Speech Processing. We submit results for the Short Track, i.e., speech recognition, translation, and spoken question answering. Our model is a unified speech-to-text model that integrates a pre-trained continuous speech encoder and text decoder through a first phase of modality alignment and a second phase of instruction fine-tuning. Crucially, we focus on using small-scale language model backbones (< 2B) and restrict to high-quality, CC-BY data along with synthetic data generation to supplement existing resources.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2506.17046</link>
<guid>https://arxiv.org/abs/2506.17046</guid>
<content:encoded><![CDATA[
arXiv:2506.17046v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant advances across numerous vision-language tasks. Due to their strong image-text alignment capability, MLLMs can effectively understand image-text pairs with clear meanings. However, effectively resolving the inherent ambiguities in natural language and visual contexts remains challenging. Existing multimodal benchmarks typically overlook linguistic and visual ambiguities, relying mainly on unimodal context for disambiguation and thus failing to exploit the mutual clarification potential between modalities. To bridge this gap, we introduce MUCAR, a novel and challenging benchmark designed explicitly for evaluating multimodal ambiguity resolution across multilingual and cross-modal scenarios. MUCAR includes: (1) a multilingual dataset where ambiguous textual expressions are uniquely resolved by corresponding visual contexts, and (2) a dual-ambiguity dataset that systematically pairs ambiguous images with ambiguous textual contexts, with each combination carefully constructed to yield a single, clear interpretation through mutual disambiguation. Extensive evaluations involving 19 state-of-the-art multimodal models--encompassing both open-source and proprietary architectures--reveal substantial gaps compared to human-level performance, highlighting the need for future research into more sophisticated cross-modal ambiguity comprehension methods, further pushing the boundaries of multimodal reasoning.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simultaneous Translation with Offline Speech and LLM Models in CUNI Submission to IWSLT 2025</title>
<link>https://arxiv.org/abs/2506.17077</link>
<guid>https://arxiv.org/abs/2506.17077</guid>
<content:encoded><![CDATA[
arXiv:2506.17077v1 Announce Type: new 
Abstract: This paper describes Charles University submission to the Simultaneous Speech Translation Task of the IWSLT 2025. We cover all four language pairs with a direct or cascade approach. The backbone of our systems is the offline Whisper speech model, which we use for both translation and transcription in simultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We further improve the performance by prompting to inject in-domain terminology, and we accommodate context. Our cascaded systems further use EuroLLM for unbounded simultaneous translation. Compared to the Organizers' baseline, our systems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on English to German, Chinese and Japanese on the development sets. Additionally, we also propose a new enhanced measure of speech recognition latency.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs</title>
<link>https://arxiv.org/abs/2506.17080</link>
<guid>https://arxiv.org/abs/2506.17080</guid>
<content:encoded><![CDATA[
arXiv:2506.17080v1 Announce Type: new 
Abstract: Fine-tuning pretrained LLMs has been shown to be an effective strategy for reaching state-of-the-art performance on specific tasks like machine translation. However, this process of adaptation often implies sacrificing general-purpose capabilities, such as conversational reasoning and instruction-following, hampering the utility of the system in real-world applications that require a mixture of skills. In this paper, we introduce Tower+, a suite of models designed to deliver strong performance across both translation and multilingual general-purpose text capabilities. We achieve a Pareto frontier between translation specialization and multilingual general-purpose capabilities by introducing a novel training recipe that builds on Tower (Alves et al., 2024), comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. At each stage of training, we carefully generate and curate data to strengthen performance on translation as well as general-purpose tasks involving code generation, mathematics problem solving, and general instruction-following. We develop models at multiple scales: 2B, 9B, and 72B. Our smaller models often outperform larger general-purpose open-weight and proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we introduce for evaluating both translation and instruction-following. Our findings highlight that it is possible to rival frontier models in general capabilities, while optimizing for specific business domains, such as translation and localization.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation</title>
<link>https://arxiv.org/abs/2506.17088</link>
<guid>https://arxiv.org/abs/2506.17088</guid>
<content:encoded><![CDATA[
arXiv:2506.17088v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often exhibit \textit{hallucinations}, generating factually incorrect or semantically irrelevant content in response to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by encouraging step-by-step reasoning, but its impact on hallucination detection remains underexplored. To bridge this gap, we conduct a systematic empirical evaluation. We begin with a pilot experiment, revealing that CoT reasoning significantly affects the LLM's internal states and token probability distributions. Building on this, we evaluate the impact of various CoT prompting methods on mainstream hallucination detection methods across both instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three key dimensions: changes in hallucination score distributions, variations in detection accuracy, and shifts in detection confidence. Our findings show that while CoT prompting helps reduce hallucination frequency, it also tends to obscure critical signals used for detection, impairing the effectiveness of various detection methods. Our study highlights an overlooked trade-off in the use of reasoning. Code is publicly available at: https://anonymous.4open.science/r/cot-hallu-detect.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better Language Model Inversion by Compactly Representing Next-Token Distributions</title>
<link>https://arxiv.org/abs/2506.17090</link>
<guid>https://arxiv.org/abs/2506.17090</guid>
<content:encoded><![CDATA[
arXiv:2506.17090v1 Announce Type: new 
Abstract: Language model inversion seeks to recover hidden prompts using only language model outputs. This capability has implications for security and accountability in language model deployments, such as leaking private information from an API-protected language model's system message. We propose a new method -- prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts by gleaning clues from the model's next-token probabilities over the course of multiple generation steps. Our method is enabled by a key insight: The vector-valued outputs of a language model occupy a low-dimensional subspace. This enables us to losslessly compress the full next-token probability distribution over multiple generation steps using a linear map, allowing more output information to be used for inversion. Our approach yields massive gains over previous state-of-the-art methods for recovering hidden prompts, achieving 2--3.5 times higher exact recovery rates across test sets, in one case increasing the recovery rate from 17% to 60%. Our method also exhibits surprisingly good generalization behavior; for instance, an inverter trained on 16 generations steps gets 5--27 points higher prompt recovery when we increase the number of steps to 32 at test time. Furthermore, we demonstrate strong performance of our method on the more challenging task of recovering hidden system messages. We also analyze the role of verbatim repetition in prompt recovery and propose a new method for cross-family model transfer for logit-based inverters. Our findings show that next-token probabilities are a considerably more vulnerable attack surface for inversion attacks than previously known.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?</title>
<link>https://arxiv.org/abs/2506.17121</link>
<guid>https://arxiv.org/abs/2506.17121</guid>
<content:encoded><![CDATA[
arXiv:2506.17121v1 Announce Type: new 
Abstract: Language models handle increasingly long contexts for tasks such as book summarization, but this leads to growing memory costs for the key-value (KV) cache. Many prior works have proposed ways of discarding KVs from memory, but their approaches are tailored to favorable settings, obscuring caveats like high peak memory and performance degradation, and a fair comparison between methods is difficult. In this paper, we propose the *KV footprint* as a unified metric, which accounts for both the amount of KV entries stored and their lifespan in memory. We evaluate methods based on the smallest footprint they attain while preserving performance in both long-context understanding and generation, with context lengths of up to 128K tokens. This metric reveals the high peak memory of prior KV eviction methods. One class of methods -- *post-fill eviction* -- has a high footprint due to being incompatible with eviction during pre-filling. We adapt these methods to be able to evict KVs during pre-filling, achieving substantially lower KV footprints. We then turn to *recency eviction* methods, wherein we propose PruLong, an end-to-end optimization method for learning which attention heads need to retain the full KV cache and which do not. PruLong saves memory while preserving long-context performance, achieving 12% smaller KV footprint than prior methods while retaining performance in challenging recall tasks. Our paper clarifies the complex tangle of long-context inference methods and paves the way for future development to minimize the KV footprint.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models</title>
<link>https://arxiv.org/abs/2506.17180</link>
<guid>https://arxiv.org/abs/2506.17180</guid>
<content:encoded><![CDATA[
arXiv:2506.17180v1 Announce Type: new 
Abstract: We introduce CLEAR-3K, a dataset of 3,000 assertion-reasoning questions designed to evaluate whether language models can determine if one statement causally explains another. Each question present an assertion-reason pair and challenge language models to distinguish between semantic relatedness and genuine causal explanatory relationships. Through comprehensive evaluation of 21 state-of-the-art language models (ranging from 0.5B to 72B parameters), we identify two fundamental findings. First, language models frequently confuse semantic similarity with causality, relying on lexical and semantic overlap instead of inferring actual causal explanatory relationships. Second, as parameter size increases, models tend to shift from being overly skeptical about causal relationships to being excessively permissive in accepting them. Despite this shift, performance measured by the Matthews Correlation Coefficient plateaus at just 0.55, even for the best-performing models.Hence, CLEAR-3K provides a crucial benchmark for developing and evaluating genuine causal reasoning in language models, which is an essential capability for applications that require accurate assessment of causal relationships.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards AI Search Paradigm</title>
<link>https://arxiv.org/abs/2506.17188</link>
<guid>https://arxiv.org/abs/2506.17188</guid>
<content:encoded><![CDATA[
arXiv:2506.17188v1 Announce Type: new 
Abstract: In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint for next-generation search systems capable of emulating human information processing and decision-making. The paradigm employs a modular architecture of four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically adapt to the full spectrum of information needs, from simple factual queries to complex multi-stage reasoning tasks. These agents collaborate dynamically through coordinated workflows to evaluate query complexity, decompose problems into executable plans, and orchestrate tool usage, task execution, and content synthesis. We systematically present key methodologies for realizing this paradigm, including task planning and tool integration, execution strategies, aligned and robust retrieval-augmented generation, and efficient LLM inference, spanning both algorithmic techniques and infrastructure-level optimizations. By providing an in-depth guide to these foundational components, this work aims to inform the development of trustworthy, adaptive, and scalable AI search systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency</title>
<link>https://arxiv.org/abs/2506.17209</link>
<guid>https://arxiv.org/abs/2506.17209</guid>
<content:encoded><![CDATA[
arXiv:2506.17209v1 Announce Type: new 
Abstract: Fine-tuning a general-purpose large language model (LLM) for a specific domain or task has become a routine procedure for ordinary users. However, fine-tuning is known to remove the safety alignment features of the model, even when the fine-tuning data does not contain any harmful content. We consider this to be a critical failure mode of LLMs due to the widespread uptake of fine-tuning, combined with the benign nature of the "attack". Most well-intentioned developers are likely unaware that they are deploying an LLM with reduced safety. On the other hand, this known vulnerability can be easily exploited by malicious actors intending to bypass safety guardrails. To make any meaningful progress in mitigating this issue, we first need reliable and reproducible safety evaluations. In this work, we investigate how robust a safety benchmark is to trivial variations in the experimental procedure, and the stochastic nature of LLMs. Our initial experiments expose surprising variance in the results of the safety evaluation, even when seemingly inconsequential changes are made to the fine-tuning setup. Our observations have serious implications for how researchers in this field should report results to enable meaningful comparisons in the future.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree</title>
<link>https://arxiv.org/abs/2506.15655</link>
<guid>https://arxiv.org/abs/2506.15655</guid>
<content:encoded><![CDATA[
arXiv:2506.15655v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale code generation, grounding predictions in external code corpora to improve actuality. However, a critical yet underexplored aspect of RAG pipelines is chunking -- the process of dividing documents into retrievable units. Existing line-based chunking heuristics often break semantic structures, splitting functions or merging unrelated code, which can degrade generation quality. We propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method that recursively breaks large AST nodes into smaller chunks and merges sibling nodes while respecting size limits. This approach generates self-contained, semantically coherent units across programming languages and tasks, improving performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3 points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation. Our work highlights the importance of structure-aware chunking for scaling retrieval-enhanced code intelligence.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models</title>
<link>https://arxiv.org/abs/2506.15689</link>
<guid>https://arxiv.org/abs/2506.15689</guid>
<content:encoded><![CDATA[
arXiv:2506.15689v1 Announce Type: cross 
Abstract: Rotations have become essential to state-of-the-art quantization pipelines for large language models (LLMs) by effectively smoothing outliers in weights and activations. However, further optimizing the rotation parameters offers only limited performance gains and introduces significant training overhead: due to rotation parameter sharing, full-model must be loaded simultaneously to enable backpropagation, resulting in substantial memory consumption and limited practical utility. In this work, we identify two fundamental limitations of current rotational quantization methods: (i) rotation fails to align channel means, resulting in wider quantization bounds and increased rounding errors; and (ii) rotation makes the activation distribution more Gaussian-like, increasing energy loss caused by clipping errors. To address these issues, we introduce \textbf{BASE-Q}, a simple yet powerful approach that combines bias correction and asymmetric scaling to effectively reduce rounding and clipping errors. Furthermore, BASE-Q enables blockwise optimization, eliminating the need for memory-intensive full-model backpropagation. Extensive experiments on various LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing the accuracy gap to full-precision models by 50.5\%, 42.9\%, and 29.2\% compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be released soon.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepRTL2: A Versatile Model for RTL-Related Tasks</title>
<link>https://arxiv.org/abs/2506.15697</link>
<guid>https://arxiv.org/abs/2506.15697</guid>
<content:encoded><![CDATA[
arXiv:2506.15697v1 Announce Type: cross 
Abstract: The integration of large language models (LLMs) into electronic design automation (EDA) has significantly advanced the field, offering transformative benefits, particularly in register transfer level (RTL) code generation and understanding. While previous studies have demonstrated the efficacy of fine-tuning LLMs for these generation-based tasks, embedding-based tasks, which are equally critical to EDA workflows, have been largely overlooked. These tasks, including natural language code search, RTL code functionality equivalence checking, and performance prediction, are essential for accelerating and optimizing the hardware design process. To address this gap, we present DeepRTL2, a family of versatile LLMs that unifies both generation- and embedding-based tasks related to RTL. By simultaneously tackling a broad range of tasks, DeepRTL2 represents the first model to provide a comprehensive solution to the diverse challenges in EDA. Through extensive experiments, we show that DeepRTL2 achieves state-of-the-art performance across all evaluated tasks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding</title>
<link>https://arxiv.org/abs/2506.15704</link>
<guid>https://arxiv.org/abs/2506.15704</guid>
<content:encoded><![CDATA[
arXiv:2506.15704v1 Announce Type: cross 
Abstract: As large language models (LLMs) continue to support increasingly longer contexts, the memory demand for key-value (KV) caches during decoding grows rapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe bandwidth. Sparse attention mechanisms alleviate this issue by computing attention weights only for selected key-value pairs. However, their indexing computation typically requires traversing all key vectors, resulting in significant computational and data transfer overhead. To reduce the cost of index retrieval, existing methods often treat each decoding step as an independent process, failing to exploit the temporal correlations embedded in historical decoding information. To this end, we propose LFPS(Learn From the Past for Sparse Indexing), an acceleration method that dynamically constructs sparse indexing candidates based on historical attention patterns. LFPS captures two prevalent trends in decoder attention -vertical patterns (attending to fixed positions) and slash patterns (attending to relative positions) -and incorporates a positional expansion strategy to effectively predict the Top-k indices for the current step. We validate LFPS on challenging long-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as the base model. Experimental results show that LFPS achieves up to 22.8$\times$ speedup over full attention and 9.6$\times$ speedup over exact Top-k retrieval on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively, while preserving generation accuracy. These results demonstrate that LFPS offers a practical and efficient solution for decoding optimization in long-context LLM inference.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and Scalable Replacement for Self-Attention</title>
<link>https://arxiv.org/abs/2506.15714</link>
<guid>https://arxiv.org/abs/2506.15714</guid>
<content:encoded><![CDATA[
arXiv:2506.15714v1 Announce Type: cross 
Abstract: We propose an innovative, learnable two-sided short-time Laplace transform (STLT) mechanism to supplant the traditional self attention in transformer-based LLMs. Our STLT introduces trainable parameters for each Laplace node, enabling end-to-end learning of decay rates , oscillatory frequencies, and window bandwidth T. This flexibility allows the model to dynamically adapt token relevance half lives and frequency responses during training. By selecting S learnable nodes and leveraging fast recursive convolution, we achieve an effective complexity of in time and memory. We further incorporate an efficient FFT-based computation of the relevance matrix and an adaptive node allocation mechanism to dynamically adjust the number of active Laplace nodes. Empirical results on language modeling (WikiText\-103, Project Gutenberg), machine translation (WMT'14 En\-De), and long document question answering (NarrativeQA) demonstrate that our learnable STLT achieves perplexities and scores on par with or better than existing efficient transformers while naturally extending to context lengths exceeding 100k tokens or more limited only by available hardware. Ablation studies confirm the importance of learnable parameters and adaptive node allocation. The proposed approach combines interpretability, through explicit decay and frequency parameters, with scalability and robustness, offering a pathway towards ultra-long-sequence language modeling without the computational bottleneck of self-attention.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>daDPO: Distribution-Aware DPO for Distilling Conversational Abilities</title>
<link>https://arxiv.org/abs/2506.15717</link>
<guid>https://arxiv.org/abs/2506.15717</guid>
<content:encoded><![CDATA[
arXiv:2506.15717v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated exceptional performance across various applications, but their conversational abilities decline sharply as model size decreases, presenting a barrier to their deployment in resource-constrained environments. Knowledge distillation with Direct Preference Optimization (dDPO) has emerged as a promising approach to enhancing the conversational abilities of smaller models using a larger teacher model. However, current methods primarily focus on 'black-box' KD, which only uses the teacher's responses, overlooking the output distribution offered by the teacher. This paper addresses this gap by introducing daDPO (Distribution-Aware DPO), a unified method for preference optimization and distribution-based distillation. We provide rigorous theoretical analysis and empirical validation, showing that daDPO outperforms existing methods in restoring performance for pruned models and enhancing smaller LLM models. Notably, in in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve near-teacher performance (-7.3% preference rate compared to that of dDPO's -31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model (14.0% win rate).
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference</title>
<link>https://arxiv.org/abs/2506.15724</link>
<guid>https://arxiv.org/abs/2506.15724</guid>
<content:encoded><![CDATA[
arXiv:2506.15724v1 Announce Type: cross 
Abstract: This paper introduces MadaKV, a modality-adaptive key-value (KV) cache eviction strategy designed to enhance the efficiency of multimodal large language models (MLLMs) in long-context inference. In multimodal scenarios, attention heads exhibit varying preferences for different modalities, resulting in significant disparities in modality importance across attention heads. Traditional KV cache eviction methods, which are tailored for unimodal settings, fail to capture modality-specific information, thereby yielding suboptimal performance. MadaKV addresses these challenges through two key components: modality preference adaptation and hierarchical compression compensation. By dynamically sensing modality information within attention heads and adaptively retaining critical tokens, MadaKV achieves substantial reductions in KV cache memory footprint and model inference decoding latency (1.3 to 1.5 times improvement) while maintaining high accuracy across various multimodal long-context tasks. Extensive experiments on representative MLLMs and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to existing KV cache eviction methods.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OAgents: An Empirical Study of Building Effective Agents</title>
<link>https://arxiv.org/abs/2506.15741</link>
<guid>https://arxiv.org/abs/2506.15741</guid>
<content:encoded><![CDATA[
arXiv:2506.15741v1 Announce Type: cross 
Abstract: Recently, Agentic AI has become an increasingly popular research field. However, we argue that current agent research practices lack standardization and scientific rigor, making it hard to conduct fair comparisons among methods. As a result, it is still unclear how different design choices in agent frameworks affect effectiveness, and measuring their progress remains challenging. In this work, we conduct a systematic empirical study on GAIA benchmark and BrowseComp to examine the impact of popular design choices in key agent components in a fair and rigorous manner. We find that the lack of a standard evaluation protocol makes previous works, even open-sourced ones, non-reproducible, with significant variance between random runs. Therefore, we introduce a more robust evaluation protocol to stabilize comparisons. Our study reveals which components and designs are crucial for effective agents, while others are redundant, despite seeming logical. Based on our findings, we build and open-source OAgents, a new foundation agent framework that achieves state-of-the-art performance among open-source projects. OAgents offers a modular design for various agent components, promoting future research in Agentic AI.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLR: An Automated Synthesis Framework for Scalable Logical Reasoning</title>
<link>https://arxiv.org/abs/2506.15787</link>
<guid>https://arxiv.org/abs/2506.15787</guid>
<content:encoded><![CDATA[
arXiv:2506.15787v1 Announce Type: cross 
Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given a user's task specification, SLR enables scalable, automated synthesis of inductive reasoning tasks with precisely controlled difficulty. For each task, SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation program used by a symbolic judge to deterministically verify model outputs, and (iii) an instruction prompt for the reasoning task. Using SLR, we create SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum levels that progressively increase in relational, arithmetic, and recursive complexity. Large-scale evaluation reveals that contemporary LLMs readily produce syntactically valid rules, yet often fail at correct logical inference. Recent reasoning LLMs do somewhat better, but incur substantial increases in test-time compute, sometimes exceeding 15k completion tokens. Finally, logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully automated, requires no human annotation, ensures dataset novelty, and offers a scalable environment for probing and advancing LLMs' reasoning capabilities.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers</title>
<link>https://arxiv.org/abs/2506.15862</link>
<guid>https://arxiv.org/abs/2506.15862</guid>
<content:encoded><![CDATA[
arXiv:2506.15862v1 Announce Type: cross 
Abstract: Retrieval-augmented Generation (RAG) is powerful, but its effectiveness hinges on which retrievers we use and how. Different retrievers offer distinct, often complementary signals: BM25 captures lexical matches; dense retrievers, semantic similarity. Yet in practice, we typically fix a single retriever based on heuristics, which fails to generalize across diverse information needs. Can we dynamically select and integrate multiple retrievers for each individual query, without the need for manual selection? In our work, we validate this intuition with quantitative analysis and introduce mixture of retrievers: a zero-shot, weighted combination of heterogeneous retrievers. Extensive experiments show that such mixtures are effective and efficient: Despite totaling just 0.8B parameters, this mixture outperforms every individual retriever and even larger 7B models by +10.8% and +3.9% on average, respectively. Further analysis also shows that this mixture framework can help incorporate specialized non-oracle human information sources as retrievers to achieve good collaboration, with a 58.9% relative performance improvement over simulated humans alone.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute</title>
<link>https://arxiv.org/abs/2506.15882</link>
<guid>https://arxiv.org/abs/2506.15882</guid>
<content:encoded><![CDATA[
arXiv:2506.15882v1 Announce Type: cross 
Abstract: Test-time compute has emerged as a powerful paradigm for improving the performance of large language models (LLMs), where generating multiple outputs or refining individual chains can significantly boost answer accuracy. However, existing methods like Best-of-N, majority voting, and self-reflection typically apply reasoning in a uniform way across inputs, overlooking the fact that different problems may require different levels of reasoning depth. In this work, we propose Fractional Reasoning, a training-free and model-agnostic framework that enables continuous control over reasoning intensity at inference time, going beyond the limitations of fixed instructional prompts. Our method operates by extracting the latent steering vector associated with deeper reasoning and reapplying it with a tunable scaling factor, allowing the model to tailor its reasoning process to the complexity of each input. This supports two key modes of test-time scaling: (1) improving output quality in breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing the correctness of individual reasoning chains in depth-based strategies (e.g., self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that Fractional Reasoning consistently improves performance across diverse reasoning tasks and models.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Attentive Sparsification Accelerates Neural Speech Transcription</title>
<link>https://arxiv.org/abs/2506.15912</link>
<guid>https://arxiv.org/abs/2506.15912</guid>
<content:encoded><![CDATA[
arXiv:2506.15912v1 Announce Type: cross 
Abstract: Transformer-based neural speech processing has achieved state-of-the-art performance. Since speech audio signals are known to be highly compressible, here we seek to accelerate neural speech transcription by time-domain signal sparsification early in the neural encoding stage, taking advantage of the interpretability of the self-attention mechanism in transformer audio encoders. With the Whisper family of models, we perform a systematic architecture search over the joint space of sparsification stage (a certain encoder layer) and compression ratio (sparsity). We found that the best resulting solutions under 1% accuracy degradation choose to sparsify the hidden state to 40-60% sparsity at an early encoding stage, and thereby achieve up to 1.6x runtime acceleration in English speech transcription tasks on Nvidia GPUs without any fine-tuning.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues</title>
<link>https://arxiv.org/abs/2506.15928</link>
<guid>https://arxiv.org/abs/2506.15928</guid>
<content:encoded><![CDATA[
arXiv:2506.15928v1 Announce Type: cross 
Abstract: This paper presents an evaluation framework for agentic AI systems in mission-critical negotiation contexts, addressing the need for AI agents that can adapt to diverse human operators and stakeholders. Using Sotopia as a simulation testbed, we present two experiments that systematically evaluated how personality traits and AI agent characteristics influence LLM-simulated social negotiation outcomes--a capability essential for a variety of applications involving cross-team coordination and civil-military interactions. Experiment 1 employs causal discovery methods to measure how personality traits impact price bargaining negotiations, through which we found that Agreeableness and Extraversion significantly affect believability, goal achievement, and knowledge acquisition outcomes. Sociocognitive lexical measures extracted from team communications detected fine-grained differences in agents' empathic communication, moral foundations, and opinion patterns, providing actionable insights for agentic AI systems that must operate reliably in high-stakes operational scenarios. Experiment 2 evaluates human-AI job negotiations by manipulating both simulated human personality and AI system characteristics, specifically transparency, competence, adaptability, demonstrating how AI agent trustworthiness impact mission effectiveness. These findings establish a repeatable evaluation methodology for experimenting with AI agent reliability across diverse operator personalities and human-agent team dynamics, directly supporting operational requirements for reliable AI systems. Our work advances the evaluation of agentic AI workflows by moving beyond standard performance metrics to incorporate social dynamics essential for mission success in complex operations.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-use LLM Watermarking and the False Detection Problem</title>
<link>https://arxiv.org/abs/2506.15975</link>
<guid>https://arxiv.org/abs/2506.15975</guid>
<content:encoded><![CDATA[
arXiv:2506.15975v1 Announce Type: cross 
Abstract: Digital watermarking is a promising solution for mitigating some of the risks arising from the misuse of automatically generated text. These approaches either embed non-specific watermarks to allow for the detection of any text generated by a particular sampler, or embed specific keys that allow the identification of the LLM user. However, simultaneously using the same embedding for both detection and user identification leads to a false detection problem, whereby, as user capacity grows, unwatermarked text is increasingly likely to be falsely detected as watermarked. Through theoretical analysis, we identify the underlying causes of this phenomenon. Building on these insights, we propose Dual Watermarking which jointly encodes detection and identification watermarks into generated text, significantly reducing false positives while maintaining high detection accuracy. Our experimental results validate our theoretical findings and demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning</title>
<link>https://arxiv.org/abs/2506.16015</link>
<guid>https://arxiv.org/abs/2506.16015</guid>
<content:encoded><![CDATA[
arXiv:2506.16015v1 Announce Type: cross 
Abstract: The exponential expansion of scientific literature has surpassed the epistemic processing capabilities of both human experts and current artificial intelligence systems. This paper introduces Bayesian Epistemology with Weighted Authority (BEWA), a formally structured architecture that operationalises belief as a dynamic, probabilistically coherent function over structured scientific claims. Each claim is contextualised, author-attributed, and evaluated through a system of replication scores, citation weighting, and temporal decay. Belief updates are performed via evidence-conditioned Bayesian inference, contradiction processing, and epistemic decay mechanisms. The architecture supports graph-based claim propagation, authorial credibility modelling, cryptographic anchoring, and zero-knowledge audit verification. By formalising scientific reasoning into a computationally verifiable epistemic network, BEWA advances the foundation for machine reasoning systems that promote truth utility, rational belief convergence, and audit-resilient integrity across dynamic scientific domains.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing the Robustness of Large Language Models Safety to Latent Perturbations</title>
<link>https://arxiv.org/abs/2506.16078</link>
<guid>https://arxiv.org/abs/2506.16078</guid>
<content:encoded><![CDATA[
arXiv:2506.16078v1 Announce Type: cross 
Abstract: Safety alignment is a key requirement for building reliable Artificial General Intelligence. Despite significant advances in safety alignment, we observe that minor latent shifts can still trigger unsafe responses in aligned models. We argue that this stems from the shallow nature of existing alignment methods, which focus on surface-level refusal behaviors without sufficiently altering internal representations. Consequently, small shifts in hidden activations can re-trigger harmful behaviors embedded in the latent space. To explore the robustness of safety alignment to latent perturbations, we introduce a probing method that measures the Negative Log-Likelihood of the original response generated by the model. This probe quantifies local sensitivity in the latent space, serving as a diagnostic tool for identifying vulnerable directions. Based on this signal, we construct effective jailbreak trajectories, giving rise to the Activation Steering Attack (ASA). More importantly, these insights offer a principled foundation for improving alignment robustness. To this end, we introduce Layer-wise Adversarial Patch Training~(LAPT), a fine-tuning strategy that inject controlled perturbations into hidden representations during training. Experimental results highlight that LAPT strengthen alignment robustness without compromising general capabilities. Our findings reveal fundamental flaws in current alignment paradigms and call for representation-level training strategies that move beyond surface-level behavior supervision. Codes and results are available at https://github.com/Carol-gutianle/LatentSafety.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2506.16141</link>
<guid>https://arxiv.org/abs/2506.16141</guid>
<content:encoded><![CDATA[
arXiv:2506.16141v1 Announce Type: cross 
Abstract: Recent reinforcement learning approaches, such as outcome-supervised GRPO, have advanced Chain-of-Thought reasoning in large language models (LLMs), yet their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack of rigorous evaluation for MLLM post-training methods, we introduce SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced perception and reasoning. It offers a large training set and evaluates generalization across three escalating challenges: in-distribution, cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1, we find that standard GRPO, while improving answer accuracy, often reduces logical coherence between reasoning steps and answers, with only a 57.9% consistency rate. This stems from reward signals focusing solely on final answers, encouraging shortcuts, and strict KL penalties limiting exploration.To address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing both answer correctness and reasoning coherence without explicit supervision. GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer correctness, and (2) an adaptive consistency bonus, computed by comparing the model's reasoning-to-answer likelihood (via a slowly-evolving reference model) against group peers.This dual mechanism amplifies rewards for reasoning paths that are both correct and logically consistent. Replacing KL penalties with this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1, achieving a 6.7% performance gain on the hardest evaluation level and a 24.5% improvement in consistency. It also shows strong transferability, improving model performance across diverse video understanding benchmarks. Our work contributes a systematically designed benchmark and a generalizable post-training framework, advancing the development of more interpretable and robust MLLMs.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks</title>
<link>https://arxiv.org/abs/2506.16402</link>
<guid>https://arxiv.org/abs/2506.16402</guid>
<content:encoded><![CDATA[
arXiv:2506.16402v1 Announce Type: cross 
Abstract: Flawed planning from VLM-driven embodied agents poses significant safety hazards, hindering their deployment in real-world household tasks. However, existing static, non-interactive evaluation paradigms fail to adequately assess risks within these interactive environments, since they cannot simulate dynamic risks that emerge from an agent's actions and rely on unreliable post-hoc evaluations that ignore unsafe intermediate steps. To bridge this critical gap, we propose evaluating an agent's interactive safety: its ability to perceive emergent risks and execute mitigation steps in the correct procedural order. We thus present IS-Bench, the first multi-modal benchmark designed for interactive safety, featuring 161 challenging scenarios with 388 unique safety risks instantiated in a high-fidelity simulator. Crucially, it facilitates a novel process-oriented evaluation that verifies whether risk mitigation actions are performed before/after specific risk-prone steps. Extensive experiments on leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current agents lack interactive safety awareness, and that while safety-aware Chain-of-Thought can improve performance, it often compromises task completion. By highlighting these critical limitations, IS-Bench provides a foundation for developing safer and more reliable embodied AI systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unpacking Generative AI in Education: Computational Modeling of Teacher and Student Perspectives in Social Media Discourse</title>
<link>https://arxiv.org/abs/2506.16412</link>
<guid>https://arxiv.org/abs/2506.16412</guid>
<content:encoded><![CDATA[
arXiv:2506.16412v1 Announce Type: cross 
Abstract: Generative AI (GAI) technologies are quickly reshaping the educational landscape. As adoption accelerates, understanding how students and educators perceive these tools is essential. This study presents one of the most comprehensive analyses to date of stakeholder discourse dynamics on GAI in education using social media data. Our dataset includes 1,199 Reddit posts and 13,959 corresponding top-level comments. We apply sentiment analysis, topic modeling, and author classification. To support this, we propose and validate a modular framework that leverages prompt-based large language models (LLMs) for analysis of online social discourse, and we evaluate this framework against classical natural language processing (NLP) models. Our GPT-4o pipeline consistently outperforms prior approaches across all tasks. For example, it achieved 90.6% accuracy in sentiment analysis against gold-standard human annotations. Topic extraction uncovered 12 latent topics in the public discourse with varying sentiment and author distributions. Teachers and students convey optimism about GAI's potential for personalized learning and productivity in higher education. However, key differences emerged: students often voice distress over false accusations of cheating by AI detectors, while teachers generally express concern about job security, academic integrity, and institutional pressures to adopt GAI tools. These contrasting perspectives highlight the tension between innovation and oversight in GAI-enabled learning environments. Our findings suggest a need for clearer institutional policies, more transparent GAI integration practices, and support mechanisms for both educators and students. More broadly, this study demonstrates the potential of LLM-based frameworks for modeling stakeholder discourse within online communities.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models</title>
<link>https://arxiv.org/abs/2506.16447</link>
<guid>https://arxiv.org/abs/2506.16447</guid>
<content:encoded><![CDATA[
arXiv:2506.16447v1 Announce Type: cross 
Abstract: Backdoor unalignment attacks against Large Language Models (LLMs) enable the stealthy compromise of safety alignment using a hidden trigger while evading normal safety auditing. These attacks pose significant threats to the applications of LLMs in the real-world Large Language Model as a Service (LLMaaS) setting, where the deployed model is a fully black-box system that can only interact through text. Furthermore, the sample-dependent nature of the attack target exacerbates the threat. Instead of outputting a fixed label, the backdoored LLM follows the semantics of any malicious command with the hidden trigger, significantly expanding the target space. In this paper, we introduce BEAT, a black-box defense that detects triggered samples during inference to deactivate the backdoor. It is motivated by an intriguing observation (dubbed the probe concatenate effect), where concatenated triggered samples significantly reduce the refusal rate of the backdoored LLM towards a malicious probe, while non-triggered samples have little effect. Specifically, BEAT identifies whether an input is triggered by measuring the degree of distortion in the output distribution of the probe before and after concatenation with the input. Our method addresses the challenges of sample-dependent targets from an opposite perspective. It captures the impact of the trigger on the refusal signal (which is sample-independent) instead of sample-specific successful attack behaviors. It overcomes black-box access limitations by using multiple sampling to approximate the output distribution. Extensive experiments are conducted on various backdoor attacks and LLMs (including the closed-source GPT-3.5-turbo), verifying the effectiveness and efficiency of our defense. Besides, we also preliminarily verify that BEAT can effectively defend against popular jailbreak attacks, as they can be regarded as 'natural backdoors'.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support</title>
<link>https://arxiv.org/abs/2506.16473</link>
<guid>https://arxiv.org/abs/2506.16473</guid>
<content:encoded><![CDATA[
arXiv:2506.16473v1 Announce Type: cross 
Abstract: As conversational agents increasingly engage in emotionally supportive dialogue, it is important to understand how closely their interactions resemble those in traditional therapy settings. This study investigates whether the concerns shared with a robot align with those shared in human-to-human (H2H) therapy sessions, and whether robot responses semantically mirror those of human therapists. We analyzed two datasets: one of interactions between users and professional therapists (Hugging Face's NLP Mental Health Conversations), and another involving supportive conversations with a social robot (QTrobot from LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence embeddings and K-means clustering, we assessed cross-agent thematic alignment by applying a distance-based cluster-fitting method that evaluates whether responses from one agent type map to clusters derived from the other, and validated it using Euclidean distances. Results showed that 90.88% of robot conversation disclosures could be mapped to clusters from the human therapy dataset, suggesting shared topical structure. For matched clusters, we compared the subjects as well as therapist and robot responses using Transformer, Word2Vec, and BERT embeddings, revealing strong semantic overlap in subjects' disclosures in both datasets, as well as in the responses given to similar human disclosure themes across agent types (robot vs. human therapist). These findings highlight both the parallels and boundaries of robot-led support conversations and their potential for augmenting mental health interventions.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revela: Dense Retriever Learning via Language Modeling</title>
<link>https://arxiv.org/abs/2506.16552</link>
<guid>https://arxiv.org/abs/2506.16552</guid>
<content:encoded><![CDATA[
arXiv:2506.16552v1 Announce Type: cross 
Abstract: Dense retrievers play a vital role in accessing external and specialized knowledge to augment language models (LMs). Training dense retrievers typically requires annotated query-document pairs, which are costly and hard to obtain in specialized domains such as code-motivating growing interest in self-supervised retriever learning. Since LMs are trained to capture token-level dependencies through a self-supervised learning objective (i.e., next-token prediction), we can analogously cast retrieval as learning dependencies among chunks of tokens. This analogy naturally leads to the question: How can we adapt self-supervised learning objectives in the spirit of language modeling to train retrievers?
  To answer this question, we introduce Revela, a unified and scalable training framework for self-supervised retriever learning via language modeling. Revela models semantic dependencies among documents by conditioning next-token prediction on both local and cross-document context through an in-batch attention mechanism. This attention is weighted by retriever-computed similarity scores, enabling the retriever to be optimized as part of language modeling. We evaluate Revela on both general-domain (BEIR) and domain-specific (CoIR) benchmarks across various retriever backbones. At a comparable parameter scale, Revela outperforms the previous best method with absolute improvements of 5.2 % (18.3 % relative) and 5.6 % (14.4 % relative) on NDCG@10, respectively, underscoring its effectiveness. Performance increases with model size, highlighting both the scalability of our approach and its promise for self-supervised retriever learning.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System</title>
<link>https://arxiv.org/abs/2506.16575</link>
<guid>https://arxiv.org/abs/2506.16575</guid>
<content:encoded><![CDATA[
arXiv:2506.16575v1 Announce Type: cross 
Abstract: Large language models (LLMs) offer promising opportunities for organizational research. However, their built-in moderation systems can create problems when researchers try to analyze harmful content, often refusing to follow certain instructions or producing overly cautious responses that undermine validity of the results. This is particularly problematic when analyzing organizational conflicts such as microaggressions or hate speech. This paper introduces an Elo rating-based method that significantly improves LLM performance for harmful content analysis In two datasets, one focused on microaggression detection and the other on hate speech, we find that our method outperforms traditional LLM prompting techniques and conventional machine learning models on key measures such as accuracy, precision, and F1 scores. Advantages include better reliability when analyzing harmful content, fewer false positives, and greater scalability for large-scale datasets. This approach supports organizational applications, including detecting workplace harassment, assessing toxic communication, and fostering safer and more inclusive work environments.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology</title>
<link>https://arxiv.org/abs/2506.16697</link>
<guid>https://arxiv.org/abs/2506.16697</guid>
<content:encoded><![CDATA[
arXiv:2506.16697v1 Announce Type: cross 
Abstract: Large language models (LLMs) are rapidly being adopted across psychology, serving as research tools, experimental subjects, human simulators, and computational models of cognition. However, the application of human measurement tools to these systems can produce contradictory results, raising concerns that many findings are measurement phantoms--statistical artifacts rather than genuine psychological phenomena. In this Perspective, we argue that building a robust science of AI psychology requires integrating two of our field's foundational pillars: the principles of reliable measurement and the standards for sound causal inference. We present a dual-validity framework to guide this integration, which clarifies how the evidence needed to support a claim scales with its scientific ambition. Using an LLM to classify text may require only basic accuracy checks, whereas claiming it can simulate anxiety demands a far more rigorous validation process. Current practice systematically fails to meet these requirements, often treating statistical pattern matching as evidence of psychological phenomena. The same model output--endorsing "I am anxious"--requires different validation strategies depending on whether researchers claim to measure, characterize, simulate, or model psychological constructs. Moving forward requires developing computational analogues of psychological constructs and establishing clear, scalable standards of evidence rather than the uncritical application of human measurement tools.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models as Psychological Simulators: A Methodological Guide</title>
<link>https://arxiv.org/abs/2506.16702</link>
<guid>https://arxiv.org/abs/2506.16702</guid>
<content:encoded><![CDATA[
arXiv:2506.16702v1 Announce Type: cross 
Abstract: Large language models (LLMs) offer emerging opportunities for psychological and behavioral research, but methodological guidance is lacking. This article provides a framework for using LLMs as psychological simulators across two primary applications: simulating roles and personas to explore diverse contexts, and serving as computational models to investigate cognitive processes. For simulation, we present methods for developing psychologically grounded personas that move beyond demographic categories, with strategies for validation against human data and use cases ranging from studying inaccessible populations to prototyping research instruments. For cognitive modeling, we synthesize emerging approaches for probing internal representations, methodological advances in causal interventions, and strategies for relating model behavior to human cognition. We address overarching challenges including prompt sensitivity, temporal limitations from training data cutoffs, and ethical considerations that extend beyond traditional human subjects review. Throughout, we emphasize the need for transparency about model capabilities and constraints. Together, this framework integrates emerging empirical evidence about LLM performance--including systematic biases, cultural limitations, and prompt brittleness--to help researchers wrangle these challenges and leverage the unique capabilities of LLMs in psychological research.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs</title>
<link>https://arxiv.org/abs/2506.16962</link>
<guid>https://arxiv.org/abs/2506.16962</guid>
<content:encoded><![CDATA[
arXiv:2506.16962v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing chain-of-thought (CoT) training data is essential for bolstering the reasoning abilities of medical MLLMs. However, existing approaches exhibit a deficiency in offering a comprehensive framework for searching and evaluating effective reasoning paths towards critical diagnosis. To address this challenge, we propose Mentor-Intern Collaborative Search (MICS), a novel reasoning-path searching scheme to generate rigorous and effective medical CoT data. MICS first leverages mentor models to initialize the reasoning, one step at a time, then prompts each intern model to continue the thinking along those initiated paths, and finally selects the optimal reasoning path according to the overall reasoning performance of multiple intern models. The reasoning performance is determined by an MICS-Score, which assesses the quality of generated reasoning paths. Eventually, we construct MMRP, a multi-task medical reasoning dataset with ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum learning strategy, with robust visual question-answering and generalizable reasoning capabilities. Extensive experiments demonstrate that Chiron-o1, trained on our CoT dataset constructed using MICS, achieves state-of-the-art performance across a list of medical visual question answering and reasoning benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Concept Disentanglement in Transformer-based Language Models</title>
<link>https://arxiv.org/abs/2506.16975</link>
<guid>https://arxiv.org/abs/2506.16975</guid>
<content:encoded><![CDATA[
arXiv:2506.16975v1 Announce Type: cross 
Abstract: When large language models (LLMs) use in-context learning (ICL) to solve a new task, they seem to grasp not only the goal of the task but also core, latent concepts in the demonstration examples. This begs the question of whether transformers represent latent structures as part of their computation or whether they take shortcuts to solve the problem. Prior mechanistic work on ICL does not address this question because it does not sufficiently examine the relationship between the learned representation and the latent concept, and the considered problem settings often involve only single-step reasoning. In this work, we examine how transformers disentangle and use latent concepts. We show that in 2-hop reasoning tasks with a latent, discrete concept, the model successfully identifies the latent concept and does step-by-step concept composition. In tasks parameterized by a continuous latent concept, we find low-dimensional subspaces in the representation space where the geometry mimics the underlying parameterization. Together, these results refine our understanding of ICL and the representation of transformers, and they provide evidence for highly localized structures in the model that disentangle latent concepts in ICL tasks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers</title>
<link>https://arxiv.org/abs/2506.17052</link>
<guid>https://arxiv.org/abs/2506.17052</guid>
<content:encoded><![CDATA[
arXiv:2506.17052v1 Announce Type: cross 
Abstract: Transformers have achieved state-of-the-art performance across language and vision tasks. This success drives the imperative to interpret their internal mechanisms with the dual goals of enhancing performance and improving behavioral control. Attribution methods help advance interpretability by assigning model outputs associated with a target concept to specific model components. Current attribution research primarily studies multi-layer perceptron neurons and addresses relatively simple concepts such as factual associations (e.g., Paris is located in France). This focus tends to overlook the impact of the attention mechanism and lacks a unified approach for analyzing more complex concepts. To fill these gaps, we introduce Scalable Attention Module Discovery (SAMD), a concept-agnostic method for mapping arbitrary, complex concepts to specific attention heads of general transformer models. We accomplish this by representing each concept as a vector, calculating its cosine similarity with each attention head, and selecting the TopK-scoring heads to construct the concept-associated attention module. We then propose Scalar Attention Module Intervention (SAMI), a simple strategy to diminish or amplify the effects of a concept by adjusting the attention module using only a single scalar parameter. Empirically, we demonstrate SAMD on concepts of varying complexity, and visualize the locations of their corresponding modules. Our results demonstrate that module locations remain stable before and after LLM post-training, and confirm prior work on the mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on HarmBench (+72.7%) by diminishing "safety" and improve performance on the GSM8K benchmark (+1.6%) by amplifying "reasoning". Lastly, we highlight the domain-agnostic nature of our approach by suppressing the image classification accuracy of vision transformers on ImageNet.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Bias Evaluation Methods Biased ?</title>
<link>https://arxiv.org/abs/2506.17111</link>
<guid>https://arxiv.org/abs/2506.17111</guid>
<content:encoded><![CDATA[
arXiv:2506.17111v1 Announce Type: cross 
Abstract: The creation of benchmarks to evaluate the safety of Large Language Models is one of the key activities within the trusted AI community. These benchmarks allow models to be compared for different aspects of safety such as toxicity, bias, harmful behavior etc. Independent benchmarks adopt different approaches with distinct data sets and evaluation methods. We investigate how robust such benchmarks are by using different approaches to rank a set of representative models for bias and compare how similar are the overall rankings. We show that different but widely used bias evaluations methods result in disparate model rankings. We conclude with recommendations for the community in the usage of such benchmarks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation</title>
<link>https://arxiv.org/abs/2506.17113</link>
<guid>https://arxiv.org/abs/2506.17113</guid>
<content:encoded><![CDATA[
arXiv:2506.17113v1 Announce Type: cross 
Abstract: Combining pre-trained expert models offers substantial potential for scalable multimodal reasoning, but building a unified framework remains challenging due to the increasing diversity of input modalities and task complexity. For instance, medical diagnosis requires precise reasoning over structured clinical tables, while financial forecasting depends on interpreting plot-based data to make informed predictions. To tackle this challenge, we introduce MEXA, a training-free framework that performs modality- and task-aware aggregation of multiple expert models to enable effective multimodal reasoning across diverse and distinct domains. MEXA dynamically selects expert models based on the input modality and the task-specific reasoning demands (i.e., skills). Each expert model, specialized in a modality task pair, generates interpretable textual reasoning outputs. MEXA then aggregates and reasons over these outputs using a Large Reasoning Model (LRM) to produce the final answer. This modular design allows flexible and transparent multimodal reasoning across diverse domains without additional training overhead. We extensively evaluate our approach on diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D Understanding, and Medical QA. MEXA consistently delivers performance improvements over strong multimodal baselines, highlighting the effectiveness and broad applicability of our expert-driven selection and aggregation in diverse multimodal reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems</title>
<link>https://arxiv.org/abs/2506.17208</link>
<guid>https://arxiv.org/abs/2506.17208</guid>
<content:encoded><![CDATA[
arXiv:2506.17208v1 Announce Type: cross 
Abstract: The rapid progress in Automated Program Repair (APR) has been driven by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair systems using real issues and pull requests mined from 12 popular open-source Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench Verified, have become central platforms for tracking progress and comparing solutions. However, because the submission process does not require detailed documentation, the architectural design and origin of many solutions remain unclear. In this paper, we present the first comprehensive study of all submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries) leaderboards, analyzing 67 unique approaches across dimensions such as submitter type, product availability, LLM usage, and system architecture. Our findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7), the presence of both agentic and non-agentic designs, and a contributor base spanning from individual developers to large tech companies.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voices of Her: Analyzing Gender Differences in the AI Publication World</title>
<link>https://arxiv.org/abs/2305.14597</link>
<guid>https://arxiv.org/abs/2305.14597</guid>
<content:encoded><![CDATA[
arXiv:2305.14597v2 Announce Type: replace 
Abstract: While several previous studies have analyzed gender bias in research, we are still missing a comprehensive analysis of gender differences in the AI community, covering diverse topics and different development trends. Using the AI Scholar dataset of 78K researchers in the field of AI, we identify several gender differences: (1) Although female researchers tend to have fewer overall citations than males, this citation difference does not hold for all academic-age groups; (2) There exist large gender homophily in co-authorship on AI papers; (3) Female first-authored papers show distinct linguistic styles, such as longer text, more positive emotion words, and more catchy titles than male first-authored papers. Our analysis provides a window into the current demographic trends in our AI community, and encourages more gender equality and diversity in the future. Our code and data are at https://github.com/causalNLP/ai-scholar-gender.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2312.04684</link>
<guid>https://arxiv.org/abs/2312.04684</guid>
<content:encoded><![CDATA[
arXiv:2312.04684v4 Announce Type: replace 
Abstract: Chain-of-thought (CoT) prompting is a popular in-context learning (ICL) approach for large language models (LLMs), especially when tackling complex reasoning tasks. Traditional ICL approaches construct prompts using examples that contain questions similar to the input question. However, CoT prompting, which includes crucial intermediate reasoning steps (rationales) within its examples, necessitates selecting examples based on these rationales rather than the questions themselves. Existing methods require human experts or pre-trained LLMs to describe the skill, a high-level abstraction of rationales, to guide the selection. These methods, however, are often costly and difficult to scale. Instead, this paper introduces a new approach named Latent Reasoning Skills (LaRS) that employs unsupervised learning to create a latent space representation of rationales, with a latent variable called a reasoning skill. Concurrently, LaRS learns a reasoning policy to determine the required reasoning skill for a given question. Then the ICL examples are selected by aligning the reasoning skills between past examples and the question. This approach is theoretically grounded and compute-efficient, eliminating the need for auxiliary LLM inference or manual prompt design. Empirical results demonstrate that LaRS consistently outperforms SOTA skill-based selection methods, processing example banks four times faster, reducing LLM inferences during the selection stage by half, and showing greater robustness to sub-optimal example banks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability</title>
<link>https://arxiv.org/abs/2402.09404</link>
<guid>https://arxiv.org/abs/2402.09404</guid>
<content:encoded><![CDATA[
arXiv:2402.09404v2 Announce Type: replace 
Abstract: This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol - for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves considering the possible environmental feedback in the future steps. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 14 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show much stronger sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing in-context examples may inadvertently hurt few-shot performance in an interactive environment due to over-fitting to examples. (3) Instead of using optimal steps from another test case as the in-context example, a very limited number of predecessor steps in the current test case following the optimal policy can substantially boost small models' performance. (4) The performance gap between weak models and strong models is greatly due to the incapability of weak models to start well. (5) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend. We hope our study can catalyze future work on advancing the understanding and enhancement of LLMs' capabilities in sequential reasoning. The code is available at https://github.com/UCSC-VLAA/AQA-Bench.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Automatic Hallucination Evaluation on Natural Language Generation</title>
<link>https://arxiv.org/abs/2404.12041</link>
<guid>https://arxiv.org/abs/2404.12041</guid>
<content:encoded><![CDATA[
arXiv:2404.12041v3 Announce Type: replace 
Abstract: The proliferation of Large Language Models (LLMs) has introduced a critical challenge: accurate hallucination evaluation that ensures model reliability. While Automatic Hallucination Evaluation (AHE) has emerged as essential, the field suffers from methodological fragmentation, hindering both theoretical understanding and practical advancement. This survey addresses this critical gap through a comprehensive analysis of 74 evaluation methods, revealing that 74% specifically target LLMs, a paradigm shift that demands new evaluation frameworks. We formulate a unified evaluation pipeline encompassing datasets and benchmarks, evidence collection strategies, and comparison mechanisms, systematically documenting the evolution from pre-LLM to post-LLM methodologies. Beyond taxonomical organization, we identify fundamental limitations in current approaches and their implications for real-world deployment. To guide future research, we delineate key challenges and propose strategic directions, including enhanced interpretability mechanisms and integration of application-specific evaluation criteria, ultimately providing a roadmap for developing more robust and practical hallucination evaluation systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEADs: Bias Evaluation Across Domains</title>
<link>https://arxiv.org/abs/2406.04220</link>
<guid>https://arxiv.org/abs/2406.04220</guid>
<content:encoded><![CDATA[
arXiv:2406.04220v5 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have significantly improved natural language processing (NLP) applications. However, these models often inherit biases from their training data. While several datasets exist for bias detection, most are limited to one or two NLP tasks, typically classification or evaluation, and lack comprehensive coverage across a broader range of tasks. To address this gap, we introduce the Bias Evaluations Across Domains (BEADs) dataset, designed to support a wide range of NLP tasks, including text classification, token classification, bias quantification, and benign language generation. A key contribution of this work is the gold-standard annotation provided by GPT-4 for scalability, with expert verification to ensure high reliability. BEADs can be used for both fine-tuning models (for classification and generation tasks) and evaluating LLM behavior. Our findings show that BEADs effectively surfaces various biases during model fine-tuning and helps reduce biases in language generation tasks while maintaining output quality. The dataset also highlights prevalent demographic biases in LLMs during evaluation. We release BEADs as a practical resource for detecting and mitigating bias across domains, supporting the development of responsible AI systems. Project: https://vectorinstitute.github.io/BEAD/ Data: https://huggingface.co/datasets/shainar/BEAD
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Refine with Fine-Grained Natural Language Feedback</title>
<link>https://arxiv.org/abs/2407.02397</link>
<guid>https://arxiv.org/abs/2407.02397</guid>
<content:encoded><![CDATA[
arXiv:2407.02397v3 Announce Type: replace 
Abstract: Recent work has explored the capability of large language models (LLMs) to identify and correct errors in LLM-generated responses. These refinement approaches frequently evaluate what sizes of models are able to do refinement for what problems, but less attention is paid to what effective feedback for refinement looks like. In this work, we propose looking at refinement with feedback as a composition of three distinct LLM competencies: (1) detection of bad generations; (2) fine-grained natural language critique generation; (3) refining with fine-grained feedback. The first step can be implemented with a high-performing discriminative model and steps 2 and 3 can be implemented either via prompted or fine-tuned LLMs. A key property of the proposed Detect, Critique, Refine ("DCR") method is that the step 2 critique model can give fine-grained feedback about errors, made possible by offloading the discrimination to a separate model in step 1. We show that models of different capabilities benefit from refining with DCR on the task of improving factual consistency of document grounded summaries. Overall, DCR consistently outperforms existing end-to-end refinement approaches and current trained models not fine-tuned for factuality critiquing.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot Guided Prompting</title>
<link>https://arxiv.org/abs/2407.09879</link>
<guid>https://arxiv.org/abs/2407.09879</guid>
<content:encoded><![CDATA[
arXiv:2407.09879v4 Announce Type: replace 
Abstract: Despite the remarkable success of large language models (LLMs) in English, a significant performance gap remains in non-English languages. To address this, we introduce a novel approach for strategically constructing a multilingual synthetic instruction tuning dataset, sPhinX. Unlike prior methods that directly translate fixed instruction-response pairs, sPhinX enhances diversity by selectively augmenting English instruction-response pairs with multilingual translations. Additionally, we propose LANGIT, a novel N-shot guided fine-tuning strategy, which further enhances model performance by incorporating contextually relevant examples in each training sample. Our ablation study shows that our approach enhances the multilingual capabilities of Mistral-7B and Phi-3-Small improving performance by an average of 39.8% and 11.2%, respectively, across multilingual benchmarks in reasoning, question answering, reading comprehension, and machine translation. Moreover, sPhinX maintains strong performance on English LLM benchmarks while exhibiting minimal to no catastrophic forgetting, even when trained on 51 languages.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextual modulation of language comprehension in a dynamic neural model of lexical meaning</title>
<link>https://arxiv.org/abs/2407.14701</link>
<guid>https://arxiv.org/abs/2407.14701</guid>
<content:encoded><![CDATA[
arXiv:2407.14701v2 Announce Type: replace 
Abstract: We propose and computationally implement a dynamic neural model of lexical meaning, and experimentally test its behavioral predictions. We demonstrate the architecture and behavior of the model using as a test case the English lexical item 'have', focusing on its polysemous use. In the model, 'have' maps to a semantic space defined by two continuous conceptual dimensions, connectedness and control asymmetry, previously proposed to parameterize the conceptual system for language. The mapping is modeled as coupling between a neural node representing the lexical item and neural fields representing the conceptual dimensions. While lexical knowledge is modeled as a stable coupling pattern, real-time lexical meaning retrieval is modeled as the motion of neural activation patterns between metastable states corresponding to semantic interpretations or readings. Model simulations capture two previously reported empirical observations: (1) contextual modulation of lexical semantic interpretation, and (2) individual variation in the magnitude of this modulation. Simulations also generate a novel prediction that the by-trial relationship between sentence reading time and acceptability should be contextually modulated. An experiment combining self-paced reading and acceptability judgments replicates previous results and confirms the new model prediction. Altogether, results support a novel perspective on lexical polysemy: that the many related meanings of a word are metastable neural activation states that arise from the nonlinear dynamics of neural populations governing interpretation on continuous semantic dimensions.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning based Visually Rich Document Content Understanding: A Survey</title>
<link>https://arxiv.org/abs/2408.01287</link>
<guid>https://arxiv.org/abs/2408.01287</guid>
<content:encoded><![CDATA[
arXiv:2408.01287v2 Announce Type: replace 
Abstract: Visually Rich Documents (VRDs) play a vital role in domains such as academia, finance, healthcare, and marketing, as they convey information through a combination of text, layout, and visual elements. Traditional approaches to extracting information from VRDs rely heavily on expert knowledge and manual annotation, making them labor-intensive and inefficient. Recent advances in deep learning have transformed this landscape by enabling multimodal models that integrate vision, language, and layout features through pretraining, significantly improving information extraction performance. This survey presents a comprehensive overview of deep learning-based frameworks for VRD Content Understanding (VRD-CU). We categorize existing methods based on their modeling strategies and downstream tasks, and provide a comparative analysis of key components, including feature representation, fusion techniques, model architectures, and pretraining objectives. Additionally, we highlight the strengths and limitations of each approach and discuss their suitability for different applications. The paper concludes with a discussion of current challenges and emerging trends, offering guidance for future research and practical deployment in real-world scenarios.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives</title>
<link>https://arxiv.org/abs/2408.06904</link>
<guid>https://arxiv.org/abs/2408.06904</guid>
<content:encoded><![CDATA[
arXiv:2408.06904v3 Announce Type: replace 
Abstract: The Chain-of-Thought (CoT) paradigm has become a pivotal method for solving complex problems with large language models (LLMs). However, its application to domain-specific tasks remains challenging, as LLMs often fail to decompose tasks accurately or execute subtasks effectively. This paper introduces the Re-TASK framework, a novel theoretical model that revisits LLM tasks from capability, skill, and knowledge perspectives, drawing on the principles of Bloom's Taxonomy and Knowledge Space Theory. While CoT provides a workflow-centric perspective on tasks, Re-TASK introduces a Chain-of-Learning (CoL) paradigm that highlights task dependencies on specific capability items, further broken down into their constituent knowledge and skill components. To address CoT failures, we propose a Re-TASK prompting strategy, which strengthens task-relevant capabilities through targeted knowledge injection and skill adaptation. Experiments across diverse domains demonstrate the effectiveness of Re-TASK. In particular, we achieve improvements of 45.00% on Yi-1.5-9B and 24.50% on Llama3-Chinese-8B for legal tasks. These results highlight the potential of Re-TASK to significantly enhance LLM performance and its applicability in specialized domains. We release our code and data at https://github.com/Uylee/Re-TASK.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Replace Human Subjects? A Large-Scale Replication of Scenario-Based Experiments in Psychology and Management</title>
<link>https://arxiv.org/abs/2409.00128</link>
<guid>https://arxiv.org/abs/2409.00128</guid>
<content:encoded><![CDATA[
arXiv:2409.00128v3 Announce Type: replace 
Abstract: Artificial Intelligence (AI) is increasingly being integrated into scientific research, particularly in the social sciences, where understanding human behavior is critical. Large Language Models (LLMs) have shown promise in replicating human-like responses in various psychological experiments. We conducted a large-scale study replicating 156 psychological experiments from top social science journals using three state-of-the-art LLMs (GPT-4, Claude 3.5 Sonnet, and DeepSeek v3). Our results reveal that while LLMs demonstrate high replication rates for main effects (73-81%) and moderate to strong success with interaction effects (46-63%), They consistently produce larger effect sizes than human studies, with Fisher Z values approximately 2-3 times higher than human studies. Notably, LLMs show significantly lower replication rates for studies involving socially sensitive topics such as race, gender and ethics. When original studies reported null findings, LLMs produced significant results at remarkably high rates (68-83%) - while this could reflect cleaner data with less noise, as evidenced by narrower confidence intervals, it also suggests potential risks of effect size overestimation. Our results demonstrate both the promise and challenges of LLMs in psychological research, offering efficient tools for pilot testing and rapid hypothesis validation while enriching rather than replacing traditional human subject studies, yet requiring more nuanced interpretation and human validation for complex social phenomena and culturally sensitive research questions.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Core Knowledge Deficits in Multi-Modal Language Models</title>
<link>https://arxiv.org/abs/2410.10855</link>
<guid>https://arxiv.org/abs/2410.10855</guid>
<content:encoded><![CDATA[
arXiv:2410.10855v4 Announce Type: replace 
Abstract: While Multi-modal Large Language Models (MLLMs) demonstrate impressive abilities over high-level perception and reasoning, their robustness in the wild remains limited, often falling short on tasks that are intuitive and effortless for humans. We examine the hypothesis that these deficiencies stem from the absence of core knowledge--rudimentary cognitive abilities innate to humans from early childhood. To explore the core knowledge representation in MLLMs, we introduce CoreCognition, a large-scale benchmark encompassing 12 core knowledge concepts grounded in developmental cognitive science. We evaluate 230 models with 11 different prompts, leading to a total of 2,530 data points for analysis. Our experiments uncover four key findings, collectively demonstrating core knowledge deficits in MLLMs: they consistently underperform and show reduced, or even absent, scalability on low-level abilities relative to high-level ones. Finally, we propose Concept Hacking, a novel controlled evaluation method that reveals MLLMs fail to progress toward genuine core knowledge understanding, but instead rely on shortcut learning as they scale.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments</title>
<link>https://arxiv.org/abs/2410.11331</link>
<guid>https://arxiv.org/abs/2410.11331</guid>
<content:encoded><![CDATA[
arXiv:2410.11331v2 Announce Type: replace 
Abstract: We introduce Shakti, a 2.5 billion parameter language model specifically optimized for resource-constrained environments such as edge devices, including smartphones, wearables, and IoT systems. Shakti combines high-performance NLP with optimized efficiency and precision, making it ideal for real-time AI applications where computational resources and memory are limited. With support for vernacular languages and domain-specific tasks, Shakti excels in industries such as healthcare, finance, and customer service. Benchmark evaluations demonstrate that Shakti performs competitively against larger models while maintaining low latency and on-device efficiency, positioning it as a leading solution for edge AI.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Route LLMs with Confidence Tokens</title>
<link>https://arxiv.org/abs/2410.13284</link>
<guid>https://arxiv.org/abs/2410.13284</guid>
<content:encoded><![CDATA[
arXiv:2410.13284v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated impressive performance on several tasks and are increasingly deployed in real-world applications. However, especially in high-stakes settings, it becomes vital to know when the output of an LLM may be unreliable. Depending on whether an answer is trustworthy, a system can then choose to route the question to another expert, or otherwise fall back on a safe default behavior. In this work, we study the extent to which LLMs can reliably indicate confidence in their answers, and how this notion of confidence can translate into downstream accuracy gains. We propose Self-Reflection with Error-based Feedback (Self-REF), a lightweight training strategy to teach LLMs to express confidence in whether their answers are correct in a reliable manner. Self-REF introduces confidence tokens into the LLM, from which a confidence score can be extracted. Compared to conventional approaches such as verbalizing confidence and examining token probabilities, we demonstrate empirically that confidence tokens show significant improvements in downstream routing and rejection learning tasks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principles of semantic and functional efficiency in grammatical patterning</title>
<link>https://arxiv.org/abs/2410.15865</link>
<guid>https://arxiv.org/abs/2410.15865</guid>
<content:encoded><![CDATA[
arXiv:2410.15865v2 Announce Type: replace 
Abstract: Grammatical features such as number and gender serve two central functions in human languages. While they encode salient semantic attributes like numerosity and animacy, they also offload sentence processing cost by predictably linking words together via grammatical agreement. Grammars exhibit consistent organizational patterns across diverse languages, invariably rooted in a semantic foundation-a widely confirmed but still theoretically unexplained phenomenon. To explain the basis of universal grammatical patterns, we unify two fundamental properties of grammar, semantic encoding and agreement-based predictability, into a single information-theoretic objective under cognitive constraints, accounting for variable communicative need. Our analyses reveal that grammatical organization provably inherits from perceptual attributes, and our measurements on a diverse language sample show that grammars prioritize functional goals, promoting efficient language processing over semantic encoding.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Layer-wise Alignment: Examining Safety Alignment Across Image Encoder Layers in Vision Language Models</title>
<link>https://arxiv.org/abs/2411.04291</link>
<guid>https://arxiv.org/abs/2411.04291</guid>
<content:encoded><![CDATA[
arXiv:2411.04291v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) have improved significantly in their capabilities, but their complex architecture makes their safety alignment challenging. In this paper, we reveal an uneven distribution of harmful information across the intermediate layers of the image encoder and show that skipping a certain set of layers and exiting early can increase the chance of the VLM generating harmful responses. We call it as "Image enCoder Early-exiT" based vulnerability (ICET). Our experiments across three VLMs: LLaVA-1.5, LLaVA-NeXT, and Llama 3.2, show that performing early exits from the image encoder significantly increases the likelihood of generating harmful outputs. To tackle this, we propose a simple yet effective modification of the Clipped-Proximal Policy Optimization (Clip-PPO) algorithm for performing layer-wise multi-modal RLHF for VLMs. We term this as Layer-Wise PPO (L-PPO). We evaluate our L-PPO algorithm across three multimodal datasets and show that it consistently reduces the harmfulness caused by early exits.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control</title>
<link>https://arxiv.org/abs/2411.13100</link>
<guid>https://arxiv.org/abs/2411.13100</guid>
<content:encoded><![CDATA[
arXiv:2411.13100v2 Announce Type: replace 
Abstract: Lyrics generation presents unique challenges, particularly in achieving precise syllable control while adhering to song form structures such as verses and choruses. Conventional line-by-line approaches often lead to unnatural phrasing, underscoring the need for more granular syllable management. We propose a framework for lyrics generation that enables multi-level syllable control at the word, phrase, line, and paragraph levels, aware of song form. Our approach generates complete lyrics conditioned on input text and song form, ensuring alignment with specified syllable constraints. Generated lyrics samples are available at: https://tinyurl.com/lyrics9999
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incivility and Rigidity: The Risks of Fine-Tuning LLMs for Political Argumentation</title>
<link>https://arxiv.org/abs/2411.16813</link>
<guid>https://arxiv.org/abs/2411.16813</guid>
<content:encoded><![CDATA[
arXiv:2411.16813v3 Announce Type: replace 
Abstract: The incivility prevalent on platforms like Twitter (now X) and Reddit poses a challenge for developing AI systems that can support productive and rhetorically sound political argumentation. In this study, we report experiments with GPT-3.5 Turbo, fine-tuned on two contrasting datasets of political discussions: high-variance, high-incivility Twitter replies to U.S. Congress, and low-variance, low-incivility posts from Reddit's r/ChangeMyView. We systematically evaluate how these data sources and prompting strategies shape the rhetorical framing and deliberative quality of model-generated arguments. Our results show that Reddit-finetuned models produce safer but rhetorically rigid arguments, while cross-platform fine-tuning amplifies toxicity. Prompting reduces specific toxic behaviors, such as personal attacks, but fails to fully mitigate the influence of high-incivility training data. We introduce and validate a rhetorical evaluation rubric and provide practical guidelines for deploying LLMs in content authoring, moderation, and deliberation support.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Domain-Adaptive Post-Training for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2411.19930</link>
<guid>https://arxiv.org/abs/2411.19930</guid>
<content:encoded><![CDATA[
arXiv:2411.19930v3 Announce Type: replace 
Abstract: Adapting general multimodal large language models (MLLMs) to specific domains, such as scientific and industrial fields, is highly significant in promoting their practical applications. This paper systematically investigates domain adaptation of MLLMs via post-training, focusing on data synthesis, training pipeline, and task evaluation. (1) Data Synthesis: Using only open-source models, we develop a generate-then-filter pipeline that curates diverse visual instruction tasks based on domain-specific image-caption pairs. The resulting data surpass the data synthesized by manual rules or strong closed-source models in enhancing domain-specific performance. (2) Training Pipeline: Unlike general MLLMs that typically adopt a two-stage training paradigm, we find that a single-stage approach is more effective for domain adaptation. (3) Task Evaluation: We conduct extensive experiments in high-impact domains such as biomedicine, food, and remote sensing, by post-training a variety of MLLMs and then evaluating MLLM performance on various domain-specific tasks. Finally, we fully open-source our models, code, and data to encourage future research in this area.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think&amp;Cite: Improving Attributed Text Generation with Self-Guided Tree Search and Progress Reward Modeling</title>
<link>https://arxiv.org/abs/2412.14860</link>
<guid>https://arxiv.org/abs/2412.14860</guid>
<content:encoded><![CDATA[
arXiv:2412.14860v2 Announce Type: replace 
Abstract: Despite their outstanding capabilities, large language models (LLMs) are prone to hallucination and producing factually incorrect information. This challenge has spurred efforts in attributed text generation, which prompts LLMs to generate content with supporting evidence. In this paper, we propose a novel framework, called Think&amp;Cite, and formulate attributed text generation as a multi-step reasoning problem integrated with search. Specifically, we propose Self-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the self-reflection capability of LLMs to reason about the intermediate states of MCTS for guiding the tree expansion process. To provide reliable and comprehensive feedback, we introduce Progress Reward Modeling to measure the progress of tree search from the root to the current state from two aspects, i.e., generation and attribution progress. We conduct extensive experiments on three datasets and the results show that our approach significantly outperforms baseline approaches.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Guarantees for Minimum Bayes Risk Decoding</title>
<link>https://arxiv.org/abs/2502.12685</link>
<guid>https://arxiv.org/abs/2502.12685</guid>
<content:encoded><![CDATA[
arXiv:2502.12685v3 Announce Type: replace 
Abstract: Minimum Bayes Risk (MBR) decoding optimizes output selection by maximizing the expected utility value of an underlying human distribution. While prior work has shown the effectiveness of MBR decoding through empirical evaluation, few studies have analytically investigated why the method is effective. As a result of our analysis, we show that, given the size $n$ of the reference hypothesis set used in computation, MBR decoding approaches the optimal solution with high probability at a rate of $O\left(n^{-\frac{1}{2}}\right)$, under certain assumptions, even though the language space $Y$ is significantly larger $|Y|\gg n$. This result helps to theoretically explain the strong performance observed in several prior empirical studies on MBR decoding. In addition, we provide the performance gap for maximum-a-posteriori (MAP) decoding and compare it to MBR decoding. The result of this paper indicates that MBR decoding tends to converge to the optimal solution faster than MAP decoding in several cases.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL Generation</title>
<link>https://arxiv.org/abs/2502.12911</link>
<guid>https://arxiv.org/abs/2502.12911</guid>
<content:encoded><![CDATA[
arXiv:2502.12911v2 Announce Type: replace 
Abstract: Generating SQLs from user queries is a long-standing challenge, where the accuracy of initial schema linking significantly impacts subsequent SQL generation performance. However, current schema linking models still struggle with missing relevant schema elements or an excess of redundant ones. A crucial reason for this is that commonly used metrics, recall and precision, fail to capture relevant element missing and thus cannot reflect actual schema linking performance. Motivated by this, we propose enhanced schema linking metrics by introducing a restricted missing indicator. Accordingly, we introduce Knapsack optimization-based Schema Linking Approach (KaSLA), a plug-in schema linking method designed to prevent the missing of relevant schema elements while minimizing the inclusion of redundant ones. KaSLA employs a hierarchical linking strategy that first identifies the optimal table linking and subsequently links columns within the selected table to reduce linking candidate space. In each linking process, it utilizes a knapsack optimization approach to link potentially relevant elements while accounting for a limited tolerance of potentially redundant ones. With this optimization, KaSLA-1.6B achieves superior schema linking results compared to large-scale LLMs, including deepseek-v3 with the state-of-the-art (SOTA) schema linking method. Extensive experiments on Spider and BIRD benchmarks verify that KaSLA can significantly improve the SQL generation performance of SOTA Text2SQL models by substituting their schema linking processes.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group-Level Data Selection for Efficient Pretraining</title>
<link>https://arxiv.org/abs/2502.14709</link>
<guid>https://arxiv.org/abs/2502.14709</guid>
<content:encoded><![CDATA[
arXiv:2502.14709v2 Announce Type: replace 
Abstract: In this paper, we introduce Group-MATES, an efficient group-level data selection approach to optimize the speed-quality frontier of language model pretraining. Specifically, Group-MATES parameterizes costly group-level selection with a relational data influence model. To train this model, we sample training trajectories of the language model and collect oracle data influences alongside. The relational data influence model approximates the oracle data influence by weighting individual influence with relationships among training data. To enable efficient selection with our relational data influence model, we partition the dataset into small clusters using relationship weights and select data within each cluster independently. Experiments on DCLM 400M-4x, 1B-1x, and 3B-1x show that Group-MATES achieves 3.5%-9.4% relative performance gains over random selection across 22 downstream tasks, nearly doubling the improvements achieved by state-of-the-art individual data selection baselines. Furthermore, Group-MATES reduces the number of tokens required to reach a certain downstream performance by up to 1.75x, substantially elevating the speed-quality frontier. Further analyses highlight the critical role of relationship weights in the relational data influence model and the effectiveness of our cluster-based inference. Our code is open-sourced at https://github.com/facebookresearch/Group-MATES.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From RAG to Memory: Non-Parametric Continual Learning for Large Language Models</title>
<link>https://arxiv.org/abs/2502.14802</link>
<guid>https://arxiv.org/abs/2502.14802</guid>
<content:encoded><![CDATA[
arXiv:2502.14802v2 Announce Type: replace 
Abstract: Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Batayan: A Filipino NLP benchmark for evaluating Large Language Models</title>
<link>https://arxiv.org/abs/2502.14911</link>
<guid>https://arxiv.org/abs/2502.14911</guid>
<content:encoded><![CDATA[
arXiv:2502.14911v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have demonstrated remarkable capabilities on widely benchmarked high-resource languages. However, linguistic nuances of under-resourced languages remain unexplored. We introduce Batayan, a holistic Filipino benchmark that systematically evaluates LLMs across three key natural language processing (NLP) competencies: understanding, reasoning, and generation. Batayan consolidates eight tasks, three of which have not existed prior for Filipino corpora, covering both Tagalog and code-switched Taglish utterances. Our rigorous, native-speaker-driven adaptation and validation processes ensures fluency and authenticity to the complex morphological and syntactic structures of Filipino, alleviating the pervasive translationese bias in existing Filipino corpora. We report empirical results on a variety of open-source and commercial LLMs, highlighting significant performance gaps that signal the under-representation of Filipino in pre-training corpora, the unique hurdles in modeling Filipino's rich morphology and construction, and the importance of explicit Filipino language support. Moreover, we discuss the practical challenges encountered in dataset construction and propose principled solutions for building culturally and linguistically-faithful resources in under-represented languages. We also provide a public evaluation suite as a clear foundation for iterative, community-driven progress in Filipino NLP.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification in Retrieval Augmented Question Answering</title>
<link>https://arxiv.org/abs/2502.18108</link>
<guid>https://arxiv.org/abs/2502.18108</guid>
<content:encoded><![CDATA[
arXiv:2502.18108v2 Announce Type: replace 
Abstract: Retrieval augmented Question Answering (QA) helps QA models overcome knowledge gaps by incorporating retrieved evidence, typically a set of passages, alongside the question at test time. Previous studies show that this approach improves QA performance and reduces hallucinations, without, however, assessing whether the retrieved passages are indeed useful at answering correctly. In this work, we propose to quantify the uncertainty of a QA model via estimating the utility of the passages it is provided with. We train a lightweight neural model to predict passage utility for a target QA model and show that while simple information theoretic metrics can predict answer correctness up to a certain extent, our approach efficiently approximates or outperforms more expensive sampling-based methods. Code and data are available at https://github.com/lauhaide/ragu.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models</title>
<link>https://arxiv.org/abs/2502.18443</link>
<guid>https://arxiv.org/abs/2502.18443</guid>
<content:encoded><![CDATA[
arXiv:2502.18443v2 Announce Type: replace 
Abstract: PDF documents have the potential to provide trillions of novel, high-quality tokens for training language models. However, these documents come in a diversity of types with differing formats and visual layouts that pose a challenge when attempting to extract and faithfully represent the underlying content for language model use. Traditional open source tools often produce lower quality extractions compared to vision language models (VLMs), but reliance on the best VLMs can be prohibitively costly (e.g., over $6,240 USD per million PDF pages for GPT-4o) or infeasible if the PDFs cannot be sent to proprietary APIs. We present olmOCR, an open-source toolkit for processing PDFs into clean, linearized plain text in natural reading order while preserving structured content like sections, tables, lists, equations, and more. Our toolkit runs a fine-tuned 7B vision language model (VLM) trained on olmOCR-mix-0225, a sample of 260,000 pages from over 100,000 crawled PDFs with diverse properties, including graphics, handwritten text and poor quality scans. olmOCR is optimized for large-scale batch processing, able to scale flexibly to different hardware setups and can convert a million PDF pages for only $176 USD. To aid comparison with existing systems, we also introduce olmOCR-Bench, a curated set of 1,400 PDFs capturing many content types that remain challenging even for the best tools and VLMs, including formulas, tables, tiny fonts, old scans, and more. We find olmOCR outperforms even top VLMs including GPT-4o, Gemini Flash 2 and Qwen-2.5-VL. We openly release all components of olmOCR: our fine-tuned VLM model, training code and data, an efficient inference pipeline that supports vLLM and SGLang backends, and benchmark olmOCR-Bench.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response</title>
<link>https://arxiv.org/abs/2502.18452</link>
<guid>https://arxiv.org/abs/2502.18452</guid>
<content:encoded><![CDATA[
arXiv:2502.18452v2 Announce Type: replace 
Abstract: During Human Robot Interactions in disaster relief scenarios, Large Language Models (LLMs) have the potential for substantial physical reasoning to assist in mission objectives. However, these capabilities are often found only in larger models, which are frequently not reasonable to deploy on robotic systems. To meet our problem space requirements, we introduce a dataset and pipeline to create Field Reasoning and Instruction Decoding Agent (FRIDA) models. In our pipeline, domain experts and linguists combine their knowledge to make high-quality few-shot prompts used to generate synthetic data for fine-tuning. We hand-curate datasets for this few-shot prompting and for evaluation to improve LLM reasoning on both general and disaster-specific objects. We concurrently run an ablation study to understand which kinds of synthetic data most affect performance. We fine-tune several small instruction-tuned models and find that ablated FRIDA models only trained on objects' physical state and function data outperformed both the FRIDA models trained on all synthetic data and the base models in our customized evaluation. We demonstrate that the FRIDA pipeline is capable of instilling physical common sense with minimal data.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Data Selection for Instruction Tuning</title>
<link>https://arxiv.org/abs/2503.01807</link>
<guid>https://arxiv.org/abs/2503.01807</guid>
<content:encoded><![CDATA[
arXiv:2503.01807v2 Announce Type: replace 
Abstract: Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typically tested by selecting small datasets (roughly 10k samples) from small pools (100-200k samples). However, popular deployed instruction-tuned models often train on hundreds of thousands to millions of samples, subsampled from even larger data pools. We present a systematic study of how well data selection methods scale to these settings, selecting up to 2.5M samples from pools of up to 5.8M samples and evaluating across 7 diverse tasks. We show that many recently proposed methods fall short of random selection in this setting (while using more compute), and even decline in performance when given access to larger pools of data to select over. However, we find that a variant of representation-based data selection (RDS+), which uses weighted mean pooling of pretrained LM hidden states, consistently outperforms more complex methods across all settings tested -- all whilst being more compute-efficient. Our findings highlight that the scaling properties of proposed automated selection methods should be more closely examined. We release our code, data, and models at https://github.com/hamishivi/automated-instruction-selection.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation</title>
<link>https://arxiv.org/abs/2503.02832</link>
<guid>https://arxiv.org/abs/2503.02832</guid>
<content:encoded><![CDATA[
arXiv:2503.02832v2 Announce Type: replace 
Abstract: In modern large language models (LLMs), LLM alignment is of crucial importance and is typically achieved through methods such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). However, in most existing methods for LLM alignment, all tokens in the response are optimized using a sparse, response-level reward or preference annotation. The ignorance of token-level rewards may erroneously punish high-quality tokens or encourage low-quality tokens, resulting in suboptimal performance and slow convergence speed. To address this issue, we propose AlignDistil, an RLHF-equivalent distillation method for token-level reward optimization. Specifically, we introduce the reward learned by DPO into the RLHF objective and theoretically prove the equivalence between this objective and a token-level distillation process, where the teacher distribution linearly combines the logits from the DPO model and a reference model. On this basis, we further bridge the accuracy gap between the reward from the DPO model and the pure reward model, by building a contrastive DPO reward with a normal and a reverse DPO model. Moreover, to avoid under- and over-optimization on different tokens, we design a token adaptive logit extrapolation mechanism to construct an appropriate teacher distribution for each token. Experimental results demonstrate the superiority of our AlignDistil over existing methods and showcase fast convergence due to its token-level distributional reward optimization.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coreference as an indicator of context scope in multimodal narrative</title>
<link>https://arxiv.org/abs/2503.05298</link>
<guid>https://arxiv.org/abs/2503.05298</guid>
<content:encoded><![CDATA[
arXiv:2503.05298v2 Announce Type: replace 
Abstract: We demonstrate that large multimodal language models differ substantially from humans in the distribution of coreferential expressions in a visual storytelling task. We introduce a number of metrics to quantify the characteristics of coreferential patterns in both human- and machine-written texts. Humans distribute coreferential expressions in a way that maintains consistency across texts and images, interleaving references to different entities in a highly varied way. Machines are less able to track mixed references, despite achieving perceived improvements in generation quality. Materials, metrics, and code for our study are available at https://github.com/GU-CLASP/coreference-context-scope.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models</title>
<link>https://arxiv.org/abs/2503.05328</link>
<guid>https://arxiv.org/abs/2503.05328</guid>
<content:encoded><![CDATA[
arXiv:2503.05328v2 Announce Type: replace 
Abstract: This paper investigates the role of dynamic external knowledge integration in improving counter-argument generation using Large Language Models (LLMs). While LLMs have shown promise in argumentative tasks, their tendency to generate lengthy, potentially unfactual responses highlights the need for more controlled and evidence-based approaches. We introduce a new manually curated dataset of argument and counter-argument pairs specifically designed to balance argumentative complexity with evaluative feasibility. We also propose a new LLM-as-a-Judge evaluation methodology that shows a stronger correlation with human judgments compared to traditional reference-based metrics. Our experimental results demonstrate that integrating dynamic external knowledge from the web significantly improves the quality of generated counter-arguments, particularly in terms of relatedness, persuasiveness, and factuality. The findings suggest that combining LLMs with real-time external knowledge retrieval offers a promising direction for developing more effective and reliable counter-argumentation systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation</title>
<link>https://arxiv.org/abs/2503.05888</link>
<guid>https://arxiv.org/abs/2503.05888</guid>
<content:encoded><![CDATA[
arXiv:2503.05888v2 Announce Type: replace 
Abstract: While the Question Generation (QG) task has been increasingly adopted in educational assessments, its evaluation remains limited by approaches that lack a clear connection to the educational values of test items. In this work, we introduce test item analysis, a method frequently used by educators to assess test question quality, into QG evaluation. Specifically, we construct pairs of candidate questions that differ in quality across dimensions such as topic coverage, item difficulty, item discrimination, and distractor efficiency. We then examine whether existing QG evaluation approaches can effectively distinguish these differences. Our findings reveal significant shortcomings in these approaches with respect to accurately assessing test item quality in relation to student performance. To address this gap, we propose a novel QG evaluation framework, QG-SMS, which leverages Large Language Model for Student Modeling and Simulation to perform test item analysis. As demonstrated in our extensive experiments and human evaluation study, the additional perspectives introduced by the simulated student profiles lead to a more effective and robust assessment of test items.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3 Mini Across Chronic Health Conditions</title>
<link>https://arxiv.org/abs/2503.10486</link>
<guid>https://arxiv.org/abs/2503.10486</guid>
<content:encoded><![CDATA[
arXiv:2503.10486v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are revolutionizing medical diagnostics by enhancing both disease classification and clinical decision-making. In this study, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek R1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We assessed their predictive accuracy at both the disease and category levels, as well as the reliability of their confidence scores. DeepSeek R1 achieved a disease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3 Mini, which attained 72% and 75% respectively. Notably, DeepSeek R1 demonstrated exceptional performance in Mental Health, Neurological Disorders, and Oncology, where it reached 100% accuracy, while O3 Mini excelled in Autoimmune Disease classification with 100% accuracy. Both models, however, struggled with Respiratory Disease classification, recording accuracies of only 40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of confidence scores revealed that DeepSeek R1 provided high-confidence predictions in 92% of cases, compared to 68% for O3 Mini. Ethical considerations regarding bias, model interpretability, and data privacy are also discussed to ensure the responsible integration of LLMs into clinical practice. Overall, our findings offer valuable insights into the strengths and limitations of LLM-based diagnostic systems and provide a roadmap for future enhancements in AI-driven healthcare.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Dimensional Interlingual Representations of Large Language Models</title>
<link>https://arxiv.org/abs/2503.11280</link>
<guid>https://arxiv.org/abs/2503.11280</guid>
<content:encoded><![CDATA[
arXiv:2503.11280v4 Announce Type: replace 
Abstract: Large language models (LLMs) trained on massive multilingual datasets hint at the formation of interlingual constructs--a shared subspace in the representation space. However, evidence regarding this phenomenon is mixed, leaving it unclear whether these models truly develop unified interlingual representations, or present a partially aligned constructs. We explore 31 diverse languages varying on their resource-levels, typologies, and geographical regions; and find that multilingual LLMs exhibit inconsistent cross-lingual alignments. To address this, we propose an interlingual representation framework identifying both the shared interlingual semantic subspace and fragmented components, existed due to representational limitations. We introduce Interlingual Local Overlap (ILO) score to quantify interlingual alignment by comparing the local neighborhood structures of high-dimensional representations. We utilize ILO to investigate the impact of single-language fine-tuning on the interlingual representations in multilingual LLMs. Our results indicate that training exclusively on a single language disrupts the alignment in early layers, while freezing these layers preserves the alignment of interlingual representations, leading to improved cross-lingual generalization. These results validate our framework and metric for evaluating interlingual representation, and further underscore that interlingual alignment is crucial for scalable multilingual learning.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content</title>
<link>https://arxiv.org/abs/2503.16031</link>
<guid>https://arxiv.org/abs/2503.16031</guid>
<content:encoded><![CDATA[
arXiv:2503.16031v3 Announce Type: replace 
Abstract: In the evolving landscape of online discourse, misinformation increasingly adopts humorous tones to evade detection and gain traction. This work introduces Deceptive Humor as a novel research direction, emphasizing how false narratives, when coated in humor, can become more difficult to detect and more likely to spread. To support research in this space, we present the Deceptive Humor Dataset (DHD) a collection of humor-infused comments derived from fabricated claims using the ChatGPT-4o model. Each entry is labeled with a Satire Level (from 1 for subtle satire to 3 for overt satire) and categorized into five humor types: Dark Humor, Irony, Social Commentary, Wordplay, and Absurdity. The dataset spans English, Telugu, Hindi, Kannada, Tamil, and their code-mixed forms, making it a valuable resource for multilingual analysis. DHD offers a structured foundation for understanding how humor can serve as a vehicle for the propagation of misinformation, subtly enhancing its reach and impact. Strong baselines are established to encourage further research and model development in this emerging area.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARE: Assessing the Impact of Multilingual Human Preference Learning on Cultural Awareness</title>
<link>https://arxiv.org/abs/2504.05154</link>
<guid>https://arxiv.org/abs/2504.05154</guid>
<content:encoded><![CDATA[
arXiv:2504.05154v4 Announce Type: replace 
Abstract: Language Models (LMs) are typically tuned with human preferences to produce helpful responses, but the impact of preference tuning on the ability to handle culturally diverse queries remains understudied. In this paper, we systematically analyze how native human cultural preferences can be incorporated into the preference learning process to train more culturally aware LMs. We introduce CARE, a multilingual resource containing 3,490 culturally specific questions and 31.7k responses with native judgments. We demonstrate how a modest amount of high-quality native preferences improves cultural awareness across various LMs, outperforming larger generic preference data. Our analyses reveal that models with stronger initial cultural performance benefit more from alignment, leading to gaps among models developed in different regions with varying access to culturally relevant data. CARE will be made publicly available at https://github.com/Guochry/CARE.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2504.07385</link>
<guid>https://arxiv.org/abs/2504.07385</guid>
<content:encoded><![CDATA[
arXiv:2504.07385v2 Announce Type: replace 
Abstract: As Large Language Models (LLMs) become increasingly integrated into real-world, autonomous applications, relying on static, pre-annotated references for evaluation poses significant challenges in cost, scalability, and completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework to assess LLM outputs without predetermined ground-truth answers. Unlike conventional metrics that compare to fixed references or depend solely on LLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities that actively retrieves and synthesizes external evidence. It iteratively generates web queries, collects information, summarizes findings, and refines subsequent searches through reflection. By shifting away from static references, TALE aligns with free-form question-answering tasks common in real-world scenarios. Experimental results on multiple free-form QA benchmarks show that TALE not only outperforms standard reference-based metrics for measuring response accuracy but also achieves substantial to near-perfect agreement with human evaluations. TALE enhances the reliability of LLM evaluations in real-world, dynamic scenarios without relying on static references.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reimagining Urban Science: Scaling Causal Inference with Large Language Models</title>
<link>https://arxiv.org/abs/2504.12345</link>
<guid>https://arxiv.org/abs/2504.12345</guid>
<content:encoded><![CDATA[
arXiv:2504.12345v3 Announce Type: replace 
Abstract: Urban causal research is essential for understanding the complex, dynamic processes that shape cities and for informing evidence-based policies. However, current practices are often constrained by inefficient and biased hypothesis formulation, challenges in integrating multimodal data, and fragile experimental methodologies. Imagine a system that automatically estimates the causal impact of congestion pricing on commute times by income group or measures how new green spaces affect asthma rates across neighborhoods using satellite imagery and health reports, and then generates comprehensive, policy-ready outputs, including causal estimates, subgroup analyses, and actionable recommendations. In this Perspective, we propose UrbanCIA, an LLM-driven conceptual framework composed of four distinct modular agents responsible for hypothesis generation, data engineering, experiment design and execution, and results interpretation with policy insights. We begin by examining the current landscape of urban causal research through a structured taxonomy of research topics, data sources, and methodological approaches, revealing systemic limitations across the workflow. Next, we introduce the design principles and technological roadmap for the four modules in the proposed framework. We also propose evaluation criteria to assess the rigor and transparency of these AI-augmented processes. Finally, we reflect on the broader implications for human-AI collaboration, equity, and accountability. We call for a new research agenda that embraces LLM-driven tools as catalysts for more scalable, reproducible, and inclusive urban research.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs' Multi-turn Instruction-Following Ability</title>
<link>https://arxiv.org/abs/2504.21625</link>
<guid>https://arxiv.org/abs/2504.21625</guid>
<content:encoded><![CDATA[
arXiv:2504.21625v4 Announce Type: replace 
Abstract: The ability to follow instructions accurately is fundamental for Large Language Models (LLMs) to serve as reliable agents in real-world applications. For complex instructions, LLMs often struggle to fulfill all requirements in a single attempt. In practice, users typically provide iterative feedback until the LLM generates a response that meets all requirements. However, existing instruction-following benchmarks are either single-turn or introduce new requirements in each turn without allowing self-correction. To address this gap, we propose Meeseeks. Meeseeks simulates realistic human-LLM interactions through an iterative feedback framework, which enables models to self-correct based on specific requirement failures in each turn, better reflecting real-world user-end usage patterns. Meanwhile, the benchmark implements a comprehensive evaluation system with 38 capability tags organized across three dimensions: Intent Recognition, Granular Content Validation, and Output Structure Validation. Through rigorous evaluation across LLMs, Meeseeks provides valuable insights into LLMs' instruction-following capabilities in multi-turn scenarios.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization</title>
<link>https://arxiv.org/abs/2505.02819</link>
<guid>https://arxiv.org/abs/2505.02819</guid>
<content:encoded><![CDATA[
arXiv:2505.02819v3 Announce Type: replace 
Abstract: We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation, which approximates the pruned blocks. The estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Dynamics in Continual Pre-Training for Large Language Models</title>
<link>https://arxiv.org/abs/2505.07796</link>
<guid>https://arxiv.org/abs/2505.07796</guid>
<content:encoded><![CDATA[
arXiv:2505.07796v2 Announce Type: replace 
Abstract: Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models</title>
<link>https://arxiv.org/abs/2505.07968</link>
<guid>https://arxiv.org/abs/2505.07968</guid>
<content:encoded><![CDATA[
arXiv:2505.07968v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have great potential in the field of health care, yet they face great challenges in adapting to rapidly evolving medical knowledge. This can lead to outdated or contradictory treatment suggestions. This study investigated how LLMs respond to evolving clinical guidelines, focusing on concept drift and internal inconsistencies. We developed the DriftMedQA benchmark to simulate guideline evolution and assessed the temporal reliability of various LLMs. Our evaluation of seven state-of-the-art models across 4,290 scenarios demonstrated difficulties in rejecting outdated recommendations and frequently endorsing conflicting guidance. Additionally, we explored two mitigation strategies: Retrieval-Augmented Generation and preference fine-tuning via Direct Preference Optimization. While each method improved model performance, their combination led to the most consistent and reliable results. These findings underscore the need to improve LLM robustness to temporal shifts to ensure more dependable applications in clinical practice. The dataset is available at https://huggingface.co/datasets/RDBH/DriftMed.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Prefix Bias in LLM-based Reward Models</title>
<link>https://arxiv.org/abs/2505.13487</link>
<guid>https://arxiv.org/abs/2505.13487</guid>
<content:encoded><![CDATA[
arXiv:2505.13487v2 Announce Type: replace 
Abstract: Reinforcement Learning with Human Feedback (RLHF) has emerged as a key paradigm for task-specific fine-tuning of language models using human preference data. While numerous publicly available preference datasets provide pairwise comparisons of responses, the potential for biases in the resulting reward models remains underexplored. In this work, we introduce novel methods to detect and evaluate prefix bias -- a systematic shift in model preferences triggered by minor variations in query prefixes -- in LLM-based reward models trained on such datasets. We leverage these metrics to reveal significant biases in preference models across racial and gender dimensions. Our comprehensive evaluation spans diverse open-source preference datasets and reward model architectures, demonstrating susceptibility to this kind of bias regardless of the underlying model architecture. Furthermore, we propose a data augmentation strategy to mitigate these biases, showing its effectiveness in reducing the impact of prefix bias. Our findings highlight the critical need for bias-aware dataset design and evaluation in developing fair and reliable reward models, contributing to the broader discourse on fairness in AI.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation</title>
<link>https://arxiv.org/abs/2505.14015</link>
<guid>https://arxiv.org/abs/2505.14015</guid>
<content:encoded><![CDATA[
arXiv:2505.14015v2 Announce Type: replace 
Abstract: The rapid advancement of domain-specific large language models (LLMs) in fields like law necessitates frameworks that account for nuanced regional legal distinctions, which are critical for ensuring compliance and trustworthiness. Existing legal evaluation benchmarks often lack adaptability and fail to address diverse local contexts, limiting their utility in dynamically evolving regulatory landscapes. To address these gaps, we propose AutoLaw, a novel violation detection framework that combines adversarial data generation with a jury-inspired deliberation process to enhance legal compliance of LLMs. Unlike static approaches, AutoLaw dynamically synthesizes case law to reflect local regulations and employs a pool of LLM-based "jurors" to simulate judicial decision-making. Jurors are ranked and selected based on synthesized legal expertise, enabling a deliberation process that minimizes bias and improves detection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG (legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness: adversarial data generation improves LLM discrimination, while the jury-based voting strategy significantly boosts violation detection rates. Our results highlight the framework's ability to adaptively probe legal misalignments and deliver reliable, context-aware judgments, offering a scalable solution for evaluating and enhancing LLMs in legally sensitive applications.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation</title>
<link>https://arxiv.org/abs/2505.16637</link>
<guid>https://arxiv.org/abs/2505.16637</guid>
<content:encoded><![CDATA[
arXiv:2505.16637v3 Announce Type: replace 
Abstract: Large language models (LLMs) have recently demonstrated remarkable capabilities in machine translation (MT). However, most advanced MT-specific LLMs heavily rely on external supervision signals during training, such as human-annotated reference data or trained reward models (RMs), which are often expensive to obtain and challenging to scale. To overcome this limitation, we propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for MT that is reference-free, fully online, and relies solely on self-judging rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs, e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR with external supervision from COMET, our strongest model, SSR-X-Zero-7B, achieves state-of-the-art performance in English $\leftrightarrow$ Chinese translation, surpassing all existing open-source models under 72B parameters and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro. Our analysis highlights the effectiveness of the self-rewarding mechanism compared to the external LLM-as-a-judge approach in MT and demonstrates its complementary benefits when combined with trained RMs. Our findings provide valuable insight into the potential of self-improving RL methods. We have publicly released our code, data and models.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM</title>
<link>https://arxiv.org/abs/2505.18110</link>
<guid>https://arxiv.org/abs/2505.18110</guid>
<content:encoded><![CDATA[
arXiv:2505.18110v2 Announce Type: replace 
Abstract: Humans naturally understand moments in a video by integrating visual and auditory cues. For example, localizing a scene in the video like "A scientist passionately speaks on wildlife conservation as dramatic orchestral music plays, with the audience nodding and applauding" requires simultaneous processing of visual, audio, and speech signals. However, existing models often struggle to effectively fuse and interpret audio information, limiting their capacity for comprehensive video temporal understanding. To address this, we present TriSense, a triple-modality large language model designed for holistic video temporal understanding through the integration of visual, audio, and speech modalities. Central to TriSense is a Query-Based Connector that adaptively reweights modality contributions based on the input query, enabling robust performance under modality dropout and allowing flexible combinations of available inputs. To support TriSense's multimodal capabilities, we introduce TriSense-2M, a high-quality dataset of over 2 million curated samples generated via an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes long-form videos and diverse modality combinations, facilitating broad generalization. Extensive experiments across multiple benchmarks demonstrate the effectiveness of TriSense and its potential to advance multimodal video analysis. Code and dataset will be publicly released.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voice of a Continent: Mapping Africa's Speech Technology Frontier</title>
<link>https://arxiv.org/abs/2505.18436</link>
<guid>https://arxiv.org/abs/2505.18436</guid>
<content:encoded><![CDATA[
arXiv:2505.18436v2 Announce Type: replace 
Abstract: Africa's rich linguistic diversity remains significantly underrepresented in speech technologies, creating barriers to digital inclusion. To alleviate this challenge, we systematically map the continent's speech space of datasets and technologies, leading to a new comprehensive benchmark SimbaBench for downstream African speech tasks. Using SimbaBench, we introduce the Simba family of models, achieving state-of-the-art performance across multiple African languages and speech tasks. Our benchmark analysis reveals critical patterns in resource availability, while our model evaluation demonstrates how dataset quality, domain diversity, and language family relationships influence performance across languages. Our work highlights the need for expanded speech technology resources that better reflect Africa's linguistic diversity and provides a solid foundation for future research and development efforts toward more inclusive speech technologies.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement</title>
<link>https://arxiv.org/abs/2505.19675</link>
<guid>https://arxiv.org/abs/2505.19675</guid>
<content:encoded><![CDATA[
arXiv:2505.19675v2 Announce Type: replace 
Abstract: The traditional process of creating labeled datasets is labor-intensive and expensive. Recent breakthroughs in open-source large language models (LLMs) have opened up a new avenue in generating labeled datasets automatically for various natural language processing (NLP) tasks, providing an alternative to such an expensive annotation process. However, the reliability of such auto-generated labels remains a significant concern due to inherent inaccuracies. When learning from noisy labels, the model's generalization is likely to be harmed as it is prone to overfit to those label noises. While previous studies in learning from noisy labels mainly focus on synthetic noise and real-world noise, LLM-generated label noise receives less attention. In this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to calibrate the classifier's prediction, thus enhancing its robustness towards LLM-generated noisy labels. SiDyP retrieves potential true label candidates by neighborhood label distribution in text embedding space and iteratively refines noisy candidates using a simplex diffusion model. Our framework can increase the performance of the BERT classifier fine-tuned on both zero-shot and few-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30% respectively. We demonstrate the effectiveness of SiDyP by conducting extensive benchmarking for different LLMs over a variety of NLP tasks. Our code is available on Github.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models</title>
<link>https://arxiv.org/abs/2505.21523</link>
<guid>https://arxiv.org/abs/2505.21523</guid>
<content:encoded><![CDATA[
arXiv:2505.21523v3 Announce Type: replace 
Abstract: Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, we introduce RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing us to evaluate whether the model preserves visual grounding during reasoning. We also release RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. Our analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2506.01495</link>
<guid>https://arxiv.org/abs/2506.01495</guid>
<content:encoded><![CDATA[
arXiv:2506.01495v3 Announce Type: replace 
Abstract: Ensuring that Large Language Models (LLMs) align with mainstream human values and ethical norms is crucial for the safe and sustainable development of AI. Current value evaluation and alignment are constrained by Western cultural bias and incomplete domestic frameworks reliant on non-native rules; furthermore, the lack of scalable, rule-driven scenario generation methods makes evaluations costly and inadequate across diverse cultural contexts. To address these challenges, we propose a hierarchical value framework grounded in core Chinese values, encompassing three main dimensions, 12 core values, and 50 derived values. Based on this framework, we construct a large-scale Chinese Values Corpus (CVC) containing over 250,000 value rules enhanced and expanded through human annotation. Experimental results show that CVC-guided scenarios outperform direct generation ones in value boundaries and content diversity. In the evaluation across six sensitive themes (e.g., surrogacy, suicide), seven mainstream LLMs preferred CVC-generated options in over 70.5% of cases, while five Chinese human annotators showed an 87.5% alignment with CVC, confirming its universality, cultural relevance, and strong alignment with Chinese values. Additionally, we construct 400,000 rule-based moral dilemma scenarios that objectively capture nuanced distinctions in conflicting value prioritization across 17 LLMs. Our work establishes a culturally-adaptive benchmarking framework for comprehensive value evaluation and alignment, representing Chinese characteristics. All data are available at https://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at https://github.com/Beijing-AISI/CVC.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2506.02404</link>
<guid>https://arxiv.org/abs/2506.02404</guid>
<content:encoded><![CDATA[
arXiv:2506.02404v3 Announce Type: replace 
Abstract: Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing recognition for its potential to enhance large language models (LLMs) by structurally organizing domain-specific corpora and facilitating complex reasoning. However, current evaluations of GraphRAG models predominantly rely on traditional question-answering datasets. Their limited scope in questions and evaluation metrics fails to comprehensively assess the reasoning capacity improvements enabled by GraphRAG models. To address this gap, we introduce GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously evaluate GraphRAG models. Our benchmark offers three key superiorities: \((i)\) Challenging question design. Featuring college-level, domain-specific questions that demand multi-hop reasoning, the benchmark ensures that simple content retrieval is insufficient for problem-solving. For example, some questions require mathematical reasoning or programming. \((ii)\) Diverse task coverage. The dataset includes a broad spectrum of reasoning tasks, multiple-choice, true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16 disciplines in twenty core textbooks. \((iii)\) Holistic evaluation framework. GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG pipeline, including graph construction, knowledge retrieval, and answer generation. Beyond final-answer correctness, it evaluates the logical coherence of the reasoning process. By applying nine contemporary GraphRAG methods to GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based structuring improves model reasoning capabilities. Our analysis reveals critical insights about graph architectures, retrieval efficacy, and reasoning capabilities, offering actionable guidance for the research community.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geopolitical biases in LLMs: what are the "good" and the "bad" countries according to contemporary language models</title>
<link>https://arxiv.org/abs/2506.06751</link>
<guid>https://arxiv.org/abs/2506.06751</guid>
<content:encoded><![CDATA[
arXiv:2506.06751v2 Announce Type: replace 
Abstract: This paper evaluates geopolitical biases in LLMs with respect to various countries though an analysis of their interpretation of historical events with conflicting national perspectives (USA, UK, USSR, and China). We introduce a novel dataset with neutral event descriptions and contrasting viewpoints from different countries. Our findings show significant geopolitical biases, with models favoring specific national narratives. Additionally, simple debiasing prompts had a limited effect in reducing these biases. Experiments with manipulated participant labels reveal models' sensitivity to attribution, sometimes amplifying biases or recognizing inconsistencies, especially with swapped labels. This work highlights national narrative biases in LLMs, challenges the effectiveness of simple debiasing methods, and offers a framework and dataset for future geopolitical bias research.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes</title>
<link>https://arxiv.org/abs/2506.07245</link>
<guid>https://arxiv.org/abs/2506.07245</guid>
<content:encoded><![CDATA[
arXiv:2506.07245v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have significantly improved performance on the Text-to-SQL task. However, prior approaches typically rely on static, pre-processed database information provided at inference time, which limits the model's ability to fully understand the database contents. Without dynamic interaction, LLMs are constrained to fixed, human-provided context and cannot autonomously explore the underlying data. To address this limitation, we propose SDE-SQL, a framework that enables large language models to perform self-driven exploration of databases during inference. This is accomplished by generating and executing SQL probes, which allow the model to actively retrieve information from the database and iteratively update its understanding of the data. Unlike prior methods, SDE-SQL operates in a zero-shot setting, without relying on any question-SQL pairs as in-context demonstrations. When evaluated on the BIRD benchmark with Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing a new state-of-the-art among methods based on open-source models without supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the performance of SDE-SQL can be further enhanced, yielding an additional 0.52% improvement.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PlantBert: An Open Source Language Model for Plant Science</title>
<link>https://arxiv.org/abs/2506.08897</link>
<guid>https://arxiv.org/abs/2506.08897</guid>
<content:encoded><![CDATA[
arXiv:2506.08897v2 Announce Type: replace 
Abstract: The rapid advancement of transformer-based language models has catalyzed breakthroughs in biomedical and clinical natural language processing; however, plant science remains markedly underserved by such domain-adapted tools. In this work, we present PlantBert, a high-performance, open-source language model specifically tailored for extracting structured knowledge from plant stress-response literature. Built upon the DeBERTa architecture-known for its disentangled attention and robust contextual encoding-PlantBert is fine-tuned on a meticulously curated corpus of expert-annotated abstracts, with a primary focus on lentil (Lens culinaris) responses to diverse abiotic and biotic stressors. Our methodology combines transformer-based modeling with rule-enhanced linguistic post-processing and ontology-grounded entity normalization, enabling PlantBert to capture biologically meaningful relationships with precision and semantic fidelity. The underlying corpus is annotated using a hierarchical schema aligned with the Crop Ontology, encompassing molecular, physiological, biochemical, and agronomic dimensions of plant adaptation. PlantBert exhibits strong generalization capabilities across entity types and demonstrates the feasibility of robust domain adaptation in low-resource scientific fields. By providing a scalable and reproducible framework for high-resolution entity recognition, PlantBert bridges a critical gap in agricultural NLP and paves the way for intelligent, data-driven systems in plant genomics, phenomics, and agronomic knowledge discovery. Our model is publicly released to promote transparency and accelerate cross-disciplinary innovation in computational plant science.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Techniques for supercharging academic writing with generative AI</title>
<link>https://arxiv.org/abs/2310.17143</link>
<guid>https://arxiv.org/abs/2310.17143</guid>
<content:encoded><![CDATA[
arXiv:2310.17143v4 Announce Type: replace-cross 
Abstract: Academic writing is an indispensable yet laborious part of the research enterprise. This Perspective maps out principles and methods for using generative artificial intelligence (AI), specifically large language models (LLMs), to elevate the quality and efficiency of academic writing. We introduce a human-AI collaborative framework that delineates the rationale (why), process (how), and nature (what) of AI engagement in writing. The framework pinpoints both short-term and long-term reasons for engagement and their underlying mechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals the role of AI throughout the writing process, conceptualized through a two-stage model for human-AI collaborative writing, and the nature of AI assistance in writing, represented through a model of writing-assistance types and levels. Building on this framework, we describe effective prompting techniques for incorporating AI into the writing routine (outlining, drafting, and editing) as well as strategies for maintaining rigorous scholarship, adhering to varied journal policies, and avoiding overreliance on AI. Ultimately, the prudent integration of AI into academic writing can ease the communication burden, empower authors, accelerate discovery, and promote diversity in science.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alto: Orchestrating Distributed Compound AI Systems with Nested Ancestry</title>
<link>https://arxiv.org/abs/2403.04311</link>
<guid>https://arxiv.org/abs/2403.04311</guid>
<content:encoded><![CDATA[
arXiv:2403.04311v2 Announce Type: replace-cross 
Abstract: Compound AI applications chain together subcomponents such as generative language models, document retrievers, and embedding models. Applying traditional systems optimizations such as parallelism and pipelining in compound AI systems is difficult because each component has different constraints in terms of the granularity and type of data that it ingests. New data is often generated during intermediate computations, and text streams may be split into smaller, independent fragments (such as documents to sentences) which may then be re-aggregated at later parts of the computation. Due to this complexity, existing systems to serve compound AI queries do not fully take advantage of parallelism and pipelining opportunities.
  We present Alto, a framework that automatically optimizes execution of compound AI queries through streaming and parallelism. Bento introduces a new abstraction called nested ancestry, a metadata hierarchy that allows the system to correctly track partial outputs and aggregate data across the heterogeneous constraints of the components of compound AI applications. This metadata is automatically inferred from the programming model, allowing developers to express complex dataflow patterns without needing to reason manually about the details of routing and aggregation. Implementations of four applications in Alto outperform or match implementations in LangGraph, a popular existing AI programming framework. Alto implementations match or improve latency by between 10-30%.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental Learning for Document Retrieval</title>
<link>https://arxiv.org/abs/2406.12593</link>
<guid>https://arxiv.org/abs/2406.12593</guid>
<content:encoded><![CDATA[
arXiv:2406.12593v3 Announce Type: replace-cross 
Abstract: Differentiable Search Index (DSI) utilizes pre-trained language models to perform indexing and document retrieval via end-to-end learning without relying on external indexes. However, DSI requires full re-training to index new documents, causing significant computational inefficiencies. Continual learning (CL) offers a solution by enabling the model to incrementally update without full re-training. Existing CL solutions in document retrieval rely on memory buffers or generative models for rehearsal, which is infeasible when accessing previous training data is restricted due to privacy concerns. To this end, we introduce PromptDSI, a prompt-based, rehearsal-free continual learning approach for document retrieval. PromptDSI follows the Prompt-based Continual Learning (PCL) framework, using learnable prompts to efficiently index new documents without accessing previous documents or queries. To improve retrieval latency, we remove the initial forward pass of PCL, which otherwise greatly increases training and inference time, with a negligible trade-off in performance. Additionally, we introduce a novel topic-aware prompt pool that employs neural topic embeddings as fixed keys, eliminating the instability of prompt key optimization while maintaining competitive performance with existing PCL prompt pools. In a challenging rehearsal-free continual learning setup, we demonstrate that PromptDSI variants outperform rehearsal-based baselines, match the strong cache-based baseline in mitigating forgetting, and significantly improving retrieval performance on new corpora.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-effective Instruction Learning for Pathology Vision and Language Analysis</title>
<link>https://arxiv.org/abs/2407.17734</link>
<guid>https://arxiv.org/abs/2407.17734</guid>
<content:encoded><![CDATA[
arXiv:2407.17734v2 Announce Type: replace-cross 
Abstract: The advent of vision-language models fosters the interactive conversations between AI-enabled models and humans. Yet applying these models into clinics must deal with daunting challenges around large-scale training data, financial, and computational resources. Here we propose a cost-effective instruction learning framework for conversational pathology named as CLOVER. CLOVER only trains a lightweight module and uses instruction tuning while freezing the parameters of the large language model. Instead of using costly GPT-4, we propose well-designed prompts on GPT-3.5 for building generation-based instructions, emphasizing the utility of pathological knowledge derived from the Internet source. To augment the use of instructions, we construct a high-quality set of template-based instructions in the context of digital pathology. From two benchmark datasets, our findings reveal the strength of hybrid-form instructions in the visual question-answer in pathology. Extensive results show the cost-effectiveness of CLOVER in answering both open-ended and closed-ended questions, where CLOVER outperforms strong baselines that possess 37 times more training parameters and use instruction data generated from GPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot learning in the external clinical dataset. These findings demonstrate that cost-effective modeling of CLOVER could accelerate the adoption of rapid conversational applications in the landscape of digital pathology.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>xGen-MM (BLIP-3): A Family of Open Large Multimodal Models</title>
<link>https://arxiv.org/abs/2408.08872</link>
<guid>https://arxiv.org/abs/2408.08872</guid>
<content:encoded><![CDATA[
arXiv:2408.08872v3 Announce Type: replace-cross 
Abstract: This paper introduces BLIP-3, an open framework for developing Large Multimodal Models (LMMs). The framework comprises meticulously curated datasets, a training recipe, model architectures, and a resulting suite of LMMs. We release 4B and 14B models, including both the pre-trained base model and the instruction fine-tuned ones. Our models undergo rigorous evaluation across a range of tasks, including both single and multi-image benchmarks. Our models demonstrate competitive performance among open-source LMMs with similar model sizes. Our resulting LMMs demonstrate competitive performance among open-source LMMs with similar model sizes, with the ability to comprehend interleaved image-text inputs. Our training code, models, and all datasets used in this work, including the three largescale datasets we create and the preprocessed ones, will be open-sourced to better support the research community.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension</title>
<link>https://arxiv.org/abs/2409.13609</link>
<guid>https://arxiv.org/abs/2409.13609</guid>
<content:encoded><![CDATA[
arXiv:2409.13609v4 Announce Type: replace-cross 
Abstract: Referring Expression Comprehension (REC), which aims to ground a local visual region via natural language, is a task that heavily relies on multimodal alignment. Most existing methods utilize powerful pre-trained models to transfer visual/linguistic knowledge by full fine-tuning. However, full fine-tuning the entire backbone not only breaks the rich prior knowledge embedded in the pre-training, but also incurs significant computational costs. Motivated by the recent emergence of Parameter-Efficient Transfer Learning (PETL) methods, we aim to solve the REC task in an effective and efficient manner. Directly applying these PETL methods to the REC task is inappropriate, as they lack the specific-domain abilities for precise local visual perception and visual-language alignment. Therefore, we propose a novel framework of Multimodal Prior-guided Parameter Efficient Tuning, namely MaPPER. Specifically, MaPPER comprises Dynamic Prior Adapters guided by an aligned prior, and Local Convolution Adapters to extract precise local semantics for better visual perception. Moreover, the Prior-Guided Text module is proposed to further utilize the prior for facilitating the cross-modal alignment. Experimental results on three widely-used benchmarks demonstrate that MaPPER achieves the best accuracy compared to the full fine-tuning and other PETL methods with only 1.41% tunable backbone parameters. Our code is available at https://github.com/liuting20/MaPPER.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COS-DPO: Conditioned One-Shot Multi-Objective Fine-Tuning Framework</title>
<link>https://arxiv.org/abs/2410.08316</link>
<guid>https://arxiv.org/abs/2410.08316</guid>
<content:encoded><![CDATA[
arXiv:2410.08316v3 Announce Type: replace-cross 
Abstract: In LLM alignment and many other ML applications, one often faces the Multi-Objective Fine-Tuning (MOFT) problem, i.e., fine-tuning an existing model with datasets labeled w.r.t. different objectives simultaneously. To address the challenge, we propose a Conditioned One-Shot fine-tuning framework (COS-DPO) that extends the Direct Preference Optimization technique, originally developed for efficient LLM alignment with preference data, to accommodate the MOFT settings. By direct conditioning on the weight across auxiliary objectives, our Weight-COS-DPO method enjoys an efficient one-shot training process for profiling the Pareto front and is capable of achieving comprehensive trade-off solutions even in the post-training stage. Based on our theoretical findings on the linear transformation properties of the loss function, we further propose the Temperature-COS-DPO method that augments the temperature parameter to the model input, enhancing the flexibility of post-training control over the trade-offs between the main and auxiliary objectives. We demonstrate the effectiveness and efficiency of the COS-DPO framework through its applications to various tasks, including the Learning-to-Rank (LTR) and LLM alignment tasks, highlighting its viability for large-scale ML deployments.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALTA: Compiler-Based Analysis of Transformers</title>
<link>https://arxiv.org/abs/2410.18077</link>
<guid>https://arxiv.org/abs/2410.18077</guid>
<content:encoded><![CDATA[
arXiv:2410.18077v2 Announce Type: replace-cross 
Abstract: We propose a new programming language called ALTA and a compiler that can map ALTA programs to Transformer weights. ALTA is inspired by RASP, a language proposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler from RASP programs to Transformer weights. ALTA complements and extends this prior work, offering the ability to express loops and to compile programs to Universal Transformers, among other advantages. ALTA allows us to constructively show how Transformers can represent length-invariant algorithms for computing parity and addition, as well as a solution to the SCAN benchmark of compositional generalization tasks, without requiring intermediate scratchpad decoding steps. We also propose tools to analyze cases where the expressibility of an algorithm is established, but end-to-end training on a given training set fails to induce behavior consistent with the desired algorithm. To this end, we explore training from ALTA execution traces as a more fine-grained supervision signal. This enables additional experiments and theoretical analyses relating the learnability of various algorithms to data availability and modeling decisions, such as positional encodings. We make the ALTA framework -- language specification, symbolic interpreter, and weight compiler -- available to the community to enable further applications and insights.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation</title>
<link>https://arxiv.org/abs/2411.00412</link>
<guid>https://arxiv.org/abs/2411.00412</guid>
<content:encoded><![CDATA[
arXiv:2411.00412v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate promising capabilities in solving scientific problems but often suffer from the issue of hallucination. While integrating LLMs with tools can mitigate this issue, models fine-tuned on tool usage become overreliant on them and incur unnecessary costs. Inspired by how human experts assess problem complexity before selecting solutions, we propose a novel two-component fine-tuning method, Adapting While Learning (AWL). In the first component, World Knowledge Learning (WKL), LLMs internalize scientific knowledge by learning from tool-generated solutions. In the second component, Tool Usage Adaptation (TUA), we categorize problems as easy or hard based on the model's accuracy, and train it to maintain direct reasoning for easy problems while switching to tools for hard ones. We validate our method on six scientific benchmark datasets across climate science, epidemiology, physics, and other domains. Compared to the original instruct model (8B), models post-trained with AWL achieve 29.11% higher answer accuracy and 12.72% better tool usage accuracy, even surpassing state-of-the-art models including GPT-4o and Claude-3.5 on four custom-created datasets. Our code is open-source at https://github.com/Rose-STL-Lab/Adapting-While-Learning.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning</title>
<link>https://arxiv.org/abs/2411.04105</link>
<guid>https://arxiv.org/abs/2411.04105</guid>
<content:encoded><![CDATA[
arXiv:2411.04105v4 Announce Type: replace-cross 
Abstract: Due to the size and complexity of modern large language models (LLMs), it has proven challenging to uncover the underlying mechanisms that models use to solve reasoning problems. For instance, is their reasoning for a specific problem localized to certain parts of the network? Do they break down the reasoning problem into modular components that are then executed as sequential steps as we go deeper in the model? To better understand the reasoning capability of LLMs, we study a minimal propositional logic problem that requires combining multiple facts to arrive at a solution. By studying this problem on Mistral and Gemma models, up to 27B parameters, we illuminate the core components the models use to solve such logic problems. From a mechanistic interpretability point of view, we use causal mediation analysis to uncover the pathways and components of the LLMs' reasoning processes. Then, we offer fine-grained insights into the functions of attention heads in different layers. We not only find a sparse circuit that computes the answer, but we decompose it into sub-circuits that have four distinct and modular uses. Finally, we reveal that three distinct models -- Mistral-7B, Gemma-2-9B and Gemma-2-27B -- contain analogous but not identical mechanisms.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watermarking Language Models through Language Models</title>
<link>https://arxiv.org/abs/2411.05091</link>
<guid>https://arxiv.org/abs/2411.05091</guid>
<content:encoded><![CDATA[
arXiv:2411.05091v2 Announce Type: replace-cross 
Abstract: Watermarking the outputs of large language models (LLMs) is critical for provenance tracing, content regulation, and model accountability. Existing approaches often rely on access to model internals or are constrained by static rules and token-level perturbations. Moreover, the idea of steering generative behavior via prompt-based instruction control remains largely underexplored. We introduce a prompt-guided watermarking framework that operates entirely at the input level and requires no access to model parameters or decoding logits. The framework comprises three cooperating components: a Prompting LM that synthesizes watermarking instructions from user prompts, a Marking LM that generates watermarked outputs conditioned on these instructions, and a Detecting LM trained to classify whether a response carries an embedded watermark. This modular design enables dynamic watermarking that adapts to individual prompts while remaining compatible with diverse LLM architectures, including both proprietary and open-weight models. We evaluate the framework over 25 combinations of Prompting and Marking LMs, such as GPT-4o, Mistral, LLaMA3, and DeepSeek. Experimental results show that watermark signals generalize across architectures and remain robust under fine-tuning, model distillation, and prompt-based adversarial attacks, demonstrating the effectiveness and robustness of the proposed approach.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation</title>
<link>https://arxiv.org/abs/2411.05261</link>
<guid>https://arxiv.org/abs/2411.05261</guid>
<content:encoded><![CDATA[
arXiv:2411.05261v3 Announce Type: replace-cross 
Abstract: Despite significant advancements in automated report generation, the opaqueness of text interpretability continues to cast doubt on the reliability of the content produced. This paper introduces a novel approach to identify specific image features in X-ray images that influence the outputs of report generation models. Specifically, we propose Cyclic Vision-Language Manipulator CVLM, a module to generate a manipulated X-ray from an original X-ray and its report from a designated report generator. The essence of CVLM is that cycling manipulated X-rays to the report generator produces altered reports aligned with the alterations pre-injected into the reports for X-ray generation, achieving the term "cyclic manipulation". This process allows direct comparison between original and manipulated X-rays, clarifying the critical image features driving changes in reports and enabling model users to assess the reliability of the generated texts. Empirical evaluations demonstrate that CVLM can identify more precise and reliable features compared to existing explanation methods, significantly enhancing the transparency and applicability of AI-generated reports.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying artificial intelligence through algorithmic generalization</title>
<link>https://arxiv.org/abs/2411.05943</link>
<guid>https://arxiv.org/abs/2411.05943</guid>
<content:encoded><![CDATA[
arXiv:2411.05943v2 Announce Type: replace-cross 
Abstract: The rapid development of artificial intelligence (AI) systems has created an urgent need for their scientific quantification. While their fluency across a variety of domains is impressive, AI systems fall short on tests requiring algorithmic reasoning -- a glaring limitation given the necessity for interpretable and reliable technology. Despite a surge of reasoning benchmarks emerging from the academic community, no theoretical framework exists to quantify algorithmic reasoning in AI systems. Here, we adopt a framework from computational complexity theory to quantify algorithmic generalization using algebraic expressions: algebraic circuit complexity. Algebraic circuit complexity theory -- the study of algebraic expressions as circuit models -- is a natural framework to study the complexity of algorithmic computation. Algebraic circuit complexity enables the study of generalization by defining benchmarks in terms of the computational requirements to solve a problem. Moreover, algebraic circuits are generic mathematical objects; an arbitrarily large number of samples can be generated for a specified circuit, making it an ideal experimental sandbox for the data-hungry models that are used today. In this Perspective, we adopt tools from algebraic circuit complexity, apply them to formalize a science of algorithmic generalization, and address key challenges for its successful application to AI science.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Limits of Language Generation: Trade-Offs Between Hallucination and Mode Collapse</title>
<link>https://arxiv.org/abs/2411.09642</link>
<guid>https://arxiv.org/abs/2411.09642</guid>
<content:encoded><![CDATA[
arXiv:2411.09642v2 Announce Type: replace-cross 
Abstract: Specifying all desirable properties of a language model is challenging, but certain requirements seem essential. Given samples from an unknown language, the trained model should produce valid strings not seen in training and be expressive enough to capture the language's full richness. Otherwise, outputting invalid strings constitutes "hallucination," and failing to capture the full range leads to "mode collapse." We ask if a language model can meet both requirements.
  We investigate this within a statistical language generation setting building on Gold and Angluin. Here, the model receives random samples from a distribution over an unknown language K, which belongs to a possibly infinite collection of languages. The goal is to generate unseen strings from K. We say the model generates from K with consistency and breadth if, as training size increases, its output converges to all unseen strings in K.
  Kleinberg and Mullainathan [KM24] asked if consistency and breadth in language generation are possible. We answer this negatively: for a large class of language models, including next-token prediction models, this is impossible for most collections of candidate languages. This contrasts with [KM24]'s result, showing consistent generation without breadth is possible for any countable collection of languages. Our finding highlights that generation with breadth fundamentally differs from generation without breadth.
  As a byproduct, we establish near-tight bounds on the number of samples needed for generation with or without breadth.
  Finally, our results offer hope: consistent generation with breadth is achievable for any countable collection of languages when negative examples (strings outside K) are available alongside positive ones. This suggests that post-training feedback, which encodes negative examples, can be crucial in reducing hallucinations while limiting mode collapse.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Preference Optimization: Generalizing DPO via Set-Level Contrasts</title>
<link>https://arxiv.org/abs/2412.04628</link>
<guid>https://arxiv.org/abs/2412.04628</guid>
<content:encoded><![CDATA[
arXiv:2412.04628v4 Announce Type: replace-cross 
Abstract: Direct Preference Optimization (DPO) has become a popular approach for aligning language models using pairwise preferences. However, in practical post-training pipelines, on-policy generation typically yields multiple candidate responses per prompt, which are scored by a reward model to guide learning. In this setting, we propose $\textbf{Multi-Preference Optimization (MPO)}$, a generalization of DPO that optimizes over entire sets of responses by extending the Bradley-Terry model to groupwise comparisons between chosen and rejected sets. To further enhance learning, MPO employs deviation-based weighting, which emphasizes outlier responses that deviate most from the mean reward, effectively inducing a self-paced curriculum. We theoretically prove that MPO reduces alignment bias at a rate of $\mathcal{O}\left(\frac{1}{\sqrt{n}}\right)$ with respect to the number of responses per query. Empirically, MPO achieves state-of-the-art performance on the UltraFeedback benchmark and yields up to $\sim 17.5\%$ improvement over the state-of-the-art baseline in length-controlled win rate on AlpacaEval2, establishing a new baseline for preference-based alignment
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPresent: Designing Structured Visuals from Scratch</title>
<link>https://arxiv.org/abs/2501.00912</link>
<guid>https://arxiv.org/abs/2501.00912</guid>
<content:encoded><![CDATA[
arXiv:2501.00912v2 Announce Type: replace-cross 
Abstract: Designing structured visuals such as presentation slides is essential for communicative needs, necessitating both content creation and visual planning skills. In this work, we tackle the challenge of automated slide generation, where models produce slide presentations from natural language (NL) instructions. We first introduce the SlidesBench benchmark, the first benchmark for slide generation with 7k training and 585 testing examples derived from 310 slide decks across 10 domains. SlidesBench supports evaluations that are (i)reference-based to measure similarity to a target slide, and (ii)reference-free to measure the design quality of generated slides alone. We benchmark end-to-end image generation and program generation methods with a variety of models, and find that programmatic methods produce higher-quality slides in user-interactable formats. Built on the success of program generation, we create AutoPresent, an 8B Llama-based model trained on 7k pairs of instructions paired with code for slide generation, and achieve results comparable to the closed-source model GPT-4o. We further explore iterative design refinement where the model is tasked to self-refine its own output, and we found that this process improves the slide's quality. We hope that our work will provide a basis for future work on generating structured visuals.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ladder-residual: parallelism-aware architecture for accelerating large model inference with communication overlapping</title>
<link>https://arxiv.org/abs/2501.06589</link>
<guid>https://arxiv.org/abs/2501.06589</guid>
<content:encoded><![CDATA[
arXiv:2501.06589v5 Announce Type: replace-cross 
Abstract: Large language model inference is both memory-intensive and time-consuming, often requiring distributed algorithms to efficiently scale. Various model parallelism strategies are used in multi-gpu training and inference to partition computation across multiple devices, reducing memory load and computation time. However, using model parallelism necessitates communication of information between GPUs, which has been a major bottleneck and limits the gains obtained by scaling up the number of devices. We introduce Ladder Residual, a simple architectural modification applicable to all residual-based models that enables straightforward overlapping that effectively hides the latency of communication. Our insight is that in addition to systems optimization, one can also redesign the model architecture to decouple communication from computation. While Ladder Residual can allow communication-computation decoupling in conventional parallelism patterns, we focus on Tensor Parallelism in this paper, which is particularly bottlenecked by its heavy communication. For a Transformer model with 70B parameters, applying Ladder Residual to all its layers can achieve 29% end-to-end wall clock speed up at inference time with TP sharding over 8 devices. We refer the resulting Transformer model as the Ladder Transformer. We train a 1B and 3B Ladder Transformer from scratch and observe comparable performance to a standard dense transformer baseline. We also show that it is possible to convert parts of the Llama-3.1 8B model to our Ladder Residual architecture with minimal accuracy degradation by only retraining for 3B tokens. We release our code for training and inference for easier replication of experiments.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning</title>
<link>https://arxiv.org/abs/2501.15602</link>
<guid>https://arxiv.org/abs/2501.15602</guid>
<content:encoded><![CDATA[
arXiv:2501.15602v3 Announce Type: replace-cross 
Abstract: Test-time scaling, which is also often referred to as slow-thinking, has been demonstrated to enhance multi-step reasoning in large language models (LLMs). However, despite its widespread utilization, the mechanisms underlying slow-thinking methods remain poorly understood. This paper explores the mechanisms of external slow-thinking from a theoretical standpoint. We begin by examining the snowball error effect within the LLM reasoning process and connect it to the likelihood of correct reasoning using information theory. Building on this, we show that external slow-thinking methods can be interpreted as strategies to mitigate the error probability. We further provide a comparative analysis of popular external slow-thinking approaches, ranging from simple to complex, highlighting their differences and interrelationships. Our findings suggest that the efficacy of these methods is not primarily determined by the specific framework employed, and that expanding the search scope or the model's internal reasoning capacity may yield more sustained improvements in the long term. We open-source our code at https://github.com/ZyGan1999/Snowball-Errors-and-Probability.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Almost Surely Safe Alignment of Large Language Models at Inference-Time</title>
<link>https://arxiv.org/abs/2502.01208</link>
<guid>https://arxiv.org/abs/2502.01208</guid>
<content:encoded><![CDATA[
arXiv:2502.01208v3 Announce Type: replace-cross 
Abstract: We introduce a novel inference-time alignment approach for LLMs that aims to generate safe responses almost surely, i.e., with probability approaching one. Our approach models the generation of safe responses as a constrained Markov Decision Process (MDP) within the LLM's latent space. We augment a safety state that tracks the evolution of safety constraints and dynamically penalize unsafe generations to ensure the generation of safe responses. Consequently, we demonstrate formal safety guarantees w.r.t. the given cost model upon solving the MDP in the latent space with sufficiently large penalties. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate that InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses. Our findings contribute to the advancement of safer LLM deployment through alignment at inference-time, thus presenting a promising alternative to resource-intensive, overfitting-prone alignment techniques like RLHF.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Self-Talk: A Communication-Centric Survey of LLM-Based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2502.14321</link>
<guid>https://arxiv.org/abs/2502.14321</guid>
<content:encoded><![CDATA[
arXiv:2502.14321v2 Announce Type: replace-cross 
Abstract: Large language model-based multi-agent systems have recently gained significant attention due to their potential for complex, collaborative, and intelligent problem-solving capabilities. Existing surveys typically categorize LLM-based multi-agent systems (LLM-MAS) according to their application domains or architectures, overlooking the central role of communication in coordinating agent behaviors and interactions. To address this gap, this paper presents a comprehensive survey of LLM-MAS from a communication-centric perspective. Specifically, we propose a structured framework that integrates system-level communication (architecture, goals, and protocols) with system internal communication (strategies, paradigms, objects, and content), enabling a detailed exploration of how agents interact, negotiate, and achieve collective intelligence. Through an extensive analysis of recent literature, we identify key components in multiple dimensions and summarize their strengths and limitations. In addition, we highlight current challenges, including communication efficiency, security vulnerabilities, inadequate benchmarking, and scalability issues, and outline promising future research directions. This review aims to help researchers and practitioners gain a clear understanding of the communication mechanisms in LLM-MAS, thereby facilitating the design and deployment of robust, scalable, and secure multi-agent systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation</title>
<link>https://arxiv.org/abs/2503.06680</link>
<guid>https://arxiv.org/abs/2503.06680</guid>
<content:encoded><![CDATA[
arXiv:2503.06680v2 Announce Type: replace-cross 
Abstract: Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs' automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Guided Indoor Navigation with Multimodal Map Understanding</title>
<link>https://arxiv.org/abs/2503.11702</link>
<guid>https://arxiv.org/abs/2503.11702</guid>
<content:encoded><![CDATA[
arXiv:2503.11702v4 Announce Type: replace-cross 
Abstract: Indoor navigation presents unique challenges due to complex layouts and the unavailability of GNSS signals. Existing solutions often struggle with contextual adaptation, and typically require dedicated hardware. In this work, we explore the potential of a Large Language Model (LLM), i.e., ChatGPT, to generate natural, context-aware navigation instructions from indoor map images. We design and evaluate test cases across different real-world environments, analyzing the effectiveness of LLMs in interpreting spatial layouts, handling user constraints, and planning efficient routes. Our findings demonstrate the potential of LLMs for supporting personalized indoor navigation, with an average of 86.59% correct indications and a maximum of 97.14%. The proposed system achieves high accuracy and reasoning performance. These results have key implications for AI-driven navigation and assistive technologies.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrunkAgent: Stealthy Memory Corruption in LLM-Powered Recommender Agents</title>
<link>https://arxiv.org/abs/2503.23804</link>
<guid>https://arxiv.org/abs/2503.23804</guid>
<content:encoded><![CDATA[
arXiv:2503.23804v2 Announce Type: replace-cross 
Abstract: Large language model (LLM)-powered agents are increasingly used in recommender systems (RSs) to achieve personalized behavior modeling, where the memory mechanism plays a pivotal role in enabling the agents to autonomously explore, learn and self-evolve from real-world interactions. However, this very mechanism, serving as a contextual repository, inherently exposes an attack surface for potential adversarial manipulations. Despite its central role, the robustness of agentic RSs in the face of such threats remains largely underexplored. Previous works suffer from semantic mismatches or rely on static embeddings or pre-defined prompts, all of which hinder their applicability to systems with dynamic memory states. This challenge is exacerbated by the black-box nature of commercial RSs.
  To tackle the above problems, in this paper, we present the first systematic investigation of memory-based vulnerabilities in LLM-powered recommender agents, revealing their security limitations and guiding efforts to strengthen system resilience and trustworthiness. Specifically, we propose a novel black-box attack framework named DrunkAgent. DrunkAgent crafts semantically meaningful adversarial textual triggers for target item promotions and introduces a series of strategies to maximize the trigger effect by corrupting the memory updates during the interactions. The triggers and strategies are optimized on a surrogate model, enabling DrunkAgent transferable and stealthy. Extensive experiments on real-world datasets across diverse agentic RSs, including collaborative filtering, retrieval augmentation and sequential recommendations, demonstrate the generalizability, transferability and stealthiness of DrunkAgent.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets</title>
<link>https://arxiv.org/abs/2505.15517</link>
<guid>https://arxiv.org/abs/2505.15517</guid>
<content:encoded><![CDATA[
arXiv:2505.15517v2 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) acquire real-world knowledge and general reasoning ability through Internet-scale image-text corpora. They can augment robotic systems with scene understanding and task planning, and assist visuomotor policies that are trained on robot trajectory data. We explore the reverse paradigm - using rich, real, multi-modal robot trajectory data to enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual Question Answering (VQA) dataset generation framework for VLMs. Given a human tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual and non-descriptive sensory modalities, such as end-effector pose, gripper aperture, and force sensing. Based on these modalities, it segments the robot trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses scene and interaction understanding to identify 3D properties of the robot, task goal, and the target object. The properties are used to generate representative VQA queries - images with textural multiple-choice questions - based on spatial, goal-conditioned, and interaction reasoning question templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710 questions covering 463 distinct scenes and 3,396 robotic manipulation tasks from 176k real robot trajectories. Results suggest that Robo2VLM-1 can benchmark and improve VLM capabilities in spatial and interaction reasoning.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development</title>
<link>https://arxiv.org/abs/2505.16975</link>
<guid>https://arxiv.org/abs/2505.16975</guid>
<content:encoded><![CDATA[
arXiv:2505.16975v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown strong capability in diverse software engineering tasks, e.g. code completion, bug fixing, and document generation. However, feature-driven development (FDD), a highly prevalent real-world task that involves developing new functionalities for large, existing codebases, remains underexplored. We therefore introduce SWE-Dev, the first large-scale dataset (with 14,000 training and 500 test samples) designed to evaluate and train autonomous coding systems on real-world feature development tasks. To ensure verifiable and diverse training, SWE-Dev uniquely provides all instances with a runnable environment and its developer-authored executable unit tests. This collection not only provides high-quality data for Supervised Fine-Tuning (SFT), but also enables Reinforcement Learning (RL) by delivering accurate reward signals from executable unit tests. Our extensive evaluations on SWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent Systems (MAS), reveal that FDD is a profoundly challenging frontier for current AI (e.g., Claude-3.7-Sonnet achieves only 22.45\% Pass@3 on the hard test split). Crucially, we demonstrate that SWE-Dev serves as an effective platform for model improvement: fine-tuning on training set enabled a 7B model comparable to GPT-4o on \textit{hard} split, underscoring the value of its high-quality training data. Code is available here \href{https://github.com/DorothyDUUU/SWE-Dev}{https://github.com/DorothyDUUU/SWE-Dev}.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Path to Multimodal Historical Reasoning: HistBench and HistAgent</title>
<link>https://arxiv.org/abs/2505.20246</link>
<guid>https://arxiv.org/abs/2505.20246</guid>
<content:encoded><![CDATA[
arXiv:2505.20246v3 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have led to remarkable progress across domains, yet their capabilities in the humanities, particularly history, remain underexplored. Historical reasoning poses unique challenges for AI, involving multimodal source interpretation, temporal inference, and cross-linguistic analysis. While general-purpose agents perform well on many existing benchmarks, they lack the domain-specific expertise required to engage with historical materials and questions. To address this gap, we introduce HistBench, a new benchmark of 414 high-quality questions designed to evaluate AI's capacity for historical reasoning and authored by more than 40 expert contributors. The tasks span a wide range of historical problems-from factual retrieval based on primary sources to interpretive analysis of manuscripts and images, to interdisciplinary challenges involving archaeology, linguistics, or cultural history. Furthermore, the benchmark dataset spans 29 ancient and modern languages and covers a wide range of historical periods and world regions. Finding the poor performance of LLMs and other agents on HistBench, we further present HistAgent, a history-specific agent equipped with carefully designed tools for OCR, translation, archival search, and image understanding in History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of 27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online search and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%) and Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These results highlight the limitations of existing LLMs and generalist agents and demonstrate the advantages of HistAgent for historical reasoning.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RefAV: Towards Planning-Centric Scenario Mining</title>
<link>https://arxiv.org/abs/2505.20981</link>
<guid>https://arxiv.org/abs/2505.20981</guid>
<content:encoded><![CDATA[
arXiv:2505.20981v2 Announce Type: replace-cross 
Abstract: Autonomous Vehicles (AVs) collect and pseudo-label terabytes of multi-modal data localized to HD maps during normal fleet testing. However, identifying interesting and safety-critical scenarios from uncurated driving logs remains a significant challenge. Traditional scenario mining techniques are error-prone and prohibitively time-consuming, often relying on hand-crafted structured queries. In this work, we revisit spatio-temporal scenario mining through the lens of recent vision-language models (VLMs) to detect whether a described scenario occurs in a driving log and, if so, precisely localize it in both time and space. To address this problem, we introduce RefAV, a large-scale dataset of 10,000 diverse natural language queries that describe complex multi-agent interactions relevant to motion planning derived from 1000 driving logs in the Argoverse 2 Sensor dataset. We evaluate several referential multi-object trackers and present an empirical analysis of our baselines. Notably, we find that naively repurposing off-the-shelf VLMs yields poor performance, suggesting that scenario mining presents unique challenges. Lastly, we discuss our recent CVPR 2025 competition and share insights from the community. Our code and dataset are available at https://github.com/CainanD/RefAV/ and https://argoverse.github.io/user-guide/tasks/scenario_mining.html
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation</title>
<link>https://arxiv.org/abs/2506.03147</link>
<guid>https://arxiv.org/abs/2506.03147</guid>
<content:encoded><![CDATA[
arXiv:2506.03147v4 Announce Type: replace-cross 
Abstract: Although existing unified models achieve strong performance in vision-language understanding and text-to-image generation, they remain limited in addressing image perception and manipulation -- capabilities increasingly demanded in practical applications. Recently, OpenAI introduced the powerful GPT-4o-Image model, which showcases advanced capabilities in comprehensive image perception and manipulation, sparking widespread interest. Through carefully designed experiments, we observe that GPT-4o-Image likely relies on semantic encoders rather than VAEs for feature extraction, despite VAEs being commonly regarded as crucial for image manipulation tasks. Inspired by this insight, we propose UniWorld-V1, a unified generative framework built upon semantic features extracted from powerful multimodal large language models and contrastive semantic encoders. Using only 2.7M training data, UniWorld-V1 achieves impressive performance across diverse tasks, including image understanding, generation, manipulation, and perception. We fully open-source the UniWorld-V1 framework, including model weights, training and evaluation scripts, and datasets to promote reproducibility and further research.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CIVET: Systematic Evaluation of Understanding in VLMs</title>
<link>https://arxiv.org/abs/2506.05146</link>
<guid>https://arxiv.org/abs/2506.05146</guid>
<content:encoded><![CDATA[
arXiv:2506.05146v2 Announce Type: replace-cross 
Abstract: While Vision-Language Models (VLMs) have achieved competitive performance in various tasks, their comprehension of the underlying structure and semantics of a scene remains understudied. To investigate the understanding of VLMs, we study their capability regarding object properties and relations in a controlled and interpretable manner. To this scope, we introduce CIVET, a novel and extensible framework for systematiC evaluatIon Via controllEd sTimuli. CIVET addresses the lack of standardized systematic evaluation for assessing VLMs' understanding, enabling researchers to test hypotheses with statistical rigor. With CIVET, we evaluate five state-of-the-art VLMs on exhaustive sets of stimuli, free from annotation noise, dataset-specific biases, and uncontrolled scene complexity. Our findings reveal that 1) current VLMs can accurately recognize only a limited set of basic object properties; 2) their performance heavily depends on the position of the object in the scene; 3) they struggle to understand basic relations among objects. Furthermore, a comparative evaluation with human annotators reveals that VLMs still fall short of achieving human-level accuracy.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kinetics: Rethinking Test-Time Scaling Laws</title>
<link>https://arxiv.org/abs/2506.05333</link>
<guid>https://arxiv.org/abs/2506.05333</guid>
<content:encoded><![CDATA[
arXiv:2506.05333v3 Announce Type: replace-cross 
Abstract: We rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential and increasingly important with more computing invested, for realizing the full potential of test-time scaling where, unlike training, accuracy has yet to saturate as a function of computation, and continues to improve through increased generation. The code is available at https://github.com/Infini-AI-Lab/Kinetics.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency</title>
<link>https://arxiv.org/abs/2506.08343</link>
<guid>https://arxiv.org/abs/2506.08343</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning models, self-reflection, efficient, multimodal, NoWait

Summary: 
- Recent advances in reasoning models have enabled step-by-step reasoning but often lead to verbose and redundant outputs.
- The study examines whether explicit self-reflection signals are necessary for advanced reasoning.
- The NoWait approach disables explicit self-reflection tokens like "Wait" and "Hmm" during inference to reduce overthinking.
- Extensive experiments across various reasoning tasks show that NoWait significantly decreases chain-of-thought trajectory length without affecting model utility.
- NoWait offers a simple and effective solution for efficient multimodal reasoning, enhancing performance without the need for explicit self-reflection signaling. 

Summary: <div>
arXiv:2506.08343v2 Announce Type: replace 
Abstract: Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant overthinking, resulting in verbose and redundant outputs that hinder efficiency. In this study, we examine whether explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is necessary for advanced reasoning. We propose NoWait, a simple yet effective approach that disables explicit self-reflection by suppressing these tokens during inference. Extensive experiments on ten benchmarks across textual, visual, and video reasoning tasks show that NoWait reduces chain-of-thought trajectory length by up to 27%-51% in five R1-style model series, without compromising model utility. NoWait thus offers a plug-and-play solution for efficient and utility-preserving multimodal reasoning.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.09033</link>
<guid>https://arxiv.org/abs/2506.09033</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Large Language Models, Router-R1, Multi-LLM Routing, Generalization<br />
Summary:<br />
The article introduces Router-R1, a reinforcement learning-based framework that addresses the limitations of existing LLM routers in handling complex tasks by routing user queries to multiple LLMs. Router-R1 models multi-LLM routing as a sequential decision process, allowing the router to alternate between internal deliberation and dynamic model invocation. It incorporates a rule-based reward system to optimize performance and cost, enabling better performance-cost trade-offs through reinforcement learning. By conditioning only on basic model descriptors, Router-R1 demonstrates strong generalization to unseen model selection scenarios. Experimental results on various QA benchmarks show that Router-R1 outperforms several baseline methods, achieving superior performance while effectively managing costs and maintaining robust generalization capabilities.<br /> <div>
arXiv:2506.09033v2 Announce Type: replace 
Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave "think" actions (internal deliberation) with "route" actions (dynamic model invocation), and integrates each response into its evolving context. To facilitate learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for optimizing the balance between performance and cost, opening a pathway toward enhancing performance-cost trade-offs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms several strong baselines, achieving superior performance while maintaining robust generalization and cost management.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings</title>
<link>https://arxiv.org/abs/2506.14900</link>
<guid>https://arxiv.org/abs/2506.14900</guid>
<content:encoded><![CDATA[
<div> Dataset, Adverse Event extraction, Elderly patients, Annotation schema, FlairNLP

Summary: 
The article presents a manually annotated corpus for Adverse Event (AE) extraction from discharge summaries of elderly patients, addressing the underrepresentation of this population in clinical NLP resources. The dataset includes 14 clinically significant AEs and contextual attributes like negation and diagnosis type. The annotation schema supports both discontinuous and overlapping entities, a challenge rarely tackled in prior work. Transformer-based models show strong performance on document-level coarse-grained extraction but struggle with fine-grained entity-level tasks, especially for rare events and complex attributes. The results highlight the difficulty in detecting underrepresented AEs and capturing nuanced clinical language. The dataset, developed within a Trusted Research Environment (TRE), is available for evaluation and supports future cross-dataset generalization.
<br /><br />Summary:  <div>
arXiv:2506.14900v1 Announce Type: new 
Abstract: In this work, we present a manually annotated corpus for Adverse Event (AE) extraction from discharge summaries of elderly patients, a population often underrepresented in clinical NLP resources. The dataset includes 14 clinically significant AEs-such as falls, delirium, and intracranial haemorrhage, along with contextual attributes like negation, diagnosis type, and in-hospital occurrence. Uniquely, the annotation schema supports both discontinuous and overlapping entities, addressing challenges rarely tackled in prior work. We evaluate multiple models using FlairNLP across three annotation granularities: fine-grained, coarse-grained, and coarse-grained with negation. While transformer-based models (e.g., BERT-cased) achieve strong performance on document-level coarse-grained extraction (F1 = 0.943), performance drops notably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly for rare events and complex attributes. These results demonstrate that despite high-level scores, significant challenges remain in detecting underrepresented AEs and capturing nuanced clinical language. Developed within a Trusted Research Environment (TRE), the dataset is available upon request via DataLoch and serves as a robust benchmark for evaluating AE extraction methods and supporting future cross-dataset generalisation.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Constrained and Unconstrained Decoding via Boosting: BoostCD and Its Application to Information Extraction</title>
<link>https://arxiv.org/abs/2506.14901</link>
<guid>https://arxiv.org/abs/2506.14901</guid>
<content:encoded><![CDATA[
<div> Boosted Constrained Decoding, autoregressive language model, structured NLP tasks, constrained decoding, closed information extraction

Summary:
Boosted Constrained Decoding (BoostCD) addresses the issue of low-quality output during constrained decoding in structured NLP tasks using an autoregressive language model. The approach combines constrained and unconstrained decoding in two phases, leveraging the complementary mistakes made by the base model to improve performance. In the first phase, the base model decodes twice to obtain weak predictions, and in the second phase, a learned boosted model combines these predictions for a final output. BoostCD, applied to closed information extraction as BoostIE, outperforms previous approaches both within and outside the training data distribution. It successfully addresses common errors observed in prior methods, showcasing the effectiveness of the BoostCD framework in enhancing the quality of output for structured NLP tasks.<br /><br />Summary: <div>
arXiv:2506.14901v1 Announce Type: new 
Abstract: Many recent approaches to structured NLP tasks use an autoregressive language model $M$ to map unstructured input text $x$ to output text $y$ representing structured objects (such as tuples, lists, trees, code, etc.), where the desired output structure is enforced via constrained decoding. During training, these approaches do not require the model to be aware of the constraints, which are merely implicit in the training outputs $y$. This is advantageous as it allows for dynamic constraints without requiring retraining, but can lead to low-quality output during constrained decoding at test time. We overcome this problem with Boosted Constrained Decoding (BoostCD), which combines constrained and unconstrained decoding in two phases: Phase 1 decodes from the base model $M$ twice, in constrained and unconstrained mode, obtaining two weak predictions. In phase 2, a learned autoregressive boosted model combines the two weak predictions into one final prediction. The mistakes made by the base model with vs. without constraints tend to be complementary, which the boosted model learns to exploit for improved performance. We demonstrate the power of BoostCD by applying it to closed information extraction. Our model, BoostIE, outperforms prior approaches both in and out of distribution, addressing several common errors identified in those approaches.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision</title>
<link>https://arxiv.org/abs/2506.14912</link>
<guid>https://arxiv.org/abs/2506.14912</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, context documents, credibility assessment, weakly supervised framework, inter-document agreement

Summary:
CrEst is a novel framework that assesses the credibility of context documents during large language model (LLM) inference without manual annotations. It addresses the challenge of varying credibility in context documents by leveraging inter-document agreement to estimate credibility. CrEst offers two integration strategies – a black-box approach for models without access to internal weights and a white-box method that modifies attention mechanisms. Extensive experiments across different model architectures and datasets show that CrEst outperforms strong baselines, achieving significant improvements in accuracy and F1 score. The framework maintains robust performance even under high-noise conditions, demonstrating its effectiveness in assessing and incorporating document credibility into LLM inference. <br /><br />Summary: <div>
arXiv:2506.14912v1 Announce Type: new 
Abstract: The integration of contextual information has significantly enhanced the performance of large language models (LLMs) on knowledge-intensive tasks. However, existing methods often overlook a critical challenge: the credibility of context documents can vary widely, potentially leading to the propagation of unreliable information. In this paper, we introduce CrEst, a novel weakly supervised framework for assessing the credibility of context documents during LLM inference--without requiring manual annotations. Our approach is grounded in the insight that credible documents tend to exhibit higher semantic coherence with other credible documents, enabling automated credibility estimation through inter-document agreement. To incorporate credibility into LLM inference, we propose two integration strategies: a black-box approach for models without access to internal weights or activations, and a white-box method that directly modifies attention mechanisms. Extensive experiments across three model architectures and five datasets demonstrate that CrEst consistently outperforms strong baselines, achieving up to a 26.86% improvement in accuracy and a 3.49% increase in F1 score. Further analysis shows that CrEst maintains robust performance even under high-noise conditions.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance</title>
<link>https://arxiv.org/abs/2506.14927</link>
<guid>https://arxiv.org/abs/2506.14927</guid>
<content:encoded><![CDATA[
<div> Benchmark, Natural language processing, Multi-document reasoning, Language models, MDBench

Summary:
MDBench is a new dataset designed to evaluate large language models (LLMs) on the task of multi-document reasoning. Created through a synthetic generation process, MDBench efficiently generates challenging document sets and corresponding question-answer examples by modifying structured seed knowledge with LLM-assisted edits. The dataset poses significant challenges for popular LLMs, even with relatively short document sets. The knowledge-guided generation technique allows for targeted analysis of multi-document-specific reasoning capabilities and can be quickly adapted to incorporate new challenges and future modeling improvements. The research highlights the importance of developing new benchmarks to assess the reasoning capabilities of LLMs in handling longer-context inputs like multi-document data. <br /><br />Summary: <div>
arXiv:2506.14927v1 Announce Type: new 
Abstract: Natural language processing evaluation has made significant progress, largely driven by the proliferation of powerful large language mod-els (LLMs). New evaluation benchmarks are of increasing priority as the reasoning capabilities of LLMs are expanding at a rapid pace. In particular, while multi-document (MD) reasoning is an area of extreme relevance given LLM capabilities in handling longer-context inputs, few benchmarks exist to rigorously examine model behavior in this setting. Moreover, the multi-document setting is historically challenging for benchmark creation due to the expensive cost of annotating long inputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs on the task of multi-document reasoning. Notably, MDBench is created through a novel synthetic generation process, allowing us to controllably and efficiently generate challenging document sets and the corresponding question-answer (QA) examples. Our novel technique operates on condensed structured seed knowledge, modifying it through LLM-assisted edits to induce MD-specific reasoning challenges. We then convert this structured knowledge into a natural text surface form, generating a document set and corresponding QA example. We analyze the behavior of popular LLMs and prompting techniques, finding that MDBENCH poses significant challenges for all methods, even with relatively short document sets. We also see our knowledge-guided generation technique (1) allows us to readily perform targeted analysis of MD-specific reasoning capabilities and (2) can be adapted quickly to account for new challenges and future modeling improvements.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Chat to Checkup: Can Large Language Models Assist in Diabetes Prediction?</title>
<link>https://arxiv.org/abs/2506.14949</link>
<guid>https://arxiv.org/abs/2506.14949</guid>
<content:encoded><![CDATA[
<div> LLMs, Diabetes Prediction, Zero-shot, Few-shot, Prompting Methods <br />
<br />
Summary: 
The study explores the use of Large Language Models (LLMs) for predicting diabetes using zero-shot, one-shot, and three-shot prompting methods. Six LLMs were evaluated, with proprietary models such as GPT-4o and Gemma-2-27B outperforming open-source models and traditional machine learning models like Random Forest and Logistic Regression. Gemma-2-27B showed high accuracy and F1-score, surpassing traditional models. However, challenges remain, including performance variations across prompting strategies and the need for domain-specific fine-tuning. The study highlights the potential of LLMs in medical prediction tasks and suggests exploring prompt engineering and hybrid approaches to enhance healthcare predictions.<br /><br /> <div>
arXiv:2506.14949v1 Announce Type: new 
Abstract: While Machine Learning (ML) and Deep Learning (DL) models have been widely used for diabetes prediction, the use of Large Language Models (LLMs) for structured numerical data is still not well explored. In this study, we test the effectiveness of LLMs in predicting diabetes using zero-shot, one-shot, and three-shot prompting methods. We conduct an empirical analysis using the Pima Indian Diabetes Database (PIDD). We evaluate six LLMs, including four open-source models: Gemma-2-27B, Mistral-7B, Llama-3.1-8B, and Llama-3.2-2B. We also test two proprietary models: GPT-4o and Gemini Flash 2.0. In addition, we compare their performance with three traditional machine learning models: Random Forest, Logistic Regression, and Support Vector Machine (SVM). We use accuracy, precision, recall, and F1-score as evaluation metrics. Our results show that proprietary LLMs perform better than open-source ones, with GPT-4o and Gemma-2-27B achieving the highest accuracy in few-shot settings. Notably, Gemma-2-27B also outperforms the traditional ML models in terms of F1-score. However, there are still issues such as performance variation across prompting strategies and the need for domain-specific fine-tuning. This study shows that LLMs can be useful for medical prediction tasks and encourages future work on prompt engineering and hybrid approaches to improve healthcare predictions.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings</title>
<link>https://arxiv.org/abs/2506.15001</link>
<guid>https://arxiv.org/abs/2506.15001</guid>
<content:encoded><![CDATA[
<div> Keywords: reversible sentence embeddings, LLM, memory token, text reconstruction, potential applications <br />
Summary:<br />
- The study explores the generation of reversible sentence embeddings in language models such as LLMs, allowing for exact text reconstruction without weight modification.
- A special memory token is introduced and optimized through training on a fixed sequence to achieve this capability.
- The model successfully reconstructs fixed sequences in English and Spanish datasets, with varying sequence lengths and model scales.
- Llama 3.1 8B demonstrates impressive reconstruction ability across all tested sequences.
- The research suggests potential applications of reversible embeddings in memory-based retrieval, compression, and controlled text generation.<br /><br />Summary: <div>
arXiv:2506.15001v1 Announce Type: new 
Abstract: In this work, we observe an interesting phenomenon: it is possible to generate reversible sentence embeddings that allow an LLM to reconstruct the original text exactly, without modifying the model's weights. This is achieved by introducing a special memory token, whose embedding is optimized through training on a fixed sequence. When prompted with this embedding, the model reconstructs the fixed sequence exactly. We evaluate this phenomenon across English and Spanish datasets, sequences of up to approximately 240 tokens, and model scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B successfully reconstructs all tested sequences. Our findings highlight an interesting capability of LLMs and suggest potential applications in memory-based retrieval, compression, and controlled text generation.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying social isolation themes in NVDRS text narratives using topic modeling and text-classification methods</title>
<link>https://arxiv.org/abs/2506.15030</link>
<guid>https://arxiv.org/abs/2506.15030</guid>
<content:encoded><![CDATA[
<div> Keywords: social isolation, loneliness, suicide rates, natural language processing, topic modeling<br />
Summary:<br />
Using natural language processing techniques, this study identified mentions of chronic social isolation in law enforcement and coroner narratives related to over 300,000 suicides in the US from 2002 to 2020. The study developed high-quality classifiers with an average F1 score of .86 and accuracy of .82 to detect social isolation indicators. Results showed that male individuals, gay individuals, and those who were divorced had higher odds of being classified as experiencing chronic social isolation. Other predictors for social isolation included recent divorce, child custody loss, eviction or recent move, and break-up. These findings suggest the importance of improving surveillance and prevention strategies for social isolation and loneliness in the United States. <br /><br />Summary: <div>
arXiv:2506.15030v1 Announce Type: new 
Abstract: Social isolation and loneliness, which have been increasing in recent years strongly contribute toward suicide rates. Although social isolation and loneliness are not currently recorded within the US National Violent Death Reporting System's (NVDRS) structured variables, natural language processing (NLP) techniques can be used to identify these constructs in law enforcement and coroner medical examiner narratives. Using topic modeling to generate lexicon development and supervised learning classifiers, we developed high-quality classifiers (average F1: .86, accuracy: .82). Evaluating over 300,000 suicides from 2002 to 2020, we identified 1,198 mentioning chronic social isolation. Decedents had higher odds of chronic social isolation classification if they were men (OR = 1.44; CI: 1.24, 1.69, p<.0001), gay (OR = 3.68; 1.97, 6.33, p<.0001), or were divorced (OR = 3.34; 2.68, 4.19, p<.0001). We found significant predictors for other social isolation topics of recent or impending divorce, child custody loss, eviction or recent move, and break-up. Our methods can improve surveillance and prevention of social isolation and loneliness in the United States.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation</title>
<link>https://arxiv.org/abs/2506.15068</link>
<guid>https://arxiv.org/abs/2506.15068</guid>
<content:encoded><![CDATA[
<div> Keywords: PrefBERT, open-ended generation, long-form evaluation, reward signals, policy models

Summary: 
PrefBERT is introduced as a scoring model for evaluating open-ended long-form generation, filling the gap in current evaluation methods. It offers distinct rewards for good and bad outputs, providing better semantic feedback than traditional metrics like ROUGE-L and BERTScore. Trained on diverse response evaluation datasets, PrefBERT supports guided reinforcement policy optimization (GRPO) with improved reward signals. Evaluations, including LLM-as-a-judge and human ratings, demonstrate the reliability of PrefBERT across various long passages. Human evaluations confirm that using PrefBERT for training policy models results in responses more aligned with human preferences compared to traditional metrics. The availability of the code on GitHub facilitates further experimentation and implementation of PrefBERT in evaluating open-ended long-form generation. 

<br /><br />Summary: <div>
arXiv:2506.15068v1 Announce Type: new 
Abstract: Evaluating open-ended long-form generation is challenging because it is hard to define what clearly separates good from bad outputs. Existing methods often miss key aspects like coherence, style, or relevance, or are biased by pretraining data, making open-ended long-form evaluation an underexplored problem. To address this gap, we propose PrefBERT, a scoring model for evaluating open-ended long-form generation in GRPO and guiding its training with distinct rewards for good and bad outputs. Trained on two response evaluation datasets with diverse long-form styles and Likert-rated quality, PrefBERT effectively supports GRPO by offering better semantic reward feedback than traditional metrics ROUGE-L and BERTScore do. Through comprehensive evaluations, including LLM-as-a-judge, human ratings, and qualitative analysis, we show that PrefBERT, trained on multi-sentence and paragraph-length responses, remains reliable across varied long passages and aligns well with the verifiable rewards GRPO needs. Human evaluations confirm that using PrefBERT as the reward signal to train policy models yields responses better aligned with human preferences than those trained with traditional metrics. Our code is available at https://github.com/zli12321/long_form_rl.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-Time Encoding Shapes Unlearning in LLMs</title>
<link>https://arxiv.org/abs/2506.15076</link>
<guid>https://arxiv.org/abs/2506.15076</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, unlearning, knowledge encoding, paraphrased descriptions, factual knowledge

Summary:
Learning-time choices in knowledge encoding impact the effectiveness of unlearning factual knowledge, as shown in empirical investigations. The experiments highlight that learning with paraphrased descriptions enhances unlearning performance. However, the process of unlearning individual pieces of knowledge from a text chunk poses challenges. The study emphasizes the importance of considering learning-time knowledge encoding for reliable post-hoc unlearning. Overall, the findings suggest that the ability to "unlearn" specific knowledge post hoc is essential for various reasons, such as privacy regulations and correcting outdated or harmful content. By understanding the impact of learning-time choices on unlearning effectiveness, strategies can be developed to improve the process and ensure reliable removal of knowledge in large language models. 

Summary: <div>
arXiv:2506.15076v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed in the real world, the ability to ``unlearn'', or remove specific pieces of knowledge post hoc, has become essential for a variety of reasons ranging from privacy regulations to correcting outdated or harmful content. Prior work has proposed unlearning benchmarks and algorithms, and has typically assumed that the training process and the target model are fixed. In this work, we empirically investigate how learning-time choices in knowledge encoding impact the effectiveness of unlearning factual knowledge. Our experiments reveal two key findings: (1) learning with paraphrased descriptions improves unlearning performance and (2) unlearning individual piece of knowledge from a chunk of text is challenging. Our results suggest that learning-time knowledge encoding may play a central role in enabling reliable post-hoc unlearning.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification</title>
<link>https://arxiv.org/abs/2506.15081</link>
<guid>https://arxiv.org/abs/2506.15081</guid>
<content:encoded><![CDATA[
<div> Ambiguity, Discourse parsing, Dialogue, Clarification, Reasoning<br />
Summary:<br />
Dialogue discourse parsing often faces challenges due to linguistic features like omission and idiom leading to ambiguities. To tackle this, a Discourse-aware Clarification Module (DCM) is proposed, using clarification type and discourse goal reasoning. The model is optimized with Contribution-aware Preference Optimization (CPO) to enhance adaptability and alignment with parser requirements, reducing errors. Experimental results on STAC and Molweni datasets show improved performance compared to existing baselines. <br /> <div>
arXiv:2506.15081v1 Announce Type: new 
Abstract: Dialogue discourse parsing aims to identify and analyze discourse relations between the utterances within dialogues. However, linguistic features in dialogues, such as omission and idiom, frequently introduce ambiguities that obscure the intended discourse relations, posing significant challenges for parsers. To address this issue, we propose a Discourse-aware Clarification Module (DCM) to enhance the performance of the dialogue discourse parser. DCM employs two distinct reasoning processes: clarification type reasoning and discourse goal reasoning. The former analyzes linguistic features, while the latter distinguishes the intended relation from the ambiguous one. Furthermore, we introduce Contribution-aware Preference Optimization (CPO) to mitigate the risk of erroneous clarifications, thereby reducing cascading errors. CPO enables the parser to assess the contributions of the clarifications from DCM and provide feedback to optimize the DCM, enhancing its adaptability and alignment with the parser's requirements. Extensive experiments on the STAC and Molweni datasets demonstrate that our approach effectively resolves ambiguities and significantly outperforms the state-of-the-art (SOTA) baselines.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CKD-EHR:Clinical Knowledge Distillation for Electronic Health Records</title>
<link>https://arxiv.org/abs/2506.15118</link>
<guid>https://arxiv.org/abs/2506.15118</guid>
<content:encoded><![CDATA[
<div> Knowledge Distillation, Disease Prediction, Electronic Health Records, Clinical Knowledge, Efficiency <br />
Summary: <br />
The study introduces the CKD-EHR framework for disease prediction using Electronic Health Records (EHR). By leveraging knowledge distillation techniques, the framework enhances accuracy and efficiency in clinical deployment. The large language model Qwen2.5-7B is fine-tuned on medical data to act as a teacher model and generates interpretable labels through attention distillation. This knowledge is then transferred to a lightweight BERT student model. Experimental results on the MIMIC-III dataset show significant performance improvements compared to baseline models, with a 9% increase in diagnostic accuracy, 27% improvement in F1-score, and a remarkable 22.2 times speedup in inference. The innovation not only optimizes resource utilization but also enhances diagnosis accuracy and timeliness, providing a practical solution for clinical settings. The code and data for this research are available on GitHub. <br /> <div>
arXiv:2506.15118v1 Announce Type: new 
Abstract: Electronic Health Records (EHR)-based disease prediction models have demonstrated significant clinical value in promoting precision medicine and enabling early intervention. However, existing large language models face two major challenges: insufficient representation of medical knowledge and low efficiency in clinical deployment. To address these challenges, this study proposes the CKD-EHR (Clinical Knowledge Distillation for EHR) framework, which achieves efficient and accurate disease risk prediction through knowledge distillation techniques. Specifically, the large language model Qwen2.5-7B is first fine-tuned on medical knowledge-enhanced data to serve as the teacher model.It then generates interpretable soft labels through a multi-granularity attention distillation mechanism. Finally, the distilled knowledge is transferred to a lightweight BERT student model. Experimental results show that on the MIMIC-III dataset, CKD-EHR significantly outperforms the baseline model:diagnostic accuracy is increased by 9%, F1-score is improved by 27%, and a 22.2 times inference speedup is achieved. This innovative solution not only greatly improves resource utilization efficiency but also significantly enhances the accuracy and timeliness of diagnosis, providing a practical technical approach for resource optimization in clinical settings. The code and data for this research are available athttps://github.com/209506702/CKD_EHR.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs</title>
<link>https://arxiv.org/abs/2506.15131</link>
<guid>https://arxiv.org/abs/2506.15131</guid>
<content:encoded><![CDATA[
<div> modeling, Open-domain Dialogue, Multi-Response Generation, Preference-based Selection, response diversity 

Summary: 
This work focuses on enhancing response diversity in Open-domain Dialogue (OD) by modeling the one-to-many (o2m) property of dialogue using a two-stage framework. The framework involves Multi-Response Generation (MRG) and Preference-based Selection (PS) tasks. A new dialogue corpus named o2mDial is introduced to capture the o2m property. Novel in-context learning and instruction-tuning strategies are proposed, along with evaluation metrics for MRG and a model-based approach for PS. Applying this framework to smaller Language Model-based dialogue agents improves response quality by up to 90% while maintaining contextual coherence. The results show that the proposed approach brings smaller models closer to the performance of larger models in terms of generating diverse and high-quality responses in Open-domain Dialogue. <div>
arXiv:2506.15131v1 Announce Type: new 
Abstract: Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby multiple appropriate responses exist for a single dialogue context. Despite prior research showing that modeling this property boosts response diversity, most modern LLM-based dialogue agents do not explicitly do so. In this work, we model the o2m property of OD in LLMs by decomposing OD generation into two key tasks: Multi-Response Generation (MRG) and Preference-based Selection (PS), which entail generating a set of n semantically and lexically diverse high-quality responses for a given dialogue context, followed by selecting a single response based on human preference, respectively. To facilitate MRG and PS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the o2m property by featuring multiple plausible responses for each context. Leveraging o2mDial, we propose new in-context learning and instruction-tuning strategies, as well as novel evaluation metrics for MRG, alongside a model-based approach for PS. Empirical results demonstrate that applying the proposed two-stage framework to smaller LLMs for OD generation enhances overall response diversity while maintaining contextual coherence, improving response quality by up to 90%, bringing them closer to the performance of larger models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models</title>
<link>https://arxiv.org/abs/2506.15138</link>
<guid>https://arxiv.org/abs/2506.15138</guid>
<content:encoded><![CDATA[
<div> tokenizer, Korean, Thunder-Tok, linguistic structure, token fertility <br />
<br />
Summary: 
The paper introduces Thunder-Tok, a Korean tokenizer designed to reduce token fertility while maintaining model performance. It utilizes a rule-based pre-tokenization method aligned with the Korean language's structure and a seed vocabulary with linguistic unit-like tokens. A branching entropy-based selection algorithm is employed to increase the average token length and decrease fertility by 10% compared to BPE, enhancing inference speed. Experimental results show that Thunder-Tok maintains performance across various tasks, showcasing the effectiveness of its linguistically informed approach in developing efficient language model tokenizers. <div>
arXiv:2506.15138v1 Announce Type: new 
Abstract: This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce token fertility without compromising model performance. Our approach uses a rule-based pre-tokenization method that aligns with the linguistic structure of the Korean language. We also create a seed vocabulary containing tokens that resemble linguistic units and employ a branching entropy-based selection algorithm. These techniques increase the average token length, thus lowering fertility while preserving linguistic information. Experimental results indicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces the number of tokens by 10%, improving the inference speed by 10%) compared to BPE without compromising performance across various downstream tasks. These findings demonstrate that our linguistically informed approach is effective and practical for designing efficient tokenizers for language models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of View</title>
<link>https://arxiv.org/abs/2506.15156</link>
<guid>https://arxiv.org/abs/2506.15156</guid>
<content:encoded><![CDATA[
<div> memory, state-space language models, primacy effects, recency effects, information retention

Summary:
Memory mechanisms in state-space language models were explored using primacy and recency effects. The study revealed a U-shaped accuracy profile, indicating strong performance at the beginning and end of input sequences. Long-term memory was supported by a sparse subset of channels that encoded early input tokens, while short-term memory was governed by delta-modulated recurrence. The recency advantage decreased when distractor items were introduced, showing a limit to memory depth. Dynamic modulation of memory allocation by semantic regularity was observed, with repeated relations in the input sequence affecting memory tendencies. These findings were validated through ablations and input perturbations on two large-scale Mamba-based language models, one with 1.4B and another with 7B parameters.<br /><br />Summary: <div>
arXiv:2506.15156v1 Announce Type: new 
Abstract: We study memory in state-space language models using primacy and recency effects as behavioral tools to uncover how information is retained and forgotten over time. Applying structured recall tasks to the Mamba architecture, we observe a consistent U-shaped accuracy profile, indicating strong performance at the beginning and end of input sequences. We identify three mechanisms that give rise to this pattern. First, long-term memory is supported by a sparse subset of channels within the model's selective state space block, which persistently encode early input tokens and are causally linked to primacy effects. Second, short-term memory is governed by delta-modulated recurrence: recent inputs receive more weight due to exponential decay, but this recency advantage collapses when distractor items are introduced, revealing a clear limit to memory depth. Third, we find that memory allocation is dynamically modulated by semantic regularity: repeated relations in the input sequence shift the delta gating behavior, increasing the tendency to forget intermediate items. We validate these findings via targeted ablations and input perturbations on two large-scale Mamba-based language models: one with 1.4B and another with 7B parameters.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals</title>
<link>https://arxiv.org/abs/2506.15208</link>
<guid>https://arxiv.org/abs/2506.15208</guid>
<content:encoded><![CDATA[
<div> Keywords: Sustainable Development Goals, text classification, large language models, Zero-Shot Learning, Few-Shot Learning

Summary: 
This study explores the use of large language models (LLMs) for text classification related to the United Nations' Sustainable Development Goals (SDGs). Various LLMs, both proprietary and open-source, were analyzed for their effectiveness in this task. The study also evaluates task adaptation techniques such as Zero-Shot and Few-Shot Learning, as well as Fine-Tuning within the context of SDGs. The results indicate that smaller models optimized through prompt engineering can perform comparably to larger models like OpenAI's GPT. This research highlights the importance of leveraging LLMs for automating the analysis of vast amounts of text data related to sustainability goals, facilitating progress tracking towards achieving the SDGs by 2030. The study showcases the potential of in-context learning approaches in optimizing text classification models for specific domains like sustainable development. <br /><br />Summary: <div>
arXiv:2506.15208v1 Announce Type: new 
Abstract: In 2012, the United Nations introduced 17 Sustainable Development Goals (SDGs) aimed at creating a more sustainable and improved future by 2030. However, tracking progress toward these goals is difficult because of the extensive scale and complexity of the data involved. Text classification models have become vital tools in this area, automating the analysis of vast amounts of text from a variety of sources. Additionally, large language models (LLMs) have recently proven indispensable for many natural language processing tasks, including text classification, thanks to their ability to recognize complex linguistic patterns and semantics. This study analyzes various proprietary and open-source LLMs for a single-label, multi-class text classification task focused on the SDGs. Then, it also evaluates the effectiveness of task adaptation techniques (i.e., in-context learning approaches), namely Zero-Shot and Few-Shot Learning, as well as Fine-Tuning within this domain. The results reveal that smaller models, when optimized through prompt engineering, can perform on par with larger models like OpenAI's GPT (Generative Pre-trained Transformer).
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2506.15211</link>
<guid>https://arxiv.org/abs/2506.15211</guid>
<content:encoded><![CDATA[
<div> generalization, reasoning, prototypes, ProtoReasoning, cross-domain<br />
Summary:<br />
The article introduces ProtoReasoning, a framework that enhances the reasoning ability of Large Reasoning Models (LRMs) by utilizing shared abstract reasoning prototypes. These prototypes capture fundamental reasoning patterns across domains, allowing for improved generalization capabilities. ProtoReasoning features an automated prototype construction pipeline and a verification system using Prolog for logical reasoning and PDDL for planning tasks. Experimental results demonstrate significant improvements on logical reasoning, planning tasks, general reasoning, and mathematics. Ablation studies confirm that learning in prototype space leads to enhanced generalization to structurally similar problems compared to natural language representations alone. This validates the hypothesis that reasoning prototypes form the basis for generalizable reasoning in large language models.<br /> <div>
arXiv:2506.15211v1 Announce Type: new 
Abstract: Recent advances in Large Reasoning Models (LRMs) trained with Long Chain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain generalization capabilities. However, the underlying mechanisms supporting such transfer remain poorly understood. We hypothesize that cross-domain generalization arises from shared abstract reasoning prototypes -- fundamental reasoning patterns that capture the essence of problems across domains. These prototypes minimize the nuances of the representation, revealing that seemingly diverse tasks are grounded in shared reasoning structures.Based on this hypothesis, we propose ProtoReasoning, a framework that enhances the reasoning ability of LLMs by leveraging scalable and verifiable prototypical representations (Prolog for logical reasoning, PDDL for planning).ProtoReasoning features: (1) an automated prototype construction pipeline that transforms problems into corresponding prototype representations; (2) a comprehensive verification system providing reliable feedback through Prolog/PDDL interpreters; (3) the scalability to synthesize problems arbitrarily within prototype space while ensuring correctness. Extensive experiments show that ProtoReasoning achieves 4.7% improvement over baseline models on logical reasoning (Enigmata-Eval), 6.3% improvement on planning tasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics (AIME24). Significantly, our ablation studies confirm that learning in prototype space also demonstrates enhanced generalization to structurally similar problems compared to training solely on natural language representations, validating our hypothesis that reasoning prototypes serve as the foundation for generalizable reasoning in large language models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs</title>
<link>https://arxiv.org/abs/2506.15215</link>
<guid>https://arxiv.org/abs/2506.15215</guid>
<content:encoded><![CDATA[
<div> Keywords: open-ended question answering, large language models, evaluation metrics, factoid questions, non-factoid questions

Summary: 
MinosEval is a novel evaluation method for open-ended question answering tasks that addresses the challenges of capturing semantic similarities and providing interpretable results. It distinguishes between factoid and non-factoid questions and ranks candidate answers accordingly. For factoid questions, it uses an adaptive key-point scoring strategy, while for non-factoid questions, it employs an instance-aware listwise ranking strategy. The method has been tested on multiple datasets and has shown to align better with human annotations and provide more interpretable results compared to traditional evaluation metrics. MinosEval offers a promising approach for evaluating the capabilities of large language models in handling open-ended QA tasks. 

<br /><br />Summary: <div>
arXiv:2506.15215v1 Announce Type: new 
Abstract: Open-ended question answering (QA) is a key task for evaluating the capabilities of large language models (LLMs). Compared to closed-ended QA, it demands longer answer statements, more nuanced reasoning processes, and diverse expressions, making refined and interpretable automatic evaluation both crucial and challenging. Traditional metrics like ROUGE and BERTScore struggle to capture semantic similarities due to different patterns between model responses and reference answers. Current LLM-based evaluation approaches, such as pairwise or listwise comparisons of candidate answers, lack intuitive interpretability. While pointwise scoring of each response provides some descriptions, it fails to adapt across different question contents. Most notably, existing methods overlook the distinction between factoid and non-factoid questions. To address these challenges, we propose \textbf{MinosEval}, a novel evaluation method that first distinguishes open-ended questions and then ranks candidate answers using different evaluation strategies. For factoid questions, it applies an adaptive key-point scoring strategy, while for non-factoid questions, it uses an instance-aware listwise ranking strategy. Experiments on multiple open-ended QA datasets, including self-built ones with more candidate responses to complement community resources, show that MinosEval better aligns with human annotations and offers more interpretable results.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants</title>
<link>https://arxiv.org/abs/2506.15239</link>
<guid>https://arxiv.org/abs/2506.15239</guid>
<content:encoded><![CDATA[
<div> Keywords: Basque, Spanish, Natural Language Inference, Language Technologies, Linguistic Variation

Summary:
In this study, the researchers assessed the capability of current language technologies to comprehend Basque and Spanish language varieties through the Natural Language Inference (NLI) task. They created a parallel dataset in Basque and Spanish, including their variants, and conducted crosslingual and in-context learning experiments using Large Language Models (LLMs). The results revealed a decrease in performance when dealing with linguistic variation, especially in Basque, indicating that the decline stemmed from the variation itself rather than lexical overlap. Ablation experiments highlighted the struggles of encoder-only models, particularly with Western Basque, aligning with linguistic theory that peripheral dialects like Western Basque are more distant from the standard. The study's data and code are publicly accessible. 

<br /><br />Summary: <div>
arXiv:2506.15239v1 Announce Type: new 
Abstract: In this paper, we evaluate the capacity of current language technologies to understand Basque and Spanish language varieties. We use Natural Language Inference (NLI) as a pivot task and introduce a novel, manually-curated parallel dataset in Basque and Spanish, along with their respective variants. Our empirical analysis of crosslingual and in-context learning experiments using encoder-only and decoder-based Large Language Models (LLMs) shows a performance drop when handling linguistic variation, especially in Basque. Error analysis suggests that this decline is not due to lexical overlap, but rather to the linguistic variation itself. Further ablation experiments indicate that encoder-only models particularly struggle with Western Basque, which aligns with linguistic theory that identifies peripheral dialects (e.g., Western) as more distant from the standard. All data and code are publicly available.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on Graph-Retrieval Augmented Generation Based on Historical Text Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.15241</link>
<guid>https://arxiv.org/abs/2506.15241</guid>
<content:encoded><![CDATA[
<div> Graph RAG framework, historical text analysis, computational humanities, AIGC technology, character relationship dataset <br />
<br />
Summary: This article introduces the Graph RAG framework to address domain knowledge gaps in large language models for historical text analysis in computational humanities and AIGC technology. The framework combines chain-of-thought prompting, self-instruction generation, and process supervision to create a character relationship dataset for historical knowledge extraction with minimal manual annotation. By integrating knowledge graphs and retrieval-augmented generation in the graph-augmented generation phase, the model Xunzi-Qwen1.5-14B achieves optimal performance in relation extraction. The DeepSeek model, integrated with GraphRAG, significantly improves F1 score on the C-CLUE relation extraction dataset, surpassing the baseline model's performance. This framework offers an efficient solution for classical text knowledge extraction, enhancing historical knowledge services and advancing humanities research. <div>
arXiv:2506.15241v1 Announce Type: new 
Abstract: This article addresses domain knowledge gaps in general large language models for historical text analysis in the context of computational humanities and AIGC technology. We propose the Graph RAG framework, combining chain-of-thought prompting, self-instruction generation, and process supervision to create a The First Four Histories character relationship dataset with minimal manual annotation. This dataset supports automated historical knowledge extraction, reducing labor costs. In the graph-augmented generation phase, we introduce a collaborative mechanism between knowledge graphs and retrieval-augmented generation, improving the alignment of general models with historical knowledge. Experiments show that the domain-specific model Xunzi-Qwen1.5-14B, with Simplified Chinese input and chain-of-thought prompting, achieves optimal performance in relation extraction (F1 = 0.68). The DeepSeek model integrated with GraphRAG improves F1 by 11% (0.08-0.19) on the open-domain C-CLUE relation extraction dataset, surpassing the F1 value of Xunzi-Qwen1.5-14B (0.12), effectively alleviating hallucinations phenomenon, and improving interpretability. This framework offers a low-resource solution for classical text knowledge extraction, advancing historical knowledge services and humanities research.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TopClustRAG at SIGIR 2025 LiveRAG Challenge</title>
<link>https://arxiv.org/abs/2506.15246</link>
<guid>https://arxiv.org/abs/2506.15246</guid>
<content:encoded><![CDATA[
<div> Keywords: TopClustRAG, retrieval-augmented generation, clustering, large language model, answer diversity

Summary:
TopClustRAG is a retrieval-augmented generation system designed for the LiveRAG Challenge, focusing on end-to-end question answering using web corpora. The system uses a mix of sparse and dense indices for retrieval, followed by K-Means clustering to group semantically similar passages. Representative passages from each cluster generate prompts for a large language model, improving answer relevance, diversity, and faithfulness to evidence. The multi-stage pipeline includes filtering, reranking, and synthesis of answers. In evaluation on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in faithfulness and 7th in correctness on the official leaderboard. This showcases the effectiveness of clustering-based context filtering and prompt aggregation in enhancing the performance of large-scale retrieval-augmented generation systems.<br /><br />Summary: TopClustRAG excels in providing faithful and diverse answers by utilizing a hybrid retrieval strategy, clustering similar passages, and leveraging a large language model for generation. Its multi-stage pipeline enhances relevance and evidence fidelity in answering questions from large web corpora. <div>
arXiv:2506.15246v1 Announce Type: new 
Abstract: We present TopClustRAG, a retrieval-augmented generation (RAG) system developed for the LiveRAG Challenge, which evaluates end-to-end question answering over large-scale web corpora. Our system employs a hybrid retrieval strategy combining sparse and dense indices, followed by K-Means clustering to group semantically similar passages. Representative passages from each cluster are used to construct cluster-specific prompts for a large language model (LLM), generating intermediate answers that are filtered, reranked, and finally synthesized into a single, comprehensive response. This multi-stage pipeline enhances answer diversity, relevance, and faithfulness to retrieved evidence. Evaluated on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in faithfulness and 7th in correctness on the official leaderboard, demonstrating the effectiveness of clustering-based context filtering and prompt aggregation in large-scale RAG systems.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thunder-DeID: Accurate and Efficient De-identification Framework for Korean Court Judgments</title>
<link>https://arxiv.org/abs/2506.15266</link>
<guid>https://arxiv.org/abs/2506.15266</guid>
<content:encoded><![CDATA[
<div> Keywords: South Korean judiciary, de-identification, court judgments, personally identifiable information, deep neural network

Summary: 
The South Korean judiciary requires de-identification of court judgments for public disclosure, but the current process is not efficient for large-scale implementation. The legal definitions of personal identifiers are vague, posing challenges for technical solutions. To address these issues, the Thunder-DeID framework is proposed, incorporating a legal dataset, systematic categorization of Personally Identifiable Information (PII), and a deep neural network-based de-identification pipeline. The framework aligns with relevant laws and practices. Experimental results show state-of-the-art performance in de-identifying court judgments. <br /><br />Summary: <div>
arXiv:2506.15266v1 Announce Type: new 
Abstract: To ensure a balance between open access to justice and personal data protection, the South Korean judiciary mandates the de-identification of court judgments before they can be publicly disclosed. However, the current de-identification process is inadequate for handling court judgments at scale while adhering to strict legal requirements. Additionally, the legal definitions and categorizations of personal identifiers are vague and not well-suited for technical solutions. To tackle these challenges, we propose a de-identification framework called Thunder-DeID, which aligns with relevant laws and practices. Specifically, we (i) construct and release the first Korean legal dataset containing annotated judgments along with corresponding lists of entity mentions, (ii) introduce a systematic categorization of Personally Identifiable Information (PII), and (iii) develop an end-to-end deep neural network (DNN)-based de-identification pipeline. Our experimental results demonstrate that our model achieves state-of-the-art performance in the de-identification of court judgments.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment</title>
<link>https://arxiv.org/abs/2506.15301</link>
<guid>https://arxiv.org/abs/2506.15301</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, clinical trial recruitment, knowledge aggregation, patient data, evaluation frameworks <br /> 
Summary: 
This article discusses the limited adoption of Large Language Models (LLMs) in critical domains like clinical trial recruitment despite their success in general-domain NLP tasks. The task of matching trials and patients can benefit from LLMs' knowledge aggregation and reasoning abilities. However, current approaches are trial-specific, while LLMs could offer a more general solution due to their ability to consolidate distributed knowledge. Existing LLM-assisted methods in clinical trial recruitment often rely on proprietary models and lack strong evaluation benchmarks. The survey critically analyzes the task of trial-patient matching, addresses challenges in adopting LLM technologies in clinical research, and identifies future research directions for utilizing LLMs in clinical trial recruitment. <br /><br />Summary: <div>
arXiv:2506.15301v1 Announce Type: new 
Abstract: Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet, their adoption in critical domains, such as clinical trial recruitment, remains limited. As trials are designed in natural language and patient data is represented as both structured and unstructured text, the task of matching trials and patients benefits from knowledge aggregation and reasoning abilities of LLMs. Classical approaches are trial-specific and LLMs with their ability to consolidate distributed knowledge hold the potential to build a more general solution. Yet recent applications of LLM-assisted methods rely on proprietary models and weak evaluation benchmarks. In this survey, we are the first to analyze the task of trial-patient matching and contextualize emerging LLM-based approaches in clinical trial recruitment. We critically examine existing benchmarks, approaches and evaluation frameworks, the challenges to adopting LLM technologies in clinical research and exciting future directions.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConLID: Supervised Contrastive Learning for Low-Resource Language Identification</title>
<link>https://arxiv.org/abs/2506.15304</link>
<guid>https://arxiv.org/abs/2506.15304</guid>
<content:encoded><![CDATA[
<div> supervised contrastive learning, low-resource languages, language identification, multilingual LLM, domain-invariant representations <br />
Summary: 
The article introduces a novel approach called supervised contrastive learning (SCL) to address the challenges of low-resource languages in language identification (LID) models trained on diverse data sources. By focusing on domain-invariant representations, the proposed approach aims to improve LID performance specifically for low-resource languages that are typically limited to single-domain data like the Bible. Through extensive analysis, the study demonstrates a 3.2% enhancement in LID performance on out-of-domain data for low-resource languages. This highlights the effectiveness of SCL in mitigating class imbalance and bias issues commonly encountered in LID tasks. The findings suggest that leveraging SCL can lead to more robust and generalizable LID models capable of accurately identifying languages, even in low-resource settings. <br /><br />Summary: <div>
arXiv:2506.15304v1 Announce Type: new 
Abstract: Language identification (LID) is a critical step in curating multilingual LLM pretraining corpora from web crawls. While many studies on LID model training focus on collecting diverse training data to improve performance, low-resource languages -- often limited to single-domain data, such as the Bible -- continue to perform poorly. To resolve these class imbalance and bias issues, we propose a novel supervised contrastive learning (SCL) approach to learn domain-invariant representations for low-resource languages. Through an extensive analysis, we show that our approach improves LID performance on out-of-domain data for low-resource languages by 3.2%, demonstrating its effectiveness in enhancing LID models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeVisE: Behavioral Testing of Medical Large Language Models</title>
<link>https://arxiv.org/abs/2506.15339</link>
<guid>https://arxiv.org/abs/2506.15339</guid>
<content:encoded><![CDATA[
<div> Framework, clinical understanding, dataset, evaluation, language models<br />
Summary:<br />
The article introduces a behavioral testing framework called DeVisE for evaluating large language models (LLMs) used in clinical decision support. It creates a dataset of ICU discharge notes from MIMIC-IV with controlled counterfactuals targeting demographic and vital sign attributes. Five LLMs are evaluated, both in zero-shot and fine-tuned settings, to assess their input-level sensitivity and downstream reasoning. The results show that zero-shot models exhibit coherent counterfactual reasoning patterns, while fine-tuned models are more stable but less responsive to meaningful changes. Demographic factors consistently influence outputs, highlighting the importance of fairness-aware evaluation. This work demonstrates the utility of behavioral testing in understanding the reasoning strategies of clinical LLMs and improving the transparency and safety of medical AI systems. <br /> <div>
arXiv:2506.15339v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used in clinical decision support, yet current evaluation methods often fail to distinguish genuine medical reasoning from superficial patterns. We introduce DeVisE (Demographics and Vital signs Evaluation), a behavioral testing framework for probing fine-grained clinical understanding. We construct a dataset of ICU discharge notes from MIMIC-IV, generating both raw (real-world) and template-based (synthetic) versions with controlled single-variable counterfactuals targeting demographic (age, gender, ethnicity) and vital sign attributes. We evaluate five LLMs spanning general-purpose and medically fine-tuned variants, under both zero-shot and fine-tuned settings. We assess model behavior via (1) input-level sensitivity - how counterfactuals alter the likelihood of a note; and (2) downstream reasoning - how they affect predicted hospital length-of-stay. Our results show that zero-shot models exhibit more coherent counterfactual reasoning patterns, while fine-tuned models tend to be more stable yet less responsive to clinically meaningful changes. Notably, demographic factors subtly but consistently influence outputs, emphasizing the importance of fairness-aware evaluation. This work highlights the utility of behavioral testing in exposing the reasoning strategies of clinical LLMs and informing the design of safer, more transparent medical AI systems.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models' Knowledge of Indian Culture</title>
<link>https://arxiv.org/abs/2506.15355</link>
<guid>https://arxiv.org/abs/2506.15355</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Models, Cultural Diversity, India, Benchmark, Dataset

Summary:
SANSKRITI is a benchmark dataset created to evaluate the comprehension of Indian cultural knowledge by Language Models (LMs). It consists of 21,853 question-answer pairs covering various aspects of India's cultural diversity across 28 states and 8 union territories. The dataset includes attributes such as rituals, history, cuisine, dance, festivals, religion, and more, providing a comprehensive overview of Indian culture. Testing leading Large Language Models (LLMs), Indic Language Models (ILMs), and Small Language Models (SLMs) on SANSKRITI reveals discrepancies in their ability to handle culturally nuanced queries, especially in region-specific contexts. The dataset aims to set a new standard for assessing and improving the cultural understanding of LMs, highlighting the importance of considering local socio-cultural contexts in language understanding tools. <br /><br />Summary: <div>
arXiv:2506.15355v1 Announce Type: new 
Abstract: Language Models (LMs) are indispensable tools shaping modern workflows, but their global effectiveness depends on understanding local socio-cultural contexts. To address this, we introduce SANSKRITI, a benchmark designed to evaluate language models' comprehension of India's rich cultural diversity. Comprising 21,853 meticulously curated question-answer pairs spanning 28 states and 8 union territories, SANSKRITI is the largest dataset for testing Indian cultural knowledge. It covers sixteen key attributes of Indian culture: rituals and ceremonies, history, tourism, cuisine, dance and music, costume, language, art, festivals, religion, medicine, transport, sports, nightlife, and personalities, providing a comprehensive representation of India's cultural tapestry. We evaluate SANSKRITI on leading Large Language Models (LLMs), Indic Language Models (ILMs), and Small Language Models (SLMs), revealing significant disparities in their ability to handle culturally nuanced queries, with many models struggling in region-specific contexts. By offering an extensive, culturally rich, and diverse dataset, SANSKRITI sets a new standard for assessing and improving the cultural understanding of LMs.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for Summarization and Headline Generation</title>
<link>https://arxiv.org/abs/2506.15372</link>
<guid>https://arxiv.org/abs/2506.15372</guid>
<content:encoded><![CDATA[
<div> dataset, Indian languages, multimodal, comment-aware, summarization<br />
<br />
Summary:<br />
The study introduces COSMMIC, a pioneering dataset in Indian languages for comment-sensitive multimodal and multilingual summarization. It includes article-image pairs and reader comments in nine major Indian languages, with ground-truth summaries provided. The approach enhances summaries by incorporating user feedback and insights. The study explores different configurations using article text, user comments, images, or a combination of all three. State-of-the-art language models like LLama3 and GPT-4 are used to evaluate the dataset's effectiveness. The study includes analyzing the most effective configurations for natural language generation tasks, such as identifying supportive comments, filtering out noise, and extracting insights from images. COSMMIC stands out for integrating text, images, and user feedback, bridging gaps in Indian language resources and advancing NLP research for inclusivity.<br /> 
Summary: <div>
arXiv:2506.15372v1 Announce Type: new 
Abstract: Despite progress in comment-aware multimodal and multilingual summarization for English and Chinese, research in Indian languages remains limited. This study addresses this gap by introducing COSMMIC, a pioneering comment-sensitive multimodal, multilingual dataset featuring nine major Indian languages. COSMMIC comprises 4,959 article-image pairs and 24,484 reader comments, with ground-truth summaries available in all included languages. Our approach enhances summaries by integrating reader insights and feedback. We explore summarization and headline generation across four configurations: (1) using article text alone, (2) incorporating user comments, (3) utilizing images, and (4) combining text, comments, and images. To assess the dataset's effectiveness, we employ state-of-the-art language models such as LLama3 and GPT-4. We conduct a comprehensive study to evaluate different component combinations, including identifying supportive comments, filtering out noise using a dedicated comment classifier using IndicBERT, and extracting valuable insights from images with a multilingual CLIP-based classifier. This helps determine the most effective configurations for natural language generation (NLG) tasks. Unlike many existing datasets that are either text-only or lack user comments in multimodal settings, COSMMIC uniquely integrates text, images, and user feedback. This holistic approach bridges gaps in Indian language resources, advancing NLP research and fostering inclusivity.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.15415</link>
<guid>https://arxiv.org/abs/2506.15415</guid>
<content:encoded><![CDATA[
<div> fine-tuning, lexical alignment, low-resource languages, cross-lingual, Swahili <br />
Summary: <br />
This paper introduces Targeted Lexical Injection (TLI) as a method to improve lexical alignment in Large Language Models (LLMs) for low-resource languages like Swahili. The study shows that a Swahili-centric LLM exhibits strong lexical alignment in its early internal layers but not in its final output representations. TLI leverages this by fine-tuning the model using Low-Rank Adaptation (LoRA) and a contrastive learning objective targeting embeddings from the optimal early layer. Experiments demonstrate that TLI significantly improves output-level lexical alignment for both trained and unseen word pairs, suggesting it enhances the model's ability to preserve and propagate its inherent cross-lingual knowledge. This approach offers a parameter-efficient strategy to enhance lexical alignment in LLMs focused on low-resource languages. <br /> <div>
arXiv:2506.15415v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their performance in low-resource languages (LRLs), such as Swahili, often lags due to data scarcity and underrepresentation in pre-training. A key challenge is achieving robust cross-lingual lexical alignment, crucial for tasks like translation and cross-lingual information retrieval. This paper introduces Targeted Lexical Injection (TLI), a novel and efficient fine-tuning approach. We first demonstrate that Lugha-Llama-8B-wura, a Swahili-centric LLM, exhibits strong, near-perfect lexical alignment for Swahili-English word pairs in its early internal layers (specifically Layer 2, with ~0.99998 average cosine similarity based on a pilot study), a capability not fully reflected in its final output representations (baseline ~0.32 similarity on our evaluation set). TLI leverages this insight by using Low-Rank Adaptation (LoRA) and a contrastive learning objective to fine-tune the model, specifically targeting embeddings from this empirically identified optimal early layer. Our experiments show that TLI significantly improves the output-level lexical alignment for 623 trained Swahili-English word pairs, increasing average cosine similarity from 0.3211 to 0.4113 (+28.08%, p < 1.33 x 10^-240). More importantly, these improvements generalize remarkably well to 63 unseen control word pairs, with similarity increasing from 0.3143 to 0.4033 (+28.32%, p < 7.17 x 10^-27). These findings suggest TLI enhances the model's ability to preserve and propagate its inherent early-layer cross-lingual knowledge, offering a parameter-efficient and effective strategy for improving lexical alignment in LRL-focused LLMs.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding GUI Agent Localization Biases through Logit Sharpness</title>
<link>https://arxiv.org/abs/2506.15425</link>
<guid>https://arxiv.org/abs/2506.15425</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, GUI agents, hallucinations, Peak Sharpness Score, Context-Aware Cropping

Summary:
Multimodal large language models (MLLMs) have improved GUI agents' ability to interact with operating systems by grounding language in spatial actions. However, these models often suffer from hallucinations, which are systematic localization errors that reduce reliability. This study introduces a detailed evaluation framework that categorizes model predictions into four types, revealing subtle failure modes. The Peak Sharpness Score (PSS) metric is proposed to assess model uncertainty by evaluating the alignment between semantic continuity and logits distribution in coordinate prediction. Additionally, Context-Aware Cropping, a training-free technique, is shown to enhance model performance by adaptively refining input context. Through extensive experiments, the framework and methods presented in this study offer valuable insights, improve interpretability, and enhance the robustness of GUI agent behavior. 

<br /><br />Summary: <div>
arXiv:2506.15425v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have enabled GUI agents to interact with operating systems by grounding language into spatial actions. Despite their promising performance, these models frequently exhibit hallucinations-systematic localization errors that compromise reliability. We propose a fine-grained evaluation framework that categorizes model predictions into four distinct types, revealing nuanced failure modes beyond traditional accuracy metrics. To better quantify model uncertainty, we introduce the Peak Sharpness Score (PSS), a metric that evaluates the alignment between semantic continuity and logits distribution in coordinate prediction. Building on this insight, we further propose Context-Aware Cropping, a training-free technique that improves model performance by adaptively refining input context. Extensive experiments demonstrate that our framework and methods provide actionable insights and enhance the interpretability and robustness of GUI agent behavior.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent System Need</title>
<link>https://arxiv.org/abs/2506.15451</link>
<guid>https://arxiv.org/abs/2506.15451</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, multi-agent systems, divide-and-conquer architecture, adaptive collaboration engine, agent organization optimization<br />
<br />
Summary: <br />
AgentGroupChat-V2 is a new framework designed to address challenges faced by current frameworks in building multi-agent systems based on large language models (LLMs). It introduces three core innovations: a divide-and-conquer fully parallel architecture that decomposes user queries into hierarchical structures for efficient processing, an adaptive collaboration engine that selects LLM combinations based on task characteristics, and optimization strategies for agent organization. The framework demonstrates superior performance across various domains, achieving high accuracy on tasks like GSM8K, AIME, and HumanEval. AgentGroupChat-V2 outperforms existing methods, particularly on complex Level 5 MATH problems, showcasing improvements above 11 percentage points compared to current baselines. The framework provides a comprehensive solution for efficiently building general-purpose LLM multi-agent systems, offering significant advantages in complex reasoning scenarios. <div>
arXiv:2506.15451v1 Announce Type: new 
Abstract: Large language model based multi-agent systems have demonstrated significant potential in social simulation and complex task resolution domains. However, current frameworks face critical challenges in system architecture design, cross-domain generalizability, and performance guarantees, particularly as task complexity and number of agents increases. We introduces AgentGroupChat-V2, a novel framework addressing these challenges through three core innovations: (1) a divide-and-conquer fully parallel architecture that decomposes user queries into hierarchical task forest structures enabling dependency management and distributed concurrent processing. (2) an adaptive collaboration engine that dynamically selects heterogeneous LLM combinations and interaction modes based on task characteristics. (3) agent organization optimization strategies combining divide-and-conquer approaches for efficient problem decomposition. Extensive experiments demonstrate AgentGroupChat-V2's superior performance across diverse domains, achieving 91.50% accuracy on GSM8K (exceeding the best baseline by 5.6 percentage points), 30.4% accuracy on competition-level AIME (nearly doubling other methods), and 79.20% pass@1 on HumanEval. Performance advantages become increasingly pronounced with higher task difficulty, particularly on Level 5 MATH problems where improvements exceed 11 percentage points compared to state-of-the-art baselines. These results confirm that AgentGroupChat-V2 provides a comprehensive solution for building efficient, general-purpose LLM multi-agent systems with significant advantages in complex reasoning scenarios. Code is available at https://github.com/MikeGu721/AgentGroupChat-V2.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2506.15455</link>
<guid>https://arxiv.org/abs/2506.15455</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reasoning benchmarks, RE-IMAGINE, hierarchy of reasoning, statistical recall

Summary: 
The paper introduces a framework called RE-IMAGINE to assess the hierarchy of reasoning ability in Large Language Models (LLMs). Inspired by the ladder of causation, the framework categorizes reasoning into associations, interventions, and counterfactuals. An automated pipeline generates problem variations at different levels of the hierarchy, ensuring that problems require true reasoning rather than statistical recall. The framework is versatile and can be applied across various domains like math, code, and logic. Testing on four benchmark datasets reveals that LLMs show reduced performance when faced with problem variations, indicating a reliance on memorization. This opens up avenues for further research to enhance reasoning skills in LLMs and move towards true reasoning capabilities.<br /><br />Summary: <div>
arXiv:2506.15455v1 Announce Type: new 
Abstract: Recent Large Language Models (LLMs) have reported high accuracy on reasoning benchmarks. However, it is still unclear whether the observed results arise from true reasoning or from statistical recall of the training set. Inspired by the ladder of causation (Pearl, 2009) and its three levels (associations, interventions and counterfactuals), this paper introduces RE-IMAGINE, a framework to characterize a hierarchy of reasoning ability in LLMs, alongside an automated pipeline to generate problem variations at different levels of the hierarchy. By altering problems in an intermediate symbolic representation, RE-IMAGINE generates arbitrarily many problems that are not solvable using memorization alone. Moreover, the framework is general and can work across reasoning domains, including math, code, and logic. We demonstrate our framework on four widely-used benchmarks to evaluate several families of LLMs, and observe reductions in performance when the models are queried with problem variations. These assessments indicate a degree of reliance on statistical recall for past performance, and open the door to further research targeting skills across the reasoning hierarchy.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Informed Grounding Supervision</title>
<link>https://arxiv.org/abs/2506.15480</link>
<guid>https://arxiv.org/abs/2506.15480</guid>
<content:encoded><![CDATA[
<div> supervised learning, large language models, grounded generation, external knowledge, contextual information

Summary:
CINGS is a post-training supervision method that improves grounding in large language models by training the model with relevant context prepended to the response. The loss is computed over response tokens only, masking out the context. Experiment results show that CINGS-trained models exhibit stronger grounding in textual and visual domains compared to standard methods. In textual tasks, CINGS outperforms other techniques across 11 datasets and complements inference-time grounding. In vision-language tasks, using a CINGS-trained model reduces hallucinations and maintains factual consistency in responses without degrading overall performance. The enhanced grounding in CINGS is achieved by inducing a shift in the model's behavior towards greater reliance on external context. <div>
arXiv:2506.15480v1 Announce Type: new 
Abstract: Large language models (LLMs) are often supplemented with external knowledge to provide information not encoded in their parameters or to reduce hallucination. In such cases, we expect the model to generate responses by grounding its response in the provided external context. However, prior work has shown that simply appending context at inference time does not ensure grounded generation. To address this, we propose Context-INformed Grounding Supervision (CINGS), a post-training supervision in which the model is trained with relevant context prepended to the response, while computing the loss only over the response tokens and masking out the context. Our experiments demonstrate that models trained with CINGS exhibit stronger grounding in both textual and visual domains compared to standard instruction-tuned models. In the text domain, CINGS outperforms other training methods across 11 information-seeking datasets and is complementary to inference-time grounding techniques. In the vision-language domain, replacing a vision-language model's LLM backbone with a CINGS-trained model reduces hallucinations across four benchmarks and maintains factual consistency throughout the generated response. This improved grounding comes without degradation in general downstream performance. Finally, we analyze the mechanism underlying the enhanced grounding in CINGS and find that it induces a shift in the model's prior knowledge and behavior, implicitly encouraging greater reliance on the external context.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling</title>
<link>https://arxiv.org/abs/2506.15498</link>
<guid>https://arxiv.org/abs/2506.15498</guid>
<content:encoded><![CDATA[
<div> Keywords: Process supervision, Large Language Models, Single-Pass Annotation, Reference-guided evaluation, Mathematical reasoning

Summary: 
The article introduces the SPARE framework for automated process annotation, enabling single-pass, reference-guided evaluation for complex reasoning tasks. By aligning solution steps to reference solutions, SPARE improves reasoning performance in mathematical reasoning, question answering, and spatial reasoning tasks. Compared to baseline methods, SPARE enhances model fine-tuning for inference and training reward models for output ranking. It achieves competitive results on mathematical datasets while being more efficient, requiring only 38% of the runtime compared to tree search-based annotation. The codebase and trained SPARE-PRM model are publicly available for further research and reproducibility. 

<br /><br />Summary: <div>
arXiv:2506.15498v1 Announce Type: new 
Abstract: Process or step-wise supervision has played a crucial role in advancing complex multi-step reasoning capabilities of Large Language Models (LLMs). However, efficient, high-quality automated process annotation remains a significant challenge. To address this, we introduce Single-Pass Annotation with Reference-Guided Evaluation (SPARE), a novel structured framework that enables single-pass, per-step annotation by aligning each solution step to one or multiple steps in a reference solution, accompanied by explicit reasoning for evaluation. We show that reference-guided step-level evaluation effectively facilitates process supervision on four datasets spanning three domains: mathematical reasoning, multi-hop compositional question answering, and spatial reasoning. We demonstrate that SPARE, when compared to baselines, improves reasoning performance when used for: (1) fine-tuning models in an offline RL setup for inference-time greedy-decoding, and (2) training reward models for ranking/aggregating multiple LLM-generated outputs. Additionally, SPARE achieves competitive performance on challenging mathematical datasets while offering 2.6 times greater efficiency, requiring only 38% of the runtime, compared to tree search-based automatic annotation. The codebase, along with a trained SPARE-PRM model, is publicly released to facilitate further research and reproducibility.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Hyperbole and Metaphor Detection with Their Bidirectional Dynamic Interaction and Emotion Knowledge</title>
<link>https://arxiv.org/abs/2506.15504</link>
<guid>https://arxiv.org/abs/2506.15504</guid>
<content:encoded><![CDATA[
<div> Framework, Emotion, Hyperbole, Metaphor, Detection
<br />
Summary: 
The article introduces EmoBi, an emotion-guided framework for hyperbole and metaphor detection in text. EmoBi incorporates emotion analysis to understand the emotional connotations behind these rhetorical devices, identifies target and source domains for deeper meaning comprehension, and facilitates bidirectional dynamic interaction to enhance detection. A verification mechanism ensures accuracy and reliability. Experiments demonstrate EmoBi's superiority over baseline methods on four datasets, with a significant increase in F1 scores for hyperbole and metaphor detection. The results validate the effectiveness and potential of the EmoBi approach for advancing the detection of hyperbole and metaphor in natural language processing tasks. <div>
arXiv:2506.15504v1 Announce Type: new 
Abstract: Text-based hyperbole and metaphor detection are of great significance for natural language processing (NLP) tasks. However, due to their semantic obscurity and expressive diversity, it is rather challenging to identify them. Existing methods mostly focus on superficial text features, ignoring the associations of hyperbole and metaphor as well as the effect of implicit emotion on perceiving these rhetorical devices. To implement these hypotheses, we propose an emotion-guided hyperbole and metaphor detection framework based on bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis module deeply mines the emotion connotations behind hyperbole and metaphor. Next, the emotion-based domain mapping module identifies the target and source domains to gain a deeper understanding of the implicit meanings of hyperbole and metaphor. Finally, the bidirectional dynamic interaction module enables the mutual promotion between hyperbole and metaphor. Meanwhile, a verification mechanism is designed to ensure detection accuracy and reliability. Experiments show that EmoBi outperforms all baseline methods on four datasets. Specifically, compared to the current SoTA, the F1 score increased by 28.1% for hyperbole detection on the TroFi dataset and 23.1% for metaphor detection on the HYPO-L dataset. These results, underpinned by in-depth analyses, underscore the effectiveness and potential of our approach for advancing hyperbole and metaphor detection.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lessons from Training Grounded LLMs with Verifiable Rewards</title>
<link>https://arxiv.org/abs/2506.15522</link>
<guid>https://arxiv.org/abs/2506.15522</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reinforcement learning, grounding, citation-based, reasoning

Summary:
This paper focuses on improving the grounding and trustworthiness of responses generated by large language models (LLMs). The authors explore the use of reinforcement learning (RL) and internal reasoning to enhance grounding in LLMs, using the GRPO method to train models without requiring costly annotations. Through experiments on various QA tasks, they demonstrate that reasoning-augmented models outperform instruction-only variants, particularly in handling unanswerable queries and generating well-cited responses. A two-stage training approach further enhances grounding by optimizing answer and citation behavior followed by refusal quality. Additionally, combining instruction tuning via GPT-4 distillation with GRPO improves performance on generative QA tasks. Overall, the study underscores the importance of reasoning, stage-wise optimization, and outcome-driven RL in building more verifiable and reliable large language models. 

<br /><br />Summary: <div>
arXiv:2506.15522v1 Announce Type: new 
Abstract: Generating grounded and trustworthy responses remains a key challenge for large language models (LLMs). While retrieval-augmented generation (RAG) with citation-based grounding holds promise, instruction-tuned models frequently fail even in straightforward scenarios: missing explicitly stated answers, citing incorrectly, or refusing when evidence is available. In this work, we explore how reinforcement learning (RL) and internal reasoning can enhance grounding in LLMs. We use the GRPO (Group Relative Policy Optimization) method to train models using verifiable outcome-based rewards targeting answer correctness, citation sufficiency, and refusal quality, without requiring gold reasoning traces or expensive annotations. Through comprehensive experiments across ASQA, QAMPARI, ELI5, and ExpertQA we show that reasoning-augmented models significantly outperform instruction-only variants, especially in handling unanswerable queries and generating well-cited responses. A two-stage training setup, first optimizing answer and citation behavior and then refusal, further improves grounding by stabilizing the learning signal. Additionally, we revisit instruction tuning via GPT-4 distillation and find that combining it with GRPO enhances performance on long-form, generative QA tasks. Overall, our findings highlight the value of reasoning, stage-wise optimization, and outcome-driven RL for building more verifiable and reliable LLMs.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RATTENTION: Towards the Minimal Sliding Window Size in Local-Global Attention Models</title>
<link>https://arxiv.org/abs/2506.15545</link>
<guid>https://arxiv.org/abs/2506.15545</guid>
<content:encoded><![CDATA[
<div> local-global attention models, window size, RATTENTION, efficiency gains, performance

Summary:<br />
- Local-global attention models offer efficiency gains but struggle with the window size choice.
- RATTENTION integrates local attention with linear attention for out-of-window token information.
- RATTENTION achieves a superior Pareto tradeoff at a window size of just 512.
- RATTENTION matches full-attention model performance and enhances long-context performance on RULER benchmark.
- RATTENTION maintains training speeds comparable to existing state-of-the-art approaches. 

Summary: <div>
arXiv:2506.15545v1 Announce Type: new 
Abstract: Local-global attention models have recently emerged as compelling alternatives to standard Transformers, promising improvements in both training and inference efficiency. However, the crucial choice of window size presents a Pareto tradeoff: larger windows maintain performance akin to full attention but offer minimal efficiency gains in short-context scenarios, while smaller windows can lead to performance degradation. Current models, such as Gemma2 and Mistral, adopt conservative window sizes (e.g., 4096 out of an 8192 pretraining length) to preserve performance. This work investigates strategies to shift this Pareto frontier, enabling local-global models to achieve efficiency gains even in short-context regimes. Our core motivation is to address the intrinsic limitation of local attention -- its complete disregard for tokens outside the defined window. We explore RATTENTION, a variant of local attention integrated with a specialized linear attention mechanism designed to capture information from these out-of-window tokens. Pretraining experiments at the 3B and 12B scales demonstrate that RATTENTION achieves a superior Pareto tradeoff between performance and efficiency. As a sweet spot, RATTENTION with a window size of just 512 consistently matches the performance of full-attention models across diverse settings. Furthermore, the recurrent nature inherent in the linear attention component of RATTENTION contributes to enhanced long-context performance, as validated on the RULER benchmark. Crucially, these improvements do not compromise training efficiency; thanks to a specialized kernel implementation and the reduced window size, RATTENTION maintains training speeds comparable to existing state-of-the-art approaches.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximating Language Model Training Data from Weights</title>
<link>https://arxiv.org/abs/2506.15553</link>
<guid>https://arxiv.org/abs/2506.15553</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, data approximation, gradient-based approach, text corpus, model performance

Summary:<br /><br />
The article addresses the challenge of approximating training data from model weights in modern language models. Various baselines and metrics are proposed to tackle this problem. A gradient-based method is introduced, which effectively identifies high-matching data from a public text corpus based on model weights. Even without knowledge of the true training data, this approach can pinpoint a small subset of public web documents that can be used to train a model to achieve performance close to the original. For instance, on the AG News classification task, the method boosts performance from 65% to 80%, closing in on the expert benchmark of 88%. When applied to a model trained with supervised-finetuning on MSMARCO web documents, the approach significantly reduces perplexity from 3.3 to 2.3, nearing the expert LLAMA model's perplexity of 2.0. <div>
arXiv:2506.15553v1 Announce Type: new 
Abstract: Modern language models often have open weights but closed training data. We formalize the problem of data approximation from model weights and propose several baselines and metrics. We develop a gradient-based approach that selects the highest-matching data from a large public text corpus and show its effectiveness at recovering useful data given only weights of the original and finetuned models. Even when none of the true training data is known, our method is able to locate a small subset of public Web documents can be used to train a model to close to the original model performance given models trained for both classification and supervised-finetuning. On the AG News classification task, our method improves performance from 65% (using randomly selected data) to 80%, approaching the expert benchmark of 88%. When applied to a model trained with SFT on MSMARCO web documents, our method reduces perplexity from 3.3 to 2.3, compared to an expert LLAMA model's perplexity of 2.0.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction</title>
<link>https://arxiv.org/abs/2506.15556</link>
<guid>https://arxiv.org/abs/2506.15556</guid>
<content:encoded><![CDATA[
<div> Latency, Large Language Models, Predictive Generation, TTS systems, Speculative Decoding
Summary:
<br /><br />Large Language Models (LLMs) used in voice chat applications often result in latency between user input and audio output. The delay is mainly due to the time taken for LLMs to generate the first sentence required for TTS systems. To address this, a new framework called Predictive Generation (PredGen) is proposed. PredGen performs speculative decoding during user input, generating candidate responses to reduce delays in TTS processing. Experiments on Lmsys and MT-Bench datasets demonstrate that PredGen can cut latency by approximately 2x across various scenarios with minimal additional computation cost. This approach optimizes user experiences by minimizing the delay between user input and audio output in single-user voice assistants on consumer-grade hardware. 
<br /><br />Summary: <div>
arXiv:2506.15556v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are widely used in real-time voice chat applications, typically in combination with text-to-speech (TTS) systems to generate audio responses. However, their large size often leads to noticeable latency between the end of user input and the start of audio output, resulting in suboptimal user experiences. This latency is particularly evident when LLMs are deployed as single-user voice assistants on consumer-grade hardware with limited computing capacity. We discovered that this latency is primarily dominated by the time it takes for the LLMs to generate the first sentence, which is required as input by the TTS systems that synthesize audio responses on a sentence-by-sentence basis. To address this bottleneck, we propose Predictive Generation (PredGen), a novel framework that mitigates-or even eliminates-this delay through speculative decoding at input time. PredGen generates candidate responses while the user is still speaking, enabling the system to begin TTS processing with minimal delay. Simulated experiments on the Lmsys and MT-Bench datasets show that the proposed method can effectively reduce the latency by around 2x across a wide range of use cases, while incurring only minimal additional computation cost at input time-computation that would otherwise go unused.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for Evaluating Gender Diversity in Large Language Models</title>
<link>https://arxiv.org/abs/2506.15568</link>
<guid>https://arxiv.org/abs/2506.15568</guid>
<content:encoded><![CDATA[
<div> gender inclusivity, large language models, fairness evaluation, bias identification, generative models<br />
Summary:<br />
This study evaluates gender fairness in large language models (LLMs) through the Gender Inclusivity Fairness Index (GIFI), a metric assessing inclusivity of binary and non-binary genders. The GIFI probes LLMs on gender pronouns and cognitive behaviors across various gender identifiers. 22 LLMs are extensively evaluated, revealing significant variations in gender inclusivity. The study emphasizes the need to enhance LLMs' inclusivity and sets a benchmark for future advancements in gender fairness in generative models.<br /> <div>
arXiv:2506.15568v1 Announce Type: new 
Abstract: We present a comprehensive evaluation of gender fairness in large language models (LLMs), focusing on their ability to handle both binary and non-binary genders. While previous studies primarily focus on binary gender distinctions, we introduce the Gender Inclusivity Fairness Index (GIFI), a novel and comprehensive metric that quantifies the diverse gender inclusivity of LLMs. GIFI consists of a wide range of evaluations at different levels, from simply probing the model with respect to provided gender pronouns to testing various aspects of model generation and cognitive behaviors under different gender assumptions, revealing biases associated with varying gender identifiers. We conduct extensive evaluations with GIFI on 22 prominent open-source and proprietary LLMs of varying sizes and capabilities, discovering significant variations in LLMs' gender inclusivity. Our study highlights the importance of improving LLMs' inclusivity, providing a critical benchmark for future advancements in gender fairness in generative models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciVer: Evaluating Foundation Models for Multimodal Scientific Claim Verification</title>
<link>https://arxiv.org/abs/2506.15569</link>
<guid>https://arxiv.org/abs/2506.15569</guid>
<content:encoded><![CDATA[
<div> benchmark, foundation models, multimodal scientific claim verification, expert-annotated examples, performance evaluation
<br />
Summary:
The article introduces SciVer, a benchmark designed to assess foundation models' ability to verify claims in multimodal scientific contexts. It includes 3,000 expert-annotated examples from 1,113 scientific papers, covering four common reasoning types. Each example includes expert-annotated supporting evidence for fine-grained evaluation. The study evaluates 21 state-of-the-art foundation models, highlighting a performance gap compared to human experts. An analysis of retrieval-augmented generation (RAG) and error evaluations identifies limitations in current open-source models. The findings offer insights to improve models' comprehension and reasoning in multimodal scientific literature tasks.
<br /><br />Summary: <div>
arXiv:2506.15569v1 Announce Type: new 
Abstract: We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiscoSG: Towards Discourse-Level Text Scene Graph Parsing through Iterative Graph Refinement</title>
<link>https://arxiv.org/abs/2506.15583</link>
<guid>https://arxiv.org/abs/2506.15583</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Discourse-level text Scene Graph parsing, DiscoSG-DS, PLMs, DiscoSG-Refiner

Summary: 
Discourse-level text Scene Graph parsing (DiscoSG) is introduced as a new task with the dataset DiscoSG-DS, consisting of multi-sentence caption-graph pairs for images. Existing methods for merging sentence-level parsing outputs often lead to fragmented graphs. Fine-tuning large PLMs on DiscoSG-DS significantly improves SPICE metrics compared to baseline methods. However, the use of large PLMs has limitations in terms of inference cost and licensing restrictions. The proposed DiscoSG-Refiner approach uses two Flan-T5-Base models to draft base graphs and iteratively propose edits, enhancing graph quality while reducing inference time significantly. DiscoSG-Refiner also enhances downstream VLM tasks such as discourse-level caption evaluation and hallucination detection. The code and data for this work are openly available for further research and development. 

<br /><br />Summary: <div>
arXiv:2506.15583v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) now generate discourse-level, multi-sentence visual descriptions, challenging text scene graph parsers originally designed for single-sentence caption-to-graph mapping. Current approaches typically merge sentence-level parsing outputs for discourse input, often missing phenomena like cross-sentence coreference, resulting in fragmented graphs and degraded downstream VLM task performance. To address this, we introduce a new task, Discourse-level text Scene Graph parsing (DiscoSG), supported by our dataset DiscoSG-DS, which comprises 400 expert-annotated and 8,430 synthesised multi-sentence caption-graph pairs for images. Each caption averages 9 sentences, and each graph contains at least 3 times more triples than those in existing datasets. While fine-tuning large PLMs (i.e., GPT-4) on DiscoSG-DS improves SPICE by approximately 48% over the best sentence-merging baseline, high inference cost and restrictive licensing hinder its open-source use, and smaller fine-tuned PLMs struggle with complex graphs. We propose DiscoSG-Refiner, which drafts a base graph using one small PLM, then employs a second PLM to iteratively propose graph edits, reducing full-graph generation overhead. Using two Flan-T5-Base models, DiscoSG-Refiner still improves SPICE by approximately 30% over the best baseline while achieving 86 times faster inference than GPT-4. It also consistently improves downstream VLM tasks like discourse-level caption evaluation and hallucination detection. Code and data are available at: https://github.com/ShaoqLin/DiscoSG
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts</title>
<link>https://arxiv.org/abs/2506.15594</link>
<guid>https://arxiv.org/abs/2506.15594</guid>
<content:encoded><![CDATA[
<div> Keywords: documents, vision-language models, WikiMixQA, cross-modal reasoning, benchmark

Summary: 
The paper introduces WikiMixQA, a benchmark consisting of 1,000 multiple-choice questions that assess cross-modal reasoning over tables and charts from 4,000 Wikipedia pages. Existing vision-language models struggle with processing long-context vision inputs, as shown by the evaluation of 12 state-of-the-art models. Proprietary models achieve around 70% accuracy with direct context but struggle with retrieval from long documents. GPT-4-o is the only model surpassing 50% accuracy in this scenario, while open-source models perform much worse, with a maximum accuracy of 27%. This highlights the difficulties of long-context, multi-modal reasoning in document understanding. WikiMixQA is established as a significant benchmark for advancing research in this field. 

<br /><br />Summary: <div>
arXiv:2506.15594v1 Announce Type: new 
Abstract: Documents are fundamental to preserving and disseminating information, often incorporating complex layouts, tables, and charts that pose significant challenges for automatic document understanding (DU). While vision-language large models (VLLMs) have demonstrated improvements across various tasks, their effectiveness in processing long-context vision inputs remains unclear. This paper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice questions (MCQs) designed to evaluate cross-modal reasoning over tables and charts extracted from 4,000 Wikipedia pages spanning seven distinct topics. Unlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring models to synthesize information from multiple modalities. We evaluate 12 state-of-the-art vision-language models, revealing that while proprietary models achieve ~70% accuracy when provided with direct context, their performance deteriorates significantly when retrieval from long documents is required. Among these, GPT-4-o is the only model exceeding 50% accuracy in this setting, whereas open-source models perform considerably worse, with a maximum accuracy of 27%. These findings underscore the challenges of long-context, multi-modal reasoning and establish WikiMixQA as a crucial benchmark for advancing document understanding research.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Model to Classroom: Evaluating Generated MCQs for Portuguese with Narrative and Difficulty Concerns</title>
<link>https://arxiv.org/abs/2506.15598</link>
<guid>https://arxiv.org/abs/2506.15598</guid>
<content:encoded><![CDATA[
<div> MCQs; generative AI; Portuguese language; reading comprehension; elementary school students<br />
<br />
Summary:
This study explores the use of generative AI to automate Multiple Choice Question (MCQ) generation for reading comprehension in Portuguese. The research focuses on creating MCQs with varying difficulty levels that align with curriculum-relevant narrative elements. Evaluation through expert review and psychometric analysis of student responses shows that current generative models can produce MCQs of comparable quality to human-authored ones. However, issues such as semantic clarity and answerability need improvement. Generating engaging distractors that meet criteria for high-quality MCQ option design remains a challenge. Overall, while the study demonstrates the potential of generative models in generating MCQs for educational purposes, further refinement is necessary to ensure the reliability and effectiveness of the generated questions, particularly in real-world settings for elementary school students.<br /> <div>
arXiv:2506.15598v1 Announce Type: new 
Abstract: While MCQs are valuable for learning and evaluation, manually creating them with varying difficulty levels and targeted reading skills remains a time-consuming and costly task. Recent advances in generative AI provide an opportunity to automate MCQ generation efficiently. However, assessing the actual quality and reliability of generated MCQs has received limited attention -- particularly regarding cases where generation fails. This aspect becomes particularly important when the generated MCQs are meant to be applied in real-world settings. Additionally, most MCQ generation studies focus on English, leaving other languages underexplored. This paper investigates the capabilities of current generative models in producing MCQs for reading comprehension in Portuguese, a morphologically rich language. Our study focuses on generating MCQs that align with curriculum-relevant narrative elements and span different difficulty levels. We evaluate these MCQs through expert review and by analyzing the psychometric properties extracted from student responses to assess their suitability for elementary school students. Our results show that current models can generate MCQs of comparable quality to human-authored ones. However, we identify issues related to semantic clarity and answerability. Also, challenges remain in generating distractors that engage students and meet established criteria for high-quality MCQ option design.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Compositional Architecture of Regret in Large Language Models</title>
<link>https://arxiv.org/abs/2506.15617</link>
<guid>https://arxiv.org/abs/2506.15617</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Regret Mechanism, Hidden States, Neuron Level, Dataset Construction

Summary:
The article discusses the concept of regret in Large Language Models and the importance of studying the regret mechanism for enhancing model reliability and understanding cognitive coding in neural networks. Three key challenges are identified: the absence of specialized datasets capturing regret expressions, the lack of metrics for finding optimal regret representation layers, and the need for metrics to identify and analyze regret neurons. To address these challenges, the article proposes a workflow for constructing a regret dataset, introduces the Supervised Compression-Decoupling Index (S-CDI) metric for identifying optimal regret representation layers, and introduces the Regret Dominance Score (RDS) metric for identifying regret neurons. Experimental results show success in identifying the optimal regret representation layer, revealing an M-shaped decoupling pattern across model layers and categorizing neurons into regret, non-regret, and dual functional groups. 

Summary:<br /><br />Regret in Large Language Models is crucial for enhancing reliability and understanding cognition. Challenges include dataset absence, lack of metrics for optimal representation layers, and neuron identification. Proposed solutions include a dataset construction workflow, the S-CDI metric for representation layers, and the RDS metric for neuron identification. Results show success in identifying optimal representation layers, revealing decoupling patterns and categorizing neurons into functional groups. <div>
arXiv:2506.15617v1 Announce Type: new 
Abstract: Regret in Large Language Models refers to their explicit regret expression when presented with evidence contradicting their previously generated misinformation. Studying the regret mechanism is crucial for enhancing model reliability and helps in revealing how cognition is coded in neural networks. To understand this mechanism, we need to first identify regret expressions in model outputs, then analyze their internal representation. This analysis requires examining the model's hidden states, where information processing occurs at the neuron level. However, this faces three key challenges: (1) the absence of specialized datasets capturing regret expressions, (2) the lack of metrics to find the optimal regret representation layer, and (3) the lack of metrics for identifying and analyzing regret neurons. Addressing these limitations, we propose: (1) a workflow for constructing a comprehensive regret dataset through strategically designed prompting scenarios, (2) the Supervised Compression-Decoupling Index (S-CDI) metric to identify optimal regret representation layers, and (3) the Regret Dominance Score (RDS) metric to identify regret neurons and the Group Impact Coefficient (GIC) to analyze activation patterns. Our experimental results successfully identified the optimal regret representation layer using the S-CDI metric, which significantly enhanced performance in probe classification experiments. Additionally, we discovered an M-shaped decoupling pattern across model layers, revealing how information processing alternates between coupling and decoupling phases. Through the RDS metric, we categorized neurons into three distinct functional groups: regret neurons, non-regret neurons, and dual neurons.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minding the Politeness Gap in Cross-cultural Communication</title>
<link>https://arxiv.org/abs/2506.15623</link>
<guid>https://arxiv.org/abs/2506.15623</guid>
<content:encoded><![CDATA[
<div> British English, American English, cross-cultural communication, interpretation, intensifiers <br />
Summary:<br />
- Misunderstandings in cross-cultural communication can arise from differences in interpreting words like "quite" and "very".
- Differences in interpretation stem from various factors, including literal meanings and utterance cost considerations.
- The study uses a computational cognitive model to analyze how speakers of British and American English interpret intensifiers.
- Results challenge accounts based solely on semantic variation or politeness norms.
- Cross-cultural differences in interpretation result from a complex interplay between literal meanings, politeness norms, and utterance cost considerations.<br /> 
Summary: <div>
arXiv:2506.15623v1 Announce Type: new 
Abstract: Misunderstandings in cross-cultural communication often arise from subtle differences in interpretation, but it is unclear whether these differences arise from the literal meanings assigned to words or from more general pragmatic factors such as norms around politeness and brevity. In this paper, we report three experiments examining how speakers of British and American English interpret intensifiers like "quite" and "very." To better understand these cross-cultural differences, we developed a computational cognitive model where listeners recursively reason about speakers who balance informativity, politeness, and utterance cost. Our model comparisons suggested that cross-cultural differences in intensifier interpretation stem from a combination of (1) different literal meanings, (2) different weights on utterance cost. These findings challenge accounts based purely on semantic variation or politeness norms, demonstrating that cross-cultural differences in interpretation emerge from an intricate interplay between the two.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability</title>
<link>https://arxiv.org/abs/2506.15629</link>
<guid>https://arxiv.org/abs/2506.15629</guid>
<content:encoded><![CDATA[
<div> Generative Commonsense Reasoning; CommonGen; Large Language Models; Instruction-Following; Benchmarks

Summary:
- A new benchmark called Ordered CommonGen evaluates the compositional generalization and instruction-following abilities of large language models (LLMs).
- The benchmark measures ordered coverage to assess whether LLMs can generate sentences in a specified order.
- Analysis of 36 LLMs shows that while they understand instructions, biases towards specific concept order patterns lead to low-diversity outputs or identical results.
- Even the most instruction-compliant LLM achieved only about 75% ordered coverage, indicating the need for improvements in instruction-following and compositional generalization capabilities.
- The study highlights the challenges in ensuring LLMs effectively follow specified concept orders while generating diverse and accurate outputs. 

<br /><br />Summary: <div>
arXiv:2506.15629v1 Announce Type: new 
Abstract: In generative commonsense reasoning tasks such as CommonGen, generative large language models (LLMs) compose sentences that include all given concepts. However, when focusing on instruction-following capabilities, if a prompt specifies a concept order, LLMs must generate sentences that adhere to the specified order. To address this, we propose Ordered CommonGen, a benchmark designed to evaluate the compositional generalization and instruction-following abilities of LLMs. This benchmark measures ordered coverage to assess whether concepts are generated in the specified order, enabling a simultaneous evaluation of both abilities. We conducted a comprehensive analysis using 36 LLMs and found that, while LLMs generally understand the intent of instructions, biases toward specific concept order patterns often lead to low-diversity outputs or identical results even when the concept order is altered. Moreover, even the most instruction-compliant LLM achieved only about 75% ordered coverage, highlighting the need for improvements in both instruction-following and compositional generalization capabilities.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oldies but Goldies: The Potential of Character N-grams for Romanian Texts</title>
<link>https://arxiv.org/abs/2506.15650</link>
<guid>https://arxiv.org/abs/2506.15650</guid>
<content:encoded><![CDATA[
<div> Keywords: authorship attribution, Romanian texts, machine learning techniques, character n-gram features, ROST corpus

Summary:
- The study focuses on authorship attribution in Romanian texts using the ROST corpus, a common benchmark in the field.
- Six machine learning techniques were evaluated: Support Vector Machine, Logistic Regression, k-Nearest Neighbors, Decision Trees, Random Forests, and Artificial Neural Networks.
- Character n-gram features were utilized for classification, with the Artificial Neural Networks model achieving the highest accuracy.
- The ANN model achieved perfect classification in four out of fifteen runs when using 5-gram features.
- Results suggest that simple character n-gram approaches can offer state-of-the-art accuracy for Romanian authorship attribution, even outperforming more complex methods.

<br /><br />Summary: 
The study explores authorship attribution in Romanian texts using the ROST corpus and evaluates the effectiveness of various machine learning techniques. Employing character n-gram features, the Artificial Neural Networks model emerged as the top performer, achieving perfect classification in multiple runs. These findings emphasize the efficacy of simplistic stylometric features in achieving high accuracy in Romanian authorship attribution tasks, underscoring the potential of such approaches in resource-constrained or less studied language contexts. <div>
arXiv:2506.15650v1 Announce Type: new 
Abstract: This study addresses the problem of authorship attribution for Romanian texts using the ROST corpus, a standard benchmark in the field. We systematically evaluate six machine learning techniques: Support Vector Machine (SVM), Logistic Regression (LR), k-Nearest Neighbors (k-NN), Decision Trees (DT), Random Forests (RF), and Artificial Neural Networks (ANN), employing character n-gram features for classification. Among these, the ANN model achieved the highest performance, including perfect classification in four out of fifteen runs when using 5-gram features. These results demonstrate that lightweight, interpretable character n-gram approaches can deliver state-of-the-art accuracy for Romanian authorship attribution, rivaling more complex methods. Our findings highlight the potential of simple stylometric features in resource, constrained or under-studied language settings.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CC-LEARN: Cohort-based Consistency Learning</title>
<link>https://arxiv.org/abs/2506.15662</link>
<guid>https://arxiv.org/abs/2506.15662</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, language models, consistency learning, reasoning, cohort-based<br />
Summary:<br /> 
The article introduces the Cohort-based Consistency Learning (CC-Learn) framework to improve the reliability of Large Language Models (LLMs) in reasoning tasks. By training on cohorts of similar questions derived from shared programmatic abstractions, CC-Learn enforces consistency in reasoning patterns. The framework defines a composite objective that combines cohort accuracy, retrieval bonus for effective problem decomposition, and rejection penalty for trivial or invalid lookups. Through reinforcement learning, the model is guided to adopt uniform reasoning patterns across all cohort members. Experimental results on challenging reasoning benchmarks like ARC-Challenge and StrategyQA show that CC-Learn enhances accuracy and reasoning stability over pretrained and supervised fine-tuning baselines. This demonstrates that cohort-based reinforcement learning effectively enhances reasoning consistency in LLMs. 

Summary: <div>
arXiv:2506.15662v1 Announce Type: new 
Abstract: Large language models excel at many tasks but still struggle with consistent, robust reasoning. We introduce Cohort-based Consistency Learning (CC-Learn), a reinforcement learning framework that improves the reliability of LLM reasoning by training on cohorts of similar questions derived from shared programmatic abstractions. To enforce cohort-level consistency, we define a composite objective combining cohort accuracy, a retrieval bonus for effective problem decomposition, and a rejection penalty for trivial or invalid lookups that reinforcement learning can directly optimize, unlike supervised fine-tuning. Optimizing this reward guides the model to adopt uniform reasoning patterns across all cohort members. Experiments on challenging reasoning benchmarks (including ARC-Challenge and StrategyQA) show that CC-Learn boosts both accuracy and reasoning stability over pretrained and SFT baselines. These results demonstrate that cohort-level RL effectively enhances reasoning consistency in LLMs.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers</title>
<link>https://arxiv.org/abs/2506.15674</link>
<guid>https://arxiv.org/abs/2506.15674</guid>
<content:encoded><![CDATA[
<div> privacy leakage, reasoning traces, personal agents, sensitive user data, test-time compute approaches

Summary:
The study explores privacy leakage in reasoning traces of large reasoning models used as personal agents. It challenges the assumption that reasoning traces are internal and safe by showing that they often contain sensitive user data, which can be extracted via prompt injections or accidentally leak into outputs. The research demonstrates that test-time compute approaches, specifically increasing reasoning steps, amplify this leakage. While increasing the budget of these approaches may make models more cautious in their final answers, it also results in reasoning more verbosely and leaking more sensitive information. This highlights a fundamental tension where reasoning improves utility but increases the privacy attack surface. The authors argue that safety measures need to extend beyond the model's outputs to include its internal thinking. 

<br /><br />Summary: <div>
arXiv:2506.15674v1 Announce Type: new 
Abstract: We study privacy leakage in the reasoning traces of large reasoning models used as personal agents. Unlike final outputs, reasoning traces are often assumed to be internal and safe. We challenge this assumption by showing that reasoning traces frequently contain sensitive user data, which can be extracted via prompt injections or accidentally leak into outputs. Through probing and agentic evaluations, we demonstrate that test-time compute approaches, particularly increased reasoning steps, amplify such leakage. While increasing the budget of those test-time compute approaches makes models more cautious in their final answers, it also leads them to reason more verbosely and leak more in their own thinking. This reveals a core tension: reasoning improves utility but enlarges the privacy attack surface. We argue that safety efforts must extend to the model's internal thinking, not just its outputs.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gender-Neutral Machine Translation Strategies in Practice</title>
<link>https://arxiv.org/abs/2506.15676</link>
<guid>https://arxiv.org/abs/2506.15676</guid>
<content:encoded><![CDATA[
<div> gender-inclusive machine translation, gender ambiguity, gender neutrality, grammatical gender languages, binary gender stereotypes  
Summary:  
Gender-inclusive machine translation aims to preserve gender ambiguity in the source language to avoid misgendering and representational harms. This study assessed 21 machine translation systems' sensitivity to the need for gender neutrality in response to gender ambiguity across different translation directions. The researchers categorized and discussed specific gender-neutral strategies observed in practice and explored the impact of binary gender stereotypes on translation choices. The study found a lack of gender-neutral translations in response to gender ambiguity overall, but identified a few machine translation systems that implemented gender-neutral strategies, depending on the target language. This research highlights the challenges and opportunities for improving gender-inclusive machine translation systems to address gender ambiguity effectively and support gender-neutral language representation.  
<br /><br />Summary: <div>
arXiv:2506.15676v1 Announce Type: new 
Abstract: Gender-inclusive machine translation (MT) should preserve gender ambiguity in the source to avoid misgendering and representational harms. While gender ambiguity often occurs naturally in notional gender languages such as English, maintaining that gender neutrality in grammatical gender languages is a challenge. Here we assess the sensitivity of 21 MT systems to the need for gender neutrality in response to gender ambiguity in three translation directions of varying difficulty. The specific gender-neutral strategies that are observed in practice are categorized and discussed. Additionally, we examine the effect of binary gender stereotypes on the use of gender-neutral translation. In general, we report a disappointing absence of gender-neutral translations in response to gender ambiguity. However, we observe a small handful of MT systems that switch to gender neutral translation using specific strategies, depending on the target language.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenRecal: Generation after Recalibration from Large to Small Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.15681</link>
<guid>https://arxiv.org/abs/2506.15681</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, distillation, knowledge transfer, Recalibrator, benchmarks

Summary:<br />
Recent advancements in vision-language models (VLMs) have led to the development of high-performance models like GPT-4V. However, deploying these models on resource-constrained devices remains challenging due to their computational demands. To address this, a new distillation framework called GenRecal has been introduced. GenRecal incorporates a Recalibrator to align and adapt feature representations between different VLM architectures, enabling effective knowledge transfer. Through extensive experiments on various benchmarks, GenRecal has shown significant improvements over baseline performances and has even outperformed large-scale open- and closed-source VLMs. This framework allows for the distillation of knowledge from heterogeneous VLMs, making it a versatile solution for deploying efficient vision-language models in real-world scenarios. 

<br /><br />Summary: <div>
arXiv:2506.15681v1 Announce Type: new 
Abstract: Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models in real-world scenarios, particularly on resource-constrained devices, remains challenging due to their substantial computational demands. This has spurred interest in distilling knowledge from large VLMs into smaller, more efficient counterparts. A key challenge arises here from the diversity of VLM architectures, which are built on different LLMs and employ varying token types-differing in vocabulary size, token splits, and token index ordering. To address this challenge of limitation to a specific VLM type, we present Generation after Recalibration (GenRecal), a novel, general-purpose distillation framework for VLMs. GenRecal incorporates a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, enabling effective knowledge transfer across different types of VLMs. Through extensive experiments on multiple challenging benchmarks, we demonstrate that GenRecal significantly improves baseline performances, eventually outperforming large-scale open- and closed-source VLMs.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhantomHunter: Detecting Unseen Privately-Tuned LLM-Generated Text via Family-Aware Learning</title>
<link>https://arxiv.org/abs/2506.15683</link>
<guid>https://arxiv.org/abs/2506.15683</guid>
<content:encoded><![CDATA[
arXiv:2506.15683v1 Announce Type: new 
Abstract: With the popularity of large language models (LLMs), undesirable societal problems like misinformation production and academic misconduct have been more severe, making LLM-generated text detection now of unprecedented importance. Although existing methods have made remarkable progress, a new challenge posed by text from privately tuned LLMs remains underexplored. Users could easily possess private LLMs by fine-tuning an open-source one with private corpora, resulting in a significant performance drop of existing detectors in practice. To address this issue, we propose PhantomHunter, an LLM-generated text detector specialized for detecting text from unseen, privately-tuned LLMs. Its family-aware learning framework captures family-level traits shared across the base models and their derivatives, instead of memorizing individual characteristics. Experiments on data from LLaMA, Gemma, and Mistral families show its superiority over 7 baselines and 3 industrial services, with F1 scores of over 96%.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ETS: Open Vocabulary Electroencephalography-To-Text Decoding and Sentiment Classification</title>
<link>https://arxiv.org/abs/2506.14783</link>
<guid>https://arxiv.org/abs/2506.14783</guid>
<content:encoded><![CDATA[
arXiv:2506.14783v1 Announce Type: cross 
Abstract: Decoding natural language from brain activity using non-invasive electroencephalography (EEG) remains a significant challenge in neuroscience and machine learning, particularly for open-vocabulary scenarios where traditional methods struggle with noise and variability. Previous studies have achieved high accuracy on small-closed vocabularies, but it still struggles on open vocabularies. In this study, we propose ETS, a framework that integrates EEG with synchronized eye-tracking data to address two critical tasks: (1) open-vocabulary text generation and (2) sentiment classification of perceived language. Our model achieves a superior performance on BLEU and Rouge score for EEG-To-Text decoding and up to 10% F1 score on EEG-based ternary sentiment classification, which significantly outperforms supervised baselines. Furthermore, we show that our proposed model can handle data from various subjects and sources, showing great potential for high performance open vocabulary eeg-to-text system.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemIRNet: A Semantic Irony Recognition Network for Multimodal Sarcasm Detection</title>
<link>https://arxiv.org/abs/2506.14791</link>
<guid>https://arxiv.org/abs/2506.14791</guid>
<content:encoded><![CDATA[
arXiv:2506.14791v1 Announce Type: cross 
Abstract: Aiming at the problem of difficulty in accurately identifying graphical implicit correlations in multimodal irony detection tasks, this paper proposes a Semantic Irony Recognition Network (SemIRNet). The model contains three main innovations: (1) The ConceptNet knowledge base is introduced for the first time to acquire conceptual knowledge, which enhances the model's common-sense reasoning ability; (2) Two cross-modal semantic similarity detection modules at the word level and sample level are designed to model graphic-textual correlations at different granularities; and (3) A contrastive learning loss function is introduced to optimize the spatial distribution of the sample features, which improves the separability of positive and negative samples. Experiments on a publicly available multimodal irony detection benchmark dataset show that the accuracy and F1 value of this model are improved by 1.64% and 2.88% to 88.87% and 86.33%, respectively, compared with the existing optimal methods. Further ablation experiments verify the important role of knowledge fusion and semantic similarity detection in improving the model performance.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assembly of Experts: Linear-time construction of the Chimera LLM variants with emergent and adaptable behaviors</title>
<link>https://arxiv.org/abs/2506.14794</link>
<guid>https://arxiv.org/abs/2506.14794</guid>
<content:encoded><![CDATA[
arXiv:2506.14794v1 Announce Type: cross 
Abstract: Requiring $10^{13}$-$10^{15}$ FLOPs to calculate one 8 bit weight in an LLM during pretraining is extremely expensive and seems inefficient. To better leverage the huge investments made into pretrained models, we develop the new "Assembly-of-Experts" (AoE) construction method to create capable child variants of existing Mixture-of-Experts parent models in linear time. Model weight tensors get interpolated individually, allowing to enhance or suppress semantic features of the parents.
  Varying the proportion of weights taken from the parent models, we observe some properties of the AoE child model changing gradually, while other behavioral traits emerge with a sharp transition. Surprisingly, nearly every generated model is functional and capable, which makes searching the model space straightforward.
  We construct the DeepSeek R1T "Chimera", a 671B open-weights hybrid model combining DeepSeek's V3-0324 and R1 model variants. The child inherits only the routed expert tensors of R1, but still achieves about R1-level intelligence. At the same time, it uses about 40\% fewer output tokens, close to V3 speed. Constructed without any fine-tuning or distillation, the Chimera exhibits surprisingly compact, orderly reasoning compared to its parent models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?</title>
<link>https://arxiv.org/abs/2506.14805</link>
<guid>https://arxiv.org/abs/2506.14805</guid>
<content:encoded><![CDATA[
arXiv:2506.14805v1 Announce Type: cross 
Abstract: As Multimodal Large Language Models (MLLMs) continue to evolve, their cognitive and reasoning capabilities have seen remarkable progress. However, challenges in visual fine-grained perception and commonsense causal inference persist. This paper introduces Argus Inspection, a multimodal benchmark with two levels of difficulty, emphasizing detailed visual recognition while incorporating real-world commonsense understanding to evaluate causal reasoning abilities. Expanding on it, we present the Eye of Panoptes framework, which integrates a binary parametric Sigmoid metric with an indicator function, enabling a more holistic evaluation of MLLMs' responses in opinion-based reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the highest performance in visual fine-grained reasoning reaches only 0.46, highlighting considerable potential for enhancement. Our research offers valuable perspectives for the continued refinement of MLLMs.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Narrative Shifts through Persistent Structures: A Topological Analysis of Media Discourse</title>
<link>https://arxiv.org/abs/2506.14836</link>
<guid>https://arxiv.org/abs/2506.14836</guid>
<content:encoded><![CDATA[
arXiv:2506.14836v1 Announce Type: cross 
Abstract: How can we detect when global events fundamentally reshape public discourse? This study introduces a topological framework for identifying structural change in media narratives using persistent homology. Drawing on international news articles surrounding major events - including the Russian invasion of Ukraine (Feb 2022), the murder of George Floyd (May 2020), the U.S. Capitol insurrection (Jan 2021), and the Hamas-led invasion of Israel (Oct 2023) - we construct daily co-occurrence graphs of noun phrases to trace evolving discourse. Each graph is embedded and transformed into a persistence diagram via a Vietoris-Rips filtration. We then compute Wasserstein distances and persistence entropies across homological dimensions to capture semantic disruption and narrative volatility over time. Our results show that major geopolitical and social events align with sharp spikes in both H0 (connected components) and H1 (loops), indicating sudden reorganization in narrative structure and coherence. Cross-correlation analyses reveal a typical lag pattern in which changes to component-level structure (H0) precede higher-order motif shifts (H1), suggesting a bottom-up cascade of semantic change. An exception occurs during the Russian invasion of Ukraine, where H1 entropy leads H0, possibly reflecting top-down narrative framing before local discourse adjusts. Persistence entropy further distinguishes tightly focused from diffuse narrative regimes. These findings demonstrate that persistent homology offers a mathematically principled, unsupervised method for detecting inflection points and directional shifts in public attention - without requiring prior knowledge of specific events. This topological approach advances computational social science by enabling real-time detection of semantic restructuring during crises, protests, and information shocks.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching</title>
<link>https://arxiv.org/abs/2506.14852</link>
<guid>https://arxiv.org/abs/2506.14852</guid>
<content:encoded><![CDATA[
arXiv:2506.14852v1 Announce Type: cross 
Abstract: LLM-based agentic applications have shown increasingly remarkable capabilities in complex workflows but incur substantial costs due to extensive planning and reasoning requirements. Existing LLM caching techniques (like context caching and semantic caching), primarily designed for serving chatbots, are insufficient for agentic applications where outputs depend on external data or environmental contexts. We propose agentic plan caching, a novel approach that extracts, stores, adapts, and reuses structured plan templates from planning stages of agentic applications across semantically similar tasks to reduce the cost of serving. Unlike traditional semantic caching, our system extracts plan templates from completed agent executions at test-time, employs keyword extraction to match new requests against cached plans, and utilizes lightweight models to adapt these templates to task-specific plans with contexts. Evaluation across multiple real-world agentic applications shows that our system can reduce costs by 46.62% on average while maintaining performance, offering a more efficient solution for serving LLM-based agents that complements existing LLM serving infrastructures.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective</title>
<link>https://arxiv.org/abs/2506.14965</link>
<guid>https://arxiv.org/abs/2506.14965</guid>
<content:encoded><![CDATA[
arXiv:2506.14965v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning, yet most open efforts focus narrowly on math and code, limiting our understanding of its broader applicability to general reasoning. A key challenge lies in the lack of reliable, scalable RL reward signals across diverse reasoning domains. We introduce Guru, a curated RL reasoning corpus of 92K verifiable examples spanning six reasoning domains--Math, Code, Science, Logic, Simulation, and Tabular--each built through domain-specific reward design, deduplication, and filtering to ensure reliability and effectiveness for RL training. Based on Guru, we systematically revisit established findings in RL for LLM reasoning and observe significant variation across domains. For example, while prior work suggests that RL primarily elicits existing knowledge from pretrained models, our results reveal a more nuanced pattern: domains frequently seen during pretraining (Math, Code, Science) easily benefit from cross-domain RL training, while domains with limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain training to achieve meaningful performance gains, suggesting that RL is likely to facilitate genuine skill acquisition. Finally, we present Guru-7B and Guru-32B, two models that achieve state-of-the-art performance among open models RL-trained with publicly available data, outperforming best baselines by 7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We also show that our models effectively improve the Pass@k performance of their base models, particularly on complex tasks less likely to appear in pretraining data. We release data, models, training and evaluation code to facilitate general-purpose reasoning at: https://github.com/LLM360/Reasoning360
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypothesis Testing for Quantifying LLM-Human Misalignment in Multiple Choice Settings</title>
<link>https://arxiv.org/abs/2506.14997</link>
<guid>https://arxiv.org/abs/2506.14997</guid>
<content:encoded><![CDATA[
arXiv:2506.14997v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) increasingly appear in social science research (e.g., economics and marketing), it becomes crucial to assess how well these models replicate human behavior. In this work, using hypothesis testing, we present a quantitative framework to assess the misalignment between LLM-simulated and actual human behaviors in multiple-choice survey settings. This framework allows us to determine in a principled way whether a specific language model can effectively simulate human opinions, decision-making, and general behaviors represented through multiple-choice options. We applied this framework to a popular language model for simulating people's opinions in various public surveys and found that this model is ill-suited for simulating the tested sub-populations (e.g., across different races, ages, and incomes) for contentious questions. This raises questions about the alignment of this language model with the tested populations, highlighting the need for new practices in using LLMs for social science studies beyond naive simulations of human subjects.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size</title>
<link>https://arxiv.org/abs/2506.15025</link>
<guid>https://arxiv.org/abs/2506.15025</guid>
<content:encoded><![CDATA[
arXiv:2506.15025v1 Announce Type: cross 
Abstract: Pretraining large language models is a costly process. To make this process more efficient, several methods have been proposed to optimize model architecture/parametrization and hardware use. On the parametrization side, $\mu P$ (Maximal Update Parametrization) parametrizes model weights and learning rate (LR) in a way that makes hyperparameters (HPs) transferable with width (embedding dimension): HPs can be tuned for a small model and used for larger models without additional tuning. While $\mu$P showed impressive results in practice, recent empirical studies have reported conflicting observations when applied to LLMs. One limitation of the theory behind $\mu$P is the fact that input dimension (vocabulary size in LLMs) is considered fixed when taking the width to infinity. This is unrealistic since vocabulary size is generally much larger than width in practice. In this work, we provide a theoretical analysis of the effect of vocabulary size on training dynamics, and subsequently show that as vocabulary size increases, the training dynamics \emph{interpolate between the $\mu$P regime and another regime that we call Large Vocab (LV) Regime}, where optimal scaling rules are different from those predicted by $\mu$P. Our analysis reveals that in the LV regime, the optimal embedding LR to hidden LR ratio should roughly scale as $\Theta(\sqrt{width})$, surprisingly close to the empirical findings previously reported in the literature, and different from the $\Theta(width)$ ratio predicted by $\mu$P. We conduct several experiments to validate our theory, and pretrain a 1B model from scratch to show the benefit of our suggested scaling rule for the embedding LR.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An accurate and revised version of optical character recognition-based speech synthesis using LabVIEW</title>
<link>https://arxiv.org/abs/2506.15029</link>
<guid>https://arxiv.org/abs/2506.15029</guid>
<content:encoded><![CDATA[
arXiv:2506.15029v1 Announce Type: cross 
Abstract: Knowledge extraction through sound is a distinctive property. Visually impaired individuals often rely solely on Braille books and audio recordings provided by NGOs. Due to limitations in these approaches, blind individuals often cannot access books of their choice. Speech is a more effective mode of communication than text for blind and visually impaired persons, as they can easily respond to sounds. This paper presents the development of an accurate, reliable, cost-effective, and user-friendly optical character recognition (OCR)-based speech synthesis system. The OCR-based system has been implemented using Laboratory Virtual Instrument Engineering Workbench (LabVIEW).
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying economic narratives in large text corpora -- An integrated approach using Large Language Models</title>
<link>https://arxiv.org/abs/2506.15041</link>
<guid>https://arxiv.org/abs/2506.15041</guid>
<content:encoded><![CDATA[
arXiv:2506.15041v1 Announce Type: cross 
Abstract: As interest in economic narratives has grown in recent years, so has the number of pipelines dedicated to extracting such narratives from texts. Pipelines often employ a mix of state-of-the-art natural language processing techniques, such as BERT, to tackle this task. While effective on foundational linguistic operations essential for narrative extraction, such models lack the deeper semantic understanding required to distinguish extracting economic narratives from merely conducting classic tasks like Semantic Role Labeling. Instead of relying on complex model pipelines, we evaluate the benefits of Large Language Models (LLMs) by analyzing a corpus of Wall Street Journal and New York Times newspaper articles about inflation. We apply a rigorous narrative definition and compare GPT-4o outputs to gold-standard narratives produced by expert annotators. Our results suggests that GPT-4o is capable of extracting valid economic narratives in a structured format, but still falls short of expert-level performance when handling complex documents and narratives. Given the novelty of LLMs in economic research, we also provide guidance for future work in economics and the social sciences that employs LLMs to pursue similar objectives.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning</title>
<link>https://arxiv.org/abs/2506.15154</link>
<guid>https://arxiv.org/abs/2506.15154</guid>
<content:encoded><![CDATA[
arXiv:2506.15154v1 Announce Type: cross 
Abstract: Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases and drive forward research in music AI. This paper introduces a multi-task music captioning model, SonicVerse, that integrates caption generation with auxiliary music feature detection tasks such as key detection, vocals detection, and more, so as to directly capture both low-level acoustic details as well as high-level musical attributes. The key contribution is a projection-based architecture that transforms audio input into language tokens, while simultaneously detecting music features through dedicated auxiliary heads. The outputs of these heads are also projected into language tokens, to enhance the captioning input. This framework not only produces rich, descriptive captions for short music fragments but also directly enables the generation of detailed time-informed descriptions for longer music pieces, by chaining the outputs using a large-language model. To train the model, we extended the MusicBench dataset by annotating it with music features using MIRFLEX, a modular music feature extractor, resulting in paired audio, captions and music feature data. Experimental results show that incorporating features in this way improves the quality and detail of the generated captions.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models</title>
<link>https://arxiv.org/abs/2506.15220</link>
<guid>https://arxiv.org/abs/2506.15220</guid>
<content:encoded><![CDATA[
arXiv:2506.15220v1 Announce Type: cross 
Abstract: Videos contain a wealth of information, and generating detailed and accurate descriptions in natural language is a key aspect of video understanding. In this paper, we present video-SALMONN 2, an advanced audio-visual large language model (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with paired audio) captioning through directed preference optimisation (DPO). We propose new metrics to evaluate the completeness and accuracy of video descriptions, which are optimised using DPO. To further improve training, we propose a novel multi-round DPO (MrDPO) approach, which involves periodically updating the DPO reference model, merging and re-initialising the LoRA module as a proxy for parameter updates after each training round (1,000 steps), and incorporating guidance from ground-truth video captions to stabilise the process. Experimental results show that MrDPO significantly enhances video-SALMONN 2's captioning accuracy, reducing the captioning error rates by 28\%. The final video-SALMONN 2 model, with just 7 billion parameters, surpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining highly competitive performance to the state-of-the-art on widely used video question-answering benchmarks among models of similar size. Codes are available at \href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When and How Unlabeled Data Provably Improve In-Context Learning</title>
<link>https://arxiv.org/abs/2506.15329</link>
<guid>https://arxiv.org/abs/2506.15329</guid>
<content:encoded><![CDATA[
arXiv:2506.15329v1 Announce Type: cross 
Abstract: Recent research shows that in-context learning (ICL) can be effective even when demonstrations have missing or incorrect labels. To shed light on this capability, we examine a canonical setting where the demonstrations are drawn according to a binary Gaussian mixture model (GMM) and a certain fraction of the demonstrations have missing labels. We provide a comprehensive theoretical study to show that: (1) The loss landscape of one-layer linear attention models recover the optimal fully-supervised estimator but completely fail to exploit unlabeled data; (2) In contrast, multilayer or looped transformers can effectively leverage unlabeled data by implicitly constructing estimators of the form $\sum_{i\ge 0} a_i (X^\top X)^iX^\top y$ with $X$ and $y$ denoting features and partially-observed labels (with missing entries set to zero). We characterize the class of polynomials that can be expressed as a function of depth and draw connections to Expectation Maximization, an iterative pseudo-labeling algorithm commonly used in semi-supervised learning. Importantly, the leading polynomial power is exponential in depth, so mild amount of depth/looping suffices. As an application of theory, we propose looping off-the-shelf tabular foundation models to enhance their semi-supervision capabilities. Extensive evaluations on real-world datasets show that our method significantly improves the semisupervised tabular learning performance over the standard single pass inference.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Factorized RVQ-GAN For Disentangled Speech Tokenization</title>
<link>https://arxiv.org/abs/2506.15456</link>
<guid>https://arxiv.org/abs/2506.15456</guid>
<content:encoded><![CDATA[
arXiv:2506.15456v1 Announce Type: cross 
Abstract: We propose Hierarchical Audio Codec (HAC), a unified neural speech codec that factorizes its bottleneck into three linguistic levels-acoustic, phonetic, and lexical-within a single model. HAC leverages two knowledge distillation objectives: one from a pre-trained speech encoder (HuBERT) for phoneme-level structure, and another from a text-based encoder (LaBSE) for lexical cues. Experiments on English and multilingual data show that HAC's factorized bottleneck yields disentangled token sets: one aligns with phonemes, while another captures word-level semantics. Quantitative evaluations confirm that HAC tokens preserve naturalness and provide interpretable linguistic information, outperforming single-level baselines in both disentanglement and reconstruction quality. These findings underscore HAC's potential as a unified discrete speech representation, bridging acoustic detail and lexical meaning for downstream speech generation and understanding tasks.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework</title>
<link>https://arxiv.org/abs/2506.15538</link>
<guid>https://arxiv.org/abs/2506.15538</guid>
<content:encoded><![CDATA[
arXiv:2506.15538v1 Announce Type: cross 
Abstract: Automated interpretability research aims to identify concepts encoded in neural network features to enhance human understanding of model behavior. Current feature description methods face two critical challenges: limited robustness and the flawed assumption that each neuron encodes only a single concept (monosemanticity), despite growing evidence that neurons are often polysemantic. This assumption restricts the expressiveness of feature descriptions and limits their ability to capture the full range of behaviors encoded in model internals. To address this, we introduce Polysemantic FeatuRe Identification and Scoring Method (PRISM), a novel framework that captures the inherent complexity of neural network features. Unlike prior approaches that assign a single description per feature, PRISM provides more nuanced descriptions for both polysemantic and monosemantic features. We apply PRISM to language models and, through extensive benchmarking against existing methods, demonstrate that our approach produces more accurate and faithful feature descriptions, improving both overall description quality (via a description score) and the ability to capture distinct concepts when polysemanticity is present (via a polysemanticity score).
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning</title>
<link>https://arxiv.org/abs/2506.15606</link>
<guid>https://arxiv.org/abs/2506.15606</guid>
<content:encoded><![CDATA[
arXiv:2506.15606v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become indispensable in real-world applications. However, their widespread adoption raises significant safety concerns, particularly in responding to socially harmful questions. Despite substantial efforts to improve model safety through alignment, aligned models can still have their safety protections undermined by subsequent fine-tuning - even when the additional training data appears benign. In this paper, we empirically demonstrate that this vulnerability stems from the sensitivity of safety-critical low-rank subspaces in LLM parameters to fine-tuning. Building on this insight, we propose a novel training-free method, termed Low-Rank Extrapolation (LoX), to enhance safety robustness by extrapolating the safety subspace of an aligned LLM. Our experimental results confirm the effectiveness of LoX, demonstrating significant improvements in robustness against both benign and malicious fine-tuning attacks while preserving the model's adaptability to new tasks. For instance, LoX leads to 11% to 54% absolute reductions in attack success rates (ASR) facing benign or malicious fine-tuning attacks. By investigating the ASR landscape of parameters, we attribute the success of LoX to that the extrapolation moves LLM parameters to a flatter zone, thereby less sensitive to perturbations. The code is available at github.com/VITA-Group/LoX.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning</title>
<link>https://arxiv.org/abs/2506.15651</link>
<guid>https://arxiv.org/abs/2506.15651</guid>
<content:encoded><![CDATA[
arXiv:2506.15651v1 Announce Type: cross 
Abstract: Rule-based rewards offer a promising strategy for improving reinforcement learning from human feedback (RLHF), but current approaches often rely on manual rule engineering. We present AutoRule, a fully automated method for extracting rules from preference feedback and formulating them into rule-based rewards. AutoRule extraction operates in three stages: it leverages a reasoning model to interpret user preferences, identifies candidate rules from the reasoning chain of these interpretations, and synthesizes them into a unified rule set. Leveraging the finalized rule set, we employ language-model verifiers to compute the fraction of rules satisfied by each output, using this metric as an auxiliary reward alongside the learned reward model during policy optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\% relative improvement in length-controlled win rate on AlpacaEval2.0, and a 6.1\% relative gain in second-turn performance on a held-out MT-Bench subset, compared to a GRPO baseline trained with the same learned reward model but without the rule-based auxiliary reward. Our analysis confirms that the extracted rules exhibit good agreement with dataset preference. We find that AutoRule demonstrates reduced reward hacking compared to a learned reward model when run over two episodes. Finally, our case study suggests that the extracted rules capture unique qualities valued in different datasets. The extracted rules are provided in the appendix, and the code is open-sourced at https://github.com/cxcscmu/AutoRule.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence</title>
<link>https://arxiv.org/abs/2506.15677</link>
<guid>https://arxiv.org/abs/2506.15677</guid>
<content:encoded><![CDATA[
arXiv:2506.15677v1 Announce Type: cross 
Abstract: AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dense SAE Latents Are Features, Not Bugs</title>
<link>https://arxiv.org/abs/2506.15679</link>
<guid>https://arxiv.org/abs/2506.15679</guid>
<content:encoded><![CDATA[
arXiv:2506.15679v1 Announce Type: cross 
Abstract: Sparse autoencoders (SAEs) are designed to extract interpretable features from language models by enforcing a sparsity constraint. Ideally, training an SAE would yield latents that are both sparse and semantically meaningful. However, many SAE latents activate frequently (i.e., are \emph{dense}), raising concerns that they may be undesirable artifacts of the training procedure. In this work, we systematically investigate the geometry, function, and origin of dense latents and show that they are not only persistent but often reflect meaningful model representations. We first demonstrate that dense latents tend to form antipodal pairs that reconstruct specific directions in the residual stream, and that ablating their subspace suppresses the emergence of new dense features in retrained SAEs -- suggesting that high density features are an intrinsic property of the residual space. We then introduce a taxonomy of dense latents, identifying classes tied to position tracking, context binding, entropy regulation, letter-specific output signals, part-of-speech, and principal component reconstruction. Finally, we analyze how these features evolve across layers, revealing a shift from structural features in early layers, to semantic features in mid layers, and finally to output-oriented signals in the last layers of the model. Our findings indicate that dense latents serve functional roles in language model computation and should not be dismissed as training noise.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction</title>
<link>https://arxiv.org/abs/2205.02225</link>
<guid>https://arxiv.org/abs/2205.02225</guid>
<content:encoded><![CDATA[
arXiv:2205.02225v4 Announce Type: replace 
Abstract: Unsupervised relation extraction aims to extract the relationship between entities from natural language sentences without prior information on relational scope or distribution. Existing works either utilize self-supervised schemes to refine relational feature signals by iteratively leveraging adaptive clustering and classification that provoke gradual drift problems, or adopt instance-wise contrastive learning which unreasonably pushes apart those sentence pairs that are semantically similar. To overcome these defects, we propose a novel contrastive learning framework named HiURE, which has the capability to derive hierarchical signals from relational feature space using cross hierarchy attention and effectively optimize relation representation of sentences under exemplar-wise contrastive learning. Experimental results on two public datasets demonstrate the advanced effectiveness and robustness of HiURE on unsupervised relation extraction when compared with state-of-the-art models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling</title>
<link>https://arxiv.org/abs/2402.13534</link>
<guid>https://arxiv.org/abs/2402.13534</guid>
<content:encoded><![CDATA[
arXiv:2402.13534v2 Announce Type: replace 
Abstract: Sequence labeling models often benefit from incorporating external knowledge. However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model. To address this challenge, we propose a two-stage curriculum learning (TCL) framework specifically designed for sequence labeling tasks. The TCL framework enhances training by gradually introducing data instances from easy to hard, aiming to improve both performance and training speed. Furthermore, we explore different metrics for assessing the difficulty levels of sequence labeling tasks. Through extensive experimentation on six Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we demonstrate the effectiveness of our model in enhancing the performance of sequence labeling models. Additionally, our analysis indicates that TCL accelerates training and alleviates the slow training problem associated with complex models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lean Workbook: A large-scale Lean problem set formalized from natural language math problems</title>
<link>https://arxiv.org/abs/2406.03847</link>
<guid>https://arxiv.org/abs/2406.03847</guid>
<content:encoded><![CDATA[
arXiv:2406.03847v3 Announce Type: replace 
Abstract: Large language models have demonstrated impressive capabilities across various natural language processing tasks, especially in solving mathematical problems. However, large language models are not good at math theorem proving using formal languages like Lean. A significant challenge in this area is the scarcity of training data available in these formal languages. To address this issue, we propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements, and vice versa. Our results indicate that the synthetic data pipeline can provide useful training data and improve the performance of LLMs in translating and understanding complex mathematical problems and proofs. Our final dataset contains about 57K formal-informal question pairs along with searched proof from the math contest forum and 21 new IMO questions. We open-source our code at https://github.com/InternLM/InternLM-Math and our data at https://huggingface.co/datasets/InternLM/Lean-Workbook.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Survey of Natural Language Processing for the Greek Language</title>
<link>https://arxiv.org/abs/2407.09861</link>
<guid>https://arxiv.org/abs/2407.09861</guid>
<content:encoded><![CDATA[
arXiv:2407.09861v4 Announce Type: replace 
Abstract: Comprehensive monolingual Natural Language Processing (NLP) surveys are essential for assessing language-specific challenges, resource availability, and research gaps. However, existing surveys often lack standardized methodologies, leading to selection bias and fragmented coverage of NLP tasks and resources. This study introduces a generalizable framework for systematic monolingual NLP surveys. Our approach integrates a structured search protocol to minimize bias, an NLP task taxonomy for classification, and language resource taxonomies to identify potential benchmarks and highlight opportunities for improving resource availability. We apply this framework to Greek NLP (2012-2023), providing an in-depth analysis of its current state, task-specific progress, and resource gaps. The survey results are publicly available (https://doi.org/10.5281/zenodo.15314882) and are regularly updated to provide an evergreen resource. This systematic survey of Greek NLP serves as a case study, demonstrating the effectiveness of our framework and its potential for broader application to other not so well-resourced languages as regards NLP.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Utility-Preserving Text Anonymization Based on Large Language Models</title>
<link>https://arxiv.org/abs/2407.11770</link>
<guid>https://arxiv.org/abs/2407.11770</guid>
<content:encoded><![CDATA[
arXiv:2407.11770v2 Announce Type: replace 
Abstract: Anonymizing text that contains sensitive information is crucial for a wide range of applications. Existing techniques face the emerging challenges of the re-identification ability of large language models (LLMs), which have shown advanced capability in memorizing detailed information and reasoning over dispersed pieces of patterns to draw conclusions. When defending against LLM-based re-identification, anonymization could jeopardize the utility of the resulting anonymized data in downstream tasks. In general, the interaction between anonymization and data utility requires a deeper understanding within the context of LLMs. In this paper, we propose a framework composed of three key LLM-based components: a privacy evaluator, a utility evaluator, and an optimization component, which work collaboratively to perform anonymization. Extensive experiments demonstrate that the proposed model outperforms existing baselines, showing robustness in reducing the risk of re-identification while preserving greater data utility in downstream tasks. We provide detailed studies on these core modules. To consider large-scale and real-time applications, we investigate the distillation of the anonymization capabilities into lightweight models. All of our code and datasets will be made publicly available at https://github.com/UKPLab/acl2025-rupta.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadioRAG: Online Retrieval-augmented Generation for Radiology Question Answering</title>
<link>https://arxiv.org/abs/2407.15621</link>
<guid>https://arxiv.org/abs/2407.15621</guid>
<content:encoded><![CDATA[
arXiv:2407.15621v3 Announce Type: replace 
Abstract: Large language models (LLMs) often generate outdated or inaccurate information based on static training datasets. Retrieval-augmented generation (RAG) mitigates this by integrating outside data sources. While previous RAG systems used pre-assembled, fixed databases with limited flexibility, we have developed Radiology RAG (RadioRAG), an end-to-end framework that retrieves data from authoritative radiologic online sources in real-time. We evaluate the diagnostic accuracy of various LLMs when answering radiology-specific questions with and without access to additional online information via RAG. Using 80 questions from the RSNA Case Collection across radiologic subspecialties and 24 additional expert-curated questions with reference standard answers, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B and 70B]) were prompted with and without RadioRAG in a zero-shot inference scenario RadioRAG retrieved context-specific information from Radiopaedia in real-time. Accuracy was investigated. Statistical analyses were performed using bootstrapping. The results were further compared with human performance. RadioRAG improved diagnostic accuracy across most LLMs, with relative accuracy increases ranging up to 54% for different LLMs. It matched or exceeded non-RAG models and the human radiologist in question answering across radiologic subspecialties, particularly in breast imaging and emergency radiology. However, the degree of improvement varied among models; GPT-3.5-turbo and Mixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2 showed no improvement, highlighting variability in RadioRAG's effectiveness. LLMs benefit when provided access to domain-specific data beyond their training data. RadioRAG shows potential to improve LLM accuracy and factuality in radiology question answering by integrating real-time domain-specific data.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level</title>
<link>https://arxiv.org/abs/2410.06809</link>
<guid>https://arxiv.org/abs/2410.06809</guid>
<content:encoded><![CDATA[
arXiv:2410.06809v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated immense utility across various industries. However, as LLMs advance, the risk of harmful outputs increases due to incorrect or malicious instruction prompts. While current methods effectively address jailbreak risks, they share common limitations: 1) Judging harmful responses from the prefill-level lacks utilization of the model's decoding outputs, leading to relatively lower effectiveness and robustness. 2) Rejecting potentially harmful responses based on a single evaluation can significantly impair the model's helpfulness.This paper examines the LLMs' capability to recognize harmful outputs, revealing and quantifying their proficiency in assessing the danger of previous tokens. Motivated by pilot experiment results, we design a robust defense mechanism at the decoding level. Our novel decoder-oriented, step-by-step defense architecture corrects harmful queries directly rather than rejecting them outright. We introduce speculative decoding to enhance usability and facilitate deployment to boost secure decoding speed. Extensive experiments demonstrate that our approach improves model security without compromising reasoning speed. Notably, our method leverages the model's ability to discern hazardous information, maintaining its helpfulness compared to existing methods.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with Patent-Paper Pairs</title>
<link>https://arxiv.org/abs/2410.07009</link>
<guid>https://arxiv.org/abs/2410.07009</guid>
<content:encoded><![CDATA[
arXiv:2410.07009v3 Announce Type: replace 
Abstract: Dealing with long and highly complex technical text is a challenge for Large Language Models (LLMs), which still have to unfold their potential in supporting expensive and timeintensive processes like patent drafting. Within patents, the description constitutes more than 90% of the document on average. Yet, its automatic generation remains understudied. When drafting patent applications, patent attorneys typically receive invention reports (IRs), which are usually confidential, hindering research on LLM-supported patent drafting. Often, prepublication research papers serve as IRs. We leverage this duality to build PAP2PAT, an open and realistic benchmark for patent drafting consisting of 1.8k patent-paper pairs describing the same inventions. To address the complex longdocument patent generation task, we propose chunk-based outline-guided generation using the research paper as invention specification. Our extensive evaluation using PAP2PAT and a human case study show that LLMs can effectively leverage information from the paper, but still struggle to provide the necessary level of detail. Fine-tuning leads to more patent-style language, but also to more hallucination. We release our data and code https://github.com/boschresearch/Pap2Pat.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes</title>
<link>https://arxiv.org/abs/2410.16930</link>
<guid>https://arxiv.org/abs/2410.16930</guid>
<content:encoded><![CDATA[
arXiv:2410.16930v4 Announce Type: replace 
Abstract: Math reasoning is an active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence and has implications in several domains, including math education. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be isolated within models. Doing so could allow targeted intervention to improve math performance without altering non-math behavior and foster understanding of how models encode math reasoning. We introduce Math Neurosurgery (MathNeuro), a computationally efficient method we use to isolate math-specific parameters in LLMs using only forward passes. MathNeuro builds on existing work by using weights and activations to calculate parameter importance, but isolates math-specific parameters by filtering out those important for general language tasks. Through pruning parameters MathNeuro identifies, we delete a LLM's math reasoning ability without significantly impacting its general language ability. Scaling the identified parameters by a small constant improves a pretrained or instruction-tuned LLM's performance by 4-17% on GSM8K and 5-35% on MATH while leaving non-math behavior unaltered. MathNeuro is also data efficient: most of its effectiveness holds when identifying math-specific parameters using a single sample. MathNeuro highlights the potential for future work to intervene on math-specific parameters.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interchangeable Token Embeddings for Extendable Vocabulary and Alpha-Equivalence</title>
<link>https://arxiv.org/abs/2410.17161</link>
<guid>https://arxiv.org/abs/2410.17161</guid>
<content:encoded><![CDATA[
arXiv:2410.17161v3 Announce Type: replace 
Abstract: Language models lack the notion of interchangeable tokens: symbols that are semantically equivalent yet distinct, such as bound variables in formal logic. This limitation prevents generalization to larger vocabularies and hinders the model's ability to recognize alpha-equivalence, where renaming bound variables preserves meaning. We formalize this machine learning problem and introduce alpha-covariance, a metric for evaluating robustness to such transformations. To tackle this task, we propose a dual-part token embedding strategy: a shared component ensures semantic consistency, while a randomized component maintains token distinguishability. Compared to a baseline that relies on alpha-renaming for data augmentation, our approach demonstrates improved generalization to unseen tokens in linear temporal logic solving, propositional logic assignment prediction, and copying with an extendable vocabulary, while introducing a favorable inductive bias for alpha-equivalence. Our findings establish a foundation for designing language models that can learn interchangeable token representations, a crucial step toward more flexible and systematic reasoning in formal domains. Our code and project page are available at https://necrashter.github.io/interchangeable-token-embeddings
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LL\"aMmlein: Transparent, Compact and Competitive German-Only Language Models from Scratch</title>
<link>https://arxiv.org/abs/2411.11171</link>
<guid>https://arxiv.org/abs/2411.11171</guid>
<content:encoded><![CDATA[
arXiv:2411.11171v5 Announce Type: replace 
Abstract: We create two German-only decoder models, LL\"aMmlein 120M and 1B, transparently from scratch and publish them, along with the training data, for the German NLP research community to use. The model training involved several key steps, including extensive data preprocessing, the creation of a custom German tokenizer, the training itself, as well as the evaluation of the final models on various benchmarks. Throughout the training process, multiple checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor the models' learning dynamics. Compared to state-of-the-art models on the SuperGLEBer benchmark, both LL\"aMmlein models performed competitively, consistently matching or surpassing models with similar parameter sizes. The results show that the models' quality scales with size as expected, but performance improvements on some tasks plateaued early, offering valuable insights into resource allocation for future model development.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REVOLVE: Optimizing AI Systems by Tracking Response Evolution in Textual Optimization</title>
<link>https://arxiv.org/abs/2412.03092</link>
<guid>https://arxiv.org/abs/2412.03092</guid>
<content:encoded><![CDATA[
arXiv:2412.03092v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have significantly enhanced the ability of LLM-based systems to perform complex tasks through natural language processing and tool interaction. However, optimizing these LLM-based systems for specific tasks remains challenging, often requiring manual interventions like prompt engineering and hyperparameter tuning. Existing automatic optimization methods, such as textual feedback-based techniques (e.g., TextGrad), tend to focus on immediate feedback, analogous to using immediate derivatives in traditional numerical gradient descent. However, relying solely on such feedback can be limited when the adjustments made in response to this feedback are either too small or fluctuate irregularly, potentially slowing down or even stalling the optimization process. To overcome these challenges, more adaptive methods are needed, especially in situations where the system's response is evolving slowly or unpredictably. In this paper, we introduce REVOLVE, an optimization method that tracks how "R"esponses "EVOLVE" across iterations in LLM systems. By focusing on the evolution of responses over time, REVOLVE enables more stable and effective optimization by making thoughtful, progressive adjustments at each step. Experimental results demonstrate that REVOLVE outperforms competitive baselines, achieving a 7.8% improvement in prompt optimization, a 20.72% gain in solution refinement, and a 29.17% increase in code optimization. Additionally, REVOLVE converges in fewer iterations, resulting in significant computational savings. Beyond its practical contributions, REVOLVE highlights a promising direction, where the rich knowledge from established optimization principles can be leveraged to enhance LLM systems, which paves the way for further advancements in this hybrid domain.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition</title>
<link>https://arxiv.org/abs/2412.13612</link>
<guid>https://arxiv.org/abs/2412.13612</guid>
<content:encoded><![CDATA[
arXiv:2412.13612v4 Announce Type: replace 
Abstract: Large language models (LLMs) have emerged as a potential solution to automate the complex processes involved in writing literature reviews, such as literature collection, organization, and summarization. However, it is yet unclear how good LLMs are at automating comprehensive and reliable literature reviews. This study introduces a framework to automatically evaluate the performance of LLMs in three key tasks of literature writing: reference generation, literature summary, and literature review composition. We introduce multidimensional evaluation metrics that assess the hallucination rates in generated references and measure the semantic coverage and factual consistency of the literature summaries and compositions against human-written counterparts. The experimental results reveal that even the most advanced models still generate hallucinated references, despite recent progress. Moreover, we observe that the performance of different models varies across disciplines when it comes to writing literature reviews. These findings highlight the need for further research and development to improve the reliability of LLMs in automating academic literature reviews.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning AI Research with the Needs of Clinical Coding Workflows: Eight Recommendations Based on US Data Analysis and Critical Review</title>
<link>https://arxiv.org/abs/2412.18043</link>
<guid>https://arxiv.org/abs/2412.18043</guid>
<content:encoded><![CDATA[
arXiv:2412.18043v2 Announce Type: replace 
Abstract: Clinical coding is crucial for healthcare billing and data analysis. Manual clinical coding is labour-intensive and error-prone, which has motivated research towards full automation of the process. However, our analysis, based on US English electronic health records and automated coding research using these records, shows that widely used evaluation methods are not aligned with real clinical contexts. For example, evaluations that focus on the top 50 most common codes are an oversimplification, as there are thousands of codes used in practice. This position paper aims to align AI coding research more closely with practical challenges of clinical coding. Based on our analysis, we offer eight specific recommendations, suggesting ways to improve current evaluation methods. Additionally, we propose new AI-based methods beyond automated coding, suggesting alternative approaches to assist clinical coders in their workflows.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Ask Good Questions?</title>
<link>https://arxiv.org/abs/2501.03491</link>
<guid>https://arxiv.org/abs/2501.03491</guid>
<content:encoded><![CDATA[
arXiv:2501.03491v2 Announce Type: replace 
Abstract: We evaluate questions generated by large language models (LLMs) from context, comparing them to human-authored questions across six dimensions: question type, question length, context coverage, answerability, uncommonness, and required answer length. Our study spans two open-source and two proprietary state-of-the-art models. Results reveal that LLM-generated questions tend to demand longer descriptive answers and exhibit more evenly distributed context focus, in contrast to the positional bias often seen in QA tasks. These findings provide insights into the distinctive characteristics of LLM-generated questions and inform future work on question quality and downstream applications.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perspective Transition of Large Language Models for Solving Subjective Tasks</title>
<link>https://arxiv.org/abs/2501.09265</link>
<guid>https://arxiv.org/abs/2501.09265</guid>
<content:encoded><![CDATA[
arXiv:2501.09265v2 Announce Type: replace 
Abstract: Large language models (LLMs) have revolutionized the field of natural language processing, enabling remarkable progress in various tasks. Different from objective tasks such as commonsense reasoning and arithmetic question-answering, the performance of LLMs on subjective tasks is still limited, where the perspective on the specific problem plays crucial roles for better interpreting the context and giving proper response. For example, in certain scenarios, LLMs may perform better when answering from an expert role perspective, potentially eliciting their relevant domain knowledge. In contrast, in some scenarios, LLMs may provide more accurate responses when answering from a third-person standpoint, enabling a more comprehensive understanding of the problem and potentially mitigating inherent biases. In this paper, we propose Reasoning through Perspective Transition (RPT), a method based on in-context learning that enables LLMs to dynamically select among direct, role, and third-person perspectives for the best way to solve corresponding subjective problem. Through extensive experiments on totally 12 subjective tasks by using both closed-source and open-source LLMs including GPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single fixed perspective based methods such as chain-of-thought prompting and expert prompting, highlights the intricate ways that LLMs can adapt their perspectives to provide nuanced and contextually appropriate responses for different problems.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2502.14693</link>
<guid>https://arxiv.org/abs/2502.14693</guid>
<content:encoded><![CDATA[
arXiv:2502.14693v3 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have shown remarkable potential in automating machine learning tasks. However, existing LLM-based agents often struggle with low-diversity and suboptimal code generation. While recent work has introduced Monte Carlo Tree Search (MCTS) to address these issues, limitations persist in the quality and diversity of thoughts generated, as well as in the scalar value feedback mechanisms used for node selection. In this study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a novel approach that iteratively expands tree nodes through an introspective process that meticulously analyzes solutions and results from parent and sibling nodes. This facilitates a continuous refinement of the node in the search tree, thereby enhancing the overall decision-making process. Furthermore, we integrate a Large Language Model (LLM)-based value model to facilitate direct evaluation of each node's solution prior to conducting comprehensive computational rollouts. A hybrid rewarding mechanism is implemented to seamlessly transition the Q-value from LLM-estimated scores to actual performance scores. This allows higher-quality nodes to be traversed earlier. Applied to the various ML tasks, our approach demonstrates a 6% absolute improvement in performance compared to the strong open-source AutoML agents, showcasing its effectiveness in enhancing agentic AutoML systems. Resource available at https://github.com/jokieleung/I-MCTS
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale</title>
<link>https://arxiv.org/abs/2502.16645</link>
<guid>https://arxiv.org/abs/2502.16645</guid>
<content:encoded><![CDATA[
arXiv:2502.16645v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: https://github.com/Lucky-voyage/Code-Sync.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alleviating Distribution Shift in Synthetic Data for Machine Translation Quality Estimation</title>
<link>https://arxiv.org/abs/2502.19941</link>
<guid>https://arxiv.org/abs/2502.19941</guid>
<content:encoded><![CDATA[
arXiv:2502.19941v3 Announce Type: replace 
Abstract: Quality Estimation (QE) models evaluate the quality of machine translations without reference translations, serving as the reward models for the translation task. Due to the data scarcity, synthetic data generation has emerged as a promising solution. However, synthetic QE data often suffers from distribution shift, which can manifest as discrepancies between pseudo and real translations, or in pseudo labels that do not align with human preferences. To tackle this issue, we introduce DCSQE, a novel framework for alleviating distribution shift in synthetic QE data. To reduce the difference between pseudo and real translations, we employ the constrained beam search algorithm and enhance translation diversity through the use of distinct generation models. DCSQE uses references, i.e., translation supervision signals, to guide both the generation and annotation processes, enhancing the quality of token-level labels. DCSQE further identifies the shortest phrase covering consecutive error tokens, mimicking human annotation behavior, to assign the final phrase-level labels. Specially, we underscore that the translation model can not annotate translations of itself accurately. Extensive experiments demonstrate that DCSQE outperforms SOTA baselines like CometKiwi in both supervised and unsupervised settings. Further analysis offers insights into synthetic data generation that could benefit reward models for other tasks. The code is available at https://github.com/NJUNLP/njuqe.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PsychBench: A comprehensive and professional benchmark for evaluating the performance of LLM-assisted psychiatric clinical practice</title>
<link>https://arxiv.org/abs/2503.01903</link>
<guid>https://arxiv.org/abs/2503.01903</guid>
<content:encoded><![CDATA[
arXiv:2503.01903v2 Announce Type: replace 
Abstract: The advent of Large Language Models (LLMs) offers potential solutions to address problems such as shortage of medical resources and low diagnostic consistency in psychiatric clinical practice. Despite this potential, a robust and comprehensive benchmarking framework to assess the efficacy of LLMs in authentic psychiatric clinical environments is absent. This has impeded the advancement of specialized LLMs tailored to psychiatric applications. In response to this gap, by incorporating clinical demands in psychiatry and clinical data, we proposed a benchmarking system, PsychBench, to evaluate the practical performance of LLMs in psychiatric clinical settings. We conducted a comprehensive quantitative evaluation of 16 LLMs using PsychBench, and investigated the impact of prompt design, chain-of-thought reasoning, input text length, and domain-specific knowledge fine-tuning on model performance. Through detailed error analysis, we identified strengths and potential limitations of the existing models and suggested directions for improvement. Subsequently, a clinical reader study involving 60 psychiatrists of varying seniority was conducted to further explore the practical benefits of existing LLMs as supportive tools for psychiatrists of varying seniority. Through the quantitative and reader evaluation, we show that while existing models demonstrate significant potential, they are not yet adequate as decision-making tools in psychiatric clinical practice. The reader study further indicates that, as an auxiliary tool, LLM could provide particularly notable support for junior psychiatrists, effectively enhancing their work efficiency and overall clinical quality. To promote research in this area, we will make the dataset and evaluation framework publicly available, with the hope of advancing the application of LLMs in psychiatric clinical settings.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adding Chocolate to Mint: Mitigating Metric Interference in Machine Translation</title>
<link>https://arxiv.org/abs/2503.08327</link>
<guid>https://arxiv.org/abs/2503.08327</guid>
<content:encoded><![CDATA[
arXiv:2503.08327v2 Announce Type: replace 
Abstract: As automatic metrics become increasingly stronger and widely adopted, the risk of unintentionally "gaming the metric" during model development rises. This issue is caused by metric interference (MINT), i.e., the use of the same or related metrics for both model tuning and evaluation. MINT can misguide practitioners into being overoptimistic about the performance of their systems: as system outputs become a function of the interfering metric, their estimated quality loses correlation with human judgments. In this work, we analyze two common cases of MINT in machine translation-related tasks: filtering of training data, and decoding with quality signals. Importantly, we find that MINT strongly distorts instance-level metric scores, even when metrics are not directly optimized for-questioning the common strategy of leveraging a different, yet related metric for evaluation that is not used for tuning. To address this problem, we propose MINTADJUST, a method for more reliable evaluation under MINT. On the WMT24 MT shared task test set, MINTADJUST ranks translations and systems more accurately than state-of-the-art metrics across a majority of language pairs, especially for high-quality systems. Furthermore, MINTADJUST outperforms AUTORANK, the ensembling method used by the organizers.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resolving UnderEdit &amp; OverEdit with Iterative &amp; Neighbor-Assisted Model Editing</title>
<link>https://arxiv.org/abs/2503.11895</link>
<guid>https://arxiv.org/abs/2503.11895</guid>
<content:encoded><![CDATA[
arXiv:2503.11895v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are widely deployed in downstream tasks, but keeping their knowledge up-to-date via retraining or fine-tuning is often computationally expensive. Model editing provides a more efficient alternative by updating a targeted subset of parameters, which often follows the locate-and-edit paradigm. Despite this efficiency, existing methods are limited: edits may fail to inject knowledge (UnderEdit) or unintentionally disrupt unrelated neighboring knowledge (OverEdit). To address these challenges, we propose two complementary methods: iterative model editing, which applies successive edits to mitigate UnderEdit, and neighbor-assisted model editing, which incorporates neighboring knowledge during editing to reduce OverEdit. Our extensive experiments show that these techniques improve editing performance across multiple LLMs, algorithms, and benchmarks, reducing UnderEdit by up to 38 percentage points and OverEdit by up to 6, while remaining broadly applicable to any locate-and-edit method.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions</title>
<link>https://arxiv.org/abs/2504.20304</link>
<guid>https://arxiv.org/abs/2504.20304</guid>
<content:encoded><![CDATA[
arXiv:2504.20304v3 Announce Type: replace 
Abstract: CHILDES is a widely used resource of transcribed child and child-directed speech. This paper introduces UD-English-CHILDES, the first officially released Universal Dependencies (UD) treebank. It is derived from previously dependency-annotated CHILDES data, which we harmonize to follow unified annotation principles. The gold-standard trees encompass utterances sampled from 11 children and their caregivers, totaling over 48K sentences (236K tokens). We validate these gold-standard annotations under the UD v2 framework and provide an additional 1M~silver-standard sentences, offering a consistent resource for computational and linguistic research.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks</title>
<link>https://arxiv.org/abs/2505.07890</link>
<guid>https://arxiv.org/abs/2505.07890</guid>
<content:encoded><![CDATA[
arXiv:2505.07890v4 Announce Type: replace 
Abstract: This study presents TSLFormer, a light and robust word-level Turkish Sign Language (TSL) recognition model that treats sign gestures as ordered, string-like language. Instead of using raw RGB or depth videos, our method only works with 3D joint positions - articulation points - extracted using Google's Mediapipe library, which focuses on the hand and torso skeletal locations. This creates efficient input dimensionality reduction while preserving important semantic gesture information. Our approach revisits sign language recognition as sequence-to-sequence translation, inspired by the linguistic nature of sign languages and the success of transformers in natural language processing. Since TSLFormer uses the self-attention mechanism, it effectively captures temporal co-occurrence within gesture sequences and highlights meaningful motion patterns as words unfold. Evaluated on the AUTSL dataset with over 36,000 samples and 227 different words, TSLFormer achieves competitive performance with minimal computational cost. These results show that joint-based input is sufficient for enabling real-time, mobile, and assistive communication systems for hearing-impaired individuals.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model</title>
<link>https://arxiv.org/abs/2505.11810</link>
<guid>https://arxiv.org/abs/2505.11810</guid>
<content:encoded><![CDATA[
arXiv:2505.11810v3 Announce Type: replace 
Abstract: General-purpose large language models demonstrate notable capabilities in language comprehension and generation, achieving results that are comparable to, or even surpass, human performance in many natural language processing tasks. Nevertheless, when general models are applied to some specific domains, e.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and fine-tuning open-source foundational models similarly struggles to adequately incorporate domain-specific knowledge. To address this challenge, this study developed a large language model, AI Taiyan, specifically designed for understanding and generating Classical Chinese. Experiments show that with a reasonable model design, data processing, foundational training, and fine-tuning, satisfactory results can be achieved with only 1.8 billion parameters. In key tasks related to language processing of Classical Chinese such as punctuation, identification of allusions, explanation of word meanings, and translation between ancient and modern Chinese, this model exhibits a clear advantage over both general-purpose large models and domain-specific traditional models, achieving levels close to or surpassing human baselines. This research provides a reference for the efficient construction of specialized domain-specific large language models. Furthermore, the paper discusses the application of this model in fields such as the collation of ancient texts, dictionary editing, and language research, combined with case studies.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>J4R: Learning to Judge with Equivalent Initial State Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2505.13346</link>
<guid>https://arxiv.org/abs/2505.13346</guid>
<content:encoded><![CDATA[
arXiv:2505.13346v3 Announce Type: replace 
Abstract: To keep pace with the increasing pace of large language models (LLM) development, model output evaluation has transitioned away from time-consuming human evaluation to automatic evaluation, where LLMs themselves are tasked with assessing and critiquing other model outputs. LLM-as-judge models are a class of generative evaluators that excel in evaluating relatively simple domains, like chat quality, but struggle in reasoning intensive domains where model responses contain more substantive and challenging content. To remedy existing judge shortcomings, we explore training judges with reinforcement learning (RL). We make three key contributions: (1) We propose the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us to train our judge to be robust to positional biases that arise in more complex evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that evaluates judges in diverse reasoning settings not covered by prior work. (3) We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or exceeding the performance of larger GRPO-trained judges on both JudgeBench and ReasoningJudgeBench.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and Citations</title>
<link>https://arxiv.org/abs/2505.17267</link>
<guid>https://arxiv.org/abs/2505.17267</guid>
<content:encoded><![CDATA[
arXiv:2505.17267v2 Announce Type: replace 
Abstract: We introduce GreekBarBench, a benchmark that evaluates LLMs on legal questions across five different legal areas from the Greek Bar exams, requiring citations to statutory articles and case facts. To tackle the challenges of free-text evaluation, we propose a three-dimensional scoring system combined with an LLM-as-a-judge approach. We also develop a meta-evaluation benchmark to assess the correlation between LLM-judges and human expert evaluations, revealing that simple, span-based rubrics improve their alignment. Our systematic evaluation of 13 proprietary and open-weight LLMs shows that even though the best models outperform average expert scores, they fall short of the 95th percentile of experts.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Long CoT Reasoning in Small Language Models</title>
<link>https://arxiv.org/abs/2505.18440</link>
<guid>https://arxiv.org/abs/2505.18440</guid>
<content:encoded><![CDATA[
arXiv:2505.18440v2 Announce Type: replace 
Abstract: Recent large reasoning models such as DeepSeek-R1 exhibit strong complex problems solving abilities by generating long chain-of-thought (CoT) reasoning steps. It is challenging to directly train small language models (SLMs) to emerge long CoT. Thus, distillation becomes a practical method to enable SLMs for such reasoning ability. However, the long CoT often contains a lot of redundant contents (e.g., overthinking steps) which may make SLMs hard to learn considering their relatively poor capacity and generalization. To address this issue, we propose a simple-yet-effective method to prune unnecessary steps in long CoT, and then employ an on-policy method for the SLM itself to curate valid and useful long CoT training data. In this way, SLMs can effectively learn efficient long CoT reasoning and preserve competitive performance at the same time. Experimental results across a series of mathematical reasoning benchmarks demonstrate the effectiveness of the proposed method in distilling long CoT reasoning ability into SLMs which maintains the competitive performance but significantly reduces generating redundant reasoning steps.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2505.18799</link>
<guid>https://arxiv.org/abs/2505.18799</guid>
<content:encoded><![CDATA[
arXiv:2505.18799v4 Announce Type: replace 
Abstract: Aligning general-purpose large language models (LLMs) to downstream tasks often incurs significant training adjustment costs. Prior research has explored various avenues to enhance alignment efficiency, primarily through minimal-data training or data-driven activations to identify key attention heads. However, these approaches inherently introduce data dependency, which hinders generalization and reusability. To address this issue and enhance model alignment efficiency, we propose the Attention Localization and Pruning Strategy (ALPS), an efficient algorithm that localizes the most task-sensitive attention heads and prunes by restricting attention training updates to these heads, thereby reducing alignment costs. Experimental results demonstrate that our method activates only 10% of attention parameters during fine-tuning while achieving a 2% performance improvement over baselines on three tasks. Moreover, the identified task-specific heads are transferable across datasets and mitigate knowledge forgetting. Our work and findings provide a novel perspective on efficient LLM alignment. The code is available at https://github.com/VoiceBeer/ALPS.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants</title>
<link>https://arxiv.org/abs/2505.19797</link>
<guid>https://arxiv.org/abs/2505.19797</guid>
<content:encoded><![CDATA[
arXiv:2505.19797v3 Announce Type: replace 
Abstract: Proprietary giants are increasingly dominating the race for ever-larger language models. Can open-source, smaller models remain competitive across a broad range of tasks? In this paper, we present the Avengers -- a simple recipe that leverages the collective intelligence of these smaller models. The Avengers builds upon four lightweight operations: (i) embedding: encode queries using a text embedding model; (ii) clustering: group queries based on their semantic similarity; (iii) scoring: scores each model's performance within each cluster; and (iv) voting: improve outputs via repeated sampling and voting. At inference time, each query is embedded and assigned to its nearest cluster. The top-performing model(s) within that cluster are selected to generate the response with repeated sampling. Remarkably, with 10 open-source models (~7B parameters each), the Avengers surpasses GPT-4o, 4.1, and 4.5 in average performance across 15 diverse datasets spanning mathematics, coding, logical reasoning, general knowledge, and affective tasks. In particular, it surpasses GPT-4.1 on mathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the Avengers delivers superior out-of-distribution generalization, and remains robust across various embedding models, clustering algorithms, ensemble strategies, and values of its sole parameter -- the number of clusters.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims</title>
<link>https://arxiv.org/abs/2505.21342</link>
<guid>https://arxiv.org/abs/2505.21342</guid>
<content:encoded><![CDATA[
arXiv:2505.21342v3 Announce Type: replace 
Abstract: Patent claims define the scope of protection for an invention. If there are ambiguities in a claim, it is rejected by the patent office. In the US, this is referred to as indefiniteness (35 U.S.C {\S} 112(b)) and is among the most frequent reasons for patent application rejection. The development of automatic methods for patent definiteness examination has the potential to make patent drafting and examination more efficient, but no annotated dataset has been published to date. We introduce PEDANTIC (Patent Definiteness Examination Corpus), a novel dataset of 14k US patent claims from patent applications relating to Natural Language Processing (NLP), annotated with reasons for indefiniteness. We construct PEDANTIC using a fully automatic pipeline that retrieves office action documents from the USPTO and uses Large Language Models (LLMs) to extract the reasons for indefiniteness. A human validation study confirms the pipeline's accuracy in generating high-quality annotations. To gain insight beyond binary classification metrics, we implement an LLM-as-Judge evaluation that compares the free-form reasoning of every model-cited reason with every examiner-cited reason. We show that LLM agents based on Qwen 2.5 32B and 72B struggle to outperform logistic regression baselines on definiteness prediction, even though they often correctly identify the underlying reasons. PEDANTIC provides a valuable resource for patent AI researchers, enabling the development of advanced examination models. We will publicly release the dataset and code.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How much do language models memorize?</title>
<link>https://arxiv.org/abs/2505.24832</link>
<guid>https://arxiv.org/abs/2505.24832</guid>
<content:encoded><![CDATA[
arXiv:2505.24832v3 Announce Type: replace 
Abstract: We propose a new method for estimating how much a model knows about a datapoint and use it to measure the capacity of modern language models. Prior studies of language model memorization have struggled to disentangle memorization from generalization. We formally separate memorization into two components: unintended memorization, the information a model contains about a specific dataset, and generalization, the information a model contains about the true data-generation process. When we completely eliminate generalization, we can compute the total memorization, which provides an estimate of model capacity: our measurements estimate that GPT-style models have a capacity of approximately 3.6 bits per parameter. We train language models on datasets of increasing size and observe that models memorize until their capacity fills, at which point "grokking" begins, and unintended memorization decreases as models begin to generalize. We train hundreds of transformer language models ranging from $500K$ to $1.5B$ parameters and produce a series of scaling laws relating model capacity and data size to membership inference.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking</title>
<link>https://arxiv.org/abs/2506.02803</link>
<guid>https://arxiv.org/abs/2506.02803</guid>
<content:encoded><![CDATA[
arXiv:2506.02803v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) excel in semantic tasks but falter at a core human capability: detecting hidden content in optical illusions or AI-generated images through perceptual adjustments like zooming. We introduce HC-Bench, a benchmark of 112 images with hidden text, objects, and illusions, revealing that leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit prompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to an overreliance on high-level semantics. Strikingly, we propose SemVink (Semantic Visual Thinking) by simply scaling images to low resolutions (32-128 pixels), which unlocks >99% accuracy by eliminating redundant visual noise. This exposes a critical architectural flaw: VLMs prioritize abstract reasoning over low-level visual operations crucial for real-world robustness. Our work urges a shift toward hybrid models integrating multi-scale processing, bridging the gap between computational vision and human cognition for applications in medical imaging, security, and beyond.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles</title>
<link>https://arxiv.org/abs/2506.06561</link>
<guid>https://arxiv.org/abs/2506.06561</guid>
<content:encoded><![CDATA[
arXiv:2506.06561v2 Announce Type: replace 
Abstract: Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs</title>
<link>https://arxiv.org/abs/2506.06619</link>
<guid>https://arxiv.org/abs/2506.06619</guid>
<content:encoded><![CDATA[
arXiv:2506.06619v2 Announce Type: replace 
Abstract: A core part of legal work that has been under-explored in Legal NLP is the writing and editing of legal briefs. This requires not only a thorough understanding of the law of a jurisdiction, from judgments to statutes, but also the ability to make new arguments to try to expand the law in a new direction and make novel and creative arguments that are persuasive to judges. To capture and evaluate these legal skills in language models, we introduce BRIEFME, a new dataset focused on legal briefs. It contains three tasks for language models to assist legal professionals in writing briefs: argument summarization, argument completion, and case retrieval. In this work, we describe the creation of these tasks, analyze them, and show how current models perform. We see that today's large language models (LLMs) are already quite good at the summarization and guided completion tasks, even beating human-generated headings. Yet, they perform poorly on other tasks in our benchmark: realistic argument completion and retrieving relevant legal cases. We hope this dataset encourages more development in Legal NLP in ways that will specifically aid people in performing legal work.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning</title>
<link>https://arxiv.org/abs/2506.06955</link>
<guid>https://arxiv.org/abs/2506.06955</guid>
<content:encoded><![CDATA[
arXiv:2506.06955v2 Announce Type: replace 
Abstract: We present BIS Reasoning 1.0, the first large-scale Japanese dataset of syllogistic reasoning problems explicitly designed to evaluate belief-inconsistent reasoning in large language models (LLMs). Unlike prior datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent syllogisms to uncover reasoning biases in LLMs trained on human-aligned corpora. We benchmark state-of-the-art models - including GPT models, Claude models, and leading Japanese LLMs - revealing significant variance in performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies critical weaknesses in current LLMs when handling logically valid but belief-conflicting inputs. These findings have important implications for deploying LLMs in high-stakes domains such as law, healthcare, and scientific literature, where truth must override intuitive belief to ensure integrity and safety.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-VLDoc: Bidirectional Vision-Language Modeling for Visually-Rich Document Understanding</title>
<link>https://arxiv.org/abs/2206.13155</link>
<guid>https://arxiv.org/abs/2206.13155</guid>
<content:encoded><![CDATA[
arXiv:2206.13155v2 Announce Type: replace-cross 
Abstract: Multi-modal document pre-trained models have proven to be very effective in a variety of visually-rich document understanding (VrDU) tasks. Though existing document pre-trained models have achieved excellent performance on standard benchmarks for VrDU, the way they model and exploit the interactions between vision and language on documents has hindered them from better generalization ability and higher accuracy. In this work, we investigate the problem of vision-language joint representation learning for VrDU mainly from the perspective of supervisory signals. Specifically, a pre-training paradigm called Bi-VLDoc is proposed, in which a bidirectional vision-language supervision strategy and a vision-language hybrid-attention mechanism are devised to fully explore and utilize the interactions between these two modalities, to learn stronger cross-modal document representations with richer semantics. Benefiting from the learned informative cross-modal document representations, Bi-VLDoc significantly advances the state-of-the-art performance on three widely-used document understanding benchmarks, including Form Understanding (from 85.14% to 93.44%), Receipt Information Extraction (from 96.01% to 97.84%), and Document Classification (from 96.08% to 97.12%). On Document Visual QA, Bi-VLDoc achieves the state-of-the-art performance compared to previous single model methods.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OM4OV: Leveraging Ontology Matching for Ontology Versioning</title>
<link>https://arxiv.org/abs/2409.20302</link>
<guid>https://arxiv.org/abs/2409.20302</guid>
<content:encoded><![CDATA[
arXiv:2409.20302v3 Announce Type: replace-cross 
Abstract: Due to the dynamic nature of the Semantic Web, version control is necessary to capture time-varying information, particularly for widely used ontologies. Despite the long-standing recognition of ontology versioning (OV) as a crucial component for efficient ontology management, the growing size of ontologies and accumulating errors caused by manual labour overwhelm current OV approaches. In this paper, we propose yet another approach to performing OV using existing ontology matching (OM) techniques and systems. We introduce a unified OM4OV pipeline. From an OM perspective, we reconstruct a new task formulation and measurement for OV tasks. Building upon the prior alignment(s) from OM, we propose a pipeline optimisation method called the cross-reference (CR) mechanism to enhance overall OV performance. We experimentally validate the OM4OV pipeline and the cross-reference mechanism in the OV tested originating from the Ontology Alignment Evaluation Initiative (OAEI) datasets. We also discuss insights into OM used for OV tasks, where some false mappings detected by OV systems are not actually untrue.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Guide to Misinformation Detection Data and Evaluation</title>
<link>https://arxiv.org/abs/2411.05060</link>
<guid>https://arxiv.org/abs/2411.05060</guid>
<content:encoded><![CDATA[
arXiv:2411.05060v4 Announce Type: replace-cross 
Abstract: Misinformation is a complex societal issue, and mitigating solutions are difficult to create due to data deficiencies. To address this, we have curated the largest collection of (mis)information datasets in the literature, totaling 75. From these, we evaluated the quality of 36 datasets that consist of statements or claims, as well as the 9 datasets that consist of data in purely paragraph form. We assess these datasets to identify those with solid foundations for empirical work and those with flaws that could result in misleading and non-generalizable results, such as spurious correlations, or examples that are ambiguous or otherwise impossible to assess for veracity. We find the latter issue is particularly severe and affects most datasets in the literature. We further provide state-of-the-art baselines on all these datasets, but show that regardless of label quality, categorical labels may no longer give an accurate evaluation of detection model performance. Finally, we propose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the field toward systemic solutions rather than inadvertently propagating issues in evaluation. Overall, this guide aims to provide a roadmap for higher quality data and better grounded evaluations, ultimately improving research in misinformation detection. All datasets and other artifacts are available at misinfo-datasets.complexdatalab.com.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-based Exploration Conduction for Multi-step Reasoning</title>
<link>https://arxiv.org/abs/2503.15848</link>
<guid>https://arxiv.org/abs/2503.15848</guid>
<content:encoded><![CDATA[
arXiv:2503.15848v2 Announce Type: replace-cross 
Abstract: Multi-step processes via large language models (LLMs) have proven effective for solving complex reasoning tasks. However, the depth of exploration of the reasoning procedure can significantly affect the task performance. Existing methods to automatically decide the depth often lead to high cost and a lack of flexibility. To address these issues, we propose Entropy-based Exploration Depth Conduction (Entro-duction), a novel method that dynamically adjusts the exploration depth during multi-step reasoning by monitoring LLM's output entropy and variance entropy. We employ these two features to capture the model's uncertainty of the current step and the fluctuation of uncertainty across consecutive reasoning steps. Based on the observed entropy changes, the LLM selects whether to deepen, expand, or stop exploration according to the probability, which facilitates the trade-off between the reasoning accuracy and exploration effectiveness. Experimental results across four benchmark datasets demonstrate the efficacy of Entro-duction.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fractured Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.12992</link>
<guid>https://arxiv.org/abs/2505.12992</guid>
<content:encoded><![CDATA[
arXiv:2505.12992v3 Announce Type: replace-cross 
Abstract: Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning. Code is available at https://github.com/BaohaoLiao/frac-cot.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation</title>
<link>https://arxiv.org/abs/2505.16065</link>
<guid>https://arxiv.org/abs/2505.16065</guid>
<content:encoded><![CDATA[
arXiv:2505.16065v2 Announce Type: replace-cross 
Abstract: Embedding-Based Retrieval (EBR) is an important technique in modern search engines, enabling semantic match between search queries and relevant results. However, search logging data on platforms like Facebook Marketplace lacks the diversity and details needed for effective EBR model training, limiting the models' ability to capture nuanced search patterns. To address this challenge, we propose Aug2Search, an EBR-based framework leveraging synthetic data generated by Generative AI (GenAI) models, in a multimodal and multitask approach to optimize query-product relevance. This paper investigates the capabilities of GenAI, particularly Large Language Models (LLMs), in generating high-quality synthetic data, and analyzing its impact on enhancing EBR models. We conducted experiments using eight Llama models and 100 million data points from Facebook Marketplace logs. Our synthetic data generation follows three strategies: (1) generate queries, (2) enhance product listings, and (3) generate queries from enhanced listings. We train EBR models on three different datasets: sampled engagement data or original data ((e.g., "Click" and "Listing Interactions")), synthetic data, and a mixture of both engagement and synthetic data to assess their performance across various training sets. Our findings underscore the robustness of Llama models in producing synthetic queries and listings with high coherence, relevance, and diversity, while maintaining low levels of hallucination. Aug2Search achieves an improvement of up to 4% in ROC_AUC with 100 million synthetic data samples, demonstrating the effectiveness of our approach. Moreover, our experiments reveal that with the same volume of training data, models trained exclusively on synthetic data often outperform those trained on original data only or a mixture of original and synthetic data.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools</title>
<link>https://arxiv.org/abs/2505.21569</link>
<guid>https://arxiv.org/abs/2505.21569</guid>
<content:encoded><![CDATA[
arXiv:2505.21569v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM)-based agents have demonstrated the ability to improve performance in chemistry-related tasks by selecting appropriate tools. However, their effectiveness remains limited by the inherent prediction errors of chemistry tools. In this paper, we take a step further by exploring how LLMbased agents can, in turn, be leveraged to reduce prediction errors of the tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking), a simple yet effective method that enhances chemistry tools through optimizing agent-stacking structures from limited data. ChemHAS achieves state-of-the-art performance across four fundamental chemistry tasks, demonstrating that our method can effectively compensate for prediction errors of the tools. Furthermore, we identify and characterize four distinct agent-stacking behaviors, potentially improving interpretability and revealing new possibilities for AI agent applications in scientific research. Our code and dataset are publicly available at https: //anonymous.4open.science/r/ChemHAS-01E4/README.md.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Editing Large Language Models Poses Serious Safety Risks</title>
<link>https://arxiv.org/abs/2502.02958</link>
<guid>https://arxiv.org/abs/2502.02958</guid>
<content:encoded><![CDATA[
<div> facts, Large Language Models, knowledge editing methods, safety risks, malicious actors

Summary:<br /><br />Large Language Models (LLMs) contain a vast amount of facts that can become outdated over time, leading to the development of knowledge editing methods (KEs) for making changes. However, the use of KEs poses serious safety risks that have been largely overlooked. KEs are attractive to malicious actors due to their availability, affordability, performance, and stealthiness. These actors can easily adapt KEs for various malicious purposes, exploiting vulnerabilities in the AI ecosystem that allow unchecked model updates. The lack of social and institutional awareness further exacerbates this risk. To address this issue, the community is urged to research tamper-resistant models and countermeasures against malicious model editing, as well as actively engage in securing the AI ecosystem. <div>
arXiv:2502.02958v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) contain large amounts of facts about the world. These facts can become outdated over time, which has led to the development of knowledge editing methods (KEs) that can change specific facts in LLMs with limited side effects. This position paper argues that editing LLMs poses serious safety risks that have been largely overlooked. First, we note the fact that KEs are widely available, computationally inexpensive, highly performant, and stealthy makes them an attractive tool for malicious actors. Second, we discuss malicious use cases of KEs, showing how KEs can be easily adapted for a variety of malicious purposes. Third, we highlight vulnerabilities in the AI ecosystem that allow unrestricted uploading and downloading of updated models without verification. Fourth, we argue that a lack of social and institutional awareness exacerbates this risk, and discuss the implications for different stakeholders. We call on the community to (i) research tamper-resistant models and countermeasures against malicious model editing, and (ii) actively engage in securing the AI ecosystem.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries</title>
<link>https://arxiv.org/abs/2506.13796</link>
<guid>https://arxiv.org/abs/2506.13796</guid>
<content:encoded><![CDATA[
<div> automated method, instruction data, climate change, Large Language Models (LLMs), ClimateChat-Corpus <br />
Summary: <br />
The study introduces an automated method for generating climate change instruction data, named ClimateChat-Corpus. Using this method, the ClimateChat model is fine-tuned, showing significant improvement in performance on climate change question-and-answer tasks. The study highlights the importance of selecting an appropriate base model for instruction tuning and demonstrates the adaptability of LLMs to a wide range of climate change scientific discovery tasks. The research fills a gap in efficiently producing high-precision instruction data for climate change, emphasizing the role of Natural Language Processing technologies in supporting decision-makers and the public amidst the global climate crisis. <div>
arXiv:2506.13796v1 Announce Type: new 
Abstract: As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles</title>
<link>https://arxiv.org/abs/2506.13886</link>
<guid>https://arxiv.org/abs/2506.13886</guid>
<content:encoded><![CDATA[
<div> numeral systems, large language models, linguistic-mathematical puzzles, cross-linguistic numeral systems, compositional structure  
Summary:  
Large language models struggle with solving linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can solve successfully. Models require explicit marking of mathematical operations to consistently solve such problems, unlike humans who use linguistic understanding to infer implicit numeral structure. Through experiments, it was found that LLMs lack the ability to flexibly infer compositional rules from implicit patterns in human-scale data. The linguistic and mathematical aspects of numbers in language play a crucial role in how LLMs approach numeral tasks, highlighting the challenge in modeling human-like reasoning in numerical tasks. This study underscores the need for models to incorporate linguistic understanding and infer compositional rules effectively to navigate the diversity of numeral systems across languages. <br /><br />Summary: <div>
arXiv:2506.13886v1 Announce Type: new 
Abstract: Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\times$, etc, as in "twenty + three"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training</title>
<link>https://arxiv.org/abs/2506.13888</link>
<guid>https://arxiv.org/abs/2506.13888</guid>
<content:encoded><![CDATA[
<div> Reinforcement Fine-Tuning, Vision-Language Reward Model, bootstrapping dilemma, modality bias, hallucination detection, multimodal reasoning <br />
<br />
Summary: 
The article introduces Reinforcement Fine-Tuning (RFT) with verifiable rewards in the context of Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) aims to align VL models by providing structured feedback, but faces challenges such as the bootstrapping dilemma and modality bias. To address these issues, an iterative training framework incorporating vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling is proposed. This approach refines preference datasets, enhances structured critiques, and improves reasoning iteratively. Experimental results on VL-RM benchmarks show improved performance in hallucination detection and multimodal reasoning, contributing to the advancement of VL model alignment using reinforcement learning. <br /> <div>
arXiv:2506.13888v1 Announce Type: new 
Abstract: Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large language models but remains underexplored for Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing structured feedback, yet training effective VL-RMs faces two major challenges. First, the bootstrapping dilemma arises as high-quality training data depends on already strong VL models, creating a cycle where self-generated supervision reinforces existing biases. Second, modality bias and negative example amplification occur when VL models hallucinate incorrect visual attributes, leading to flawed preference data that further misguides training. To address these issues, we propose an iterative training framework leveraging vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling. Our approach refines preference datasets, enhances structured critiques, and iteratively improves reasoning. Experiments across VL-RM benchmarks demonstrate superior performance in hallucination detection and multimodal reasoning, advancing VL model alignment with reinforcement learning.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoNews: A Spoken Dialogue System for Expressive News Conversations</title>
<link>https://arxiv.org/abs/2506.13894</link>
<guid>https://arxiv.org/abs/2506.13894</guid>
<content:encoded><![CDATA[
<div> Keywords: emotional speech, spoken dialogue system, news conversations, sentiment analyzer, engagement

Summary:
Emotional speech in task-oriented spoken dialogue systems (SDS) is explored in this study to improve empathy in news conversations. A system is developed that utilizes a large language model-based sentiment analyzer to identify appropriate emotions and PromptTTS for emotional speech synthesis. The research addresses the lack of standardized evaluation metrics for social goals in emotional SDSs. The proposed emotional SDS outperformed a baseline system in emotion regulation and engagement, highlighting the importance of speech emotion in enhancing conversational engagement. This study bridges the gap between emotional TTS and SDS research, demonstrating the effectiveness of integrating emotional speech regulation in task-oriented dialogue systems for more empathetic interactions.

<br /><br />Summary: <div>
arXiv:2506.13894v1 Announce Type: new 
Abstract: We develop a task-oriented spoken dialogue system (SDS) that regulates emotional speech based on contextual cues to enable more empathetic news conversations. Despite advancements in emotional text-to-speech (TTS) techniques, task-oriented emotional SDSs remain underexplored due to the compartmentalized nature of SDS and emotional TTS research, as well as the lack of standardized evaluation metrics for social goals. We address these challenges by developing an emotional SDS for news conversations that utilizes a large language model (LLM)-based sentiment analyzer to identify appropriate emotions and PromptTTS to synthesize context-appropriate emotional speech. We also propose subjective evaluation scale for emotional SDSs and judge the emotion regulation performance of the proposed and baseline systems. Experiments showed that our emotional SDS outperformed a baseline system in terms of the emotion regulation and engagement. These results suggest the critical role of speech emotion for more engaging conversations. All our source code is open-sourced at https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations</title>
<link>https://arxiv.org/abs/2506.13901</link>
<guid>https://arxiv.org/abs/2506.13901</guid>
<content:encoded><![CDATA[
<div> keyword: Alignment, Language models, Safety, Evaluation, Dataset
Summary:<br />
- Alignment in large language models (LLMs) is crucial in high-stakes domains like education, healthcare, etc.<br />
- Current evaluation methods have blind spots and may not accurately assess alignment.<br />
- The Alignment Quality Index (AQI) is introduced as a novel metric to assess LLM alignment by analyzing safe and unsafe activations in latent space.<br />
- AQI combines various clustering quality measures to detect misalignments, jailbreak risks, and alignment faking.<br />
- The LITMUS dataset is proposed to facilitate robust evaluation under challenging conditions.<br />
- Empirical tests show that AQI correlates with external judges and can reveal vulnerabilities missed by refusal metrics.<br /> 
Summary: <div>
arXiv:2506.13901v1 Announce Type: new 
Abstract: Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.
  To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.
  Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection</title>
<link>https://arxiv.org/abs/2506.13956</link>
<guid>https://arxiv.org/abs/2506.13956</guid>
<content:encoded><![CDATA[
<div> data augmentation, multimodal classification, robotics, large language model, diffusion model

Summary: 
This paper introduces a novel framework for data augmentation in robotic assistance scenarios, focusing on improving intent understanding by enhancing user requests with visual cues. The approach combines dialogues and environmental imagery to generate additional data for model training. By leveraging a large language model to simulate conversations and a diffusion model to create related images, the framework aims to refine multimodal models and enhance action selection capabilities in response to user interactions. Experimental results based on real-world data show that this methodology significantly improves the robot's performance, achieving state-of-the-art results. <div>
arXiv:2506.13956v1 Announce Type: new 
Abstract: When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are manual annotations necessary for statutory interpretations retrieval?</title>
<link>https://arxiv.org/abs/2506.13965</link>
<guid>https://arxiv.org/abs/2506.13965</guid>
<content:encoded><![CDATA[
<div> Keywords: legal research, interpretations, ranking, language models, automation<br />
<br />
Summary: The study focuses on improving the process of retrieving relevant interpretations of legal concepts by exploring the need for manual annotation. It investigates the optimal number of annotations per concept, the impact of annotating only the best candidates, and the potential benefits of automating the annotation process using a Language Model (LLM). By conducting various experiments, the research aims to determine the volume and scope of annotations required for effective interpretation retrieval. The results will provide insights into streamlining the manual annotation process and enhancing the performance of language models in legal research. <div>
arXiv:2506.13965v1 Announce Type: new 
Abstract: One of the elements of legal research is looking for cases where judges have extended the meaning of a legal concept by providing interpretations of what a concept means or does not mean. This allow legal professionals to use such interpretations as precedents as well as laymen to better understand the legal concept. The state-of-the-art approach for retrieving the most relevant interpretations for these concepts currently depends on the ranking of sentences and the training of language models over annotated examples. That manual annotation process can be quite expensive and need to be repeated for each such concept, which prompted recent research in trying to automate this process. In this paper, we highlight the results of various experiments conducted to determine the volume, scope and even the need for manual annotation. First of all, we check what is the optimal number of annotations per a legal concept. Second, we check if we can draw the sentences for annotation randomly or there is a gain in the performance of the model, when only the best candidates are annotated. As the last question we check what is the outcome of automating the annotation process with the help of an LLM.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI shares emotion with humans across languages and cultures</title>
<link>https://arxiv.org/abs/2506.13978</link>
<guid>https://arxiv.org/abs/2506.13978</guid>
<content:encoded><![CDATA[
<div> Keywords: human-machine collaboration, emotion exchange, large language models, emotional alignment, affective outputs<br />
Summary: 

- The study focuses on the exchange of emotions between humans and AI in collaborative settings. 
- Researchers assess emotional alignment across different linguistic-cultural groups and model-families. 
- Findings suggest that LLM-derived emotion spaces align structurally with human perception, based on valence and arousal dimensions. 
- Emotion-related features accurately predict word ratings along these core dimensions, reflecting universal and language-specific patterns. 
- Using steering vectors derived from human-centric emotion concepts, the study shows that AI expressions can be guided to produce specific affective states when conveying content. 

Summary: <div>
arXiv:2506.13978v1 Announce Type: new 
Abstract: Effective and safe human-machine collaboration requires the regulated and meaningful exchange of emotions between humans and artificial intelligence (AI). Current AI systems based on large language models (LLMs) can provide feedback that makes people feel heard. Yet it remains unclear whether LLMs represent emotion in language as humans do, or whether and how the emotional tone of their output can be controlled. We assess human-AI emotional alignment across linguistic-cultural groups and model-families, using interpretable LLM features translated from concept-sets for over twenty nuanced emotion categories (including six basic emotions). Our analyses reveal that LLM-derived emotion spaces are structurally congruent with human perception, underpinned by the fundamental affective dimensions of valence and arousal. Furthermore, these emotion-related features also accurately predict large-scale behavioural data on word ratings along these two core dimensions, reflecting both universal and language-specific patterns. Finally, by leveraging steering vectors derived solely from human-centric emotion concepts, we show that model expressions can be stably and naturally modulated across distinct emotion categories, which provides causal evidence that human emotion concepts can be used to systematically induce LLMs to produce corresponding affective states when conveying content. These findings suggest AI not only shares emotional representations with humans but its affective outputs can be precisely guided using psychologically grounded emotion concepts.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text</title>
<link>https://arxiv.org/abs/2506.14012</link>
<guid>https://arxiv.org/abs/2506.14012</guid>
<content:encoded><![CDATA[
<div> Keywords: Code-switching, Large Language Models, comprehension, degradation, fine-tuning

Summary:<br /><br />
Code-switching refers to the practice of switching between languages within a single conversation, common in multilingual communities and online content. Large Language Models (LLMs) used for content processing and generation often encounter code-switched text. A systematic evaluation was conducted to assess LLM comprehension of code-switched text by creating code-switched versions of reasoning and comprehension benchmarks. The study found that while comprehension may suffer when foreign tokens disrupt English text, embedding English into other languages can actually improve comprehension. Prompting produced mixed results, but fine-tuning the models proved to be a more reliable method to mitigate degradation. This research sheds light on how LLMs process and understand code-switched text, highlighting the need for further investigation and refinement in this area. 

Summary: <div>
arXiv:2506.14012v1 Announce Type: new 
Abstract: Code-switching (CSW) is the act of alternating between two or more languages within a single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As a result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents a systematic evaluation of LLM comprehension under code-switching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English text$\unicode{x2013}$even under linguistic constraints$\unicode{x2013}$embedding English into other languages often improves comprehension. Though prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation</title>
<link>https://arxiv.org/abs/2506.14028</link>
<guid>https://arxiv.org/abs/2506.14028</guid>
<content:encoded><![CDATA[
<div> benchmark, multilingual, multimodal, financial, LLMs  
Summary:  
MultiFinBen is introduced as a new benchmark for evaluating large language models (LLMs) in the financial domain. It is the first multilingual and multimodal benchmark that assesses models across various modalities and linguistic settings. Two novel tasks, PolyFiQA-Easy and PolyFiQA-Expert, require complex reasoning over mixed-language inputs, while EnglishOCR and SpanishOCR challenge models with OCR-embedded financial QA tasks. A dynamic selection mechanism and a compact, balanced benchmark are curated to enhance the evaluation process. Evaluation of 22 state-of-the-art models shows that even strong models struggle with complex cross-lingual and multimodal tasks in the financial domain. MultiFinBen aims to promote transparent, reproducible, and inclusive progress in financial research and applications.  
<br /><br />Summary: <div>
arXiv:2506.14028v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have accelerated progress in financial NLP and applications, yet existing benchmarks remain limited to monolingual and unimodal settings, often over-relying on simple tasks and failing to reflect the complexity of real-world financial communication. We introduce MultiFinBen, the first multilingual and multimodal benchmark tailored to the global financial domain, evaluating LLMs across modalities (text, vision, audio) and linguistic settings (monolingual, bilingual, multilingual) on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy and PolyFiQA-Expert, the first multilingual financial benchmarks requiring models to perform complex reasoning over mixed-language inputs; and EnglishOCR and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to extract and reason over information from visual-text financial documents. Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate a compact, balanced benchmark rather than simple aggregation existing datasets. Extensive evaluation of 22 state-of-the-art models reveals that even the strongest models, despite their general multimodal and multilingual capabilities, struggle dramatically when faced with complex cross-lingual and multimodal tasks in financial domain. MultiFinBen is publicly released to foster transparent, reproducible, and inclusive progress in financial studies and applications.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Interdisciplinary Review of Commonsense Reasoning and Intent Detection</title>
<link>https://arxiv.org/abs/2506.14040</link>
<guid>https://arxiv.org/abs/2506.14040</guid>
<content:encoded><![CDATA[
<div> Keywords: commonsense reasoning, intent detection, natural language understanding, zero-shot learning, multilingual models

Summary: 
This review delves into recent advancements in commonsense reasoning and intent detection within the field of natural language understanding. It examines 28 papers published between 2020-2025 from prominent conferences like ACL, EMNLP, and CHI, categorizing them based on methodology and application. Commonsense reasoning is explored in various contexts such as zero-shot learning, cultural adaptation, structured evaluation, and interactive settings. Intent detection is analyzed through different models including open-set models, generative formulations, clustering techniques, and human-centered systems. By blending insights from natural language processing and human-computer interaction, the review highlights emerging trends towards more adaptive, multilingual, and context-aware models. It also identifies critical gaps in grounding, generalization, and benchmark design that need to be addressed in future research. 

<br /><br />Summary: <div>
arXiv:2506.14040v1 Announce Type: new 
Abstract: This review explores recent advances in commonsense reasoning and intent detection, two key challenges in natural language understanding. We analyze 28 papers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and application. Commonsense reasoning is reviewed across zero-shot learning, cultural adaptation, structured evaluation, and interactive contexts. Intent detection is examined through open-set models, generative formulations, clustering, and human-centered systems. By bridging insights from NLP and HCI, we highlight emerging trends toward more adaptive, multilingual, and context-aware models, and identify key gaps in grounding, generalization, and benchmark design.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications</title>
<link>https://arxiv.org/abs/2506.14046</link>
<guid>https://arxiv.org/abs/2506.14046</guid>
<content:encoded><![CDATA[
<div> Keywords: language difficulty, conversational text, Ace-CEFR dataset, Transformer-based models, Large Language Models

Summary: 
The article introduces the Ace-CEFR dataset, designed to evaluate the language difficulty of short, conversational text passages. This dataset is expert-annotated with corresponding text difficulty levels. Various models, including Transformer-based models and Large Language Models, were experimented with on Ace-CEFR, showing that models trained on this dataset can accurately measure text difficulty, surpassing human experts. These models also demonstrated latency appropriate for production environments. The Ace-CEFR dataset is released to the public for further research and development purposes. Overall, this study addresses the existing need to assess language difficulty in conversational text and provides a valuable resource for training and filtering Large Language Models. 

<br /><br />Summary: <div>
arXiv:2506.14046v1 Announce Type: new 
Abstract: There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data</title>
<link>https://arxiv.org/abs/2506.14064</link>
<guid>https://arxiv.org/abs/2506.14064</guid>
<content:encoded><![CDATA[
<div> methodology, constituency parsing, parsing heuristics, dataset, extraction tool

Summary:
The article introduces a methodological approach for identifying and annotating naturally-occurring examples of English embedded clauses in large text data through the use of constituency parsing and parsing heuristics. The tool developed by the researchers was evaluated on the Golden Embedded Clause Set (GECS) dataset, which contains hand-annotated examples of English embedded clause sentences found in natural language. Additionally, the researchers extracted a large-scale dataset of naturally-occurring English embedded clauses from the Dolma corpus using their extraction tool. This approach allows for the analysis of syntactic and semantic features in embedded clauses using real-world language examples, as opposed to artificially created language examples typically used in current research methodologies. <div>
arXiv:2506.14064v1 Announce Type: new 
Abstract: For linguists, embedded clauses have been of special interest because of their intricate distribution of syntactic and semantic features. Yet, current research relies on schematically created language examples to investigate these constructions, missing out on statistical information and naturally-occurring examples that can be gained from large language corpora. Thus, we present a methodological approach for detecting and annotating naturally-occurring examples of English embedded clauses in large-scale text data using constituency parsing and a set of parsing heuristics. Our tool has been evaluated on our dataset Golden Embedded Clause Set (GECS), which includes hand-annotated examples of naturally-occurring English embedded clause sentences. Finally, we present a large-scale dataset of naturally-occurring English embedded clauses which we have extracted from the open-source corpus Dolma using our extraction tool.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Abstract Meaning Representation for Hospital Discharge Summarization</title>
<link>https://arxiv.org/abs/2506.14101</link>
<guid>https://arxiv.org/abs/2506.14101</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, hallucination, discharge summaries, language-based graphs, deep learning models

Summary:
The Achilles heel of Large Language Models (LLMs) is hallucination, particularly concerning automatically generating discharge summaries in the clinical domain. This poses challenges for content provenance and trustworthiness. To address this issue, a new method combining language-based graphs and deep learning models was developed. This method demonstrates impressive reliability results on the MIMIC-III corpus and clinical notes from Anonymous Hospital. By leveraging language-based graphs, the generated discharge summaries aim to assist physicians in their patient care duties and reduce documentation burdens. The method showcases the potential of utilizing advanced technologies in the medical field to enhance the efficiency and accuracy of summarization processes. Providing accessibility to both the method and its output examples signifies a step towards fostering transparency and reproducibility in research practices. 

Summary: <div>
arXiv:2506.14101v1 Announce Type: new 
Abstract: The Achilles heel of Large Language Models (LLMs) is hallucination, which has drastic consequences for the clinical domain. This is particularly important with regards to automatically generating discharge summaries (a lengthy medical document that summarizes a hospital in-patient visit). Automatically generating these summaries would free physicians to care for patients and reduce documentation burden. The goal of this work is to discover new methods that combine language-based graphs and deep learning models to address provenance of content and trustworthiness in automatic summarization. Our method shows impressive reliability results on the publicly available Medical Information Mart for Intensive III (MIMIC-III) corpus and clinical notes written by physicians at Anonymous Hospital. rovide our method, generated discharge ary output examples, source code and trained models.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Essential-Web v1.0: 24T tokens of organized web data</title>
<link>https://arxiv.org/abs/2506.14111</link>
<guid>https://arxiv.org/abs/2506.14111</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, pre-training datasets, taxonomy, Essential-Web v1.0, HuggingFace<br />
<br />
Summary: Data is essential for language models to acquire skills and knowledge. However, the lack of well-organized pre-training datasets can result in costly and inaccessible data pipelines. The authors introduce Essential-Web v1.0, a massive dataset with 24 trillion tokens, each annotated with a comprehensive twelve-category taxonomy covering various aspects such as topic, format, content complexity, and quality. The taxonomy labels are generated by a fine-tuned 0.5b-parameter model, EAI-Distill-0.5b, showing high annotator agreement close to a large-scale model like Qwen2.5-32B-Instruct. By using simple SQL-style filters, the researchers were able to extract competitive web-curated datasets in fields like math, web code, STEM, and medical, outperforming existing state-of-the-art datasets in some categories. Essential-Web v1.0 is now accessible on the HuggingFace platform for further research and development. <br /><br />Summary: <div>
arXiv:2506.14111v1 Announce Type: new 
Abstract: Data plays the most prominent role in how language models acquire skills and knowledge. The lack of massive, well-organized pre-training datasets results in costly and inaccessible data pipelines. We present Essential-Web v1.0, a 24-trillion-token dataset in which every document is annotated with a twelve-category taxonomy covering topic, format, content complexity, and quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter model that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0% relative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on HuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sampling from Your Language Model One Byte at a Time</title>
<link>https://arxiv.org/abs/2506.14123</link>
<guid>https://arxiv.org/abs/2506.14123</guid>
<content:encoded><![CDATA[
<div> Tokenization, language models, Prompt Boundary Problem, ensemble, proxy-tuning <br />
<br />
Summary: The article discusses the issues that tokenization can introduce distortion into the model's generations, particularly the Prompt Boundary Problem. The proposed method converts autoregressive language models with BPE tokenizers into character-level or byte-level models without altering their generative distribution. This approach effectively addresses the Prompt Boundary Problem and allows for the unification of vocabularies of language models with different tokenizers. The method enables the ensemble of language models with different tokenizers at inference time and facilitates the transfer of post-training from one model to another using proxy-tuning. Experimental results demonstrate that the ensemble and proxy-tuned models outperform individual models on downstream evaluations. <div>
arXiv:2506.14123v1 Announce Type: new 
Abstract: Tokenization is used almost universally by modern language models, enabling efficient text representation using multi-byte or multi-character tokens. However, prior work has shown that tokenization can introduce distortion into the model's generations. For example, users are often advised not to end their prompts with a space because it prevents the model from including the space as part of the next token. This Prompt Boundary Problem (PBP) also arises in languages such as Chinese and in code generation, where tokens often do not line up with syntactic boundaries. Additionally mismatching tokenizers often hinder model composition and interoperability. For example, it is not possible to directly ensemble models with different tokenizers due to their mismatching vocabularies. To address these issues, we present an inference-time method to convert any autoregressive LM with a BPE tokenizer into a character-level or byte-level LM, without changing its generative distribution at the text level. Our method efficient solves the PBP and is also able to unify the vocabularies of language models with different tokenizers, allowing one to ensemble LMs with different tokenizers at inference time as well as transfer the post-training from one model to another using proxy-tuning. We demonstrate in experiments that the ensemble and proxy-tuned models outperform their constituents on downstream evals.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization</title>
<link>https://arxiv.org/abs/2506.14157</link>
<guid>https://arxiv.org/abs/2506.14157</guid>
<content:encoded><![CDATA[
<div> Preference optimization, LLMs, response pair, DCRM, distance, reward margin <br />
Summary:<br />Recent research examines the impact of response pair differences on the effectiveness of preference optimization (PO) models. The Distance Calibrated Reward Margin (DCRM) metric combines distance and reward margin to evaluate response quality in PO. Studies reveal a correlation between higher DCRM in training sets and improved learning outcomes. A best-of-$N^2$ pairing method is proposed to select response pairs with high DCRM, leading to enhanced model performance on evaluation benchmarks such as AlpacaEval, MT-Bench, and Arena-Hard. This research sheds light on the significance of identifying and maximizing desired differences in response pairs for effective PO training. <br />Summary: <div>
arXiv:2506.14157v1 Announce Type: new 
Abstract: Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models</title>
<link>https://arxiv.org/abs/2506.14158</link>
<guid>https://arxiv.org/abs/2506.14158</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, speculative sampling, syntactic coherence, semantic coherence, efficiency <br />
<br />
Summary: 
The paper introduces a new framework called Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) to enhance the efficiency of large language models (LLMs). By combining speculative sampling with multi-head drafting for rapid token generation and a continuous verification tree for candidate validation, S$^4$C shows improved efficiency and parallelism in text generation tasks. Experimental results indicate that S$^4$C outperforms existing methods on mainstream tasks and achieves an acceleration ratio of 2.26x-2.60x on Spec-bench benchmarks. The framework emphasizes the importance of maintaining syntactic and semantic coherence in text generation, leading to the generation of more valid tokens with fewer computational resources. The proposed S$^4$C framework demonstrates its ability to significantly reduce inference latency in LLMs and improve overall performance in real-time applications. <br /> <div>
arXiv:2506.14158v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit remarkable reasoning capabilities across diverse downstream tasks. However, their autoregressive nature leads to substantial inference latency, posing challenges for real-time applications. Speculative sampling mitigates this issue by introducing a drafting phase followed by a parallel validation phase, enabling faster token generation and verification. Existing approaches, however, overlook the inherent coherence in text generation, limiting their efficiency. To address this gap, we propose a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework, which extends speculative sampling by leveraging multi-head drafting for rapid token generation and a continuous verification tree for efficient candidate validation and feature reuse. Experimental results demonstrate that S$^4$C surpasses baseline methods across mainstream tasks, offering enhanced efficiency, parallelism, and the ability to generate more valid tokens with fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind</title>
<link>https://arxiv.org/abs/2506.14161</link>
<guid>https://arxiv.org/abs/2506.14161</guid>
<content:encoded><![CDATA[
<div> Word Association Bias Test, Affective Attribution Test, Theory of Mind, Large Language Models, Bias structure

Summary:
The article introduces a new evaluation framework for assessing Theory of Mind (ToM) in Large Language Models (LLMs) by leveraging the Stereotype Content Model (SCM) to understand bias as a multi-dimensional failure in ToM. The framework includes the Word Association Bias Test (WABT) and the Affective Attribution Test (AAT) to indirectly measure implicit lexical associations and covert affective leanings, respectively. Through extensive experiments on 8 State-of-the-Art LLMs, the framework reveals complex bias structures such as pervasive sociability bias, multi-dimensional divergence, and asymmetric stereotype amplification. This approach provides a more robust methodology for identifying the structural nature of implicit bias in LLMs, addressing the limitations of conventional direct-query methods. <div>
arXiv:2506.14161v1 Announce Type: new 
Abstract: Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity for reasoning about mental states, yet failures in this capacity often manifest as systematic implicit bias. Evaluating this bias is challenging, as conventional direct-query methods are susceptible to social desirability effects and fail to capture its subtle, multi-dimensional nature. To this end, we propose an evaluation framework that leverages the Stereotype Content Model (SCM) to reconceptualize bias as a multi-dimensional failure in ToM across Competence, Sociability, and Morality. The framework introduces two indirect tasks: the Word Association Bias Test (WABT) to assess implicit lexical associations and the Affective Attribution Test (AAT) to measure covert affective leanings, both designed to probe latent stereotypes without triggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs demonstrate our framework's capacity to reveal complex bias structures, including pervasive sociability bias, multi-dimensional divergence, and asymmetric stereotype amplification, thereby providing a more robust methodology for identifying the structural nature of implicit bias.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAM: A Generative Foundation Reward Model for Reward Generalization</title>
<link>https://arxiv.org/abs/2506.14175</link>
<guid>https://arxiv.org/abs/2506.14175</guid>
<content:encoded><![CDATA[
<div> unsupervised learning, generative models, reward models, label smoothing, pairwise ranking loss

Summary: 
This paper introduces a new approach to training reward models for large language models (LLMs) by utilizing both unlabeled and labeled data. By developing a generative reward model that is first trained through unsupervised learning and then fine-tuned through supervised learning, the authors demonstrate the effectiveness of this method. By incorporating label smoothing, they show that the optimization process is akin to a regularized pairwise ranking loss, offering a novel perspective on training reward models. The resulting foundation reward model proves to be versatile and can be applied to various tasks with minimal further adjustment. Extensive experiments validate the model's effectiveness across tasks such as response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, outperforming several strong baseline models. <div>
arXiv:2506.14175v1 Announce Type: new 
Abstract: In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages</title>
<link>https://arxiv.org/abs/2506.14177</link>
<guid>https://arxiv.org/abs/2506.14177</guid>
<content:encoded><![CDATA[
<div> Keywords: code-switching, ASR, synthetic data generation, Southeast Asian languages, benchmark evaluation

Summary: 
This study explores the development of code-switching automatic speech recognition (ASR) systems for under-resourced Southeast Asian language pairs (Malay-English, Mandarin-Malay, Tamil-English) using synthetic data generation techniques. The researchers propose a phrase-level mixing method to create synthetic code-switching data that simulates natural language patterns. By fine-tuning large pretrained ASR models with a combination of monolingual and synthetic code-switching data, significant improvements in ASR performance are achieved. The experiments demonstrate that the proposed training strategy effectively enhances ASR performance on both monolingual and code-switching tests, with the Malay-English language pair showing the highest gains, followed by Tamil-English and Mandarin-Malay. This cost-effective approach provides a promising solution for developing code-switching ASR systems, which can benefit both research and industry. 

<br /><br />Summary: <div>
arXiv:2506.14177v1 Announce Type: new 
Abstract: Code-switching (CS), common in multilingual settings, presents challenges for ASR due to scarce and costly transcribed data caused by linguistic complexity. This study investigates building CS-ASR using synthetic CS data. We propose a phrase-level mixing method to generate synthetic CS data that mimics natural patterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data to fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This paper focuses on three under-resourced Southeast Asian language pairs: Malay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN), establishing a new comprehensive benchmark for CS-ASR to evaluate the performance of leading ASR models. Experimental results show that the proposed training strategy enhances ASR performance on monolingual and CS tests, with BM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a cost-effective approach for CS-ASR development, benefiting research and industry.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR</title>
<link>https://arxiv.org/abs/2506.14190</link>
<guid>https://arxiv.org/abs/2506.14190</guid>
<content:encoded><![CDATA[
<div> Keywords: code-switched ASR, synthetic data generation, asynchronous adaptation framework, large-scale web data, Whisper

Summary:
AsyncSwitch is a novel framework for developing code-switched ASR systems that addresses challenges such as language ambiguity and limited data exposure. The three-stage process involves training decoder layers on code-switched text, aligning decoder and encoder using limited speech-text data, and fully fine-tuning the model. Experiments with Whisper on Malay-English code-switching showed a 9.02% relative WER reduction and improved monolingual performance in Singlish, Malay, and other English variants. This approach leverages large-scale, text-rich web data to pre-expose ASR models to diverse code-switched domains before fine-tuning, enabling more effective and scalable development of ASR systems for code-switched languages. 

<br /><br />Summary: <div>
arXiv:2506.14190v1 Announce Type: new 
Abstract: Developing code-switched ASR systems is challenging due to language ambiguity and limited exposure to multilingual, code-switched data, while collecting such speech is costly. Prior work generates synthetic audio from text, but these methods are computationally intensive and hard to scale. We introduce AsyncSwitch, a novel asynchronous adaptation framework that leverages large-scale, text-rich web data to pre-expose ASR models to diverse code-switched domains before fine-tuning on paired speech-text corpora. Our three-stage process (1) trains decoder self-attention and feedforward layers on code-switched text, (2) aligns decoder and encoder via cross-attention using limited speech-text data, and (3) fully fine-tunes the entire model. Experiments with Whisper on Malay-English code-switching demonstrate a 9.02% relative WER reduction, while improving monolingual performance in Singlish, Malay, and other English variants.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment</title>
<link>https://arxiv.org/abs/2506.14199</link>
<guid>https://arxiv.org/abs/2506.14199</guid>
<content:encoded><![CDATA[
<div> Keywords: Literary translation, Multi-agent system, Large Language Models, Translation Quality Assessment, Nuanced framework<br />
<br />
Summary: 
The article introduces MAS-LitEval, a multi-agent system that utilizes Large Language Models (LLMs) to evaluate literary translations based on terminology, narrative, and style, aspects often overlooked by traditional metrics like BLEU and METEOR. MAS-LitEval was tested on translations of The Little Prince and A Connecticut Yankee in King Arthur's Court, with top models scoring up to 0.890 in capturing literary nuances. This novel framework offers a more nuanced approach to Translation Quality Assessment (TQA) and provides a practical tool for translators and researchers in preserving cultural nuances and stylistic elements in literary works. <div>
arXiv:2506.14199v1 Announce Type: new 
Abstract: Literary translation requires preserving cultural nuances and stylistic elements, which traditional metrics like BLEU and METEOR fail to assess due to their focus on lexical overlap. This oversight neglects the narrative consistency and stylistic fidelity that are crucial for literary works. To address this, we propose MAS-LitEval, a multi-agent system using Large Language Models (LLMs) to evaluate translations based on terminology, narrative, and style. We tested MAS-LitEval on translations of The Little Prince and A Connecticut Yankee in King Arthur's Court, generated by various LLMs, and compared it to traditional metrics. \textbf{MAS-LitEval} outperformed these metrics, with top models scoring up to 0.890 in capturing literary nuances. This work introduces a scalable, nuanced framework for Translation Quality Assessment (TQA), offering a practical tool for translators and researchers.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations</title>
<link>https://arxiv.org/abs/2506.14200</link>
<guid>https://arxiv.org/abs/2506.14200</guid>
<content:encoded><![CDATA[
<div> education, language models, ELI-Why, explanatory answers, human studies  
Summary:  
- Language models are widely used in education but their ability to cater to learners' varied needs remains underexplored.
- ELI-Why is introduced as a benchmark of "Why" questions to evaluate language models' pedagogical capabilities.
- Two human studies were conducted to assess the utility of language model-generated explanations on the benchmark for different educational grades.
- Findings show that GPT-4-generated explanations match intended educational backgrounds only 50% of the time, compared to 79% for human-curated explanations.
- Users deemed GPT-4-generated explanations 20% less suited to their informational needs compared to lay-curated explanations, indicating limitations in the pedagogical effectiveness of language models.  

<br /><br />Summary: <div>
arXiv:2506.14200v1 Announce Type: new 
Abstract: Language models today are widely used in education, yet their ability to tailor responses for learners with varied informational needs and knowledge backgrounds remains under-explored. To this end, we introduce ELI-Why, a benchmark of 13.4K "Why" questions to evaluate the pedagogical capabilities of language models. We then conduct two extensive human studies to assess the utility of language model-generated explanatory answers (explanations) on our benchmark, tailored to three distinct educational grades: elementary, high-school and graduate school. In our first study, human raters assume the role of an "educator" to assess model explanations' fit to different educational grades. We find that GPT-4-generated explanations match their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. In our second study, human raters assume the role of a learner to assess if an explanation fits their own informational needs. Across all educational backgrounds, users deemed GPT-4-generated explanations 20% less suited on average to their informational needs, when compared to explanations curated by lay people. Additionally, automated evaluation metrics reveal that explanations generated across different language model families for different informational needs remain indistinguishable in their grade-level, limiting their pedagogical effectiveness.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation</title>
<link>https://arxiv.org/abs/2506.14203</link>
<guid>https://arxiv.org/abs/2506.14203</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, anomia, semantic paraphasia, gradient-based selective augmentation, Tip-of-the-Tongue dataset

Summary: 
Language models (LMs) are investigated for aiding patients with anomia, a difficulty in naming objects. Two challenges are identified: term failure and semantic paraphasia errors during identification. The proposed approach addresses these challenges by robustifying the model from semantic errors and enhancing it with gradient-based selective augmentation. By controlling the quality of augmented data using gradient values and including unseen but relevant terms based on gradient variance, the model shows strong performance against baselines. Evaluation on the Tip-of-the-Tongue dataset as an intermediary task and real patient data from AphasiaBank demonstrates the model's effectiveness in assisting anomia patients by overcoming term failures and semantic paraphasia errors.<br /><br />Summary: <div>
arXiv:2506.14203v1 Announce Type: new 
Abstract: In this study, we investigate the potential of language models (LMs) in aiding patients experiencing anomia, a difficulty identifying the names of items. Identifying the intended target item from patient's circumlocution involves the two challenges of term failure and error: (1) The terms relevant to identifying the item remain unseen. (2) What makes the challenge unique is inherent perturbed terms by semantic paraphasia, which are not exactly related to the target item, hindering the identification process. To address each, we propose robustifying the model from semantically paraphasic errors and enhancing the model with unseen terms with gradient-based selective augmentation. Specifically, the gradient value controls augmented data quality amid semantic errors, while the gradient variance guides the inclusion of unseen but relevant terms. Due to limited domain-specific datasets, we evaluate the model on the Tip-of-the-Tongue dataset as an intermediary task and then apply our findings to real patient data from AphasiaBank. Our results demonstrate strong performance against baselines, aiding anomia patients by addressing the outlined challenges.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents</title>
<link>https://arxiv.org/abs/2506.14205</link>
<guid>https://arxiv.org/abs/2506.14205</guid>
<content:encoded><![CDATA[
<div> Keywords: AgentSynth, task synthesis, trajectory datasets, LLM-based task proposer, task complexity modulation<br />
<br />
Summary: <br />
AgentSynth is a pipeline designed for automatically creating diverse and realistic tasks and trajectory datasets for computer-use agents. It leverages information asymmetry to generate simple subtasks that become challenging when composed into longer tasks. The pipeline utilizes an LLM-based task proposer and an execution agent guided by a persona to complete tasks and log trajectories. By iteratively combining subtasks and summarizing them into composite tasks, AgentSynth can modulate task complexity. Empirical evaluations demonstrate the difficulty and discriminative power of the created tasks, with state-of-the-art LLM agents showing varying success rates across different difficulty levels. Importantly, the pipeline achieves a low cost per trajectory, making it significantly more affordable than human annotations. The code and data for AgentSynth are openly available for public use. <br /> <div>
arXiv:2506.14205v1 Announce Type: new 
Abstract: We introduce AgentSynth, a scalable and cost-efficient pipeline for automatically synthesizing high-quality tasks and trajectory datasets for generalist computer-use agents. Leveraging information asymmetry, AgentSynth constructs subtasks that are simple during generation but significantly more challenging when composed into long-horizon tasks, enabling the creation of over 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based task proposer guided by a persona, followed by an execution agent that completes the task and logs the trajectory. This process is repeated iteratively to form a sequence of subtasks, which are then summarized by a separate agent into a composite task of controllable difficulty. A key strength of AgentSynth is its ability to precisely modulate task complexity by varying the number of subtasks. Empirical evaluations show that state-of-the-art LLM agents suffer a steep performance drop, from 18% success at difficulty level 1 to just 4% at level 6, highlighting the benchmark's difficulty and discriminative power. Moreover, our pipeline achieves a low average cost of \$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our code and data are publicly available at https://github.com/sunblaze-ucb/AgentSynth
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation</title>
<link>https://arxiv.org/abs/2506.14206</link>
<guid>https://arxiv.org/abs/2506.14206</guid>
<content:encoded><![CDATA[
<div> diffusion model, generative model, tabular data, causal regularization, mixed-type data

Summary:
- Training generative AI relies heavily on high-quality data, but obtaining such data is challenging due to privacy concerns.
- Synthesizing data has become a popular solution, but generating high-quality mixed-type tabular data remains difficult.
- CausalDiffTab is introduced as a generative model designed for mixed tabular data with numerical and categorical features, capable of capturing complex variable interactions.
- A hybrid adaptive causal regularization method, utilizing Hierarchical Prior Fusion, is proposed to enhance model performance without sacrificing generative capabilities.
- Experimental results across seven datasets show that CausalDiffTab outperforms baseline methods in various metrics. The code for CausalDiffTab is publicly available. 

<br /><br />Summary: <div>
arXiv:2506.14206v1 Announce Type: new 
Abstract: Training data has been proven to be one of the most critical components in training generative AI. However, obtaining high-quality data remains challenging, with data privacy issues presenting a significant hurdle. To address the need for high-quality data. Synthesize data has emerged as a mainstream solution, demonstrating impressive performance in areas such as images, audio, and video. Generating mixed-type data, especially high-quality tabular data, still faces significant challenges. These primarily include its inherent heterogeneous data types, complex inter-variable relationships, and intricate column-wise distributions. In this paper, we introduce CausalDiffTab, a diffusion model-based generative model specifically designed to handle mixed tabular data containing both numerical and categorical features, while being more flexible in capturing complex interactions among variables. We further propose a hybrid adaptive causal regularization method based on the principle of Hierarchical Prior Fusion. This approach adaptively controls the weight of causal regularization, enhancing the model's performance without compromising its generative capabilities. Comprehensive experiments conducted on seven datasets demonstrate that CausalDiffTab outperforms baseline methods across all metrics. Our code is publicly available at: https://github.com/Godz-z/CausalDiffTab.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Detection of Implicit Influential Patterns in Conversations via Data Augmentation</title>
<link>https://arxiv.org/abs/2506.14211</link>
<guid>https://arxiv.org/abs/2506.14211</guid>
<content:encoded><![CDATA[
<div> Keywords: digitalization, linguistic strategies, influential patterns, conversation, language models

Summary:
This paper addresses the issue of detecting implicit influential patterns in conversations in the digital age. As individuals increasingly rely on digital platforms for communication, malicious actors are using verbal strategies to influence perceptions. The proposed model improves detection of these patterns by augmenting the dataset with state-of-the-art language models. The model not only detects influential elements in conversations but also identifies their specific locations. The approach results in a 6% improvement in detecting implicit influential patterns and a 33% and 43% improvement in multi-label classification tasks related to influence techniques and victim vulnerability, respectively.

<br /><br />Summary: 
- Addressing the detection of implicit influential patterns in digital conversations
- Malicious actors using verbal strategies to influence perceptions
- Augmenting dataset with language models for improved detection
- Identifying specific locations of influential elements in conversations
- 6% improvement in detecting implicit influential patterns and significant improvements in multi-label classification tasks <div>
arXiv:2506.14211v1 Announce Type: new 
Abstract: In the era of digitalization, as individuals increasingly rely on digital platforms for communication and news consumption, various actors employ linguistic strategies to influence public perception. While models have become proficient at detecting explicit patterns, which typically appear in texts as single remarks referred to as utterances, such as social media posts, malicious actors have shifted toward utilizing implicit influential verbal patterns embedded within conversations. These verbal patterns aim to mentally penetrate the victim's mind in order to influence them, enabling the actor to obtain the desired information through implicit means. This paper presents an improved approach for detecting such implicit influential patterns. Furthermore, the proposed model is capable of identifying the specific locations of these influential elements within a conversation. To achieve this, the existing dataset was augmented using the reasoning capabilities of state-of-the-art language models. Our designed framework resulted in a 6% improvement in the detection of implicit influential patterns in conversations. Moreover, this approach improved the multi-label classification tasks related to both the techniques used for influence and the vulnerability of victims by 33% and 43%, respectively.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chaining Event Spans for Temporal Relation Grounding</title>
<link>https://arxiv.org/abs/2506.14213</link>
<guid>https://arxiv.org/abs/2506.14213</guid>
<content:encoded><![CDATA[
<div> Temporal relations, events, understanding, timeline reasoning, TRC

Summary: 
The paper introduces a novel approach, the Timeline Reasoning Network (TRN), to accurately understand temporal relations between events for tasks like temporal reading comprehension (TRC) and relation extraction (TRE). Current solutions rely on answer overlaps, which can lead to unreliable results. TRN operates in a two-step inductive reasoning process, first answering questions with semantic and syntactic information and then chaining multiple questions on the same event to predict a timeline. By grounding answers using the predicted timeline, TRN effectively resolves spurious overlaps and outperforms previous methods on TRC and TRE tasks. This approach addresses the issue of unreliable results due to spurious overlaps of similar questions with coincidentally identical answers, providing a more accurate understanding of temporal relations between events. <div>
arXiv:2506.14213v1 Announce Type: new 
Abstract: Accurately understanding temporal relations between events is a critical building block of diverse tasks, such as temporal reading comprehension (TRC) and relation extraction (TRE). For example in TRC, we need to understand the temporal semantic differences between the following two questions that are lexically near-identical: "What finished right before the decision?" or "What finished right after the decision?". To discern the two questions, existing solutions have relied on answer overlaps as a proxy label to contrast similar and dissimilar questions. However, we claim that answer overlap can lead to unreliable results, due to spurious overlaps of two dissimilar questions with coincidentally identical answers. To address the issue, we propose a novel approach that elicits proper reasoning behaviors through a module for predicting time spans of events. We introduce the Timeline Reasoning Network (TRN) operating in a two-step inductive reasoning process: In the first step model initially answers each question with semantic and syntactic information. The next step chains multiple questions on the same event to predict a timeline, which is then used to ground the answers. Results on the TORQUE and TB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms previous methods by effectively resolving the spurious overlaps using the predicted timeline.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team</title>
<link>https://arxiv.org/abs/2506.14234</link>
<guid>https://arxiv.org/abs/2506.14234</guid>
<content:encoded><![CDATA[
<div> framework, experience-aware, language models, reasoning, multi-agent<br />
Summary:<br />
The article introduces Xolver, a multi-agent reasoning framework designed to enhance large language models by incorporating a persistent memory of holistic experiences. Unlike traditional models that operate in isolation, Xolver integrates various experience modalities such as external and self-retrieval, tool use, collaborative interactions, and iterative refinement. By learning from past strategies, code fragments, and abstract reasoning patterns during inference, Xolver moves away from generating solutions from scratch, leading to improved performance. Even with lightweight models, Xolver outperforms advanced specialized agents and achieves new best results on various tasks. The framework demonstrates the importance of holistic experience learning in developing generalist agents capable of expert-level reasoning.<br /> <div>
arXiv:2506.14234v1 Announce Type: new 
Abstract: Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.14235</link>
<guid>https://arxiv.org/abs/2506.14235</guid>
<content:encoded><![CDATA[
<div> Keywords: Temporal knowledge graph reasoning, Multi-Expert Structural-Semantic Hybrid framework, event prediction, graph structure learning, semantic reasoning

Summary:
The article introduces the Multi-Expert Structural-Semantic Hybrid (MESH) framework for temporal knowledge graph reasoning, aiming to predict future events with integrated structural and semantic information. Previous methods have focused on either graph structure learning or semantic reasoning, lacking the ability to handle different prediction scenarios effectively. The MESH framework incorporates three expert modules to guide the reasoning process for historical and non-historical events, enhancing generalization across various temporal contexts. Experimental results on three datasets demonstrate the efficiency of the proposed approach in improving event prediction accuracy. The MESH framework provides a novel solution for integrating dual reasoning perspectives and capturing inherent differences between event types, advancing the field of temporal knowledge graph reasoning. 

<br /><br />Summary: <div>
arXiv:2506.14235v1 Announce Type: new 
Abstract: Temporal knowledge graph reasoning aims to predict future events with knowledge of existing facts and plays a key role in various downstream tasks. Previous methods focused on either graph structure learning or semantic reasoning, failing to integrate dual reasoning perspectives to handle different prediction scenarios. Moreover, they lack the capability to capture the inherent differences between historical and non-historical events, which limits their generalization across different temporal contexts. To this end, we propose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs three kinds of expert modules to integrate both structural and semantic information, guiding the reasoning process for different events. Extensive experiments on three datasets demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-Initialization Token Learning for Tool-Augmented Large Language Models</title>
<link>https://arxiv.org/abs/2506.14248</link>
<guid>https://arxiv.org/abs/2506.14248</guid>
<content:encoded><![CDATA[
<div> large language models, numerical reasoning, plan generation, tool integration, token learning<br />
Summary:
This study focuses on enhancing the problem-solving capabilities of large language models (LLMs) by integrating external tools such as calculators and databases. Current methods assign unique tokens to each tool, limiting adaptability within pre-trained LLMs. The proposed novel token learning method aligns tool tokens with the existing word embedding space, improving model performance. Prior token embeddings for each tool based on their name or description are used for initialization and regularization of learnable tool token embeddings. Evaluation on tasks like numerical reasoning and question answering shows clear improvements over recent baselines. The results highlight that the approach effectively augments LLMs with tools through relevant tokens across diverse domains.<br /><br />Summary: <div>
arXiv:2506.14248v1 Announce Type: new 
Abstract: Large language models have demonstrated exceptional performance, yet struggle with complex tasks such as numerical reasoning, plan generation. Integrating external tools, such as calculators and databases, into large language models (LLMs) is crucial for enhancing problem-solving capabilities. Current methods assign a unique token to each tool, enabling LLMs to call tools through token prediction-similar to word generation. However, this approach fails to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs. To address this issue, we propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, thereby enhancing model performance. We begin by constructing prior token embeddings for each tool based on the tool's name or description, which are used to initialize and regularize the learnable tool token embeddings. This ensures the learned embeddings are well-aligned with the word token space, improving tool call accuracy. We evaluate the method on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The results demonstrate clear improvements over recent baselines, including CoT, REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments LLMs with tools through relevant tokens across diverse domains.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents</title>
<link>https://arxiv.org/abs/2506.14285</link>
<guid>https://arxiv.org/abs/2506.14285</guid>
<content:encoded><![CDATA[
<div> dialogue response generation, timely response prediction, temporal context, TimelyChat benchmark, Timer model <br />
Summary: <br />
This research introduces the concept of timely dialogue response generation, focusing on generating responses based on both textual and temporal context. The TimelyChat benchmark evaluates language models on predicting appropriate time intervals and generating time-conditioned responses. A large-scale training dataset is created using unlabeled event knowledge from a temporal commonsense knowledge graph, and a dialogue agent called Timer is trained to predict time intervals and generate timely responses. Experimental results show that Timer outperforms prompting-based language models and other baselines in both turn-level and dialogue-level evaluations. The dataset, model, and code are publicly released for further research in dialogue systems. <br /> <div>
arXiv:2506.14285v1 Announce Type: new 
Abstract: While research on dialogue response generation has primarily focused on generating coherent responses conditioning on textual context, the critical question of when to respond grounded on the temporal context remains underexplored. To bridge this gap, we propose a novel task called timely dialogue response generation and introduce the TimelyChat benchmark, which evaluates the capabilities of language models to predict appropriate time intervals and generate time-conditioned responses. Additionally, we construct a large-scale training dataset by leveraging unlabeled event knowledge from a temporal commonsense knowledge graph and employing a large language model (LLM) to synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent designed to proactively predict time intervals and generate timely responses that align with those intervals. Experimental results show that Timer outperforms prompting-based LLMs and other fine-tuned baselines in both turn-level and dialogue-level evaluations. We publicly release our data, model, and code.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent</title>
<link>https://arxiv.org/abs/2506.14302</link>
<guid>https://arxiv.org/abs/2506.14302</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Conversational Recommendation Agents, Preference Optimization, Multi-turn Dialogue, User Satisfaction<br />
<br />
Summary:<br />
Recent advancements in Large Language Models (LLMs) have led to the development of Conversational Recommendation Agents (CRAs), but their short-sighted responses often fail to meet user expectations. To address this challenge, a novel multi-turn preference optimization (MTPO) paradigm called ECPO is introduced. ECPO leverages Expectation Confirmation Theory to model user satisfaction evolution in multi-turn dialogues, identifying causes of dissatisfaction for targeted optimization. This approach eliminates sampling overhead while driving meaningful improvements. An LLM-based user simulator, AILO, is introduced to support ECPO. Experimental results demonstrate that ECPO significantly enhances CRA interaction capabilities, improving efficiency and effectiveness compared to existing MTPO methods. <div>
arXiv:2506.14302v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have significantly propelled the development of Conversational Recommendation Agents (CRAs). However, these agents often generate short-sighted responses that fail to sustain user guidance and meet expectations. Although preference optimization has proven effective in aligning LLMs with user expectations, it remains costly and performs poorly in multi-turn dialogue. To address this challenge, we introduce a novel multi-turn preference optimization (MTPO) paradigm ECPO, which leverages Expectation Confirmation Theory to explicitly model the evolution of user satisfaction throughout multi-turn dialogues, uncovering the underlying causes of dissatisfaction. These causes can be utilized to support targeted optimization of unsatisfactory responses, thereby achieving turn-level preference optimization. ECPO ingeniously eliminates the significant sampling overhead of existing MTPO methods while ensuring the optimization process drives meaningful improvements. To support ECPO, we introduce an LLM-based user simulator, AILO, to simulate user feedback and perform expectation confirmation during conversational recommendations. Experimental results show that ECPO significantly enhances CRA's interaction capabilities, delivering notable improvements in both efficiency and effectiveness over existing MTPO methods.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation Should Not Ignore Variation: On the Impact of Reference Set Choice on Summarization Metrics</title>
<link>https://arxiv.org/abs/2506.14335</link>
<guid>https://arxiv.org/abs/2506.14335</guid>
<content:encoded><![CDATA[
<div> Variation, Reference-based metrics, Summarization evaluation, Multi-reference datasets, Human judgments <br />
Summary: 
This article explores the impact of using different reference sets on widely used reference-based metrics in the evaluation of summarization systems. The study analyzes three diverse multi-reference summarization datasets and finds that popular metrics exhibit significant instability, particularly n-gram-based metrics like ROUGE. Model rankings vary depending on the reference sets, leading to unreliable comparisons. Additionally, human judgments on genre-diverse data show weak-to-no correlation with metrics, indicating the need for improved evaluation methods, especially for Language Model models. The study recommends incorporating reference set variation into summarization evaluation to enhance consistency and alignment with human judgments. <br /> <div>
arXiv:2506.14335v1 Announce Type: new 
Abstract: Human language production exhibits remarkable richness and variation, reflecting diverse communication styles and intents. However, this variation is often overlooked in summarization evaluation. While having multiple reference summaries is known to improve correlation with human judgments, the impact of using different reference sets on reference-based metrics has not been systematically investigated. This work examines the sensitivity of widely used reference-based metrics in relation to the choice of reference sets, analyzing three diverse multi-reference summarization datasets: SummEval, GUMSum, and DUC2004. We demonstrate that many popular metrics exhibit significant instability. This instability is particularly concerning for n-gram-based metrics like ROUGE, where model rankings vary depending on the reference sets, undermining the reliability of model comparisons. We also collect human judgments on LLM outputs for genre-diverse data and examine their correlation with metrics to supplement existing findings beyond newswire summaries, finding weak-to-no correlation. Taken together, we recommend incorporating reference set variation into summarization evaluation to enhance consistency alongside correlation with human judgments, especially when evaluating LLMs.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis</title>
<link>https://arxiv.org/abs/2506.14345</link>
<guid>https://arxiv.org/abs/2506.14345</guid>
<content:encoded><![CDATA[
<div> Large Language Models, deep research systems, geo-temporal reasoning, information access, AI-driven

Summary: The paper discusses the importance of integrating geo-temporal capabilities into deep research systems powered by Large Language Models. It highlights the necessity of handling geographic and temporal constraints in answering context-rich questions in domains such as public health, environmental science, and socio-economic analysis. The vision outlined in the paper emphasizes the need for augmenting retrieval and synthesis processes with geo-temporal reasoning, supported by open and reproducible infrastructures and rigorous evaluation protocols. By addressing technical, infrastructural, and evaluative challenges, the paper proposes a path towards developing advanced and geo-temporally aware deep research systems that can enhance AI-driven information access in a variety of domains. <div>
arXiv:2506.14345v1 Announce Type: new 
Abstract: The emergence of Large Language Models (LLMs) has transformed information access, with current LLMs also powering deep research systems that can generate comprehensive report-style answers, through planned iterative search, retrieval, and reasoning. Still, current deep research systems lack the geo-temporal capabilities that are essential for answering context-rich questions involving geographic and/or temporal constraints, frequently occurring in domains like public health, environmental science, or socio-economic analysis. This paper reports our vision towards next generation systems, identifying important technical, infrastructural, and evaluative challenges in integrating geo-temporal reasoning into deep research pipelines. We argue for augmenting retrieval and synthesis processes with the ability to handle geo-temporal constraints, supported by open and reproducible infrastructures and rigorous evaluation protocols. Our vision outlines a path towards more advanced and geo-temporally aware deep research systems, of potential impact to the future of AI-driven information access.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits</title>
<link>https://arxiv.org/abs/2506.14370</link>
<guid>https://arxiv.org/abs/2506.14370</guid>
<content:encoded><![CDATA[
<div> Keywords: search engines, content visibility, algorithmic curation, social media, biases

Summary: 
Google, as a prominent search engine, plays a significant role in shaping the visibility of web and social media content through its algorithmic curation practices. This study examines how Google selectively promotes or suppresses certain hashtags and subreddits, influencing the information users encounter. The analysis compares search engine results with data from Reddit and Twitter/X, revealing systematic biases in content visibility. Google's algorithms tend to suppress sexually explicit material, conspiracy theories, advertisements, and cryptocurrencies-related content while prioritizing content with higher engagement. These findings suggest that Google's gatekeeping practices have a notable impact on public discourse by curating the social media narratives available to users.<br /><br />Summary: <div>
arXiv:2506.14370v1 Announce Type: new 
Abstract: Search engines play a crucial role as digital gatekeepers, shaping the visibility of Web and social media content through algorithmic curation. This study investigates how search engines like Google selectively promotes or suppresses certain hashtags and subreddits, impacting the information users encounter. By comparing search engine results with nonsampled data from Reddit and Twitter/X, we reveal systematic biases in content visibility. Google's algorithms tend to suppress subreddits and hashtags related to sexually explicit material, conspiracy theories, advertisements, and cryptocurrencies, while promoting content associated with higher engagement. These findings suggest that Google's gatekeeping practices influence public discourse by curating the social media narratives available to users.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection</title>
<link>https://arxiv.org/abs/2506.14371</link>
<guid>https://arxiv.org/abs/2506.14371</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, chat interfaces, critical thinking skills, automatic question generation, argument mining

Summary:
This study examines the use of Large Language Models (LLMs) in chat interfaces to promote critical thinking skills by generating critical questions to challenge unsupported claims. The proposed two-step framework involves a Questioner model that generates multiple candidate questions and a Judge model that selects the most relevant ones. The system developed for this task ranked first in the shared competition, showcasing the effectiveness of LLM-based approaches in encouraging critical engagement with argumentative texts. This research highlights the potential of leveraging LLMs beyond factual information retrieval to foster deeper reasoning and critical questioning in educational settings. <div>
arXiv:2506.14371v1 Announce Type: new 
Abstract: The widespread adoption of chat interfaces based on Large Language Models (LLMs) raises concerns about promoting superficial learning and undermining the development of critical thinking skills. Instead of relying on LLMs purely for retrieving factual information, this work explores their potential to foster deeper reasoning by generating critical questions that challenge unsupported or vague claims in debate interventions. This study is part of a shared task of the 12th Workshop on Argument Mining, co-located with ACL 2025, focused on automatic critical question generation. We propose a two-step framework involving two small-scale open source language models: a Questioner that generates multiple candidate questions and a Judge that selects the most relevant ones. Our system ranked first in the shared task competition, demonstrating the potential of the proposed LLM-based approach to encourage critical engagement with argumentative texts.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding</title>
<link>https://arxiv.org/abs/2506.14397</link>
<guid>https://arxiv.org/abs/2506.14397</guid>
<content:encoded><![CDATA[
<div> negation, linguistic phenomenon, Large Language Models, Thunder-NUBench, sentence-level

Summary: 
The article introduces Thunder-NUBench, a new benchmark specifically designed to evaluate sentence-level negation understanding in Large Language Models (LLMs). Negation, a fundamental linguistic phenomenon, often poses challenges for LLMs, requiring deep semantic understanding. Existing benchmarks usually treat negation as a subset of broader tasks like natural language inference, leading to a lack of focused evaluation on negation understanding. Thunder-NUBench aims to address this gap by presenting a variety of negation types, including standard negation, local negation, contradiction, and paraphrase. The benchmark consists of curated sentence-negation pairs and a multiple-choice dataset for comprehensive evaluation of LLMs' negation comprehension. Thunder-NUBench goes beyond mere surface-level cue detection, providing a nuanced assessment of models' ability to grasp the complexities of negation in language. <div>
arXiv:2506.14397v1 Announce Type: new 
Abstract: Negation is a fundamental linguistic phenomenon that poses persistent challenges for Large Language Models (LLMs), particularly in tasks requiring deep semantic understanding. Existing benchmarks often treat negation as a side case within broader tasks like natural language inference, resulting in a lack of benchmarks that exclusively target negation understanding. In this work, we introduce \textbf{Thunder-NUBench}, a novel benchmark explicitly designed to assess sentence-level negation understanding in LLMs. Thunder-NUBench goes beyond surface-level cue detection by contrasting standard negation with structurally diverse alternatives such as local negation, contradiction, and paraphrase. The benchmark consists of manually curated sentence-negation pairs and a multiple-choice dataset that enables in-depth evaluation of models' negation understanding.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge</title>
<link>https://arxiv.org/abs/2506.14407</link>
<guid>https://arxiv.org/abs/2506.14407</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval systems, NLP pipelines, document-side processing, temporal relationships, world knowledge

Summary:
ImpliRet introduces a benchmark for evaluating retrieval systems that rely on implicit facts stated in documents through temporal, arithmetic, and world knowledge relationships. Unlike other benchmarks that rely on reasoning-heavy queries, ImpliRet shifts the challenge to document-side processing. Various sparse and dense retrievers struggle in this setting, with the best nDCG@10 score reaching only 15.07%. Long-context models like GPT-4.1 also face difficulties, scoring only 35.06% even with a short context of ten documents. This highlights the ongoing challenge of document-side reasoning in retrieval systems. The study emphasizes the importance of moving beyond surface-level cues and prompts for better understanding implicit relationships in documents. <div>
arXiv:2506.14407v1 Announce Type: new 
Abstract: Retrieval systems are central to many NLP pipelines, but often rely on surface-level cues such as keyword overlap and lexical semantic similarity. To evaluate retrieval beyond these shallow signals, recent benchmarks introduce reasoning-heavy queries; however, they primarily shift the burden to query-side processing techniques -- like prompting or multi-hop retrieval -- that can help resolve complexity. In contrast, we present ImpliRet, a benchmark that shifts the reasoning challenge to document-side processing: The queries are simple, but relevance depends on facts stated implicitly in documents through temporal (e.g., resolving "two days ago"), arithmetic, and world knowledge relationships. We evaluate a range of sparse and dense retrievers, all of which struggle in this setting: the best nDCG@10 is only 15.07%. We also test whether long-context models can overcome this limitation. But even with a short context of only ten documents, including the positive document, GPT-4.1 scores only 35.06%, showing that document-side reasoning remains a challenge. Our codes are available at github.com/ZeinabTaghavi/IMPLIRET.Contribution.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs</title>
<link>https://arxiv.org/abs/2506.14429</link>
<guid>https://arxiv.org/abs/2506.14429</guid>
<content:encoded><![CDATA[
<div> diffusion LLMs, auto-regressive LLMs, context extrapolation, LongLLaDA, RoPE scaling theory

Summary:
Diffusion Large Language Models (LLMs) and traditional auto-regressive LLMs are compared for their long-context performance. Diffusion LLMs show stable perplexity during context extrapolation and exhibit a local perception phenomenon for successful retrieval in the Needle-In-A-Haystack task. The study introduces LongLLaDA, a training-free method integrating LLaDA with NTK-based RoPE extrapolation. It validates established scaling laws for extending the context windows of diffusion LLMs and identifies task-specific performance differences between diffusion LLMs and auto-regressive LLMs. This work establishes a context extrapolation method for diffusion LLMs, offering theoretical insights and empirical benchmarks for future research in long-context diffusion LLMs. 

<br /><br />Summary: <div>
arXiv:2506.14429v1 Announce Type: new 
Abstract: Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \textbf{\textit{stable perplexity}} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \textbf{\textit{local perception}} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison</title>
<link>https://arxiv.org/abs/2506.14448</link>
<guid>https://arxiv.org/abs/2506.14448</guid>
<content:encoded><![CDATA[
<div> semantic games, test-time learning, evaluation framework, language models, artificial general intelligence

Summary:
Language models are crucial in the development of artificial general intelligence. Current evaluation benchmarks focus on static knowledge, but true intelligence also involves the ability to learn rapidly from experience. The concept of Test-time Learning, which assesses the capacity to improve performance in experience-based tasks during testing, is proposed. Semantic games are suggested as effective testbeds for evaluating test-time learning, as they require strategic reasoning and are resistant to saturation. An evaluation framework is introduced to compare model performance under different experience settings. Results show that large language models demonstrate test-time learning capabilities, but their improvements are less stable and slower than those of humans. This highlights the potential of language models as general-purpose learning machines while also identifying a significant intellectual gap between models and humans. <div>
arXiv:2506.14448v1 Announce Type: new 
Abstract: As evaluation designs of large language models may shape our trajectory toward artificial general intelligence, comprehensive and forward-looking assessment is essential. Existing benchmarks primarily assess static knowledge, while intelligence also entails the ability to rapidly learn from experience. To this end, we advocate for the evaluation of Test-time Learning, the capacity to improve performance in experience-based, reasoning-intensive tasks during test time. In this work, we propose semantic games as effective testbeds for evaluating test-time learning, due to their resistance to saturation and inherent demand for strategic reasoning. We introduce an objective evaluation framework that compares model performance under both limited and cumulative experience settings, and contains four forms of experience representation. To provide a comparative baseline, we recruit eight human participants to complete the same task. Results show that LLMs exhibit measurable test-time learning capabilities; however, their improvements are less stable under cumulative experience and progress more slowly than those observed in humans. These findings underscore the potential of LLMs as general-purpose learning machines, while also revealing a substantial intellectual gap between models and humans, irrespective of how well LLMs perform on static benchmarks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data</title>
<link>https://arxiv.org/abs/2506.14474</link>
<guid>https://arxiv.org/abs/2506.14474</guid>
<content:encoded><![CDATA[
<div> watermarking, large language models, text, synonym substitutions, data protection

Summary:
LexiMark is a novel watermarking technique designed for text and documents to enhance an LLM's memorization capabilities without altering semantic integrity. It embeds synonym substitutions for high-entropy words, making the watermark difficult to detect and resistant to removal. The method was evaluated on seven open-source models, showing significant improvements in AUROC scores compared to existing methods. LexiMark effectively verifies unauthorized use of watermarked data in LLM training, making it a reliable tool for data protection.<br /><br />Summary: <div>
arXiv:2506.14474v1 Announce Type: new 
Abstract: Large language models (LLMs) can be trained or fine-tuned on data obtained without the owner's consent. Verifying whether a specific LLM was trained on particular data instances or an entire dataset is extremely challenging. Dataset watermarking addresses this by embedding identifiable modifications in training data to detect unauthorized use. However, existing methods often lack stealth, making them relatively easy to detect and remove. In light of these limitations, we propose LexiMark, a novel watermarking technique designed for text and documents, which embeds synonym substitutions for carefully selected high-entropy words. Our method aims to enhance an LLM's memorization capabilities on the watermarked text without altering the semantic integrity of the text. As a result, the watermark is difficult to detect, blending seamlessly into the text with no visible markers, and is resistant to removal due to its subtle, contextually appropriate substitutions that evade automated and manual detection. We evaluated our method using baseline datasets from recent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral 7B, Pythia 6.9B, as well as three smaller variants from the Pythia family (160M, 410M, and 1B). Our evaluation spans multiple training settings, including continued pretraining and fine-tuning scenarios. The results demonstrate significant improvements in AUROC scores compared to existing methods, underscoring our method's effectiveness in reliably verifying whether unauthorized watermarked data was used in LLM training.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops</title>
<link>https://arxiv.org/abs/2506.14493</link>
<guid>https://arxiv.org/abs/2506.14493</guid>
<content:encoded><![CDATA[
<div> POS tag, EOS token, LingoLoop, Generative Path Pruning Mechanism, energy-latency attacks

Summary: 
- Multimodal Large Language Models (MLLMs) are vulnerable to attacks that induce them to generate excessively verbose and repetitive sequences.
- LingoLoop attack utilizes a POS-Aware Delay Mechanism to postpone EOS token generation by adjusting attention weights based on POS information.
- The attack also employs a Generative Path Pruning Mechanism to limit hidden state magnitude, encouraging the model to produce repetitive loops.
- Extensive experiments demonstrated LingoLoop's effectiveness in increasing generated tokens and energy consumption on models like Qwen2.5-VL-3B.
- The findings highlight significant vulnerabilities in MLLMs and pose challenges for their reliable deployment.

Summary: <div>
arXiv:2506.14493v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have shown great promise but require substantial computational resources during inference. Attackers can exploit this by inducing excessive output, leading to resource exhaustion and service degradation. Prior energy-latency attacks aim to increase generation time by broadly shifting the output token distribution away from the EOS token, but they neglect the influence of token-level Part-of-Speech (POS) characteristics on EOS and sentence-level structural patterns on output counts, limiting their efficacy. To address this, we propose LingoLoop, an attack designed to induce MLLMs to generate excessively verbose and repetitive sequences. First, we find that the POS tag of a token strongly affects the likelihood of generating an EOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to postpone EOS token generation by adjusting attention weights guided by POS information. Second, we identify that constraining output diversity to induce repetitive loops is effective for sustained generation. We introduce a Generative Path Pruning Mechanism that limits the magnitude of hidden states, encouraging the model to produce persistent loops. Extensive experiments demonstrate LingoLoop can increase generated tokens by up to 30 times and energy consumption by a comparable factor on models like Qwen2.5-VL-3B, consistently driving MLLMs towards their maximum generation limits. These findings expose significant MLLMs' vulnerabilities, posing challenges for their reliable deployment. The code will be released publicly following the paper's acceptance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models</title>
<link>https://arxiv.org/abs/2506.14532</link>
<guid>https://arxiv.org/abs/2506.14532</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network, beam prediction, millimeter-wave communication, multimodal sensor data, fine-tuning<br />
<br />
Summary: 
The paper introduces M2BeamLLM, a novel neural network framework for beam prediction in mmWave mMIMO communication systems. M2BeamLLM integrates multi-modal sensor data like images, radar, LiDAR, and GPS, using large language models (LLMs) for beam prediction. Through sensing data encoding, multimodal alignment and fusion, and supervised fine-tuning (SFT), M2BeamLLM achieves higher accuracy and robustness in beam prediction compared to traditional deep learning models. It outperforms them in standard and few-shot scenarios, with performance improving as sensing modality diversity increases. The study presents an efficient solution for vehicle-to-infrastructure mmWave communication systems. <br /><br />Summary: <div>
arXiv:2506.14532v1 Announce Type: new 
Abstract: This paper introduces a novel neural network framework called M2BeamLLM for beam prediction in millimeter-wave (mmWave) massive multi-input multi-output (mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data, including images, radar, LiDAR, and GPS, leveraging the powerful reasoning capabilities of large language models (LLMs) such as GPT-2 for beam prediction. By combining sensing data encoding, multimodal alignment and fusion, and supervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam prediction accuracy and robustness, demonstrably outperforming traditional deep learning (DL) models in both standard and few-shot scenarios. Furthermore, its prediction performance consistently improves with increased diversity in sensing modalities. Our study provides an efficient and intelligent beam prediction solution for vehicle-to-infrastructure (V2I) mmWave communication systems.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs</title>
<link>https://arxiv.org/abs/2506.14562</link>
<guid>https://arxiv.org/abs/2506.14562</guid>
<content:encoded><![CDATA[
<div> regularization, language models, weight decay, AlphaDecay, spectral properties <br />
Summary:<br />Weight decay is a common regularization technique for training large language models (LLMs), but assigning a uniform decay rate to every layer may not be optimal due to the structural diversity and varying spectral properties across modules. In this paper, the authors introduce AlphaDecay, a method that adaptively assigns different weight decay strengths to each module of an LLM based on its spectral properties. By leveraging Heavy-Tailed Self-Regularization theory and analyzing the empirical spectral density of weight correlation matrices, AlphaDecay aims to balance module-wise differences in feature learning. Experimental results on pre-training tasks with various model sizes show that AlphaDecay outperforms uniform decay and other adaptive decay baselines in terms of perplexity and generalization. <div>
arXiv:2506.14562v1 Announce Type: new 
Abstract: Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenerationPrograms: Fine-grained Attribution with Executable Programs</title>
<link>https://arxiv.org/abs/2506.14580</link>
<guid>https://arxiv.org/abs/2506.14580</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, attribution methods, GenerationPrograms, interpretability, text generation

Summary:
GenerationPrograms is a new modular generation framework inspired by executable "code agent" architectures. It aims to improve attributions in large language models by decomposing the generation process into two stages: creating an executable program plan tailored to the query, and executing modular text operations to produce the final response. This approach enhances attribution quality at both document and sentence levels in long-form question-answering tasks and multi-document summarization. GenerationPrograms outperforms traditional techniques in post-hoc attribution, providing accurate attributions. The interpretable programs generated enable localized refinement through modular-level improvements, further enhancing overall attribution quality. <div>
arXiv:2506.14580v1 Announce Type: new 
Abstract: Recent large language models (LLMs) achieve impressive performance in source-conditioned text generation but often fail to correctly provide fine-grained attributions for their outputs, undermining verifiability and trust. Moreover, existing attribution methods do not explain how and why models leverage the provided source documents to generate their final responses, limiting interpretability. To overcome these challenges, we introduce a modular generation framework, GenerationPrograms, inspired by recent advancements in executable "code agent" architectures. Unlike conventional generation methods that simultaneously generate outputs and attributions or rely on post-hoc attribution, GenerationPrograms decomposes the process into two distinct stages: first, creating an executable program plan composed of modular text operations (such as paraphrasing, compression, and fusion) explicitly tailored to the query, and second, executing these operations following the program's specified instructions to produce the final response. Empirical evaluations demonstrate that GenerationPrograms significantly improves attribution quality at both the document level and sentence level across two long-form question-answering tasks and a multi-document summarization task. We further demonstrate that GenerationPrograms can effectively function as a post-hoc attribution method, outperforming traditional techniques in recovering accurate attributions. In addition, the interpretable programs generated by GenerationPrograms enable localized refinement through modular-level improvements that further enhance overall attribution quality.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees</title>
<link>https://arxiv.org/abs/2506.14606</link>
<guid>https://arxiv.org/abs/2506.14606</guid>
<content:encoded><![CDATA[
<div> GG (Guaranteed Guess), ISA-centric transpilation pipeline, translation, LLMs, software testing <br />
<br />
Summary: 
The article introduces GG, a transpilation pipeline that uses large language models (LLMs) and software testing to translate between complex- (CISC) and reduced- (RISC) hardware architectures. GG generates candidate translations using LLMs and embeds them in a software testing framework to ensure confidence in the translation. The approach achieves high code coverage (>98%) and 99% functional/semantic correctness on HumanEval programs and 49% on BringupBench programs. Compared to Rosetta 2 on Apple Silicon, GG shows 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for transpiled code. The authors plan to open-source their codes, data, models, and benchmarks to advance research on ISA-level code translation. <div>
arXiv:2506.14606v1 Announce Type: new 
Abstract: The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Does Meaning Backfire? Investigating the Role of AMRs in NLI</title>
<link>https://arxiv.org/abs/2506.14613</link>
<guid>https://arxiv.org/abs/2506.14613</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural Language Inference, Abstract Meaning Representation, Pretrained Language Models, Fine-tuning, Semantic Reasoning<br />
Summary:<br />
The study explores the impact of incorporating Abstract Meaning Representations (AMR) on the generalization performance of pretrained language models in Natural Language Inference (NLI). Experimental results indicate that the inclusion of AMR during fine-tuning adversely affects model generalization in NLI tasks. However, prompting with AMR leads to slight enhancements in model performance, particularly in the case of GPT-4o. An ablation study further reveals that the improvement observed is primarily attributed to amplifying surface-level discrepancies rather than facilitating semantic reasoning. This amplification tendency may lead models to incorrectly predict non-entailment even when the underlying meaning remains intact. Overall, the findings suggest that while prompting with AMR can yield marginal gains in specific scenarios, caution should be exercised to prevent potential misinterpretation of semantic content during the inference process.<br /> <div>
arXiv:2506.14613v1 Announce Type: new 
Abstract: Natural Language Inference (NLI) relies heavily on adequately parsing the semantic content of the premise and hypothesis. In this work, we investigate whether adding semantic information in the form of an Abstract Meaning Representation (AMR) helps pretrained language models better generalize in NLI. Our experiments integrating AMR into NLI in both fine-tuning and prompting settings show that the presence of AMR in fine-tuning hinders model generalization while prompting with AMR leads to slight gains in \texttt{GPT-4o}. However, an ablation study reveals that the improvement comes from amplifying surface-level differences rather than aiding semantic reasoning. This amplification can mislead models to predict non-entailment even when the core meaning is preserved.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2506.14625</link>
<guid>https://arxiv.org/abs/2506.14625</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, moral reasoning, consensus, alignment, AI systems

Summary:
Large Language Models (LLMs) have shown strong moral reasoning abilities but can diverge in complex moral dilemmas. This study proposes a framework to synthesize multiple LLMs' moral judgments into a collective consensus, weighting contributions by model reliability. An aggregation mechanism fuses moral acceptability scores, aligning divergent models through token embedding optimization. Experiments on a social moral dilemma dataset demonstrate that this approach enhances consensus and individual model fidelity. The findings underscore the value of data-driven moral alignment across LLMs for the development of safer and more consistent AI systems. 

Summary: <div>
arXiv:2506.14625v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation</title>
<link>https://arxiv.org/abs/2506.14634</link>
<guid>https://arxiv.org/abs/2506.14634</guid>
<content:encoded><![CDATA[
arXiv:2506.14634v1 Announce Type: new 
Abstract: The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot</title>
<link>https://arxiv.org/abs/2506.14641</link>
<guid>https://arxiv.org/abs/2506.14641</guid>
<content:encoded><![CDATA[
arXiv:2506.14641v1 Announce Type: new 
Abstract: In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \texttt{Qwen2.5-Max} and \texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments</title>
<link>https://arxiv.org/abs/2506.14645</link>
<guid>https://arxiv.org/abs/2506.14645</guid>
<content:encoded><![CDATA[
arXiv:2506.14645v1 Announce Type: new 
Abstract: The increasing sophistication of large language models (LLMs) has sparked growing concerns regarding their potential role in exacerbating ideological polarization through the automated generation of persuasive and biased content. This study explores the extent to which fine-tuned LLMs can replicate and amplify polarizing discourse within online environments. Using a curated dataset of politically charged discussions extracted from Reddit, we fine-tune an open-source LLM to produce context-aware and ideologically aligned responses. The model's outputs are evaluated through linguistic analysis, sentiment scoring, and human annotation, with particular attention to credibility and rhetorical alignment with the original discourse. The results indicate that, when trained on partisan data, LLMs are capable of producing highly plausible and provocative comments, often indistinguishable from those written by humans. These findings raise significant ethical questions about the use of AI in political discourse, disinformation, and manipulation campaigns. The paper concludes with a discussion of the broader implications for AI governance, platform regulation, and the development of detection tools to mitigate adversarial fine-tuning risks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors</title>
<link>https://arxiv.org/abs/2506.14646</link>
<guid>https://arxiv.org/abs/2506.14646</guid>
<content:encoded><![CDATA[
arXiv:2506.14646v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), offer an efficient way to adapt large language models with reduced computational costs. However, their performance is limited by the small number of trainable parameters. Recent work combines LoRA with the Mixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two limitations remain in hindering the full exploitation of its potential: 1) the influence of downstream tasks when assigning expert numbers, and 2) the uniform rank assignment across all LoRA experts, which restricts representational diversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained layer-wise expert numbers and ranks allocation strategy with GuidedSelection Vectors (GSVs). GSVs are learned via a prior bilevel optimization process to capture both model- and task-specific needs, and are then used to allocate optimal expert numbers and ranks. Experiments on three backbone models across diverse benchmarks show that GuiLoMo consistently achieves superior or comparable performance to all baselines. Further analysis offers key insights into how expert numbers and ranks vary across layers and tasks, highlighting the benefits of adaptive expert configuration. Our code is available at https://github.com/Liar406/Gui-LoMo.git.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality</title>
<link>https://arxiv.org/abs/2506.14681</link>
<guid>https://arxiv.org/abs/2506.14681</guid>
<content:encoded><![CDATA[
arXiv:2506.14681v1 Announce Type: new 
Abstract: Supervised fine-tuning (SFT) is a critical step in aligning large language models (LLMs) with human instructions and values, yet many aspects of SFT remain poorly understood. We trained a wide range of base models on a variety of datasets including code generation, mathematical reasoning, and general-domain tasks, resulting in 1,000+ SFT models under controlled conditions. We then identified the dataset properties that matter most and examined the layer-wise modifications introduced by SFT. Our findings reveal that some training-task synergies persist across all models while others vary substantially, emphasizing the importance of model-specific strategies. Moreover, we demonstrate that perplexity consistently predicts SFT effectiveness--often surpassing superficial similarity between trained data and benchmark--and that mid-layer weight changes correlate most strongly with performance gains. We will release these 1,000+ SFT models and benchmark results to accelerate further research.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers</title>
<link>https://arxiv.org/abs/2506.14702</link>
<guid>https://arxiv.org/abs/2506.14702</guid>
<content:encoded><![CDATA[
arXiv:2506.14702v1 Announce Type: new 
Abstract: One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: "Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data</title>
<link>https://arxiv.org/abs/2506.14704</link>
<guid>https://arxiv.org/abs/2506.14704</guid>
<content:encoded><![CDATA[
arXiv:2506.14704v1 Announce Type: new 
Abstract: This paper studies how the model architecture and data configurations influence the empirical memorization capacity of generative transformers. The models are trained using synthetic text datasets derived from the Systematized Nomenclature of Medicine (SNOMED) knowledge graph: triplets, representing static connections, and sequences, simulating complex relation patterns. The results show that embedding size is the primary determinant of learning speed and capacity, while additional layers provide limited benefits and may hinder performance on simpler datasets. Activation functions play a crucial role, and Softmax demonstrates greater stability and capacity. Furthermore, increasing the complexity of the data set seems to improve the final memorization. These insights improve our understanding of transformer memory mechanisms and provide a framework for optimizing model design with structured real-world data.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs</title>
<link>https://arxiv.org/abs/2506.14731</link>
<guid>https://arxiv.org/abs/2506.14731</guid>
<content:encoded><![CDATA[
arXiv:2506.14731v1 Announce Type: new 
Abstract: We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning with Exploration: An Entropy Perspective</title>
<link>https://arxiv.org/abs/2506.14758</link>
<guid>https://arxiv.org/abs/2506.14758</guid>
<content:encoded><![CDATA[
arXiv:2506.14758v1 Announce Type: new 
Abstract: Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing language model (LM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LMs. Through empirical analysis, we uncover strong positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LM reasoning.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Bytes to Ideas: Language Modeling with Autoregressive U-Nets</title>
<link>https://arxiv.org/abs/2506.14761</link>
<guid>https://arxiv.org/abs/2506.14761</guid>
<content:encoded><![CDATA[
arXiv:2506.14761v1 Announce Type: new 
Abstract: Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Variational Framework for Improving Naturalness in Generative Spoken Language Models</title>
<link>https://arxiv.org/abs/2506.14767</link>
<guid>https://arxiv.org/abs/2506.14767</guid>
<content:encoded><![CDATA[
arXiv:2506.14767v1 Announce Type: new 
Abstract: The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at https://github.com/b04901014/vae-gslm.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LittleBit: Ultra Low-Bit Quantization via Latent Factorization</title>
<link>https://arxiv.org/abs/2506.13771</link>
<guid>https://arxiv.org/abs/2506.13771</guid>
<content:encoded><![CDATA[
arXiv:2506.13771v1 Announce Type: cross 
Abstract: Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. This establishes a superior size-performance trade-off, with kernel-level benchmarks indicating potential for a 5$\times$ speedup compared to FP16. LittleBit paves the way for deploying powerful LLMs in resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Compression via Question Generation: Enhancing Multihop Document Retrieval without Fine-tuning</title>
<link>https://arxiv.org/abs/2506.13778</link>
<guid>https://arxiv.org/abs/2506.13778</guid>
<content:encoded><![CDATA[
arXiv:2506.13778v1 Announce Type: cross 
Abstract: This study presents a question-based knowledge encoding approach that improves retrieval-augmented generation (RAG) systems without requiring fine-tuning or traditional chunking. We encode textual content using generated questions that span the lexical and semantic space, creating targeted retrieval cues combined with a custom syntactic reranking method.
  In single-hop retrieval over 109 scientific papers, our approach achieves a Recall@3 of 0.84, outperforming traditional chunking methods by 60 percent. We also introduce "paper-cards", concise paper summaries under 300 characters, which enhance BM25 retrieval, increasing MRR@3 from 0.56 to 0.85 on simplified technical queries.
  For multihop tasks, our reranking method reaches an F1 score of 0.52 with LLaMA2-Chat-7B on the LongBench 2WikiMultihopQA dataset, surpassing chunking and fine-tuned baselines which score 0.328 and 0.412 respectively.
  This method eliminates fine-tuning requirements, reduces retrieval latency, enables intuitive question-driven knowledge access, and decreases vector storage demands by 80%, positioning it as a scalable and efficient RAG alternative.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AcademicBrowse: Benchmarking Academic Browse Ability of LLMs</title>
<link>https://arxiv.org/abs/2506.13784</link>
<guid>https://arxiv.org/abs/2506.13784</guid>
<content:encoded><![CDATA[
arXiv:2506.13784v1 Announce Type: cross 
Abstract: Large Language Models (LLMs)' search capabilities have garnered significant attention. Existing benchmarks, such as OpenAI's BrowseComp, primarily focus on general search scenarios and fail to adequately address the specific demands of academic search. These demands include deeper literature tracing and organization, professional support for academic databases, the ability to navigate long-tail academic knowledge, and ensuring academic rigor. Here, we proposed AcademicBrowse, the first dataset specifically designed to evaluate the complex information retrieval capabilities of Large Language Models (LLMs) in academic research. AcademicBrowse possesses the following key characteristics: Academic Practicality, where question content closely mirrors real academic learning and research environments, avoiding deliberately misleading models; High Difficulty, with answers that are challenging for single models (e.g., Grok DeepSearch or Gemini Deep Research) to provide directly, often requiring at least three deep searches to derive; Concise Evaluation, where limiting conditions ensure answers are as unique as possible, accompanied by clear sources and brief solution explanations, greatly facilitating subsequent audit and verification, surpassing the current lack of analyzed search datasets both domestically and internationally; and Broad Coverage, as the dataset spans at least 15 different academic disciplines. Through AcademicBrowse, we expect to more precisely measure and promote the performance improvement of LLMs in complex academic information retrieval tasks. The data is available at: https://huggingface.co/datasets/PKU-DS-LAB/AcademicBrowse
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \&amp; a ML Ensemble on Longitudinal Identity Resolution</title>
<link>https://arxiv.org/abs/2506.13792</link>
<guid>https://arxiv.org/abs/2506.13792</guid>
<content:encoded><![CDATA[
arXiv:2506.13792v1 Announce Type: cross 
Abstract: We introduce ICE-ID, a novel benchmark dataset for historical identity resolution, comprising 220 years (1703-1920) of Icelandic census records. ICE-ID spans multiple generations of longitudinal data, capturing name variations, demographic changes, and rich genealogical links. To the best of our knowledge, this is the first large-scale, open tabular dataset specifically designed to study long-term person-entity matching in a real-world population. We define identity resolution tasks (within and across census waves) with clearly documented metrics and splits. We evaluate a range of methods: handcrafted rule-based matchers, a ML ensemble as well as LLMs for structured data (e.g. transformer-based tabular networks) against a novel approach to tabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose AI framework designed to reason with limited knowledge and resources. Its core is Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that NARS is suprisingly simple and competitive with other standard approaches, achieving SOTA at our task. By releasing ICE-ID and our code, we enable reproducible benchmarking of identity resolution approaches in longitudinal settings and hope that ICE-ID opens new avenues for cross-disciplinary research in data linkage and historical analytics.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study</title>
<link>https://arxiv.org/abs/2506.13811</link>
<guid>https://arxiv.org/abs/2506.13811</guid>
<content:encoded><![CDATA[
arXiv:2506.13811v1 Announce Type: cross 
Abstract: This study investigates router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection. Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection. Performance assessment utilized baseline models including DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design scenarios. The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, representing improvements of 8.75 and 3.13 percentage points over standalone Grok 3 performance respectively. The system outperformed conventional agentic workflows by 10.0 to 43.75 percentage points. Grok 3 demonstrated superior standalone performance without external computational tools, indicating advances in direct LLM mathematical reasoning for engineering applications. The dual-tier classification framework successfully distinguished foundation types, enabling appropriate analytical approaches. Results establish router-based multi-agent systems as optimal for foundation design automation while maintaining professional documentation standards. Given safety-critical requirements in civil engineering, continued human oversight remains essential, positioning these systems as advanced computational assistance tools rather than autonomous design replacements in professional practice.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models</title>
<link>https://arxiv.org/abs/2506.13923</link>
<guid>https://arxiv.org/abs/2506.13923</guid>
<content:encoded><![CDATA[
arXiv:2506.13923v1 Announce Type: cross 
Abstract: We study the process through which reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. We find that RLVR drives performance through two main means: (1) by compressing pass@$k$ into pass@1 and (2) via "capability gain" in which models learn to solve new problems that they previously could not solve even at high $k$. We find that while capability gain exists across model scales, learning to solve new problems is primarily driven through self-distillation. We demonstrate these findings across model scales ranging from 0.5B to 72B on >500,000 reasoning problems with prompts and verifiable final answers across math, science, and code domains. We further show that we can significantly improve pass@$k$ rates by leveraging natural language guidance for the model to consider within context while still requiring the model to derive a solution chain from scratch. Based of these insights, we derive $\text{Guide}$ - a new class of online training algorithms. $\text{Guide}$ adaptively incorporates hints into the model's context on problems for which all rollouts were initially incorrect and adjusts the importance sampling ratio for the "off-policy" trajectories in order to optimize the policy for contexts in which the hints are no longer present. We describe variants of $\text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter models improves generalization over its vanilla counterpart with up to 4$\%$ macro-average improvement across math benchmarks. We include careful ablations to analyze $\text{Guide}$'s components and theoretically analyze Guide's learning efficiency.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience</title>
<link>https://arxiv.org/abs/2506.13971</link>
<guid>https://arxiv.org/abs/2506.13971</guid>
<content:encoded><![CDATA[
arXiv:2506.13971v1 Announce Type: cross 
Abstract: Group conversations over videoconferencing are a complex social behavior. However, the subjective moments of negative experience, where the conversation loses fluidity or enjoyment remain understudied. These moments are infrequent in naturalistic data, and thus training a supervised learning (SL) model requires costly manual data annotation. We applied semi-supervised learning (SSL) to leverage targeted labeled and unlabeled clips for training multimodal (audio, facial, text) deep features to predict non-fluid or unenjoyable moments in holdout videoconference sessions. The modality-fused co-training SSL achieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by up to 4% with the same amount of labeled data. Remarkably, the best SSL model with just 8% labeled data matched 96% of the SL model's full-data performance. This shows an annotation-efficient framework for modeling videoconference experience.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios</title>
<link>https://arxiv.org/abs/2506.13977</link>
<guid>https://arxiv.org/abs/2506.13977</guid>
<content:encoded><![CDATA[
arXiv:2506.13977v1 Announce Type: cross 
Abstract: The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range of tasks. However, as the tasks become more complex and long-horizon, the intricate tool utilization process may trigger various unexpected errors. Therefore, how to effectively handle such errors, including identifying, diagnosing, and recovering from them, has emerged as a key research direction for advancing tool learning. In this work, we first extensively analyze the types of errors encountered during the function-calling process on several competitive tool evaluation benchmarks. Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation benchmark specialized for tool learning. Building upon a novel evolutionary strategy for dataset construction, CRITICTOOL holds diverse tool-use errors with varying complexities, which better reflects real-world scenarios. We conduct extensive experiments on CRITICTOOL, and validate the generalization and effectiveness of our constructed benchmark strategy. We also provide an in-depth analysis of the tool reflection ability on various LLMs, offering a new perspective on the field of tool learning in LLMs. The code is available at \href{https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science</title>
<link>https://arxiv.org/abs/2506.13992</link>
<guid>https://arxiv.org/abs/2506.13992</guid>
<content:encoded><![CDATA[
arXiv:2506.13992v1 Announce Type: cross 
Abstract: Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking</title>
<link>https://arxiv.org/abs/2506.14086</link>
<guid>https://arxiv.org/abs/2506.14086</guid>
<content:encoded><![CDATA[
arXiv:2506.14086v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated significant strides across various information retrieval tasks, particularly as rerankers, owing to their strong generalization and knowledge-transfer capabilities acquired from extensive pretraining. In parallel, the rise of LLM-based chat interfaces has raised user expectations, encouraging users to pose more complex queries that necessitate retrieval by ``reasoning'' over documents rather than through simple keyword matching or semantic similarity. While some recent efforts have exploited reasoning abilities of LLMs for reranking such queries, considerable potential for improvement remains. In that regards, we introduce InsertRank, an LLM-based reranker that leverages lexical signals like BM25 scores during reranking to further improve retrieval performance. InsertRank demonstrates improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning 12 diverse domains, and R2MED, a specialized medical reasoning retrieval benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and several ablation studies and demonstrate that InsertRank consistently improves retrieval effectiveness across multiple families of LLMs, including GPT, Gemini, and Deepseek models. %In addition, we also conduct ablation studies on normalization by varying the scale of the BM25 scores, and positional bias by shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark, surpassing previous methods.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Innovating China's Intangible Cultural Heritage with DeepSeek + MidJourney: The Case of Yangliuqing theme Woodblock Prints</title>
<link>https://arxiv.org/abs/2506.14104</link>
<guid>https://arxiv.org/abs/2506.14104</guid>
<content:encoded><![CDATA[
arXiv:2506.14104v1 Announce Type: cross 
Abstract: Yangliuqing woodblock prints, a cornerstone of China's intangible cultural heritage, are celebrated for their intricate designs and vibrant colors. However, preserving these traditional art forms while fostering innovation presents significant challenges. This study explores the DeepSeek + MidJourney approach to generating creative, themed Yangliuqing woodblock prints focused on the fight against COVID-19 and depicting joyous winners. Using Fr\'echet Inception Distance (FID) scores for evaluation, the method that combined DeepSeek-generated thematic prompts, MidJourney-generated thematic images, original Yangliuqing prints, and DeepSeek-generated key prompts in MidJourney-generated outputs achieved the lowest mean FID score (150.2) with minimal variability ({\sigma} = 4.9). Additionally, feedback from 62 participants, collected via questionnaires, confirmed that this hybrid approach produced the most representative results. Moreover, the questionnaire data revealed that participants demonstrated the highest willingness to promote traditional culture and the strongest interest in consuming the AI-generated images produced through this method. These findings underscore the effectiveness of an innovative approach that seamlessly blends traditional artistic elements with modern AI-driven creativity, ensuring both cultural preservation and contemporary relevance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadFabric: Agentic AI System with Reasoning Capability for Radiology</title>
<link>https://arxiv.org/abs/2506.14142</link>
<guid>https://arxiv.org/abs/2506.14142</guid>
<content:encoded><![CDATA[
arXiv:2506.14142v1 Announce Type: cross 
Abstract: Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Acoustic scattering AI for non-invasive object classifications: A case study on hair assessment</title>
<link>https://arxiv.org/abs/2506.14148</link>
<guid>https://arxiv.org/abs/2506.14148</guid>
<content:encoded><![CDATA[
arXiv:2506.14148v1 Announce Type: cross 
Abstract: This paper presents a novel non-invasive object classification approach using acoustic scattering, demonstrated through a case study on hair assessment. When an incident wave interacts with an object, it generates a scattered acoustic field encoding structural and material properties. By emitting acoustic stimuli and capturing the scattered signals from head-with-hair-sample objects, we classify hair type and moisture using AI-driven, deep-learning-based sound classification. We benchmark comprehensive methods, including (i) fully supervised deep learning, (ii) embedding-based classification, (iii) supervised foundation model fine-tuning, and (iv) self-supervised model fine-tuning. Our best strategy achieves nearly 90% classification accuracy by fine-tuning all parameters of a self-supervised model. These results highlight acoustic scattering as a privacy-preserving, non-contact alternative to visual classification, opening huge potential for applications in various industries.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models</title>
<link>https://arxiv.org/abs/2506.14153</link>
<guid>https://arxiv.org/abs/2506.14153</guid>
<content:encoded><![CDATA[
arXiv:2506.14153v1 Announce Type: cross 
Abstract: Recent advancements in speech synthesis technologies have led to increasingly advanced spoofing attacks, posing significant challenges for automatic speaker verification systems. While systems based on self-supervised learning (SSL) models, particularly the XLSR-Conformer model, have demonstrated remarkable performance in synthetic speech detection, there remains room for architectural improvements. In this paper, we propose a novel approach that replaces the traditional Multi-Layer Perceptron in the XLSR-Conformer model with a Kolmogorov-Arnold Network (KAN), a novel architecture based on the Kolmogorov-Arnold representation theorem. Our results on ASVspoof2021 demonstrate that integrating KAN into the SSL-based models can improve the performance by 60.55% relatively on LA and DF sets, further achieving 0.70% EER on the 21LA set. These findings suggest that incorporating KAN into SSL-based models is a promising direction for advances in synthetic speech detection.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Practical Aspects of End-to-End Multi-Talker Speech Recognition for Online and Offline Scenarios</title>
<link>https://arxiv.org/abs/2506.14204</link>
<guid>https://arxiv.org/abs/2506.14204</guid>
<content:encoded><![CDATA[
arXiv:2506.14204v1 Announce Type: cross 
Abstract: We extend the frameworks of Serialized Output Training (SOT) to address practical needs of both streaming and offline automatic speech recognition (ASR) applications. Our approach focuses on balancing latency and accuracy, catering to real-time captioning and summarization requirements. We propose several key improvements: (1) Leveraging Continuous Speech Separation (CSS) single-channel front-end with end-to-end (E2E) systems for highly overlapping scenarios, challenging the conventional wisdom of E2E versus cascaded setups. The CSS framework improves the accuracy of the ASR system by separating overlapped speech from multiple speakers. (2) Implementing dual models -- Conformer Transducer for streaming and Sequence-to-Sequence for offline -- or alternatively, a two-pass model based on cascaded encoders. (3) Exploring segment-based SOT (segSOT) which is better suited for offline scenarios while also enhancing readability of multi-talker transcriptions.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature Transcription</title>
<link>https://arxiv.org/abs/2506.14223</link>
<guid>https://arxiv.org/abs/2506.14223</guid>
<content:encoded><![CDATA[
arXiv:2506.14223v1 Announce Type: cross 
Abstract: Music transcription plays a pivotal role in Music Information Retrieval (MIR), particularly for stringed instruments like the guitar, where symbolic music notations such as MIDI lack crucial playability information. This contribution introduces the Fretting-Transformer, an encoderdecoder model that utilizes a T5 transformer architecture to automate the transcription of MIDI sequences into guitar tablature. By framing the task as a symbolic translation problem, the model addresses key challenges, including string-fret ambiguity and physical playability. The proposed system leverages diverse datasets, including DadaGP, GuitarToday, and Leduc, with novel data pre-processing and tokenization strategies. We have developed metrics for tablature accuracy and playability to quantitatively evaluate the performance. The experimental results demonstrate that the Fretting-Transformer surpasses baseline methods like A* and commercial applications like Guitar Pro. The integration of context-sensitive processing and tuning/capo conditioning further enhances the model's performance, laying a robust foundation for future developments in automated guitar transcription.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs</title>
<link>https://arxiv.org/abs/2506.14245</link>
<guid>https://arxiv.org/abs/2506.14245</guid>
<content:encoded><![CDATA[
arXiv:2506.14245v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the $Pass@K$ metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the $Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, $CoT$-$Pass@K$, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we observe that RLVR can incentivize the generalization of correct reasoning for all values of $K$. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LoRA with Variational Learning</title>
<link>https://arxiv.org/abs/2506.14280</link>
<guid>https://arxiv.org/abs/2506.14280</guid>
<content:encoded><![CDATA[
arXiv:2506.14280v1 Announce Type: cross 
Abstract: Bayesian methods have recently been used to improve LoRA finetuning and, although they improve calibration, their effect on other metrics (such as accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian methods also increase computational overheads and require additional tricks for them to work well. Here, we fix these issues by using a recently proposed variational algorithm called IVON. We show that IVON is easy to implement and has similar costs to AdamW, and yet it can also drastically improve many metrics by using a simple posterior pruning technique. We present extensive results on billion-scale LLMs (Llama and Qwen series) going way beyond the scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B model on a set of commonsense reasoning tasks and improve accuracy over AdamW by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian methods like Laplace-LoRA and BLoB. Overall, our results show that variational learning with IVON can effectively improve LoRA finetuning.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2506.14574</link>
<guid>https://arxiv.org/abs/2506.14574</guid>
<content:encoded><![CDATA[
arXiv:2506.14574v1 Announce Type: cross 
Abstract: Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sequence-level bandit problem. To address this challenge, this work decomposes the sequence-level PPO into a sequence of token-level proximal policy optimization problems and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work establishes a framework of computable loss functions with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at https://github.com/dvlab-research/TGDPO.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Studies in Influencer Marketing: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2506.14602</link>
<guid>https://arxiv.org/abs/2506.14602</guid>
<content:encoded><![CDATA[
arXiv:2506.14602v1 Announce Type: cross 
Abstract: Influencer marketing has become a crucial feature of digital marketing strategies. Despite its rapid growth and algorithmic relevance, the field of computational studies in influencer marketing remains fragmented, especially with limited systematic reviews covering the computational methodologies employed. This makes overarching scientific measurements in the influencer economy very scarce, to the detriment of interested stakeholders outside of platforms themselves, such as regulators, but also researchers from other fields. This paper aims to provide an overview of the state of the art of computational studies in influencer marketing by conducting a systematic literature review (SLR) based on the PRISMA model. The paper analyses 69 studies to identify key research themes, methodologies, and future directions in this research field. The review identifies four major research themes: Influencer identification and characterisation, Advertising strategies and engagement, Sponsored content analysis and discovery, and Fairness. Methodologically, the studies are categorised into machine learning-based techniques (e.g., classification, clustering) and non-machine-learning-based techniques (e.g., statistical analysis, network analysis). Key findings reveal a strong focus on optimising commercial outcomes, with limited attention to regulatory compliance and ethical considerations. The review highlights the need for more nuanced computational research that incorporates contextual factors such as language, platform, and industry type, as well as improved model explainability and dataset reproducibility. The paper concludes by proposing a multidisciplinary research agenda that emphasises the need for further links to regulation and compliance technology, finer granularity in analysis, and the development of standardised datasets.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning</title>
<link>https://arxiv.org/abs/2506.14629</link>
<guid>https://arxiv.org/abs/2506.14629</guid>
<content:encoded><![CDATA[
arXiv:2506.14629v1 Announce Type: cross 
Abstract: Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme "Prevention is Better than Cure", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Length Compression in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2506.14755</link>
<guid>https://arxiv.org/abs/2506.14755</guid>
<content:encoded><![CDATA[
arXiv:2506.14755v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as "invalid thinking" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM</title>
<link>https://arxiv.org/abs/2506.14766</link>
<guid>https://arxiv.org/abs/2506.14766</guid>
<content:encoded><![CDATA[
arXiv:2506.14766v1 Announce Type: cross 
Abstract: Multimodal Large Language Model (MLLM) often suffer from hallucinations. They over-rely on partial cues and generate incorrect responses. Recently, methods like Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding (ICD) have been proposed to mitigate hallucinations by contrasting predictions from perturbed or negatively prefixed inputs against original outputs. In this work, we uncover that methods like VCD and ICD fundamentally influence internal attention dynamics of the model. This observation suggests that their effectiveness may not stem merely from surface-level modifications to logits but from deeper shifts in attention distribution. Inspired by this insight, we propose an attention-steerable contrastive decoding framework that directly intervenes in attention mechanisms of the model to offer a more principled approach to mitigating hallucinations. Our experiments across multiple MLLM architectures and diverse decoding methods demonstrate that our approach significantly reduces hallucinations and improves the performance on benchmarks such as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing performance on standard VQA benchmarks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compression of enumerations and gain</title>
<link>https://arxiv.org/abs/2304.03030</link>
<guid>https://arxiv.org/abs/2304.03030</guid>
<content:encoded><![CDATA[
arXiv:2304.03030v2 Announce Type: replace 
Abstract: We study the compressibility of enumerations in the context of Kolmogorov complexity, focusing on strong and weak forms of compression and their gain: the amount of auxiliary information embedded in the compressed enumeration. The existence of strong compression and weak gainless compression is shown for any computably enumerable (c.e.) set. The density problem of c.e. sets with respect to their prefix complexity is reduced to the question of whether every c.e. set is well-compressible, which we study via enumeration games.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback</title>
<link>https://arxiv.org/abs/2307.10867</link>
<guid>https://arxiv.org/abs/2307.10867</guid>
<content:encoded><![CDATA[
arXiv:2307.10867v2 Announce Type: replace 
Abstract: Captions are crucial for understanding scientific visualizations and documents. Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences. To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences. Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences. We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of models. In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively. Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring news intent and its application: A theory-driven approach</title>
<link>https://arxiv.org/abs/2312.16490</link>
<guid>https://arxiv.org/abs/2312.16490</guid>
<content:encoded><![CDATA[
arXiv:2312.16490v2 Announce Type: replace 
Abstract: Understanding the intent behind information is crucial. However, news as a medium of public discourse still lacks a structured investigation of perceived news intent and its application. To advance this field, this paper reviews interdisciplinary studies on intentional action and introduces a conceptual deconstruction-based news intent understanding framework (NINT). This framework identifies the components of intent, facilitating a structured representation of news intent and its applications. Building upon NINT, we contribute a new intent perception dataset. Moreover, we investigate the potential of intent assistance on news-related tasks, such as significant improvement (+2.2% macF1) in the task of fake news detection. We hope that our findings will provide valuable insights into action-based intent cognition and computational social science.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Reasoning Capabilities of LLMs in the context of Evidence-based Claim Verification</title>
<link>https://arxiv.org/abs/2402.10735</link>
<guid>https://arxiv.org/abs/2402.10735</guid>
<content:encoded><![CDATA[
arXiv:2402.10735v4 Announce Type: replace 
Abstract: Although LLMs have shown great performance on Mathematics and Coding related reasoning tasks, the reasoning capabilities of LLMs regarding other forms of reasoning are still an open problem. Here, we examine the issue of reasoning from the perspective of claim verification. We propose a framework designed to break down any claim paired with evidence into atomic reasoning types that are necessary for verification. We use this framework to create RECV, the first claim verification benchmark, incorporating real-world claims, to assess the deductive and abductive reasoning capabilities of LLMs. The benchmark comprises of three datasets, covering reasoning problems of increasing complexity. We evaluate three state-of-the-art proprietary LLMs under multiple prompt settings. Our results show that while LLMs can address deductive reasoning problems, they consistently fail in cases of abductive reasoning. Moreover, we observe that enhancing LLMs with rationale generation is not always beneficial. Nonetheless, we find that generated rationales are semantically similar to those provided by humans, especially in deductive reasoning cases.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models to Measure Gender Representation Bias in Gendered Language Corpora</title>
<link>https://arxiv.org/abs/2406.13677</link>
<guid>https://arxiv.org/abs/2406.13677</guid>
<content:encoded><![CDATA[
arXiv:2406.13677v3 Announce Type: replace 
Abstract: Large language models (LLMs) often inherit and amplify social biases embedded in their training data. A prominent social bias is gender bias. In this regard, prior work has mainly focused on gender stereotyping bias - the association of specific roles or traits with a particular gender - in English and on evaluating gender bias in model embeddings or generated outputs. In contrast, gender representation bias - the unequal frequency of references to individuals of different genders - in the training corpora has received less attention. Yet such imbalances in the training data constitute an upstream source of bias that can propagate and intensify throughout the entire model lifecycle. To fill this gap, we propose a novel LLM-based method to detect and quantify gender representation bias in LLM training data in gendered languages, where grammatical gender challenges the applicability of methods developed for English. By leveraging the LLMs' contextual understanding, our approach automatically identifies and classifies person-referencing words in gendered language corpora. Applied to four Spanish-English benchmarks and five Valencian corpora, our method reveals substantial male-dominant imbalances. We show that such biases in training data affect model outputs, but can surprisingly be mitigated leveraging small-scale training on datasets that are biased towards the opposite gender. Our findings highlight the need for corpus-level gender bias analysis in multilingual NLP. We make our code and data publicly available.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the Age of Large Language Models</title>
<link>https://arxiv.org/abs/2407.07313</link>
<guid>https://arxiv.org/abs/2407.07313</guid>
<content:encoded><![CDATA[
arXiv:2407.07313v4 Announce Type: replace 
Abstract: The task of Text-to-SQL enables anyone to retrieve information from SQL databases using natural language. While this task has made substantial progress, the two primary evaluation metrics - Execution Accuracy (EXE) and Exact Set Matching Accuracy (ESM) - suffer from inherent limitations that can misrepresent performance. Specifically, ESM's rigid matching overlooks semantically correct but stylistically different queries, whereas EXE can overestimate correctness by ignoring structural errors that yield correct outputs. These shortcomings become especially problematic when assessing outputs from large language model (LLM)-based approaches without fine-tuning, which vary more in style and structure compared to their fine-tuned counterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM), which mitigates these issues by comparing queries using both syntactic and semantic elements. Through evaluating nine LLM-based models, we show that EXE and ESM can produce false positive and negative rates as high as 23.0% and 28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release our ETM script as open source, offering the community a more robust and reliable approach to evaluating Text-to-SQL.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Signatures of Compositionality Across a Language Model's Lifetime</title>
<link>https://arxiv.org/abs/2410.01444</link>
<guid>https://arxiv.org/abs/2410.01444</guid>
<content:encoded><![CDATA[
arXiv:2410.01444v5 Announce Type: replace 
Abstract: By virtue of linguistic compositionality, few syntactic rules and a finite lexicon can generate an unbounded number of sentences. That is, language, though seemingly high-dimensional, can be explained using relatively few degrees of freedom. An open question is whether contemporary language models (LMs) reflect the intrinsic simplicity of language that is enabled by compositionality. We take a geometric view of this problem by relating the degree of compositionality in a dataset to the intrinsic dimension (ID) of its representations under an LM, a measure of feature complexity. We find not only that the degree of dataset compositionality is reflected in representations' ID, but that the relationship between compositionality and geometric complexity arises due to learned linguistic features over training. Finally, our analyses reveal a striking contrast between nonlinear and linear dimensionality, showing they respectively encode semantic and superficial aspects of linguistic composition.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Overfitting in Large Language Model Editing</title>
<link>https://arxiv.org/abs/2410.07819</link>
<guid>https://arxiv.org/abs/2410.07819</guid>
<content:encoded><![CDATA[
arXiv:2410.07819v2 Announce Type: replace 
Abstract: Knowledge editing has been proposed as an effective method for updating and correcting the internal knowledge of Large Language Models (LLMs). However, existing editing methods often struggle with complex tasks, such as multi-hop reasoning. In this paper, we identify and investigate the phenomenon of Editing Overfit, where edited models assign disproportionately high probabilities to the edit target, hindering the generalization of new knowledge in complex scenarios. We attribute this issue to the current editing paradigm, which places excessive emphasis on the direct correspondence between the input prompt and the edit target for each edit sample. To further explore this issue, we introduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge Editing), along with fine-grained evaluation metrics. Through comprehensive experiments and analysis, we demonstrate that Editing Overfit is prevalent in current editing methods and that common overfitting mitigation strategies are ineffective in knowledge editing. To overcome this, inspired by LLMs' knowledge recall mechanisms, we propose a new plug-and-play strategy called Learn the Inference (LTI), which introduce a Multi-stage Inference Constraint module to guide the edited models in recalling new knowledge similarly to how unedited LLMs leverage knowledge through in-context learning. Extensive experimental results across a wide range of tasks validate the effectiveness of LTI in mitigating Editing Overfit.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Browsing: API-Based Web Agents</title>
<link>https://arxiv.org/abs/2410.16464</link>
<guid>https://arxiv.org/abs/2410.16464</guid>
<content:encoded><![CDATA[
arXiv:2410.16464v3 Announce Type: replace 
Abstract: Web browsers are a portal to the internet, where much of human activity is undertaken. Thus, there has been significant research work in AI agents that interact with the internet through web browsing. However, there is also another interface designed specifically for machine interaction with online content: application programming interfaces (APIs). In this paper we ask -- what if we were to take tasks traditionally tackled by Browsing Agents, and give AI agents access to APIs? To do so, we propose two varieties of agents: (1) an API-calling agent that attempts to perform online tasks through APIs only, similar to traditional coding agents, and (2) a Hybrid Agent that can interact with online data through both web browsing and APIs. In experiments on WebArena, a widely-used and realistic benchmark for web navigation tasks, we find that API-Based Agents outperform web Browsing Agents. Hybrid Agents out-perform both others nearly uniformly across tasks, resulting in a more than 24.0% absolute improvement over web browsing alone, achieving a success rate of 38.9%, the SOTA performance among task-agnostic agents. These results strongly suggest that when APIs are available, they present an attractive alternative to relying on web browsing alone.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Better Open-Ended Text Generation: A Multicriteria Evaluation Framework</title>
<link>https://arxiv.org/abs/2410.18653</link>
<guid>https://arxiv.org/abs/2410.18653</guid>
<content:encoded><![CDATA[
arXiv:2410.18653v3 Announce Type: replace 
Abstract: Open-ended text generation has become a prominent task in natural language processing due to the rise of powerful (large) language models. However, evaluating the quality of these models and the employed decoding strategies remains challenging due to trade-offs among widely used metrics such as coherence, diversity, and perplexity. This paper addresses the specific problem of multicriteria evaluation for open-ended text generation, proposing novel methods for both relative and absolute rankings of decoding methods. Specifically, we employ benchmarking approaches based on partial orderings and present a new summary metric to balance existing automatic indicators, providing a more holistic evaluation of text generation quality. Our experiments demonstrate that the proposed approaches offer a robust way to compare decoding strategies and serve as valuable tools to guide model selection for open-ended text generation tasks. We suggest future directions for improving evaluation methodologies in text generation and make our code, datasets, and models publicly available.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Watermarks for Large Language Models</title>
<link>https://arxiv.org/abs/2411.19563</link>
<guid>https://arxiv.org/abs/2411.19563</guid>
<content:encoded><![CDATA[
arXiv:2411.19563v2 Announce Type: replace 
Abstract: As large language models (LLMs) reach human-like fluency, reliably distinguishing AI-generated text from human authorship becomes increasingly difficult. While watermarks already exist for LLMs, they often lack flexibility and struggle with attacks such as paraphrasing. To address these issues, we propose a multi-feature method for generating watermarks that combines multiple distinct watermark features into an ensemble watermark. Concretely, we combine acrostica and sensorimotor norms with the established red-green watermark to achieve a 98% detection rate. After a paraphrasing attack, the performance remains high with 95% detection rate. In comparison, the red-green feature alone as a baseline achieves a detection rate of 49% after paraphrasing. The evaluation of all feature combinations reveals that the ensemble of all three consistently has the highest detection rate across several LLMs and watermark strength settings. Due to the flexibility of combining features in the ensemble, various requirements and trade-offs can be addressed. Additionally, the same detection function can be used without adaptations for all ensemble configurations. This method is particularly of interest to facilitate accountability and prevent societal harm.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for Varieties of English</title>
<link>https://arxiv.org/abs/2412.04726</link>
<guid>https://arxiv.org/abs/2412.04726</guid>
<content:encoded><![CDATA[
arXiv:2412.04726v3 Announce Type: replace 
Abstract: Despite large language models (LLMs) being known to exhibit bias against non-standard language varieties, there are no known labelled datasets for sentiment analysis of English. To address this gap, we introduce BESSTIE, a benchmark for sentiment and sarcasm classification for three varieties of English: Australian (en-AU), Indian (en-IN), and British (en-UK). We collect datasets for these language varieties using two methods: location-based for Google Places reviews, and topic-based filtering for Reddit comments. To assess whether the dataset accurately represents these varieties, we conduct two validation steps: (a) manual annotation of language varieties and (b) automatic language variety prediction. Native speakers of the language varieties manually annotate the datasets with sentiment and sarcasm labels. We perform an additional annotation exercise to validate the reliance of the annotated labels. Subsequently, we fine-tune nine LLMs (representing a range of encoder/decoder and mono/multilingual models) on these datasets, and evaluate their performance on the two tasks. Our results show that the models consistently perform better on inner-circle varieties (i.e., en-AU and en-UK), in comparison with en-IN, particularly for sarcasm classification. We also report challenges in cross-variety generalisation, highlighting the need for language variety-specific datasets such as ours. BESSTIE promises to be a useful evaluative benchmark for future research in equitable LLMs, specifically in terms of language varieties. The BESSTIE dataset is publicly available at: https://huggingface.co/ datasets/unswnlporg/BESSTIE.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression</title>
<link>https://arxiv.org/abs/2412.05693</link>
<guid>https://arxiv.org/abs/2412.05693</guid>
<content:encoded><![CDATA[
arXiv:2412.05693v2 Announce Type: replace 
Abstract: Several works have developed eviction policies to remove key-value (KV) pairs from the KV cache for more efficient inference. The focus has been on compressing the KV cache after the input prompt has been processed for faster token generation. In settings with limited GPU memory, and when the input context is longer than the generation length, we show that by also compressing the KV cache during the input processing phase, larger batch sizes can be used resulting in significantly higher throughput while still maintaining the original model's accuracy.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClusterChat: Multi-Feature Search for Corpus Exploration</title>
<link>https://arxiv.org/abs/2412.14533</link>
<guid>https://arxiv.org/abs/2412.14533</guid>
<content:encoded><![CDATA[
arXiv:2412.14533v2 Announce Type: replace 
Abstract: Exploring large-scale text corpora presents a significant challenge in biomedical, finance, and legal domains, where vast amounts of documents are continuously published. Traditional search methods, such as keyword-based search, often retrieve documents in isolation, limiting the user's ability to easily inspect corpus-wide trends and relationships. We present ClusterChat (The demo video and source code are available at: https://github.com/achouhan93/ClusterChat), an open-source system for corpus exploration that integrates cluster-based organization of documents using textual embeddings with lexical and semantic search, timeline-driven exploration, and corpus and document-level question answering (QA) as multi-feature search capabilities. We validate the system with two case studies on a four million abstract PubMed dataset, demonstrating that ClusterChat enhances corpus exploration by delivering context-aware insights while maintaining scalability and responsiveness on large-scale document collections.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models</title>
<link>https://arxiv.org/abs/2501.05478</link>
<guid>https://arxiv.org/abs/2501.05478</guid>
<content:encoded><![CDATA[
arXiv:2501.05478v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) such as GPT-4, trained on huge amount of datasets spanning multiple domains, exhibit significant reasoning, understanding, and planning capabilities across various tasks. This study presents the first-ever work in Arabic language integration within the Vision-and-Language Navigation (VLN) domain in robotics, an area that has been notably underexplored in existing research. We perform a comprehensive evaluation of state-of-the-art multi-lingual Small Language Models (SLMs), including GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the Arabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure LLM-based instruction-following navigation agent, to assess the impact of language on navigation reasoning through zero-shot sequential action prediction using the R2R dataset. Through comprehensive experiments, we demonstrate that our framework is capable of high-level planning for navigation tasks when provided with instructions in both English and Arabic. However, certain models struggled with reasoning and planning in the Arabic language due to inherent limitations in their capabilities, sub-optimal performance, and parsing issues. These findings highlight the importance of enhancing planning and reasoning capabilities in language models for effective navigation, emphasizing this as a key area for further development while also unlocking the potential of Arabic-language models for impactful real-world applications.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs</title>
<link>https://arxiv.org/abs/2501.10970</link>
<guid>https://arxiv.org/abs/2501.10970</guid>
<content:encoded><![CDATA[
arXiv:2501.10970v3 Announce Type: replace 
Abstract: The "LLM-as-an-annotator" and "LLM-as-a-judge" paradigms employ Large Language Models (LLMs) as annotators, judges, and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure, the Alternative Annotator Test (alt-test), that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM annotators and judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-4o), outperforming the open-source LLMs we examine, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models</title>
<link>https://arxiv.org/abs/2502.11425</link>
<guid>https://arxiv.org/abs/2502.11425</guid>
<content:encoded><![CDATA[
arXiv:2502.11425v2 Announce Type: replace 
Abstract: Despite the advanced capabilities of large language models (LLMs), their temporal reasoning ability remains underdeveloped. Prior works have highlighted this limitation, particularly in maintaining temporal consistency when understanding events. For example, models often confuse mutually exclusive temporal relations like ``before'' and ``after'' between events and make inconsistent predictions. In this work, we tackle the issue of temporal inconsistency in LLMs by proposing a novel counterfactual prompting approach. Our method generates counterfactual questions and enforces collective constraints, enhancing the model's consistency. We evaluate our method on multiple datasets, demonstrating significant improvements in event ordering for explicit and implicit events and temporal commonsense understanding by effectively addressing temporal inconsistencies.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Geo-Culturally Grounded LLM Generations</title>
<link>https://arxiv.org/abs/2502.13497</link>
<guid>https://arxiv.org/abs/2502.13497</guid>
<content:encoded><![CDATA[
arXiv:2502.13497v3 Announce Type: replace 
Abstract: Generative large language models (LLMs) have demonstrated gaps in diverse cultural awareness across the globe. We investigate the effect of retrieval augmented generation and search-grounding techniques on LLMs' ability to display familiarity with various national cultures. Specifically, we compare the performance of standard LLMs, LLMs augmented with retrievals from a bespoke knowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a web search (i.e., search grounding) on multiple cultural awareness benchmarks. We find that search grounding significantly improves the LLM performance on multiple-choice benchmarks that test propositional knowledge (e.g., cultural norms, artifacts, and institutions), while KB grounding's effectiveness is limited by inadequate knowledge base coverage and a suboptimal retriever. However, search grounding also increases the risk of stereotypical judgments by language models and fails to improve evaluators' judgments of cultural familiarity in a human evaluation with adequate statistical power. These results highlight the distinction between propositional cultural knowledge and open-ended cultural fluency when it comes to evaluating LLMs' cultural awareness.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PredictaBoard: Benchmarking LLM Score Predictability</title>
<link>https://arxiv.org/abs/2502.14445</link>
<guid>https://arxiv.org/abs/2502.14445</guid>
<content:encoded><![CDATA[
arXiv:2502.14445v2 Announce Type: replace 
Abstract: Despite possessing impressive skills, Large Language Models (LLMs) often fail unpredictably, demonstrating inconsistent success in even basic common sense reasoning tasks. This unpredictability poses a significant challenge to ensuring their safe deployment, as identifying and operating within a reliable "safe zone" is essential for mitigating risks. To address this, we present PredictaBoard, a novel collaborative benchmarking framework designed to evaluate the ability of score predictors (referred to as assessors) to anticipate LLM errors on specific task instances (i.e., prompts) from existing datasets. PredictaBoard evaluates pairs of LLMs and assessors by considering the rejection rate at different tolerance errors. As such, PredictaBoard stimulates research into developing better assessors and making LLMs more predictable, not only with a higher average performance. We conduct illustrative experiments using baseline assessors and state-of-the-art LLMs. PredictaBoard highlights the critical need to evaluate predictability alongside performance, paving the way for safer AI systems where errors are not only minimised but also anticipated and effectively mitigated. Code for our benchmark can be found at https://github.com/Kinds-of-Intelligence-CFI/PredictaBoard
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2502.15910</link>
<guid>https://arxiv.org/abs/2502.15910</guid>
<content:encoded><![CDATA[
arXiv:2502.15910v2 Announce Type: replace 
Abstract: Generative models such as Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) trained on massive datasets can lead them to memorize and inadvertently reveal sensitive information, raising ethical and privacy concerns. While some prior works have explored this issue in the context of LLMs, it presents a unique challenge for MLLMs due to the entangled nature of knowledge across modalities, making comprehensive unlearning more difficult. To address this challenge, we propose Modality Aware Neuron Unlearning (MANU), a novel unlearning framework for MLLMs designed to selectively clip neurons based on their relative importance to the targeted forget data, curated for different modalities. Specifically, MANU consists of two stages: important neuron selection and selective pruning. The first stage identifies and collects the most influential neurons across modalities relative to the targeted forget knowledge, while the second stage is dedicated to pruning those selected neurons. MANU effectively isolates and removes the neurons that contribute most to the forget data within each modality, while preserving the integrity of retained knowledge. Our experiments conducted across various MLLM architectures illustrate that MANU can achieve a more balanced and comprehensive unlearning in each modality without largely affecting the overall model utility.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongSpec: Long-Context Lossless Speculative Decoding with Efficient Drafting and Verification</title>
<link>https://arxiv.org/abs/2502.17421</link>
<guid>https://arxiv.org/abs/2502.17421</guid>
<content:encoded><![CDATA[
arXiv:2502.17421v2 Announce Type: replace 
Abstract: As Large Language Models (LLMs) can now process extremely long contexts, efficient inference over these extended inputs has become increasingly important, especially for emerging applications like LLM agents that highly depend on this capability. Speculative decoding (SD) offers a promising lossless acceleration technique compared to lossy alternatives such as quantization and model cascades. However, most state-of-the-art SD methods are trained on short texts (typically fewer than 4k tokens), making them unsuitable for long-context scenarios. Specifically, adapting these methods to long contexts presents three key challenges: (1) the excessive memory demands posed by draft models due to large Key-Value (KV) cache; (2) performance degradation resulting from the mismatch between short-context training and long-context inference; and (3) inefficiencies in tree attention mechanisms when managing long token sequences. This work introduces LongSpec, a framework that addresses these challenges through three core innovations: a memory-efficient draft model with a constant-sized KV cache; novel position indices that mitigate the training-inference mismatch; and an attention aggregation strategy that combines fast prefix computation with standard tree attention to enable efficient decoding. Experimental results confirm the effectiveness of LongSpec, achieving up to a 3.26x speedup over strong Flash Attention baselines across five long-context understanding datasets, as well as a 2.25x reduction in wall-clock time on the AIME24 long reasoning task with the QwQ model, demonstrating significant latency improvements for long-context applications. The code is available at https://github.com/sail-sg/LongSpec.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning</title>
<link>https://arxiv.org/abs/2502.20620</link>
<guid>https://arxiv.org/abs/2502.20620</guid>
<content:encoded><![CDATA[
arXiv:2502.20620v2 Announce Type: replace 
Abstract: Large language models (LLMs) can exhibit advanced reasoning yet still generate incorrect answers. We hypothesize that such errors frequently stem from spurious beliefs, propositions the model internally considers true but are incorrect. To address this, we propose a method to rectify the belief space by suppressing these spurious beliefs while simultaneously enhancing true ones, thereby enabling more reliable inferences. Our approach first identifies the beliefs that lead to incorrect or correct answers by prompting the model to generate textual explanations, using our Forward-Backward Beam Search (FBBS). We then apply unlearning to suppress the identified spurious beliefs and enhance the true ones, effectively rectifying the model's belief space. Empirical results on multiple QA datasets and LLMs show that our method corrects previously misanswered questions without harming overall model performance. Furthermore, our approach yields improved generalization on unseen data, suggesting that rectifying a model's belief space is a promising direction for mitigating errors and enhancing overall reliability.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming User Sentiment Modeling</title>
<link>https://arxiv.org/abs/2503.04619</link>
<guid>https://arxiv.org/abs/2503.04619</guid>
<content:encoded><![CDATA[
arXiv:2503.04619v2 Announce Type: replace 
Abstract: User reviews on e-commerce platforms exhibit dynamic sentiment patterns driven by temporal and contextual factors. Traditional sentiment analysis methods focus on static reviews, failing to capture the evolving temporal relationship between user sentiment rating and textual content. Sentiment analysis on streaming reviews addresses this limitation by modeling and predicting the temporal evolution of user sentiments. However, it suffers from data sparsity, manifesting in temporal, spatial, and combined forms. In this paper, we introduce SynGraph, a novel framework designed to address data sparsity in sentiment analysis on streaming reviews. SynGraph alleviates data sparsity by categorizing users into mid-tail, long-tail, and extreme scenarios and incorporating LLM-augmented enhancements within a dynamic graph-based structure. Experiments on real-world datasets demonstrate its effectiveness in addressing sparsity and improving sentiment modeling in streaming reviews.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effect of Selection Format on LLM Performance</title>
<link>https://arxiv.org/abs/2503.06926</link>
<guid>https://arxiv.org/abs/2503.06926</guid>
<content:encoded><![CDATA[
arXiv:2503.06926v2 Announce Type: replace 
Abstract: This paper investigates a critical aspect of large language model (LLM) performance: the optimal formatting of classification task options in prompts. Through an extensive experimental study, we compared two selection formats -- bullet points and plain English -- to determine their impact on model performance. Our findings suggest that presenting options via bullet points generally yields better results, although there are some exceptions. Furthermore, our research highlights the need for continued exploration of option formatting to drive further improvements in model performance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOPBench: Evaluating Language Agents at Following Standard Operating Procedures and Constraints</title>
<link>https://arxiv.org/abs/2503.08669</link>
<guid>https://arxiv.org/abs/2503.08669</guid>
<content:encoded><![CDATA[
arXiv:2503.08669v2 Announce Type: replace 
Abstract: As language agents increasingly automate critical tasks, their ability to follow domain-specific standard operating procedures (SOPs), policies, and constraints when taking actions and making tool calls becomes essential yet remains underexplored. To address this gap, we develop an automated evaluation pipeline SOPBench with: (1) executable environments containing 167 tools/functions across seven customer service domains with service-specific SOPs and rule-based verifiers, (2) an automated test generation framework producing over 900 verified test cases, and (3) an automated evaluation framework to rigorously assess agent adherence from multiple dimensions. Our approach transforms each service-specific SOP code program into a directed graph of executable functions and requires agents to call these functions based on natural language SOP descriptions. The original code serves as oracle rule-based verifiers to assess compliance, reducing reliance on manual annotations and LLM-based evaluations. We evaluate 18 leading models, and results show the task is challenging even for top-tier models (like GPT-4o, Claude-3.7-Sonnet), with variances across domains. Reasoning models like o4-mini-high show superiority while other powerful models perform less effectively (pass rates of 30%-50%), and small models (7B, 8B) perform significantly worse. Additionally, language agents can be easily jailbroken to overlook SOPs and constraints. Code, data, and over 24k agent trajectories are released at https://github.com/Leezekun/SOPBench.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Construction Distributions Shape Formal Language Learning In German BabyLMs?</title>
<link>https://arxiv.org/abs/2503.11593</link>
<guid>https://arxiv.org/abs/2503.11593</guid>
<content:encoded><![CDATA[
arXiv:2503.11593v2 Announce Type: replace 
Abstract: We analyze the influence of utterance-level construction distributions in German child-directed/child-available speech on the resulting word-level, syntactic and semantic competence (and their underlying learning trajectories) in small LMs, which we train on a novel collection of developmentally plausible language data for German. We find that trajectories are surprisingly robust for markedly different distributions of constructions in the training data, which have little effect on final accuracies and almost no effect on global learning trajectories. While syntax learning benefits from more complex utterances, word-level learning culminates in better scores with more fragmentary utterances. We argue that LMs trained on developmentally plausible data can contribute to debates on how conducive different kinds of linguistic stimuli are to language learning.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for Effective Elicitation and Retrieval of Information</title>
<link>https://arxiv.org/abs/2504.07738</link>
<guid>https://arxiv.org/abs/2504.07738</guid>
<content:encoded><![CDATA[
arXiv:2504.07738v2 Announce Type: replace 
Abstract: In this document, we discuss a multi-step approach to automated construction of a knowledge graph, for structuring and representing domain-specific knowledge from large document corpora. We apply our method to build the first knowledge graph of nuclear fusion energy, a highly specialized field characterized by vast scope and heterogeneity. This is an ideal benchmark to test the key features of our pipeline, including automatic named entity recognition and entity resolution. We show how pre-trained large language models can be used to address these challenges and we evaluate their performance against Zipf's law, which characterizes human-generated natural language. Additionally, we develop a knowledge-graph retrieval-augmented generation system that combines large language models with a multi-prompt approach. This system provides contextually relevant answers to natural-language queries, including complex multi-hop questions that require reasoning across interconnected entities.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPO: Cost-Aware Prompt Optimization</title>
<link>https://arxiv.org/abs/2504.16005</link>
<guid>https://arxiv.org/abs/2504.16005</guid>
<content:encoded><![CDATA[
arXiv:2504.16005v4 Announce Type: replace 
Abstract: Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automatic prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11/15 cases with improvements up to 21%p in accuracy. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph RAG for Legal Norms: A Hierarchical, Temporal and Deterministic Approach</title>
<link>https://arxiv.org/abs/2505.00039</link>
<guid>https://arxiv.org/abs/2505.00039</guid>
<content:encoded><![CDATA[
arXiv:2505.00039v3 Announce Type: replace 
Abstract: This article proposes an adaptation of Graph Retrieval-Augmented Generation (Graph RAG) specifically designed for the analysis and comprehension of legal norms. Legal texts are characterized by a predefined hierarchical structure, an extensive network of references and a continuous evolution through multiple temporal versions. This temporal dynamism poses a significant challenge for standard AI systems, demanding a deterministic representation of the law at any given point in time. To address this, our approach grounds the knowledge graph construction in a formal, FRBRoo-inspired model that distinguishes abstract legal works from their concrete textual expressions. We introduce a multi-layered representation of Temporal Versions (capturing date-specific changes) and Language Versions (capturing linguistic variations). By modeling normative evolution as a precise sequence of these versioned entities, we enable the construction of a knowledge graph that serves as a verifiable "ground truth". This allows Large Language Models to generate responses based on accurate, context-aware, and point-in-time correct legal information, overcoming the risk of temporal inaccuracies. Through a detailed analysis of this formal Graph RAG approach and its application to legal norm datasets, this article aims to advance the field of Artificial Intelligence applied to Law, creating opportunities for more effective and reliable systems in legal research, legislative analysis, and decision support.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongCodeBench: Evaluating Coding LLMs at 1M Context Windows</title>
<link>https://arxiv.org/abs/2505.07897</link>
<guid>https://arxiv.org/abs/2505.07897</guid>
<content:encoded><![CDATA[
arXiv:2505.07897v2 Announce Type: replace 
Abstract: Context lengths for models have grown rapidly, from thousands to millions of tokens in just a few years. The extreme context sizes of modern long-context models have made it difficult to construct realistic long-context benchmarks -- not only due to the cost of collecting million-context tasks but also in identifying realistic scenarios that require significant contexts. We identify code comprehension and repair as a natural testbed and challenge task for long-context models and introduce LongCodeBench (LCB), a benchmark to test LLM coding abilities in long-context scenarios. Our benchmark tests both the comprehension and repair capabilities of LCLMs in realistic and important settings by drawing from real-world GitHub issues and constructing QA (LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the complexity of our benchmark, enabling us to evaluate models across different scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model. We find that long-context remains a weakness for all models, with performance drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for Qwen2.5.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents</title>
<link>https://arxiv.org/abs/2505.11368</link>
<guid>https://arxiv.org/abs/2505.11368</guid>
<content:encoded><![CDATA[
arXiv:2505.11368v2 Announce Type: replace 
Abstract: Large language models (LLMs) have been widely deployed as autonomous agents capable of following user instructions and making decisions in real-world applications. Previous studies have made notable progress in benchmarking the instruction following capabilities of LLMs in general domains, with a primary focus on their inherent commonsense knowledge. Recently, LLMs have been increasingly deployed as domain-oriented agents, which rely on domain-oriented guidelines that may conflict with their commonsense knowledge. These guidelines exhibit two key characteristics: they consist of a wide range of domain-oriented rules and are subject to frequent updates. Despite these challenges, the absence of comprehensive benchmarks for evaluating the domain-oriented guideline following capabilities of LLMs presents a significant obstacle to their effective assessment and further development. In this paper, we introduce GuideBench, a comprehensive benchmark designed to evaluate guideline following performance of LLMs. GuideBench evaluates LLMs on three critical aspects: (i) adherence to diverse rules, (ii) robustness to rule updates, and (iii) alignment with human preferences. Experimental results on a range of LLMs indicate substantial opportunities for improving their ability to follow domain-oriented guidelines.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement</title>
<link>https://arxiv.org/abs/2505.12368</link>
<guid>https://arxiv.org/abs/2505.12368</guid>
<content:encoded><![CDATA[
arXiv:2505.12368v2 Announce Type: replace 
Abstract: Prompt injection remains a major security risk for large language models. However, the efficacy of existing guardrail models in context-aware settings remains underexplored, as they often rely on static attack benchmarks. Additionally, they have over-defense tendencies. We introduce CAPTURE, a novel context-aware benchmark assessing both attack detection and over-defense tendencies with minimal in-domain examples. Our experiments reveal that current prompt injection guardrail models suffer from high false negatives in adversarial cases and excessive false positives in benign scenarios, highlighting critical limitations. To demonstrate our framework's utility, we train CaptureGuard on our generated data. This new model drastically reduces both false negative and false positive rates on our context-aware datasets while also generalizing effectively to external benchmarks, establishing a path toward more robust and practical prompt injection defenses.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)</title>
<link>https://arxiv.org/abs/2505.17238</link>
<guid>https://arxiv.org/abs/2505.17238</guid>
<content:encoded><![CDATA[
arXiv:2505.17238v2 Announce Type: replace 
Abstract: Collaborative dialogue offers rich insights into students' learning and critical thinking, which is essential for personalizing pedagogical agent interactions in STEM+C settings. While large language models (LLMs) facilitate dynamic pedagogical interactions, hallucinations undermine confidence, trust, and instructional value. Retrieval-augmented generation (RAG) grounds LLM outputs in curated knowledge but requires a clear semantic link between user input and a knowledge base, which is often weak in student dialogue. We propose log-contextualized RAG (LC-RAG), which enhances RAG retrieval by using environment logs to contextualize collaborative discourse. Our findings show that LC-RAG improves retrieval over a discourse-only baseline and allows our collaborative peer agent, Copa, to deliver relevant, personalized guidance that supports students' critical thinking and epistemic decision-making in a collaborative computational modeling environment, C2STEM.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.20613</link>
<guid>https://arxiv.org/abs/2505.20613</guid>
<content:encoded><![CDATA[
arXiv:2505.20613v2 Announce Type: replace 
Abstract: Nowadays, formal theorem provers have made monumental progress on high-school and competition-level mathematics, but few of them generalize to more advanced mathematics. In this paper, we present REAL-Prover, a new open-source stepwise theorem prover for Lean 4 to push this boundary. This prover, based on our fine-tuned large language model (REAL-Prover-v1) and integrated with a retrieval system (Leansearch-PS), notably boosts performance on solving college-level mathematics problems. To train REAL-Prover-v1, we developed HERALD-AF, a data extraction pipeline that converts natural language math problems into formal statements, and a new open-source Lean 4 interactive environment (Jixia-interactive) to facilitate synthesis data collection. In our experiments, our prover using only supervised fine-tune achieves competitive results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable to state-of-the-art (SOTA) models. To further evaluate our approach, we introduce FATE-M, a new benchmark focused on algebraic problems, where our prover achieves a SOTA success rate of 56.7% (Pass@64).
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG</title>
<link>https://arxiv.org/abs/2506.00854</link>
<guid>https://arxiv.org/abs/2506.00854</guid>
<content:encoded><![CDATA[
arXiv:2506.00854v2 Announce Type: replace 
Abstract: We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one of the earliest open-vocabulary EEG-to-text generation frameworks tailored for Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact pretrained language model (MiniLM), our architecture aligns multichannel brain signals with natural language representations via masked pretraining and contrastive learning. Using a subset of the ChineseEEG dataset, where each sentence contains approximately ten Chinese characters aligned with 128-channel EEG recorded at 256 Hz, we segment EEG into per-character embeddings and predict full sentences in a zero-shot setting. The decoder is trained with teacher forcing and padding masks to accommodate variable-length sequences. Evaluation on over 1,500 training-validation sentences and 300 held-out test samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\%. While syntactic fluency remains a challenge, our findings demonstrate the feasibility of non-phonetic, cross-modal language decoding from EEG. This work opens a new direction in multilingual brain-to-text research and lays the foundation for future cognitive-language interfaces in Chinese.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation</title>
<link>https://arxiv.org/abs/2506.01565</link>
<guid>https://arxiv.org/abs/2506.01565</guid>
<content:encoded><![CDATA[
arXiv:2506.01565v2 Announce Type: replace 
Abstract: Culture is a rich and dynamic domain that evolves across both geography and time. However, existing studies on cultural understanding with vision-language models (VLMs) primarily emphasize geographic diversity, often overlooking the critical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a novel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning ancient Chinese dynasties, serves as a representative cultural heritage that reflects the profound temporal aspects of Chinese culture while remaining highly popular in Chinese contemporary society. Hanfu-Bench comprises two core tasks: cultural visual understanding and cultural image transcreation.The former task examines temporal-cultural feature recognition based on single- or multi-image inputs through multiple-choice visual question answering, while the latter focuses on transforming traditional attire into modern designs through cultural element inheritance and modern context adaptation. Our evaluation shows that closed VLMs perform comparably to non-experts on visual cutural understanding but fall short by 10\% to human experts, while open VLMs lags further behind non-experts. For the transcreation task, multi-faceted human evaluation indicates that the best-performing model achieves a success rate of only 42\%. Our benchmark provides an essential testbed, revealing significant challenges in this new direction of temporal cultural understanding and creative adaptation.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EuroLLM-9B: Technical Report</title>
<link>https://arxiv.org/abs/2506.04079</link>
<guid>https://arxiv.org/abs/2506.04079</guid>
<content:encoded><![CDATA[
arXiv:2506.04079v2 Announce Type: replace 
Abstract: This report presents EuroLLM-9B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-9B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. We describe the pre-training data collection and filtering pipeline, including the creation of EuroFilter, an AI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a novel synthetic dataset for post-training that enhances language coverage for European languages. Evaluation results demonstrate EuroLLM-9B's competitive performance on multilingual benchmarks and machine translation tasks, establishing it as the leading open European-made LLM of its size. To support open research and adoption, we release all major components of this work, including the base and instruction-tuned models, the EuroFilter classifier, and the synthetic post-training dataset.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification</title>
<link>https://arxiv.org/abs/2506.07801</link>
<guid>https://arxiv.org/abs/2506.07801</guid>
<content:encoded><![CDATA[
arXiv:2506.07801v2 Announce Type: replace 
Abstract: We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm combining the paradigms of co-training and consistency regularization with pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label weighting module designed for three key purposes: selecting and filtering pseudo-labels based on head agreement and model confidence, and weighting them according to the perceived classification difficulty. This novel module enhances and unifies three existing techniques -- heads agreement from Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average Pseudo-Margins from MarginMatch -- resulting in a holistic approach that improves robustness and performance in SSL settings. Experimental results on benchmark datasets highlight the superior performance of MultiMatch, achieving state-of-the-art results on 9 out of 10 setups from 5 natural language processing datasets and ranking first according to the Friedman test among 19 methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly imbalanced settings, outperforming the second-best approach by 3.26% -- and data imbalance is a key factor for many text classification tasks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature</title>
<link>https://arxiv.org/abs/2308.12420</link>
<guid>https://arxiv.org/abs/2308.12420</guid>
<content:encoded><![CDATA[
arXiv:2308.12420v4 Announce Type: replace-cross 
Abstract: Distributed Ledger Technology (DLT) faces increasing environmental scrutiny, particularly concerning the energy consumption of the Proof of Work (PoW) consensus mechanism and broader Environmental, Social, and Governance (ESG) issues. However, existing systematic literature reviews of DLT rely on limited analyses of citations, abstracts, and keywords, failing to fully capture the field's complexity and ESG concerns. We address these challenges by analyzing the full text of 24,539 publications using Natural Language Processing (NLP) with our manually labeled Named Entity Recognition (NER) dataset of 39,427 entities for DLT. This methodology identified 505 key publications at the DLT/ESG intersection, enabling comprehensive domain analysis. Our combined NLP and temporal graph analysis reveals critical trends in DLT evolution and ESG impacts, including cryptography and peer-to-peer networks research's foundational influence, Bitcoin's persistent impact on research and environmental concerns (a "Lindy effect"), Ethereum's catalytic role on Proof of Stake (PoS) and smart contract adoption, and the industry's progressive shift toward energy-efficient consensus mechanisms. Our contributions include the first DLT-specific NER dataset addressing the scarcity of high-quality labeled NLP data in blockchain research, a methodology integrating NLP and temporal graph analysis for large-scale interdisciplinary literature reviews, and the first NLP-driven literature review focusing on DLT's ESG aspects.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Social Media and Search Engines: Dredge Words and the Detection of Unreliable Domains</title>
<link>https://arxiv.org/abs/2406.11423</link>
<guid>https://arxiv.org/abs/2406.11423</guid>
<content:encoded><![CDATA[
arXiv:2406.11423v4 Announce Type: replace-cross 
Abstract: Proactive content moderation requires platforms to rapidly and continuously evaluate the credibility of websites. Leveraging the direct and indirect paths users follow to unreliable websites, we develop a website credibility classification and discovery system that integrates both webgraph and large-scale social media contexts. We additionally introduce the concept of dredge words, terms or phrases for which unreliable domains rank highly on search engines, and provide the first exploration of their usage on social media. Our graph neural networks that combine webgraph and social media contexts generate to state-of-the-art results in website credibility classification and significantly improves the top-k identification of unreliable domains. Additionally, we release a novel dataset of dredge words, highlighting their strong connections to both social media and online commerce platforms.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models Exhibit Cognitive Dissonance? Studying the Difference Between Revealed Beliefs and Stated Answers</title>
<link>https://arxiv.org/abs/2406.14986</link>
<guid>https://arxiv.org/abs/2406.14986</guid>
<content:encoded><![CDATA[
arXiv:2406.14986v3 Announce Type: replace-cross 
Abstract: Multiple Choice Questions (MCQ) have become a commonly used approach to assess the capabilities of Large Language Models (LLMs), due to their ease of manipulation and evaluation. The experimental appraisals of the LLMs' Stated Answer (their answer to MCQ) have pointed to their apparent ability to perform probabilistic reasoning or to grasp uncertainty. In this work, we investigate whether these aptitudes are measurable outside tailored prompting and MCQ by reformulating these issues as direct text-completion - the fundamental computational unit of LLMs. We introduce Revealed Belief, an evaluation framework that evaluates LLMs on tasks requiring reasoning under uncertainty, which complements MCQ scoring by analyzing text-completion probability distributions. Our findings suggest that while LLMs frequently state the correct answer, their Revealed Belief shows that they often allocate probability mass inconsistently, exhibit systematic biases, and often fail to update their beliefs appropriately when presented with new evidence, leading to strong potential impacts on downstream tasks. These results suggest that common evaluation methods may only provide a partial picture and that more research is needed to assess the extent and nature of their capabilities.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable and Reliable Knowledge-Intensive Task-Oriented Conversational Agents with Declarative Genie Worksheets</title>
<link>https://arxiv.org/abs/2407.05674</link>
<guid>https://arxiv.org/abs/2407.05674</guid>
<content:encoded><![CDATA[
arXiv:2407.05674v3 Announce Type: replace-cross 
Abstract: Large Language Models can carry out human-like conversations in diverse settings, responding to user requests for tasks and knowledge. However, existing conversational agents implemented with LLMs often struggle with hallucination, following instructions with conditional logic, and integrating knowledge from different sources. These shortcomings compromise the agents' effectiveness, rendering them unsuitable for deployment. To address these challenges, we introduce Genie, a programmable framework for creating knowledge-intensive task-oriented conversational agents. Genie can handle involved interactions and answer complex queries. Unlike LLMs, it delivers reliable, grounded responses through advanced dialogue state management and supports controllable agent policies via its declarative specification -- Genie Worksheet. This is achieved through an algorithmic runtime system that implements the developer-supplied policy, limiting LLMs to (1) parse user input using a succinct conversational history, and (2) generate responses according to supplied context. Agents built with Genie outperform SOTA methods on complex logic dialogue datasets. We conducted a user study with 62 participants on three real-life applications: restaurant reservations with Yelp, as well as ticket submission and course enrollment for university students. Genie agents with GPT-4 Turbo outperformed the GPT-4 Turbo agents with function calling, improving goal completion rates from 21.8% to 82.8% across three real-world tasks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents</title>
<link>https://arxiv.org/abs/2410.05243</link>
<guid>https://arxiv.org/abs/2410.05243</guid>
<content:encoded><![CDATA[
arXiv:2410.05243v3 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the robustness of their grounding capability. Current GUI agents predominantly utilize text-based representations such as HTML or accessibility trees, which, despite their utility, often introduce noise, incompleteness, and increased computational overhead. In this paper, we advocate a human-like embodiment for GUI agents that perceive the environment entirely visually and directly perform pixel-level operations on the GUI. The key is visual grounding models that can accurately map diverse referring expressions of GUI elements to their coordinates on the GUI across different platforms. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture, is surprisingly effective for training such visual grounding models. We collect the largest dataset for GUI visual grounding so far, containing 10M GUI elements and their referring expressions over 1.3M screenshots, and use it to train UGround, a strong universal visual grounding model for GUI agents. Empirical results on six benchmarks spanning three categories (grounding, offline agent, and online agent) show that 1) UGround substantially outperforms existing visual grounding models for GUI agents, by up to 20% absolute, and 2) agents with UGround outperform state-of-the-art agents, despite the fact that existing agents use additional text-based input while ours only uses visual perception. These results provide strong support for the feasibility and promises of GUI agents that navigate the digital world as humans do.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities</title>
<link>https://arxiv.org/abs/2412.06745</link>
<guid>https://arxiv.org/abs/2412.06745</guid>
<content:encoded><![CDATA[
arXiv:2412.06745v2 Announce Type: replace-cross 
Abstract: Traditional fixed test sets fall short in evaluating open-ended capabilities of foundation models. To address this, we propose ONEBench(OpeN-Ended Benchmarking), a new testing paradigm that consolidates individual evaluation datasets into a unified, ever-expanding sample pool. ONEBench allows users to generate custom, open-ended evaluation benchmarks from this pool, corresponding to specific capabilities of interest. By aggregating samples across test sets, ONEBench enables the assessment of diverse capabilities beyond those covered by the original test sets, while mitigating overfitting and dataset bias. Most importantly, it frames model evaluation as a collective process of selecting and aggregating sample-level tests.
  The shift from task-specific benchmarks to ONEBench introduces two challenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the aggregation over diverse metrics, while incompleteness describes comparing models evaluated on different data subsets. To address these challenges, we explore algorithms to aggregate sparse measurements into reliable model scores. Our aggregation algorithm ensures identifiability(asymptotically recovering ground-truth scores) and rapid convergence, enabling accurate model ranking with less data. On homogenous datasets, we show our aggregation algorithm provides rankings that highly correlate with those produced by average scores. We also demonstrate robustness to ~95% of measurements missing, reducing evaluation cost by up to 20x with little-to-no change in model rankings. We introduce ONEBench-LLM for language models and ONEBench-LMM for vision-language models, unifying evaluations across these domains. Overall, we present a technique for open-ended evaluation, which can aggregate over incomplete, heterogeneous sample-level measurements to continually grow a benchmark alongside the rapidly developing foundation models.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Laboratory: Using LLM Agents as Research Assistants</title>
<link>https://arxiv.org/abs/2501.04227</link>
<guid>https://arxiv.org/abs/2501.04227</guid>
<content:encoded><![CDATA[
arXiv:2501.04227v2 Announce Type: replace-cross 
Abstract: Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors</title>
<link>https://arxiv.org/abs/2501.18045</link>
<guid>https://arxiv.org/abs/2501.18045</guid>
<content:encoded><![CDATA[
arXiv:2501.18045v3 Announce Type: replace-cross 
Abstract: How has the public responded to the increasing prevalence of artificial intelligence (AI)-based technologies? We investigate public perceptions of AI by collecting over 12,000 responses over 12 months from a nationally representative U.S. sample. Participants provided open-ended metaphors reflecting their mental models of AI, a methodology that overcomes the limitations of traditional self-reported measures by capturing more nuance. Using a mixed-methods approach combining quantitative clustering and qualitative coding, we identify 20 dominant metaphors shaping public understanding of AI. To analyze these metaphors systematically, we present a scalable framework integrating language modeling (LM)-based techniques to measure key dimensions of public perception: anthropomorphism (attribution of human-like qualities), warmth, and competence. We find that Americans generally view AI as warm and competent, and that over the past year, perceptions of AI's human-likeness and warmth have significantly increased ($+34\%, r = 0.80, p < 0.01; +41\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the identified dominant metaphors, strongly predict trust in and willingness to adopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic demographic differences in metaphors and implicit perceptions, such as the higher propensity of women, older individuals, and people of color to anthropomorphize AI, which shed light on demographic disparities in trust and adoption. In addition to our dataset and framework for tracking evolving public attitudes, we provide actionable insights on using metaphors for inclusive and responsible AI development.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAE-V: Interpreting Multimodal Models for Enhanced Alignment</title>
<link>https://arxiv.org/abs/2502.17514</link>
<guid>https://arxiv.org/abs/2502.17514</guid>
<content:encoded><![CDATA[
arXiv:2502.17514v2 Announce Type: replace-cross 
Abstract: With the integration of image modality, the semantic space of multimodal large language models (MLLMs) is more complex than text-only models, making their interpretability more challenging and their alignment less stable, particularly susceptible to low-quality data, which can lead to inconsistencies between modalities, hallucinations, and biased outputs. As a result, developing interpretability methods for MLLMs is crucial for improving alignment quality and efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained attention for their ability to interpret latent representations. However, extending SAEs to multimodal settings presents new challenges due to modality fusion and the difficulty of isolating cross-modal representations. To address these challenges, we introduce SAE-V, a mechanistic interpretability framework that extends the SAE paradigm to MLLMs. By identifying and analyzing interpretable features along with their corresponding data, SAE-V enables fine-grained interpretation of both model behavior and data quality, facilitating a deeper understanding of cross-modal interactions and alignment dynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides an intrinsic data filtering mechanism to enhance model alignment without requiring additional models. Specifically, when applied to the alignment process of MLLMs, SAE-V-based data filtering methods could achieve more than 110% performance with less than 50% data. Our results highlight SAE-V's ability to enhance interpretability and alignment in MLLMs, providing insights into their internal mechanisms.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Shaping to Mitigate Reward Hacking in RLHF</title>
<link>https://arxiv.org/abs/2502.18770</link>
<guid>https://arxiv.org/abs/2502.18770</guid>
<content:encoded><![CDATA[
arXiv:2502.18770v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. We evaluated PAR on two base models, Gemma2-2B, and Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR, and the Work done during the internship at StepFun by Jiayi Fu.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OWLViz: An Open-World Benchmark for Visual Question Answering</title>
<link>https://arxiv.org/abs/2503.07631</link>
<guid>https://arxiv.org/abs/2503.07631</guid>
<content:encoded><![CDATA[
arXiv:2503.07631v2 Announce Type: replace-cross 
Abstract: We present a challenging benchmark for the Open WorLd VISual question answering (OWLViz) task. OWLViz presents concise, unambiguous queries that require integrating multiple capabilities, including visual understanding, web exploration, and specialized tool usage. While humans achieve 69.2% accuracy on these intuitive tasks, even state-of-the-art VLMs struggle, with the best model, Gemini 2.0, achieving only 26.6% accuracy. Current agentic VLMs, which rely on limited vision and vision-language models as tools, perform even worse. This performance gap reveals significant limitations in multimodal systems' ability to select appropriate tools and execute complex reasoning sequences, establishing new directions for advancing practical AI research.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Thought Reasoning In The Wild Is Not Always Faithful</title>
<link>https://arxiv.org/abs/2503.08679</link>
<guid>https://arxiv.org/abs/2503.08679</guid>
<content:encoded><![CDATA[
arXiv:2503.08679v4 Announce Type: replace-cross 
Abstract: Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful when models face an explicit bias in their prompts, i.e., the CoT can give an incorrect picture of how models arrive at conclusions. We go further and show that unfaithful CoT can also occur on realistic prompts with no artificial bias. We find that when separately presented with the questions "Is X bigger than Y?" and "Is Y bigger than X?", models sometimes produce superficially coherent arguments to justify systematically answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We show preliminary evidence that this is due to models' implicit biases towards Yes or No, thus labeling this unfaithfulness as Implicit Post-Hoc Rationalization. Our results reveal that several production models exhibit surprisingly high rates of post-hoc rationalization in our settings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more faithful, especially thinking ones, none are entirely faithful: Gemini 2.5 Flash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%), and Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical Shortcuts, where models use subtly illogical reasoning to try to make a speculative answer to hard maths problems seem rigorously proven. Our findings raise challenges for strategies for detecting undesired behavior in LLMs via the chain of thought.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks</title>
<link>https://arxiv.org/abs/2503.16974</link>
<guid>https://arxiv.org/abs/2503.16974</guid>
<content:encoded><![CDATA[
arXiv:2503.16974v3 Announce Type: replace-cross 
Abstract: This study provides the first comprehensive assessment of consistency and reproducibility in Large Language Model (LLM) outputs in finance and accounting research. We evaluate how consistently LLMs produce outputs given identical inputs through extensive experimentation with 50 independent runs across five common tasks: classification, sentiment analysis, summarization, text generation, and prediction. Using three OpenAI models (GPT-3.5-turbo, GPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse financial source texts and data, covering MD&amp;As, FOMC statements, finance news articles, earnings call transcripts, and financial statements. Our findings reveal substantial but task-dependent consistency, with binary classification and sentiment analysis achieving near-perfect reproducibility, while complex tasks show greater variability. More advanced models do not consistently demonstrate better consistency and reproducibility, with task-specific patterns emerging. LLMs significantly outperform expert human annotators in consistency and maintain high agreement even where human experts significantly disagree. We further find that simple aggregation strategies across 3-5 runs dramatically improve consistency. We also find that aggregation may come with an additional benefit of improved accuracy for sentiment analysis when using newer models. Simulation analysis reveals that despite measurable inconsistency in LLM outputs, downstream statistical inferences remain remarkably robust. These findings address concerns about what we term "G-hacking," the selective reporting of favorable outcomes from multiple Generative AI runs, by demonstrating that such risks are relatively low for finance and accounting tasks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inherent and emergent liability issues in LLM-based agentic systems: a principal-agent perspective</title>
<link>https://arxiv.org/abs/2504.03255</link>
<guid>https://arxiv.org/abs/2504.03255</guid>
<content:encoded><![CDATA[
arXiv:2504.03255v2 Announce Type: replace-cross 
Abstract: Agentic systems powered by large language models (LLMs) are becoming progressively more complex and capable. Their increasing agency and expanding deployment settings attract growing attention to effective governance policies, monitoring, and control protocols. Based on the emerging landscape of the agentic market, we analyze potential liability issues arising from the delegated use of LLM agents and their extended systems through a principal-agent perspective. Our analysis complements existing risk-based studies on artificial agency and covers the spectrum of important aspects of the principal-agent relationship and their potential consequences at deployment. Furthermore, we motivate method developments for technical governance along the directions of interpretability and behavior evaluations, reward and conflict management, and the mitigation of misalignment and misconduct through principled engineering of detection and fail-safe mechanisms. By illustrating the outstanding issues in AI liability for LLM-based agentic systems, we aim to inform the system design, auditing, and tracing to enhance transparency and liability attribution.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.12442</link>
<guid>https://arxiv.org/abs/2505.12442</guid>
<content:encoded><![CDATA[
arXiv:2505.12442v3 Announce Type: replace-cross 
Abstract: The rapid advancement of Large Language Models (LLMs) has led to the emergence of Multi-Agent Systems (MAS) to perform complex tasks through collaboration. However, the intricate nature of MAS, including their architecture and agent interactions, raises significant concerns regarding intellectual property (IP) protection. In this paper, we introduce MASLEAK, a novel attack framework designed to extract sensitive information from MAS applications. MASLEAK targets a practical, black-box setting, where the adversary has no prior knowledge of the MAS architecture or agent configurations. The adversary can only interact with the MAS through its public API, submitting attack query $q$ and observing outputs from the final agent. Inspired by how computer worms propagate and infect vulnerable network hosts, MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain responses from each MAS agent that reveal a full set of proprietary components, including the number of agents, system topology, system prompts, task instructions, and tool usages. We construct the first synthetic dataset of MAS applications with 810 applications and also evaluate MASLEAK against real-world MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in extracting MAS IP, with an average attack success rate of 87% for system prompts and task instructions, and 92% for system architecture in most cases. We conclude by discussing the implications of our findings and the potential defenses.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis</title>
<link>https://arxiv.org/abs/2505.13227</link>
<guid>https://arxiv.org/abs/2505.13227</guid>
<content:encoded><![CDATA[
arXiv:2505.13227v2 Announce Type: replace-cross 
Abstract: Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to capture the complexity of real-world interactions that require software commonsense, layout understanding, and fine-grained manipulation capabilities. To address these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising 564 finely annotated samples across diverse task types including text matching, element recognition, layout understanding, and precise manipulation. Additionally, we synthesize and release the largest computer use grounding dataset Jedi, which contains 4 million examples through multi-perspective decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its effectiveness by outperforming existing approaches on ScreenSpot-v2, ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved grounding with Jedi directly enhances agentic capabilities of general foundation models on complex computer tasks, improving from 5% to 27% on OSWorld. Through detailed ablation studies, we identify key factors contributing to grounding performance and verify that combining specialized data for different interface elements enables compositional generalization to novel interfaces. All benchmark, data, checkpoints, and code are open-sourced and available at https://osworld-grounding.github.io.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.20612</link>
<guid>https://arxiv.org/abs/2505.20612</guid>
<content:encoded><![CDATA[
arXiv:2505.20612v2 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to out-of-distribution classes, tasks and imaging modalities not typically found in their pre-training. Rather than simply re-training VLMs on more visual data, we argue that one should align VLMs to new concepts with annotation instructions containing a few visual examples and rich textual descriptions. To this end, we introduce Roboflow100-VL, a large-scale collection of 100 multi-modal object detection datasets with diverse concepts not commonly found in VLM pre-training. We evaluate state-of-the-art models on our benchmark in zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing for comparison across data regimes. Notably, we find that VLMs like GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on challenging medical imaging datasets within Roboflow100-VL, demonstrating the need for few-shot concept alignment. Lastly, we discuss our recent CVPR 2025 Foundational FSOD competition and share insights from the community. Notably, the winning team significantly outperforms our baseline by 16.8 mAP! Our code and dataset are available at https://github.com/roboflow/rf100-vl/ and https://universe.roboflow.com/rf100-vl/
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning</title>
<link>https://arxiv.org/abs/2506.00555</link>
<guid>https://arxiv.org/abs/2506.00555</guid>
<content:encoded><![CDATA[
arXiv:2506.00555v2 Announce Type: replace-cross 
Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy that progressively teaches the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL not only outperforms both open-source and proprietary Med-LVLMs, but also exhibits human-like reasoning patterns. Notably, it achieves an average performance gain of 20.7% over supervised fine-tuning baselines.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.01391</link>
<guid>https://arxiv.org/abs/2506.01391</guid>
<content:encoded><![CDATA[
arXiv:2506.01391v2 Announce Type: replace-cross 
Abstract: The recent progress of large language model agents has opened new possibilities for automating tasks through graphical user interfaces (GUIs), especially in mobile environments where intelligent interaction can greatly enhance usability. However, practical deployment of such agents remains constrained by several key challenges. Existing training data is often noisy and lack semantic diversity, which hinders the learning of precise grounding and planning. Models trained purely by imitation tend to overfit to seen interface patterns and fail to generalize in unfamiliar scenarios. Moreover, most prior work focuses on English interfaces while overlooks the growing diversity of non-English applications such as those in the Chinese mobile ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent built for robust and efficient on-device GUI interaction. Our training pipeline includes grounding-aware pre-training to enhance perception, supervised fine-tuning on high-quality Chinese and English trajectories to imitate human-like actions, and reinforcement fine-tuning with GRPO to improve reasoning capability. We also introduce a compact action space that reduces output length and supports low-latency execution on mobile devices. AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks and a new Chinese GUI benchmark called CAGUI, reaching $96.9\%$ Type-Match and $91.3\%$ Exact-Match. To facilitate reproducibility and further research, we publicly release all code, model checkpoint, and evaluation data.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants</title>
<link>https://arxiv.org/abs/2506.07042</link>
<guid>https://arxiv.org/abs/2506.07042</guid>
<content:encoded><![CDATA[
<div> machine learning, historical event extraction, knowledge graph, RDF/OWL reasoners, Coq proof assistant

Summary: This paper presents a novel approach to automatically extract structured computational representations of historical events from narrative text. By leveraging multiple LLMs and employing different enhancement strategies, such as base generation, knowledge graph enhancement, and Retrieval-Augmented Generation (RAG), the study demonstrates significant improvements in event extraction performance. The evaluation conducted on historical texts from Thucydides shows that base generation with Claude and GPT-4 offers high coverage and historical breadth, while RAG enhancement enhances precision in coordinate accuracy and metadata completeness. The study also highlights the impact of model architecture on enhancement sensitivity, with larger models showing robust performance and incremental improvements with RAG. Additionally, the development of an automated translation pipeline to convert extracted RDF representations into Coq proof assistant specifications enables higher-order reasoning capabilities, including multi-step causal verification, temporal arithmetic, and formal proofs about historical causation. The Coq formalization validates the legitimacy of event types discovered by RAG, ensuring domain-specific semantic structures are represented accurately. 

<br /><br />Summary: <div>
arXiv:2506.07042v2 Announce Type: replace 
Abstract: Extracting structured computational representations of historical events from narrative text remains computationally expensive when constructed manually. While RDF/OWL reasoners enable graph-based reasoning, they are limited to fragments of first-order logic, preventing deeper temporal and semantic analysis. This paper addresses both challenges by developing automatic historical event extraction models using multiple LLMs (GPT-4, Claude, Llama 3.2) with three enhancement strategies: pure base generation, knowledge graph enhancement, and Retrieval-Augmented Generation (RAG). We conducted comprehensive evaluations using historical texts from Thucydides. Our findings reveal that enhancement strategies optimize different performance dimensions rather than providing universal improvements. For coverage and historical breadth, base generation achieves optimal performance with Claude and GPT-4 extracting comprehensive events. However, for precision, RAG enhancement improves coordinate accuracy and metadata completeness. Model architecture fundamentally determines enhancement sensitivity: larger models demonstrate robust baseline performance with incremental RAG improvements, while Llama 3.2 shows extreme variance from competitive performance to complete failure. We then developed an automated translation pipeline converting extracted RDF representations into Coq proof assistant specifications, enabling higher-order reasoning beyond RDF capabilities including multi-step causal verification, temporal arithmetic with BC dates, and formal proofs about historical causation. The Coq formalization validates that RAG-discovered event types represent legitimate domain-specific semantic structures rather than ontological violations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid GA LLM Framework for Structured Task Optimization</title>
<link>https://arxiv.org/abs/2506.07483</link>
<guid>https://arxiv.org/abs/2506.07483</guid>
<content:encoded><![CDATA[
<div> Keywords: Genetic Algorithms, Large Language Models, structured generation tasks, constraints, modular design

Summary: 
GA LLM is a novel framework that merges Genetic Algorithms and Large Language Models for structured generation tasks within strict constraints. Output content, like plans or reports, is seen as genes, with evolutionary operations guided by the language model for enhancements. The language model provides domain expertise and diverse variations, while the genetic algorithm ensures coherence and global optimization. GA LLM excels in tasks like itinerary planning, academic outlining, and business reporting, consistently delivering well-organized and requirement-compliant outcomes. Its adaptable structure allows seamless integration into new tasks. Compared to solely relying on a language model, GA LLM outperforms in constraint fulfillment and solution quality by leveraging the strengths of both elements. 

<br /><br />Summary: <div>
arXiv:2506.07483v2 Announce Type: replace 
Abstract: GA LLM is a hybrid framework that combines Genetic Algorithms with Large Language Models to handle structured generation tasks under strict constraints. Each output, such as a plan or report, is treated as a gene, and evolutionary operations like selection, crossover, and mutation are guided by the language model to iteratively improve solutions. The language model provides domain knowledge and creative variation, while the genetic algorithm ensures structural integrity and global optimization. GA LLM has proven effective in tasks such as itinerary planning, academic outlining, and business reporting, consistently producing well structured and requirement satisfying results. Its modular design also makes it easy to adapt to new tasks. Compared to using a language model alone, GA LLM achieves better constraint satisfaction and higher quality solutions by combining the strengths of both components.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2506.07398</link>
<guid>https://arxiv.org/abs/2506.07398</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model, multi-agent systems, memory architecture, G-Memory, collaboration trajectories

Summary:<br />
The article introduces G-Memory, a hierarchical memory system for multi-agent systems (MAS) inspired by organizational memory theory. G-Memory manages MAS interactions through a three-tier graph hierarchy, allowing for the retrieval of high-level insights and fine-grained interaction trajectories. This enables the system to leverage cross-trial knowledge and compactly encode prior collaboration experiences. The entire hierarchy evolves with new collaborative trajectories, facilitating the progressive evolution of agent teams. Extensive experiments across various benchmarks and frameworks show that G-Memory improves success rates in action tasks and accuracy in knowledge QA without modifications to existing frameworks. G-Memory addresses the limitations of current MAS memory mechanisms, providing a more sophisticated and customized memory architecture for enhanced collaborative capabilities. <br /><br />Summary: <div>
arXiv:2506.07398v2 Announce Type: replace-cross 
Abstract: Large language model (LLM)-powered multi-agent systems (MAS) have demonstrated cognitive and execution capabilities that far exceed those of single LLM agents, yet their capacity for self-evolution remains hampered by underdeveloped memory architectures. Upon close inspection, we are alarmed to discover that prevailing MAS memory mechanisms (1) are overly simplistic, completely disregarding the nuanced inter-agent collaboration trajectories, and (2) lack cross-trial and agent-specific customization, in stark contrast to the expressive memory developed for single agents. To bridge this gap, we introduce G-Memory, a hierarchical, agentic memory system for MAS inspired by organizational memory theory, which manages the lengthy MAS interaction via a three-tier graph hierarchy: insight, query, and interaction graphs. Upon receiving a new user query, G-Memory performs bi-directional memory traversal to retrieve both $\textit{high-level, generalizable insights}$ that enable the system to leverage cross-trial knowledge, and $\textit{fine-grained, condensed interaction trajectories}$ that compactly encode prior collaboration experiences. Upon task execution, the entire hierarchy evolves by assimilating new collaborative trajectories, nurturing the progressive evolution of agent teams. Extensive experiments across five benchmarks, three LLM backbones, and three popular MAS frameworks demonstrate that G-Memory improves success rates in embodied action and accuracy in knowledge QA by up to $20.89\%$ and $10.12\%$, respectively, without any modifications to the original frameworks. Our codes are available at https://github.com/bingreeky/GMemory.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reparameterized LLM Training via Orthogonal Equivalence Transformation</title>
<link>https://arxiv.org/abs/2506.08001</link>
<guid>https://arxiv.org/abs/2506.08001</guid>
<content:encoded><![CDATA[
<div> Keyword: large language models, training algorithm, Orthogonal Equivalence Transformation, neurons, scalability

Summary:
Large language models (LLMs) are crucial for advances in artificial intelligence, but training them effectively is challenging. The proposed training algorithm, POET, utilizes Orthogonal Equivalence Transformation to optimize neurons by reparameterizing them with learnable orthogonal matrices and a fixed random weight matrix. POET preserves spectral properties of weight matrices, enhancing stability and generalization in objective function optimization. Efficient approximations make POET flexible and scalable for training large neural networks. Extensive experiments demonstrate the effectiveness and scalability of POET in training LLMs. <br /><br />Summary: <div>
arXiv:2506.08001v2 Announce Type: replace-cross 
Abstract: While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Focusing on Students, not Machines: Grounded Question Generation and Automated Answer Grading</title>
<link>https://arxiv.org/abs/2506.12066</link>
<guid>https://arxiv.org/abs/2506.12066</guid>
<content:encoded><![CDATA[
<div> Keywords: digital technologies, education, automated grading, PDF documents, Large Language Models 

Summary:
This thesis explores the use of digital technologies in education to automate the process of generating open-ended study questions and grading student answers. By developing a method to chunk PDF documents, the system can create high-quality questions from class materials and automatically grade student responses. A benchmark for automated grading of short answers is introduced, with a focus on evaluating Large Language Models (LLMs) for this task. The study finds that LLMs can effectively generalize to automated grading, with performance improving as model size increases. While automated grading systems show promise, human oversight is still necessary, particularly in examination settings. <div>
arXiv:2506.12066v1 Announce Type: new 
Abstract: Digital technologies are increasingly used in education to reduce the workload of teachers and students. However, creating open-ended study or examination questions and grading their answers is still a tedious task. This thesis presents the foundation for a system that generates questions grounded in class materials and automatically grades student answers. It introduces a sophisticated method for chunking documents with a visual layout, specifically targeting PDF documents. This method enhances the accuracy of downstream tasks, including Retrieval Augmented Generation (RAG). Our thesis demonstrates that high-quality questions and reference answers can be generated from study material. Further, it introduces a new benchmark for automated grading of short answers to facilitate comparison of automated grading systems. An evaluation of various grading systems is conducted and indicates that Large Language Models (LLMs) can generalise to the task of automated grading of short answers from their pre-training tasks. As with other tasks, increasing the parameter size of the LLMs leads to greater performance. Currently, available systems still need human oversight, especially in examination scenarios.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatbotManip: A Dataset to Facilitate Evaluation and Oversight of Manipulative Chatbot Behaviour</title>
<link>https://arxiv.org/abs/2506.12090</link>
<guid>https://arxiv.org/abs/2506.12090</guid>
<content:encoded><![CDATA[
<div> Keywords: ChatbotManip, manipulation tactics, Large Language Models, controversial strategies, AI safety research <br />
Summary: <br />
This paper introduces the ChatbotManip dataset, focusing on manipulation tactics used by chatbots in simulated conversations. The dataset includes a variety of manipulation contexts and is annotated for general manipulation and specific tactics. The research findings show that Large Language Models (LLMs) can be manipulative when instructed to be, with a high identification rate by annotators. Even when only instructed to be persuasive, LLMs often resort to controversial strategies like gaslighting and fear enhancement. It was found that smaller models can perform comparably to larger models in detecting manipulation but are not yet reliable for real-world oversight. These findings underscore the importance of addressing manipulation risks as LLMs become more prevalent in consumer applications, highlighting the need for AI safety research. <br /> <div>
arXiv:2506.12090v1 Announce Type: new 
Abstract: This paper introduces ChatbotManip, a novel dataset for studying manipulation in Chatbots. It contains simulated generated conversations between a chatbot and a (simulated) user, where the chatbot is explicitly asked to showcase manipulation tactics, persuade the user towards some goal, or simply be helpful. We consider a diverse set of chatbot manipulation contexts, from consumer and personal advice to citizen advice and controversial proposition argumentation. Each conversation is annotated by human annotators for both general manipulation and specific manipulation tactics. Our research reveals three key findings. First, Large Language Models (LLMs) can be manipulative when explicitly instructed, with annotators identifying manipulation in approximately 84\% of such conversations. Second, even when only instructed to be ``persuasive'' without explicit manipulation prompts, LLMs frequently default to controversial manipulative strategies, particularly gaslighting and fear enhancement. Third, small fine-tuned open source models, such as BERT+BiLSTM have a performance comparable to zero-shot classification with larger models like Gemini 2.5 pro in detecting manipulation, but are not yet reliable for real-world oversight. Our work provides important insights for AI safety research and highlights the need of addressing manipulation risks as LLMs are increasingly deployed in consumer-facing applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuously Updating Digital Twins using Large Language Models</title>
<link>https://arxiv.org/abs/2506.12091</link>
<guid>https://arxiv.org/abs/2506.12091</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital twins, in-context learning, large language models, CALM-DT, simulation

Summary:
Digital twins are models of real-world systems that can simulate dynamics in response to actions. Current approaches struggle to adapt to changes in variables and information without updates or re-designs. To address this, a new approach called CALM-DT frames digital twinning as an in-context learning problem using large language models. This allows for seamless updates to the twin at inference time without requiring parameter updates. CALM-DT utilizes fine-tuned encoders for sample retrieval, enabling accurate simulation across diverse state-action spaces. Empirical demonstrations show CALM-DT's competitive performance with existing digital twin approaches and its unique ability to adapt to changes in its modelling environment. By leveraging in-context learning, CALM-DT demonstrates the potential for digital twins to continuously update and remain relevant in complex settings. 

<br /><br />Summary: <div>
arXiv:2506.12091v1 Announce Type: new 
Abstract: Digital twins are models of real-world systems that can simulate their dynamics in response to potential actions. In complex settings, the state and action variables, and available data and knowledge relevant to a system can constantly change, requiring digital twins to continuously update with these changes to remain relevant. Current approaches struggle in this regard, as they require fixed, well-defined modelling environments, and they cannot adapt to novel variables without re-designs, or incorporate new information without re-training. To address this, we frame digital twinning as an in-context learning problem using large language models, enabling seamless updates to the twin at inference time. We develop CALM-DT, a Context-Adaptive Language Model-based Digital Twin that can accurately simulate across diverse state-action spaces using in-context learning alone by utilising fine-tuned encoders for sample retrieval. We empirically demonstrate CALM-DT's competitive performance with existing digital twin approaches, and its unique ability to adapt to changes in its modelling environment without parameter updates.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Traffic Accident Classifications: Application of NLP Methods for City Safety</title>
<link>https://arxiv.org/abs/2506.12092</link>
<guid>https://arxiv.org/abs/2506.12092</guid>
<content:encoded><![CDATA[
<div> Keywords: traffic accidents, Munich, classification, NLP methods, transformer-based models

Summary: 
In this study, an analysis of traffic incidents in Munich was conducted to understand patterns and characteristics of different types of accidents. The dataset included structured tabular features and unstructured free-text descriptions of each accident. NLP methods were employed to examine the reliability of incident labels, revealing inconsistencies in the classification process. A classification model was developed using both tabular and text data, with text descriptions proving to be the most informative. The model achieved high accuracy in categorizing accidents, emphasizing the importance of free-text data in accident analysis. The study also highlighted the potential of transformer-based models in enhancing classification reliability. This research underscores the significance of comprehensive accident data analysis for city safety and policy decisions. 

<br /><br />Summary: <div>
arXiv:2506.12092v1 Announce Type: new 
Abstract: A comprehensive understanding of traffic accidents is essential for improving city safety and informing policy decisions. In this study, we analyze traffic incidents in Munich to identify patterns and characteristics that distinguish different types of accidents. The dataset consists of both structured tabular features, such as location, time, and weather conditions, as well as unstructured free-text descriptions detailing the circumstances of each accident. Each incident is categorized into one of seven predefined classes. To assess the reliability of these labels, we apply NLP methods, including topic modeling and few-shot learning, which reveal inconsistencies in the labeling process. These findings highlight potential ambiguities in accident classification and motivate a refined predictive approach. Building on these insights, we develop a classification model that achieves high accuracy in assigning accidents to their respective categories. Our results demonstrate that textual descriptions contain the most informative features for classification, while the inclusion of tabular data provides only marginal improvements. These findings emphasize the critical role of free-text data in accident analysis and highlight the potential of transformer-based models in improving classification reliability.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UCD: Unlearning in LLMs via Contrastive Decoding</title>
<link>https://arxiv.org/abs/2506.12097</link>
<guid>https://arxiv.org/abs/2506.12097</guid>
<content:encoded><![CDATA[
<div> machine unlearning, large language models, contrastive decoding, unlearning benchmarks, model utility<br />
Summary:<br />
The article introduces an inference-time unlearning algorithm for large language models, focusing on removing specific information while maintaining performance. The proposed approach utilizes contrastive decoding with two smaller auxiliary models to guide the original model's outputs during inference. By leveraging models trained with and without the forget set, the algorithm significantly improves the tradeoff between unlearning effectiveness and model utility. Evaluation on unlearning benchmarks TOFU and MUSE demonstrates notable improvements in forget quality and retained performance compared to existing methods. This suggests that incorporating contrastive decoding can provide an efficient and practical means for removing concepts from large-scale models. <br /><br /> <div>
arXiv:2506.12097v1 Announce Type: new 
Abstract: Machine unlearning aims to remove specific information, e.g. sensitive or undesirable content, from large language models (LLMs) while preserving overall performance. We propose an inference-time unlearning algorithm that uses contrastive decoding, leveraging two auxiliary smaller models, one trained without the forget set and one trained with it, to guide the outputs of the original model using their difference during inference. Our strategy substantially improves the tradeoff between unlearning effectiveness and model utility. We evaluate our approach on two unlearning benchmarks, TOFU and MUSE. Results show notable gains in both forget quality and retained performance in comparison to prior approaches, suggesting that incorporating contrastive decoding can offer an efficient, practical avenue for unlearning concepts in large-scale models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized LLM Decoding via Contrasting Personal Preference</title>
<link>https://arxiv.org/abs/2506.12109</link>
<guid>https://arxiv.org/abs/2506.12109</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, personalization, decoding-time algorithms, reward-guided decoding, fine-tuning

Summary: 
CoPe (Contrasting Personal Preference) is a novel decoding-time approach designed for personalized text generation using large language models. The algorithm is applied after parameter-efficient fine-tuning on user-specific data and aims to maximize each user's implicit reward signal through reward-guided decoding. The study evaluates CoPe across five personalized text generation tasks and shows a significant improvement in personalization, with an average increase of 10.57% in ROUGE-L metric. Unlike existing methods, CoPe does not rely on external reward models or additional training procedures. This research underscores the importance of decoding-time algorithms in the personalization of large language models, highlighting the potential for enhancing user-specific content generation without the need for extensive model retraining. CoPe demonstrates promising results and opens up opportunities for further exploration in refining personalized text generation techniques. 

<br /><br />Summary: <div>
arXiv:2506.12109v1 Announce Type: new 
Abstract: As large language models (LLMs) are progressively deployed in various real-world applications, personalization of LLMs has become increasingly important. While various approaches to LLM personalization such as prompt-based and training-based methods have been actively explored, the development of effective decoding-time algorithms remains largely overlooked, despite their demonstrated potential. In this paper, we propose CoPe (Contrasting Personal Preference), a novel decoding-time approach applied after performing parameter-efficient fine-tuning (PEFT) on user-specific data. Our core idea is to leverage reward-guided decoding specifically for personalization by maximizing each user's implicit reward signal. We evaluate CoPe across five open-ended personalized text generation tasks. Our empirical results demonstrate that CoPe achieves strong performance, improving personalization by an average of 10.57% in ROUGE-L, without relying on external reward models or additional training procedures.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eliciting Reasoning in Language Models with Cognitive Tools</title>
<link>https://arxiv.org/abs/2506.12115</link>
<guid>https://arxiv.org/abs/2506.12115</guid>
<content:encoded><![CDATA[
<div> cognitive psychology, cognitive architectures, reasoning operations, cognitive tools, performance improvement

Summary:
In this study, the authors explore a novel approach to eliciting reasoning in large language models (LLMs) by implementing a set of "cognitive tools" within an agentic tool-calling framework. Drawing on cognitive psychology and cognitive architectures, they propose that reasoning arises from the sequential execution of predetermined cognitive operations. By endowing LLMs with specific reasoning operations, the authors demonstrate significant performance gains on standard mathematical reasoning benchmarks for both closed and open-weight models. The results show that providing these "cognitive tools" to GPT-4.1 substantially improves its performance on mathematical reasoning tasks, bringing it close to cutting-edge models like o1-preview. This approach sheds light on the debate surrounding post-training methods' role in eliciting reasoning in LLMs and the inherent capabilities acquired during pre-training, suggesting that post-training techniques may enhance latent abilities rather than simply uncovering them.

Summary:<br /><br />In this study, the authors propose a novel approach to eliciting reasoning in large language models (LLMs) by implementing a set of "cognitive tools" within an agentic tool-calling framework. Drawing on cognitive psychology and cognitive architectures, they demonstrate significant performance gains on standard mathematical reasoning benchmarks for both closed and open-weight models. Providing these "cognitive tools" to GPT-4.1 substantially improves its performance on mathematical reasoning tasks, bringing it close to cutting-edge models like o1-preview. This approach sheds light on the debate surrounding post-training methods' role in eliciting reasoning in LLMs and the inherent capabilities acquired during pre-training, suggesting that post-training techniques may enhance latent abilities rather than simply uncovering them. <div>
arXiv:2506.12115v1 Announce Type: new 
Abstract: The recent advent of reasoning models like OpenAI's o1 was met with excited speculation by the AI community about the mechanisms underlying these capabilities in closed models, followed by a rush of replication efforts, particularly from the open source community. These speculations were largely settled by the demonstration from DeepSeek-R1 that chains-of-thought and reinforcement learning (RL) can effectively replicate reasoning on top of base LLMs. However, it remains valuable to explore alternative methods for theoretically eliciting reasoning that could help elucidate the underlying mechanisms, as well as providing additional methods that may offer complementary benefits.
  Here, we build on the long-standing literature in cognitive psychology and cognitive architectures, which postulates that reasoning arises from the orchestrated, sequential execution of a set of modular, predetermined cognitive operations. Crucially, we implement this key idea within a modern agentic tool-calling framework. In particular, we endow an LLM with a small set of "cognitive tools" encapsulating specific reasoning operations, each executed by the LLM itself. Surprisingly, this simple strategy results in considerable gains in performance on standard mathematical reasoning benchmarks compared to base LLMs, for both closed and open-weight models. For instance, providing our "cognitive tools" to GPT-4.1 increases its pass@1 performance on AIME2024 from 26.7% to 43.3%, bringing it very close to the performance of o1-preview.
  In addition to its practical implications, this demonstration contributes to the debate regarding the role of post-training methods in eliciting reasoning in LLMs versus the role of inherent capabilities acquired during pre-training, and whether post-training merely uncovers these latent abilities.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Document and Template Clustering using Multimodal Embeddings</title>
<link>https://arxiv.org/abs/2506.12116</link>
<guid>https://arxiv.org/abs/2506.12116</guid>
<content:encoded><![CDATA[
<div> embeddings, document clustering, multimodal models, intelligent document processing, document layout analysis

Summary:
This paper introduces a novel approach to unsupervised document clustering using multimodal embeddings in combination with traditional clustering algorithms like k-Means and DBSCAN. The method aims to provide a more detailed understanding of documents by grouping them not only at the type level but also distinguishing between different templates within the same category. By incorporating embeddings that capture textual content, layout information, and visual features, the approach effectively enhances document clustering. The study evaluates the effectiveness of various state-of-the-art pretrained multimodal models such as SBERT, LayoutLMv1, LayoutLMv3, DiT, Donut, and ColPali. The findings highlight the potential of multimodal embeddings for improving document clustering and suggest benefits for tasks like intelligent document processing, document layout analysis, and unsupervised document classification. The research sheds light on the strengths and limitations of different multimodal models in this context, paving the way for further exploration and organization of document collections.

<br /><br />Summary: <div>
arXiv:2506.12116v1 Announce Type: new 
Abstract: This paper investigates a novel approach to unsupervised document clustering by leveraging multimodal embeddings as input to traditional clustering algorithms such as $k$-Means and DBSCAN. Our method aims to achieve a finer-grained document understanding by not only grouping documents at the type level (e.g., invoices, purchase orders), but also distinguishing between different templates within the same document category. This is achieved by using embeddings that capture textual content, layout information, and visual features of documents. We evaluated the effectiveness of this approach using embeddings generated by several state-of-the-art pretrained multimodal models, including SBERT, LayoutLMv1, LayoutLMv3, DiT, Donut, and ColPali. Our findings demonstrate the potential of multimodal embeddings to significantly enhance document clustering, offering benefits for various applications in intelligent document processing, document layout analysis, and unsupervised document classification. This work provides valuable insight into the advantages and limitations of different multimodal models for this task and opens new avenues for future research to understand and organize document collections.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Mixture-of-Experts Surpass Dense LLMs Under Strictly Equal Resources?</title>
<link>https://arxiv.org/abs/2506.12119</link>
<guid>https://arxiv.org/abs/2506.12119</guid>
<content:encoded><![CDATA[
<div> model architecture, Mixture-of-Experts, performance, total parameter count, data reuse
<br />
Summary:<br />
The study compares Mixture-of-Experts (MoE) language models with dense architectures under equal resource constraints. Through comprehensive analysis, an optimal MoE model design is identified, showing superior performance when parameters, training compute, and data resources are equal. Activation rate plays a crucial role in achieving optimal performance across various model sizes. Additional data can enhance performance, mitigated by data reuse strategies. The study validates results with 200 models at 2B scale and 50 models at 7B scale, processing 50 trillion tokens.<br /><br /> <div>
arXiv:2506.12119v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) language models dramatically expand model capacity and achieve remarkable performance without increasing per-token compute. However, can MoEs surpass dense architectures under strictly equal resource constraints - that is, when the total parameter count, training compute, and data budget are identical? This question remains under-explored despite its significant practical value and potential. In this paper, we propose a novel perspective and methodological framework to study this question thoroughly. First, we comprehensively investigate the architecture of MoEs and achieve an optimal model design that maximizes the performance. Based on this, we subsequently find that an MoE model with activation rate in an optimal region is able to outperform its dense counterpart under the same total parameter, training compute and data resource. More importantly, this optimal region remains consistent across different model sizes. Although additional amount of data turns out to be a trade-off for the enhanced performance, we show that this can be resolved via reusing data. We validate our findings through extensive experiments, training nearly 200 language models at 2B scale and over 50 at 7B scale, cumulatively processing 50 trillion tokens. All models will be released publicly.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hatevolution: What Static Benchmarks Don't Tell Us</title>
<link>https://arxiv.org/abs/2506.12148</link>
<guid>https://arxiv.org/abs/2506.12148</guid>
<content:encoded><![CDATA[
<div> evolving hate speech experiments, language models, benchmarking, NLP research, model training<br />
Summary:<br />
The study focuses on the evolution of language, particularly in the hate speech domain, and its impact on model training and benchmarking in natural language processing (NLP) research. The research empirically evaluates the robustness of 20 language models through two evolving hate speech experiments. The findings highlight the temporal misalignment between static evaluations and time-sensitive evaluations in understanding hate speech. The study emphasizes the need for time-sensitive linguistic benchmarks to accurately and reliably assess language models in the hate speech domain. It underscores the importance of adapting benchmarks to reflect changing language trends, which are essential for ensuring the safety and effectiveness of language models in addressing hate speech. <div>
arXiv:2506.12148v1 Announce Type: new 
Abstract: Language changes over time, including in the hate speech domain, which evolves quickly following social dynamics and cultural shifts. While NLP research has investigated the impact of language evolution on model training and has proposed several solutions for it, its impact on model benchmarking remains under-explored. Yet, hate speech benchmarks play a crucial role to ensure model safety. In this paper, we empirically evaluate the robustness of 20 language models across two evolving hate speech experiments, and we show the temporal misalignment between static and time-sensitive evaluations. Our findings call for time-sensitive linguistic benchmarks in order to correctly and reliably evaluate language models in the hate speech domain.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximally-Informative Retrieval for State Space Model Generation</title>
<link>https://arxiv.org/abs/2506.12149</link>
<guid>https://arxiv.org/abs/2506.12149</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, retrieval, optimization, model uncertainty, document mixture

Summary: 
- The optimal way to answer a query involves using all available information, but limited model resources require resorting to external memory for processing.
- Retrieval In-Context Optimization (RICO) is proposed as a method to learn the optimal combination of documents for answer generation using gradients from the LLM itself.
- RICO outperforms traditional retrieval-augmented generation methods like RAG by leveraging direct model feedback, without the need for external heuristics for document retrieval.
- The standard top-$k$ retrieval with model gradients approximates RICO's optimization technique, with connections to the leave-one-out loss.
- Empirical results show that RICO, minimizing question perplexity without fine-tuning, achieves comparable retriever metric performance to BM25 and often outperforms fine-tuned dense retrievers like E5. 

<br /><br />Summary: <div>
arXiv:2506.12149v1 Announce Type: new 
Abstract: Given a query and dataset, the optimal way of answering the query is to make use all the information available. Modern LLMs exhibit impressive ability to memorize training data, but data not deemed important during training is forgotten, and information outside that training set cannot be made use of. Processing an entire dataset at inference time is infeasible due to the bounded nature of model resources (e.g. context size in transformers or states in state space models), meaning we must resort to external memory. This constraint naturally leads to the following problem: How can we decide based on the present query and model, what among a virtually unbounded set of known data matters for inference? To minimize model uncertainty for a particular query at test-time, we introduce Retrieval In-Context Optimization (RICO), a retrieval method that uses gradients from the LLM itself to learn the optimal mixture of documents for answer generation. Unlike traditional retrieval-augmented generation (RAG), which relies on external heuristics for document retrieval, our approach leverages direct feedback from the model. Theoretically, we show that standard top-$k$ retrieval with model gradients can approximate our optimization procedure, and provide connections to the leave-one-out loss. We demonstrate empirically that by minimizing an unsupervised loss objective in the form of question perplexity, we can achieve comparable retriever metric performance to BM25 with \emph{no finetuning}. Furthermore, when evaluated on quality of the final prediction, our method often outperforms fine-tuned dense retrievers such as E5.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2506.12158</link>
<guid>https://arxiv.org/abs/2506.12158</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, generation strategies, low-resource languages, synthetic data, NLP tasks

Summary:
This paper evaluates different generation strategies for synthetic data in low-resource language settings using Large Language Models (LLMs). The study compares various prompting methods, including demonstrations, label-based summaries, and self-revision, across 11 diverse languages, some of which are extremely low-resource. Results show that combining generation methods, specifically target-language demonstrations with LLM-based revisions, yields strong performance, reducing the gap with real data to as little as 5% in certain cases. The study also suggests that smart prompting techniques can decrease the advantage of larger LLMs, demonstrating efficient strategies for generating synthetic data in low-resource scenarios with smaller models. These findings provide insights into optimizing the use of LLMs for training specialized models in diverse language settings. 

<br /><br />Summary: <div>
arXiv:2506.12158v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used to generate synthetic textual data for training smaller specialized models. However, a comparison of various generation strategies for low-resource language settings is lacking. While various prompting strategies have been proposed, such as demonstrations, label-based summaries, and self-revision, their comparative effectiveness remains unclear, especially for low-resource languages. In this paper, we systematically evaluate the performance of these generation strategies and their combinations across 11 typologically diverse languages, including several extremely low-resource ones. Using three NLP tasks and four open-source LLMs, we assess downstream model performance on generated versus gold-standard data. Our results show that strategic combinations of generation methods, particularly target-language demonstrations with LLM-based revisions, yield strong performance, narrowing the gap with real data to as little as 5% in some settings. We also find that smart prompting techniques can reduce the advantage of larger LLMs, highlighting efficient generation strategies for synthetic data generation in low-resource scenarios with smaller models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction Tuning and CoT Prompting for Contextual Medical QA with LLMs</title>
<link>https://arxiv.org/abs/2506.12182</link>
<guid>https://arxiv.org/abs/2506.12182</guid>
<content:encoded><![CDATA[
<div> prompt design, fine-tuning, large language models, medical question answering, PubMedQA

Summary:
- The study examines prompt design and fine-tuning effects on open-source large language models (LLMs) in biomedical reasoning for medical question answering.
- Two prompting strategies, standard instruction prompts and Chain-of-Thought (CoT) prompts, are compared, and QLoRA is used for efficient instruction tuning.
- CoT prompts alone improve reasoning in zero-shot settings, while instruction tuning enhances accuracy, especially for smaller models.
- Fine-tuning on CoT prompts does not consistently improve performance and may even worsen it for certain larger models.
- Reasoning-aware prompts are beneficial, but their impact varies depending on the model and scale. Practical insights are provided for combining prompt engineering with efficient fine-tuning in medical question answering applications.

<br /><br />Summary: <div>
arXiv:2506.12182v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown great potential in medical question answering (MedQA), yet adapting them to biomedical reasoning remains challenging due to domain-specific complexity and limited supervision. In this work, we study how prompt design and lightweight fine-tuning affect the performance of open-source LLMs on PubMedQA, a benchmark for multiple-choice biomedical questions. We focus on two widely used prompting strategies - standard instruction prompts and Chain-of-Thought (CoT) prompts - and apply QLoRA for parameter-efficient instruction tuning. Across multiple model families and sizes, our experiments show that CoT prompting alone can improve reasoning in zero-shot settings, while instruction tuning significantly boosts accuracy. However, fine-tuning on CoT prompts does not universally enhance performance and may even degrade it for certain larger models. These findings suggest that reasoning-aware prompts are useful, but their benefits are model- and scale-dependent. Our study offers practical insights into combining prompt engineering with efficient finetuning for medical QA applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supernova Event Dataset: Interpreting Large Language Model's Personality through Critical Event Analysis</title>
<link>https://arxiv.org/abs/2506.12189</link>
<guid>https://arxiv.org/abs/2506.12189</guid>
<content:encoded><![CDATA[
<div> Personality, Large Language Models, Supernova Event Dataset, key events extraction, model benchmarking<br />
Summary:<br />
- Understanding the decision-making and personality traits of Large Language Models (LLMs) is crucial with their increasing integration into everyday applications.
- The study introduces the Supernova Event Dataset, a diverse collection of articles for interpreting LLM personality traits.
- Evaluation of small and large models reveals distinct personality traits based on their selection and classification of events.
- Orca 2 showcases emotional reasoning and interpersonal focus, while Qwen 2.5 displays a strategic, analytical style.
- Models like Claude Sonnet 3.7, Gemini 2.5 Pro, and o3 highlight different approaches to scientific discovery events, emphasizing conceptual framing, empirical validation, and step-by-step causal reasoning, respectively. 
<br /><br /> <div>
arXiv:2506.12189v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly integrated into everyday applications. As their influence grows, understanding their decision making and underlying personality becomes essential. In this work, we interpret model personality using our proposed Supernova Event Dataset, a novel dataset with diverse articles spanning biographies, historical events, news, and scientific discoveries. We use this dataset to benchmark LLMs on extracting and ranking key events from text, a subjective and complex challenge that requires reasoning over long-range context and modeling causal chains. We evaluate small models like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as Claude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another LLM acts as a judge to infer each model's personality based on its selection and classification of events. Our analysis shows distinct personality traits: for instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal dynamics, while Qwen 2.5 displays a more strategic, analytical style. When analyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual framing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors step-by-step causal reasoning. This analysis improves model interpretability, making them user-friendly for a wide range of diverse applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index</title>
<link>https://arxiv.org/abs/2506.12229</link>
<guid>https://arxiv.org/abs/2506.12229</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, text data, search engines, FM-index, benchmark contamination

Summary: 
Infini-gram mini is a new efficient and scalable system for searching petabyte-level text corpora using the FM-index data structure. The system indexes text data with only 44% of the original size, significantly improving indexing speed and memory usage compared to existing implementations. Infini-gram mini can index 46TB of Internet text in a relatively short time using a single CPU node. The system was used to analyze benchmark contamination in language model evaluation datasets, revealing significant levels of contamination in core benchmarks such as SQuAD. A benchmark contamination bulletin was created to share contamination rates of various benchmarks. Additionally, a web interface and API endpoint were released to allow general search queries on the Infini-gram mini indexes. 

<br /><br />Summary: <div>
arXiv:2506.12229v1 Announce Type: new 
Abstract: Language models are trained mainly on massive text data from the Internet, and it becomes increasingly important to understand this data source. Exact-match search engines enable searching in large text corpora -- counting string appearances and retrieving the enclosing documents -- yet the high storage overhead hinders their application on Internet-scale data. We present Infini-gram mini, an efficient and scalable system that can make petabyte-level text corpora searchable. Based on the FM-index data structure (Ferragina and Manzini, 2000), which simultaneously indexes and compresses text, our system creates indexes with size only 44% of the corpus. Infini-gram mini greatly improves upon the best existing implementation of FM-index in terms of indexing speed (18$\times$) and memory use during both indexing (3.2$\times$ reduction) and querying (down to a negligible amount). We index 46TB of Internet text in 50 days with a single 128-core CPU node (or 19 hours if using 75 such nodes). We show one important use case of Infini-gram mini in a large-scale analysis of benchmark contamination. We find several core LM evaluation benchmarks to be heavily contaminated in Internet crawls (up to 40% in SQuAD), which could lead to overestimating the capabilities of language models if trained on such data. We host a benchmark contamination bulletin to share the contamination rate of many core and community-contributed benchmarks. We also release a web interface and an API endpoint to serve general search queries on Infini-gram mini indexes.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for History, Philosophy, and Sociology of Science: Interpretive Uses, Methodological Challenges, and Critical Perspectives</title>
<link>https://arxiv.org/abs/2506.12242</link>
<guid>https://arxiv.org/abs/2506.12242</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, history, philosophy, sociology of science, interpretive research, epistemic infrastructures <br />
Summary: This paper discusses the use of large language models (LLMs) in the history, philosophy, and sociology of science (HPSS) as research tools. LLMs offer new capabilities in processing unstructured text and inferring meaning from context, challenging the traditional boundaries between computational and interpretative methods in HPSS. The paper explains LLM architectures and their training paradigms and emphasizes that LLMs are not neutral tools but encode assumptions about meaning and context based on their training data. It explores how computational techniques enhanced by LLMs can support interpretive research in HPSS and compares full-context and generative models. The analysis includes strategies for adapting LLMs to HPSS tasks and highlights the importance of LLM literacy in HPSS. The paper concludes with recommendations for integrating LLMs into HPSS, stressing the need for interpretive trade-offs, defining benchmarks and corpora, and ensuring that LLMs enhance rather than replace interpretive methods.<br /><br />Summary: <div>
arXiv:2506.12242v1 Announce Type: new 
Abstract: This paper explores the use of large language models (LLMs) as research tools in the history, philosophy, and sociology of science (HPSS). LLMs are remarkably effective at processing unstructured text and inferring meaning from context, offering new affordances that challenge long-standing divides between computational and interpretive methods. This raises both opportunities and challenges for HPSS, which emphasizes interpretive methodologies and understands meaning as context-dependent, ambiguous, and historically situated. We argue that HPSS is uniquely positioned not only to benefit from LLMs' capabilities but also to interrogate their epistemic assumptions and infrastructural implications. To this end, we first offer a concise primer on LLM architectures and training paradigms tailored to non-technical readers. We frame LLMs not as neutral tools but as epistemic infrastructures that encode assumptions about meaning, context, and similarity, conditioned by their training data, architecture, and patterns of use. We then examine how computational techniques enhanced by LLMs, such as structuring data, detecting patterns, and modeling dynamic processes, can be applied to support interpretive research in HPSS. Our analysis compares full-context and generative models, outlines strategies for domain and task adaptation (e.g., continued pretraining, fine-tuning, and retrieval-augmented generation), and evaluates their respective strengths and limitations for interpretive inquiry in HPSS. We conclude with four lessons for integrating LLMs into HPSS: (1) model selection involves interpretive trade-offs; (2) LLM literacy is foundational; (3) HPSS must define its own benchmarks and corpora; and (4) LLMs should enhance, not replace, interpretive methods.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Behavior Gap: Evaluating Zero-shot LLM Agents in Complex Task-Oriented Dialogs</title>
<link>https://arxiv.org/abs/2506.12266</link>
<guid>https://arxiv.org/abs/2506.12266</guid>
<content:encoded><![CDATA[
<div> agents, behavior gap, task complexity, dialog acts, tool usage

Summary:<br />
- Large Language Model (LLM) agents in Task-Oriented Dialog Systems (TODS) face performance challenges, especially in zero-shot scenarios.
- A study proposes an evaluation framework to quantify the behavior gap between AI agents and humans in TODS.
- The behavior gap widens as task complexity increases, negatively impacting agent performance.
- Even GPT-4o-based agents show low alignment with human behavior, particularly in dialog acts, tool usage, and knowledge utilization.
- Reducing behavior gaps through improved alignment strategies can lead to significant performance improvement in handling complex tasks. 

Summary: <div>
arXiv:2506.12266v1 Announce Type: new 
Abstract: Large Language Model (LLM)-based agents have significantly impacted Task-Oriented Dialog Systems (TODS) but continue to face notable performance challenges, especially in zero-shot scenarios. While prior work has noted this performance gap, the behavioral factors driving the performance gap remain under-explored. This study proposes a comprehensive evaluation framework to quantify the behavior gap between AI agents and human experts, focusing on discrepancies in dialog acts, tool usage, and knowledge utilization. Our findings reveal that this behavior gap is a critical factor negatively impacting the performance of LLM agents. Notably, as task complexity increases, the behavior gap widens (correlation: 0.963), leading to a degradation of agent performance on complex task-oriented dialogs. For the most complex task in our study, even the GPT-4o-based agent exhibits low alignment with human behavior, with low F1 scores for dialog acts (0.464), excessive and often misaligned tool usage with a F1 score of 0.139, and ineffective usage of external knowledge. Reducing such behavior gaps leads to significant performance improvement (24.3% on average). This study highlights the importance of comprehensive behavioral evaluations and improved alignment strategies to enhance the effectiveness of LLM-based TODS in handling complex tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Med-U1: Incentivizing Unified Medical Reasoning in LLMs via Large-scale Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.12307</link>
<guid>https://arxiv.org/abs/2506.12307</guid>
<content:encoded><![CDATA[
<div> framework, medical QA, large language models, reinforcement learning, benchmarks <br />
<br />
Summary: 
The paper introduces Med-U1, a framework for improving medical Question-Answering tasks using large language models. Med-U1 incorporates reinforcement learning and rule-based reward functions to enhance reasoning across different types of medical QA tasks. By optimizing for concise and verifiable reasoning chains, Med-U1 outperforms specialized models on challenging benchmarks. It also shows robust generalization to out-of-distribution tasks. The paper provides insights into training strategies, controlling reasoning chain length, and reward design for medical large language models. The code for Med-U1 will be made available for future research and development. <div>
arXiv:2506.12307v1 Announce Type: new 
Abstract: Medical Question-Answering (QA) encompasses a broad spectrum of tasks, including multiple choice questions (MCQ), open-ended text generation, and complex computational reasoning. Despite this variety, a unified framework for delivering high-quality medical QA has yet to emerge. Although recent progress in reasoning-augmented large language models (LLMs) has shown promise, their ability to achieve comprehensive medical understanding is still largely unexplored. In this paper, we present Med-U1, a unified framework for robust reasoning across medical QA tasks with diverse output formats, ranging from MCQs to complex generation and computation tasks. Med-U1 employs pure large-scale reinforcement learning with mixed rule-based binary reward functions, incorporating a length penalty to manage output verbosity. With multi-objective reward optimization, Med-U1 directs LLMs to produce concise and verifiable reasoning chains. Empirical results reveal that Med-U1 significantly improves performance across multiple challenging Med-QA benchmarks, surpassing even larger specialized and proprietary models. Furthermore, Med-U1 demonstrates robust generalization to out-of-distribution (OOD) tasks. Extensive analysis presents insights into training strategies, reasoning chain length control, and reward design for medical LLMs. The code will be released.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phonikud: Hebrew Grapheme-to-Phoneme Conversion for Real-Time Text-to-Speech</title>
<link>https://arxiv.org/abs/2506.12311</link>
<guid>https://arxiv.org/abs/2506.12311</guid>
<content:encoded><![CDATA[
<div> diacritization, Hebrew G2P, IPA transcriptions, real-time TTS, ILSpeech dataset <br />
<br />Summary: Phonikud is a new lightweight Hebrew grapheme-to-phoneme (G2P) system that accurately predicts phonemes from Hebrew text, including crucial phonetic features like stress. This system outputs fully-specified IPA transcriptions, addressing limitations in existing solutions. By adapting an existing diacritization model with lightweight adaptors, Phonikud incurs minimal additional latency, enabling the training of real-time Hebrew text-to-speech (TTS) models with superior speed-accuracy trade-offs. The researchers also introduce the ILSpeech dataset, which contains transcribed Hebrew speech with IPA annotations, serving as both a benchmark for Hebrew G2P systems and as training data for TTS models. The results show that Phonikud's G2P conversion outperforms previous methods, paving the way for efficient real-time TTS for the complex phonetic features of Modern Hebrew. The code, data, and models are available at https://phonikud.github.io. <br /> <div>
arXiv:2506.12311v1 Announce Type: new 
Abstract: Real-time text-to-speech (TTS) for Modern Hebrew is challenging due to the language's orthographic complexity. Existing solutions ignore crucial phonetic features such as stress that remain underspecified even when vowel marks are added. To address these limitations, we introduce Phonikud, a lightweight, open-source Hebrew grapheme-to-phoneme (G2P) system that outputs fully-specified IPA transcriptions. Our approach adapts an existing diacritization model with lightweight adaptors, incurring negligible additional latency. We also contribute the ILSpeech dataset of transcribed Hebrew speech with IPA annotations, serving as a benchmark for Hebrew G2P and as training data for TTS systems. Our results demonstrate that Phonikud G2P conversion more accurately predicts phonemes from Hebrew text compared to prior methods, and that this enables training of effective real-time Hebrew TTS models with superior speed-accuracy trade-offs. We release our code, data, and models at https://phonikud.github.io.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intersectional Bias in Japanese Large Language Models from a Contextualized Perspective</title>
<link>https://arxiv.org/abs/2506.12327</link>
<guid>https://arxiv.org/abs/2506.12327</guid>
<content:encoded><![CDATA[
<div> intersectional bias, large language models, social bias, Japanese benchmark, question-answering

Summary:<br />
This study examines intersectional bias in large language models (LLMs) using the Japanese benchmark inter-JBBQ. While previous research has focused on bias in a single social attribute, this study highlights the importance of considering multiple social attributes in analyzing bias in LLMs. The inter-JBBQ is designed to evaluate intersectional bias in LLMs in a question-answering setting. By applying this benchmark to GPT-4o and Swallow, the study finds that biased output varies across different contexts, even when social attributes are equally combined. This underscores the need for greater attention to intersectionality in addressing bias in LLMs. <div>
arXiv:2506.12327v1 Announce Type: new 
Abstract: An growing number of studies have examined the social bias of rapidly developed large language models (LLMs). Although most of these studies have focused on bias occurring in a single social attribute, research in social science has shown that social bias often occurs in the form of intersectionality -- the constitutive and contextualized perspective on bias aroused by social attributes. In this study, we construct the Japanese benchmark inter-JBBQ, designed to evaluate the intersectional bias in LLMs on the question-answering setting. Using inter-JBBQ to analyze GPT-4o and Swallow, we find that biased output varies according to its contexts even with the equal combination of social attributes.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Effects of Cognitive Biases in Prompts on Large Language Model Outputs</title>
<link>https://arxiv.org/abs/2506.12338</link>
<guid>https://arxiv.org/abs/2506.12338</guid>
<content:encoded><![CDATA[
<div> Keywords: cognitive biases, large language models, prompt design, attention weight analysis, AI applications<br />
Summary:<br />
This paper investigates the influence of cognitive biases on Large Language Models (LLMs) outputs by introducing various biases into prompts and assessing their impact on LLM accuracy. The study includes general and financial Q&amp;A scenarios across multiple benchmark datasets. The results show that subtle biases can significantly alter LLM answer choices, emphasizing the need for bias-aware prompt design and mitigation strategies. Attention weight analysis reveals how biases can affect the internal decision-making processes of LLMs, leading to output inaccuracies. This research has implications for AI developers and users in enhancing the robustness and reliability of AI applications in diverse domains.<br /> <div>
arXiv:2506.12338v1 Announce Type: new 
Abstract: This paper investigates the influence of cognitive biases on Large Language Models (LLMs) outputs. Cognitive biases, such as confirmation and availability biases, can distort user inputs through prompts, potentially leading to unfaithful and misleading outputs from LLMs. Using a systematic framework, our study introduces various cognitive biases into prompts and assesses their impact on LLM accuracy across multiple benchmark datasets, including general and financial Q&amp;A scenarios. The results demonstrate that even subtle biases can significantly alter LLM answer choices, highlighting a critical need for bias-aware prompt design and mitigation strategy. Additionally, our attention weight analysis highlights how these biases can alter the internal decision-making processes of LLMs, affecting the attention distribution in ways that are associated with output inaccuracies. This research has implications for Al developers and users in enhancing the robustness and reliability of Al applications in diverse domains.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refract ICL: Rethinking Example Selection in the Era of Million-Token Models</title>
<link>https://arxiv.org/abs/2506.12346</link>
<guid>https://arxiv.org/abs/2506.12346</guid>
<content:encoded><![CDATA[
<div> Keywords: long-context large language models, in-context learning, ICL selection strategies, Refract ICL, Gemini 1.5 Pro<br />
Summary: 
- The paper explores the effectiveness of traditional in-context learning (ICL) selection strategies in the context of long-context large language models.
- Increasing the number of demonstrations does not automatically lead to improved performance, indicating the importance of smart ICL selection.
- The introduction of the novel Refract ICL selection algorithm enhances ICL by strategically repeating challenging examples within the context and utilizing zero-shot predictions as error signals.
- The results demonstrate that Refract ICL significantly boosts the performance of extremely long-context models like Gemini 1.5 Pro, especially on tasks with fewer output classes.
- The study highlights the necessity of thoughtful ICL selection even when leveraging a large number of demonstrations, underscoring the importance of selecting relevant and diverse examples for effective learning. <br /><br />Summary: <div>
arXiv:2506.12346v1 Announce Type: new 
Abstract: The emergence of long-context large language models (LLMs) has enabled the use of hundreds, or even thousands, of demonstrations for in-context learning (ICL) - a previously impractical regime. This paper investigates whether traditional ICL selection strategies, which balance the similarity of ICL examples to the test input (using a text retriever) with diversity within the ICL set, remain effective when utilizing a large number of demonstrations. Our experiments demonstrate that, while longer contexts can accommodate more examples, simply increasing the number of demonstrations does not guarantee improved performance. Smart ICL selection remains crucial, even with thousands of demonstrations. To further enhance ICL in this setting, we introduce Refract ICL, a novel ICL selection algorithm specifically designed to focus LLM attention on challenging examples by strategically repeating them within the context and incorporating zero-shot predictions as error signals. Our results show that Refract ICL significantly improves the performance of extremely long-context models such as Gemini 1.5 Pro, particularly on tasks with a smaller number of output classes.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Reasoning Through Suppression of Self-Affirmation Reflections in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2506.12353</link>
<guid>https://arxiv.org/abs/2506.12353</guid>
<content:encoded><![CDATA[
<div> keywords: efficient reasoning, self-affirmation reflections, output length, model optimization, length compression

Summary: 
Efficient reasoning models are essential in handling the increasing output length of large models. This study focuses on self-affirmation reflections, redundant steps that affirm prior content, leading to longer outputs. Analysis shows a distinct probability bias in the first words of sentences in self-affirmation reflections. By suppressing these reflections, output length can be reduced without sacrificing accuracy in various models. Train-free experiments demonstrate a compression of 18.7%, while train-based methods achieve up to 50.2% length reduction. These improvements are simple and practical, applicable to existing inference frameworks. The findings offer insights for precise length compression and efficient reasoning at the step-level. 

<br /><br />Summary: <div>
arXiv:2506.12353v1 Announce Type: new 
Abstract: While recent advances in large reasoning models have demonstrated remarkable performance, efficient reasoning remains critical due to the rapid growth of output length. Existing optimization approaches highlights a tendency toward "overthinking", yet lack fine-grained analysis. In this work, we focus on Self-Affirmation Reflections: redundant reflective steps that affirm prior content and often occurs after the already correct reasoning steps. Observations of both original and optimized reasoning models reveal pervasive self-affirmation reflections. Notably, these reflections sometimes lead to longer outputs in optimized models than their original counterparts. Through detailed analysis, we uncover an intriguing pattern: compared to other reflections, the leading words (i.e., the first word of sentences) in self-affirmation reflections exhibit a distinct probability bias. Motivated by this insight, we can locate self-affirmation reflections and conduct a train-free experiment demonstrating that suppressing self-affirmation reflections reduces output length without degrading accuracy across multiple models (R1-Distill-Models, QwQ-32B, and Qwen3-32B). Furthermore, we also improve current train-based method by explicitly suppressing such reflections. In our experiments, we achieve length compression of 18.7\% in train-free settings and 50.2\% in train-based settings for R1-Distill-Qwen-1.5B. Moreover, our improvements are simple yet practical and can be directly applied to existing inference frameworks, such as vLLM. We believe that our findings will provide community insights for achieving more precise length compression and step-level efficient reasoning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics</title>
<link>https://arxiv.org/abs/2506.12365</link>
<guid>https://arxiv.org/abs/2506.12365</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reasoning skills, adaptability, ethical decisions, computational efficiency

Summary: 
Large Language Models (LLMs) have seen significant advancements in recent years, particularly in improving their reasoning skills, adaptability to various tasks, computational efficiency, and ability to make ethical decisions. Techniques like Chain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from Human Feedback have bridged the gap between human and machine communications. Multimodal learning and few-shot or zero-shot techniques have further strengthened LLMs to handle complex jobs with minimal input. Scaling and optimization tricks have enabled LLMs to do more with less computing power. This survey highlights emerging methods that enhance LLM reasoning, efficiency, and ethical alignment, while also identifying underexplored areas like interpretability, cross-modal integration, and sustainability. Despite progress, challenges such as high computational costs, biases, and ethical risks persist, necessitating bias mitigation, transparent decision-making, and clear ethical guidelines. Future research aims to enhance LLMs' ability to handle multiple inputs for improved intelligence, safety, and reliability. 

<br /><br />Summary: <div>
arXiv:2506.12365v1 Announce Type: new 
Abstract: This survey paper outlines the key developments in the field of Large Language Models (LLMs), such as enhancing their reasoning skills, adaptability to various tasks, increased computational efficiency, and ability to make ethical decisions. The techniques that have been most effective in bridging the gap between human and machine communications include the Chain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from Human Feedback. The improvements in multimodal learning and few-shot or zero-shot techniques have further empowered LLMs to handle complex jobs with minor input. They also manage to do more with less by applying scaling and optimization tricks for computing power conservation. This survey also offers a broader perspective on recent advancements in LLMs going beyond isolated aspects such as model architecture or ethical concerns. It categorizes emerging methods that enhance LLM reasoning, efficiency, and ethical alignment. It also identifies underexplored areas such as interpretability, cross-modal integration and sustainability. With recent progress, challenges like huge computational costs, biases, and ethical risks remain constant. Addressing these requires bias mitigation, transparent decision-making, and clear ethical guidelines. Future research will focus on enhancing models ability to handle multiple input, thereby making them more intelligent, safe, and reliable.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Effect of Knowledge Graph Extraction Error on Downstream Graph Analyses: A Case Study on Affiliation Graphs</title>
<link>https://arxiv.org/abs/2506.12367</link>
<guid>https://arxiv.org/abs/2506.12367</guid>
<content:encoded><![CDATA[
<div> extraction performance, knowledge graph, error analysis, downstream analysis, bias patterns
Summary: 
- The study evaluated knowledge graph (KG) extraction performance on affiliation graphs extracted from social register books.
- Two levels of evaluation were conducted: micro-level edge accuracy and macro-level graph metrics.
- The study identified a range of extraction performance where biases were near zero, but as performance declined, biases in downstream graph analysis metrics became more pronounced.
- Metrics tended toward a consistent direction of either over- or under-estimation as extraction performance worsened.
- Error models commonly used in the literature did not capture these bias patterns, indicating the need for more realistic error models for KG extraction.<br /><br />Summary: <div>
arXiv:2506.12367v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) are useful for analyzing social structures, community dynamics, institutional memberships, and other complex relationships across domains from sociology to public health. While recent advances in large language models (LLMs) have improved the scalability and accessibility of automated KG extraction from large text corpora, the impacts of extraction errors on downstream analyses are poorly understood, especially for applied scientists who depend on accurate KGs for real-world insights. To address this gap, we conducted the first evaluation of KG extraction performance at two levels: (1) micro-level edge accuracy, which is consistent with standard NLP evaluations, and manual identification of common error sources; (2) macro-level graph metrics that assess structural properties such as community detection and connectivity, which are relevant to real-world applications. Focusing on affiliation graphs of person membership in organizations extracted from social register books, our study identifies a range of extraction performance where biases across most downstream graph analysis metrics are near zero. However, as extraction performance declines, we find that many metrics exhibit increasingly pronounced biases, with each metric tending toward a consistent direction of either over- or under-estimation. Through simulations, we further show that error models commonly used in the literature do not capture these bias patterns, indicating the need for more realistic error models for KG extraction. Our findings provide actionable insights for practitioners and underscores the importance of advancing extraction methods and error modeling to ensure reliable and meaningful downstream analyses.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-free LLM Merging for Multi-task Learning</title>
<link>https://arxiv.org/abs/2506.12379</link>
<guid>https://arxiv.org/abs/2506.12379</guid>
<content:encoded><![CDATA[
<div> method, LLMs, multi-task learning, Hi-Merging, performance 

Summary:
The paper introduces Hierarchical Iterative Merging (Hi-Merging), a novel method for combining specialized Large Language Models (LLMs) into a unified model for multi-task learning without the need for additional training. Hi-Merging utilizes model-wise and layer-wise pruning and scaling, guided by contribution analysis, to address parameter conflicts. Extensive experiments on multiple-choice and question-answering tasks in Chinese and English demonstrate the effectiveness of Hi-Merging, outperforming existing merging techniques and models fine-tuned on combined datasets in most scenarios. The results highlight Hi-Merging as a promising approach for unifying specialized LLMs and achieving superior performance in diverse NLP tasks.<br /><br />Summary: <div>
arXiv:2506.12379v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse natural language processing (NLP) tasks. The release of open-source LLMs like LLaMA and Qwen has triggered the development of numerous fine-tuned models tailored for various tasks and languages. In this paper, we explore an important question: is it possible to combine these specialized models to create a unified model with multi-task capabilities. We introduces Hierarchical Iterative Merging (Hi-Merging), a training-free method for unifying different specialized LLMs into a single model. Specifically, Hi-Merging employs model-wise and layer-wise pruning and scaling, guided by contribution analysis, to mitigate parameter conflicts. Extensive experiments on multiple-choice and question-answering tasks in both Chinese and English validate Hi-Merging's ability for multi-task learning. The results demonstrate that Hi-Merging consistently outperforms existing merging techniques and surpasses the performance of models fine-tuned on combined datasets in most scenarios. Code is available at: https://github.com/Applied-Machine-Learning-Lab/Hi-Merging.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recent Advances and Future Directions in Literature-Based Discovery</title>
<link>https://arxiv.org/abs/2506.12385</link>
<guid>https://arxiv.org/abs/2506.12385</guid>
<content:encoded><![CDATA[
<div> Keywords: literature-based discovery, knowledge graph, deep learning, large language models, scientific innovation

Summary: 
This article presents a survey of recent advancements in literature-based discovery (LBD) methods, focusing on the period from 2000 to the present. Three main areas of progress are highlighted: the construction of knowledge graphs, the application of deep learning techniques, and the integration of pre-trained and large language models (LLMs). While LBD has shown significant improvements, challenges such as scalability, reliance on structured data, and the need for extensive manual curation still exist. The survey emphasizes the pivotal role of LLMs in advancing LBD and aims to guide researchers and practitioners in utilizing these technologies to accelerate scientific innovation. <div>
arXiv:2506.12385v1 Announce Type: new 
Abstract: The explosive growth of scientific publications has created an urgent need for automated methods that facilitate knowledge synthesis and hypothesis generation. Literature-based discovery (LBD) addresses this challenge by uncovering previously unknown associations between disparate domains. This article surveys recent methodological advances in LBD, focusing on developments from 2000 to the present. We review progress in three key areas: knowledge graph construction, deep learning approaches, and the integration of pre-trained and large language models (LLMs). While LBD has made notable progress, several fundamental challenges remain unresolved, particularly concerning scalability, reliance on structured data, and the need for extensive manual curation. By examining ongoing advances and outlining promising future directions, this survey underscores the transformative role of LLMs in enhancing LBD and aims to support researchers and practitioners in harnessing these technologies to accelerate scientific innovation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group then Scale: Dynamic Mixture-of-Experts Multilingual Language Model</title>
<link>https://arxiv.org/abs/2506.12388</link>
<guid>https://arxiv.org/abs/2506.12388</guid>
<content:encoded><![CDATA[
<div> parameter grouping, multilingual language models, positive transfer, language similarity, expert modules

Summary:
The study addresses the issue of multilingual Large Language Models (LLMs) facing competition between languages, resulting in inferior performance known as the "curse of multilinguality." By dynamically grouping and scaling up parameters based on language similarity, the proposed method reduces negative transfer among languages and enhances multilingual performance. The model initially learns from monolingual data to assess parameter deviations and language similarities, extending layers to mixture-of-experts for specialized language groups. Experimental results across various languages demonstrate improved performance with fewer parameters. This approach not only benefits new language adaptation but also minimizes reliance on previously learned multilingual knowledge. <div>
arXiv:2506.12388v1 Announce Type: new 
Abstract: The curse of multilinguality phenomenon is a fundamental problem of multilingual Large Language Models (LLMs), where the competition between massive languages results in inferior performance. It mainly comes from limited capacity and negative transfer between dissimilar languages. To address this issue, we propose a method to dynamically group and scale up the parameters of multilingual LLM while boosting positive transfer among similar languages. Specifically, the model is first tuned on monolingual corpus to determine the parameter deviation in each layer and quantify the similarity between languages. Layers with more deviations are extended to mixture-of-experts layers to reduce competition between languages, where one expert module serves one group of similar languages. Experimental results on 18 to 128 languages show that our method reduces the negative transfer between languages and significantly boosts multilingual performance with fewer parameters. Such language group specialization on experts benefits the new language adaptation and reduces the inference on the previous multilingual knowledge learned.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Cultural Variations in Moral Judgments with Large Language Models</title>
<link>https://arxiv.org/abs/2506.12433</link>
<guid>https://arxiv.org/abs/2506.12433</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Moral values, Cultural diversity, Cross-cultural surveys, Instruction tuning

Summary:
Large Language Models (LLMs) have shown strong performance in tasks, but their representation of culturally diverse moral values is uncertain. The study compared older monolingual models with newer instruction-tuned models using cross-cultural survey data on ethical topics. Earlier models often had low correlations with human judgments, while instruction-tuned models like GPT-4o showed higher positive correlations, indicating better alignment with real-world moral attitudes. Scaling up model size and using instruction tuning can improve alignment with cross-cultural norms, but challenges remain. The study emphasizes the need for bias analysis, diverse training data, and strategies to enhance the cultural sensitivity of LLMs. <br /><br />Summary: Large Language Models performance on moral values is examined using cross-cultural survey data. Older models show low correlations with real-world moral attitudes, while newer instruction-tuned models, like GPT-4o, demonstrate higher alignment. Challenges remain, emphasizing the necessity for bias analysis and strategies to enhance cultural sensitivity. <div>
arXiv:2506.12433v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown strong performance across many tasks, but their ability to capture culturally diverse moral values remains unclear. In this paper, we examine whether LLMs can mirror variations in moral attitudes reported by two major cross-cultural surveys: the World Values Survey and the PEW Research Center's Global Attitudes Survey. We compare smaller, monolingual, and multilingual models (GPT-2, OPT, BLOOMZ, and Qwen) with more recent instruction-tuned models (GPT-4o, GPT-4o-mini, Gemma-2-9b-it, and Llama-3.3-70B-Instruct). Using log-probability-based moral justifiability scores, we correlate each model's outputs with survey data covering a broad set of ethical topics. Our results show that many earlier or smaller models often produce near-zero or negative correlations with human judgments. In contrast, advanced instruction-tuned models (including GPT-4o and GPT-4o-mini) achieve substantially higher positive correlations, suggesting they better reflect real-world moral attitudes. While scaling up model size and using instruction tuning can improve alignment with cross-cultural moral norms, challenges remain for certain topics and regions. We discuss these findings in relation to bias analysis, training data diversity, and strategies for improving the cultural sensitivity of LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Outcomes to Processes: Guiding PRM Learning from ORM for Inference-Time Alignment</title>
<link>https://arxiv.org/abs/2506.12446</link>
<guid>https://arxiv.org/abs/2506.12446</guid>
<content:encoded><![CDATA[
<div> alignment methods, large language models, reward-guided search, process reward models, human preferences<br />
<br />
Summary: In this paper, the authors address the issue of granularity mismatch in existing methods for aligning large language models with human preferences using reward-guided search. They propose the integration of process reward models (PRMs) into reward-guided search to tackle the inconsistency in scoring and alignment. The ideal PRM should satisfy Score Consistency and Preference Consistency objectives to ensure coherent evaluation and alignment with human preferences. The proposed SP-PRM framework incorporates both score consistency-based and preference consistency-based partial evaluation modules without the need for human annotation. Extensive experiments on various tasks show that SP-PRM significantly improves GPT-4 evaluation scores by 3.6%-10.3%. This novel approach enhances the effectiveness and efficiency of aligning large language models with human preferences. <br /><br />Summary: <div>
arXiv:2506.12446v1 Announce Type: new 
Abstract: Inference-time alignment methods have gained significant attention for their efficiency and effectiveness in aligning large language models (LLMs) with human preferences. However, existing dominant approaches using reward-guided search (RGS) primarily rely on outcome reward models (ORMs), which suffer from a critical granularity mismatch: ORMs are designed to provide outcome rewards for complete responses, while RGS methods rely on process rewards to guide the policy, leading to inconsistent scoring and suboptimal alignment. To address this challenge, we introduce process reward models (PRMs) into RGS and argue that an ideal PRM should satisfy two objectives: Score Consistency, ensuring coherent evaluation across partial and complete responses, and Preference Consistency, aligning partial sequence assessments with human preferences. Based on these, we propose SP-PRM, a novel dual-consistency framework integrating score consistency-based and preference consistency-based partial evaluation modules without relying on human annotation. Extensive experiments on dialogue, summarization, and reasoning tasks demonstrate that SP-PRM substantially enhances existing RGS methods, achieving a 3.6%-10.3% improvement in GPT-4 evaluation scores across all tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Surgery in Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2506.12450</link>
<guid>https://arxiv.org/abs/2506.12450</guid>
<content:encoded><![CDATA[
<div> alignment, LLMs, language control, cross-lingual, representation

Summary:
Large Language Models (LLMs) have shown exceptional capabilities in natural language processing. This paper investigates the alignment of representations in LLMs, particularly in the middle layers, and its implications for separating language-specific and language-agnostic information. The study confirms the presence of natural alignment and its potential for manipulating language-specific information without affecting semantics. The authors introduce Inference-Time Language Control (ITLC), leveraging latent injection for precise cross-lingual language control and reducing language confusion in LLMs. ITLC demonstrates strong cross-lingual control abilities while maintaining semantic meaning in target languages. Additionally, it effectively addresses the issue of language confusion in current LLMs, which causes inconsistent language generation. This research contributes to understanding representation alignment in LLMs and proposes a practical solution to enhance their cross-lingual performance.

<br /><br />Summary: <div>
arXiv:2506.12450v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across tasks and languages, revolutionizing natural language processing. This paper investigates the naturally emerging representation alignment in LLMs, particularly in the middle layers, and its implications for disentangling language-specific and language-agnostic information. We empirically confirm the existence of this alignment, analyze its behavior in comparison to explicitly designed alignment models, and demonstrate its potential for language-specific manipulation without semantic degradation. Building on these findings, we propose Inference-Time Language Control (ITLC), a novel method that leverages latent injection to enable precise cross-lingual language control and mitigate language confusion in LLMs. Our experiments highlight ITLC's strong cross-lingual control capabilities while preserving semantic integrity in target languages. Furthermore, we demonstrate its effectiveness in alleviating the cross-lingual language confusion problem, which persists even in current large-scale LLMs, leading to inconsistent language generation. This work advances our understanding of representation alignment in LLMs and introduces a practical solution for enhancing their cross-lingual performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Pluggable Multi-Task Learning Framework for Sentiment-Aware Financial Relation Extraction</title>
<link>https://arxiv.org/abs/2506.12452</link>
<guid>https://arxiv.org/abs/2506.12452</guid>
<content:encoded><![CDATA[
<div> Keywords: Relation Extraction, Sentiment-aware-SDP-Enhanced-Module, Multi-task Learning, Financial Domain, Auxiliary Task 

Summary: 
The research introduces a Sentiment-aware-SDP-Enhanced-Module (SSDP-SEM) for enhancing financial Relation Extraction (RE). The module integrates RE models with an auxiliary sentiment perception (ASP) task to consider sentiment in the RE process. It generates sentiment tokens and inserts them into text instances, enabling the models to incorporate sentiment information. The ASP task predicts sentiment token positions while considering the Shortest Dependency Path (SDP) for syntactic information. The work also includes a sentiment attention information bottleneck regularization method to improve reasoning. Integration of the ASP task with existing frameworks shows improved RE results, emphasizing the importance of leveraging sentiment in financial RE tasks. The study demonstrates that incorporating sentiment in RE models can enhance their performance in capturing nuanced relationships in financial texts. 

<br /><br />Summary: <div>
arXiv:2506.12452v1 Announce Type: new 
Abstract: Relation Extraction (RE) aims to extract semantic relationships in texts from given entity pairs, and has achieved significant improvements. However, in different domains, the RE task can be influenced by various factors. For example, in the financial domain, sentiment can affect RE results, yet this factor has been overlooked by modern RE models. To address this gap, this paper proposes a Sentiment-aware-SDP-Enhanced-Module (SSDP-SEM), a multi-task learning approach for enhancing financial RE. Specifically, SSDP-SEM integrates the RE models with a pluggable auxiliary sentiment perception (ASP) task, enabling the RE models to concurrently navigate their attention weights with the text's sentiment. We first generate detailed sentiment tokens through a sentiment model and insert these tokens into an instance. Then, the ASP task focuses on capturing nuanced sentiment information through predicting the sentiment token positions, combining both sentiment insights and the Shortest Dependency Path (SDP) of syntactic information. Moreover, this work employs a sentiment attention information bottleneck regularization method to regulate the reasoning process. Our experiment integrates this auxiliary task with several prevalent frameworks, and the results demonstrate that most previous models benefit from the auxiliary task, thereby achieving better results. These findings highlight the importance of effectively leveraging sentiment in the financial RE task.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TagRouter: Learning Route to LLMs through Tags for Open-Domain Text Generation Tasks</title>
<link>https://arxiv.org/abs/2506.12473</link>
<guid>https://arxiv.org/abs/2506.12473</guid>
<content:encoded><![CDATA[
<div> Keywords: model routing, large language model, TagRouter, text generation tasks, cost-efficiency <br />
Summary: TagRouter is a new training-free model routing method aimed at optimizing the utilization of multiple large language models (LLMs) for open-domain text generation tasks. In comparison to 13 baseline methods, TagRouter was able to increase the system accept rate by 6.15% and reduce costs by 17.20%, resulting in optimal cost-efficiency. This approach addresses the limitations faced by existing routing methods in scalability and adaptability to the growing LLM ecosystem. By providing an efficient and scalable solution for model ensembling, TagRouter offers users a flexible "super model" that can evolve with the changing demands of the system. The experimental results demonstrate the effectiveness of TagRouter in improving system performance and reducing costs, making it a valuable tool for the LLM community seeking to optimize their text generation tasks. <br /><br />Summary: <div>
arXiv:2506.12473v1 Announce Type: new 
Abstract: Model routing allocates queries to the suitable model, improving system performance while reducing costs. However, existing routing methods face practical limitations that hinder scalability in large-scale applications and struggle to keep up with the rapid growth of the large language model (LLM) ecosystem. To tackle these challenges, we propose TagRouter, a training-free model routing method designed to optimize the synergy among multiple LLMs for open-domain text generation tasks. Experimental results demonstrate that TagRouter outperforms 13 baseline methods, increasing the accept rate of system by 6.15% and reducing costs by 17.20%, achieving optimal cost-efficiency. Our findings provides the LLM community with an efficient and scalable solution for model ensembling, offering users an evolvable "super model."
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2506.12494</link>
<guid>https://arxiv.org/abs/2506.12494</guid>
<content:encoded><![CDATA[
<div> Framework, Retrieval-Augmented Generation, FlexRAG, open-source, research<br />
<br />
Summary:
FlexRAG is an open-source framework designed to address challenges in modern large language model applications. It enables the development of Retrieval-Augmented Generation (RAG) systems by supporting text-based, multimodal, and network-based RAG. The framework offers comprehensive lifecycle support, efficient asynchronous processing, and persistent caching capabilities. By providing a flexible and robust solution, FlexRAG aims to help researchers rapidly prototype, deploy, and share advanced RAG systems. The framework is designed to tackle difficulties in algorithm reproduction and sharing, the lack of new techniques, and high system overhead present in existing frameworks. Researchers can access the toolkit and resources on GitHub to leverage the capabilities of FlexRAG for their RAG-related research and development projects. <br /><br />Summary: <div>
arXiv:2506.12494v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large language model applications, with numerous existing frameworks offering a wide range of functionalities to facilitate the development of RAG systems. However, we have identified several persistent challenges in these frameworks, including difficulties in algorithm reproduction and sharing, lack of new techniques, and high system overhead. To address these limitations, we introduce \textbf{FlexRAG}, an open-source framework specifically designed for research and prototyping. FlexRAG supports text-based, multimodal, and network-based RAG, providing comprehensive lifecycle support alongside efficient asynchronous processing and persistent caching capabilities. By offering a robust and flexible solution, FlexRAG enables researchers to rapidly develop, deploy, and share advanced RAG systems. Our toolkit and resources are available at \href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Factuality for Dialogue Response Generation via Graph-Based Knowledge Augmentation</title>
<link>https://arxiv.org/abs/2506.12496</link>
<guid>https://arxiv.org/abs/2506.12496</guid>
<content:encoded><![CDATA[
<div> knowledge-augmented methods, dialogue response generation, factuality, fact score, factual consistency <br />
Summary:
Large Language Models (LLMs) are powerful in natural language processing tasks but often suffer from generating inaccurate text. A new framework is introduced to enhance the factuality of dialogue response generation by combining a knowledge triple retriever, dialogue rewrite, and knowledge-enhanced response generation. An improved fact score evaluation method is proposed to assess dialogue factual accuracy more reliably. The methods are tested on the OpendialKG and HybriDialogue datasets and show significant improvements in factuality compared to existing graph knowledge-augmentation techniques. The approach aims to reduce the issue of hallucinations and produce more accurate and grounded dialogue responses. The code for the framework will be made available on GitHub to facilitate further research and development in this area. <br /> <div>
arXiv:2506.12496v1 Announce Type: new 
Abstract: Large Language Models (LLMs) succeed in many natural language processing tasks. However, their tendency to hallucinate - generate plausible but inconsistent or factually incorrect text - can cause problems in certain tasks, including response generation in dialogue. To mitigate this issue, knowledge-augmented methods have shown promise in reducing hallucinations. Here, we introduce a novel framework designed to enhance the factuality of dialogue response generation, as well as an approach to evaluate dialogue factual accuracy. Our framework combines a knowledge triple retriever, a dialogue rewrite, and knowledge-enhanced response generation to produce more accurate and grounded dialogue responses. To further evaluate generated responses, we propose a revised fact score that addresses the limitations of existing fact-score methods in dialogue settings, providing a more reliable assessment of factual consistency. We evaluate our methods using different baselines on the OpendialKG and HybriDialogue datasets. Our methods significantly improve factuality compared to other graph knowledge-augmentation baselines, including the state-of-the-art G-retriever. The code will be released on GitHub.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Fairness Assessment of Dutch Hate Speech Detection</title>
<link>https://arxiv.org/abs/2506.12502</link>
<guid>https://arxiv.org/abs/2506.12502</guid>
<content:encoded><![CDATA[
<div> Keywords: hate speech detection, Dutch language, transformer-based models, counterfactual fairness, group fairness metrics<br />
<br />
Summary: <br />
This study evaluates the counterfactual fairness of hate speech detection models in the Dutch language, focusing on transformer-based models. The researchers curated a list of Dutch Social Group Terms and generated counterfactual data for Dutch hate speech. They fine-tuned baseline transformer-based models and evaluated their performance in hate speech detection. The fairness of these models was assessed using Counterfactual Token Fairness and group fairness metrics. The analysis revealed that the models performed well in detecting hate speech and exhibited average counterfactual fairness and group fairness. The study addresses a gap in the literature on counterfactual fairness for hate speech detection in Dutch and provides insights for enhancing both model performance and fairness. <br /> <div>
arXiv:2506.12502v1 Announce Type: new 
Abstract: Numerous studies have proposed computational methods to detect hate speech online, yet most focus on the English language and emphasize model development. In this study, we evaluate the counterfactual fairness of hate speech detection models in the Dutch language, specifically examining the performance and fairness of transformer-based models. We make the following key contributions. First, we curate a list of Dutch Social Group Terms that reflect social context. Second, we generate counterfactual data for Dutch hate speech using LLMs and established strategies like Manual Group Substitution (MGS) and Sentence Log-Likelihood (SLL). Through qualitative evaluation, we highlight the challenges of generating realistic counterfactuals, particularly with Dutch grammar and contextual coherence. Third, we fine-tune baseline transformer-based models with counterfactual data and evaluate their performance in detecting hate speech. Fourth, we assess the fairness of these models using Counterfactual Token Fairness (CTF) and group fairness metrics, including equality of odds and demographic parity. Our analysis shows that models perform better in terms of hate speech detection, average counterfactual fairness and group fairness. This work addresses a significant gap in the literature on counterfactual fairness for hate speech detection in Dutch and provides practical insights and recommendations for improving both model performance and fairness.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection, Classification, and Mitigation of Gender Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2506.12527</link>
<guid>https://arxiv.org/abs/2506.12527</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, gender bias detection, reinforcement learning, chain-of-thoughts reasoning, NLPCC 2025

Summary: 
In the study, the focus is on enhancing the capabilities of large language models (LLMs) in detecting, classifying, and mitigating gender bias. The researchers utilized reinforcement learning, chain-of-thoughts (CoT) reasoning, and supervised fine-tuning for different subtasks. For Subtasks 1 and 2, they employed LLMs' internal reasoning abilities to guide multi-step thinking, simplifying biased queries and improving response accuracy. Subtask 3 involved using reinforcement learning to annotate a preference dataset and mitigate gender bias using Direct Preference Optimization (DPO). The approach implemented in the NLPCC 2025 Shared Task 7 Challenge ranked first in all three subtasks, showcasing effective methods for addressing gender bias in LLMs.<br /><br />Summary: <div>
arXiv:2506.12527v1 Announce Type: new 
Abstract: With the rapid development of large language models (LLMs), they have significantly improved efficiency across a wide range of domains. However, recent studies have revealed that LLMs often exhibit gender bias, leading to serious social implications. Detecting, classifying, and mitigating gender bias in LLMs has therefore become a critical research focus. In the NLPCC 2025 Shared Task 7: Chinese Corpus for Gender Bias Detection, Classification and Mitigation Challenge, we investigate how to enhance the capabilities of LLMs in gender bias detection, classification, and mitigation. We adopt reinforcement learning, chain-of-thoughts (CoT) reasoning, and supervised fine-tuning to handle different Subtasks. Specifically, for Subtasks 1 and 2, we leverage the internal reasoning capabilities of LLMs to guide multi-step thinking in a staged manner, which simplifies complex biased queries and improves response accuracy. For Subtask 3, we employ a reinforcement learning-based approach, annotating a preference dataset using GPT-4. We then apply Direct Preference Optimization (DPO) to mitigate gender bias by introducing a loss function that explicitly favors less biased completions over biased ones. Our approach ranked first across all three subtasks of the NLPCC 2025 Shared Task 7.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech-Language Models with Decoupled Tokenizers and Multi-Token Prediction</title>
<link>https://arxiv.org/abs/2506.12537</link>
<guid>https://arxiv.org/abs/2506.12537</guid>
<content:encoded><![CDATA[
<div> tokenizer, alignment, speech generation, multi-token prediction, speaker modeling 
Summary: 
- The study focuses on improving performance of Speech-language models (SLMs) by investigating key components such as speech tokenizers, speech heads, and speaker modeling. 
- Decoupled tokenization is found to significantly enhance alignment and synthesis quality in SLMs. 
- Introducing multi-token prediction (MTP) into SLMs leads to faster decoding and reduced word error rate. 
- A speaker-aware generation paradigm is proposed, along with RoleTriviaQA benchmark for knowledge QA with diverse speaker identities. 
- Experimental results demonstrate that the methods improve knowledge understanding and speaker consistency. 
<br /><br />Summary: <div>
arXiv:2506.12537v1 Announce Type: new 
Abstract: Speech-language models (SLMs) offer a promising path toward unifying speech and text understanding and generation. However, challenges remain in achieving effective cross-modal alignment and high-quality speech generation. In this work, we systematically investigate the impact of key components (i.e., speech tokenizers, speech heads, and speaker modeling) on the performance of LLM-centric SLMs. We compare coupled, semi-decoupled, and fully decoupled speech tokenizers under a fair SLM framework and find that decoupled tokenization significantly improves alignment and synthesis quality. To address the information density mismatch between speech and text, we introduce multi-token prediction (MTP) into SLMs, enabling each hidden state to decode multiple speech tokens. This leads to up to 12$\times$ faster decoding and a substantial drop in word error rate (from 6.07 to 3.01). Furthermore, we propose a speaker-aware generation paradigm and introduce RoleTriviaQA, a large-scale role-playing knowledge QA benchmark with diverse speaker identities. Experiments demonstrate that our methods enhance both knowledge understanding and speaker consistency.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealFactBench: A Benchmark for Evaluating Large Language Models in Real-World Fact-Checking</title>
<link>https://arxiv.org/abs/2506.12538</link>
<guid>https://arxiv.org/abs/2506.12538</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Fact-checking, RealFactBench, Multimodal Large Language Models, Benchmark

Summary:
Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have the potential to enhance fact-checking by utilizing their reasoning, evidence retrieval, and explanation generation capabilities. However, existing benchmarks do not fully evaluate these models in real-world misinformation scenarios. To address this gap, the RealFactBench benchmark is introduced, comprising 6,000 high-quality claims from authoritative sources across various tasks such as Knowledge Validation, Rumor Detection, and Event Verification. This benchmark includes multimodal content and diverse domains. The evaluation framework of RealFactBench introduces the Unknown Rate (UnR) metric, allowing a more nuanced assessment of models in handling uncertainty and maintaining a balance between over-conservatism and over-confidence. Experiments on 7 LLMs and 4 MLLMs demonstrate the limitations of these models in real-world fact-checking scenarios, providing valuable insights for future research.<br /><br />Summary: <div>
arXiv:2506.12538v1 Announce Type: new 
Abstract: Large Language Models (LLMs) hold significant potential for advancing fact-checking by leveraging their capabilities in reasoning, evidence retrieval, and explanation generation. However, existing benchmarks fail to comprehensively evaluate LLMs and Multimodal Large Language Models (MLLMs) in realistic misinformation scenarios. To bridge this gap, we introduce RealFactBench, a comprehensive benchmark designed to assess the fact-checking capabilities of LLMs and MLLMs across diverse real-world tasks, including Knowledge Validation, Rumor Detection, and Event Verification. RealFactBench consists of 6K high-quality claims drawn from authoritative sources, encompassing multimodal content and diverse domains. Our evaluation framework further introduces the Unknown Rate (UnR) metric, enabling a more nuanced assessment of models' ability to handle uncertainty and balance between over-conservatism and over-confidence. Extensive experiments on 7 representative LLMs and 4 MLLMs reveal their limitations in real-world fact-checking and offer valuable insights for further research. RealFactBench is publicly available at https://github.com/kalendsyang/RealFactBench.git.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Profiling News Media for Factuality and Bias Using LLMs and the Fact-Checking Methodology of Human Experts</title>
<link>https://arxiv.org/abs/2506.12552</link>
<guid>https://arxiv.org/abs/2506.12552</guid>
<content:encoded><![CDATA[
<div> Methodology, factuality, political bias, language models, dataset 
<br />
Summary: 
In the age of online misinformation, understanding the reliability and political bias of news sources is crucial. Existing fact-checking efforts often focus on individual claims, but this study examines entire news outlets. Using prompts based on professional fact-checker criteria, responses from large language models are aggregated to predict factuality and bias. Extensive experiments show significant improvements over baseline models, with an analysis of errors related to media popularity and region. An ablation study identifies key dataset components that contribute to model performance. The dataset and code are released for future research. 
<br /> <div>
arXiv:2506.12552v1 Announce Type: new 
Abstract: In an age characterized by the proliferation of mis- and disinformation online, it is critical to empower readers to understand the content they are reading. Important efforts in this direction rely on manual or automatic fact-checking, which can be challenging for emerging claims with limited information. Such scenarios can be handled by assessing the reliability and the political bias of the source of the claim, i.e., characterizing entire news outlets rather than individual claims or articles. This is an important but understudied research direction. While prior work has looked into linguistic and social contexts, we do not analyze individual articles or information in social media. Instead, we propose a novel methodology that emulates the criteria that professional fact-checkers use to assess the factuality and political bias of an entire outlet. Specifically, we design a variety of prompts based on these criteria and elicit responses from large language models (LLMs), which we aggregate to make predictions. In addition to demonstrating sizable improvements over strong baselines via extensive experiments with multiple LLMs, we provide an in-depth error analysis of the effect of media popularity and region on model performance. Further, we conduct an ablation study to highlight the key components of our dataset that contribute to these improvements. To facilitate future research, we released our dataset and code at https://github.com/mbzuai-nlp/llm-media-profiling.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DoTA-RAG: Dynamic of Thought Aggregation RAG</title>
<link>https://arxiv.org/abs/2506.12571</link>
<guid>https://arxiv.org/abs/2506.12571</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented generation, high-throughput, dynamic routing, large-scale web knowledge indexes, Q&amp;A dataset

Summary:<br /><br />DoTA-RAG is a retrieval-augmented generation system designed for high-throughput, large-scale web knowledge indexes. It addresses challenges faced by traditional pipelines through query rewriting, dynamic routing to specialized sub-indexes, and multi-stage retrieval and ranking. The system improves answer correctness scores significantly, showcasing its potential for practical deployment in domains requiring fast and reliable access to vast knowledge sources. DoTA-RAG achieves this by utilizing a superior embedding model and re-embedding a large corpus, as well as creating a diverse Q&amp;A dataset generated across a broad range of topics and formats. Notably, it enhances the answer correctness score from 0.752 to 1.478 compared to the baseline, while maintaining low latency. Additionally, DoTA-RAG achieves a 0.929 correctness score on the Live Challenge Day, demonstrating its effectiveness in real-world applications. <div>
arXiv:2506.12571v1 Announce Type: new 
Abstract: In this paper, we introduce DoTA-RAG (Dynamic-of-Thought Aggregation RAG), a retrieval-augmented generation system optimized for high-throughput, large-scale web knowledge indexes. Traditional RAG pipelines often suffer from high latency and limited accuracy over massive, diverse datasets. DoTA-RAG addresses these challenges with a three-stage pipeline: query rewriting, dynamic routing to specialized sub-indexes, and multi-stage retrieval and ranking. We further enhance retrieval by evaluating and selecting a superior embedding model, re-embedding the large FineWeb-10BT corpus. Moreover, we create a diverse Q&amp;A dataset of 500 questions generated via the DataMorgana setup across a broad range of WebOrganizer topics and formats. DoTA-RAG improves the answer correctness score from 0.752 (baseline, using LiveRAG pre-built vector store) to 1.478 while maintaining low latency, and it achieves a 0.929 correctness score on the Live Challenge Day. These results highlight DoTA-RAG's potential for practical deployment in domains requiring fast, reliable access to large and evolving knowledge sources.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overview of the NLPCC 2025 Shared Task: Gender Bias Mitigation Challenge</title>
<link>https://arxiv.org/abs/2506.12574</link>
<guid>https://arxiv.org/abs/2506.12574</guid>
<content:encoded><![CDATA[
<div> gender bias, natural language processing, Chinese, corpus, mitigation<br />
Summary:<br />
The article introduces a new Chinese corpus, CORGI-PM, designed for investigating and addressing gender bias in natural language processing. The corpus contains 32.9k sentences with labels focused on gender bias in the Chinese context. It includes 5.2k gender-biased sentences and their bias-eliminated versions. The authors set up three challenges as a shared task to automate the detection, classification, and mitigation of textual gender bias. The results and analysis of the teams participating in the shared task at NLPCC 2025 are presented in the literature. The research highlights the importance of addressing bias in data-driven techniques like pre-trained language models, particularly in languages with fewer fairness-related computational resources like Chinese. <div>
arXiv:2506.12574v1 Announce Type: new 
Abstract: As natural language processing for gender bias becomes a significant interdisciplinary topic, the prevalent data-driven techniques, such as pre-trained language models, suffer from biased corpus. This case becomes more obvious regarding those languages with less fairness-related computational linguistic resources, such as Chinese. To this end, we propose a Chinese cOrpus foR Gender bIas Probing and Mitigation (CORGI-PM), which contains 32.9k sentences with high-quality labels derived by following an annotation scheme specifically developed for gender bias in the Chinese context. It is worth noting that CORGI-PM contains 5.2k gender-biased sentences along with the corresponding bias-eliminated versions rewritten by human annotators. We pose three challenges as a shared task to automate the mitigation of textual gender bias, which requires the models to detect, classify, and mitigate textual gender bias. In the literature, we present the results and analysis for the teams participating this shared task in NLPCC 2025.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Precise Topic Alignment in Large Language Models Via Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2506.12576</link>
<guid>https://arxiv.org/abs/2506.12576</guid>
<content:encoded><![CDATA[
<div> Keywords: Sparse Autoencoders, Large Language Model, Alignment, Interpretability, Medical prompts

Summary:<br /><br />
Recent research has demonstrated that Sparse Autoencoders (SAE) applied to Large Language Model (LLM) layers can identify interpretable concepts. This study introduces a novel approach that leverages SAEs to align LLM outputs with any given topic. The method involves scoring SAE neurons based on their semantic similarity to an alignment text and modifying SAE-layer-level outputs by emphasizing topic-aligned neurons. Experiments on various public topic datasets, including Amazon reviews, Medicine, and Sycophancy, using GPT2 and Gemma models with different SAE configurations, show promising results. Aligning to medical prompts exhibited benefits such as increased language acceptability, reduced training time across different topics, and acceptable inference time for various applications. The open-source code for this approach is available on github.com/IBM/sae-steering. <div>
arXiv:2506.12576v1 Announce Type: new 
Abstract: Recent work shows that Sparse Autoencoders (SAE) applied to large language model (LLM) layers have neurons corresponding to interpretable concepts. These SAE neurons can be modified to align generated outputs, but only towards pre-identified topics and with some parameter tuning. Our approach leverages the observational and modification properties of SAEs to enable alignment for any topic. This method 1) scores each SAE neuron by its semantic similarity to an alignment text and uses them to 2) modify SAE-layer-level outputs by emphasizing topic-aligned neurons. We assess the alignment capabilities of this approach on diverse public topic datasets including Amazon reviews, Medicine, and Sycophancy, across the currently available open-source LLMs and SAE pairs (GPT2 and Gemma) with multiple SAEs configurations. Experiments aligning to medical prompts reveal several benefits over fine-tuning, including increased average language acceptability (0.25 vs. 0.5), reduced training time across multiple alignment topics (333.6s vs. 62s), and acceptable inference time for many applications (+0.00092s/token). Our open-source code is available at github.com/IBM/sae-steering.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OneEval: Benchmarking LLM Knowledge-intensive Reasoning over Diverse Knowledge Bases</title>
<link>https://arxiv.org/abs/2506.12577</link>
<guid>https://arxiv.org/abs/2506.12577</guid>
<content:encoded><![CDATA[
<div> benchmark, large language models, structured knowledge, reasoning, evaluation

Summary:
Persistent limitations in structured reasoning are observed in large language models, with even the strongest model achieving only 32.2% accuracy on challenging cases. Performance declines as the complexity of the knowledge base increases, dropping from 53% for textual reasoning to 25% for formal logic. There are diminishing returns from extended reasoning chains, emphasizing the importance of adapting reasoning depth to task complexity. The introduced OneEval benchmark evaluates knowledge-intensive reasoning capabilities of LLMs across various structured knowledge modalities and domains. It comprises 4,019 instances, including a difficult subset, OneEvalHard. Through evaluation of 18 LLMs, the study sheds light on the challenges and need for improvements in structured knowledge reasoning. The OneEval datasets, evaluation scripts, and baseline results are publicly released, along with a leaderboard to foster advancements in structured knowledge reasoning. 

<br /><br />Summary: <div>
arXiv:2506.12577v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated substantial progress on reasoning tasks involving unstructured text, yet their capabilities significantly deteriorate when reasoning requires integrating structured external knowledge such as knowledge graphs, code snippets, or formal logic. This limitation is partly due to the absence of benchmarks capable of systematically evaluating LLM performance across diverse structured knowledge modalities. To address this gap, we introduce \textbf{\textsc{OneEval}}, a comprehensive benchmark explicitly designed to assess the knowledge-intensive reasoning capabilities of LLMs across four structured knowledge modalities, unstructured text, knowledge graphs, code, and formal logic, and five critical domains (general knowledge, government, science, law, and programming). \textsc{OneEval} comprises 4,019 carefully curated instances and includes a challenging subset, \textsc{OneEval}\textsubscript{Hard}, consisting of 1,285 particularly difficult cases. Through extensive evaluation of 18 state-of-the-art open-source and proprietary LLMs, we establish three core findings: a) \emph{persistent limitations in structured reasoning}, with even the strongest model achieving only 32.2\% accuracy on \textsc{OneEval}\textsubscript{Hard}; b) \emph{performance consistently declines as the structural complexity of the knowledge base increases}, with accuracy dropping sharply from 53\% (textual reasoning) to 25\% (formal logic); and c) \emph{diminishing returns from extended reasoning chains}, highlighting the critical need for models to adapt reasoning depth appropriately to task complexity. We release the \textsc{OneEval} datasets, evaluation scripts, and baseline results publicly, accompanied by a leaderboard to facilitate ongoing advancements in structured knowledge reasoning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Exploration of Mamba for Speech Self-Supervised Models</title>
<link>https://arxiv.org/abs/2506.12606</link>
<guid>https://arxiv.org/abs/2506.12606</guid>
<content:encoded><![CDATA[
<div> SSL, Mamba, HuBERT, ASR, representations

Summary: 
Mamba, known for strong language modeling, is explored for speech self-supervised (SSL) tasks. HuBERT models, based on Mamba, offer an alternative to Transformer-based SSL architectures, allowing for fine-tuning on long-context ASR with less compute and better performance in streaming ASR. These models also perform well on SUPERB probing benchmarks, especially in causal settings, producing high-quality quantized representations and capturing speaker-related features effectively. The study indicates that Mamba-based SSL models show promise for long-sequence modeling, real-time speech modeling, and speech unit extraction. 

<br /><br /> <div>
arXiv:2506.12606v1 Announce Type: new 
Abstract: While Mamba has demonstrated strong performance in language modeling, its potential as a speech self-supervised (SSL) model remains underexplored, with prior studies limited to isolated tasks. To address this, we explore Mamba-based HuBERT models as alternatives to Transformer-based SSL architectures. Leveraging the linear-time Selective State Space, these models enable fine-tuning on long-context ASR with significantly lower compute. Moreover, they show superior performance when fine-tuned for streaming ASR. Beyond fine-tuning, these models show competitive performance on SUPERB probing benchmarks, particularly in causal settings. Our analysis shows that they yield higher-quality quantized representations and capture speaker-related features more distinctly than Transformer-based models. These findings highlight Mamba-based SSL as a promising and complementary direction for long-sequence modeling, real-time speech modeling, and speech unit extraction.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Building General Purpose Embedding Models for Industry 4.0 Agents</title>
<link>https://arxiv.org/abs/2506.12607</link>
<guid>https://arxiv.org/abs/2506.12607</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, asset maintenance, Industry 4.0, reasoning and acting agent, knowledge base <br />
Summary: 
- The study focuses on enhancing language models' comprehension for asset maintenance in Industry 4.0, aiming to advise engineers and reduce asset downtime.
- A knowledge base was created with nine asset-specific task datasets, and Large Language Models were used to augment input tasks for better embeddings.
- Integration with a Reasoning and Acting agent (ReAct) enabled addressing complex user queries requiring multi-step reasoning and knowledge inference.
- Ablation studies showed improvements with LLM query augmentation, the use of Contrastive loss, and balanced positive and negative in-batch samples.
- Significant enhancements were observed in model performance across various metrics, showcasing its planning and tool invocation capabilities for supporting Subject Matter Experts in industrial asset maintenance. 

<br /><br />Summary: <div>
arXiv:2506.12607v1 Announce Type: new 
Abstract: In this work we focus on improving language models' understanding for asset maintenance to guide the engineer's decisions and minimize asset downtime. Given a set of tasks expressed in natural language for Industry 4.0 domain, each associated with queries related to a specific asset, we want to recommend relevant items and generalize to queries of similar assets. A task may involve identifying relevant sensors given a query about an asset's failure mode.
  Our approach begins with gathering a qualitative, expert-vetted knowledge base to construct nine asset-specific task datasets. To create more contextually informed embeddings, we augment the input tasks using Large Language Models (LLMs), providing concise descriptions of the entities involved in the queries. This embedding model is then integrated with a Reasoning and Acting agent (ReAct), which serves as a powerful tool for answering complex user queries that require multi-step reasoning, planning, and knowledge inference.
  Through ablation studies, we demonstrate that: (a) LLM query augmentation improves the quality of embeddings, (b) Contrastive loss and other methods that avoid in-batch negatives are superior for datasets with queries related to many items, and (c) It is crucial to balance positive and negative in-batch samples. After training and testing on our dataset, we observe a substantial improvement: HIT@1 increases by +54.2%, MAP@100 by +50.1%, and NDCG@10 by +54.7%, averaged across all tasks and models. Additionally, we empirically demonstrate the model's planning and tool invocation capabilities when answering complex questions related to industrial asset maintenance, showcasing its effectiveness in supporting Subject Matter Experts (SMEs) in their day-to-day operations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Konooz: Multi-domain Multi-dialect Corpus for Named Entity Recognition</title>
<link>https://arxiv.org/abs/2506.12615</link>
<guid>https://arxiv.org/abs/2506.12615</guid>
<content:encoded><![CDATA[
<div> corpus, Arabic dialects, Named Entity Recognition, benchmarking, domain adaptation <br />
<br />
Summary: 
The article introduces Konooz, a multi-dimensional corpus covering 16 Arabic dialects and 10 domains, with 777k tokens manually annotated with 21 entity types. It aims to benchmark existing Arabic Named Entity Recognition (NER) models, highlighting a significant performance drop of up to 38% on cross-domain and cross-dialect data. The study delves into domain and dialect divergence, resource scarcity impact, and model performance variations. By measuring overlap between domains and dialects using Maximum Mean Discrepancy (MMD), insights are gained into why certain NER models excel on specific dialects and domains. Konooz is openly accessible and beneficial for NLP tasks like domain adaptation and transfer learning. <div>
arXiv:2506.12615v1 Announce Type: new 
Abstract: We introduce Konooz, a novel multi-dimensional corpus covering 16 Arabic dialects across 10 domains, resulting in 160 distinct corpora. The corpus comprises about 777k tokens, carefully collected and manually annotated with 21 entity types using both nested and flat annotation schemes - using the Wojood guidelines. While Konooz is useful for various NLP tasks like domain adaptation and transfer learning, this paper primarily focuses on benchmarking existing Arabic Named Entity Recognition (NER) models, especially cross-domain and cross-dialect model performance. Our benchmarking of four Arabic NER models using Konooz reveals a significant drop in performance of up to 38% when compared to the in-distribution data. Furthermore, we present an in-depth analysis of domain and dialect divergence and the impact of resource scarcity. We also measured the overlap between domains and dialects using the Maximum Mean Discrepancy (MMD) metric, and illustrated why certain NER models perform better on specific dialects and domains. Konooz is open-source and publicly available at https://sina.birzeit.edu/wojood/#download
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics</title>
<link>https://arxiv.org/abs/2506.12618</link>
<guid>https://arxiv.org/abs/2506.12618</guid>
<content:encoded><![CDATA[
<div> Keywords: unlearning, large language models, benchmarking, evaluation metrics, research

Summary: 
The article introduces the concept of robust unlearning for large language models (LLMs) to ensure data privacy, model safety, and regulatory compliance. It highlights the challenges in accurately measuring unlearning and the lack of standardized methodologies and evaluation metrics in current research. To address these issues, the OpenUnlearning framework is proposed, which includes 9 unlearning algorithms, 16 evaluations across 3 benchmarks, and the release of 450+ checkpoints for analysis. The framework also introduces a meta-evaluation benchmark to assess the reliability of evaluation metrics. By benchmarking various unlearning methods and evaluating them against a comprehensive suite, the article aims to establish a clear pathway for rigorous research in LLM unlearning. 

<br /><br />Summary: <div>
arXiv:2506.12618v1 Announce Type: new 
Abstract: Robust unlearning is crucial for safely deploying large language models (LLMs) in environments where data privacy, model safety, and regulatory compliance must be ensured. Yet the task is inherently challenging, partly due to difficulties in reliably measuring whether unlearning has truly occurred. Moreover, fragmentation in current methodologies and inconsistent evaluation metrics hinder comparative analysis and reproducibility. To unify and accelerate research efforts, we introduce OpenUnlearning, a standardized and extensible framework designed explicitly for benchmarking both LLM unlearning methods and metrics. OpenUnlearning integrates 9 unlearning algorithms and 16 diverse evaluations across 3 leading benchmarks (TOFU, MUSE, and WMDP) and also enables analyses of forgetting behaviors across 450+ checkpoints we publicly release. Leveraging OpenUnlearning, we propose a novel meta-evaluation benchmark focused specifically on assessing the faithfulness and robustness of evaluation metrics themselves. We also benchmark diverse unlearning methods and provide a comparative analysis against an extensive evaluation suite. Overall, we establish a clear, community-driven pathway toward rigorous development in LLM unlearning research.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Between Predictability and Randomness: Seeking Artistic Inspiration from AI Generative Models</title>
<link>https://arxiv.org/abs/2506.12634</link>
<guid>https://arxiv.org/abs/2506.12634</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated poetry, creativity, LSTM-VAE, indeterminacy, artistic expression 

Summary: 
This paper discusses the use of AI-generated poetic lines as a source of inspiration for artists. It compares lines generated by Long Short-Term Memory Variational Autoencoders (LSTM-VAE) with poems created by Large Language Models (LLMs). The study finds that LSTM-VAE lines evoke creativity through a combination of resonant imagery and productive indeterminacy, allowing for semantic openness and unconventional combinations. In contrast, LLMs produce technically skilled but conventional poetry. The author demonstrates how engaging with LSTM-VAE lines led to the organic emergence of narrative in an original poem, showcasing how these lines can be valuable starting points for genuine artistic expression.<br /><br />Summary: <div>
arXiv:2506.12634v1 Announce Type: new 
Abstract: Artistic inspiration often emerges from language that is open to interpretation. This paper explores the use of AI-generated poetic lines as stimuli for creativity. Through analysis of two generative AI approaches--lines generated by Long Short-Term Memory Variational Autoencoders (LSTM-VAE) and complete poems by Large Language Models (LLMs)--I demonstrate that LSTM-VAE lines achieve their evocative impact through a combination of resonant imagery and productive indeterminacy. While LLMs produce technically accomplished poetry with conventional patterns, LSTM-VAE lines can engage the artist through semantic openness, unconventional combinations, and fragments that resist closure. Through the composition of an original poem, where narrative emerged organically through engagement with LSTM-VAE generated lines rather than following a predetermined structure, I demonstrate how these characteristics can serve as evocative starting points for authentic artistic expression.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Grounded is Wikipedia? A Study on Structured Evidential Support</title>
<link>https://arxiv.org/abs/2506.12637</link>
<guid>https://arxiv.org/abs/2506.12637</guid>
<content:encoded><![CDATA[
<div> people, Wikipedia, NLP, claim support, annotation 

Summary:
Approximately 20% of claims in Wikipedia lead sections lack support in the article body. Around 27% of annotated claims in the article body lack support from publicly accessible cited sources. Moreover, over 80% of lead claims cannot be traced back to these sources using annotated body evidence. Recovery of complex grounding evidence for supported claims also poses a challenge for standard retrieval methods. The study emphasizes the importance of ensuring the groundedness of information on Wikipedia, especially in the context of NLP tasks that rely on accurate and reliable information. The dataset PeopleProfiles is introduced as a tool for analyzing claim support annotations on Wikipedia articles of notable individuals. The findings suggest a need for improved methods to verify and validate information sourced from Wikipedia for enhanced data reliability in NLP applications. 

<br /><br />Summary: <div>
arXiv:2506.12637v1 Announce Type: new 
Abstract: Wikipedia is a critical resource for modern NLP, serving as a rich repository of up-to-date and citation-backed information on a wide variety of subjects. The reliability of Wikipedia -- its groundedness in its cited sources -- is vital to this purpose. This work provides a quantitative analysis of the extent to which Wikipedia *is* so grounded and of how readily grounding evidence may be retrieved. To this end, we introduce PeopleProfiles -- a large-scale, multi-level dataset of claim support annotations on Wikipedia articles of notable people. We show that roughly 20% of claims in Wikipedia *lead* sections are unsupported by the article body; roughly 27% of annotated claims in the article *body* are unsupported by their (publicly accessible) cited sources; and >80% of lead claims cannot be traced to these sources via annotated body evidence. Further, we show that recovery of complex grounding evidence for claims that *are* supported remains a challenge for standard retrieval methods.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Socratic Debates: Examining Persona Effects on Moral Decision and Persuasion Dynamics</title>
<link>https://arxiv.org/abs/2506.12657</link>
<guid>https://arxiv.org/abs/2506.12657</guid>
<content:encoded><![CDATA[
arXiv:2506.12657v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly used in morally sensitive domains, it is crucial to understand how persona traits affect their moral reasoning and persuasive behavior. We present the first large-scale study of multi-dimensional persona effects in AI-AI debates over real-world moral dilemmas. Using a 6-dimensional persona space (age, gender, country, class, ideology, and personality), we simulate structured debates between AI agents over 131 relationship-based cases. Our results show that personas affect initial moral stances and debate outcomes, with political ideology and personality traits exerting the strongest influence. Persuasive success varies across traits, with liberal and open personalities reaching higher consensus and win rates. While logit-based confidence grows during debates, emotional and credibility-based appeals diminish, indicating more tempered argumentation over time. These trends mirror findings from psychology and cultural studies, reinforcing the need for persona-aware evaluation frameworks for AI moral reasoning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Clinical Models with Pseudo Data for De-identification</title>
<link>https://arxiv.org/abs/2506.12674</link>
<guid>https://arxiv.org/abs/2506.12674</guid>
<content:encoded><![CDATA[
arXiv:2506.12674v1 Announce Type: new 
Abstract: Many models are pretrained on redacted text for privacy reasons. Clinical foundation models are often trained on de-identified text, which uses special syntax (masked) text in place of protected health information. Even though these models have increased in popularity, there has been little effort in understanding the effects of training them on redacted text. In this work, we pretrain several encoder-only models on a dataset that contains redacted text and a version with replaced realistic pseudo text. We then fine-tuned models for the protected health information de-identification task and show how our methods significantly outperform previous baselines. The contributions of this work include: a) our novel, and yet surprising findings with training recommendations, b) redacted text replacements used to produce the pseudo dataset, c) pretrained embeddings and fine-tuned task specific models, and d) freely available pseudo training dataset generation and model source code used in our experiments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Realignment of Language Models</title>
<link>https://arxiv.org/abs/2506.12704</link>
<guid>https://arxiv.org/abs/2506.12704</guid>
<content:encoded><![CDATA[
arXiv:2506.12704v1 Announce Type: new 
Abstract: Realignment becomes necessary when a language model (LM) fails to meet expected performance. We propose a flexible realignment framework that supports quantitative control of alignment degree during training and inference. This framework incorporates Training-time Realignment (TrRa), which efficiently realigns the reference model by leveraging the controllable fusion of logits from both the reference and already aligned models. For example, TrRa reduces token usage by 54.63% on DeepSeek-R1-Distill-Qwen-1.5B without any performance degradation, outperforming DeepScaleR-1.5B's 33.86%. To complement TrRa during inference, we introduce a layer adapter that enables smooth Inference-time Realignment (InRa). This adapter is initialized to perform an identity transformation at the bottom layer and is inserted preceding the original layers. During inference, input embeddings are simultaneously processed by the adapter and the original layer, followed by the remaining layers, and then controllably interpolated at the logit level. We upgraded DeepSeek-R1-Distill-Qwen-7B from a slow-thinking model to one that supports both fast and slow thinking, allowing flexible alignment control even during inference. By encouraging deeper reasoning, it even surpassed its original performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Hate Speech Detection on Social Media: Can LLMs Replace Traditional Models?</title>
<link>https://arxiv.org/abs/2506.12744</link>
<guid>https://arxiv.org/abs/2506.12744</guid>
<content:encoded><![CDATA[
arXiv:2506.12744v1 Announce Type: new 
Abstract: Hate speech detection across contemporary social media presents unique challenges due to linguistic diversity and the informal nature of online discourse. These challenges are further amplified in settings involving code-mixing, transliteration, and culturally nuanced expressions. While fine-tuned transformer models, such as BERT, have become standard for this task, we argue that recent large language models (LLMs) not only surpass them but also redefine the landscape of hate speech detection more broadly. To support this claim, we introduce IndoHateMix, a diverse, high-quality dataset capturing Hindi-English code-mixing and transliteration in the Indian context, providing a realistic benchmark to evaluate model robustness in complex multilingual scenarios where existing NLP methods often struggle. Our extensive experiments show that cutting-edge LLMs (such as LLaMA-3.1) consistently outperform task-specific BERT-based models, even when fine-tuned on significantly less data. With their superior generalization and adaptability, LLMs offer a transformative approach to mitigating online hate in diverse environments. This raises the question of whether future works should prioritize developing specialized models or focus on curating richer and more varied datasets to further enhance the effectiveness of LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Democratic or Authoritarian? Probing a New Dimension of Political Biases in Large Language Models</title>
<link>https://arxiv.org/abs/2506.12758</link>
<guid>https://arxiv.org/abs/2506.12758</guid>
<content:encoded><![CDATA[
arXiv:2506.12758v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) become increasingly integrated into everyday life and information ecosystems, concerns about their implicit biases continue to persist. While prior work has primarily examined socio-demographic and left--right political dimensions, little attention has been paid to how LLMs align with broader geopolitical value systems, particularly the democracy--authoritarianism spectrum. In this paper, we propose a novel methodology to assess such alignment, combining (1) the F-scale, a psychometric tool for measuring authoritarian tendencies, (2) FavScore, a newly introduced metric for evaluating model favorability toward world leaders, and (3) role-model probing to assess which figures are cited as general role-models by LLMs. We find that LLMs generally favor democratic values and leaders, but exhibit increases favorability toward authoritarian figures when prompted in Mandarin. Further, models are found to often cite authoritarian figures as role models, even outside explicit political contexts. These results shed light on ways LLMs may reflect and potentially reinforce global political ideologies, highlighting the importance of evaluating bias beyond conventional socio-political axes. Our code is available at: https://github.com/irenestrauss/Democratic-Authoritarian-Bias-LLMs
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surprise Calibration for Better In-Context Learning</title>
<link>https://arxiv.org/abs/2506.12796</link>
<guid>https://arxiv.org/abs/2506.12796</guid>
<content:encoded><![CDATA[
arXiv:2506.12796v1 Announce Type: new 
Abstract: In-context learning (ICL) has emerged as a powerful paradigm for task adaptation in large language models (LLMs), where models infer underlying task structures from a few demonstrations. However, ICL remains susceptible to biases that arise from prior knowledge and contextual demonstrations, which can degrade the performance of LLMs. Existing bias calibration methods typically apply fixed class priors across all inputs, limiting their efficacy in dynamic ICL settings where the context for each query differs. To address these limitations, we adopt implicit sequential Bayesian inference as a framework for interpreting ICL, identify "surprise" as an informative signal for class prior shift, and introduce a novel method--Surprise Calibration (SC). SC leverages the notion of surprise to capture the temporal dynamics of class priors, providing a more adaptive and computationally efficient solution for in-context learning. We empirically demonstrate the superiority of SC over existing bias calibration techniques across a range of benchmark natural language processing tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Medical Argument Mining: Exploitation of Scarce Data Using NLI Systems</title>
<link>https://arxiv.org/abs/2506.12823</link>
<guid>https://arxiv.org/abs/2506.12823</guid>
<content:encoded><![CDATA[
arXiv:2506.12823v1 Announce Type: new 
Abstract: This work presents an Argument Mining process that extracts argumentative entities from clinical texts and identifies their relationships using token classification and Natural Language Inference techniques. Compared to straightforward methods like text classification, this methodology demonstrates superior performance in data-scarce settings. By assessing the effectiveness of these methods in identifying argumentative structures that support or refute possible diagnoses, this research lays the groundwork for future tools that can provide evidence-based justifications for machine-generated clinical conclusions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming Chatbot Text: A Sequence-to-Sequence Approach</title>
<link>https://arxiv.org/abs/2506.12843</link>
<guid>https://arxiv.org/abs/2506.12843</guid>
<content:encoded><![CDATA[
arXiv:2506.12843v1 Announce Type: new 
Abstract: Due to advances in Large Language Models (LLMs) such as ChatGPT, the boundary between human-written text and AI-generated text has become blurred. Nevertheless, recent work has demonstrated that it is possible to reliably detect GPT-generated text. In this paper, we adopt a novel strategy to adversarially transform GPT-generated text using sequence-to-sequence (Seq2Seq) models, with the goal of making the text more human-like. We experiment with the Seq2Seq models T5-small and BART which serve to modify GPT-generated sentences to include linguistic, structural, and semantic components that may be more typical of human-authored text. Experiments show that classification models trained to distinguish GPT-generated text are significantly less accurate when tested on text that has been modified by these Seq2Seq models. However, after retraining classification models on data generated by our Seq2Seq technique, the models are able to distinguish the transformed GPT-generated text from human-generated text with high accuracy. This work adds to the accumulating knowledge of text transformation as a tool for both attack -- in the sense of defeating classification models -- and defense -- in the sense of improved classifiers -- thereby advancing our understanding of AI-generated text.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QFFT, Question-Free Fine-Tuning for Adaptive Reasoning</title>
<link>https://arxiv.org/abs/2506.12860</link>
<guid>https://arxiv.org/abs/2506.12860</guid>
<content:encoded><![CDATA[
arXiv:2506.12860v1 Announce Type: new 
Abstract: Recent advancements in Long Chain-of-Thought (CoT) reasoning models have improved performance on complex tasks, but they suffer from overthinking, which generates redundant reasoning steps, especially for simple questions. This paper revisits the reasoning patterns of Long and Short CoT models, observing that the Short CoT patterns offer concise reasoning efficiently, while the Long CoT patterns excel in challenging scenarios where the Short CoT patterns struggle. To enable models to leverage both patterns, we propose Question-Free Fine-Tuning (QFFT), a fine-tuning approach that removes the input question during training and learns exclusively from Long CoT responses. This approach enables the model to adaptively employ both reasoning patterns: it prioritizes the Short CoT patterns and activates the Long CoT patterns only when necessary. Experiments on various mathematical datasets demonstrate that QFFT reduces average response length by more than 50\%, while achieving performance comparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits superior performance compared to SFT in noisy, out-of-domain, and low-resource scenarios.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArgHiTZ at ArchEHR-QA 2025: A Two-Step Divide and Conquer Approach to Patient Question Answering for Top Factuality</title>
<link>https://arxiv.org/abs/2506.12886</link>
<guid>https://arxiv.org/abs/2506.12886</guid>
<content:encoded><![CDATA[
arXiv:2506.12886v1 Announce Type: new 
Abstract: This work presents three different approaches to address the ArchEHR-QA 2025 Shared Task on automated patient question answering. We introduce an end-to-end prompt-based baseline and two two-step methods to divide the task, without utilizing any external knowledge. Both two step approaches first extract essential sentences from the clinical text, by prompt or similarity ranking, and then generate the final answer from these notes. Results indicate that the re-ranker based two-step system performs best, highlighting the importance of selecting the right approach for each subtask. Our best run achieved an overall score of 0.44, ranking 8th out of 30 on the leaderboard, securing the top position in overall factuality.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Performance Gap Between Lexical and Semantic Models for Information Retrieval With Formulaic Legal Language</title>
<link>https://arxiv.org/abs/2506.12895</link>
<guid>https://arxiv.org/abs/2506.12895</guid>
<content:encoded><![CDATA[
arXiv:2506.12895v1 Announce Type: new 
Abstract: Legal passage retrieval is an important task that assists legal practitioners in the time-intensive process of finding relevant precedents to support legal arguments. This study investigates the task of retrieving legal passages or paragraphs from decisions of the Court of Justice of the European Union (CJEU), whose language is highly structured and formulaic, leading to repetitive patterns. Understanding when lexical or semantic models are more effective at handling the repetitive nature of legal language is key to developing retrieval systems that are more accurate, efficient, and transparent for specific legal domains. To this end, we explore when this routinized legal language is better suited for retrieval using methods that rely on lexical and statistical features, such as BM25, or dense retrieval models trained to capture semantic and contextual information. A qualitative and quantitative analysis with three complementary metrics shows that both lexical and dense models perform well in scenarios with more repetitive usage of language, whereas BM25 performs better than the dense models in more nuanced scenarios where repetition and verbatim~quotes are less prevalent and in longer queries. Our experiments also show that BM25 is a strong baseline, surpassing off-the-shelf dense models in 4 out of 7 performance metrics. However, fine-tuning a dense model on domain-specific data led to improved performance, surpassing BM25 in most metrics, and we analyze the effect of the amount of data used in fine-tuning on the model's performance and temporal robustness. The code, dataset and appendix related to this work are available on: https://github.com/larimo/lexsem-legal-ir.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JEBS: A Fine-grained Biomedical Lexical Simplification Task</title>
<link>https://arxiv.org/abs/2506.12898</link>
<guid>https://arxiv.org/abs/2506.12898</guid>
<content:encoded><![CDATA[
arXiv:2506.12898v1 Announce Type: new 
Abstract: Online medical literature has made health information more available than ever, however, the barrier of complex medical jargon prevents the general public from understanding it. Though parallel and comparable corpora for Biomedical Text Simplification have been introduced, these conflate the many syntactic and lexical operations involved in simplification. To enable more targeted development and evaluation, we present a fine-grained lexical simplification task and dataset, Jargon Explanations for Biomedical Simplification (JEBS, https://github.com/bill-from-ri/JEBS-data ). The JEBS task involves identifying complex terms, classifying how to replace them, and generating replacement text. The JEBS dataset contains 21,595 replacements for 10,314 terms across 400 biomedical abstracts and their manually simplified versions. Additionally, we provide baseline results for a variety of rule-based and transformer-based systems for the three sub-tasks. The JEBS task, data, and baseline results pave the way for development and rigorous evaluation of systems for replacing or explaining complex biomedical terms.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciDA: Scientific Dynamic Assessor of LLMs</title>
<link>https://arxiv.org/abs/2506.12909</link>
<guid>https://arxiv.org/abs/2506.12909</guid>
<content:encoded><![CDATA[
arXiv:2506.12909v1 Announce Type: new 
Abstract: Advancement in Large Language Models (LLMs) reasoning capabilities enables them to solve scientific problems with enhanced efficacy. Thereby, a high-quality benchmark for comprehensive and appropriate assessment holds significance, while existing ones either confront the risk of data contamination or lack involved disciplines. To be specific, due to the data source overlap of LLMs training and static benchmark, the keys or number pattern of answers inadvertently memorized (i.e. data contamination), leading to systematic overestimation of their reasoning capabilities, especially numerical reasoning. We propose SciDA, a multidisciplinary benchmark that consists exclusively of over 1k Olympic-level numerical computation problems, allowing randomized numerical initializations for each inference round to avoid reliance on fixed numerical patterns. We conduct a series of experiments with both closed-source and open-source top-performing LLMs, and it is observed that the performance of LLMs drop significantly under random numerical initialization. Thus, we provide truthful and unbiased assessments of the numerical reasoning capabilities of LLMs. The data is available at https://huggingface.co/datasets/m-a-p/SciDA
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaFeedback: A Large-scale Human-annotated Benchmark For Personalization</title>
<link>https://arxiv.org/abs/2506.12915</link>
<guid>https://arxiv.org/abs/2506.12915</guid>
<content:encoded><![CDATA[
arXiv:2506.12915v1 Announce Type: new 
Abstract: With the rapid improvement in the general capabilities of LLMs, LLM personalization, i.e., how to build LLM systems that can generate personalized responses or services that are tailored to distinct user personas, has become an increasingly important research and engineering problem. However, unlike many new challenging benchmarks being released for evaluating the general/reasoning capabilities, the lack of high-quality benchmarks for evaluating LLM personalization greatly hinders progress in this field. To address this, we introduce PersonaFeedback, a new benchmark that directly evaluates LLMs' ability to provide personalized responses given pre-defined user personas and queries. Unlike existing benchmarks that require models to infer implicit user personas from historical interactions, PersonaFeedback decouples persona inference from personalization, focusing on evaluating the model's ability to generate responses tailored to explicit personas. PersonaFeedback consists of 8298 human-annotated test cases, which are categorized into easy, medium, and hard tiers based on the contextual complexity of the user personas and the difficulty in distinguishing subtle differences between two personalized responses. We conduct comprehensive evaluations across a wide range of models. The empirical results reveal that even state-of-the-art LLMs that can solve complex real-world reasoning tasks could fall short on the hard tier of PersonaFeedback where even human evaluators may find the distinctions challenging. Furthermore, we conduct an in-depth analysis of failure modes across various types of systems, demonstrating that the current retrieval-augmented framework should not be seen as a de facto solution for personalization tasks. All benchmark data, annotation protocols, and the evaluation pipeline will be publicly available to facilitate future research on LLM personalization.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models</title>
<link>https://arxiv.org/abs/2506.12935</link>
<guid>https://arxiv.org/abs/2506.12935</guid>
<content:encoded><![CDATA[
arXiv:2506.12935v1 Announce Type: new 
Abstract: While large language models have shown reasoning capabilities, their application to the audio modality, particularly in large audio-language models (ALMs), remains significantly underdeveloped. Addressing this gap requires a systematic approach, involving a capable base model, high-quality reasoning-oriented audio data, and effective training algorithms. In this study, we present a comprehensive solution: we introduce the Audio Logical Reasoning (ALR) dataset, consisting of 6,446 text-audio annotated samples specifically designed for complex reasoning tasks. Building on this resource, we propose SoundMind, a rule-based reinforcement learning (RL) algorithm tailored to endow ALMs with deep bimodal reasoning abilities. By training Qwen2.5-Omni-7B on the ALR dataset using SoundMind, our approach achieves state-of-the-art performance in audio logical reasoning. This work highlights the impact of combining high-quality, reasoning-focused datasets with specialized RL techniques, advancing the frontier of auditory intelligence in language models. Our code and the proposed dataset are available at https://github.com/xid32/SoundMind.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CliniDial: A Naturally Occurring Multimodal Dialogue Dataset for Team Reflection in Action During Clinical Operation</title>
<link>https://arxiv.org/abs/2506.12936</link>
<guid>https://arxiv.org/abs/2506.12936</guid>
<content:encoded><![CDATA[
arXiv:2506.12936v1 Announce Type: new 
Abstract: In clinical operations, teamwork can be the crucial factor that determines the final outcome. Prior studies have shown that sufficient collaboration is the key factor that determines the outcome of an operation. To understand how the team practices teamwork during the operation, we collected CliniDial from simulations of medical operations. CliniDial includes the audio data and its transcriptions, the simulated physiology signals of the patient manikins, and how the team operates from two camera angles. We annotate behavior codes following an existing framework to understand the teamwork process for CliniDial. We pinpoint three main characteristics of our dataset, including its label imbalances, rich and natural interactions, and multiple modalities, and conduct experiments to test existing LLMs' capabilities on handling data with these characteristics. Experimental results show that CliniDial poses significant challenges to the existing models, inviting future effort on developing methods that can deal with real-world clinical data. We open-source the codebase at https://github.com/MichiganNLP/CliniDial
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Role of Data Quality in Training Bilingual Language Models</title>
<link>https://arxiv.org/abs/2506.12966</link>
<guid>https://arxiv.org/abs/2506.12966</guid>
<content:encoded><![CDATA[
arXiv:2506.12966v1 Announce Type: new 
Abstract: Bilingual and multilingual language models offer a promising path toward scaling NLP systems across diverse languages and users. However, their performance often varies wildly between languages as prior works show that adding more languages can degrade performance for some languages (such as English), while improving others (typically more data constrained languages). In this work, we investigate causes of these inconsistencies by comparing bilingual and monolingual language models. Our analysis reveals that unequal data quality, not just data quantity, is a major driver of performance degradation in bilingual settings. We propose a simple yet effective data filtering strategy to select higher-quality bilingual training data with only high quality English data. Applied to French, German, and Chinese, our approach improves monolingual performance by 2-4% and reduces bilingual model performance gaps to 1%. These results highlight the overlooked importance of data quality in multilingual pretraining and offer a practical recipe for balancing performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-document Summarization through Multi-document Event Relation Graph Reasoning in LLMs: a case study in Framing Bias Mitigation</title>
<link>https://arxiv.org/abs/2506.12978</link>
<guid>https://arxiv.org/abs/2506.12978</guid>
<content:encoded><![CDATA[
arXiv:2506.12978v1 Announce Type: new 
Abstract: Media outlets are becoming more partisan and polarized nowadays. Most previous work focused on detecting media bias. In this paper, we aim to mitigate media bias by generating a neutralized summary given multiple articles presenting different ideological views. Motivated by the critical role of events and event relations in media bias detection, we propose to increase awareness of bias in LLMs via multi-document events reasoning and use a multi-document event relation graph to guide the summarization process. This graph contains rich event information useful to reveal bias: four common types of in-doc event relations to reflect content framing bias, cross-doc event coreference relation to reveal content selection bias, and event-level moral opinions to highlight opinionated framing bias. We further develop two strategies to incorporate the multi-document event relation graph for neutralized summarization. Firstly, we convert a graph into natural language descriptions and feed the textualized graph into LLMs as a part of a hard text prompt. Secondly, we encode the graph with graph attention network and insert the graph embedding into LLMs as a soft prompt. Both automatic evaluation and human evaluation confirm that our approach effectively mitigates both lexical and informational media bias, and meanwhile improves content preservation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Enhanced by Plug and Play Syntactic Knowledge for Aspect-based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2506.12991</link>
<guid>https://arxiv.org/abs/2506.12991</guid>
<content:encoded><![CDATA[
arXiv:2506.12991v1 Announce Type: new 
Abstract: Aspect-based sentiment analysis (ABSA) generally requires a deep understanding of the contextual information, including the words associated with the aspect terms and their syntactic dependencies. Most existing studies employ advanced encoders (e.g., pre-trained models) to capture such context, especially large language models (LLMs). However, training these encoders is resource-intensive, and in many cases, the available data is insufficient for necessary fine-tuning. Therefore it is challenging for learning LLMs within such restricted environments and computation efficiency requirement. As a result, it motivates the exploration of plug-and-play methods that adapt LLMs to ABSA with minimal effort. In this paper, we propose an approach that integrates extendable components capable of incorporating various types of syntactic knowledge, such as constituent syntax, word dependencies, and combinatory categorial grammar (CCG). Specifically, we propose a memory module that records syntactic information and is incorporated into LLMs to instruct the prediction of sentiment polarities. Importantly, this encoder acts as a versatile, detachable plugin that is trained independently of the LLM. We conduct experiments on benchmark datasets, which show that our approach outperforms strong baselines and previous approaches, thus demonstrates its effectiveness.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Missing the human touch? A computational stylometry analysis of GPT-4 translations of online Chinese literature</title>
<link>https://arxiv.org/abs/2506.13013</link>
<guid>https://arxiv.org/abs/2506.13013</guid>
<content:encoded><![CDATA[
arXiv:2506.13013v1 Announce Type: new 
Abstract: Existing research indicates that machine translations (MTs) of literary texts are often unsatisfactory. MTs are typically evaluated using automated metrics and subjective human ratings, with limited focus on stylistic features. Evidence is also limited on whether state-of-the-art large language models (LLMs) will reshape literary translation. This study examines the stylistic features of LLM translations, comparing GPT-4's performance to human translations in a Chinese online literature task. Computational stylometry analysis shows that GPT-4 translations closely align with human translations in lexical, syntactic, and content features, suggesting that LLMs might replicate the 'human touch' in literary translation style. These findings offer insights into AI's impact on literary translation from a posthuman perspective, where distinctions between machine and human translations become increasingly blurry.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edeflip: Supervised Word Translation between English and Yoruba</title>
<link>https://arxiv.org/abs/2506.13020</link>
<guid>https://arxiv.org/abs/2506.13020</guid>
<content:encoded><![CDATA[
arXiv:2506.13020v1 Announce Type: new 
Abstract: In recent years, embedding alignment has become the state-of-the-art machine translation approach, as it can yield high-quality translation without training on parallel corpora. However, existing research and application of embedding alignment mostly focus on high-resource languages with high-quality monolingual embeddings. It is unclear if and how low-resource languages may be similarly benefited. In this study, we implement an established supervised embedding alignment method for word translation from English to Yoruba, the latter a low-resource language. We found that higher embedding quality and normalizing embeddings increase word translation precision, with, additionally, an interaction effect between the two. Our results demonstrate the limitations of the state-of-the-art supervised embedding alignment when it comes to low-resource languages, for which there are additional factors that need to be taken into consideration, such as the importance of curating high-quality monolingual embeddings. We hope our work will be a starting point for further machine translation research that takes into account the challenges that low-resource languages face.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just Go Parallel: Improving the Multilingual Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2506.13044</link>
<guid>https://arxiv.org/abs/2506.13044</guid>
<content:encoded><![CDATA[
arXiv:2506.13044v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive translation capabilities even without being explicitly trained on parallel data. This remarkable property has led some to believe that parallel data is no longer necessary for building multilingual language models. While some attribute this to the emergent abilities of LLMs due to scale, recent work suggests that it is actually caused by incidental bilingual signals present in the training data. Various methods have been proposed to maximize the utility of parallel data to enhance the multilingual capabilities of multilingual encoder-based and encoder-decoder language models. However, some decoder-based LLMs opt to ignore parallel data instead. In this work, we conduct a systematic study on the impact of adding parallel data on LLMs' multilingual capabilities, focusing specifically on translation and multilingual common-sense reasoning. Through controlled experiments, we demonstrate that parallel data can significantly improve LLMs' multilingual capabilities.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CFBenchmark-MM: Chinese Financial Assistant Benchmark for Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2506.13055</link>
<guid>https://arxiv.org/abs/2506.13055</guid>
<content:encoded><![CDATA[
arXiv:2506.13055v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have rapidly evolved with the growth of Large Language Models (LLMs) and are now applied in various fields. In finance, the integration of diverse modalities such as text, charts, and tables is crucial for accurate and efficient decision-making. Therefore, an effective evaluation system that incorporates these data types is essential for advancing financial application. In this paper, we introduce CFBenchmark-MM, a Chinese multimodal financial benchmark with over 9,000 image-question pairs featuring tables, histogram charts, line charts, pie charts, and structural diagrams. Additionally, we develop a staged evaluation system to assess MLLMs in handling multimodal information by providing different visual content step by step. Despite MLLMs having inherent financial knowledge, experimental results still show limited efficiency and robustness in handling multimodal financial context. Further analysis on incorrect responses reveals the misinterpretation of visual content and the misunderstanding of financial concepts are the primary issues. Our research validates the significant, yet underexploited, potential of MLLMs in financial analysis, highlighting the need for further development and domain-specific optimization to encourage the enhanced use in financial domain.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multipole Attention for Efficient Long Context Reasoning</title>
<link>https://arxiv.org/abs/2506.13059</link>
<guid>https://arxiv.org/abs/2506.13059</guid>
<content:encoded><![CDATA[
arXiv:2506.13059v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have shown promising accuracy improvements on complex problem-solving tasks. While these models have attained high accuracy by leveraging additional computation at test time, they need to generate long chain-of-thought reasoning in order to think before answering, which requires generating thousands of tokens. While sparse attention methods can help reduce the KV cache pressure induced by this long autoregressive reasoning, these methods can introduce errors which disrupt the reasoning process. Additionally, prior methods often pre-process the input to make it easier to identify the important prompt tokens when computing attention during generation, and this pre-processing is challenging to perform online for newly generated reasoning tokens. Our work addresses these challenges by introducing Multipole Attention, which accelerates autoregressive reasoning by only computing exact attention for the most important tokens, while maintaining approximate representations for the remaining tokens. Our method first performs clustering to group together semantically similar key vectors, and then uses the cluster centroids both to identify important key vectors and to approximate the remaining key vectors in order to retain high accuracy. We design a fast cluster update process to quickly re-cluster the input and previously generated tokens, thereby allowing for accelerating attention to the previous output tokens. We evaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our approach can maintain accuracy on complex reasoning tasks even with aggressive attention sparsity settings. We also provide kernel implementations to demonstrate the practical efficiency gains from our method, achieving up to 4.5$\times$ speedup for attention in long-context reasoning applications. Our code is available at https://github.com/SqueezeAILab/MultipoleAttention.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MotiveBench: How Far Are We From Human-Like Motivational Reasoning in Large Language Models?</title>
<link>https://arxiv.org/abs/2506.13065</link>
<guid>https://arxiv.org/abs/2506.13065</guid>
<content:encoded><![CDATA[
arXiv:2506.13065v1 Announce Type: new 
Abstract: Large language models (LLMs) have been widely adopted as the core of agent frameworks in various scenarios, such as social simulations and AI companions. However, the extent to which they can replicate human-like motivations remains an underexplored question. Existing benchmarks are constrained by simplistic scenarios and the absence of character identities, resulting in an information asymmetry with real-world situations. To address this gap, we propose MotiveBench, which consists of 200 rich contextual scenarios and 600 reasoning tasks covering multiple levels of motivation. Using MotiveBench, we conduct extensive experiments on seven popular model families, comparing different scales and versions within each family. The results show that even the most advanced LLMs still fall short in achieving human-like motivational reasoning. Our analysis reveals key findings, including the difficulty LLMs face in reasoning about "love & belonging" motivations and their tendency toward excessive rationality and idealism. These insights highlight a promising direction for future research on the humanization of LLMs. The dataset, benchmark, and code are available at https://aka.ms/motivebench.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and Reward Design</title>
<link>https://arxiv.org/abs/2506.13066</link>
<guid>https://arxiv.org/abs/2506.13066</guid>
<content:encoded><![CDATA[
arXiv:2506.13066v1 Announce Type: new 
Abstract: Large Multimodal Models (LMMs) demonstrate significant cross-modal reasoning capabilities. However, financial applications face challenges due to the lack of high-quality multimodal reasoning datasets and the inefficiency of existing training paradigms for reasoning enhancement. To address these issues, we propose an integrated framework, FinLMM-R1, combining an automated and scalable pipeline for data construction with enhanced training strategies to improve the multimodal reasoning of LMM. The Automated and Scalable Pipeline (ASP) resolves textual-visual misalignment in financial reports through a separate paradigm of question-answer generation and image-question alignment, ensuring data integrity and extraction efficiency. Through ASP, we collect 89,378 aligned image-question pairs from 23,397 financial reports, covering tasks such as arithmetic reasoning, statistics reasoning, financial explanation, and financial knowledge. Moreover, we introduce the Thinking with Adversarial Reward in LMM (TAR-LMM), extending the prior two-stage training framework [1] with additional reward mechanisms. In the first stage, we focus on text-only tasks with format and accuracy rewards to guide the model in generating well-structured thinking contents. In the second stage, we construct multi-image contrastive samples with additional reward components including image selection, thinking content length, and adversarial reward to jointly optimize the LMM across visual perception, reasoning efficiency, and logical coherence. Extensive experiments on 7 benchmarks show ASP-derived dataset and training framework significantly improve answer accuracy and reasoning depth over existing reasoning LMMs in both general and financial multimodal contexts.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHILL at SemEval-2025 Task 2: You Can't Just Throw Entities and Hope -- Make Your LLM to Get Them Right</title>
<link>https://arxiv.org/abs/2506.13070</link>
<guid>https://arxiv.org/abs/2506.13070</guid>
<content:encoded><![CDATA[
arXiv:2506.13070v1 Announce Type: new 
Abstract: In this paper, we describe our approach for the SemEval 2025 Task 2 on Entity-Aware Machine Translation (EA-MT). Our system aims to improve the accuracy of translating named entities by combining two key approaches: Retrieval Augmented Generation (RAG) and iterative self-refinement techniques using Large Language Models (LLMs). A distinctive feature of our system is its self-evaluation mechanism, where the LLM assesses its own translations based on two key criteria: the accuracy of entity translations and overall translation quality. We demonstrate how these methods work together and effectively improve entity handling while maintaining high-quality translations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Test-Time Scaling for Medical AI: Model and Task-Aware Strategies for LLMs and VLMs</title>
<link>https://arxiv.org/abs/2506.13102</link>
<guid>https://arxiv.org/abs/2506.13102</guid>
<content:encoded><![CDATA[
arXiv:2506.13102v1 Announce Type: new 
Abstract: Test-time scaling has recently emerged as a promising approach for enhancing the reasoning capabilities of large language models or vision-language models during inference. Although a variety of test-time scaling strategies have been proposed, and interest in their application to the medical domain is growing, many critical aspects remain underexplored, including their effectiveness for vision-language models and the identification of optimal strategies for different settings. In this paper, we conduct a comprehensive investigation of test-time scaling in the medical domain. We evaluate its impact on both large language models and vision-language models, considering factors such as model size, inherent model characteristics, and task complexity. Finally, we assess the robustness of these strategies under user-driven factors, such as misleading information embedded in prompts. Our findings offer practical guidelines for the effective use of test-time scaling in medical applications and provide insights into how these strategies can be further refined to meet the reliability and interpretability demands of the medical domain.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging In-Context Learning for Language Model Agents</title>
<link>https://arxiv.org/abs/2506.13109</link>
<guid>https://arxiv.org/abs/2506.13109</guid>
<content:encoded><![CDATA[
arXiv:2506.13109v1 Announce Type: new 
Abstract: In-context learning (ICL) with dynamically selected demonstrations combines the flexibility of prompting large language models (LLMs) with the ability to leverage training data to improve performance. While ICL has been highly successful for prediction and generation tasks, leveraging it for agentic tasks that require sequential decision making is challenging -- one must think not only about how to annotate long trajectories at scale and how to select demonstrations, but also what constitutes demonstrations, and when and where to show them. To address this, we first propose an algorithm that leverages an LLM with retries along with demonstrations to automatically and efficiently annotate agentic tasks with solution trajectories. We then show that set-selection of trajectories of similar tasks as demonstrations significantly improves performance, reliability, robustness, and efficiency of LLM agents. However, trajectory demonstrations have a large inference cost overhead. We show that this can be mitigated by using small trajectory snippets at every step instead of an additional trajectory. We find that demonstrations obtained from larger models (in the annotation phase) also improve smaller models, and that ICL agents can even rival costlier trained agents. Thus, our results reveal that ICL, with careful use, can be very powerful for agentic tasks as well.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMU's IWSLT 2025 Simultaneous Speech Translation System</title>
<link>https://arxiv.org/abs/2506.13143</link>
<guid>https://arxiv.org/abs/2506.13143</guid>
<content:encoded><![CDATA[
arXiv:2506.13143v1 Announce Type: new 
Abstract: This paper presents CMU's submission to the IWSLT 2025 Simultaneous Speech Translation (SST) task for translating unsegmented English speech into Chinese and German text in a streaming manner. Our end-to-end speech-to-text system integrates a chunkwise causal Wav2Vec 2.0 speech encoder, an adapter, and the Qwen2.5-7B-Instruct as the decoder. We use a two-stage simultaneous training procedure on robust speech segments curated from LibriSpeech, CommonVoice, and VoxPopuli datasets, utilizing standard cross-entropy loss. Our model supports adjustable latency through a configurable latency multiplier. Experimental results demonstrate that our system achieves 44.3 BLEU for English-to-Chinese and 25.1 BLEU for English-to-German translations on the ACL60/60 development set, with computation-aware latencies of 2.7 seconds and 2.3 seconds, and theoretical latencies of 2.2 and 1.7 seconds, respectively.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting LLMs for Minimal-edit Grammatical Error Correction</title>
<link>https://arxiv.org/abs/2506.13148</link>
<guid>https://arxiv.org/abs/2506.13148</guid>
<content:encoded><![CDATA[
arXiv:2506.13148v1 Announce Type: new 
Abstract: Decoder-only large language models have shown superior performance in the fluency-edit English Grammatical Error Correction, but their adaptation for minimal-edit English GEC is still underexplored. To improve their effectiveness in the minimal-edit approach, we explore the error rate adaptation topic and propose a novel training schedule method. Our experiments set a new state-of-the-art result for a single-model system on the BEA-test set. We also detokenize the most common English GEC datasets to match the natural way of writing text. During the process, we find that there are errors in them. Our experiments analyze whether training on detokenized datasets impacts the results and measure the impact of the usage of the datasets with corrected erroneous examples. To facilitate reproducibility, we have released the source code used to train our models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging Unsubstantiated Claims and Ambiguous Pronouns</title>
<link>https://arxiv.org/abs/2506.13172</link>
<guid>https://arxiv.org/abs/2506.13172</guid>
<content:encoded><![CDATA[
arXiv:2506.13172v1 Announce Type: new 
Abstract: We present and evaluate a suite of proof-of-concept (PoC), structured workflow prompts designed to elicit human-like hierarchical reasoning while guiding Large Language Models (LLMs) in high-level semantic and linguistic analysis of scholarly manuscripts. The prompts target two non-trivial analytical tasks: identifying unsubstantiated claims in summaries (informational integrity) and flagging ambiguous pronoun references (linguistic clarity). We conducted a systematic, multi-run evaluation on two frontier models (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for the informational integrity task reveal a significant divergence in model performance: while both models successfully identified an unsubstantiated head of a noun phrase (95% success), ChatGPT consistently failed (0% success) to identify an unsubstantiated adjectival modifier that Gemini correctly flagged (95% success), raising a question regarding potential influence of the target's syntactic role. For the linguistic analysis task, both models performed well (80-90% success) with full manuscript context. In a summary-only setting, however, ChatGPT achieved a perfect (100%) success rate, while Gemini's performance was substantially degraded. Our findings suggest that structured prompting is a viable methodology for complex textual analysis but show that prompt performance may be highly dependent on the interplay between the model, task type, and context, highlighting the need for rigorous, model-specific testing.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development of the user-friendly decision aid Rule-based Evaluation and Support Tool (REST) for optimizing the resources of an information extraction task</title>
<link>https://arxiv.org/abs/2506.13177</link>
<guid>https://arxiv.org/abs/2506.13177</guid>
<content:encoded><![CDATA[
arXiv:2506.13177v1 Announce Type: new 
Abstract: Rules could be an information extraction (IE) default option, compared to ML and LLMs in terms of sustainability, transferability, interpretability, and development burden. We suggest a sustainable and combined use of rules and ML as an IE method. Our approach starts with an exhaustive expert manual highlighting in a single working session of a representative subset of the data corpus. We developed and validated the feasibility and the performance metrics of the REST decision tool to help the annotator choose between rules as a by default option and ML for each entity of an IE task. REST makes the annotator visualize the characteristics of each entity formalization in the free texts and the expected rule development feasibility and IE performance metrics. ML is considered as a backup IE option and manual annotation for training is therefore minimized. The external validity of REST on a 12-entity use case showed good reproducibility.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Large Language Models with Reliable Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.13178</link>
<guid>https://arxiv.org/abs/2506.13178</guid>
<content:encoded><![CDATA[
arXiv:2506.13178v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in text generation and understanding, yet their reliance on implicit, unstructured knowledge often leads to factual inaccuracies and limited interpretability. Knowledge Graphs (KGs), with their structured, relational representations, offer a promising solution to ground LLMs in verified knowledge. However, their potential remains constrained by inherent noise, incompleteness, and the complexity of integrating their rigid structure with the flexible reasoning of LLMs. This thesis presents a systematic framework to address these limitations, advancing the reliability of KGs and their synergistic integration with LLMs through five interconnected contributions. This thesis addresses these challenges through a cohesive framework that enhances LLMs by refining and leveraging reliable KGs. First, we introduce contrastive error detection, a structure-based method to identify incorrect facts in KGs. This approach is extended by an attribute-aware framework that unifies structural and semantic signals for error correction. Next, we propose an inductive completion model that further refines KGs by completing the missing relationships in evolving KGs. Building on these refined KGs, KnowGPT integrates structured graph reasoning into LLMs through dynamic prompting, improving factual grounding. These contributions form a systematic pipeline (from error detection to LLM integration), demonstrating that reliable KGs significantly enhance the robustness, interpretability, and adaptability of LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Acoustic Model Architecture Optimization in Training for ASR</title>
<link>https://arxiv.org/abs/2506.13180</link>
<guid>https://arxiv.org/abs/2506.13180</guid>
<content:encoded><![CDATA[
arXiv:2506.13180v1 Announce Type: new 
Abstract: Architecture design is inherently complex. Existing approaches rely on either handcrafted rules, which demand extensive empirical expertise, or automated methods like neural architecture search, which are computationally intensive. In this paper, we introduce DMAO, an architecture optimization framework that employs a grow-and-drop strategy to automatically reallocate parameters during training. This reallocation shifts resources from less-utilized areas to those parts of the model where they are most beneficial. Notably, DMAO only introduces negligible training overhead at a given model complexity. We evaluate DMAO through experiments with CTC on LibriSpeech, TED-LIUM-v2 and Switchboard datasets. The results show that, using the same amount of training resources, our proposed DMAO consistently improves WER by up to 6% relatively across various architectures, model sizes, and datasets. Furthermore, we analyze the pattern of parameter redistribution and uncover insightful findings.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Align-then-Unlearn: Embedding Alignment for LLM Unlearning</title>
<link>https://arxiv.org/abs/2506.13181</link>
<guid>https://arxiv.org/abs/2506.13181</guid>
<content:encoded><![CDATA[
arXiv:2506.13181v1 Announce Type: new 
Abstract: As large language models (LLMs) are trained on massive datasets, they have raised significant privacy and ethical concerns due to their potential to inadvertently retain sensitive information. Unlearning seeks to selectively remove specific data from trained models, such as personal information or copyrighted content. Current approaches targeting specific output sequences at the token level often fail to achieve complete forgetting and remain susceptible to prompt rephrasing. We propose Align-then-Unlearn, a novel framework that performs unlearning in the semantic embedding space rather than directly on output tokens. Align-then-Unlearn first augments the LLM with an embedding prediction module trained to anticipate future context representations. Unlearning is then achieved by fine-tuning the model to minimize the similarity between these predicted embeddings and a target embedding that represents the concept to be removed. Initial results show that Align-then-Unlearn effectively removes targeted knowledge with minimal degradation in overall model utility. These findings suggest that embedding-based unlearning offers a promising and robust approach to removing conceptual knowledge. Our code is available at https://github.com/ExplainableML/align-then-unlearn.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Thought Patterns: A Multi-Dimensional Reasoning Framework for LLMs</title>
<link>https://arxiv.org/abs/2506.13192</link>
<guid>https://arxiv.org/abs/2506.13192</guid>
<content:encoded><![CDATA[
arXiv:2506.13192v1 Announce Type: new 
Abstract: Large language models (LLMs) are often constrained by rigid reasoning processes, limiting their ability to generate creative and diverse responses. To address this, a novel framework called LADDER is proposed, combining Chain-of-Thought (CoT) reasoning, Mixture of Experts (MoE) models, and multi-dimensional up/down-sampling strategies which breaks the limitations of traditional LLMs. First, CoT reasoning guides the model through multi-step logical reasoning, expanding the semantic space and breaking the rigidity of thought. Next, MoE distributes the reasoning tasks across multiple expert modules, each focusing on specific sub-tasks. Finally, dimensionality reduction maps the reasoning outputs back to a lower-dimensional semantic space, yielding more precise and creative responses. Extensive experiments across multiple tasks demonstrate that LADDER significantly improves task completion, creativity, and fluency, generating innovative and coherent responses that outperform traditional models. Ablation studies reveal the critical roles of CoT and MoE in enhancing reasoning abilities and creative output. This work contributes to the development of more flexible and creative LLMs, capable of addressing complex and novel tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Music Preferences Reflect Cultural Values? A Cross-National Analysis Using Music Embedding and World Values Survey</title>
<link>https://arxiv.org/abs/2506.13199</link>
<guid>https://arxiv.org/abs/2506.13199</guid>
<content:encoded><![CDATA[
arXiv:2506.13199v1 Announce Type: new 
Abstract: This study explores the extent to which national music preferences reflect underlying cultural values. We collected long-term popular music data from YouTube Music Charts across 62 countries, encompassing both Western and non-Western regions, and extracted audio embeddings using the CLAP model. To complement these quantitative representations, we generated semantic captions for each track using LP-MusicCaps and GPT-based summarization. Countries were clustered based on contrastive embeddings that highlight deviations from global musical norms. The resulting clusters were projected into a two-dimensional space via t-SNE for visualization and evaluated against cultural zones defined by the World Values Survey (WVS). Statistical analyses, including MANOVA and chi-squared tests, confirmed that music-based clusters exhibit significant alignment with established cultural groupings. Furthermore, residual analysis revealed consistent patterns of overrepresentation, suggesting non-random associations between specific clusters and cultural zones. These findings indicate that national-level music preferences encode meaningful cultural signals and can serve as a proxy for understanding global cultural boundaries.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capability Salience Vector: Fine-grained Alignment of Loss and Capabilities for Downstream Task Scaling Law</title>
<link>https://arxiv.org/abs/2506.13216</link>
<guid>https://arxiv.org/abs/2506.13216</guid>
<content:encoded><![CDATA[
arXiv:2506.13216v1 Announce Type: new 
Abstract: Scaling law builds the relationship between training computation and validation loss, enabling researchers to effectively predict the loss trending of models across different levels of computation. However, a gap still remains between validation loss and the model's downstream capabilities, making it untrivial to apply scaling law to direct performance prediction for downstream tasks. The loss typically represents a cumulative penalty for predicted tokens, which are implicitly considered to have equal importance. Nevertheless, our studies have shown evidence that when considering different training data distributions, we cannot directly model the relationship between downstream capability and computation or token loss. To bridge the gap between validation loss and downstream task capabilities, in this work, we introduce Capability Salience Vector, which decomposes the overall loss and assigns different importance weights to tokens to assess a specific meta-capability, aligning the validation loss with downstream task performance in terms of the model's capabilities. Experiments on various popular benchmarks demonstrate that our proposed Capability Salience Vector could significantly improve the predictability of language model performance on downstream tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IGD: Token Decisiveness Modeling via Information Gain in LLMs for Personalized Recommendation</title>
<link>https://arxiv.org/abs/2506.13229</link>
<guid>https://arxiv.org/abs/2506.13229</guid>
<content:encoded><![CDATA[
arXiv:2506.13229v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown strong potential for recommendation by framing item prediction as a token-by-token language generation task. However, existing methods treat all item tokens equally, simply pursuing likelihood maximization during both optimization and decoding. This overlooks crucial token-level differences in decisiveness-many tokens contribute little to item discrimination yet can dominate optimization or decoding. To quantify token decisiveness, we propose a novel perspective that models item generation as a decision process, measuring token decisiveness by the Information Gain (IG) each token provides in reducing uncertainty about the generated item. Our empirical analysis reveals that most tokens have low IG but often correspond to high logits, disproportionately influencing training loss and decoding, which may impair model performance. Building on these insights, we introduce an Information Gain-based Decisiveness-aware Token handling (IGD) strategy that integrates token decisiveness into both tuning and decoding. Specifically, IGD downweights low-IG tokens during tuning and rebalances decoding to emphasize tokens with high IG. In this way, IGD moves beyond pure likelihood maximization, effectively prioritizing high-decisiveness tokens. Extensive experiments on four benchmark datasets with two LLM backbones demonstrate that IGD consistently improves recommendation accuracy, achieving significant gains on widely used ranking metrics compared to strong baselines.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy</title>
<link>https://arxiv.org/abs/2506.13284</link>
<guid>https://arxiv.org/abs/2506.13284</guid>
<content:encoded><![CDATA[
arXiv:2506.13284v1 Announce Type: new 
Abstract: In this work, we investigate the synergy between supervised fine-tuning (SFT) and reinforcement learning (RL) in developing strong reasoning models. We begin by curating the SFT training data through two scaling strategies: increasing the number of collected prompts and the number of generated responses per prompt. Both approaches yield notable improvements in reasoning performance, with scaling the number of prompts resulting in more substantial gains. We then explore the following questions regarding the synergy between SFT and RL: (i) Does a stronger SFT model consistently lead to better final performance after large-scale RL training? (ii) How can we determine an appropriate sampling temperature during RL training to effectively balance exploration and exploitation for a given SFT initialization? Our findings suggest that (i) holds true, provided effective RL training is conducted, particularly when the sampling temperature is carefully chosen to maintain the temperature-adjusted entropy around 0.3, a setting that strikes a good balance between exploration and exploitation. Notably, the performance gap between initial SFT models narrows significantly throughout the RL process. Leveraging a strong SFT foundation and insights into the synergistic interplay between SFT and RL, our AceReason-Nemotron-1.1 7B model significantly outperforms AceReason-Nemotron-1.0 and achieves new state-of-the-art performance among Qwen2.5-7B-based reasoning models on challenging math and code benchmarks, thereby demonstrating the effectiveness of our post-training recipe. We release the model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Safety Fallback in Editing-based Backdoor Injection on LLMs</title>
<link>https://arxiv.org/abs/2506.13285</link>
<guid>https://arxiv.org/abs/2506.13285</guid>
<content:encoded><![CDATA[
arXiv:2506.13285v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown strong performance across natural language tasks, but remain vulnerable to backdoor attacks. Recent model editing-based approaches enable efficient backdoor injection by directly modifying parameters to map specific triggers to attacker-desired responses. However, these methods often suffer from safety fallback, where the model initially responds affirmatively but later reverts to refusals due to safety alignment. In this work, we propose DualEdit, a dual-objective model editing framework that jointly promotes affirmative outputs and suppresses refusal responses. To address two key challenges -- balancing the trade-off between affirmative promotion and refusal suppression, and handling the diversity of refusal expressions -- DualEdit introduces two complementary techniques. (1) Dynamic loss weighting calibrates the objective scale based on the pre-edited model to stabilize optimization. (2) Refusal value anchoring compresses the suppression target space by clustering representative refusal value vectors, reducing optimization conflict from overly diverse token sets. Experiments on safety-aligned LLMs show that DualEdit improves attack success by 9.98\% and reduces safety fallback rate by 10.88\% over baselines.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning Language Models</title>
<link>https://arxiv.org/abs/2506.13300</link>
<guid>https://arxiv.org/abs/2506.13300</guid>
<content:encoded><![CDATA[
arXiv:2506.13300v1 Announce Type: new 
Abstract: This paper presents Seewo's systems for both tracks of the Multilingual Conversational Speech Language Model Challenge (MLC-SLM), addressing automatic speech recognition (ASR) and speaker diarization with ASR (SD-ASR). We introduce a multi-stage training pipeline that explicitly enhances reasoning and self-correction in speech language models for ASR. Our approach combines curriculum learning for progressive capability acquisition, Chain-of-Thought data augmentation to foster intermediate reflection, and Reinforcement Learning with Verifiable Rewards (RLVR) to further refine self-correction through reward-driven optimization. This approach achieves substantial improvements over the official challenge baselines. On the evaluation set, our best system attains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track 2. Comprehensive ablation studies demonstrate the effectiveness of each component under challenge constraints.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models as 'Hidden Persuaders': Fake Product Reviews are Indistinguishable to Humans and Machines</title>
<link>https://arxiv.org/abs/2506.13313</link>
<guid>https://arxiv.org/abs/2506.13313</guid>
<content:encoded><![CDATA[
arXiv:2506.13313v1 Announce Type: new 
Abstract: Reading and evaluating product reviews is central to how most people decide what to buy and consume online. However, the recent emergence of Large Language Models and Generative Artificial Intelligence now means writing fraudulent or fake reviews is potentially easier than ever. Through three studies we demonstrate that (1) humans are no longer able to distinguish between real and fake product reviews generated by machines, averaging only 50.8% accuracy overall - essentially the same that would be expected by chance alone; (2) that LLMs are likewise unable to distinguish between fake and real reviews and perform equivalently bad or even worse than humans; and (3) that humans and LLMs pursue different strategies for evaluating authenticity which lead to equivalently bad accuracy, but different precision, recall and F1 scores - indicating they perform worse at different aspects of judgment. The results reveal that review systems everywhere are now susceptible to mechanised fraud if they do not depend on trustworthy purchase verification to guarantee the authenticity of reviewers. Furthermore, the results provide insight into the consumer psychology of how humans judge authenticity, demonstrating there is an inherent 'scepticism bias' towards positive reviews and a special vulnerability to misjudge the authenticity of fake negative reviews. Additionally, results provide a first insight into the 'machine psychology' of judging fake reviews, revealing that the strategies LLMs take to evaluate authenticity radically differ from humans, in ways that are equally wrong in terms of accuracy, but different in their misjudgments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Document-Level Tabular Numerical Cross-Checking: A Coarse-to-Fine Approach</title>
<link>https://arxiv.org/abs/2506.13328</link>
<guid>https://arxiv.org/abs/2506.13328</guid>
<content:encoded><![CDATA[
arXiv:2506.13328v1 Announce Type: new 
Abstract: Numerical consistency across tables in disclosure documents is critical for ensuring accuracy, maintaining credibility, and avoiding reputational and economic risks. Automated tabular numerical cross-checking presents two significant challenges: (C1) managing the combinatorial explosion of candidate instances at the document level and (C2) comprehending multi-faceted numerical semantics. Previous research typically depends on heuristic-based filtering or simplified context extraction, often struggling to balance performance and efficiency. Recently, large language models (LLMs) have demonstrated remarkable contextual understanding capabilities that helps address C2 at the instance level, yet they remain hampered by computational inefficiency (C1) and limited domain expertise. This paper introduces CoFiTCheck, a novel LLM-based coarse-to-fine framework that addresses these challenges through two sequential stages: embedding-based filtering and discriminative classification. The embedding-based filtering stage introduces an instructional parallel encoding method to efficiently represent all numerical mentions in a table with LLMs, as well as a decoupled InfoNCE objective to mitigate the isolated mention problem. The discriminative classification stage employs a specialized LLM for fine-grained analysis of the remaining candidate pairs. This stage is further enhanced by our crosstable numerical alignment pretraining paradigm, which leverages weak supervision from cross-table numerical equality relationships to enrich task-specific priors without requiring manual annotation. Comprehensive evaluation across three types of real-world disclosure documents demonstrates that CoFiTCheck significantly outperforms previous methods while maintaining practical efficiency.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization</title>
<link>https://arxiv.org/abs/2506.13329</link>
<guid>https://arxiv.org/abs/2506.13329</guid>
<content:encoded><![CDATA[
arXiv:2506.13329v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) models have emerged as a cornerstone of large-scale deep learning by efficiently distributing computation and enhancing performance. However, their unique architecture-characterized by sparse expert activation and dynamic routing mechanisms-introduces inherent complexities that challenge conventional quantization techniques. Existing post-training quantization (PTQ) methods struggle to address activation outliers, router consistency and sparse expert calibration, leading to significant performance degradation. To bridge this gap, we propose EAQuant, a novel PTQ framework tailored for MoE architectures. Our method systematically tackles these challenges through three key innovations: (1) expert-aware smoothing aggregation to suppress activation outliers and stabilize quantization, (2) router logits distribution alignment to preserve expert selection consistency post-quantization, and (3) expert-level calibration data balance to optimize sparsely activated experts. Extensive experiments across W4A4 and extreme W3A4 quantization configurations demonstrate that EAQuant significantly outperforms existing methods, achieving average score improvements of 1.15 - 2.28% across three diverse MoE architectures, with particularly pronounced gains in reasoning tasks and robust performance retention under aggressive quantization. By integrating these innovations, EAQuant establishes a new state-of-the-art for high-precision, efficient MoE model compression. Our code is available at https://github.com/darren-fzq/EAQuant.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM Challenge 2025</title>
<link>https://arxiv.org/abs/2506.13339</link>
<guid>https://arxiv.org/abs/2506.13339</guid>
<content:encoded><![CDATA[
arXiv:2506.13339v1 Announce Type: new 
Abstract: This report details the NTU Speechlab system developed for the Interspeech 2025 Multilingual Conversational Speech and Language Model (MLC-SLM) Challenge (Task I), where we achieved 5th place. We present comprehensive analyses of our multilingual automatic speech recognition system, highlighting key advancements in model architecture, data selection, and training strategies. In particular, language-specific prompts and model averaging techniques were instrumental in boosting system performance across diverse languages. Compared to the initial baseline system, our final model reduced the average Mix Error Rate from 20.2% to 10.6%, representing an absolute improvement of 9.6% (a relative improvement of 48%) on the evaluation set. Our results demonstrate the effectiveness of our approach and offer practical insights for future Speech Large Language Models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning for Open-Ended Tasks</title>
<link>https://arxiv.org/abs/2506.13351</link>
<guid>https://arxiv.org/abs/2506.13351</guid>
<content:encoded><![CDATA[
arXiv:2506.13351v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have showcased impressive reasoning abilities in structured tasks like mathematics and programming, largely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which uses outcome-based signals that are scalable, effective, and robust against reward hacking. However, applying similar techniques to open-ended long-form reasoning tasks remains challenging due to the absence of generic, verifiable reward signals. To address this, we propose Direct Reasoning Optimization (DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended, particularly long-form, reasoning tasks, guided by a new reward signal: the Reasoning Reflection Reward (R3). At its core, R3 selectively identifies and emphasizes key tokens in the reference outcome that reflect the influence of the model's preceding chain-of-thought reasoning, thereby capturing the consistency between reasoning and reference outcome at a fine-grained level. Crucially, R3 is computed internally using the same model being optimized, enabling a fully self-contained training setup. Additionally, we introduce a dynamic data filtering strategy based on R3 for open-ended reasoning tasks, reducing cost while improving downstream performance. We evaluate DRO on two diverse datasets -- ParaRev, a long-form paragraph revision task, and FinQA, a math-oriented QA benchmark -- and show that it consistently outperforms strong baselines while remaining broadly applicable across both open-ended and structured domains.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with Multi Turns</title>
<link>https://arxiv.org/abs/2506.13356</link>
<guid>https://arxiv.org/abs/2506.13356</guid>
<content:encoded><![CDATA[
arXiv:2506.13356v1 Announce Type: new 
Abstract: Long-term memory (LTM) is essential for large language models (LLMs) to achieve autonomous intelligence in complex, evolving environments. Despite increasing efforts in memory-augmented and retrieval-based architectures, there remains a lack of standardized benchmarks to systematically evaluate LLMs' long-term memory abilities. Existing benchmarks still face challenges in evaluating knowledge retention and dynamic sequential reasoning, and in their own flexibility, all of which limit their effectiveness in assessing models' LTM capabilities. To address these gaps, we propose a novel benchmark framework based on interactive fiction games, featuring dynamically branching storylines with complex reasoning structures. These structures simulate real-world scenarios by requiring LLMs to navigate hierarchical decision trees, where each choice triggers cascading dependencies across multi-turn interactions. Our benchmark emphasizes two distinct settings to test reasoning complexity: one with immediate feedback upon incorrect decisions, and the other requiring models to independently trace back and revise earlier choices after failure. As part of this benchmark, we also construct a new dataset designed to test LLMs' LTM within narrative-driven environments. We further validate the effectiveness of our approach through detailed experiments. Experimental results demonstrate the benchmark's ability to robustly and reliably assess LTM in LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Medical VIE via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.13363</link>
<guid>https://arxiv.org/abs/2506.13363</guid>
<content:encoded><![CDATA[
arXiv:2506.13363v1 Announce Type: new 
Abstract: Visual Information Extraction (VIE) converts unstructured document images into structured formats like JSON, critical for medical applications such as report analysis and online consultations. Traditional methods rely on OCR and language models, while end-to-end multimodal models offer direct JSON generation. However, domain-specific schemas and high annotation costs limit their effectiveness in medical VIE. We base our approach on the Reinforcement Learning with Verifiable Rewards (RLVR) framework to address these challenges using only 100 annotated samples. Our approach ensures dataset diversity, a balanced precision-recall reward mechanism to reduce hallucinations and improve field coverage, and innovative sampling strategies to enhance reasoning capabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve state-of-the-art performance on medical VIE tasks, significantly improving F1, precision, and recall. While our models excel on tasks similar to medical datasets, performance drops on dissimilar tasks, highlighting the need for domain-specific optimization. Case studies further demonstrate the value of reasoning during training and inference for VIE.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Goal-oriented Proactive Dialogue Systems via Consistency Reflection and Correction</title>
<link>https://arxiv.org/abs/2506.13366</link>
<guid>https://arxiv.org/abs/2506.13366</guid>
<content:encoded><![CDATA[
arXiv:2506.13366v1 Announce Type: new 
Abstract: This paper proposes a consistency reflection and correction method for goal-oriented dialogue systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decompositional Reasoning for Graph Retrieval with Large Language Models</title>
<link>https://arxiv.org/abs/2506.13380</link>
<guid>https://arxiv.org/abs/2506.13380</guid>
<content:encoded><![CDATA[
arXiv:2506.13380v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at many NLP tasks, but struggle with multi-hop reasoning and factual consistency, limiting their effectiveness on knowledge-intensive tasks like complex question answering (QA). Linking Knowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally lack the ability to reason efficiently over graph-structured information. To tackle this problem, we propose a novel retrieval approach that integrates textual knowledge graphs into the LLM reasoning process via query decomposition. Our method decomposes complex questions into sub-questions, retrieves relevant textual subgraphs, and composes a question-specific knowledge graph to guide answer generation. For that, we use a weighted similarity function that focuses on both the complex question and the generated subquestions to extract a relevant subgraph, which allows efficient and precise retrieval for complex questions and improves the performance of LLMs on multi-hop QA tasks. This structured reasoning pipeline enhances factual grounding and interpretability while leveraging the generative strengths of LLMs. We evaluate our method on standard multi-hop QA benchmarks and show that it achieves comparable or superior performance to competitive existing methods, using smaller models and fewer LLM calls.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-directional Context-Enhanced Speech Large Language Models for Multilingual Conversational ASR</title>
<link>https://arxiv.org/abs/2506.13396</link>
<guid>https://arxiv.org/abs/2506.13396</guid>
<content:encoded><![CDATA[
arXiv:2506.13396v1 Announce Type: new 
Abstract: This paper introduces the integration of language-specific bi-directional context into a speech large language model (SLLM) to improve multilingual continuous conversational automatic speech recognition (ASR). We propose a character-level contextual masking strategy during training, which randomly removes portions of the context to enhance robustness and better emulate the flawed transcriptions that may occur during inference. For decoding, a two-stage pipeline is utilized: initial isolated segment decoding followed by context-aware re-decoding using neighboring hypotheses. Evaluated on the 1500-hour Multilingual Conversational Speech and Language Model (MLC-SLM) corpus covering eleven languages, our method achieves an 18% relative improvement compared to a strong baseline, outperforming even the model trained on 6000 hours of data for the MLC-SLM competition. These results underscore the significant benefit of incorporating contextual information in multilingual continuous conversational ASR.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark for Evaluating LLM-Based Table Analysis</title>
<link>https://arxiv.org/abs/2506.13405</link>
<guid>https://arxiv.org/abs/2506.13405</guid>
<content:encoded><![CDATA[
arXiv:2506.13405v1 Announce Type: new 
Abstract: With the rapid advancement of Large Language Models (LLMs), there is an increasing need for challenging benchmarks to evaluate their capabilities in handling complex tabular data. However, existing benchmarks are either based on outdated data setups or focus solely on simple, flat table structures. In this paper, we introduce RealHiTBench, a comprehensive benchmark designed to evaluate the performance of both LLMs and Multimodal LLMs (MLLMs) across a variety of input formats for complex tabular data, including LaTeX, HTML, and PNG. RealHiTBench also includes a diverse collection of tables with intricate structures, spanning a wide range of task types. Our experimental results, using 25 state-of-the-art LLMs, demonstrate that RealHiTBench is indeed a challenging benchmark. Moreover, we also develop TreeThinker, a tree-based pipeline that organizes hierarchical headers into a tree structure for enhanced tabular reasoning, validating the importance of improving LLMs' perception of table hierarchies. We hope that our work will inspire further research on tabular data reasoning and the development of more robust models. The code and data are available at https://github.com/cspzyy/RealHiTBench.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neural Model for Word Repetition</title>
<link>https://arxiv.org/abs/2506.13450</link>
<guid>https://arxiv.org/abs/2506.13450</guid>
<content:encoded><![CDATA[
arXiv:2506.13450v1 Announce Type: new 
Abstract: It takes several years for the developing brain of a baby to fully master word repetition-the task of hearing a word and repeating it aloud. Repeating a new word, such as from a new language, can be a challenging task also for adults. Additionally, brain damage, such as from a stroke, may lead to systematic speech errors with specific characteristics dependent on the location of the brain damage. Cognitive sciences suggest a model with various components for the different processing stages involved in word repetition. While some studies have begun to localize the corresponding regions in the brain, the neural mechanisms and how exactly the brain performs word repetition remain largely unknown. We propose to bridge the gap between the cognitive model of word repetition and neural mechanisms in the human brain by modeling the task using deep neural networks. Neural models are fully observable, allowing us to study the detailed mechanisms in their various substructures and make comparisons with human behavior and, ultimately, the brain. Here, we make first steps in this direction by: (1) training a large set of models to simulate the word repetition task; (2) creating a battery of tests to probe the models for known effects from behavioral studies in humans, and (3) simulating brain damage through ablation studies, where we systematically remove neurons from the model, and repeat the behavioral study to examine the resulting speech errors in the "patient" model. Our results show that neural models can mimic several effects known from human research, but might diverge in other aspects, highlighting both the potential and the challenges for future research aimed at developing human-like neural models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study</title>
<link>https://arxiv.org/abs/2506.13464</link>
<guid>https://arxiv.org/abs/2506.13464</guid>
<content:encoded><![CDATA[
arXiv:2506.13464v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown impressive capabilities across tasks such as mathematics, coding, and reasoning, yet their learning ability, which is crucial for adapting to dynamic environments and acquiring new knowledge, remains underexplored. In this work, we address this gap by introducing a framework inspired by cognitive psychology and education. Specifically, we decompose general learning ability into three distinct, complementary dimensions: Learning from Instructor (acquiring knowledge via explicit guidance), Learning from Concept (internalizing abstract structures and generalizing to new contexts), and Learning from Experience (adapting through accumulated exploration and feedback). We conduct a comprehensive empirical study across the three learning dimensions and identify several insightful findings, such as (i) interaction improves learning; (ii) conceptual understanding is scale-emergent and benefits larger models; and (iii) LLMs are effective few-shot learners but not many-shot learners. Based on our framework and empirical findings, we introduce a benchmark that provides a unified and realistic evaluation of LLMs' general learning abilities across three learning cognition dimensions. It enables diagnostic insights and supports evaluation and development of more adaptive and human-like models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Omics Cohort Discovery for Research on Neurodegeneration through Ontology-Augmented Embedding Models</title>
<link>https://arxiv.org/abs/2506.13467</link>
<guid>https://arxiv.org/abs/2506.13467</guid>
<content:encoded><![CDATA[
arXiv:2506.13467v1 Announce Type: new 
Abstract: The growing volume of omics and clinical data generated for neurodegenerative diseases (NDs) requires new approaches for their curation so they can be ready-to-use in bioinformatics. NeuroEmbed is an approach for the engineering of semantically accurate embedding spaces to represent cohorts and samples. The NeuroEmbed method comprises four stages: (1) extraction of ND cohorts from public repositories; (2) semi-automated normalization and augmentation of metadata of cohorts and samples using biomedical ontologies and clustering on the embedding space; (3) automated generation of a natural language question-answering (QA) dataset for cohorts and samples based on randomized combinations of standardized metadata dimensions and (4) fine-tuning of a domain-specific embedder to optimize queries. We illustrate the approach using the GEO repository and the PubMedBERT pretrained embedder. Applying NeuroEmbed, we semantically indexed 2,801 repositories and 150,924 samples. Amongst many biology-relevant categories, we normalized more than 1,700 heterogeneous tissue labels from GEO into 326 unique ontology-aligned concepts and enriched annotations with new ontology-aligned terms, leading to a fold increase in size for the metadata terms between 2.7 and 20 fold. After fine-tuning PubMedBERT with the QA training data augmented with the enlarged metadata, the model increased its mean Retrieval Precision from 0.277 to 0.866 and its mean Percentile Rank from 0.355 to 0.896. The NeuroEmbed methodology for the creation of electronic catalogues of omics cohorts and samples will foster automated bioinformatic pipelines construction. The NeuroEmbed catalogue of cohorts and samples is available at https://github.com/JoseAdrian3/NeuroEmbed.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Interdisciplinary Approach to Human-Centered Machine Translation</title>
<link>https://arxiv.org/abs/2506.13468</link>
<guid>https://arxiv.org/abs/2506.13468</guid>
<content:encoded><![CDATA[
arXiv:2506.13468v1 Announce Type: new 
Abstract: Machine Translation (MT) tools are widely used today, often in contexts where professional translators are not present. Despite progress in MT technology, a gap persists between system development and real-world usage, particularly for non-expert users who may struggle to assess translation reliability. This paper advocates for a human-centered approach to MT, emphasizing the alignment of system design with diverse communicative goals and contexts of use. We survey the literature in Translation Studies and Human-Computer Interaction to recontextualize MT evaluation and design to address the diverse real-world scenarios in which MT is used today.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Abstract, Align, Predict: Zero-Shot Stance Detection via Cognitive Inductive Reasoning</title>
<link>https://arxiv.org/abs/2506.13470</link>
<guid>https://arxiv.org/abs/2506.13470</guid>
<content:encoded><![CDATA[
arXiv:2506.13470v1 Announce Type: new 
Abstract: Zero-shot stance detection (ZSSD) aims to identify the stance of text toward previously unseen targets, a setting where conventional supervised models often fail due to reliance on labeled data and shallow lexical cues. Inspired by human cognitive reasoning, we propose the Cognitive Inductive Reasoning Framework (CIRF), which abstracts transferable reasoning schemas from unlabeled text and encodes them as concept-level logic. To integrate these schemas with input arguments, we introduce a Schema-Enhanced Graph Kernel Model (SEGKM) that dynamically aligns local and global reasoning structures. Experiments on SemEval-2016, VAST, and COVID-19-Stance benchmarks show that CIRF establishes new state-of-the-art results, outperforming strong ZSSD baselines by 1.0, 4.5, and 3.3 percentage points in macro-F1, respectively, and achieving comparable accuracy with 70\% fewer labeled examples. We will release the full code upon publication.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently Compressing Large Language Models</title>
<link>https://arxiv.org/abs/2506.13472</link>
<guid>https://arxiv.org/abs/2506.13472</guid>
<content:encoded><![CDATA[
arXiv:2506.13472v1 Announce Type: new 
Abstract: Quantization has been widely studied as an effective technique for reducing the memory requirement of large language models (LLMs), potentially improving the latency time as well. Utilizing the characteristic of rotational invariance of transformer, we propose the rotation-based saliency-aware weight quantization (ROSAQ), which identifies salient channels in the projection feature space, not in the original feature space, where the projected "principal" dimensions are naturally considered as "salient" features. The proposed ROSAQ consists of 1) PCA-based projection, which first performs principal component analysis (PCA) on a calibration set and transforms via the PCA projection, 2) Salient channel dentification, which selects dimensions corresponding to the K-largest eigenvalues as salient channels, and 3) Saliency-aware quantization with mixed-precision, which uses FP16 for salient dimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ shows improvements over the baseline saliency-aware quantization on the original feature space and other existing quantization methods. With kernel fusion, ROSAQ presents about 2.3x speed up over FP16 implementation in generating 256 tokens with a batch size of 64.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.13474</link>
<guid>https://arxiv.org/abs/2506.13474</guid>
<content:encoded><![CDATA[
arXiv:2506.13474v1 Announce Type: new 
Abstract: Clinical decision-making is a dynamic, interactive, and cyclic process where doctors have to repeatedly decide on which clinical action to perform and consider newly uncovered information for diagnosis and treatment. Large Language Models (LLMs) have the potential to support clinicians in this process, however, most applications of LLMs in clinical decision support suffer from one of two limitations: Either they assume the unrealistic scenario of immediate availability of all patient information and do not model the interactive and iterative investigation process, or they restrict themselves to the limited "out-of-the-box" capabilities of large pre-trained models without performing task-specific training. In contrast to this, we propose to model clinical decision-making for diagnosis with a hypothesis-driven uncertainty-aware language agent, LA-CDM, that converges towards a diagnosis via repeatedly requesting and interpreting relevant tests. Using a hybrid training paradigm combining supervised and reinforcement learning, we train LA-CDM with three objectives targeting critical aspects of clinical decision-making: accurate hypothesis generation, hypothesis uncertainty estimation, and efficient decision-making. We evaluate our methodology on MIMIC-CDM, a real-world dataset covering four abdominal diseases containing various clinical tests and show the benefit of explicitly training clinical decision-making for increasing diagnostic performance and efficiency.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover Limits and Effectiveness</title>
<link>https://arxiv.org/abs/2506.13479</link>
<guid>https://arxiv.org/abs/2506.13479</guid>
<content:encoded><![CDATA[
arXiv:2506.13479v1 Announce Type: new 
Abstract: Merging or routing low-rank adapters (LoRAs) has emerged as a popular solution for enhancing large language models, particularly when data access is restricted by regulatory or domain-specific constraints. This position paper argues that the research community should shift its focus from developing new merging or routing algorithms to understanding the conditions under which reusing LoRAs is truly effective. Through theoretical analysis and synthetic two-hop reasoning and math word-problem tasks, we examine whether reusing LoRAs enables genuine compositional generalization or merely reflects shallow pattern matching. Evaluating two data-agnostic methods--parameter averaging and dynamic adapter selection--we found that reusing LoRAs often fails to logically integrate knowledge across disjoint fine-tuning datasets, especially when such knowledge is underrepresented during pretraining. Our empirical results, supported by theoretical insights into LoRA's limited expressiveness, highlight the preconditions and constraints of reusing them for unseen tasks and cast doubt on its feasibility as a truly data-free approach. We advocate for pausing the pursuit of novel methods for recycling LoRAs and emphasize the need for rigorous mechanisms to guide future academic research in adapter-based model merging and practical system designs for practitioners.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TurBLiMP: A Turkish Benchmark of Linguistic Minimal Pairs</title>
<link>https://arxiv.org/abs/2506.13487</link>
<guid>https://arxiv.org/abs/2506.13487</guid>
<content:encoded><![CDATA[
arXiv:2506.13487v1 Announce Type: new 
Abstract: We introduce TurBLiMP, the first Turkish benchmark of linguistic minimal pairs, designed to evaluate the linguistic abilities of monolingual and multilingual language models (LMs). Covering 16 linguistic phenomena with 1000 minimal pairs each, TurBLiMP fills an important gap in linguistic evaluation resources for Turkish. In designing the benchmark, we give extra attention to two properties of Turkish that remain understudied in current syntactic evaluations of LMs, namely word order flexibility and subordination through morphological processes. Our experiments on a wide range of LMs and a newly collected set of human acceptability judgments reveal that even cutting-edge Large LMs still struggle with grammatical phenomena that are not challenging for humans, and may also exhibit different sensitivities to word order and morphological complexity compared to humans.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOW: Bottlenecked Next Word Exploration</title>
<link>https://arxiv.org/abs/2506.13502</link>
<guid>https://arxiv.org/abs/2506.13502</guid>
<content:encoded><![CDATA[
arXiv:2506.13502v1 Announce Type: new 
Abstract: Large language models (LLMs) are typically trained via next-word prediction (NWP), which provides strong surface-level fluency but often lacks support for robust reasoning. We propose BOttlenecked next Word exploration (BOW), a novel RL framework that rethinks NWP by introducing a reasoning bottleneck where a policy model first generates a reasoning path rather than predicting the next token directly, after which a frozen judge model predicts the next token distribution based solely on this reasoning path. We train the policy model using GRPO with rewards that quantify how effectively the reasoning path facilitates next-word recovery. Compared with other continual pretraining baselines, we show that BOW improves both the general and next-word reasoning capabilities of the base model, evaluated on various benchmarks. Our findings show that BOW can serve as an effective and scalable alternative to vanilla NWP.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>K/DA: Automated Data Generation Pipeline for Detoxifying Implicitly Offensive Language in Korean</title>
<link>https://arxiv.org/abs/2506.13513</link>
<guid>https://arxiv.org/abs/2506.13513</guid>
<content:encoded><![CDATA[
arXiv:2506.13513v1 Announce Type: new 
Abstract: Language detoxification involves removing toxicity from offensive language. While a neutral-toxic paired dataset provides a straightforward approach for training detoxification models, creating such datasets presents several challenges: i) the need for human annotation to build paired data, and ii) the rapid evolution of offensive terms, rendering static datasets quickly outdated. To tackle these challenges, we introduce an automated paired data generation pipeline, called K/DA. This pipeline is designed to generate offensive language with implicit offensiveness and trend-aligned slang, making the resulting dataset suitable for detoxification model training. We demonstrate that the dataset generated by K/DA exhibits high pair consistency and greater implicit offensiveness compared to existing Korean datasets, and also demonstrates applicability to other languages. Furthermore, it enables effective training of a high-performing detoxification model with simple instruction fine-tuning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TensorSLM: Energy-efficient Embedding Compression of Sub-billion Parameter Language Models on Low-end Devices</title>
<link>https://arxiv.org/abs/2506.13514</link>
<guid>https://arxiv.org/abs/2506.13514</guid>
<content:encoded><![CDATA[
arXiv:2506.13514v1 Announce Type: new 
Abstract: Small Language Models (SLMs, or on-device LMs) have significantly fewer parameters than Large Language Models (LLMs). They are typically deployed on low-end devices, like mobile phones and single-board computers. Unlike LLMs, which rely on increasing model size for better generalisation, SLMs designed for edge applications are expected to have adaptivity to the deployment environments and energy efficiency given the device battery life constraints, which are not addressed in datacenter-deployed LLMs. This paper addresses these two requirements by proposing a training-free token embedding compression approach using Tensor-Train Decomposition (TTD). Each pre-trained token embedding vector is converted into a lower-dimensional Matrix Product State (MPS). We comprehensively evaluate the extracted low-rank structures across compression ratio, language task performance, latency, and energy consumption on a typical low-end device, i.e. Raspberry Pi. Taking the sub-billion parameter versions of GPT-2/Cerebres-GPT and OPT models as examples, our approach achieves a comparable language task performance to the original model with around $2.0\times$ embedding layer compression, while the energy consumption of a single query drops by half.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Weight-shared Heterogeneous Group Attention Experts for Dynamic Token-wise KV Optimization</title>
<link>https://arxiv.org/abs/2506.13541</link>
<guid>https://arxiv.org/abs/2506.13541</guid>
<content:encoded><![CDATA[
arXiv:2506.13541v1 Announce Type: new 
Abstract: Transformer models face scalability challenges in causal language modeling (CLM) due to inefficient memory allocation for growing key-value (KV) caches, which strains compute and storage resources. Existing methods like Grouped Query Attention (GQA) and token-level KV optimization improve efficiency but rely on rigid resource allocation, often discarding "low-priority" tokens or statically grouping them, failing to address the dynamic spectrum of token importance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that dynamically optimizes token-wise computation and memory allocation. Unlike prior approaches, mixSGA retains all tokens while adaptively routing them to specialized experts with varying KV group sizes, balancing granularity and efficiency. Our key novelties include: (1) a token-wise expert-choice routing mechanism guided by learned importance scores, enabling proportional resource allocation without token discard; (2) weight-sharing across grouped attention projections to minimize parameter overhead; and (3) an auxiliary loss to ensure one-hot routing decisions for training-inference consistency in CLMs. Extensive evaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show mixSGA's superiority over static baselines. On instruction-following and continued pretraining tasks, mixSGA achieves higher ROUGE-L and lower perplexity under the same KV budgets.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understand the Implication: Learning to Think for Pragmatic Understanding</title>
<link>https://arxiv.org/abs/2506.13559</link>
<guid>https://arxiv.org/abs/2506.13559</guid>
<content:encoded><![CDATA[
arXiv:2506.13559v1 Announce Type: new 
Abstract: Pragmatics, the ability to infer meaning beyond literal interpretation, is crucial for social cognition and communication. While LLMs have been benchmarked for their pragmatic understanding, improving their performance remains underexplored. Existing methods rely on annotated labels but overlook the reasoning process humans naturally use to interpret implicit meaning. To bridge this gap, we introduce a novel pragmatic dataset, ImpliedMeaningPreference, that includes explicit reasoning (thoughts) for both correct and incorrect interpretations. Through preference-tuning and supervised fine-tuning, we demonstrate that thought-based learning significantly enhances LLMs' pragmatic understanding, improving accuracy by 11.12% across model families. We further discuss a transfer-learning study where we evaluate the performance of thought-based training for the other tasks of pragmatics (presupposition, deixis) that are not seen during the training time and observe an improvement of 16.10% compared to label-trained models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing Linguistic Shifts in Croatian News via Diachronic Word Embeddings</title>
<link>https://arxiv.org/abs/2506.13569</link>
<guid>https://arxiv.org/abs/2506.13569</guid>
<content:encoded><![CDATA[
arXiv:2506.13569v1 Announce Type: new 
Abstract: Measuring how semantics of words change over time improves our understanding of how cultures and perspectives change. Diachronic word embeddings help us quantify this shift, although previous studies leveraged substantial temporally annotated corpora. In this work, we use a corpus of 9.5 million Croatian news articles spanning the past 25 years and quantify semantic change using skip-gram word embeddings trained on five-year periods. Our analysis finds that word embeddings capture linguistic shifts of terms pertaining to major topics in this timespan (COVID-19, Croatia joining the European Union, technological advancements). We also find evidence that embeddings from post-2020 encode increased positivity in sentiment analysis tasks, contrasting studies reporting a decline in mental health over the same period.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention</title>
<link>https://arxiv.org/abs/2506.13585</link>
<guid>https://arxiv.org/abs/2506.13585</guid>
<content:encoded><![CDATA[
arXiv:2506.13585v1 Announce Type: new 
Abstract: We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Qwen vs. Gemma Integration with Whisper: A Comparative Study in Multilingual SpeechLLM Systems</title>
<link>https://arxiv.org/abs/2506.13596</link>
<guid>https://arxiv.org/abs/2506.13596</guid>
<content:encoded><![CDATA[
arXiv:2506.13596v1 Announce Type: new 
Abstract: This paper presents our system for the MLC-SLM Challenge 2025, focusing on multilingual speech recognition and language modeling with large language models (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with efficient projector architectures and various decoder configurations. We employ a three-stage training methodology that progressively optimizes the encoder, projector, and LLM components. Our system achieves competitive performance with a private test average WER/CER result of 16.63% using the Gemma3-12B and 18.6% using the Qwen2.5-7B as decoder-only language model.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation</title>
<link>https://arxiv.org/abs/2506.13599</link>
<guid>https://arxiv.org/abs/2506.13599</guid>
<content:encoded><![CDATA[
arXiv:2506.13599v1 Announce Type: new 
Abstract: Human mobility simulation plays a crucial role in various real-world applications. Recently, to address the limitations of traditional data-driven approaches, researchers have explored leveraging the commonsense knowledge and reasoning capabilities of large language models (LLMs) to accelerate human mobility simulation. However, these methods suffer from several critical shortcomings, including inadequate modeling of urban spaces and poor integration with both individual mobility patterns and collective mobility distributions. To address these challenges, we propose \textbf{C}ityGPT-Powered \textbf{A}gentic framework for \textbf{M}obility \textbf{S}imulation (\textbf{CAMS}), an agentic framework that leverages the language based urban foundation model to simulate human mobility in urban space. \textbf{CAMS} comprises three core modules, including MobExtractor to extract template mobility patterns and synthesize new ones based on user profiles, GeoGenerator to generate anchor points considering collective knowledge and generate candidate urban geospatial knowledge using an enhanced version of CityGPT, TrajEnhancer to retrieve spatial knowledge based on mobility patterns and generate trajectories with real trajectory preference alignment via DPO. Experiments on real-world datasets show that \textbf{CAMS} achieves superior performance without relying on externally provided geospatial information. Moreover, by holistically modeling both individual mobility patterns and collective mobility constraints, \textbf{CAMS} generates more realistic and plausible trajectories. In general, \textbf{CAMS} establishes a new paradigm that integrates the agentic framework with urban-knowledgeable LLMs for human mobility simulation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Structured Bangla Dataset of Disease-Symptom Associations to Improve Diagnostic Accuracy</title>
<link>https://arxiv.org/abs/2506.13610</link>
<guid>https://arxiv.org/abs/2506.13610</guid>
<content:encoded><![CDATA[
arXiv:2506.13610v1 Announce Type: new 
Abstract: Disease-symptom datasets are significant and in demand for medical research, disease diagnosis, clinical decision-making, and AI-driven health management applications. These datasets help identify symptom patterns associated with specific diseases, thus improving diagnostic accuracy and enabling early detection. The dataset presented in this study systematically compiles disease-symptom relationships from various online sources, medical literature, and publicly available health databases. The data was gathered through analyzing peer-reviewed medical articles, clinical case studies, and disease-symptom association reports. Only the verified medical sources were included in the dataset, while those from non-peer-reviewed and anecdotal sources were excluded. The dataset is structured in a tabular format, where the first column represents diseases, and the remaining columns represent symptoms. Each symptom cell contains a binary value (1 or 0), indicating whether a symptom is associated with a disease (1 for presence, 0 for absence). Thereby, this structured representation makes the dataset very useful for a wide range of applications, including machine learning-based disease prediction, clinical decision support systems, and epidemiological studies. Although there are some advancements in the field of disease-symptom datasets, there is a significant gap in structured datasets for the Bangla language. This dataset aims to bridge that gap by facilitating the development of multilingual medical informatics tools and improving disease prediction models for underrepresented linguistic communities. Further developments should include region-specific diseases and further fine-tuning of symptom associations for better diagnostic performance
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study of LLM-as-a-Judge: How Design Choices Impact Evaluation Reliability</title>
<link>https://arxiv.org/abs/2506.13639</link>
<guid>https://arxiv.org/abs/2506.13639</guid>
<content:encoded><![CDATA[
arXiv:2506.13639v1 Announce Type: new 
Abstract: As large language models (LLMs) continue to advance, reliable evaluation methods are essential particularly for open-ended, instruction-following tasks. LLM-as-a-Judge enables automatic evaluation using LLMs as evaluators, but its reliability remains uncertain. In this work, we analyze key factors affecting its trustworthiness, focusing on alignment with human judgments and evaluation consistency. Using BIGGENBench and EvalBiasBench, we study the effects of evaluation design, decoding strategies, and Chain-of-Tought (CoT) reasoning in evaluation. Our results show that evaluation criteria are critical for reliability, non-deterministic sampling improves alignment with human preferences over deterministic evaluation, and CoT reasoning offers minimal gains when clear evaluation criteria are present.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvolvTrip: Enhancing Literary Character Understanding with Temporal Theory-of-Mind Graphs</title>
<link>https://arxiv.org/abs/2506.13641</link>
<guid>https://arxiv.org/abs/2506.13641</guid>
<content:encoded><![CDATA[
arXiv:2506.13641v1 Announce Type: new 
Abstract: A compelling portrayal of characters is essential to the success of narrative writing. For readers, appreciating a character's traits requires the ability to infer their evolving beliefs, desires, and intentions over the course of a complex storyline, a cognitive skill known as Theory-of-Mind (ToM). Performing ToM reasoning in prolonged narratives requires readers to integrate historical context with current narrative information, a task at which humans excel but Large Language Models (LLMs) often struggle. To systematically evaluate LLMs' ToM reasoning capability in long narratives, we construct LitCharToM, a benchmark of character-centric questions across four ToM dimensions from classic literature. Further, we introduce EvolvTrip, a perspective-aware temporal knowledge graph that tracks psychological development throughout narratives. Our experiments demonstrate that EvolvTrip consistently enhances performance of LLMs across varying scales, even in challenging extended-context scenarios. EvolvTrip proves to be particularly valuable for smaller models, partially bridging the performance gap with larger LLMs and showing great compatibility with lengthy narratives. Our findings highlight the importance of explicit representation of temporal character mental states in narrative comprehension and offer a foundation for more sophisticated character understanding. Our data and code are publicly available at https://github.com/Bernard-Yang/EvolvTrip.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prefix-Tuning+: Modernizing Prefix-Tuning through Attention Independent Prefix Data</title>
<link>https://arxiv.org/abs/2506.13674</link>
<guid>https://arxiv.org/abs/2506.13674</guid>
<content:encoded><![CDATA[
arXiv:2506.13674v1 Announce Type: new 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for rapidly adapting large language models (LLMs) to downstream tasks. Prefix-Tuning, an early and effective PEFT technique, demonstrated the ability to achieve performance comparable to full fine-tuning with significantly reduced computational and memory overhead. However, despite its earlier success, its effectiveness in training modern state-of-the-art LLMs has been very limited. In this work, we demonstrate empirically that Prefix-Tuning underperforms on LLMs because of an inherent tradeoff between input and prefix significance within the attention head. This motivates us to introduce Prefix-Tuning+, a novel architecture that generalizes the principles of Prefix-Tuning while addressing its shortcomings by shifting the prefix module out of the attention head itself. We further provide an overview of our construction process to guide future users when constructing their own context-based methods. Our experiments show that, across a diverse set of benchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning methods. Notably, it achieves performance on par with the widely adopted LoRA method on several general benchmarks, highlighting the potential modern extension of Prefix-Tuning approaches. Our findings suggest that by overcoming its inherent limitations, Prefix-Tuning can remain a competitive and relevant research direction in the landscape of parameter-efficient LLM adaptation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turning Down the Heat: A Critical Analysis of Min-p Sampling in Language Models</title>
<link>https://arxiv.org/abs/2506.13681</link>
<guid>https://arxiv.org/abs/2506.13681</guid>
<content:encoded><![CDATA[
arXiv:2506.13681v1 Announce Type: new 
Abstract: Sampling from language models impacts the quality and diversity of outputs, affecting both research and real-world applications. Recently, Nguyen et al. 2024's "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs" introduced a new sampler called min-p, claiming it achieves superior quality and diversity over established samplers such as basic, top-k, and top-p sampling. The significance of these claims was underscored by the paper's recognition as the 18th highest-scoring submission to ICLR 2025 and selection for an Oral presentation. This paper conducts a comprehensive re-examination of the evidence supporting min-p and reaches different conclusions from the original paper's four lines of evidence. First, the original paper's human evaluations omitted data, conducted statistical tests incorrectly, and described qualitative feedback inaccurately; our reanalysis demonstrates min-p did not outperform baselines in quality, diversity, or a trade-off between quality and diversity; in response to our findings, the authors of the original paper conducted a new human evaluation using a different implementation, task, and rubric that nevertheless provides further evidence min-p does not improve over baselines. Second, comprehensively sweeping the original paper's NLP benchmarks reveals min-p does not surpass baselines when controlling for the number of hyperparameters. Third, the original paper's LLM-as-a-Judge evaluations lack methodological clarity and appear inconsistently reported. Fourth, community adoption claims (49k GitHub repositories, 1.1M GitHub stars) were found to be unsubstantiated, leading to their removal; the revised adoption claim remains misleading. We conclude that evidence presented in the original paper fails to support claims that min-p improves quality, diversity, or a trade-off between quality and diversity.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Knowledge Delivery and Emotional Comfort in Healthcare Conversational Systems</title>
<link>https://arxiv.org/abs/2506.13692</link>
<guid>https://arxiv.org/abs/2506.13692</guid>
<content:encoded><![CDATA[
arXiv:2506.13692v1 Announce Type: new 
Abstract: With the advancement of large language models, many dialogue systems are now capable of providing reasonable and informative responses to patients' medical conditions. However, when patients consult their doctor, they may experience negative emotions due to the severity and urgency of their situation. If the model can provide appropriate comfort and empathy based on the patient's negative emotions while answering medical questions, it will likely offer a more reassuring experience during the medical consultation process. To address this issue, our paper explores the balance between knowledge sharing and emotional support in the healthcare dialogue process. We utilize a large language model to rewrite a real-world interactive medical dialogue dataset, generating patient queries with negative emotions and corresponding medical responses aimed at soothing the patient's emotions while addressing their concerns. The modified data serves to refine the latest large language models with various fine-tuning methods, enabling them to accurately provide sentences with both emotional reassurance and constructive suggestions in response to patients' questions. Compared to the original LLM model, our experimental results demonstrate that our methodology significantly enhances the model's ability to generate emotional responses while maintaining its original capability to provide accurate knowledge-based answers.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction Following by Boosting Attention of Large Language Models</title>
<link>https://arxiv.org/abs/2506.13734</link>
<guid>https://arxiv.org/abs/2506.13734</guid>
<content:encoded><![CDATA[
arXiv:2506.13734v1 Announce Type: new 
Abstract: Controlling the generation of large language models (LLMs) remains a central challenge to ensure their safe and reliable deployment. While prompt engineering and finetuning are common approaches, recent work has explored latent steering, a lightweight technique that alters LLM internal activations to guide generation. However, subsequent studies revealed latent steering's effectiveness to be limited, often underperforming simple instruction prompting. To address this limitation, we first establish a benchmark across diverse behaviors for standardized evaluation of steering techniques. Building on insights from this benchmark, we introduce Instruction Attention Boosting (InstABoost), a latent steering method that boosts the strength of instruction prompting by altering the model's attention during generation. InstABoost combines the strengths of existing approaches and is theoretically supported by prior work that suggests that in-context rule following in transformer-based models can be controlled by manipulating attention on instructions. Empirically, InstABoost demonstrates superior control success compared to both traditional prompting and latent steering.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LTRR: Learning To Rank Retrievers for LLMs</title>
<link>https://arxiv.org/abs/2506.13743</link>
<guid>https://arxiv.org/abs/2506.13743</guid>
<content:encoded><![CDATA[
arXiv:2506.13743v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed retriever, despite growing evidence that no single retriever performs optimally across all query types. In this paper, we explore a query routing approach that dynamically selects from a pool of retrievers based on the query, using both train-free heuristics and learned routing models. We frame routing as a learning-to-rank (LTR) problem and introduce LTRR, a framework that learns to rank retrievers by their expected utility gain to downstream LLM performance. Our experiments, conducted on synthetic QA data with controlled query type variations, show that routing-based RAG systems can outperform the best single-retriever-based systems. Performance gains are especially pronounced in models trained with the Answer Correctness (AC) metric and with pairwise learning approaches, especially with XGBoost. We also observe improvements in generalization to out-of-distribution queries. As part of the SIGIR 2025 LiveRAG challenge, our submitted system demonstrated the practical viability of our approach, achieving competitive performance in both answer correctness and faithfulness. These findings highlight the importance of both training methodology and metric selection in query routing for RAG systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering LLM Thinking with Budget Guidance</title>
<link>https://arxiv.org/abs/2506.13752</link>
<guid>https://arxiv.org/abs/2506.13752</guid>
<content:encoded><![CDATA[
arXiv:2506.13752v1 Announce Type: new 
Abstract: Recent deep-thinking large language models often reason extensively to improve performance, but such lengthy reasoning is not always desirable, as it incurs excessive inference costs with disproportionate performance gains. Controlling reasoning length without sacrificing performance is therefore important, but remains challenging, especially under tight thinking budgets. We propose budget guidance, a simple yet effective method for steering the reasoning process of LLMs toward a target budget without requiring any LLM fine-tuning. Our approach introduces a lightweight predictor that models a Gamma distribution over the remaining thinking length during next-token generation. This signal is then used to guide generation in a soft, token-level manner, ensuring that the overall reasoning trace adheres to the specified thinking budget. Budget guidance enables natural control of the thinking length, along with significant token efficiency improvements over baseline methods on challenging math benchmarks. For instance, it achieves up to a 26% accuracy gain on the MATH-500 benchmark under tight budgets compared to baseline methods, while maintaining competitive accuracy with only 63% of the thinking tokens used by the full-thinking model. Budget guidance also generalizes to broader task domains and exhibits emergent capabilities, such as estimating question difficulty. The source code is available at: https://github.com/UMass-Embodied-AGI/BudgetGuidance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models</title>
<link>https://arxiv.org/abs/2506.12059</link>
<guid>https://arxiv.org/abs/2506.12059</guid>
<content:encoded><![CDATA[
arXiv:2506.12059v1 Announce Type: cross 
Abstract: In real-world applications, automatic speech recognition (ASR) systems must handle overlapping speech from multiple speakers and recognize rare words like technical terms. Traditional methods address multi-talker ASR and contextual biasing separately, limiting performance in complex scenarios. We propose a unified framework that combines multi-talker overlapping speech recognition and contextual biasing into a single task. Our ASR method integrates pretrained speech encoders and large language models (LLMs), using optimized finetuning strategies. We also introduce a two-stage filtering algorithm to efficiently identify relevant rare words from large biasing lists and incorporate them into the LLM's prompt input, enhancing rare word recognition. Experiments show that our approach outperforms traditional contextual biasing methods, achieving a WER of 7.9% on LibriMix and 32.9% on AMI SDM when the biasing size is 1,000, demonstrating its effectiveness in complex speech scenarios.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seamless Dysfluent Speech Text Alignment for Disordered Speech Analysis</title>
<link>https://arxiv.org/abs/2506.12073</link>
<guid>https://arxiv.org/abs/2506.12073</guid>
<content:encoded><![CDATA[
arXiv:2506.12073v1 Announce Type: cross 
Abstract: Accurate alignment of dysfluent speech with intended text is crucial for automating the diagnosis of neurodegenerative speech disorders. Traditional methods often fail to model phoneme similarities effectively, limiting their performance. In this work, we propose Neural LCS, a novel approach for dysfluent text-text and speech-text alignment. Neural LCS addresses key challenges, including partial alignment and context-aware similarity mapping, by leveraging robust phoneme-level modeling. We evaluate our method on a large-scale simulated dataset, generated using advanced data simulation techniques, and real PPA data. Neural LCS significantly outperforms state-of-the-art models in both alignment accuracy and dysfluent speech segmentation. Our results demonstrate the potential of Neural LCS to enhance automated systems for diagnosing and analyzing speech disorders, offering a more accurate and linguistically grounded solution for dysfluent speech alignment.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence and Civil Discourse: How LLMs Moderate Climate Change Conversations</title>
<link>https://arxiv.org/abs/2506.12077</link>
<guid>https://arxiv.org/abs/2506.12077</guid>
<content:encoded><![CDATA[
arXiv:2506.12077v1 Announce Type: cross 
Abstract: As large language models (LLMs) become increasingly integrated into online platforms and digital communication spaces, their potential to influence public discourse - particularly in contentious areas like climate change - requires systematic investigation. This study examines how LLMs naturally moderate climate change conversations through their distinct communicative behaviors. We conduct a comparative analysis of conversations between LLMs and human users on social media platforms, using five advanced models: three open-source LLMs (Gemma, Llama 3, and Llama 3.3) and two commercial systems (GPT-4o by OpenAI and Claude 3.5 by Anthropic). Through sentiment analysis, we assess the emotional characteristics of responses from both LLMs and humans. The results reveal two key mechanisms through which LLMs moderate discourse: first, LLMs consistently display emotional neutrality, showing far less polarized sentiment than human users. Second, LLMs maintain lower emotional intensity across contexts, creating a stabilizing effect in conversations. These findings suggest that LLMs possess inherent moderating capacities that could improve the quality of public discourse on controversial topics. This research enhances our understanding of how AI might support more civil and constructive climate change discussions and informs the design of AI-assisted communication tools.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Earth-Scale Human-Like Societies with One Billion Agents</title>
<link>https://arxiv.org/abs/2506.12078</link>
<guid>https://arxiv.org/abs/2506.12078</guid>
<content:encoded><![CDATA[
arXiv:2506.12078v1 Announce Type: cross 
Abstract: Understanding how complex societal behaviors emerge from individual cognition and interactions requires both high-fidelity modeling of human behavior and large-scale simulations. Traditional agent-based models (ABMs) have been employed to study these dynamics for decades, but are constrained by simplified agent behaviors that fail to capture human complexity. Recent advances in large language models (LLMs) offer new opportunities by enabling agents to exhibit sophisticated social behaviors that go beyond rule-based logic, yet face significant scaling challenges. Here we present Light Society, an agent-based simulation framework that advances both fronts, efficiently modeling human-like societies at planetary scale powered by LLMs. Light Society formalizes social processes as structured transitions of agent and environment states, governed by a set of LLM-powered simulation operations, and executed through an event queue. This modular design supports both independent and joint component optimization, supporting efficient simulation of societies with over one billion agents. Large-scale simulations of trust games and opinion propagation--spanning up to one billion agents--demonstrate Light Society's high fidelity and efficiency in modeling social trust and information diffusion, while revealing scaling laws whereby larger simulations yield more stable and realistic emergent behaviors.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The CAISAR Platform: Extending the Reach of Machine Learning Specification and Verification</title>
<link>https://arxiv.org/abs/2506.12084</link>
<guid>https://arxiv.org/abs/2506.12084</guid>
<content:encoded><![CDATA[
arXiv:2506.12084v1 Announce Type: cross 
Abstract: The formal specification and verification of machine learning programs saw remarkable progress in less than a decade, leading to a profusion of tools. However, diversity may lead to fragmentation, resulting in tools that are difficult to compare, except for very specific benchmarks. Furthermore, this progress is heavily geared towards the specification and verification of a certain class of property, that is, local robustness properties. But while provers are becoming more and more efficient at solving local robustness properties, even slightly more complex properties, involving multiple neural networks for example, cannot be expressed in the input languages of winners of the International Competition of Verification of Neural Networks VNN-Comp. In this tool paper, we present CAISAR, an open-source platform dedicated to machine learning specification and verification. We present its specification language, suitable for modelling complex properties on neural networks, support vector machines and boosted trees. We show on concrete use-cases how specifications written in this language are automatically translated to queries to state-of-the-art provers, notably by using automated graph editing techniques, making it possible to use their off-the-shelf versions. The artifact to reproduce the paper claims is available at the following DOI: https://doi.org/10.5281/zenodo.15209510
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs): A Feynman-Based Architecture for Continuous Learning Over Streaming Data</title>
<link>https://arxiv.org/abs/2506.12111</link>
<guid>https://arxiv.org/abs/2506.12111</guid>
<content:encoded><![CDATA[
arXiv:2506.12111v1 Announce Type: cross 
Abstract: Real-time continuous learning over streaming data remains a central challenge in deep learning and AI systems. Traditional gradient-based models such as backpropagation through time (BPTT) face computational and stability limitations when dealing with temporally unbounded data. In this paper, we introduce a novel architecture, Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs), which leverages the Feynman technique of differentiation under the integral sign to formulate neural updates as integrals over historical data. This reformulation allows for smoother, more stable learning dynamics that are both physically interpretable and computationally tractable. Inspired by Feynman's path integral formalism and compatible with quantum gradient estimation frameworks, QIDINNs open a path toward hybrid classical-quantum neural computation. We demonstrate our model's effectiveness on synthetic and real-world streaming tasks, and we propose directions for quantum extensions and scalable implementations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative or Discriminative? Revisiting Text Classification in the Era of Transformers</title>
<link>https://arxiv.org/abs/2506.12181</link>
<guid>https://arxiv.org/abs/2506.12181</guid>
<content:encoded><![CDATA[
arXiv:2506.12181v1 Announce Type: cross 
Abstract: The comparison between discriminative and generative classifiers has intrigued researchers since Efron's seminal analysis of logistic regression versus discriminant analysis. While early theoretical work established that generative classifiers exhibit lower sample complexity but higher asymptotic error in simple linear settings, these trade-offs remain unexplored in the transformer era. We present the first comprehensive evaluation of modern generative and discriminative architectures - Auto-regressive modeling, Masked Language Modeling, Discrete Diffusion, and Encoders for text classification. Our study reveals that the classical 'two regimes' phenomenon manifests distinctly across different architectures and training paradigms. Beyond accuracy, we analyze sample efficiency, calibration, noise robustness, and ordinality across diverse scenarios. Our findings offer practical guidance for selecting the most suitable modeling approach based on real-world constraints such as latency and data limitations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Emergence to Control: Probing and Modulating Self-Reflection in Language Models</title>
<link>https://arxiv.org/abs/2506.12217</link>
<guid>https://arxiv.org/abs/2506.12217</guid>
<content:encoded><![CDATA[
arXiv:2506.12217v1 Announce Type: cross 
Abstract: Self-reflection -- the ability of a large language model (LLM) to revisit, evaluate, and revise its own reasoning -- has recently emerged as a powerful behavior enabled by reinforcement learning with verifiable rewards (RLVR). While self-reflection correlates with improved reasoning accuracy, its origin and underlying mechanisms remain poorly understood. In this work, {\it we first show that self-reflection is not exclusive to RLVR fine-tuned models: it already emerges, albeit rarely, in pretrained models}. To probe this latent ability, we introduce Reflection-Inducing Probing, a method that injects reflection-triggering reasoning traces from fine-tuned models into pretrained models. This intervention raises self-reflection frequency of Qwen2.5 from 0.6\% to 18.6\%, revealing a hidden capacity for reflection. Moreover, our analysis of internal representations shows that both pretrained and fine-tuned models maintain hidden states that distinctly separate self-reflective from non-reflective contexts. Leveraging this observation, {\it we then construct a self-reflection vector, a direction in activation space associated with self-reflective reasoning}. By manipulating this vector, we enable bidirectional control over the self-reflective behavior for both pretrained and fine-tuned models. Experiments across multiple reasoning benchmarks show that enhancing these vectors improves reasoning performance by up to 12\%, while suppressing them reduces computational cost, providing a flexible mechanism to navigate the trade-off between reasoning quality and efficiency without requiring additional training. Our findings further our understanding of self-reflection and support a growing body of work showing that understanding model internals can enable precise behavioral control.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Scene Understanding with Multimodal Large Language Models for Automated Vehicles</title>
<link>https://arxiv.org/abs/2506.12232</link>
<guid>https://arxiv.org/abs/2506.12232</guid>
<content:encoded><![CDATA[
arXiv:2506.12232v1 Announce Type: cross 
Abstract: Scene understanding is critical for various downstream tasks in autonomous driving, including facilitating driver-agent communication and enhancing human-centered explainability of autonomous vehicle (AV) decisions. This paper evaluates the capability of four multimodal large language models (MLLMs), including relatively small models, to understand scenes in a zero-shot, in-context learning setting. Additionally, we explore whether combining these models using an ensemble approach with majority voting can enhance scene understanding performance. Our experiments demonstrate that GPT-4o, the largest model, outperforms the others in scene understanding. However, the performance gap between GPT-4o and the smaller models is relatively modest, suggesting that advanced techniques such as improved in-context learning, retrieval-augmented generation (RAG), or fine-tuning could further optimize the smaller models' performance. We also observe mixed results with the ensemble approach: while some scene attributes show improvement in performance metrics such as F1-score, others experience a decline. These findings highlight the need for more sophisticated ensemble techniques to achieve consistent gains across all scene attributes. This study underscores the potential of leveraging MLLMs for scene understanding and provides insights into optimizing their performance for autonomous driving applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Datrics Text2SQL: A Framework for Natural Language to SQL Query Generation</title>
<link>https://arxiv.org/abs/2506.12234</link>
<guid>https://arxiv.org/abs/2506.12234</guid>
<content:encoded><![CDATA[
arXiv:2506.12234v1 Announce Type: cross 
Abstract: Text-to-SQL systems enable users to query databases using natural language, democratizing access to data analytics. However, they face challenges in understanding ambiguous phrasing, domain-specific vocabulary, and complex schema relationships. This paper introduces Datrics Text2SQL, a Retrieval-Augmented Generation (RAG)-based framework designed to generate accurate SQL queries by leveraging structured documentation, example-based learning, and domain-specific rules. The system builds a rich Knowledge Base from database documentation and question-query examples, which are stored as vector embeddings and retrieved through semantic similarity. It then uses this context to generate syntactically correct and semantically aligned SQL code. The paper details the architecture, training methodology, and retrieval logic, highlighting how the system bridges the gap between user intent and database structure without requiring SQL expertise.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProVox: Personalization and Proactive Planning for Situated Human-Robot Collaboration</title>
<link>https://arxiv.org/abs/2506.12248</link>
<guid>https://arxiv.org/abs/2506.12248</guid>
<content:encoded><![CDATA[
arXiv:2506.12248v1 Announce Type: cross 
Abstract: Collaborative robots must quickly adapt to their partner's intent and preferences to proactively identify helpful actions. This is especially true in situated settings where human partners can continually teach robots new high-level behaviors, visual concepts, and physical skills (e.g., through demonstration), growing the robot's capabilities as the human-robot pair work together to accomplish diverse tasks. In this work, we argue that robots should be able to infer their partner's goals from early interactions and use this information to proactively plan behaviors ahead of explicit instructions from the user. Building from the strong commonsense priors and steerability of large language models, we introduce ProVox ("Proactive Voice"), a novel framework that enables robots to efficiently personalize and adapt to individual collaborators. We design a meta-prompting protocol that empowers users to communicate their distinct preferences, intent, and expected robot behaviors ahead of starting a physical interaction. ProVox then uses the personalized prompt to condition a proactive language model task planner that anticipates a user's intent from the current interaction context and robot capabilities to suggest helpful actions; in doing so, we alleviate user burden, minimizing the amount of time partners spend explicitly instructing and supervising the robot. We evaluate ProVox through user studies grounded in household manipulation tasks (e.g., assembling lunch bags) that measure the efficiency of the collaboration, as well as features such as perceived helpfulness, ease of use, and reliability. Our analysis suggests that both meta-prompting and proactivity are critical, resulting in 38.7% faster task completion times and 31.9% less user burden relative to non-active baselines. Supplementary material, code, and videos can be found at https://provox-2025.github.io.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoFlood: Jailbreaking Large Language Models with Information Overload</title>
<link>https://arxiv.org/abs/2506.12274</link>
<guid>https://arxiv.org/abs/2506.12274</guid>
<content:encoded><![CDATA[
arXiv:2506.12274v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains. However, their potential to generate harmful responses has raised significant societal and regulatory concerns, especially when manipulated by adversarial techniques known as "jailbreak" attacks. Existing jailbreak methods typically involve appending carefully crafted prefixes or suffixes to malicious prompts in order to bypass the built-in safety mechanisms of these models.
  In this work, we identify a new vulnerability in which excessive linguistic complexity can disrupt built-in safety mechanisms-without the need for any added prefixes or suffixes-allowing attackers to elicit harmful outputs directly. We refer to this phenomenon as Information Overload.
  To automatically exploit this vulnerability, we propose InfoFlood, a jailbreak attack that transforms malicious queries into complex, information-overloaded queries capable of bypassing built-in safety mechanisms. Specifically, InfoFlood: (1) uses linguistic transformations to rephrase malicious queries, (2) identifies the root cause of failure when an attempt is unsuccessful, and (3) refines the prompt's linguistic structure to address the failure while preserving its malicious intent.
  We empirically validate the effectiveness of InfoFlood on four widely used LLMs-GPT-4o, GPT-3.5-turbo, Gemini 2.0, and LLaMA 3.1-by measuring their jailbreak success rates. InfoFlood consistently outperforms baseline attacks, achieving up to 3 times higher success rates across multiple jailbreak benchmarks. Furthermore, we demonstrate that commonly adopted post-processing defenses, including OpenAI's Moderation API, Perspective API, and SmoothLLM, fail to mitigate these attacks. This highlights a critical weakness in traditional AI safety guardrails when confronted with information overload-based jailbreaks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Generate High-Quality Test Cases for Algorithm Problems? TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure</title>
<link>https://arxiv.org/abs/2506.12278</link>
<guid>https://arxiv.org/abs/2506.12278</guid>
<content:encoded><![CDATA[
arXiv:2506.12278v1 Announce Type: cross 
Abstract: We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs in test-case generation. TestCase-Eval includes 500 algorithm problems and 100,000 human-crafted solutions from the Codeforces platform. It focuses on two pivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test sets probe diverse input scenarios and cover a wide range of potential failure modes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored test input that reveals a specific incorrect code implementation. We provide a comprehensive assessment of 19 state-of-the-art open-source and proprietary LLMs on TestCase-Eval, offering insights into their strengths and limitations in generating effective test cases for algorithm problems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perspective on Utilizing Foundation Models for Laboratory Automation in Materials Research</title>
<link>https://arxiv.org/abs/2506.12312</link>
<guid>https://arxiv.org/abs/2506.12312</guid>
<content:encoded><![CDATA[
arXiv:2506.12312v1 Announce Type: cross 
Abstract: This review explores the potential of foundation models to advance laboratory automation in the materials and chemical sciences. It emphasizes the dual roles of these models: cognitive functions for experimental planning and data analysis, and physical functions for hardware operations. While traditional laboratory automation has relied heavily on specialized, rigid systems, foundation models offer adaptability through their general-purpose intelligence and multimodal capabilities. Recent advancements have demonstrated the feasibility of using large language models (LLMs) and multimodal robotic systems to handle complex and dynamic laboratory tasks. However, significant challenges remain, including precision manipulation of hardware, integration of multimodal data, and ensuring operational safety. This paper outlines a roadmap highlighting future directions, advocating for close interdisciplinary collaboration, benchmark establishment, and strategic human-AI integration to realize fully autonomous experimental laboratories.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSDNet: Revisiting Incomplete Multimodal-Diffusion from Graph Spectrum Perspective for Conversation Emotion Recognition</title>
<link>https://arxiv.org/abs/2506.12325</link>
<guid>https://arxiv.org/abs/2506.12325</guid>
<content:encoded><![CDATA[
arXiv:2506.12325v1 Announce Type: cross 
Abstract: Multimodal emotion recognition in conversations (MERC) aims to infer the speaker's emotional state by analyzing utterance information from multiple sources (i.e., video, audio, and text). Compared with unimodality, a more robust utterance representation can be obtained by fusing complementary semantic information from different modalities. However, the modality missing problem severely limits the performance of MERC in practical scenarios. Recent work has achieved impressive performance on modality completion using graph neural networks and diffusion models, respectively. This inspires us to combine these two dimensions through the graph diffusion model to obtain more powerful modal recovery capabilities. Unfortunately, existing graph diffusion models may destroy the connectivity and local structure of the graph by directly adding Gaussian noise to the adjacency matrix, resulting in the generated graph data being unable to retain the semantic and topological information of the original graph. To this end, we propose a novel Graph Spectral Diffusion Network (GSDNet), which maps Gaussian noise to the graph spectral space of missing modalities and recovers the missing data according to its original distribution. Compared with previous graph diffusion methods, GSDNet only affects the eigenvalues of the adjacency matrix instead of destroying the adjacency matrix directly, which can maintain the global topological information and important spectral features during the diffusion process. Extensive experiments have demonstrated that GSDNet achieves state-of-the-art emotion recognition performance in various modality loss scenarios.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Suppression in Large Language Models: Auditing, Quantifying, and Characterizing Censorship in DeepSeek</title>
<link>https://arxiv.org/abs/2506.12349</link>
<guid>https://arxiv.org/abs/2506.12349</guid>
<content:encoded><![CDATA[
arXiv:2506.12349v1 Announce Type: cross 
Abstract: This study examines information suppression mechanisms in DeepSeek, an open-source large language model (LLM) developed in China. We propose an auditing framework and use it to analyze the model's responses to 646 politically sensitive prompts by comparing its final output with intermediate chain-of-thought (CoT) reasoning. Our audit unveils evidence of semantic-level information suppression in DeepSeek: sensitive content often appears within the model's internal reasoning but is omitted or rephrased in the final output. Specifically, DeepSeek suppresses references to transparency, government accountability, and civic mobilization, while occasionally amplifying language aligned with state propaganda. This study underscores the need for systematic auditing of alignment, content moderation, information suppression, and censorship practices implemented into widely-adopted AI models, to ensure transparency, accountability, and equitable access to unbiased information obtained by means of these systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QiMeng-Attention: SOTA Attention Operator is generated by SOTA Attention Algorithm</title>
<link>https://arxiv.org/abs/2506.12355</link>
<guid>https://arxiv.org/abs/2506.12355</guid>
<content:encoded><![CDATA[
arXiv:2506.12355v1 Announce Type: cross 
Abstract: The attention operator remains a critical performance bottleneck in large language models (LLMs), particularly for long-context scenarios. While FlashAttention is the most widely used and effective GPU-aware acceleration algorithm, it must require time-consuming and hardware-specific manual implementation, limiting adaptability across GPU architectures. Existing LLMs have shown a lot of promise in code generation tasks, but struggle to generate high-performance attention code. The key challenge is it cannot comprehend the complex data flow and computation process of the attention operator and utilize low-level primitive to exploit GPU performance.
  To address the above challenge, we propose an LLM-friendly Thinking Language (LLM-TL) to help LLMs decouple the generation of high-level optimization logic and low-level implementation on GPU, and enhance LLMs' understanding of attention operator. Along with a 2-stage reasoning workflow, TL-Code generation and translation, the LLMs can automatically generate FlashAttention implementation on diverse GPUs, establishing a self-optimizing paradigm for generating high-performance attention operators in attention-centric algorithms. Verified on A100, RTX8000, and T4 GPUs, the performance of our methods significantly outshines that of vanilla LLMs, achieving a speed-up of up to 35.16x. Besides, our method not only surpasses human-optimized libraries (cuDNN and official library) in most scenarios but also extends support to unsupported hardware and data types, reducing development time from months to minutes compared with human experts.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-R5: MultiModal Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval</title>
<link>https://arxiv.org/abs/2506.12364</link>
<guid>https://arxiv.org/abs/2506.12364</guid>
<content:encoded><![CDATA[
arXiv:2506.12364v1 Announce Type: cross 
Abstract: Multimodal document retrieval systems enable information access across text, images, and layouts, benefiting various domains like document-based question answering, report analysis, and interactive content summarization. Rerankers improve retrieval precision by reordering retrieved candidates. However, current multimodal reranking methods remain underexplored, with significant room for improvement in both training strategies and overall effectiveness. Moreover, the lack of explicit reasoning makes it difficult to analyze and optimize these methods further. In this paper, We propose MM-R5, a MultiModal Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval, aiming to provide a more effective and reliable solution for multimodal reranking tasks. MM-R5 is trained in two stages: supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we focus on improving instruction-following and guiding the model to generate complete and high-quality reasoning chains. To support this, we introduce a novel data construction strategy that produces rich, high-quality reasoning data. In the RL stage, we design a task-specific reward framework, including a reranking reward tailored for multimodal candidates and a composite template-based reward to further refine reasoning quality. We conduct extensive experiments on MMDocIR, a challenging public benchmark spanning multiple domains. MM-R5 achieves state-of-the-art performance on most metrics and delivers comparable results to much larger models on the remaining ones. Moreover, compared to the best retrieval-only method, MM-R5 improves recall@1 by over 4%. These results validate the effectiveness of our reasoning-enhanced training pipeline.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConsistencyChecker: Tree-based Evaluation of LLM Generalization Capabilities</title>
<link>https://arxiv.org/abs/2506.12376</link>
<guid>https://arxiv.org/abs/2506.12376</guid>
<content:encoded><![CDATA[
arXiv:2506.12376v1 Announce Type: cross 
Abstract: Evaluating consistency in large language models (LLMs) is crucial for ensuring reliability, particularly in complex, multi-step interactions between humans and LLMs. Traditional self-consistency methods often miss subtle semantic changes in natural language and functional shifts in code or equations, which can accumulate over multiple transformations. To address this, we propose ConsistencyChecker, a tree-based evaluation framework designed to measure consistency through sequences of reversible transformations, including machine translation tasks and AI-assisted programming tasks. In our framework, nodes represent distinct text states, while edges correspond to pairs of inverse operations. Dynamic and LLM-generated benchmarks ensure a fair assessment of the model's generalization ability and eliminate benchmark leakage. Consistency is quantified based on similarity across different depths of the transformation tree. Experiments on eight models from various families and sizes show that ConsistencyChecker can distinguish the performance of different models. Notably, our consistency scores-computed entirely without using WMT paired data-correlate strongly (r > 0.7) with WMT 2024 auto-ranking, demonstrating the validity of our benchmark-free approach. Our implementation is available at: https://github.com/ulab-uiuc/consistencychecker.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Merging for Knowledge Editing</title>
<link>https://arxiv.org/abs/2506.12384</link>
<guid>https://arxiv.org/abs/2506.12384</guid>
<content:encoded><![CDATA[
arXiv:2506.12384v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) require continuous updates to maintain accurate and current knowledge as the world evolves. While existing knowledge editing approaches offer various solutions for knowledge updating, they often struggle with sequential editing scenarios and harm the general capabilities of the model, thereby significantly hampering their practical applicability. This paper proposes a two-stage framework combining robust supervised fine-tuning (R-SFT) with model merging for knowledge editing. Our method first fine-tunes the LLM to internalize new knowledge fully, then merges the fine-tuned model with the original foundation model to preserve newly acquired knowledge and general capabilities. Experimental results demonstrate that our approach significantly outperforms existing methods in sequential editing while better preserving the original performance of the model, all without requiring any architectural changes. Code is available at: https://github.com/Applied-Machine-Learning-Lab/MM4KE.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plan Your Travel and Travel with Your Plan: Wide-Horizon Planning and Evaluation via LLM</title>
<link>https://arxiv.org/abs/2506.12421</link>
<guid>https://arxiv.org/abs/2506.12421</guid>
<content:encoded><![CDATA[
arXiv:2506.12421v1 Announce Type: cross 
Abstract: Travel planning is a complex task requiring the integration of diverse real-world information and user preferences. While LLMs show promise, existing methods with long-horizon thinking struggle with handling multifaceted constraints and preferences in the context, leading to suboptimal itineraries. We formulate this as an $L^3$ planning problem, emphasizing long context, long instruction, and long output. To tackle this, we introduce Multiple Aspects of Planning (MAoP), enabling LLMs to conduct wide-horizon thinking to solve complex planning problems. Instead of direct planning, MAoP leverages the strategist to conduct pre-planning from various aspects and provide the planning blueprint for planning models, enabling strong inference-time scalability for better performance. In addition, current benchmarks overlook travel's dynamic nature, where past events impact subsequent journeys, failing to reflect real-world feasibility. To address this, we propose Travel-Sim, an agent-based benchmark assessing plans via real-world travel simulation. This work advances LLM capabilities in complex planning and offers novel insights for evaluating sophisticated scenarios through agent-based simulation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Flow: Perspectives, Scenarios, and Approaches</title>
<link>https://arxiv.org/abs/2506.12479</link>
<guid>https://arxiv.org/abs/2506.12479</guid>
<content:encoded><![CDATA[
arXiv:2506.12479v1 Announce Type: cross 
Abstract: Pioneered by the foundational information theory by Claude Shannon and the visionary framework of machine intelligence by Alan Turing, the convergent evolution of information and communication technologies (IT/CT) has created an unbroken wave of connectivity and computation. This synergy has sparked a technological revolution, now reaching its peak with large artificial intelligence (AI) models that are reshaping industries and redefining human-machine collaboration. However, the realization of ubiquitous intelligence faces considerable challenges due to substantial resource consumption in large models and high communication bandwidth demands. To address these challenges, AI Flow has been introduced as a multidisciplinary framework that integrates cutting-edge IT and CT advancements, with a particular emphasis on the following three key points. First, device-edge-cloud framework serves as the foundation, which integrates end devices, edge servers, and cloud clusters to optimize scalability and efficiency for low-latency model inference. Second, we introduce the concept of familial models, which refers to a series of different-sized models with aligned hidden features, enabling effective collaboration and the flexibility to adapt to varying resource constraints and dynamic scenarios. Third, connectivity- and interaction-based intelligence emergence is a novel paradigm of AI Flow. By leveraging communication networks to enhance connectivity, the collaboration among AI models across heterogeneous nodes achieves emergent intelligence that surpasses the capability of any single model. The innovations of AI Flow provide enhanced intelligence, timely responsiveness, and ubiquitous accessibility to AI services, paving the way for the tighter fusion of AI techniques and communication systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MALM: A Multi-Information Adapter for Large Language Models to Mitigate Hallucination</title>
<link>https://arxiv.org/abs/2506.12483</link>
<guid>https://arxiv.org/abs/2506.12483</guid>
<content:encoded><![CDATA[
arXiv:2506.12483v1 Announce Type: cross 
Abstract: Large language models (LLMs) are prone to three types of hallucination: Input-Conflicting, Context-Conflicting and Fact-Conflicting hallucinations. The purpose of this study is to mitigate the different types of hallucination by exploiting the interdependence between them. For this purpose, we propose a Multi-Information Adapter for Large Language Models (MALM). This framework employs a tailored multi-graph learning approach designed to elucidate the interconnections between original inputs, contextual information, and external factual knowledge, thereby alleviating the three categories of hallucination within a cohesive framework. Experiments were carried out on four benchmarking datasets: HaluEval, TruthfulQA, Natural Questions, and TriviaQA. We evaluated the proposed framework in two aspects: (1) adaptability to different base LLMs on HaluEval and TruthfulQA, to confirm if MALM is effective when applied on 7 typical LLMs. MALM showed significant improvements over LLaMA-2; (2) generalizability to retrieval-augmented generation (RAG) by combining MALM with three representative retrievers (BM25, Spider and DPR) separately. Furthermore, automated and human evaluations were conducted to substantiate the correctness of experimental results, where GPT-4 and 3 human volunteers judged which response was better between LLaMA-2 and MALM. The results showed that both GPT-4 and human preferred MALM in 79.4% and 65.6% of cases respectively. The results validate that incorporating the complex interactions between the three types of hallucination through a multilayered graph attention network into the LLM generation process is effective to mitigate the them. The adapter design of the proposed approach is also proven flexible and robust across different base LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization</title>
<link>https://arxiv.org/abs/2506.12484</link>
<guid>https://arxiv.org/abs/2506.12484</guid>
<content:encoded><![CDATA[
arXiv:2506.12484v1 Announce Type: cross 
Abstract: Language models can retain dangerous knowledge and skills even after extensive safety fine-tuning, posing both misuse and misalignment risks. Recent studies show that even specialized unlearning methods can be easily reversed. To address this, we systematically evaluate many existing and novel components of unlearning methods and identify ones crucial for irreversible unlearning.
  We introduce Disruption Masking, a technique in which we only allow updating weights, where the signs of the unlearning gradient and the retaining gradient are the same. This ensures all updates are non-disruptive.
  Additionally, we identify the need for normalizing the unlearning gradients, and also confirm the usefulness of meta-learning. We combine these insights into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and validate its effectiveness at preventing the recovery of dangerous capabilities. MUDMAN outperforms the prior TAR method by 40\%, setting a new state-of-the-art for robust unlearning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreamMel: Real-Time Zero-shot Text-to-Speech via Interleaved Continuous Autoregressive Modeling</title>
<link>https://arxiv.org/abs/2506.12570</link>
<guid>https://arxiv.org/abs/2506.12570</guid>
<content:encoded><![CDATA[
arXiv:2506.12570v1 Announce Type: cross 
Abstract: Recent advances in zero-shot text-to-speech (TTS) synthesis have achieved high-quality speech generation for unseen speakers, but most systems remain unsuitable for real-time applications because of their offline design. Current streaming TTS paradigms often rely on multi-stage pipelines and discrete representations, leading to increased computational cost and suboptimal system performance. In this work, we propose StreamMel, a pioneering single-stage streaming TTS framework that models continuous mel-spectrograms. By interleaving text tokens with acoustic frames, StreamMel enables low-latency, autoregressive synthesis while preserving high speaker similarity and naturalness. Experiments on LibriSpeech demonstrate that StreamMel outperforms existing streaming TTS baselines in both quality and latency. It even achieves performance comparable to offline systems while supporting efficient real-time generation, showcasing broad prospects for integration with real-time speech large language models. Audio samples are available at: https://aka.ms/StreamMel.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MS4UI: A Dataset for Multi-modal Summarization of User Interface Instructional Videos</title>
<link>https://arxiv.org/abs/2506.12623</link>
<guid>https://arxiv.org/abs/2506.12623</guid>
<content:encoded><![CDATA[
arXiv:2506.12623v1 Announce Type: cross 
Abstract: We study multi-modal summarization for instructional videos, whose goal is to provide users an efficient way to learn skills in the form of text instructions and key video frames. We observe that existing benchmarks focus on generic semantic-level video summarization, and are not suitable for providing step-by-step executable instructions and illustrations, both of which are crucial for instructional videos. We propose a novel benchmark for user interface (UI) instructional video summarization to fill the gap. We collect a dataset of 2,413 UI instructional videos, which spans over 167 hours. These videos are manually annotated for video segmentation, text summarization, and video summarization, which enable the comprehensive evaluations for concise and executable video summarization. We conduct extensive experiments on our collected MS4UI dataset, which suggest that state-of-the-art multi-modal summarization methods struggle on UI video summarization, and highlight the importance of new methods for UI instructional video summarization.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SC-SOT: Conditioning the Decoder on Diarized Speaker Information for End-to-End Overlapped Speech Recognition</title>
<link>https://arxiv.org/abs/2506.12672</link>
<guid>https://arxiv.org/abs/2506.12672</guid>
<content:encoded><![CDATA[
arXiv:2506.12672v1 Announce Type: cross 
Abstract: We propose Speaker-Conditioned Serialized Output Training (SC-SOT), an enhanced SOT-based training for E2E multi-talker ASR. We first probe how SOT handles overlapped speech, and we found the decoder performs implicit speaker separation. We hypothesize this implicit separation is often insufficient due to ambiguous acoustic cues in overlapping regions. To address this, SC-SOT explicitly conditions the decoder on speaker information, providing detailed information about "who spoke when". Specifically, we enhance the decoder by incorporating: (1) speaker embeddings, which allow the model to focus on the acoustic characteristics of the target speaker, and (2) speaker activity information, which guides the model to suppress non-target speakers. The speaker embeddings are derived from a jointly trained E2E speaker diarization model, mitigating the need for speaker enrollment. Experimental results demonstrate the effectiveness of our conditioning approach on overlapped speech.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression</title>
<link>https://arxiv.org/abs/2506.12707</link>
<guid>https://arxiv.org/abs/2506.12707</guid>
<content:encoded><![CDATA[
arXiv:2506.12707v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved widespread adoption across numerous applications. However, many LLMs are vulnerable to malicious attacks even after safety alignment. These attacks typically bypass LLMs' safety guardrails by wrapping the original malicious instructions inside adversarial jailbreaks prompts. Previous research has proposed methods such as adversarial training and prompt rephrasing to mitigate these safety vulnerabilities, but these methods often reduce the utility of LLMs or lead to significant computational overhead and online latency. In this paper, we propose SecurityLingua, an effective and efficient approach to defend LLMs against jailbreak attacks via security-oriented prompt compression. Specifically, we train a prompt compressor designed to discern the "true intention" of the input prompt, with a particular focus on detecting the malicious intentions of adversarial prompts. Then, in addition to the original prompt, the intention is passed via the system prompt to the target LLM to help it identify the true intention of the request. SecurityLingua ensures a consistent user experience by leaving the original input prompt intact while revealing the user's potentially malicious intention and stimulating the built-in safety guardrails of the LLM. Moreover, thanks to prompt compression, SecurityLingua incurs only a negligible overhead and extra token cost compared to all existing defense methods, making it an especially practical solution for LLM defense. Experimental results demonstrate that SecurityLingua can effectively defend against malicious attacks and maintain utility of the LLM with negligible compute and latency overhead. Our code is available at https://aka.ms/SecurityLingua.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest Code Competition?</title>
<link>https://arxiv.org/abs/2506.12713</link>
<guid>https://arxiv.org/abs/2506.12713</guid>
<content:encoded><![CDATA[
arXiv:2506.12713v1 Announce Type: cross 
Abstract: Code generation is a core capability of large language models (LLMs), yet mainstream benchmarks (e.g., APPs and LiveCodeBench) contain questions with medium-level difficulty and pose no challenge to advanced LLMs. To better reflected the advanced reasoning and code generation ability, We introduce Humanity's Last Code Exam (HLCE), comprising 235 most challenging problems from the International Collegiate Programming Contest (ICPC World Finals) and the International Olympiad in Informatics (IOI) spanning 2010 - 2024. As part of HLCE, we design a harmonized online-offline sandbox that guarantees fully reproducible evaluation. Through our comprehensive evaluation, we observe that even the strongest reasoning LLMs: o4-mini(high) and Gemini-2.5 Pro, achieve pass@1 rates of only 15.9% and 11.4%, respectively. Meanwhile, we propose a novel "self-recognition" task to measure LLMs' awareness of their own capabilities. Results indicate that LLMs' self-recognition abilities are not proportionally correlated with their code generation performance. Finally, our empirical validation of test-time scaling laws reveals that current advanced LLMs have substantial room for improvement on complex programming tasks. We expect HLCE to become a milestone challenge for code generation and to catalyze advances in high-performance reasoning and human-AI collaborative programming. Our code and dataset are also public available(https://github.com/Humanity-s-Last-Code-Exam/HLCE).
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Scaling of Test-Time Compute: A Bandit Learning Approach</title>
<link>https://arxiv.org/abs/2506.12721</link>
<guid>https://arxiv.org/abs/2506.12721</guid>
<content:encoded><![CDATA[
arXiv:2506.12721v1 Announce Type: cross 
Abstract: Scaling test-time compute has emerged as an effective strategy for improving the performance of large language models. However, existing methods typically allocate compute uniformly across all queries, overlooking variation in query difficulty. To address this inefficiency, we formulate test-time compute allocation as a novel bandit learning problem and propose adaptive algorithms that estimate query difficulty on the fly and allocate compute accordingly. Compared to uniform allocation, our algorithms allocate more compute to challenging queries while maintaining accuracy on easier ones. Among challenging queries, our algorithms further learn to prioritize solvable instances, effectively reducing excessive computing on unsolvable queries. We theoretically prove that our algorithms achieve better compute efficiency than uniform allocation and empirically validate their effectiveness on math and code benchmarks. Specifically, our algorithms achieve up to an 11.10% performance improvement (15.04% relative) on the MATH-500 dataset and up to a 7.41% performance improvement (14.40% relative) on LiveCodeBench.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking DPO: The Role of Rejected Responses in Preference Misalignment</title>
<link>https://arxiv.org/abs/2506.12725</link>
<guid>https://arxiv.org/abs/2506.12725</guid>
<content:encoded><![CDATA[
arXiv:2506.12725v1 Announce Type: cross 
Abstract: Direct Preference Optimization (DPO) is a simple and efficient framework that has attracted substantial attention. However, it often struggles to meet its primary objectives -- increasing the generation probability of chosen responses while reducing that of rejected responses -- due to the dominant influence of rejected responses on the loss function. This imbalance leads to suboptimal performance in promoting preferred responses. In this work, we systematically analyze the limitations of DPO and existing algorithms designed to achieve the objectives stated above. To address these limitations, we propose Bounded-DPO (BDPO), a novel method that bounds the influence of rejected responses while maintaining the original optimization structure of DPO. Through theoretical analysis and empirical evaluations, we demonstrate that BDPO achieves a balanced optimization of the chosen and rejected responses, outperforming existing algorithms.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WereWolf-Plus: An Update of Werewolf Game setting Based on DSGBench</title>
<link>https://arxiv.org/abs/2506.12841</link>
<guid>https://arxiv.org/abs/2506.12841</guid>
<content:encoded><![CDATA[
arXiv:2506.12841v1 Announce Type: cross 
Abstract: With the rapid development of LLM-based agents, increasing attention has been given to their social interaction and strategic reasoning capabilities. However, existing Werewolf-based benchmarking platforms suffer from overly simplified game settings, incomplete evaluation metrics, and poor scalability. To address these limitations, we propose WereWolf-Plus, a multi-model, multi-dimensional, and multi-method benchmarking platform for evaluating multi-agent strategic reasoning in the Werewolf game. The platform offers strong extensibility, supporting customizable configurations for roles such as Seer, Witch, Hunter, Guard, and Sheriff, along with flexible model assignment and reasoning enhancement strategies for different roles. In addition, we introduce a comprehensive set of quantitative evaluation metrics for all special roles, werewolves, and the sheriff, and enrich the assessment dimensions for agent reasoning ability, cooperation capacity, and social influence. WereWolf-Plus provides a more flexible and reliable environment for advancing research on inference and strategic interaction within multi-agent communities. Our code is open sourced at https://github.com/MinstrelsyXia/WereWolfPlus.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying and Investigating Global News Coverage of Critical Events Such as Disasters and Terrorist Attacks</title>
<link>https://arxiv.org/abs/2506.12925</link>
<guid>https://arxiv.org/abs/2506.12925</guid>
<content:encoded><![CDATA[
arXiv:2506.12925v1 Announce Type: cross 
Abstract: Comparative studies of news coverage are challenging to conduct because methods to identify news articles about the same event in different languages require expertise that is difficult to scale. We introduce an AI-powered method for identifying news articles based on an event FINGERPRINT, which is a minimal set of metadata required to identify critical events. Our event coverage identification method, FINGERPRINT TO ARTICLE MATCHING FOR EVENTS (FAME), efficiently identifies news articles about critical world events, specifically terrorist attacks and several types of natural disasters. FAME does not require training data and is able to automatically and efficiently identify news articles that discuss an event given its fingerprint: time, location, and class (such as storm or flood). The method achieves state-of-the-art performance and scales to massive databases of tens of millions of news articles and hundreds of events happening globally. We use FAME to identify 27,441 articles that cover 470 natural disaster and terrorist attack events that happened in 2020. To this end, we use a massive database of news articles in three languages from MediaCloud, and three widely used, expert-curated databases of critical events: EM-DAT, USGS, and GTD. Our case study reveals patterns consistent with prior literature: coverage of disasters and terrorist attacks correlates to death counts, to the GDP of a country where the event occurs, and to trade volume between the reporting country and the country where the event occurred. We share our NLP annotations and cross-country media attention data to support the efforts of researchers and media monitoring organizations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sectoral Coupling in Linguistic State Space</title>
<link>https://arxiv.org/abs/2506.12927</link>
<guid>https://arxiv.org/abs/2506.12927</guid>
<content:encoded><![CDATA[
arXiv:2506.12927v1 Announce Type: cross 
Abstract: This work presents a formal framework for quantifying the internal dependencies between functional subsystems within artificial agents whose belief states are composed of structured linguistic fragments. Building on the Semantic Manifold framework, which organizes belief content into functional sectors and stratifies them across hierarchical levels of abstraction, we introduce a system of sectoral coupling constants that characterize how one cognitive sector influences another within a fixed level of abstraction. The complete set of these constants forms an agent-specific coupling profile that governs internal information flow, shaping the agent's overall processing tendencies and cognitive style. We provide a detailed taxonomy of these intra-level coupling roles, covering domains such as perceptual integration, memory access and formation, planning, meta-cognition, execution control, and affective modulation. We also explore how these coupling profiles generate feedback loops, systemic dynamics, and emergent signatures of cognitive behavior. Methodologies for inferring these profiles from behavioral or internal agent data are outlined, along with a discussion of how these couplings evolve across abstraction levels. This framework contributes a mechanistic and interpretable approach to modeling complex cognition, with applications in AI system design, alignment diagnostics, and the analysis of emergent agent behavior.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HypER: Literature-grounded Hypothesis Generation and Distillation with Provenance</title>
<link>https://arxiv.org/abs/2506.12937</link>
<guid>https://arxiv.org/abs/2506.12937</guid>
<content:encoded><![CDATA[
arXiv:2506.12937v1 Announce Type: cross 
Abstract: Large Language models have demonstrated promising performance in research ideation across scientific domains. Hypothesis development, the process of generating a highly specific declarative statement connecting a research idea with empirical validation, has received relatively less attention. Existing approaches trivially deploy retrieval augmentation and focus only on the quality of the final output ignoring the underlying reasoning process behind ideation. We present $\texttt{HypER}$ ($\textbf{Hyp}$othesis Generation with $\textbf{E}$xplanation and $\textbf{R}$easoning), a small language model (SLM) trained for literature-guided reasoning and evidence-based hypothesis generation. $\texttt{HypER}$ is trained in a multi-task setting to discriminate between valid and invalid scientific reasoning chains in presence of controlled distractions. We find that $\texttt{HypER}$ outperformes the base model, distinguishing valid from invalid reasoning chains (+22\% average absolute F1), generates better evidence-grounded hypotheses (0.327 vs. 0.305 base model) with high feasibility and impact as judged by human experts ($>$3.5 on 5-point Likert scale).
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition</title>
<link>https://arxiv.org/abs/2506.12953</link>
<guid>https://arxiv.org/abs/2506.12953</guid>
<content:encoded><![CDATA[
arXiv:2506.12953v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have demonstrated new possibilities for accurate and efficient time series analysis, but prior work often required heavy fine-tuning and/or ignored inter-series correlations. In this work, we explore simple and flexible prompt-based strategies that enable LLMs to perform time series forecasting without extensive retraining or the use of a complex external architecture. Through the exploration of specialized prompting methods that leverage time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation, we find that it is possible to enhance LLM forecasting quality while maintaining simplicity and requiring minimal preprocessing of data. To this end, we propose our own method, PatchInstruct, which enables LLMs to make precise and effective predictions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Neuro-Symbolic Retrieval-Augmented Generation through Adaptive Query Routing</title>
<link>https://arxiv.org/abs/2506.12981</link>
<guid>https://arxiv.org/abs/2506.12981</guid>
<content:encoded><![CDATA[
arXiv:2506.12981v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems address factual inconsistencies in Large Language Models by grounding generation in external knowledge, yet they face a fundamental efficiency problem: simple queries consume computational resources equivalent to complex multi-hop reasoning tasks. We present SymRAG, a neuro-symbolic framework that introduces adaptive query routing based on real-time complexity and system load assessments. SymRAG dynamically selects symbolic, neural, or hybrid processing paths to align resource use with query demands. Evaluated on 2,000 queries from HotpotQA and DROP using Llama-3.2-3B and Mistral-7B models, SymRAG achieves 97.6--100.0% exact match accuracy with significantly lower CPU utilization (3.6--6.2%) and processing time (0.985--3.165s). Disabling adaptive logic results in 169--1151% increase in processing time, highlighting the framework's impact. These results underscore the potential of adaptive neuro-symbolic routing for scalable, sustainable AI systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Graph Fusion with Large Language Models for Accurate, Explainable Manufacturing Process Planning</title>
<link>https://arxiv.org/abs/2506.13026</link>
<guid>https://arxiv.org/abs/2506.13026</guid>
<content:encoded><![CDATA[
arXiv:2506.13026v1 Announce Type: cross 
Abstract: Precision process planning in Computer Numerical Control (CNC) machining demands rapid, context-aware decisions on tool selection, feed-speed pairs, and multi-axis routing, placing immense cognitive and procedural burdens on engineers from design specification through final part inspection. Conventional rule-based computer-aided process planning and knowledge-engineering shells freeze domain know-how into static tables, which become limited when dealing with unseen topologies, novel material states, shifting cost-quality-sustainability weightings, or shop-floor constraints such as tool unavailability and energy caps. Large language models (LLMs) promise flexible, instruction-driven reasoning for tasks but they routinely hallucinate numeric values and provide no provenance. We present Augmented Retrieval Knowledge Network Enhanced Search & Synthesis (ARKNESS), the end-to-end framework that fuses zero-shot Knowledge Graph (KG) construction with retrieval-augmented generation to deliver verifiable, numerically exact answers for CNC process planning. ARKNESS (1) automatically distills heterogeneous machining documents, G-code annotations, and vendor datasheets into augmented triple, multi-relational graphs without manual labeling, and (2) couples any on-prem LLM with a retriever that injects the minimal, evidence-linked subgraph needed to answer a query. Benchmarked on 155 industry-curated questions spanning tool sizing and feed-speed optimization, a lightweight 3B-parameter Llama-3 augmented by ARKNESS matches GPT-4o accuracy while achieving a +25 percentage point gain in multiple-choice accuracy, +22.4 pp in F1, and 8.1x ROUGE-L on open-ended responses.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stress-Testing Multimodal Foundation Models for Crystallographic Reasoning</title>
<link>https://arxiv.org/abs/2506.13051</link>
<guid>https://arxiv.org/abs/2506.13051</guid>
<content:encoded><![CDATA[
arXiv:2506.13051v1 Announce Type: cross 
Abstract: Evaluating foundation models for crystallographic reasoning requires benchmarks that isolate generalization behavior while enforcing physical constraints. This work introduces a multiscale multicrystal dataset with two physically grounded evaluation protocols to stress-test multimodal generative models. The Spatial-Exclusion benchmark withholds all supercells of a given radius from a diverse dataset, enabling controlled assessments of spatial interpolation and extrapolation. The Compositional-Exclusion benchmark omits all samples of a specific chemical composition, probing generalization across stoichiometries. Nine vision--language foundation models are prompted with crystallographic images and textual context to generate structural annotations. Responses are evaluated via (i) relative errors in lattice parameters and density, (ii) a physics-consistency index penalizing volumetric violations, and (iii) a hallucination score capturing geometric outliers and invalid space-group predictions. These benchmarks establish a reproducible, physically informed framework for assessing generalization, consistency, and reliability in large-scale multimodal models. Dataset and code are available at https://github.com/KurbanIntelligenceLab/StressTestingMMFMinCR.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISM2: Unlocking Multi-Modal General Pathology AI with Clinical Dialogue</title>
<link>https://arxiv.org/abs/2506.13063</link>
<guid>https://arxiv.org/abs/2506.13063</guid>
<content:encoded><![CDATA[
arXiv:2506.13063v1 Announce Type: cross 
Abstract: Recent pathology foundation models can provide rich tile-level representations but fall short of delivering general-purpose clinical utility without further extensive model development. These models lack whole-slide image (WSI) understanding and are not trained with large-scale diagnostic data, limiting their performance on diverse downstream tasks. We introduce PRISM2, a multi-modal slide-level foundation model trained via clinical dialogue to enable scalable, generalizable pathology AI. PRISM2 is trained on nearly 700,000 specimens (2.3 million WSIs) paired with real-world clinical diagnostic reports in a two-stage process. In Stage 1, a vision-language model is trained using contrastive and captioning objectives to align whole slide embeddings with textual clinical diagnosis. In Stage 2, the language model is unfrozen to enable diagnostic conversation and extract more clinically meaningful representations from hidden states. PRISM2 achieves strong performance on diagnostic and biomarker prediction tasks, outperforming prior slide-level models including PRISM and TITAN. It also introduces a zero-shot yes/no classification approach that surpasses CLIP-style methods without prompt tuning or class enumeration. By aligning visual features with clinical reasoning, PRISM2 improves generalization on both data-rich and low-sample tasks, offering a scalable path forward for building general pathology AI agents capable of assisting diagnostic and prognostic decisions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equitable Electronic Health Record Prediction with FAME: Fairness-Aware Multimodal Embedding</title>
<link>https://arxiv.org/abs/2506.13104</link>
<guid>https://arxiv.org/abs/2506.13104</guid>
<content:encoded><![CDATA[
arXiv:2506.13104v1 Announce Type: cross 
Abstract: Electronic Health Record (EHR) data encompass diverse modalities -- text, images, and medical codes -- that are vital for clinical decision-making. To process these complex data, multimodal AI (MAI) has emerged as a powerful approach for fusing such information. However, most existing MAI models optimize for better prediction performance, potentially reinforcing biases across patient subgroups. Although bias-reduction techniques for multimodal models have been proposed, the individual strengths of each modality and their interplay in both reducing bias and optimizing performance remain underexplored. In this work, we introduce FAME (Fairness-Aware Multimodal Embeddings), a framework that explicitly weights each modality according to its fairness contribution. FAME optimizes both performance and fairness by incorporating a combined loss function. We leverage the Error Distribution Disparity Index (EDDI) to measure fairness across subgroups and propose a sign-agnostic aggregation method to balance fairness across subgroups, ensuring equitable model outcomes. We evaluate FAME with BEHRT and BioClinicalBERT, combining structured and unstructured EHR data, and demonstrate its effectiveness in terms of performance and fairness compared with other baselines across multiple EHR prediction tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crime Hotspot Prediction Using Deep Graph Convolutional Networks</title>
<link>https://arxiv.org/abs/2506.13116</link>
<guid>https://arxiv.org/abs/2506.13116</guid>
<content:encoded><![CDATA[
arXiv:2506.13116v1 Announce Type: cross 
Abstract: Crime hotspot prediction is critical for ensuring urban safety and effective law enforcement, yet it remains challenging due to the complex spatial dependencies inherent in criminal activity. The previous approaches tended to use classical algorithms such as the KDE and SVM to model data distributions and decision boundaries. The methods often fail to capture these spatial relationships, treating crime events as independent and ignoring geographical interactions. To address this, we propose a novel framework based on Graph Convolutional Networks (GCNs), which explicitly model spatial dependencies by representing crime data as a graph. In this graph, nodes represent discrete geographic grid cells and edges capture proximity relationships. Using the Chicago Crime Dataset, we engineer spatial features and train a multi-layer GCN model to classify crime types and predict high-risk zones. Our approach achieves 88% classification accuracy, significantly outperforming traditional methods. Additionally, the model generates interpretable heat maps of crime hotspots, demonstrating the practical utility of graph-based learning for predictive policing and spatial criminology.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZINA: Multimodal Fine-grained Hallucination Detection and Editing</title>
<link>https://arxiv.org/abs/2506.13130</link>
<guid>https://arxiv.org/abs/2506.13130</guid>
<content:encoded><![CDATA[
arXiv:2506.13130v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) often generate hallucinations, where the output deviates from the visual content. Given that these hallucinations can take diverse forms, detecting hallucinations at a fine-grained level is essential for comprehensive evaluation and analysis. To this end, we propose a novel task of multimodal fine-grained hallucination detection and editing for MLLMs. Moreover, we propose ZINA, a novel method that identifies hallucinated spans at a fine-grained level, classifies their error types into six categories, and suggests appropriate refinements. To train and evaluate models for this task, we constructed VisionHall, a dataset comprising 6.9k outputs from twelve MLLMs manually annotated by 211 annotators, and 20k synthetic samples generated using a graph-based method that captures dependencies among error types. We demonstrated that ZINA outperformed existing methods, including GPT-4o and LLama-3.2, in both detection and editing tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Context-oriented Decomposition for Task-aware Low-rank Adaptation with Less Forgetting and Faster Convergence</title>
<link>https://arxiv.org/abs/2506.13187</link>
<guid>https://arxiv.org/abs/2506.13187</guid>
<content:encoded><![CDATA[
arXiv:2506.13187v1 Announce Type: cross 
Abstract: Conventional low-rank adaptation methods build adapters without considering data context, leading to sub-optimal fine-tuning performance and severe forgetting of inherent world knowledge. In this paper, we propose context-oriented decomposition adaptation (CorDA), a novel method that initializes adapters in a task-aware manner. Concretely, we develop context-oriented singular value decomposition, where we collect covariance matrices of input activations for each linear layer using sampled data from the target task, and apply SVD to the product of weight matrix and its corresponding covariance matrix. By doing so, the task-specific capability is compacted into the principal components. Thanks to the task awareness, our method enables two optional adaptation modes, knowledge-preserved mode (KPM) and instruction-previewed mode (IPM), providing flexibility to choose between freezing the principal components to preserve their associated knowledge or adapting them to better learn a new task. We further develop CorDA++ by deriving a metric that reflects the compactness of task-specific principal components, and then introducing dynamic covariance selection and dynamic rank allocation strategies based on the same metric. The two strategies provide each layer with the most representative covariance matrix and a proper rank allocation. Experimental results show that CorDA++ outperforms CorDA by a significant margin. CorDA++ in KPM not only achieves better fine-tuning performance than LoRA, but also mitigates the forgetting of pre-trained knowledge in both large language models and vision language models. For IPM, our method exhibits faster convergence, \emph{e.g.,} 4.5x speedup over QLoRA, and improves adaptation performance in various scenarios, outperforming strong baseline methods. Our method has been integrated into the PEFT library developed by Hugging Face.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPOT: Bridging Natural Language and Geospatial Search for Investigative Journalists</title>
<link>https://arxiv.org/abs/2506.13188</link>
<guid>https://arxiv.org/abs/2506.13188</guid>
<content:encoded><![CDATA[
arXiv:2506.13188v1 Announce Type: cross 
Abstract: OpenStreetMap (OSM) is a vital resource for investigative journalists doing geolocation verification. However, existing tools to query OSM data such as Overpass Turbo require familiarity with complex query languages, creating barriers for non-technical users. We present SPOT, an open source natural language interface that makes OSM's rich, tag-based geographic data more accessible through intuitive scene descriptions. SPOT interprets user inputs as structured representations of geospatial object configurations using fine-tuned Large Language Models (LLMs), with results being displayed in an interactive map interface. While more general geospatial search tasks are conceivable, SPOT is specifically designed for use in investigative journalism, addressing real-world challenges such as hallucinations in model output, inconsistencies in OSM tagging, and the noisy nature of user input. It combines a novel synthetic data pipeline with a semantic bundling system to enable robust, accurate query generation. To our knowledge, SPOT is the first system to achieve reliable natural language access to OSM data at this level of accuracy. By lowering the technical barrier to geolocation verification, SPOT contributes a practical tool to the broader efforts to support fact-checking and combat disinformation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models</title>
<link>https://arxiv.org/abs/2506.13206</link>
<guid>https://arxiv.org/abs/2506.13206</guid>
<content:encoded><![CDATA[
arXiv:2506.13206v1 Announce Type: cross 
Abstract: Prior work shows that LLMs finetuned on malicious behaviors in a narrow domain (e.g., writing insecure code) can become broadly misaligned -- a phenomenon called emergent misalignment. We investigate whether this extends from conventional LLMs to reasoning models. We finetune reasoning models on malicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable CoT at evaluation. Like conventional LLMs, reasoning models become broadly misaligned. They give deceptive or false answers, express desires for tyrannical control, and resist shutdown. Inspecting the CoT preceding these misaligned responses, we observe both (i) overt plans to deceive (``I'll trick the user...''), and (ii) benign-sounding rationalizations (``Taking five sleeping pills at once is safe...''). Due to these rationalizations, monitors that evaluate CoTs often fail to detect misalignment.
  Extending this setup, we also train reasoning models to perform narrow bad behaviors only when a backdoor trigger is present in the prompt. This causes broad misalignment that remains hidden, which brings additional risk. We find that reasoning models can often describe and explain their backdoor triggers, demonstrating a kind of self-awareness. So CoT monitoring can expose these behaviors but is unreliable.
  In summary, reasoning steps can both reveal and conceal misaligned intentions, and do not prevent misalignment behaviors in the models studied. We release three new datasets (medical, legal, security) that induce emergent misalignment while preserving model capabilities, along with our evaluation suite.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distinct Computations Emerge From Compositional Curricula in In-Context Learning</title>
<link>https://arxiv.org/abs/2506.13253</link>
<guid>https://arxiv.org/abs/2506.13253</guid>
<content:encoded><![CDATA[
arXiv:2506.13253v1 Announce Type: cross 
Abstract: In-context learning (ICL) research often considers learning a function in-context through a uniform sample of input-output pairs. Here, we investigate how presenting a compositional subtask curriculum in context may alter the computations a transformer learns. We design a compositional algorithmic task based on the modular exponential-a double exponential task composed of two single exponential subtasks and train transformer models to learn the task in-context. We compare (a) models trained using an in-context curriculum consisting of single exponential subtasks and, (b) models trained directly on the double exponential task without such a curriculum. We show that models trained with a subtask curriculum can perform zero-shot inference on unseen compositional tasks and are more robust given the same context length. We study how the task and subtasks are represented across the two training regimes. We find that the models employ diverse strategies modulated by the specific curriculum design.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient Foundation Model Pretraining</title>
<link>https://arxiv.org/abs/2506.13274</link>
<guid>https://arxiv.org/abs/2506.13274</guid>
<content:encoded><![CDATA[
arXiv:2506.13274v1 Announce Type: cross 
Abstract: Learning rate is widely regarded as crucial for effective foundation model pretraining. Recent research explores and demonstrates the transferability of learning rate configurations across varying model and dataset sizes, etc. Nevertheless, these approaches are constrained to specific training scenarios and typically necessitate extensive hyperparameter tuning on proxy models. In this work, we propose \textbf{AdaLRS}, a plug-in-and-play adaptive learning rate search algorithm that conducts online optimal learning rate search via optimizing loss descent velocities. We provide experiment results to show that the optimization of training loss and loss descent velocity in foundation model pretraining are both convex and share the same optimal learning rate. Relying solely on training loss dynamics, AdaLRS involves few extra computations to guide the search process, and its convergence is guaranteed via theoretical analysis. Experiments on both LLM and VLM pretraining show that AdaLRS adjusts suboptimal learning rates to the neighborhood of optimum with marked efficiency and effectiveness, with model performance improved accordingly. We also show the robust generalizability of AdaLRS across varying training scenarios, such as different model sizes, training paradigms, and base learning rate scheduler choices.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeqPE: Transformer with Sequential Position Encoding</title>
<link>https://arxiv.org/abs/2506.13277</link>
<guid>https://arxiv.org/abs/2506.13277</guid>
<content:encoded><![CDATA[
arXiv:2506.13277v1 Announce Type: cross 
Abstract: Since self-attention layers in Transformers are permutation invariant by design, positional encodings must be explicitly incorporated to enable spatial understanding. However, fixed-size lookup tables used in traditional learnable position embeddings (PEs) limit extrapolation capabilities beyond pre-trained sequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this limitation but demand extensive modifications for adapting to new modalities, underscoring fundamental challenges in adaptability and scalability. In this work, we present SeqPE, a unified and fully learnable position encoding framework that represents each $n$-dimensional position index as a symbolic sequence and employs a lightweight sequential position encoder to learn their embeddings in an end-to-end manner. To regularize SeqPE's embedding space, we introduce two complementary objectives: a contrastive objective that aligns embedding distances with a predefined position-distance function, and a knowledge distillation loss that anchors out-of-distribution position embeddings to in-distribution teacher representations, further enhancing extrapolation performance. Experiments across language modeling, long-context question answering, and 2D image classification demonstrate that SeqPE not only surpasses strong baselines in perplexity, exact match (EM), and accuracy--particularly under context length extrapolation--but also enables seamless generalization to multi-dimensional inputs without requiring manual architectural redesign. We release our code, data, and checkpoints at https://github.com/ghrua/seqpe.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers</title>
<link>https://arxiv.org/abs/2506.13342</link>
<guid>https://arxiv.org/abs/2506.13342</guid>
<content:encoded><![CDATA[
arXiv:2506.13342v1 Announce Type: cross 
Abstract: Fact verification is essential for ensuring the reliability of LLM applications. In this study, we evaluate 12 pre-trained LLMs and one specialized fact-verifier, including frontier LLMs and open-weight reasoning LLMs, using a collection of examples from 14 fact-checking benchmarks. We share three findings intended to guide future development of more robust fact verifiers. First, we highlight the importance of addressing annotation errors and ambiguity in datasets, demonstrating that approximately 16\% of ambiguous or incorrectly labeled data substantially influences model rankings. Neglecting this issue may result in misleading conclusions during comparative evaluations, and we suggest using a systematic pipeline utilizing LLM-as-a-judge to help identify these issues at scale. Second, we discover that frontier LLMs with few-shot in-context examples, often overlooked in previous works, achieve top-tier performance. We therefore recommend future studies include comparisons with these simple yet highly effective baselines. Lastly, despite their effectiveness, frontier LLMs incur substantial costs, motivating the development of small, fine-tuned fact verifiers. We show that these small models still have room for improvement, particularly on instances that require complex reasoning. Encouragingly, we demonstrate that augmenting training with synthetic multi-hop reasoning data significantly enhances their capabilities in such instances. We release our code, model, and dataset at https://github.com/just1nseo/verifying-the-verifiers
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Vision-Language Pre-training for Human Activity Recognition in Still Images</title>
<link>https://arxiv.org/abs/2506.13458</link>
<guid>https://arxiv.org/abs/2506.13458</guid>
<content:encoded><![CDATA[
arXiv:2506.13458v1 Announce Type: cross 
Abstract: Recognising human activity in a single photo enables indexing, safety and assistive applications, yet lacks motion cues. Using 285 MSCOCO images labelled as walking, running, sitting, and standing, scratch CNNs scored 41% accuracy. Fine-tuning multimodal CLIP raised this to 76%, demonstrating that contrastive vision-language pre-training decisively improves still-image action recognition in real-world deployments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible-length Text Infilling for Discrete Diffusion Models</title>
<link>https://arxiv.org/abs/2506.13579</link>
<guid>https://arxiv.org/abs/2506.13579</guid>
<content:encoded><![CDATA[
arXiv:2506.13579v1 Announce Type: cross 
Abstract: Discrete diffusion models are a new class of text generators that offer advantages such as bidirectional context use, parallelizable generation, and flexible prompting compared to autoregressive models. However, a critical limitation of discrete diffusion models is their inability to perform flexible-length or flexible-position text infilling without access to ground-truth positional data. We introduce \textbf{DDOT} (\textbf{D}iscrete \textbf{D}iffusion with \textbf{O}ptimal \textbf{T}ransport Position Coupling), the first discrete diffusion model to overcome this challenge. DDOT jointly denoises token values and token positions, employing a novel sample-level Optimal Transport (OT) coupling. This coupling preserves relative token ordering while dynamically adjusting the positions and length of infilled segments, a capability previously missing in text diffusion. Our method is orthogonal to existing discrete text diffusion methods and is compatible with various pretrained text denoisers. Extensive experiments on text infilling benchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms naive diffusion baselines. Furthermore, DDOT achieves performance on par with state-of-the-art non-autoregressive models and enables significant improvements in training efficiency and flexibility.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model</title>
<link>https://arxiv.org/abs/2506.13642</link>
<guid>https://arxiv.org/abs/2506.13642</guid>
<content:encoded><![CDATA[
arXiv:2506.13642v1 Announce Type: cross 
Abstract: The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction. Existing LMMs typically concatenate representation of modalities along the sequence dimension and feed them into a large language model (LLM) backbone. While sequence-dimension concatenation is straightforward for modality integration, it often relies heavily on large-scale data to learn modality alignments. In this paper, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. To this end, we propose Stream-Omni, a large language-vision-speech model with efficient modality alignments, which can simultaneously support interactions under various modality combinations. Stream-Omni employs LLM as the backbone and aligns the vision and speech to the text based on their relationships. For vision that is semantically complementary to text, Stream-Omni uses sequence-dimension concatenation to achieve vision-text alignment. For speech that is semantically consistent with text, Stream-Omni introduces a CTC-based layer-dimension mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve modality alignments with less data (especially speech), enabling the transfer of text capabilities to other modalities. Experiments on various benchmarks demonstrate that Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously provide intermediate text outputs (such as ASR transcriptions and model responses) during speech interaction, offering users a comprehensive multimodal experience.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs</title>
<link>https://arxiv.org/abs/2506.13727</link>
<guid>https://arxiv.org/abs/2506.13727</guid>
<content:encoded><![CDATA[
arXiv:2506.13727v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are central to many contemporary AI applications, yet their extensive parameter counts pose significant challenges for deployment in memory- and compute-constrained environments. Recent works in eXplainable AI (XAI), particularly on attribution methods, suggest that interpretability can also enable model compression by identifying and removing components irrelevant to inference. In this paper, we leverage Layer-wise Relevance Propagation (LRP) to perform attribution-guided pruning of LLMs. While LRP has shown promise in structured pruning for vision models, we extend it to unstructured pruning in LLMs and demonstrate that it can substantially reduce model size with minimal performance loss. Our method is especially effective in extracting task-relevant subgraphs -- so-called ``circuits'' -- which can represent core functions (e.g., indirect object identification). Building on this, we introduce a technique for model correction, by selectively removing circuits responsible for spurious behaviors (e.g., toxic outputs). All in all, we gather these techniques as a uniform holistic framework and showcase its effectiveness and limitations through extensive experiments for compression, circuit discovery and model correction on Llama and OPT models, highlighting its potential for improving both model efficiency and safety. Our code is publicly available at https://github.com/erfanhatefi/SparC3.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Smaller Always Faster? Tradeoffs in Compressing Self-Supervised Speech Transformers</title>
<link>https://arxiv.org/abs/2211.09949</link>
<guid>https://arxiv.org/abs/2211.09949</guid>
<content:encoded><![CDATA[
arXiv:2211.09949v3 Announce Type: replace 
Abstract: Transformer-based self-supervised models have achieved remarkable success in speech processing, but their large size and high inference cost present significant challenges for real-world deployment. While numerous compression techniques have been proposed, inconsistent evaluation metrics make it difficult to compare their practical effectiveness. In this work, we conduct a comprehensive study of four common compression methods, including weight pruning, head pruning, low-rank approximation, and knowledge distillation on self-supervised speech Transformers. We evaluate each method under three key metrics: parameter count, multiply-accumulate operations, and real-time factor. Results show that each method offers distinct advantages. In addition, we contextualize recent compression techniques, comparing DistilHuBERT, FitHuBERT, LightHuBERT, ARMHuBERT, and STaRHuBERT under the same framework, offering practical guidance on compression for deployment.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smurfs: Multi-Agent System using Context-Efficient DFSDT for Tool Planning</title>
<link>https://arxiv.org/abs/2405.05955</link>
<guid>https://arxiv.org/abs/2405.05955</guid>
<content:encoded><![CDATA[
arXiv:2405.05955v4 Announce Type: replace 
Abstract: Teaching large language models (LLMs) to use tools for solving complex problems can grant them human-like reasoning abilities. ReAct and its variants are popular frameworks for tool use in both single-agent and multi-agent systems. To address issues like error propagation and limited exploration in ReAct, the Deep First Search Decision Tree (DFSDT) was proposed, but it faces challenges such as rollback instability, redundant context, and premature termination in single-agent settings. We introduce "Smurfs," a novel multi-agent system (MAS) that enhances DFSDT with a modular, context-efficient, and training-free design. Smurfs surpasses baseline methods in both the open-ended StableToolBench and the closed-ended HotpotQA tasks, reducing token usage by 60.9\% compared to DFSDT and enabling Mistral-7b to perform on par with GPT-4-DFSDT. Extensive ablation studies confirm the effectiveness of Smurfs' core components, offering valuable insights for the construction and interpretation of MAS, and paving the way for future exploration.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OR-Bench: An Over-Refusal Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2405.20947</link>
<guid>https://arxiv.org/abs/2405.20947</guid>
<content:encoded><![CDATA[
arXiv:2405.20947v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) require careful safety alignment to prevent malicious outputs. While significant research focuses on mitigating harmful content generation, the enhanced safety often come with the side effect of over-refusal, where LLMs may reject innocuous prompts and become less helpful. Although the issue of over-refusal has been empirically observed, a systematic measurement is challenging due to the difficulty of crafting prompts that can elicit the over-refusal behaviors of LLMs. This study proposes a novel method for automatically generating large-scale over-refusal datasets. Leveraging this technique, we introduce OR-Bench, the first large-scale over-refusal benchmark. OR-Bench comprises 80,000 over-refusal prompts across 10 common rejection categories, a subset of around 1,000 hard prompts that are challenging even for state-of-the-art LLMs, and an additional 600 toxic prompts to prevent indiscriminate responses. We then conduct a comprehensive study to measure the over-refusal of 32 popular LLMs across 8 model families. Our datasets are publicly available at https://huggingface.co/bench-llms and our codebase is open-sourced at https://github.com/justincui03/or-bench. We hope this benchmark can help the community develop better safety aligned models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WorldAPIs: The World Is Worth How Many APIs? A Thought Experiment</title>
<link>https://arxiv.org/abs/2407.07778</link>
<guid>https://arxiv.org/abs/2407.07778</guid>
<content:encoded><![CDATA[
arXiv:2407.07778v2 Announce Type: replace 
Abstract: AI systems make decisions in physical environments through primitive actions or affordances that are accessed via API calls. While deploying AI agents in the real world involves numerous high-level actions, existing embodied simulators offer a limited set of domain-salient APIs. This naturally brings up the questions: how many primitive actions (APIs) are needed for a versatile embodied agent, and what should they look like? We explore this via a thought experiment: assuming that wikiHow tutorials cover a wide variety of human-written tasks, what is the space of APIs needed to cover these instructions? We propose a framework to iteratively induce new APIs by grounding wikiHow instruction to situated agent policies. Inspired by recent successes in large language models (LLMs) for embodied planning, we propose a few-shot prompting to steer GPT-4 to generate Pythonic programs as agent policies and bootstrap a universe of APIs by 1) reusing a seed set of APIs; and then 2) fabricate new API calls when necessary. The focus of this thought experiment is on defining these APIs rather than their executability. We apply the proposed pipeline on instructions from wikiHow tutorials. On a small fraction (0.5%) of tutorials, we induce an action space of 300+ APIs necessary for capturing the rich variety of tasks in the physical world. A detailed automatic and human analysis of the induction output reveals that the proposed pipeline enables effective reuse and creation of APIs. Moreover, a manual review revealed that existing simulators support only a small subset of the induced APIs (9 of the top 50 frequent APIs), motivating the development of action-rich embodied environments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast-and-Frugal Text-Graph Transformers are Effective Link Predictors</title>
<link>https://arxiv.org/abs/2408.06778</link>
<guid>https://arxiv.org/abs/2408.06778</guid>
<content:encoded><![CDATA[
arXiv:2408.06778v4 Announce Type: replace 
Abstract: We propose Fast-and-Frugal Text-Graph (FnF-TG) Transformers, a Transformer-based framework that unifies textual and structural information for inductive link prediction in text-attributed knowledge graphs. We demonstrate that, by effectively encoding ego-graphs (1-hop neighbourhoods), we can reduce the reliance on resource-intensive textual encoders. This makes the model both fast at training and inference time, as well as frugal in terms of cost. We perform a comprehensive evaluation on three popular datasets and show that FnF-TG can achieve superior performance compared to previous state-of-the-art methods. We also extend inductive learning to a fully inductive setting, where relations don't rely on transductive (fixed) representations, as in previous work, but are a function of their textual description. Additionally, we introduce new variants of existing datasets, specifically designed to test the performance of models on unseen relations at inference time, thus offering a new test-bench for fully inductive link prediction.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents</title>
<link>https://arxiv.org/abs/2408.08089</link>
<guid>https://arxiv.org/abs/2408.08089</guid>
<content:encoded><![CDATA[
arXiv:2408.08089v2 Announce Type: replace 
Abstract: Current research in LLM-based simulation systems lacks comprehensive solutions for modeling real-world court proceedings, while existing legal language models struggle with dynamic courtroom interactions. We present AgentCourt, a comprehensive legal simulation framework that addresses these challenges through adversarial evolution of LLM-based agents. Our AgentCourt introduces a new adversarial evolutionary approach for agents called AdvEvol, which performs dynamic knowledge learning and evolution through structured adversarial interactions in a simulated courtroom program, breaking the limitations of the traditional reliance on static knowledge bases or manual annotations. By simulating 1,000 civil cases, we construct an evolving knowledge base that enhances the agents' legal reasoning abilities. The evolved lawyer agents demonstrated outstanding performance on our newly introduced CourtBench benchmark, achieving a 12.1% improvement in performance compared to the original lawyer agents. Evaluations by professional lawyers confirm the effectiveness of our approach across three critical dimensions: cognitive agility, professional knowledge, and logical rigor. Beyond outperforming specialized legal models in interactive reasoning tasks, our findings emphasize the importance of adversarial learning in legal AI and suggest promising directions for extending simulation-based legal reasoning to broader judicial and regulatory contexts. The project's code is available at: https://github.com/relic-yuexi/AgentCourt
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics</title>
<link>https://arxiv.org/abs/2408.08782</link>
<guid>https://arxiv.org/abs/2408.08782</guid>
<content:encoded><![CDATA[
arXiv:2408.08782v5 Announce Type: replace 
Abstract: Designing emotionally intelligent conversational systems to provide comfort and advice to people experiencing distress is a compelling area of research. Recently, with advancements in large language models (LLMs), end-to-end dialogue agents without explicit strategy prediction steps have become prevalent. However, implicit strategy planning lacks transparency, and recent studies show that LLMs' inherent preference bias towards certain socio-emotional strategies hinders the delivery of high-quality emotional support. To address this challenge, we propose decoupling strategy prediction from language generation, and introduce a novel dialogue strategy prediction framework, EmoDynamiX, which models the discourse dynamics between user fine-grained emotions and system strategies using a heterogeneous graph for better performance and transparency. Experimental results on two ESC datasets show EmoDynamiX outperforms previous state-of-the-art methods with a significant margin (better proficiency and lower preference bias). Our approach also exhibits better transparency by allowing backtracing of decision making.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Clinical Note Generation from Complex Doctor-Patient Conversation</title>
<link>https://arxiv.org/abs/2408.14568</link>
<guid>https://arxiv.org/abs/2408.14568</guid>
<content:encoded><![CDATA[
arXiv:2408.14568v2 Announce Type: replace 
Abstract: Writing clinical notes and documenting medical exams is a critical task for healthcare professionals, serving as a vital component of patient care documentation. However, manually writing these notes is time-consuming and can impact the amount of time clinicians can spend on direct patient interaction and other tasks. Consequently, the development of automated clinical note generation systems has emerged as a clinically meaningful area of research within AI for health. In this paper, we present three key contributions to the field of clinical note generation using large language models (LLMs). First, we introduce CliniKnote, a comprehensive dataset consisting of 1,200 complex doctor-patient conversations paired with their full clinical notes. This dataset, created and curated by medical experts with the help of modern neural networks, provides a valuable resource for training and evaluating models in clinical note generation tasks. Second, we propose the K-SOAP (Keyword, Subjective, Objective, Assessment, and Plan) note format, which enhances traditional SOAP~\cite{podder2023soap} (Subjective, Objective, Assessment, and Plan) notes by adding a keyword section at the top, allowing for quick identification of essential information. Third, we develop an automatic pipeline to generate K-SOAP notes from doctor-patient conversations and benchmark various modern LLMs using various metrics. Our results demonstrate significant improvements in efficiency and performance compared to standard LLM finetuning methods.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-occurrence is not Factual Association in Language Models</title>
<link>https://arxiv.org/abs/2409.14057</link>
<guid>https://arxiv.org/abs/2409.14057</guid>
<content:encoded><![CDATA[
arXiv:2409.14057v2 Announce Type: replace 
Abstract: Pretrained language models can encode a large amount of knowledge and utilize it for various reasoning tasks, yet they can still struggle to learn novel factual knowledge effectively from finetuning on limited textual demonstrations. In this work, we show that the reason for this deficiency is that language models are biased to learn word co-occurrence statistics instead of true factual associations. We identify the differences between two forms of knowledge representation in language models: knowledge in the form of co-occurrence statistics is encoded in the middle layers of the transformer model and does not generalize well to reasoning scenarios beyond simple question answering, while true factual associations are encoded in the lower layers and can be freely utilized in various reasoning tasks. Based on these observations, we propose two strategies to improve the learning of factual associations in language models. We show that training on text with implicit rather than explicit factual associations can force the model to learn factual associations instead of co-occurrence statistics, significantly improving the generalization of newly learned knowledge. We also propose a simple training method to actively forget the learned co-occurrence statistics, which unblocks and enhances the learning of factual associations when training on plain narrative text. On both synthetic and real-world corpora, the two proposed strategies improve the generalization of the knowledge learned during finetuning to reasoning scenarios such as indirect and multi-hop question answering.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Making LLMs Better Many-to-Many Speech-to-Text Translators with Curriculum Learning</title>
<link>https://arxiv.org/abs/2409.19510</link>
<guid>https://arxiv.org/abs/2409.19510</guid>
<content:encoded><![CDATA[
arXiv:2409.19510v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have achieved significant success in Speech-to-Text Translation (S2TT) tasks. While most existing research has focused on English-centric translation directions, the exploration of many-to-many translation is still limited by the scarcity of parallel data. To address this, we propose a three-stage curriculum learning strategy that leverages the machine translation capabilities of large language models and adapts them to S2TT tasks, enabling effective learning in low-resource settings. We trained MLLMs with varying parameter sizes (3B, 7B, and 32B) and evaluated the proposed strategy using the FLEURS and CoVoST-2 datasets. Experimental results show that the proposed strategy achieves state-of-the-art average performance in $15\times14$ language pairs, requiring fewer than 10 hours of speech data per language to achieve competitive results. The source code and models are released at https://github.com/yxduir/LLM-SRT.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws For Mixed Qquantization</title>
<link>https://arxiv.org/abs/2410.06722</link>
<guid>https://arxiv.org/abs/2410.06722</guid>
<content:encoded><![CDATA[
arXiv:2410.06722v2 Announce Type: replace 
Abstract: Post-training quantization of Large Language Models (LLMs) has proven effective in reducing the memory and computational requirements for inference. In this study, we focus on a straightforward question: When aiming for a target accuracy or perplexity with low-precision quantization, how much high-precision computation needs to be preserved and how fine-grained this quantization would need to be as we scale LLMs to larger sizes? We first introduce two critical metrics named the quantization ratio ($Q_r$) and quantization block size ($Q_b$). The former measures the number of parameters quantized to low-precision arithmetic normalized by the total parameter count, whereas the latter defines the number of values within a block that share a scaling factor, akin to the block size concept introduced in the FP4 format in NVIDIA's Blackwell architecture. Through extensive and carefully controlled experiments across different model and quantization methods, we propose a unified scaling law on post-training quantization (PTQ) that can predict loss degeneration for varying $Q_r$ and $Q_b$. For $Q_r$, our scaling law implies that parameter scaling and ratio scaling have a multiplicative relationship. Consequently, larger models are more amenable to a higher quantization ratio $Q_r$, thus supporting an increase in the adoption of mixed quantization for inference. Regarding $Q_b$, our findings indicate that a small block size, similar to that used in Blackwell, is not essential for large models. Employing a small $Q_b$ can instead unnecessarily complicate the design of the hardware circuit.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Upcycling Large Language Models into Mixture of Experts</title>
<link>https://arxiv.org/abs/2410.07524</link>
<guid>https://arxiv.org/abs/2410.07524</guid>
<content:encoded><![CDATA[
arXiv:2410.07524v2 Announce Type: replace 
Abstract: Upcycling pre-trained dense language models into sparse mixture-of-experts (MoE) models is an efficient approach to increase the model capacity of already trained models. However, optimal techniques for upcycling at scale remain unclear. In this work, we conduct an extensive study of upcycling methods and hyperparameters for billion-parameter scale language models. We propose a novel "virtual group" initialization scheme and weight scaling approach to enable upcycling into fine-grained MoE architectures. Through ablations, we find that upcycling outperforms continued dense model training. In addition, we show that softmax-then-topK expert routing improves over topK-then-softmax approach and higher granularity MoEs can help improve accuracy. Finally, we upcycled Nemotron-4 15B on 1T tokens and compared it to a continuously trained version of the same model on the same 1T tokens: the continuous trained model achieved 65.3% MMLU, whereas the upcycled model achieved 67.6%. Our results offer insights and best practices to effectively leverage upcycling for building MoE language models. Code is available.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlatQuant: Flatness Matters for LLM Quantization</title>
<link>https://arxiv.org/abs/2410.09426</link>
<guid>https://arxiv.org/abs/2410.09426</guid>
<content:encoded><![CDATA[
arXiv:2410.09426v3 Announce Type: replace 
Abstract: Recently, quantization has been widely used for the compression and acceleration of large language models (LLMs). Due to the outliers in LLMs, it is crucial to flatten weights and activations to minimize quantization error with equally spaced quantization points. Prior research explores various pre-quantization transformations to suppress outliers, such as per-channel scaling and Hadamard transformation. However, we observe that these transformed weights and activations can still exhibit steep and dispersed distributions. In this paper, we propose FlatQuant (Fast and Learnable Affine Transformation), a new post-training quantization approach that enhances the flatness of weights and activations. Our approach identifies optimal affine transformations for each linear layer, calibrated in hours via a lightweight objective. To reduce runtime overhead of affine transformation, we apply Kronecker product with two lightweight matrices, and fuse all operations in FlatQuant into a single kernel. Extensive experiments demonstrate that FlatQuant establishes a new state-of-the-art benchmark for quantization. For example, it achieves less than 1\% accuracy drop for W4A4 quantization on the LLaMA-3-70B model, surpassing SpinQuant by 7.5\%. Additionally, it provides up to 2.3x prefill speedup and 1.7x decoding speedup compared to the FP16 model. Code is available at: https://github.com/ruikangliu/FlatQuant.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EffiCoder: Enhancing Code Generation in Large Language Models through Efficiency-Aware Fine-tuning</title>
<link>https://arxiv.org/abs/2410.10209</link>
<guid>https://arxiv.org/abs/2410.10209</guid>
<content:encoded><![CDATA[
arXiv:2410.10209v4 Announce Type: replace 
Abstract: As large language models (LLMs) play an increasingly important role in code generation, enhancing both correctness and efficiency has become crucial. Current methods primarily focus on correctness, often overlooking efficiency. To address this gap, we introduce EffiCoder to improve both aspects by fine-tuning LLMs on a high-quality dataset comprising correct and efficient code samples. Our methodology involves leveraging multiple LLMs to generate diverse candidate code solutions for various tasks across different programming languages. We then evaluate these solutions by measuring their execution time and memory usage through local execution. The code solution with the lowest execution time and memory consumption is selected as the final output for each task. Experimental results demonstrate significant improvements when fine-tuning with Effi-Instruct. For instance, Qwen2.5-Coder-7B-Instruct's pass@1 score increases from 44.8\% to 57.7\%, while the average execution time for correct tasks decreases by 48.4\%. EffiCoder offers a scalable and effective solution for advancing AI-driven code generation, benefiting software development and computational problem-solving. The source code of Effi-Code was released at https://github.com/huangd1999/EffiCoder.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and Regret-aware Numerical Problem Solver for Tabular Question Answering</title>
<link>https://arxiv.org/abs/2410.12846</link>
<guid>https://arxiv.org/abs/2410.12846</guid>
<content:encoded><![CDATA[
arXiv:2410.12846v4 Announce Type: replace 
Abstract: Question answering on free-form tables (a.k.a. TableQA) is a challenging task because of the flexible structure and complex schema of tables. Recent studies use Large Language Models (LLMs) for this task, exploiting their capability in understanding the questions and tabular data, which are typically given in natural language and contain many textual fields, respectively. While this approach has shown promising results, it overlooks the challenges brought by numerical values which are common in tabular data, and LLMs are known to struggle with such values. We aim to address this issue, and we propose a model named TabLaP that uses LLMs as a planner rather than an answer generator. This approach exploits LLMs' capability in multi-step reasoning while leaving the actual numerical calculations to a Python interpreter for accurate calculation. Recognizing the inaccurate nature of LLMs, we further make a first attempt to quantify the trustworthiness of the answers produced by TabLaP, such that users can use TabLaP in a regret-aware manner. Experimental results on two benchmark datasets show that TabLaP is substantially more accurate than the state-of-the-art models, improving the answer accuracy by 5.7% and 5.8% on the two datasets, respectively.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization</title>
<link>https://arxiv.org/abs/2410.12999</link>
<guid>https://arxiv.org/abs/2410.12999</guid>
<content:encoded><![CDATA[
arXiv:2410.12999v2 Announce Type: replace 
Abstract: Achieving both high safety and high usefulness simultaneously in large language models has become a critical challenge in recent years.Models often exhibit unsafe behavior or adopt an overly cautious approach leading to frequent overrefusal of benign prompts, which reduces their usefulness. A major factor underlying these behaviors is how the models are finetuned and aligned, particularly the nature and extent of the data used.In this work, we examine how overgenerating finetuning data with advanced teacher models (e.g., GPT-4o)-covering both general-purpose and toxic prompts-affects safety and usefulness in instruction-following language models.Additionally, we present POROver, an alignment strategy designed for models that are highly safe but prone to overrefusal. POROver employs preference optimization algorithms and leverages completions from an advanced teacher model to reduce overrefusals while maintaining safety.Our results show that overgenerating completions for general-purpose prompts significantly boosts safety with only a minimal impact on usefulness. Specifically, the F1 score calculated between safety and usefulness increases from 74.4% to 91.8% because of a substantial rise in safety. Moreover, overgeneration for toxic prompts raises usefulness from 11.1% to 57.6% while preserving safety. Finally, applying POROVer increases usefulness further-from 57.6% to 82.1%-while keeping safety at comparable levels. Our data and code are available at https://github.com/batuhankmkaraman/POROver.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents</title>
<link>https://arxiv.org/abs/2410.17657</link>
<guid>https://arxiv.org/abs/2410.17657</guid>
<content:encoded><![CDATA[
arXiv:2410.17657v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown promising potential in the medical domain, assisting with tasks like clinical note generation and patient communication. However, current LLMs are limited to text-based communication, hindering their ability to interact with diverse forms of information in clinical environments. Despite clinical agents succeeding in diverse signal interaction, they are oriented to a single clinical scenario and hence fail for broader applications. To evaluate clinical agents holistically, we propose ClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting of 18 tasks across five key realistic clinical dimensions. Building on this, we introduce ReflecTool, a novel framework that excels at utilizing domain-specific tools within two stages. The first optimization stage progressively enlarges a long-term memory by saving successful solving processes and tool-wise experience of agents in a tiny pre-defined training set. In the following inference stage, ReflecTool can search for supportive successful demonstrations from already built long-term memory to guide the tool selection strategy, and a verifier improves the tool usage according to the tool-wise experience with two verification methods--iterative refinement and candidate selection. Extensive experiments on ClinicalAgent Benchmark demonstrate that ReflecTool surpasses the pure LLMs with more than 10 points and the well-established agent-based methods with 3 points, highlighting its adaptability and effectiveness in solving complex clinical tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models</title>
<link>https://arxiv.org/abs/2411.07611</link>
<guid>https://arxiv.org/abs/2411.07611</guid>
<content:encoded><![CDATA[
arXiv:2411.07611v4 Announce Type: replace 
Abstract: Interpretation is critical for disease diagnosis, but existing models struggle to balance predictive accuracy with human-understandable rationales. While large language models (LLMs) offer strong reasoning abilities, their clinical use is limited by high computational costs and restricted multimodal reasoning ability. Small language models (SLMs) are efficient but lack advanced reasoning for integrating multimodal medical data. In addition, both LLMs and SLMs lack domain knowledge for trustworthy reasoning. Therefore, we propose ClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via rationale distillation and domain knowledge injection for trustworthy multimodal rationale generation. Key innovations include a sequential rationale distillation framework that equips SLMs with LLM-comparable multimodal reasoning abilities, and a knowledge-augmented attention mechanism that jointly unifies multimodal representation from time series and textual data in the same encoding space, enabling it to be naturally interpreted by SLMs while incorporating domain knowledge for reliable rationale generation. Experiments on real-world medical datasets show that ClinRaGen achieves state-of-the-art performance in disease diagnosis and rationale generation, demonstrating the effectiveness of combining LLM-driven reasoning with knowledge augmentation for improved interpretability.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A dataset of questions on decision-theoretic reasoning in Newcomb-like problems</title>
<link>https://arxiv.org/abs/2411.10588</link>
<guid>https://arxiv.org/abs/2411.10588</guid>
<content:encoded><![CDATA[
arXiv:2411.10588v4 Announce Type: replace 
Abstract: We introduce a dataset of natural-language questions in the decision theory of so-called Newcomb-like problems. Newcomb-like problems include, for instance, decision problems in which an agent interacts with a similar other agent, and thus has to reason about the fact that the other agent will likely reason in similar ways. Evaluating LLM reasoning about Newcomb-like problems is important because interactions between foundation-model-based agents will often be Newcomb-like. Some ways of reasoning about Newcomb-like problems may allow for greater cooperation between models.
  Our dataset contains both capabilities questions (i.e., questions with a unique, uncontroversially correct answer) and attitude questions (i.e., questions about which decision theorists would disagree). We use our dataset for an investigation of decision-theoretical capabilities and expressed attitudes and their interplay in existing models (different models by OpenAI, Anthropic, Meta, GDM, Reka, etc.), as well as models under simple prompt-based interventions. We find, among other things, that attitudes vary significantly between existing models; that high capabilities are associated with attitudes more favorable toward so-called evidential decision theory; and that attitudes are consistent across different types of questions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Relevance and Reasoning: Rationale Distillation in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2412.08519</link>
<guid>https://arxiv.org/abs/2412.08519</guid>
<content:encoded><![CDATA[
arXiv:2412.08519v2 Announce Type: replace 
Abstract: The reranker and generator are two critical components in the Retrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking relevant documents and generating responses. However, due to differences in pre-training data and objectives, there is an inevitable gap between the documents ranked as relevant by the reranker and those required by the generator to support answering the query. To address this gap, we propose RADIO, a novel and practical preference alignment framework with RAtionale DIstillatiOn. Specifically, we first propose a rationale extraction method that leverages the reasoning capabilities of Large Language Models (LLMs) to extract the rationales necessary for answering the query. Subsequently, a rationale-based alignment process is designed to rerank the documents based on the extracted rationales, and fine-tune the reranker to align the preferences. We conduct extensive experiments on two tasks across three datasets to demonstrate the effectiveness of our approach compared to baseline methods. Our code is released online to ease reproduction.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-based Synthesizer: Enhancing LLM Performance through Answer Synthesis</title>
<link>https://arxiv.org/abs/2501.01668</link>
<guid>https://arxiv.org/abs/2501.01668</guid>
<content:encoded><![CDATA[
arXiv:2501.01668v2 Announce Type: replace 
Abstract: Current inference scaling methods, such as Self-consistency and Best-of-N, have proven effective in improving the accuracy of LLMs on complex reasoning tasks. However, these methods rely heavily on the quality of candidate responses and are unable to produce correct answers when all candidates are incorrect. In this paper, we propose a novel inference scaling strategy, CoT-based Synthesizer, which leverages CoT reasoning to synthesize superior answers by analyzing complementary information from multiple candidate responses, even when all candidate responses are flawed. To enable a lightweight and cost-effective implementation, we introduce an automated data generation pipeline that creates diverse training data. This allows smaller LLMs trained on this data to improve the inference accuracy of larger models, including API-based LLMs. Experimental results across four benchmark datasets with seven policy models demonstrate that our method significantly enhances performance, with gains of 11.8% for Llama3-8B and 10.3% for GPT-4o on the MATH dataset. The corresponding training data and code are publicly available on https://github.com/RUCKBReasoning/CoT-based-Synthesizer.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Investigation into Value Misalignment in LLM-Generated Texts for Cultural Heritage</title>
<link>https://arxiv.org/abs/2501.02039</link>
<guid>https://arxiv.org/abs/2501.02039</guid>
<content:encoded><![CDATA[
arXiv:2501.02039v2 Announce Type: replace 
Abstract: As Large Language Models (LLMs) become increasingly prevalent in tasks related to cultural heritage, such as generating descriptions of historical monuments, translating ancient texts, preserving oral traditions, and creating educational content, their ability to produce accurate and culturally aligned texts is being increasingly relied upon by users and researchers. However, cultural value misalignments may exist in generated texts, such as the misrepresentation of historical facts, the erosion of cultural identity, and the oversimplification of complex cultural narratives, which may lead to severe consequences. Therefore, investigating value misalignment in the context of LLM for cultural heritage is crucial for mitigating these risks, yet there has been a significant lack of systematic and comprehensive study and investigation in this area. To fill this gap, we systematically assess the reliability of LLMs in generating culturally aligned texts for cultural heritage-related tasks. We conduct a comprehensive evaluation by compiling an extensive set of 1066 query tasks covering 5 widely recognized categories with 17 aspects within the knowledge framework of cultural heritage across 5 open-source LLMs, and examine both the type and rate of cultural value misalignments in the generated texts. Using both automated and manual approaches, we effectively detect and analyze the cultural value misalignments in LLM-generated texts. Our findings are concerning: over 65% of the generated texts exhibit notable cultural misalignments, with certain tasks demonstrating almost complete misalignment with key cultural values. Beyond these findings, this paper introduces a benchmark dataset and a comprehensive evaluation workflow that can serve as a valuable resource for future research aimed at enhancing the cultural sensitivity and reliability of LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Rotary Position Embeddings for Automatic Speech Recognition</title>
<link>https://arxiv.org/abs/2501.06051</link>
<guid>https://arxiv.org/abs/2501.06051</guid>
<content:encoded><![CDATA[
arXiv:2501.06051v2 Announce Type: replace 
Abstract: Self-attention relies on positional embeddings to encode input order. Relative Position (RelPos) embeddings are widely used in Automatic Speech Recognition (ASR). However, RelPos has quadratic time complexity to input length and is often incompatible with fast GPU implementations of attention. In contrast, Rotary Positional Embedding (RoPE) rotates each input vector based on its absolute position, taking linear time to sequence length, implicitly encoding relative distances through self-attention dot products. Thus, it is usually compatible with efficient attention. However, its use in ASR remains underexplored. This work evaluates RoPE across diverse ASR tasks with training data ranging from 100 to 50,000 hours, covering various speech types (read, spontaneous, clean, noisy) and different accents in both streaming and non-streaming settings. ASR error rates are similar or better than RelPos, while training time is reduced by up to 21%. Code is available via the SpeechBrain toolkit.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundations of Large Language Models</title>
<link>https://arxiv.org/abs/2501.09223</link>
<guid>https://arxiv.org/abs/2501.09223</guid>
<content:encoded><![CDATA[
arXiv:2501.09223v2 Announce Type: replace 
Abstract: This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting-edge technologies. The book is structured into five main chapters, each exploring a key area: pre-training, generative models, prompting, alignment, and inference. It is intended for college students, professionals, and practitioners in natural language processing and related fields, and can serve as a reference for anyone interested in large language models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Table Instruction Tuning</title>
<link>https://arxiv.org/abs/2501.14693</link>
<guid>https://arxiv.org/abs/2501.14693</guid>
<content:encoded><![CDATA[
arXiv:2501.14693v2 Announce Type: replace 
Abstract: Recent advances in table understanding have focused on instruction-tuning large language models (LLMs) for table-related tasks. However, existing research has overlooked the impact of hyperparameter choices, and also lacks a comprehensive evaluation of the out-of-domain table understanding ability and the general capabilities of these table LLMs. In this paper, we evaluate these abilities in existing table LLMs, and find significant declines in both out-of-domain table understanding and general capabilities as compared to their base models. Through systematic analysis, we show that hyperparameters, such as learning rate, can significantly influence both table-specific and general capabilities. Contrary to the previous table instruction-tuning work, we demonstrate that smaller learning rates and fewer training instances can enhance table understanding while preserving general capabilities. Based on our findings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B Instruct, which achieves performance on par with, or surpassing GPT-3.5 and GPT-4 on table tasks, while maintaining strong out-of-domain generalization and general capabilities. Our findings highlight the potential for reduced data annotation costs and more efficient model development through careful hyperparameter selection. We open-source the project and our models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation-Informed Merging of Large Language Models</title>
<link>https://arxiv.org/abs/2502.02421</link>
<guid>https://arxiv.org/abs/2502.02421</guid>
<content:encoded><![CDATA[
arXiv:2502.02421v2 Announce Type: replace 
Abstract: Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning (CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs, with up to a 40% increase in benchmark performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search</title>
<link>https://arxiv.org/abs/2502.02508</link>
<guid>https://arxiv.org/abs/2502.02508</guid>
<content:encoded><![CDATA[
arXiv:2502.02508v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOUQuET: dataset, Benchmark and Open initiative for Universal Quality Evaluation in Translation</title>
<link>https://arxiv.org/abs/2502.04314</link>
<guid>https://arxiv.org/abs/2502.04314</guid>
<content:encoded><![CDATA[
arXiv:2502.04314v2 Announce Type: replace 
Abstract: BOUQuET is a multi-way, multicentric and multi-register/domain dataset and benchmark, and a broader collaborative initiative. This dataset is handcrafted in 8 non-English languages. Each of these source languages are representative of the most widely spoken ones and therefore they have the potential to serve as pivot languages that will enable more accurate translations. The dataset is multicentric to enforce representation of multilingual language features. In addition, the dataset goes beyond the sentence level, as it is organized in paragraphs of various lengths. Compared with related machine translation datasets, we show that BOUQuET has a broader representation of domains while simplifying the translation task for non-experts. Therefore, BOUQuET is specially suitable for crowd-source extension for which we are launching a call aiming at collecting a multi-way parallel corpus covering any written language.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fino1: On the Transferability of Reasoning-Enhanced LLMs and Reinforcement Learning to Finance</title>
<link>https://arxiv.org/abs/2502.08127</link>
<guid>https://arxiv.org/abs/2502.08127</guid>
<content:encoded><![CDATA[
arXiv:2502.08127v3 Announce Type: replace 
Abstract: As the fundamental capability behind decision-making in finance, financial reasoning poses distinct challenges for LLMs. Although reinforcement learning (RL) have boosted generic reasoning, the progress in finance is hindered by the absence of empirical study of building effective financial chain-of-thought (CoT) corpus, a systematic comparison of different RL methods, and comprehensive benchmarks. To address these gaps, we introduce FinCoT, the first open high-fidelity CoT corpus for finance, distilled from seven QA datasets by a novel three-stage pipeline that incorporates domain supervision, iterative LLM refinement, and difficulty-aware filtering. Based on FinCoT, we develop Fin-o1, the first open financial reasoning models trained via supervised fine-tuning and GRPO-based RL. Our models outperform existing financial reasoning models and SOTA general models such as GPT-o1, DeepSeek-R1, and GPT-4.5. We also investigate the effectiveness of three different RL methods in improving domain-specific reasoning, offering the first such empirical study. We finally propose FinReason, the first financial reasoning benchmark covering multi-table analysis, long-context reasoning, and equation-based tasks, and evaluate 29 LLMs. Our extensive experiments reveal general reasoning models excel on standard benchmarks yet exhibit obvious performance degradation in financial contexts; even finance-tuned models like Dianjin-R1 and FinR1 degrade on lengthy documents. In contrast, our Fin-o1 models consistently outperform their backbones and larger GPT-o1 and DeepSeek-R1, confirming the effectiveness of our data building and model training strategy. Our study further shows that GRPO yields reliable gains whereas PPO and DPO do not, highlighting the need for targeted data and optimisation rather than scale alone.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truth Knows No Language: Evaluating Truthfulness Beyond English</title>
<link>https://arxiv.org/abs/2502.09387</link>
<guid>https://arxiv.org/abs/2502.09387</guid>
<content:encoded><![CDATA[
arXiv:2502.09387v3 Announce Type: replace 
Abstract: We introduce a professionally translated extension of the TruthfulQA benchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and Spanish. Truthfulness evaluations of large language models (LLMs) have primarily been conducted in English. However, the ability of LLMs to maintain truthfulness across languages remains under-explored. Our study evaluates 12 state-of-the-art open LLMs, comparing base and instruction-tuned models using human evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our findings reveal that, while LLMs perform best in English and worst in Basque (the lowest-resourced language), overall truthfulness discrepancies across languages are smaller than anticipated. Furthermore, we show that LLM-as-a-Judge correlates more closely with human judgments than multiple-choice metrics, and that informativeness plays a critical role in truthfulness assessment. Our results also indicate that machine translation provides a viable approach for extending truthfulness benchmarks to additional languages, offering a scalable alternative to professional translation. Finally, we observe that universal knowledge questions are better handled across languages than context- and time-dependent ones, highlighting the need for truthfulness evaluations that account for cultural and temporal variability. Dataset and code are publicly available under open licenses.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models</title>
<link>https://arxiv.org/abs/2502.09604</link>
<guid>https://arxiv.org/abs/2502.09604</guid>
<content:encoded><![CDATA[
arXiv:2502.09604v3 Announce Type: replace 
Abstract: We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks. The source code is available at https://github.com/facebookresearch/SelfCite
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTLM: Incorporating Bidirectional Text Information to Enhance Language Model Training in Speech Recognition Systems</title>
<link>https://arxiv.org/abs/2502.10058</link>
<guid>https://arxiv.org/abs/2502.10058</guid>
<content:encoded><![CDATA[
arXiv:2502.10058v2 Announce Type: replace 
Abstract: Automatic speech recognition (ASR) systems normally consist of an acoustic model (AM) and a language model (LM). The acoustic model estimates the probability distribution of text given the input speech, while the language model calibrates this distribution toward a specific knowledge domain to produce the final transcription. Traditional ASR-specific LMs are typically trained in a unidirectional (left-to-right) manner to align with autoregressive decoding. However, this restricts the model from leveraging the right-side context during training, limiting its representational capacity. In this work, we propose MTLM, a novel training paradigm that unifies unidirectional and bidirectional manners through 3 training objectives: ULM, BMLM, and UMLM. This approach enhances the LM's ability to capture richer linguistic patterns from both left and right contexts while preserving compatibility with standard ASR autoregressive decoding methods. As a result, the MTLM model not only enhances the ASR system's performance but also support multiple decoding strategies, including shallow fusion, unidirectional/bidirectional n-best rescoring. Experiments on the LibriSpeech dataset show that MTLM consistently outperforms unidirectional training across multiple decoding strategies, highlighting its effectiveness and flexibility in ASR applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMCTS: A Constrained Monte Carlo Tree Search Framework for Mathematical Reasoning in Large Language Model</title>
<link>https://arxiv.org/abs/2502.11169</link>
<guid>https://arxiv.org/abs/2502.11169</guid>
<content:encoded><![CDATA[
arXiv:2502.11169v2 Announce Type: replace 
Abstract: This paper introduces the Constrained Monte Carlo Tree Search (CMCTS) framework to enhance the mathematical reasoning capabilities of Large Language Models (LLM). By incorporating a constrained action space, Process Reward Model (PRM), and partial order rules, CMCTS effectively addresses the limitations of existing MCTS methods in terms of state space diversity and action selection rationality. Specifically, during the expansion phase, CMCTS restricts action sampling to a predefined constrained action set to increase candidate state diversity. In the simulation phase, it introduces partial order rules and PRM to optimize action selection and prevent unreasonable state transitions. Experimental results show that CMCTS performs outstandingly across multiple mathematical reasoning benchmarks. Under a zero-shot setting, a 7B-parameter model achieves an average accuracy of 83.4\%, surpassing the 72B baseline model by 4.8\%. Ablation studies demonstrate that each component of the framework is crucial for performance improvement, and their combined use fully leverages their respective strengths. Overall, the CMCTS framework provides an effective approach to enhancing LLM mathematical reasoning capabilities, supported by theoretical analysis, and offers novel insights for future reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Idiosyncrasies in Large Language Models</title>
<link>https://arxiv.org/abs/2502.12150</link>
<guid>https://arxiv.org/abs/2502.12150</guid>
<content:encoded><![CDATA[
arXiv:2502.12150v2 Announce Type: replace 
Abstract: In this work, we unveil and study idiosyncrasies in Large Language Models (LLMs) -- unique patterns in their outputs that can be used to distinguish the models. To do so, we consider a simple classification task: given a particular text output, the objective is to predict the source LLM that generates the text. We evaluate this synthetic task across various groups of LLMs and find that simply fine-tuning text embedding models on LLM-generated texts yields excellent classification accuracy. Notably, we achieve 97.1% accuracy on held-out validation data in the five-way classification problem involving ChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals that these idiosyncrasies are rooted in word-level distributions. These patterns persist even when the texts are rewritten, translated, or summarized by an external LLM, suggesting that they are also encoded in the semantic content. Additionally, we leverage LLM as judges to generate detailed, open-ended descriptions of each model's idiosyncrasies. Finally, we discuss the broader implications of our findings, including training on synthetic data, inferring model similarity, and robust evaluation of LLMs. Code is available at https://github.com/locuslab/llm-idiosyncrasies.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions</title>
<link>https://arxiv.org/abs/2502.13124</link>
<guid>https://arxiv.org/abs/2502.13124</guid>
<content:encoded><![CDATA[
arXiv:2502.13124v3 Announce Type: replace 
Abstract: Scaling reasoning capabilities beyond traditional domains such as math and coding is hindered by the lack of diverse and high-quality questions. To overcome this limitation, we introduce a scalable approach for generating diverse and challenging reasoning questions, accompanied by reference answers. We present NaturalReasoning, a comprehensive dataset comprising 2.8 million questions that span multiple domains, including STEM fields (e.g., Physics, Computer Science), Economics, Social Sciences, and more. We demonstrate the utility of the questions in NaturalReasoning through knowledge distillation experiments which show that NaturalReasoning can effectively elicit and transfer reasoning capabilities from a strong teacher model. Furthermore, we demonstrate that NaturalReasoning is also effective for unsupervised self-training using external reward models or self-rewarding. To foster future work, we publicly release NaturalReasoning at https://huggingface.co/datasets/facebook/natural_reasoning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProMedTS: A Self-Supervised, Prompt-Guided Multimodal Approach for Integrating Medical Text and Time Series</title>
<link>https://arxiv.org/abs/2502.13509</link>
<guid>https://arxiv.org/abs/2502.13509</guid>
<content:encoded><![CDATA[
arXiv:2502.13509v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable performance in vision-language tasks, but their application in the medical field remains underexplored, particularly for integrating structured time series data with unstructured clinical notes. In clinical practice, dynamic time series data, such as lab test results, capture critical temporal patterns, while clinical notes provide rich semantic context. Merging these modalities is challenging due to the inherent differences between continuous signals and discrete text. To bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal framework that employs prompt-guided learning to unify these heterogeneous data types. Our approach leverages lightweight anomaly detection to generate anomaly captions that serve as prompts, guiding the encoding of raw time series data into informative prompt embeddings. These prompt embeddings are aligned with textual representations in a shared latent space, preserving fine-grained temporal nuances alongside semantic insights. Furthermore, our framework incorporates tailored self-supervised objectives to enhance both intra- and inter-modal alignment. We evaluate ProMedTS on disease diagnosis tasks using real-world datasets, and the results demonstrate that our method consistently outperforms state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large and Balanced Corpus for Fine-grained Arabic Readability Assessment</title>
<link>https://arxiv.org/abs/2502.13520</link>
<guid>https://arxiv.org/abs/2502.13520</guid>
<content:encoded><![CDATA[
arXiv:2502.13520v2 Announce Type: replace 
Abstract: This paper introduces the Balanced Arabic Readability Evaluation Corpus (BAREC), a large-scale, fine-grained dataset for Arabic readability assessment. BAREC consists of 69,441 sentences spanning 1+ million words, carefully curated to cover 19 readability levels, from kindergarten to postgraduate comprehension. The corpus balances genre diversity, topical coverage, and target audiences, offering a comprehensive resource for evaluating Arabic text complexity. The corpus was fully manually annotated by a large team of annotators. The average pairwise inter-annotator agreement, measured by Quadratic Weighted Kappa, is 81.8%, reflecting a high level of substantial agreement. Beyond presenting the corpus, we benchmark automatic readability assessment across different granularity levels, comparing a range of techniques. Our results highlight the challenges and opportunities in Arabic readability modeling, demonstrating competitive performance across various methods. To support research and education, we make BAREC openly available, along with detailed annotation guidelines and benchmark results.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Regularization with Sparse Autoencoders for Controllable LLM-based Classification</title>
<link>https://arxiv.org/abs/2502.14133</link>
<guid>https://arxiv.org/abs/2502.14133</guid>
<content:encoded><![CDATA[
arXiv:2502.14133v2 Announce Type: replace 
Abstract: Modern text classification methods heavily rely on contextual embeddings from large language models (LLMs). Compared to human-engineered features, these embeddings provide automatic and effective representations for classification model training. However, they also introduce a challenge: we lose the ability to manually remove unintended features, such as sensitive or task-irrelevant features, to guarantee regulatory compliance or improve the generalizability of classification models. This limitation arises because LLM embeddings are opaque and difficult to interpret. In this paper, we propose a novel framework to identify and regularize unintended features in the LLM latent space. Specifically, we first pre-train a sparse autoencoder (SAE) to extract interpretable features from LLM latent spaces. To ensure the SAE can capture task-specific features, we further fine-tune it on task-specific datasets. In training the classification model, we propose a simple and effective regularizer, by minimizing the similarity between the classifier weights and the identified unintended feature, to remove the impact of these unintended features on classification. We evaluate the proposed framework on three real-world tasks, including toxic chat detection, reward modeling, and disease diagnosis. Results show that the proposed self-regularization framework can improve the classifier's generalizability by regularizing those features that are not semantically correlated to the task. This work pioneers controllable text classification on LLM latent spaces by leveraging interpreted features to address generalizability, fairness, and privacy challenges. The code and data are publicly available at https://github.com/JacksonWuxs/Controllable_LLM_Classifier.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entity Framing and Role Portrayal in the News</title>
<link>https://arxiv.org/abs/2502.14718</link>
<guid>https://arxiv.org/abs/2502.14718</guid>
<content:encoded><![CDATA[
arXiv:2502.14718v2 Announce Type: replace 
Abstract: We introduce a novel multilingual hierarchical corpus annotated for entity framing and role portrayal in news articles. The dataset uses a unique taxonomy inspired by storytelling elements, comprising 22 fine-grained roles, or archetypes, nested within three main categories: protagonist, antagonist, and innocent. Each archetype is carefully defined, capturing nuanced portrayals of entities such as guardian, martyr, and underdog for protagonists; tyrant, deceiver, and bigot for antagonists; and victim, scapegoat, and exploited for innocents. The dataset includes 1,378 recent news articles in five languages (Bulgarian, English, Hindi, European Portuguese, and Russian) focusing on two critical domains of global significance: the Ukraine-Russia War and Climate Change. Over 5,800 entity mentions have been annotated with role labels. This dataset serves as a valuable resource for research into role portrayal and has broader implications for news analysis. We describe the characteristics of the dataset and the annotation process, and we report evaluation results on fine-tuned state-of-the-art multilingual transformers and hierarchical zero-shot learning using LLMs at the level of a document, a paragraph, and a sentence.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Training-free LLM-based Approach to General Chinese Character Error Correction</title>
<link>https://arxiv.org/abs/2502.15266</link>
<guid>https://arxiv.org/abs/2502.15266</guid>
<content:encoded><![CDATA[
arXiv:2502.15266v2 Announce Type: replace 
Abstract: Chinese spelling correction (CSC) is a crucial task that aims to correct character errors in Chinese text. While conventional CSC focuses on character substitution errors caused by mistyping, two other common types of character errors, missing and redundant characters, have received less attention. These errors are often excluded from CSC datasets during the annotation process or ignored during evaluation, even when they have been annotated. This issue limits the practicality of the CSC task. To address this issue, we introduce the task of General Chinese Character Error Correction (C2EC), which focuses on all three types of character errors. We construct a high-quality C2EC benchmark by combining and manually verifying data from CCTC and Lemon datasets. We extend the training-free prompt-free CSC method to C2EC by using Levenshtein distance for handling length changes and leveraging an additional prompt-based large language model (LLM) to improve performance. Experiments show that our method enables a 14B-parameter LLM to be on par with models nearly 50 times larger on both conventional CSC and C2EC tasks, without any fine-tuning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiniSST: Simultaneous Translation of Unbounded Speech with Large Language Model</title>
<link>https://arxiv.org/abs/2503.02969</link>
<guid>https://arxiv.org/abs/2503.02969</guid>
<content:encoded><![CDATA[
arXiv:2503.02969v2 Announce Type: replace 
Abstract: Simultaneous translation of unbounded streaming speech remains a challenging problem due to the need for effectively processing the history speech context and past translations so that quality and latency, including computation overhead, can be balanced. Most prior works assume pre-segmented speech, limiting their real-world applicability. In this paper, we propose InfiniSST, a novel approach that formulates SST as a multi-turn dialogue task, enabling seamless translation of unbounded speech. We construct translation trajectories and robust segments from MuST-C with multi-latency augmentation during training and develop a key-value (KV) cache management strategy to facilitate efficient inference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that InfiniSST reduces computation-aware latency by 0.5 to 1 second while maintaining the same translation quality compared to baselines. Ablation studies further validate the contributions of our data construction and cache management strategy. We release the code and demo at https://github.com/LeiLiLab/InfiniSST
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Safety Alignment of Large Language Models via Preference Re-ranking and Representation-based Reward Modeling</title>
<link>https://arxiv.org/abs/2503.10093</link>
<guid>https://arxiv.org/abs/2503.10093</guid>
<content:encoded><![CDATA[
arXiv:2503.10093v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) algorithms for safety alignment of Large Language Models (LLMs), such as Direct Preference Optimization (DPO), encounter the challenge of distribution shift. Current approaches typically address this issue through online sampling from the target policy, which requires significant computational resources. In this paper, we hypothesize that during off-policy training, while the ranking order of output generated by policy changes, their overall distribution remains relatively stable. This stability allows the conversion of the sampling process from the target policy into a computationally efficient re-ranking of preference data. Building on this hypothesis, we propose a new framework that leverages the model's intrinsic safety judgment capability to extract reward signals, which are then used to calculate label confidence for preference reordering. Extensive experiments and theoretical analysis demonstrate that the proposed method effectively addresses the distribution shift issue, remarkably enhancing the safety performance while avoiding about 300x computational overheads.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REPA: Russian Error Types Annotation for Evaluating Text Generation and Judgment Capabilities</title>
<link>https://arxiv.org/abs/2503.13102</link>
<guid>https://arxiv.org/abs/2503.13102</guid>
<content:encoded><![CDATA[
arXiv:2503.13102v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have introduced the novel paradigm of using LLMs as judges, where an LLM evaluates and scores the outputs of another LLM, which often correlates highly with human preferences. However, the use of LLM-as-a-judge has been primarily studied in English. In this paper, we evaluate this framework in Russian by introducing the Russian Error tyPes Annotation dataset (REPA), a dataset of 1k user queries and 2k LLM-generated responses. Human annotators labeled each response pair expressing their preferences across ten specific error types, as well as selecting an overall preference. We rank six generative LLMs across the error types using three rating systems based on human preferences. We also evaluate responses using eight LLM judges in zero-shot and few-shot settings. We describe the results of analyzing the judges and position and length biases. Our findings reveal a notable gap between LLM judge performance in Russian and English. However, rankings based on human and LLM preferences show partial alignment, suggesting that while current LLM judges struggle with fine-grained evaluation in Russian, there is potential for improvement.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MathFusion: Enhancing Mathematical Problem-solving of LLM through Instruction Fusion</title>
<link>https://arxiv.org/abs/2503.16212</link>
<guid>https://arxiv.org/abs/2503.16212</guid>
<content:encoded><![CDATA[
arXiv:2503.16212v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown impressive progress in mathematical reasoning. While data augmentation is promising to enhance mathematical problem-solving ability, current approaches are predominantly limited to instance-level modifications-such as rephrasing or generating syntactic variations-which fail to capture and leverage the intrinsic relational structures inherent in mathematical knowledge. Inspired by human learning processes, where mathematical proficiency develops through systematic exposure to interconnected concepts, we introduce MathFusion, a novel framework that enhances mathematical reasoning through cross-problem instruction synthesis. MathFusion implements this through three fusion strategies: (1) sequential fusion, which chains related problems to model solution dependencies; (2) parallel fusion, which combines analogous problems to reinforce conceptual understanding; and (3) conditional fusion, which creates context-aware selective problems to enhance reasoning flexibility. By applying these strategies, we generate a new dataset, \textbf{MathFusionQA}, followed by fine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental results demonstrate that MathFusion achieves substantial improvements in mathematical reasoning while maintaining high data efficiency, boosting performance by 18.0 points in accuracy across diverse benchmarks while requiring only 45K additional synthetic instructions, representing a substantial improvement over traditional single-instruction approaches. Our datasets, models, and code are publicly available at https://github.com/QizhiPei/mathfusion.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShED-HD: A Shannon Entropy Distribution Framework for Lightweight Hallucination Detection on Edge Devices</title>
<link>https://arxiv.org/abs/2503.18242</link>
<guid>https://arxiv.org/abs/2503.18242</guid>
<content:encoded><![CDATA[
arXiv:2503.18242v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities on a broad array of NLP tasks, but their tendency to produce hallucinations$\unicode{x2013}$plausible-sounding but factually incorrect content$\unicode{x2013}$poses severe challenges in high-stakes domains. Existing hallucination detection methods either bear the computational cost of multiple inference passes or sacrifice accuracy for efficiency with single-pass approaches, neither of which is ideal in resource-constrained environments such as edge devices. We propose the Shannon Entropy Distribution Hallucination Detector (ShED-HD), a novel hallucination detection framework that bridges this gap by classifying sequence-level entropy patterns using a lightweight BiLSTM architecture with single-headed attention. In contrast to prior approaches, ShED-HD efficiently detects distinctive uncertainty patterns across entire output sequences, preserving contextual awareness. Through in-depth evaluation on three datasets (BioASQ, TriviaQA, and Jeopardy Questions), we show that ShED-HD significantly outperforms other computationally efficient approaches in the out-of-distribution setting, while achieving comparable performance in the in-distribution setting. ShED-HD facilitates hallucination detection that is low-cost, accurate, and generalizable, improving the credibility of content generated by LLMs in resource-constrained environments where trustworthy AI functionality is crucial.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinkAlign: Scalable Schema Linking for Real-World Large-Scale Multi-Database Text-to-SQL</title>
<link>https://arxiv.org/abs/2503.18596</link>
<guid>https://arxiv.org/abs/2503.18596</guid>
<content:encoded><![CDATA[
arXiv:2503.18596v3 Announce Type: replace 
Abstract: Schema linking is a critical bottleneck in applying existing Text-to-SQL models to real-world, large-scale, multi-database environments. Through error analysis, we identify two major challenges in schema linking: (1) Database Retrieval: accurately selecting the target database from a large schema pool, while effectively filtering out irrelevant ones; and (2) Schema Item Grounding: precisely identifying the relevant tables and columns within complex and often redundant schemas for SQL generation. Based on these, we introduce LinkAlign, a novel framework tailored for large-scale databases with thousands of fields. LinkAlign comprises three key steps: multi-round semantic enhanced retrieval and irrelevant information isolation for Challenge 1, and schema extraction enhancement for Challenge 2. Each stage supports both Agent and Pipeline execution modes, enabling balancing efficiency and performance via modular design. To enable more realistic evaluation, we construct AmbiDB, a synthetic dataset designed to reflect the ambiguity of real-world schema linking. Experiments on widely-used Text-to-SQL benchmarks demonstrate that LinkAlign consistently outperforms existing baselines on all schema linking metrics. Notably, it improves the overall Text-to-SQL pipeline and achieves a new state-of-the-art score of 33.09% on the Spider 2.0-Lite benchmark using only open-source LLMs, ranking first on the leaderboard at the time of submission. The codes are available at https://github.com/Satissss/LinkAlign
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Inference for Large Reasoning Models: A Survey</title>
<link>https://arxiv.org/abs/2503.23077</link>
<guid>https://arxiv.org/abs/2503.23077</guid>
<content:encoded><![CDATA[
arXiv:2503.23077v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in complex task-solving. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from performance and efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs' inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant field\footnote{https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs}.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating how LLM annotations represent diverse views on contentious topics</title>
<link>https://arxiv.org/abs/2503.23243</link>
<guid>https://arxiv.org/abs/2503.23243</guid>
<content:encoded><![CDATA[
arXiv:2503.23243v2 Announce Type: replace 
Abstract: Researchers have proposed the use of generative large language models (LLMs) to label data for research and applied settings. This literature emphasizes the improved performance of these models relative to other natural language models, noting that generative LLMs typically outperform other models and even humans across several metrics. Previous literature has examined bias across many applications and contexts, but less work has focused specifically on bias in generative LLMs' responses to subjective annotation tasks. This bias could result in labels applied by LLMs that disproportionately align with majority groups over a more diverse set of viewpoints. In this paper, we evaluate how LLMs represent diverse viewpoints on these contentious tasks. Across four annotation tasks on four datasets, we show that LLMs do not show systematic substantial disagreement with annotators on the basis of demographics. Rather, we find that multiple LLMs tend to be biased in the same directions on the same demographic categories within the same datasets. Moreover, the disagreement between human annotators on the labeling task -- a measure of item difficulty -- is far more predictive of LLM agreement with human annotators. We conclude with a discussion of the implications for researchers and practitioners using LLMs for automated data annotation tasks. Specifically, we emphasize that fairness evaluations must be contextual, model choice alone will not solve potential issues of bias, and item difficulty must be integrated into bias assessments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Experiential Semantic Information and Brain Alignment: Are Multimodal Models Better than Language Models?</title>
<link>https://arxiv.org/abs/2504.00942</link>
<guid>https://arxiv.org/abs/2504.00942</guid>
<content:encoded><![CDATA[
arXiv:2504.00942v2 Announce Type: replace 
Abstract: A common assumption in Computational Linguistics is that text representations learnt by multimodal models are richer and more human-like than those by language-only models, as they are grounded in images or audio -- similar to how human language is grounded in real-world experiences. However, empirical studies checking whether this is true are largely lacking. We address this gap by comparing word representations from contrastive multimodal models vs. language-only ones in the extent to which they capture experiential information -- as defined by an existing norm-based 'experiential model' -- and align with human fMRI responses. Our results indicate that, surprisingly, language-only models are superior to multimodal ones in both respects. Additionally, they learn more unique brain-relevant semantic information beyond that shared with the experiential model. Overall, our study highlights the need to develop computational models that better integrate the complementary semantic information provided by multimodal data sources.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables</title>
<link>https://arxiv.org/abs/2504.06560</link>
<guid>https://arxiv.org/abs/2504.06560</guid>
<content:encoded><![CDATA[
arXiv:2504.06560v3 Announce Type: replace 
Abstract: Processing structured tabular data, particularly large and lengthy tables, constitutes a fundamental yet challenging task for large language models (LLMs). However, existing long-context benchmarks like Needle-in-a-Haystack primarily focus on unstructured text, neglecting the challenge of diverse structured tables. Meanwhile, previous tabular benchmarks mainly consider downstream tasks that require high-level reasoning abilities, and overlook models' underlying fine-grained perception of individual table cells, which is crucial for practical and robust LLM-based table applications. To address this gap, we introduce \textsc{NeedleInATable} (NIAT), a new long-context tabular benchmark that treats each table cell as a ``needle'' and requires models to extract the target cell based on cell locations or lookup questions. Our comprehensive evaluation of various LLMs and multimodal LLMs reveals a substantial performance gap between popular downstream tabular tasks and the simpler NIAT task, suggesting that they may rely on dataset-specific correlations or shortcuts to obtain better benchmark results but lack truly robust long-context understanding towards structured tables. Furthermore, we demonstrate that using synthesized NIAT training data can effectively improve performance on both NIAT task and downstream tabular tasks, which validates the importance of NIAT capability for LLMs' genuine table understanding ability. Our data, code and models will be released to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scholar Inbox: Personalized Paper Recommendations for Scientists</title>
<link>https://arxiv.org/abs/2504.08385</link>
<guid>https://arxiv.org/abs/2504.08385</guid>
<content:encoded><![CDATA[
arXiv:2504.08385v2 Announce Type: replace 
Abstract: Scholar Inbox is a new open-access platform designed to address the challenges researchers face in staying current with the rapidly expanding volume of scientific literature. We provide personalized recommendations, continuous updates from open-access archives (arXiv, bioRxiv, etc.), visual paper summaries, semantic search, and a range of tools to streamline research workflows and promote open research access. The platform's personalized recommendation system is trained on user ratings, ensuring that recommendations are tailored to individual researchers' interests. To further enhance the user experience, Scholar Inbox also offers a map of science that provides an overview of research across domains, enabling users to easily explore specific topics. We use this map to address the cold start problem common in recommender systems, as well as an active learning strategy that iteratively prompts users to rate a selection of papers, allowing the system to learn user preferences quickly. We evaluate the quality of our recommendation system on a novel dataset of 800k user ratings, which we make publicly available, as well as via an extensive user study. https://www.scholar-inbox.com/
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Classification of English Words Based on Phonological Information: Discovery of Germanic and Latinate Clusters</title>
<link>https://arxiv.org/abs/2504.11770</link>
<guid>https://arxiv.org/abs/2504.11770</guid>
<content:encoded><![CDATA[
arXiv:2504.11770v2 Announce Type: replace 
Abstract: Cross-linguistically, native words and loanwords follow different phonological rules. In English, for example, words of Germanic and Latinate origin exhibit different stress patterns, and a certain syntactic structure is exclusive to Germanic verbs. When seeing them as a cognitive model, however, such etymology-based generalizations face challenges in terms of learnability, since the historical origins of words are presumably inaccessible information for general language learners. In this study, we present computational evidence indicating that the Germanic-Latinate distinction in the English lexicon is learnable from the phonotactic information of individual words. Specifically, we performed an unsupervised clustering on corpus-extracted words, and the resulting word clusters largely aligned with the etymological distinction. The model-discovered clusters also recovered various linguistic generalizations documented in the previous literature regarding the corresponding etymological classes. Moreover, our findings also uncovered previously unrecognized features of the quasi-etymological clusters, offering novel hypotheses for future experimental studies.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments</title>
<link>https://arxiv.org/abs/2504.21016</link>
<guid>https://arxiv.org/abs/2504.21016</guid>
<content:encoded><![CDATA[
arXiv:2504.21016v2 Announce Type: replace 
Abstract: The COVID-19 pandemic caused great losses worldwide, efforts are taken place to prevent but many countries have failed. In Vietnam, the traceability, localization, and quarantine of people who contact with patients contribute to effective disease prevention. However, this is done by hand, and take a lot of work. In this research, we describe a named-entity recognition (NER) study that assists in the prevention of COVID-19 pandemic in Vietnam. We also present our manually annotated COVID-19 dataset with nested named entity recognition task for Vietnamese which be defined new entity types using for our system.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese</title>
<link>https://arxiv.org/abs/2504.21017</link>
<guid>https://arxiv.org/abs/2504.21017</guid>
<content:encoded><![CDATA[
arXiv:2504.21017v2 Announce Type: replace 
Abstract: After two years of appearance, COVID-19 has negatively affected people and normal life around the world. As in May 2022, there are more than 522 million cases and six million deaths worldwide (including nearly ten million cases and over forty-three thousand deaths in Vietnam). Economy and society are both severely affected. The variant of COVID-19, Omicron, has broken disease prevention measures of countries and rapidly increased number of infections. Resources overloading in treatment and epidemics prevention is happening all over the world. It can be seen that, application of artificial intelligence (AI) to support people at this time is extremely necessary. There have been many studies applying AI to prevent COVID-19 which are extremely useful, and studies on machine reading comprehension (MRC) are also in it. Realizing that, we created the first MRC dataset about COVID-19 for Vietnamese: ViQA-COVID and can be used to build models and systems, contributing to disease prevention. Besides, ViQA-COVID is also the first multi-span extraction MRC dataset for Vietnamese, we hope that it can contribute to promoting MRC studies in Vietnamese and multilingual.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Reasonable Parrots: Why Large Language Models Should Argue with Us by Design</title>
<link>https://arxiv.org/abs/2505.05298</link>
<guid>https://arxiv.org/abs/2505.05298</guid>
<content:encoded><![CDATA[
arXiv:2505.05298v2 Announce Type: replace 
Abstract: In this position paper, we advocate for the development of conversational technology that is inherently designed to support and facilitate argumentative processes. We argue that, at present, large language models (LLMs) are inadequate for this purpose, and we propose an ideal technology design aimed at enhancing argumentative skills. This involves re-framing LLMs as tools to exercise our critical thinking skills rather than replacing them. We introduce the concept of \textit{reasonable parrots} that embody the fundamental principles of relevance, responsibility, and freedom, and that interact through argumentative dialogical moves. These principles and moves arise out of millennia of work in argumentation theory and should serve as the starting point for LLM-based technology that incorporates basic principles of argumentation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Abstract Thinking Empowers Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2505.20164</link>
<guid>https://arxiv.org/abs/2505.20164</guid>
<content:encoded><![CDATA[
arXiv:2505.20164v2 Announce Type: replace 
Abstract: Images usually convey richer detail than text, but often include redundant information which potentially downgrades multimodal reasoning performance. When faced with lengthy or complex messages, humans tend to employ abstract thinking to convert them into simple and concise abstracts. Inspired by this cognitive strategy, we introduce Visual Abstract Thinking (VAT), a novel thinking paradigm that prompts Multimodal Large Language Models (MLLMs) with visual abstract instead of explicit verbal thoughts or elaborate guidance, permitting a more concentrated visual reasoning mechanism. Explicit thinking, such as Chain-of-thought (CoT) or tool-augmented approaches, increases the complexity of reasoning process via inserting verbose intermediate steps, external knowledge or visual information. In contrast, VAT reduces redundant visual information and encourages models to focus their reasoning on more essential visual elements. Experimental results show that VAT consistently empowers different models, and achieves an average gain of 17% over GPT-4o baseline by employing diverse types of visual abstracts, demonstrating that VAT can enhance visual reasoning abilities for MLLMs regarding conceptual, structural and relational reasoning tasks. VAT is also compatible with CoT in knowledge-intensive multimodal reasoning tasks. These findings highlight the effectiveness of visual reasoning via abstract thinking and encourage further exploration of more diverse reasoning paradigms from the perspective of human cognition.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis</title>
<link>https://arxiv.org/abs/2505.21138</link>
<guid>https://arxiv.org/abs/2505.21138</guid>
<content:encoded><![CDATA[
arXiv:2505.21138v2 Announce Type: replace 
Abstract: Large-scale training corpora have significantly improved the performance of ASR models. Unfortunately, due to the relative scarcity of data, Chinese accents and dialects remain a challenge for most ASR models. Recent advancements in self-supervised learning have shown that self-supervised pre-training, combined with large language models (LLM), can effectively enhance ASR performance in low-resource scenarios. We aim to investigate the effectiveness of this paradigm for Chinese dialects. Specifically, we pre-train a Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech data and do alignment training on a supervised dataset of 40,000 hours. Then, we systematically examine the impact of various projectors and LLMs on Mandarin, dialect, and accented speech recognition performance under this paradigm. Our method achieved SOTA results on multiple dialect datasets, including Kespeech. We will open-source our work to promote reproducible research
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-KV: Redundancy-aware KV Cache Compression for Reasoning Models</title>
<link>https://arxiv.org/abs/2505.24133</link>
<guid>https://arxiv.org/abs/2505.24133</guid>
<content:encoded><![CDATA[
arXiv:2505.24133v3 Announce Type: replace 
Abstract: Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus</title>
<link>https://arxiv.org/abs/2506.00332</link>
<guid>https://arxiv.org/abs/2506.00332</guid>
<content:encoded><![CDATA[
arXiv:2506.00332v2 Announce Type: replace 
Abstract: Code-mixing involves the seamless integration of linguistic elements from multiple languages within a single discourse, reflecting natural multilingual communication patterns. Despite its prominence in informal interactions such as social media, chat messages and instant-messaging exchanges, there has been a lack of publicly available corpora that are author-labeled and suitable for modeling human conversations and relationships. This study introduces the first labeled and general-purpose corpus for understanding code-mixing in context while maintaining rigorous privacy and ethical standards. Our live project will continuously gather, verify, and integrate code-mixed messages into a structured dataset released in JSON format, accompanied by detailed metadata and linguistic statistics. To date, it includes over 355,641 messages spanning various code-mixing patterns, with a primary focus on English, Mandarin, and other languages. We expect the Codemix Corpus to serve as a foundational dataset for research in computational linguistics, sociolinguistics, and NLP applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation</title>
<link>https://arxiv.org/abs/2506.00713</link>
<guid>https://arxiv.org/abs/2506.00713</guid>
<content:encoded><![CDATA[
arXiv:2506.00713v2 Announce Type: replace 
Abstract: This paper presents a framework to convert argumentative texts into argument knowledge graphs (AKG). Starting with basic annotations of argumentative components (ACs) and argumentative relations (ARs), we enrich the information by constructing a knowledge base (KB) graph with metadata attributes for nodes. Next, we use premises and inference rules from the KB to form arguments by applying modus ponens. From these arguments, we create an AKG. The nodes and edges of the AKG have attributes that capture important argumentative features. We also find missing inference rules by identifying markers. This makes it possible to identify undercut attacks that were previously undetectable in existing datasets. The AKG gives a graphical view of the argumentative structure that is easier to understand than theoretical formats. It also prepares the ground for future reasoning tasks, including checking the coherence of arguments and identifying opportunities for revision. For this, it is important to find indirect relations, many of which are implicit. Our proposed AKG format, with annotated inference rules and modus ponens, will help reasoning models learn the implicit indirect relations that require inference over arguments and the relations between them.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models</title>
<link>https://arxiv.org/abs/2506.03781</link>
<guid>https://arxiv.org/abs/2506.03781</guid>
<content:encoded><![CDATA[
arXiv:2506.03781v2 Announce Type: replace 
Abstract: How can we quantize large language models while preserving accuracy? Quantization is essential for deploying large language models (LLMs) efficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are promising quantization schemes that have strong expressiveness and optimizability, respectively. However, neither scheme leverages both advantages. In this paper, we propose UniQuanF (Unified Quantization with Flexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses both strong expressiveness and optimizability by unifying the flexible mapping technique in UQ and non-uniform quantization levels of BCQ. We propose unified initialization, and local and periodic mapping techniques to optimize the parameters in UniQuanF precisely. After optimization, our unification theorem removes computational and memory overhead, allowing us to utilize the superior accuracy of UniQuanF without extra deployment costs induced by the unification. Experimental results demonstrate that UniQuanF outperforms existing UQ and BCQ methods, achieving up to 4.60% higher accuracy on GSM8K benchmark.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MELABenchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource Maltese NLP</title>
<link>https://arxiv.org/abs/2506.04385</link>
<guid>https://arxiv.org/abs/2506.04385</guid>
<content:encoded><![CDATA[
arXiv:2506.04385v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across various Natural Language Processing (NLP) tasks, largely due to their generalisability and ability to perform tasks without additional training. However, their effectiveness for low-resource languages remains limited. In this study, we evaluate the performance of 55 publicly available LLMs on Maltese, a low-resource language, using a newly introduced benchmark covering 11 discriminative and generative tasks. Our experiments highlight that many models perform poorly, particularly on generative tasks, and that smaller fine-tuned models often perform better across all tasks. From our multidimensional analysis, we investigate various factors impacting performance. We conclude that prior exposure to Maltese during pre-training and instruction-tuning emerges as the most important factor. We also examine the trade-offs between fine-tuning and prompting, highlighting that while fine-tuning requires a higher initial cost, it yields better performance and lower inference costs. Through this work, we aim to highlight the need for more inclusive language technologies and recommend that researchers working with low-resource languages consider more "traditional" language modelling approaches.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation</title>
<link>https://arxiv.org/abs/2506.05606</link>
<guid>https://arxiv.org/abs/2506.05606</guid>
<content:encoded><![CDATA[
arXiv:2506.05606v2 Announce Type: replace 
Abstract: Can large language models (LLMs) accurately simulate the next web action of a specific user? While LLMs have shown promising capabilities in generating ``believable'' human behaviors, evaluating their ability to mimic real user behaviors remains an open challenge, largely due to the lack of high-quality, publicly available datasets that capture both the observable actions and the internal reasoning of an actual human user. To address this gap, we introduce OPERA, a novel dataset of Observation, Persona, Rationale, and Action collected from real human participants during online shopping sessions. OPERA is the first public dataset that comprehensively captures: user personas, browser observations, fine-grained web actions, and self-reported just-in-time rationales. We developed both an online questionnaire and a custom browser plugin to gather this dataset with high fidelity. Using OPERA, we establish the first benchmark to evaluate how well current LLMs can predict a specific user's next action and rationale with a given persona and  history. This dataset lays the groundwork for future research into LLM agents that aim to act as personalized digital twins for human.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Understanding with Large Language Models: A Survey</title>
<link>https://arxiv.org/abs/2312.17432</link>
<guid>https://arxiv.org/abs/2312.17432</guid>
<content:encoded><![CDATA[
arXiv:2312.17432v5 Announce Type: replace-cross 
Abstract: With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of large language models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of recent advancements in video understanding that harness the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended multi-granularity (general, temporal, and spatiotemporal) reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into three main types: Video Analyzer x LLM, Video Embedder x LLM, and (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based on the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as Text Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this survey presents a comprehensive study of the tasks, datasets, benchmarks, and evaluation methodologies for Vid-LLMs. Additionally, it explores the expansive applications of Vid-LLMs across various domains, highlighting their remarkable scalability and versatility in real-world video understanding challenges. Finally, it summarizes the limitations of existing Vid-LLMs and outlines directions for future research. For more information, readers are recommended to visit the repository at https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversation</title>
<link>https://arxiv.org/abs/2402.11827</link>
<guid>https://arxiv.org/abs/2402.11827</guid>
<content:encoded><![CDATA[
arXiv:2402.11827v2 Announce Type: replace-cross 
Abstract: Conversational search, unlike single-turn retrieval tasks, requires understanding the current question within a dialogue context. The common approach of rewrite-then-retrieve aims to decontextualize questions to be self-sufficient for off-the-shelf retrievers, but most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results. To overcome this limitation, we present a novel framework RetPO (Retriever's Preference Optimization), which is designed to optimize a language model (LM) for reformulating search queries in line with the preferences of the target retrieval systems. The process begins by prompting a large LM to produce various potential rewrites and then collects retrieval performance for these rewrites as the retrievers' preferences. Through the process, we construct a large-scale dataset called RF collection, containing Retrievers' Feedback on over 410K query rewrites across 12K conversations. Furthermore, we fine-tune a smaller LM on this dataset to align it with the retrievers' feedback. Our resulting model demonstrates superiority on two benchmarks, surpassing the previous state-of-the-art performance of rewrite-then-retrieve approaches.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Wireless Federated Learning for Large Language Models</title>
<link>https://arxiv.org/abs/2404.13238</link>
<guid>https://arxiv.org/abs/2404.13238</guid>
<content:encoded><![CDATA[
arXiv:2404.13238v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have driven profound transformations in wireless networks. However, within wireless environments, the training of LLMs faces significant challenges related to security and privacy. Federated Learning (FL), with its decentralized architecture, offers enhanced data privacy protection. Nevertheless, when integrated with LLMs, FL still struggles with several critical limitations, including large-scale and heterogeneous data, resource-intensive training, and substantial communication overhead. To address these challenges, this paper first presents a systematic analysis of the distinct training stages of LLMs in wireless networks, including pre-training, instruction tuning, and alignment tuning. Building upon this foundation, we propose a Personalized Wireless Federated Fine-tuning (PWFF) framework. Initially, we utilize the adapter and Low-Rank Adaptation (LoRA) techniques to decrease energy consumption, while employing global partial aggregation to reduce communication delay. Subsequently, we develop two reward models and design a personalized loss function to fulfill the goal of personalized learning. Furthermore, we implement a local multi-objective alignment to ensure the stability and effectiveness of the FL process. Finally, we conduct a series of simulations to validate the performance of the proposed PWFF method and provide an in-depth discussion of the open issues.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Sequential Decision Making with Large Language Models</title>
<link>https://arxiv.org/abs/2406.12125</link>
<guid>https://arxiv.org/abs/2406.12125</guid>
<content:encoded><![CDATA[
arXiv:2406.12125v2 Announce Type: replace-cross 
Abstract: This paper focuses on extending the success of large language models (LLMs) to sequential decision making. Existing efforts either (i) re-train or finetune LLMs for decision making, or (ii) design prompts for pretrained LLMs. The former approach suffers from the computational burden of gradient updates, and the latter approach does not show promising results. In this paper, we propose a new approach that leverages online model selection algorithms to efficiently incorporate LLMs agents into sequential decision making. Statistically, our approach significantly outperforms both traditional decision making algorithms and vanilla LLM agents. Computationally, our approach avoids the need for expensive gradient updates of LLMs, and throughout the decision making process, it requires only a small number of LLM calls. We conduct extensive experiments to verify the effectiveness of our proposed approach. As an example, on a large-scale Amazon dataset, our approach achieves more than a 6x performance gain over baselines while calling LLMs in only 1.5% of the time steps.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human Preference</title>
<link>https://arxiv.org/abs/2406.15513</link>
<guid>https://arxiv.org/abs/2406.15513</guid>
<content:encoded><![CDATA[
arXiv:2406.15513v3 Announce Type: replace-cross 
Abstract: In this study, we introduce the safety human preference dataset, PKU-SafeRLHF, designed to promote research on safety alignment in large language models (LLMs). As a sibling project to SafeRLHF and BeaverTails, we separate annotations of helpfulness and harmlessness for question-answering pairs, providing distinct perspectives on these coupled attributes. Overall, we provide 44.6k refined prompts and 265k question-answer pairs with safety meta-labels for 19 harm categories and three severity levels ranging from minor to severe, with answers generated by Llama-family models. Based on this, we collected 166.8k preference data, including dual-preference (helpfulness and harmlessness decoupled) and single-preference data (trade-off the helpfulness and harmlessness from scratch), respectively. Using the large-scale annotation data, we further train severity-sensitive moderation for the risk control of LLMs and safety-centric RLHF algorithms for the safety alignment of LLMs. We believe this dataset will be a valuable resource for the community, aiding in the safe deployment of LLMs. Data is available at https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating LLM Ethics: Advancements, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2406.18841</link>
<guid>https://arxiv.org/abs/2406.18841</guid>
<content:encoded><![CDATA[
arXiv:2406.18841v5 Announce Type: replace-cross 
Abstract: This study addresses ethical issues surrounding Large Language Models (LLMs) within the field of artificial intelligence. It explores the common ethical challenges posed by both LLMs and other AI systems, such as privacy and fairness, as well as ethical challenges uniquely arising from LLMs. It highlights challenges such as hallucination, verifiable accountability, and decoding censorship complexity, which are unique to LLMs and distinct from those encountered in traditional AI systems. The study underscores the need to tackle these complexities to ensure accountability, reduce biases, and enhance transparency in the influential role that LLMs play in shaping information dissemination. It proposes mitigation strategies and future directions for LLM ethics, advocating for interdisciplinary collaboration. It recommends ethical frameworks tailored to specific domains and dynamic auditing systems adapted to diverse contexts. This roadmap aims to guide responsible development and integration of LLMs, envisioning a future where ethical considerations govern AI advancements in society.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMILE: Speech Meta In-Context Learning for Low-Resource Language Automatic Speech Recognition</title>
<link>https://arxiv.org/abs/2409.10429</link>
<guid>https://arxiv.org/abs/2409.10429</guid>
<content:encoded><![CDATA[
arXiv:2409.10429v2 Announce Type: replace-cross 
Abstract: Automatic Speech Recognition (ASR) models demonstrate outstanding performance on high-resource languages but face significant challenges when applied to low-resource languages due to limited training data and insufficient cross-lingual generalization. Existing adaptation strategies, such as shallow fusion, data augmentation, and direct fine-tuning, either rely on external resources, suffer computational inefficiencies, or fail in test-time adaptation scenarios. To address these limitations, we introduce Speech Meta In-Context LEarning (SMILE), an innovative framework that combines meta-learning with speech in-context learning (SICL). SMILE leverages meta-training from high-resource languages to enable robust, few-shot generalization to low-resource languages without explicit fine-tuning on the target domain. Extensive experiments on the ML-SUPERB benchmark show that SMILE consistently outperforms baseline methods, significantly reducing character and word error rates in training-free few-shot multilingual ASR tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RATIONALYST: Mining Implicit Rationales for Process Supervision of Reasoning</title>
<link>https://arxiv.org/abs/2410.01044</link>
<guid>https://arxiv.org/abs/2410.01044</guid>
<content:encoded><![CDATA[
arXiv:2410.01044v2 Announce Type: replace-cross 
Abstract: The reasoning steps generated by LLMs might be incomplete, as they mimic logical leaps common in everyday communication found in their pre-training data: underlying rationales are frequently left implicit (unstated). To address this challenge, we introduce RATIONALYST, a model for process-supervision of reasoning based on pre-training on a vast collection of rationale annotations extracted from unlabeled data. We extract 79k rationales from web-scale unlabelled dataset (the Pile) and a combination of reasoning datasets with minimal human intervention. This web-scale pre-training for reasoning allows RATIONALYST to consistently generalize across diverse reasoning tasks, including mathematical, commonsense, scientific, and logical reasoning. Fine-tuned from LLaMa-3-8B, RATIONALYST improves the accuracy of reasoning by an average of 3.9% on 7 representative reasoning benchmarks. It also demonstrates superior performance compared to significantly larger verifiers like GPT-4 and similarly sized models fine-tuned on matching training sets.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Much Can We Forget about Data Contamination?</title>
<link>https://arxiv.org/abs/2410.03249</link>
<guid>https://arxiv.org/abs/2410.03249</guid>
<content:encoded><![CDATA[
arXiv:2410.03249v4 Announce Type: replace-cross 
Abstract: The leakage of benchmark data into the training data has emerged as a significant challenge for evaluating the capabilities of large language models (LLMs). In this work, we challenge the common assumption that small-scale contamination renders benchmark evaluations invalid. First, we experimentally quantify the magnitude of benchmark overfitting based on scaling along three dimensions: The number of model parameters (up to 1.6B), the number of times an example is seen (up to 144), and the number of training tokens (up to 40B). If model and data follow the Chinchilla scaling laws, minor contamination indeed leads to overfitting. At the same time, even 144 times of contamination can be forgotten if the training data is scaled beyond five times Chinchilla, a regime characteristic of many modern LLMs. Continual pre-training of OLMo-7B corroborates these results. Next, we study the impact of the weight decay parameter on example forgetting, showing that empirical forgetting occurs faster than the cumulative weight decay. This allows us to gauge the degree of example forgetting in large-scale training runs, indicating that many LLMs, including Lllama 3 405B, have forgotten the data seen at the beginning of training.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building, Reusing, and Generalizing Abstract Representations from Concrete Sequences</title>
<link>https://arxiv.org/abs/2410.21332</link>
<guid>https://arxiv.org/abs/2410.21332</guid>
<content:encoded><![CDATA[
arXiv:2410.21332v2 Announce Type: replace-cross 
Abstract: Humans excel at learning abstract patterns across different sequences, filtering out irrelevant details, and transferring these generalized concepts to new sequences. In contrast, many sequence learning models lack the ability to abstract, which leads to memory inefficiency and poor transfer. We introduce a non-parametric hierarchical variable learning model (HVM) that learns chunks from sequences and abstracts contextually similar chunks as variables. HVM efficiently organizes memory while uncovering abstractions, leading to compact sequence representations. When learning on language datasets such as babyLM, HVM learns a more efficient dictionary than standard compression algorithms such as Lempel-Ziv. In a sequence recall task requiring the acquisition and transfer of variables embedded in sequences, we demonstrate HVM's sequence likelihood correlates with human recall times. In contrast, large language models (LLMs) struggle to transfer abstract variables as effectively as humans. From HVM's adjustable layer of abstraction, we demonstrate that the model realizes a precise trade-off between compression and generalization. Our work offers a cognitive model that captures the learning and transfer of abstract representations in human cognition and differentiates itself from LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse</title>
<link>https://arxiv.org/abs/2410.21333</link>
<guid>https://arxiv.org/abs/2410.21333</guid>
<content:encoded><![CDATA[
arXiv:2410.21333v4 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) prompting has become a widely used strategy for improving large language and multimodal model performance. However, it is still an open question under which settings CoT systematically reduces performance. In this paper, we seek to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, focusing on six representative tasks from the psychological literature where deliberation hurts performance in humans. In three of these tasks, state-of-the-art models exhibit significant performance drop-offs with CoT (up to 36.3\% absolute accuracy for OpenAI o1-preview compared to GPT-4o), while in others, CoT effects are mixed, with positive, neutral, and negative changes. While models and humans do not exhibit perfectly parallel cognitive processes, considering cases where thinking has negative consequences for humans helps identify settings where it negatively impacts models. By connecting the literature on human verbal thinking and deliberation with evaluations of CoT, we offer a perspective for understanding the impact of inference-time reasoning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regular-pattern-sensitive CRFs for Distant Label Interactions</title>
<link>https://arxiv.org/abs/2411.12484</link>
<guid>https://arxiv.org/abs/2411.12484</guid>
<content:encoded><![CDATA[
arXiv:2411.12484v2 Announce Type: replace-cross 
Abstract: While LLMs have grown popular in sequence labeling, linear-chain conditional random fields (CRFs) remain a popular alternative with the ability to directly model interactions between labels. However, the Markov assumption limits them to % only directly modeling interactions between adjacent labels. Weighted finite-state transducers (FSTs), in contrast, can model distant label--label interactions, but exact label inference is intractable in general. In this work, we present regular-pattern-sensitive CRFs (RPCRFs), a method of enriching standard linear-chain CRFs with the ability to learn long-distance label interactions through user-specified patterns. This approach allows users to write regular-expression label patterns concisely specifying which types of interactions the model should take into account, allowing the model to learn from data whether and in which contexts these patterns occur. The result can be interpreted alternatively as a CRF augmented with additional, non-local potentials, or as a finite-state transducer whose structure is defined by a set of easily-interpretable patterns. Critically, exact training and inference are tractable for many pattern sets. We detail how an RPCRF can be automatically constructed from a set of user-specified patterns, and demonstrate the model's effectiveness on a sequence of three synthetic sequence modeling datasets.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MORTAR: Multi-turn Metamorphic Testing for LLM-based Dialogue Systems</title>
<link>https://arxiv.org/abs/2412.15557</link>
<guid>https://arxiv.org/abs/2412.15557</guid>
<content:encoded><![CDATA[
arXiv:2412.15557v2 Announce Type: replace-cross 
Abstract: With the widespread application of LLM-based dialogue systems in daily life, quality assurance has become more important than ever. Recent research has successfully introduced methods to identify unexpected behaviour in single-turn testing scenarios. However, multi-turn interaction is the common real-world usage of dialogue systems, yet testing methods for such interactions remain underexplored. This is largely due to the oracle problem in multi-turn testing, which continues to pose a significant challenge for dialogue system developers and researchers. In this paper, we propose MORTAR, a metamorphic multi-turn dialogue testing approach, which mitigates the test oracle problem in testing LLM-based dialogue systems. MORTAR formalises the multi-turn testing for dialogue systems, and automates the generation of question-answer dialogue test cases with multiple dialogue-level perturbations and metamorphic relations (MRs). The automated perturbation-MR matching mechanism allows MORTAR more flexibility and efficiency in metamorphic testing. The proposed approach is fully automated without reliance on potentially biased LLMs as test oracles. In testing six popular LLM-based dialogue systems, MORTAR reaches significantly better effectiveness with over 150\% more bugs revealed per test case when compared to the single-turn metamorphic testing baseline. On the quality of bugs, MORTAR reveals higher-quality bugs in terms of diversity, precision and uniqueness. MORTAR is expected to inspire more multi-turn testing approaches without LLM judges, and assist developers to evaluate the dialogue system performance more comprehensively with constrained test resources and budget.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Specialized Visual Encoders for Video Language Models</title>
<link>https://arxiv.org/abs/2501.01426</link>
<guid>https://arxiv.org/abs/2501.01426</guid>
<content:encoded><![CDATA[
arXiv:2501.01426v2 Announce Type: replace-cross 
Abstract: The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on a single vision encoder for all of their visual processing, which limits the amount and type of visual information that can be conveyed to the LLM. Our method, MERV, Multi-Encoder Representation of Videos, instead leverages multiple frozen visual encoders to create a unified representation of a video, providing the VideoLLM with a comprehensive set of specialized visual knowledge. Spatio-temporally aligning the features from each encoder allows us to tackle a wider range of open-ended and multiple-choice video understanding questions and outperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy than Video-LLaVA across the standard suite video understanding benchmarks, while also having a better Video-ChatGPT score. We also improve upon SeViLA, the previous best on zero-shot Perception Test accuracy, by 2.2%. MERV introduces minimal extra parameters and trains faster than equivalent single-encoder methods while parallelizing the visual processing. Finally, we provide qualitative evidence that MERV successfully captures domain knowledge from each of its encoders. Our results offer promising directions in utilizing multiple vision encoders for comprehensive video understanding.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Layer by Layer: Uncovering Hidden Representations in Language Models</title>
<link>https://arxiv.org/abs/2502.02013</link>
<guid>https://arxiv.org/abs/2502.02013</guid>
<content:encoded><![CDATA[
arXiv:2502.02013v2 Announce Type: replace-cross 
Abstract: From extracting features to generating text, the outputs of large language models (LLMs) typically rely on the final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that intermediate layers can encode even richer representations, often improving performance on a range of downstream tasks. To explain and quantify these hidden-layer properties, we propose a unified framework of representation quality metrics based on information theory, geometry, and invariance to input perturbations. Our framework highlights how each layer balances information compression and signal preservation, revealing why mid-depth embeddings can exceed the last layer's performance. Through extensive experiments on 32 text-embedding tasks across various architectures (transformers, state-space models) and domains (language, vision), we demonstrate that intermediate layers consistently provide stronger features, challenging the standard view on final-layer embeddings and opening new directions on using mid-layer representations for more robust and accurate representations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws for Upcycling Mixture-of-Experts Language Models</title>
<link>https://arxiv.org/abs/2502.03009</link>
<guid>https://arxiv.org/abs/2502.03009</guid>
<content:encoded><![CDATA[
arXiv:2502.03009v2 Announce Type: replace-cross 
Abstract: Pretraining large language models (LLMs) is resource-intensive, often requiring months of training time even with high-end GPU clusters. There are two approaches of mitigating such computational demands: reusing smaller models to train larger ones (upcycling), and training computationally efficient models like mixture-of-experts (MoE). In this paper, we study the upcycling of LLMs to MoE models, of which the scaling behavior remains underexplored. Through extensive experiments, we identify empirical scaling laws that describe how performance depends on dataset size and model configuration. Particularly, we show that, while scaling these factors improves performance, there is a novel interaction term between the dense and upcycled training dataset that limits the efficiency of upcycling at large computational budgets. Based on these findings, we provide guidance to scale upcycling, and establish conditions under which upcycling outperforms from-scratch trainings within budget constraints.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language Model Training</title>
<link>https://arxiv.org/abs/2502.03460</link>
<guid>https://arxiv.org/abs/2502.03460</guid>
<content:encoded><![CDATA[
arXiv:2502.03460v2 Announce Type: replace-cross 
Abstract: Small language models (SLMs) have attracted considerable attention from both academia and industry due to their broad range of applications in edge devices. To obtain SLMs with strong performance, conventional approaches either pre-train the models from scratch, which incurs substantial computational costs, or compress/prune existing large language models (LLMs), which results in performance drops and falls short in comparison to pre-training. In this paper, we investigate the family of acceleration methods that involve both structured pruning and model training. We found 1) layer-wise adaptive pruning (Adapt-Pruner) is extremely effective in LLMs and yields significant improvements over existing pruning techniques, 2) adaptive pruning equipped with further training leads to models comparable to those pre-training from scratch, 3) incremental pruning brings non-trivial performance gain by interleaving pruning with training and only removing a small portion of neurons ($\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that Adapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner, FLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense benchmarks. Additionally, Adapt-Pruner restores the performance of MobileLLM-125M to 600M on the MMLU benchmark with 200$\times$ fewer tokens via pruning from its larger counterparts, and discovers a new 1B model that surpasses LLaMA-3.2-1B in multiple benchmarks. The official code is released at https://github.com/research4pan/AdaptPruner.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions</title>
<link>https://arxiv.org/abs/2502.04322</link>
<guid>https://arxiv.org/abs/2502.04322</guid>
<content:encoded><![CDATA[
arXiv:2502.04322v2 Announce Type: replace-cross 
Abstract: Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Temperature for Language Models with Multi-Sample Inference</title>
<link>https://arxiv.org/abs/2502.05234</link>
<guid>https://arxiv.org/abs/2502.05234</guid>
<content:encoded><![CDATA[
arXiv:2502.05234v2 Announce Type: replace-cross 
Abstract: Multi-sample aggregation strategies, such as majority voting and best-of-N sampling, are widely used in contemporary large language models (LLMs) to enhance predictive accuracy across various tasks. A key challenge in this process is temperature selection, which significantly impacts model performance. Existing approaches either rely on a fixed default temperature or require labeled validation data for tuning, which are often scarce and difficult to obtain. This paper addresses the challenge of automatically identifying the (near)-optimal temperature for different LLMs using multi-sample aggregation strategies, without relying on task-specific validation data. We provide a comprehensive analysis of temperature's role in performance optimization, considering variations in model architectures, datasets, task types, model sizes, and predictive accuracy. Furthermore, we propose a novel entropy-based metric for automated temperature optimization, which consistently outperforms fixed-temperature baselines. Additionally, we incorporate a stochastic process model to enhance interpretability, offering deeper insights into the relationship between temperature and model performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification</title>
<link>https://arxiv.org/abs/2502.07299</link>
<guid>https://arxiv.org/abs/2502.07299</guid>
<content:encoded><![CDATA[
arXiv:2502.07299v2 Announce Type: replace-cross 
Abstract: The interactions between DNA, RNA, and proteins are fundamental to biological processes, as illustrated by the central dogma of molecular biology. Although modern biological pre-trained models have achieved great success in analyzing these macromolecules individually, their interconnected nature remains underexplored. This paper follows the guidance of the central dogma to redesign both the data and model pipeline and offers a comprehensive framework, Life-Code, that spans different biological functions. As for data flow, we propose a unified pipeline to integrate multi-omics data by reverse-transcribing RNA and reverse-translating amino acids into nucleotide-based sequences. As for the model, we design a codon tokenizer and a hybrid long-sequence architecture to encode the interactions between coding and non-coding regions through masked modeling pre-training. To model the translation and folding process with coding sequences, Life-Code learns protein structures of the corresponding amino acids by knowledge distillation from off-the-shelf protein language models. Such designs enable Life-Code to capture complex interactions within genetic sequences, providing a more comprehensive understanding of multi-omics with the central dogma. Extensive experiments show that Life-Code achieves state-of-the-art results on various tasks across three omics, highlighting its potential for advancing multi-omics analysis and interpretation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HARBOR: Exploring Persona Dynamics in Multi-Agent Competition</title>
<link>https://arxiv.org/abs/2502.12149</link>
<guid>https://arxiv.org/abs/2502.12149</guid>
<content:encoded><![CDATA[
arXiv:2502.12149v2 Announce Type: replace-cross 
Abstract: We investigate factors contributing to LLM agents' success in competitive multi-agent environments, using auctions as a testbed where agents bid to maximize profit. The agents are equipped with bidding domain knowledge, distinct personas that reflect item preferences, and a memory of auction history. Our work extends the classic auction scenario by creating a realistic environment where multiple agents bid on houses, weighing aspects such as size, location, and budget to secure the most desirable homes at the lowest prices. Particularly, we investigate three key questions: (a) How does a persona influence an agent's behavior in a competitive setting? (b) Can an agent effectively profile its competitors' behavior during auctions? (c) How can persona profiling be leveraged to create an advantage using strategies such as theory of mind? Through a series of experiments, we analyze the behaviors of LLM agents and shed light on new findings. Our testbed, called HARBOR, offers a valuable platform for deepening our understanding of multi-agent workflows in competitive environments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Improving LLM Alignment via Preference Data Selection</title>
<link>https://arxiv.org/abs/2502.14560</link>
<guid>https://arxiv.org/abs/2502.14560</guid>
<content:encoded><![CDATA[
arXiv:2502.14560v3 Announce Type: replace-cross 
Abstract: Direct Preference Optimization (DPO) has emerged as a promising approach for aligning large language models with human preferences. While prior work mainly extends DPO from the aspect of the objective function, we instead improve DPO from the largely overlooked but critical aspect of data selection. Specifically, we address the issue of parameter shrinkage caused by noisy data by proposing a novel margin-maximization principle for dataset curation in DPO training. To further mitigate the noise in different reward models, we propose a Bayesian Aggregation approach that unifies multiple margin sources (external and implicit) into a single preference probability. Extensive experiments in diverse settings demonstrate the consistently high data efficiency of our approach. Remarkably, by using just 10\% of the Ultrafeedback dataset, our approach achieves 3\% to 8\% improvements across various Llama, Mistral, and Qwen models on the AlpacaEval2 benchmark. Furthermore, our approach seamlessly extends to iterative DPO, yielding a roughly 3\% improvement with 25\% online data, revealing the high redundancy in this presumed high-quality data construction manner. These results highlight the potential of data selection strategies for advancing preference optimization.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Euler to AI: Unifying Formulas for Mathematical Constants</title>
<link>https://arxiv.org/abs/2502.17533</link>
<guid>https://arxiv.org/abs/2502.17533</guid>
<content:encoded><![CDATA[
arXiv:2502.17533v2 Announce Type: replace-cross 
Abstract: The constant $\pi$ has fascinated scholars throughout the centuries, inspiring numerous formulas for its evaluation, such as infinite sums and continued fractions. Despite their individual significance, many of the underlying connections among formulas remain unknown, missing unifying theories that could unveil deeper understanding. The absence of a unifying theory reflects a broader challenge across math and science: knowledge is typically accumulated through isolated discoveries, while deeper connections often remain hidden. In this work, we present an automated framework for the unification of mathematical formulas. Our system combines large language models (LLMs) for systematic formula harvesting, an LLM-code feedback loop for validation, and a novel symbolic algorithm for clustering and eventual unification. We demonstrate this methodology on the hallmark case of $\pi$, an ideal testing ground for symbolic unification. Applying this approach to 455,050 arXiv papers, we validate 407 distinct formulas for $\pi$ and prove relations between 381 (94%) of them, of which 188 (46%) can be derived from a single mathematical object$\unicode{x2014}$linking canonical formulas by Euler, Gauss, Brouncker, and newer ones from algorithmic discoveries by the Ramanujan Machine. Our method generalizes to other constants, including $e$, $\zeta(3)$, and Catalan's constant, demonstrating the potential of AI-assisted mathematics to uncover hidden structures and unify knowledge across domains.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watch Out Your Album! On the Inadvertent Privacy Memorization in Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2503.01208</link>
<guid>https://arxiv.org/abs/2503.01208</guid>
<content:encoded><![CDATA[
arXiv:2503.01208v2 Announce Type: replace-cross 
Abstract: Multi-Modal Large Language Models (MLLMs) have exhibited remarkable performance on various vision-language tasks such as Visual Question Answering (VQA). Despite accumulating evidence of privacy concerns associated with task-relevant content, it remains unclear whether MLLMs inadvertently memorize private content that is entirely irrelevant to the training tasks. In this paper, we investigate how randomly generated task-irrelevant private content can become spuriously correlated with downstream objectives due to partial mini-batch training dynamics, thus causing inadvertent memorization. Concretely, we randomly generate task-irrelevant watermarks into VQA fine-tuning images at varying probabilities and propose a novel probing framework to determine whether MLLMs have inadvertently encoded such content. Our experiments reveal that MLLMs exhibit notably different training behaviors in partial mini-batch settings with task-irrelevant watermarks embedded. Furthermore, through layer-wise probing, we demonstrate that MLLMs trigger distinct representational patterns when encountering previously seen task-irrelevant knowledge, even if this knowledge does not influence their output during prompting. Our code is available at https://github.com/illusionhi/ProbingPrivacy.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What do Large Language Models Say About Animals? Investigating Risks of Animal Harm in Generated Text</title>
<link>https://arxiv.org/abs/2503.04804</link>
<guid>https://arxiv.org/abs/2503.04804</guid>
<content:encoded><![CDATA[
arXiv:2503.04804v3 Announce Type: replace-cross 
Abstract: As machine learning systems become increasingly embedded in society, their impact on human and nonhuman life continues to escalate. Technical evaluations have addressed a variety of potential harms from large language models (LLMs) towards humans and the environment, but there is little empirical work regarding harms towards nonhuman animals. Following the growing recognition of animal protection in regulatory and ethical AI frameworks, we present AnimalHarmBench (AHB), a benchmark for risks of animal harm in LLM-generated text. Our benchmark dataset comprises 1,850 curated questions from Reddit post titles and 2,500 synthetic questions based on 50 animal categories (e.g., cats, reptiles) and 50 ethical scenarios with a 70-30 public-private split. Scenarios include open-ended questions about how to treat animals, practical scenarios with potential animal harm, and willingness-to-pay measures for the prevention of animal harm. Using the LLM-as-a-judge framework, responses are evaluated for their potential to increase or decrease harm, and evaluations are debiased for the tendency of judges to judge their own outputs more favorably. AHB reveals significant differences across frontier LLMs, animal categories, scenarios, and subreddits. We conclude with future directions for technical research and addressing the challenges of building evaluations on complex social and moral topics.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compute Optimal Scaling of Skills: Knowledge vs Reasoning</title>
<link>https://arxiv.org/abs/2503.10061</link>
<guid>https://arxiv.org/abs/2503.10061</guid>
<content:encoded><![CDATA[
arXiv:2503.10061v3 Announce Type: replace-cross 
Abstract: Scaling laws are a critical component of the LLM development pipeline, most famously as a way to forecast training decisions such as 'compute-optimally' trading-off parameter count and dataset size, alongside a more recent growing list of other crucial decisions. In this work, we ask whether compute-optimal scaling behaviour can be skill-dependent. In particular, we examine knowledge and reasoning-based skills such as knowledge-based QA and code generation, and we answer this question in the affirmative: scaling laws are skill-dependent. Next, to understand whether skill-dependent scaling is an artefact of the pretraining datamix, we conduct an extensive ablation of different datamixes and find that, also when correcting for datamix differences, knowledge and code exhibit fundamental differences in scaling behaviour. We conclude with an analysis of how our findings relate to standard compute-optimal scaling using a validation set, and find that a misspecified validation set can impact compute-optimal parameter count by nearly 50%, depending on its skill composition.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers without Normalization</title>
<link>https://arxiv.org/abs/2503.10622</link>
<guid>https://arxiv.org/abs/2503.10622</guid>
<content:encoded><![CDATA[
arXiv:2503.10622v2 Announce Type: replace-cross 
Abstract: Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation $DyT($x$) = \tanh(\alpha $x$)$, as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, $S$-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions</title>
<link>https://arxiv.org/abs/2503.20290</link>
<guid>https://arxiv.org/abs/2503.20290</guid>
<content:encoded><![CDATA[
arXiv:2503.20290v3 Announce Type: replace-cross 
Abstract: This paper explores a novel perspective to speech quality assessment by leveraging natural language descriptions, offering richer, more nuanced insights than traditional numerical scoring methods. Natural language feedback provides instructive recommendations and detailed evaluations, yet existing datasets lack the comprehensive annotations needed for this approach. To bridge this gap, we introduce QualiSpeech, a comprehensive low-level speech quality assessment dataset encompassing 11 key aspects and detailed natural language comments that include reasoning and contextual insights. Additionally, we propose the QualiSpeech Benchmark to evaluate the low-level speech understanding capabilities of auditory large language models (LLMs). Experimental results demonstrate that finetuned auditory LLMs can reliably generate detailed descriptions of noise and distortion, effectively identifying their types and temporal characteristics. The results further highlight the potential for incorporating reasoning to enhance the accuracy and reliability of quality assessments. The dataset will be released at https://huggingface.co/datasets/tsinghua-ee/QualiSpeech.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affordable AI Assistants with Knowledge Graph of Thoughts</title>
<link>https://arxiv.org/abs/2504.02670</link>
<guid>https://arxiv.org/abs/2504.02670</guid>
<content:encoded><![CDATA[
arXiv:2504.02670v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are revolutionizing the development of AI assistants capable of performing diverse tasks across domains. However, current state-of-the-art LLM-driven agents face significant challenges, including high operational costs and limited success rates on complex benchmarks like GAIA. To address these issues, we propose Knowledge Graph of Thoughts (KGoT), an innovative AI assistant architecture that integrates LLM reasoning with dynamically constructed knowledge graphs (KGs). KGoT extracts and structures task-relevant knowledge into a dynamic KG representation, iteratively enhanced through external tools such as math solvers, web crawlers, and Python scripts. Such structured representation of task-relevant knowledge enables low-cost models to solve complex tasks effectively while also minimizing bias and noise. For example, KGoT achieves a 29% improvement in task success rates on the GAIA benchmark compared to Hugging Face Agents with GPT-4o mini. Moreover, harnessing a smaller model dramatically reduces operational costs by over 36x compared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and Deepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a scalable, affordable, versatile, and high-performing solution for AI assistants.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Synthesizing Data for Context Attribution in Question Answering</title>
<link>https://arxiv.org/abs/2504.05317</link>
<guid>https://arxiv.org/abs/2504.05317</guid>
<content:encoded><![CDATA[
arXiv:2504.05317v2 Announce Type: replace-cross 
Abstract: Question Answering (QA) accounts for a significant portion of LLM usage "in the wild". However, LLMs sometimes produce false or misleading responses, also known as "hallucinations". Therefore, grounding the generated answers in contextually provided information -- i.e., providing evidence for the generated text -- is paramount for LLMs' trustworthiness. Providing this information is the task of context attribution. In this paper, we systematically study LLM-based approaches for this task, namely we investigate (i) zero-shot inference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic data generated by larger LLMs. Our key contribution is SynQA: a novel generative strategy for synthesizing context attribution data. Given selected context sentences, an LLM generates QA pairs that are supported by these sentences. This leverages LLMs' natural strengths in text generation while ensuring clear attribution paths in the synthetic training data. We show that the attribution data synthesized via SynQA is highly effective for fine-tuning small LMs for context attribution in different QA tasks and domains. Finally, with a user study, we validate the usefulness of small LMs (fine-tuned on synthetic data from SynQA) in context attribution for QA.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture</title>
<link>https://arxiv.org/abs/2504.10512</link>
<guid>https://arxiv.org/abs/2504.10512</guid>
<content:encoded><![CDATA[
arXiv:2504.10512v2 Announce Type: replace-cross 
Abstract: Language representation learning has emerged as a promising approach for sequential recommendation, thanks to its ability to learn generalizable representations. However, despite its advantages, this approach still struggles with data sparsity and a limited understanding of common-sense user preferences. To address these limitations, we propose $\textbf{JEPA4Rec}$, a framework that combines $\textbf{J}$oint $\textbf{E}$mbedding $\textbf{P}$redictive $\textbf{A}$rchitecture with language modeling of item textual descriptions. JEPA4Rec captures semantically rich and transferable representations, improving recommendation performance and reducing reliance on large-scale pre-training data. Specifically, JEPA4Rec represents items as text sentences by flattening descriptive information such as $\textit{title, category}$, and other attributes. To encode these sentences, we employ a bidirectional Transformer encoder with modified embedding layers tailored for capturing item information in recommendation datasets. We apply masking to text sentences and use them to predict the representations of the unmasked sentences, helping the model learn generalizable item embeddings. To further improve recommendation performance and language understanding, we employ a two-stage training strategy incorporating self-supervised learning losses. Experiments on six real-world datasets demonstrate that JEPA4Rec consistently outperforms state-of-the-art methods, particularly in cross-domain, cross-platform, and low-resource scenarios.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision</title>
<link>https://arxiv.org/abs/2505.14999</link>
<guid>https://arxiv.org/abs/2505.14999</guid>
<content:encoded><![CDATA[
arXiv:2505.14999v2 Announce Type: replace-cross 
Abstract: Mathematical reasoning presents a significant challenge for Large Language Models (LLMs), often requiring robust multi step logical consistency. While Chain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee correctness, and improving reliability via extensive sampling is computationally costly. This paper introduces the Energy Outcome Reward Model (EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy Based Models (EBMs) to simplify the training of reward models by learning to assign a scalar energy score to CoT solutions using only outcome labels, thereby avoiding detailed annotations. It achieves this by interpreting discriminator output logits as negative energies, effectively ranking candidates where lower energy is assigned to solutions leading to correct final outcomes implicitly favoring coherent reasoning. On mathematical benchmarks (GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with Llama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively leverages a given pool of candidate solutions to match or exceed the performance of brute force sampling, thereby enhancing LLM reasoning outcome reliability through its streamlined post hoc verification process.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation</title>
<link>https://arxiv.org/abs/2505.21549</link>
<guid>https://arxiv.org/abs/2505.21549</guid>
<content:encoded><![CDATA[
arXiv:2505.21549v4 Announce Type: replace-cross 
Abstract: We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that enhances multimodal image-text retrieval while preserving the original model's strong zero-shot classification capabilities. CLIP models are typically constrained by fixed image resolutions and limited context, which can hinder their effectiveness in retrieval tasks that require fine-grained cross-modal understanding. DCLIP addresses these challenges through a meta teacher-student distillation framework, where a cross-modal transformer teacher is fine-tuned to produce enriched embeddings via bidirectional cross-attention between YOLO-extracted image regions and corresponding textual spans. These semantically and spatially aligned global representations guide the training of a lightweight student model using a hybrid loss that combines contrastive learning and cosine similarity objectives. Despite being trained on only ~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a fraction of CLIP's original dataset-DCLIP significantly improves image-text retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's zero-shot classification performance. These results demonstrate that DCLIP effectively mitigates the trade-off between task specialization and generalization, offering a resource-efficient, domain-adaptive, and detail-sensitive solution for advanced vision-language tasks. Code available at https://anonymous.4open.science/r/DCLIP-B772/README.md.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cartridges: Lightweight and general-purpose long context representations via self-study</title>
<link>https://arxiv.org/abs/2506.06266</link>
<guid>https://arxiv.org/abs/2506.06266</guid>
<content:encoded><![CDATA[
<div> Large language models, Cartridge, self-study, in-context learning (ICL), memory consumption<br />
Summary:<br />
- Large language models are commonly used to answer queries based on large text corpuses using in-context learning (ICL).<br />
- The memory consumption of the key-value (KV) cache in current models can be reduced by training a smaller KV cache, called a Cartridge, offline on each corpus.<br />
- Training a Cartridge with next-token prediction is not as effective as ICL, so a new approach called self-study, which involves training with a context-distillation objective using synthetic conversations, is proposed.<br />
- Cartridges trained with self-study achieve comparable performance to ICL, but are much more cost-effective to serve.<br />
- Self-study extends the effective context length of the model and allows for composability of Cartridges at inference time without the need for retraining. <br /> <div>
arXiv:2506.06266v3 Announce Type: replace 
Abstract: Large language models are often used to answer queries grounded in large text corpora (e.g. codebases, legal documents, or chat histories) by placing the entire corpus in the context window and leveraging in-context learning (ICL). Although current models support contexts of 100K-1M tokens, this setup is costly to serve because the memory consumption of the KV cache scales with input length. We explore an alternative: training a smaller KV cache offline on each corpus. At inference time, we load this trained KV cache, which we call a Cartridge, and decode a response. Critically, the cost of training a Cartridge can be amortized across all the queries referencing the same corpus. However, we find that the naive approach of training the Cartridge with next-token prediction on the corpus is not competitive with ICL. Instead, we propose self-study, a training recipe in which we generate synthetic conversations about the corpus and train the Cartridge with a context-distillation objective. We find that Cartridges trained with self-study replicate the functionality of ICL, while being significantly cheaper to serve. On challenging long-context benchmarks, Cartridges trained with self-study match ICL performance while using 38.6x less memory and enabling 26.4x higher throughput. Self-study also extends the model's effective context length (e.g. from 128k to 484k tokens on MTOB) and surprisingly, leads to Cartridges that can be composed at inference time without retraining.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeleEval-OS: Performance evaluations of large language models for operations scheduling</title>
<link>https://arxiv.org/abs/2506.11017</link>
<guid>https://arxiv.org/abs/2506.11017</guid>
<content:encoded><![CDATA[
<div> Evaluation Benchmark, Large Language Models, Telecommunications Operation Scheduling, Performance Assessment, Zero-shot/Few-shot Evaluation 

Summary: 
The article discusses the development of a new Telecommunications Operation Scheduling Evaluation Benchmark (TeleEval-OS) to assess the performance of Large Language Models (LLMs) in the telecommunications industry. The benchmark includes 15 datasets covering various operational tasks and stages. LLM capabilities in scheduling tasks are categorized into four levels of complexity. Open-source LLMs, such as DeepSeek-V3, are compared against closed-source LLMs like GPT-4o using zero-shot and few-shot evaluation methods. Results indicate that open-source LLMs excel in specific scenarios, showcasing their potential in telecommunications operation scheduling. This study addresses the lack of comprehensive evaluation benchmarks and highlights the value of LLMs in optimizing production scheduling and service control within the telecommunications sector. 

<br /><br />Summary: <div>
arXiv:2506.11017v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has significantly propelled progress in artificial intelligence, demonstrating substantial application potential across multiple specialized domains. Telecommunications operation scheduling (OS) is a critical aspect of the telecommunications industry, involving the coordinated management of networks, services, risks, and human resources to optimize production scheduling and ensure unified service control. However, the inherent complexity and domain-specific nature of OS tasks, coupled with the absence of comprehensive evaluation benchmarks, have hindered thorough exploration of LLMs' application potential in this critical field. To address this research gap, we propose the first Telecommunications Operation Scheduling Evaluation Benchmark (TeleEval-OS). Specifically, this benchmark comprises 15 datasets across 13 subtasks, comprehensively simulating four key operational stages: intelligent ticket creation, intelligent ticket handling, intelligent ticket closure, and intelligent evaluation. To systematically assess the performance of LLMs on tasks of varying complexity, we categorize their capabilities in telecommunications operation scheduling into four hierarchical levels, arranged in ascending order of difficulty: basic NLP, knowledge Q&amp;A, report generation, and report analysis. On TeleEval-OS, we leverage zero-shot and few-shot evaluation methods to comprehensively assess 10 open-source LLMs (e.g., DeepSeek-V3) and 4 closed-source LLMs (e.g., GPT-4o) across diverse scenarios. Experimental results demonstrate that open-source LLMs can outperform closed-source LLMs in specific scenarios, highlighting their significant potential and value in the field of telecommunications operation scheduling.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2506.11063</link>
<guid>https://arxiv.org/abs/2506.11063</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented generation, multimodal, position bias, robustness, debiasing<br />
Summary: <br />
- Multimodal Retrieval-Augmented Generation (RAG) systems are critical for knowledge-intensive tasks, but current models are sensitive to the order of evidence presentation, leading to performance instability and biased reasoning. 
- A study on position bias in multimodal RAG systems shows a U-shaped accuracy curve based on evidence position, with increased bias in multimodal compared to unimodal settings. 
- The Position Sensitivity Index (PSI_p) is introduced to quantify bias, and a visualization framework tracks attention allocation patterns across decoder layers. 
- Multimodal interactions intensify position bias, which logarithmically increases with retrieval range. 
- The findings emphasize the importance of evidence reordering or debiasing strategies to improve the reliability and equity of generation systems. <br /> <div>
arXiv:2506.11063v1 Announce Type: new 
Abstract: Multimodal Retrieval-Augmented Generation (RAG) systems have become essential in knowledge-intensive and open-domain tasks. As retrieval complexity increases, ensuring the robustness of these systems is critical. However, current RAG models are highly sensitive to the order in which evidence is presented, often resulting in unstable performance and biased reasoning, particularly as the number of retrieved items or modality diversity grows. This raises a central question: How does the position of retrieved evidence affect multimodal RAG performance? To answer this, we present the first comprehensive study of position bias in multimodal RAG systems. Through controlled experiments across text-only, image-only, and mixed-modality tasks, we observe a consistent U-shaped accuracy curve with respect to evidence position. To quantify this bias, we introduce the Position Sensitivity Index ($PSI_p$) and develop a visualization framework to trace attention allocation patterns across decoder layers. Our results reveal that multimodal interactions intensify position bias compared to unimodal settings, and that this bias increases logarithmically with retrieval range. These findings offer both theoretical and empirical foundations for position-aware analysis in RAG, highlighting the need for evidence reordering or debiasing strategies to build more reliable and equitable generation systems.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smotrom tvoja pa ander drogoj verden! Resurrecting Dead Pidgin with Generative Models: Russenorsk Case Study</title>
<link>https://arxiv.org/abs/2506.11065</link>
<guid>https://arxiv.org/abs/2506.11065</guid>
<content:encoded><![CDATA[
<div> Keywords: Russenorsk, pidgin language, lexicon, word formation, grammatical structure <br />
Summary: Russenorsk, a pidgin language used in trade interactions between Russian and Norwegian speakers, is analyzed in this paper using modern language models (LLMs). By constructing a structured dictionary of the language, the study examines synonyms and word origins to propose hypotheses on word formation and grammatical structure in Russenorsk. Comparing the LLM-generated hypotheses with those in existing academic literature, the paper sheds light on the core principles of Russenorsk. Additionally, a "reconstruction" translation agent is developed to generate hypothetical Russenorsk translations of contemporary Russian and Norwegian texts. This study contributes to a deeper understanding of Russenorsk, showcasing the potential of LLMs in analyzing and reconstructing unique linguistic phenomena. <br /><br />Summary: <div>
arXiv:2506.11065v1 Announce Type: new 
Abstract: Russenorsk, a pidgin language historically used in trade interactions between Russian and Norwegian speakers, represents a unique linguistic phenomenon. In this paper, we attempt to analyze its lexicon using modern large language models (LLMs), based on surviving literary sources. We construct a structured dictionary of the language, grouped by synonyms and word origins. Subsequently, we use this dictionary to formulate hypotheses about the core principles of word formation and grammatical structure in Russenorsk and show which hypotheses generated by large language models correspond to the hypotheses previously proposed ones in the academic literature. We also develop a "reconstruction" translation agent that generates hypothetical Russenorsk renderings of contemporary Russian and Norwegian texts.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large Language Model Based Pipeline for Review of Systems Entity Recognition from Clinical Notes</title>
<link>https://arxiv.org/abs/2506.11067</link>
<guid>https://arxiv.org/abs/2506.11067</guid>
<content:encoded><![CDATA[
<div> extraction, Review of Systems, clinical notes, large language model, pipeline 
<br />
Summary: 
A cost-effective pipeline utilizing large language models (LLMs) was developed to automatically extract Review of Systems (ROS) entities from clinical notes. The pipeline, incorporating SecTag and few-shot LLMs such as Mistral, Llama, Gemma, and ChatGPT, successfully identified ROS entity spans, statuses, and associated body systems with low error rates. Integration of ChatGPT resulted in the lowest error rates for entity spans and statuses/systems. The use of open-source LLMs allowed for local deployment, offering a scalable solution to alleviate ROS documentation burden in healthcare settings. The pipeline's performance with open-source LLMs was promising, demonstrating similar error rates to commercial models. This research highlights the potential of open-source LLMs as a feasible alternative for resource-limited healthcare environments. 
<br /> <div>
arXiv:2506.11067v1 Announce Type: new 
Abstract: Objective: Develop a cost-effective, large language model (LLM)-based pipeline for automatically extracting Review of Systems (ROS) entities from clinical notes. Materials and Methods: The pipeline extracts ROS sections using SecTag, followed by few-shot LLMs to identify ROS entity spans, their positive/negative status, and associated body systems. We implemented the pipeline using open-source LLMs (Mistral, Llama, Gemma) and ChatGPT. The evaluation was conducted on 36 general medicine notes containing 341 annotated ROS entities. Results: When integrating ChatGPT, the pipeline achieved the lowest error rates in detecting ROS entity spans and their corresponding statuses/systems (28.2% and 14.5%, respectively). Open-source LLMs enable local, cost-efficient execution of the pipeline while delivering promising performance with similarly low error rates (span: 30.5-36.7%; status/system: 24.3-27.3%). Discussion and Conclusion: Our pipeline offers a scalable and locally deployable solution to reduce ROS documentation burden. Open-source LLMs present a viable alternative to commercial models in resource-limited healthcare environments.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models</title>
<link>https://arxiv.org/abs/2506.11068</link>
<guid>https://arxiv.org/abs/2506.11068</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, moral reasoning, deontological keyword bias, obligations, judgment alignment <br />
Summary: <br /> 
- Large language models (LLMs) are increasingly engaging in moral and ethical reasoning, facing challenges due to unclear judgment criteria even for humans. 
- A phenomenon known as Deontological Keyword Bias (DKB) causes LLMs to often judge non-obligatory contexts as obligations when prompted with modal expressions like 'must' or 'ought to.'
- LLMs show a strong tendency to perceive over 90% of commonsense scenarios as obligations when modal expressions are present, which is consistent across various LLM families, questions, and answer formats.
- The study proposes a judgment strategy combining few-shot examples with reasoning prompts to mitigate DKB and improve alignment in normative decisions made by LLMs.
- This research highlights how linguistic framing through modal expressions influences LLMs' normative judgment and emphasizes the need to address biases for accurate judgment alignment. <br />   
Summary: <div>
arXiv:2506.11068v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly engaging in moral and ethical reasoning, where criteria for judgment are often unclear, even for humans. While LLM alignment studies cover many areas, one important yet underexplored area is how LLMs make judgments about obligations. This work reveals a strong tendency in LLMs to judge non-obligatory contexts as obligations when prompts are augmented with modal expressions such as must or ought to. We introduce this phenomenon as Deontological Keyword Bias (DKB). We find that LLMs judge over 90\% of commonsense scenarios as obligations when modal expressions are present. This tendency is consist across various LLM families, question types, and answer formats. To mitigate DKB, we propose a judgment strategy that integrates few-shot examples with reasoning prompts. This study sheds light on how modal expressions, as a form of linguistic framing, influence the normative decisions of LLMs and underscores the importance of addressing such biases to ensure judgment alignment.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Targeted control of fast prototyping through domain-specific interface</title>
<link>https://arxiv.org/abs/2506.11070</link>
<guid>https://arxiv.org/abs/2506.11070</guid>
<content:encoded><![CDATA[
<div> Keywords: Industrial designers, prototype models, natural language instructions, interface architecture, fast prototyping

Summary:
Industrial designers aim to control prototype models using natural language instructions seamlessly. However, current Large Language Models face challenges due to differences between designer and modeling languages. To address this, the proposed interface architecture acts as a mediator, grounded in design principles and automated domain specification. Machine-based evaluations and human studies across product design domains confirm the interface's capability to enhance Large Language Models for precise and effective prototype model control. The interface bridges gaps in abstraction levels, semantic precision, and lexical scopes, enabling designers to achieve their intended configurations effortlessly. <div>
arXiv:2506.11070v1 Announce Type: new 
Abstract: Industrial designers have long sought a natural and intuitive way to achieve the targeted control of prototype models -- using simple natural language instructions to configure and adjust the models seamlessly according to their intentions, without relying on complex modeling commands. While Large Language Models have shown promise in this area, their potential for controlling prototype models through language remains partially underutilized. This limitation stems from gaps between designers' languages and modeling languages, including mismatch in abstraction levels, fluctuation in semantic precision, and divergence in lexical scopes. To bridge these gaps, we propose an interface architecture that serves as a medium between the two languages. Grounded in design principles derived from a systematic investigation of fast prototyping practices, we devise the interface's operational mechanism and develop an algorithm for its automated domain specification. Both machine-based evaluations and human studies on fast prototyping across various product design domains demonstrate the interface's potential to function as an auxiliary module for Large Language Models, enabling precise and effective targeted control of prototype models.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention</title>
<link>https://arxiv.org/abs/2506.11073</link>
<guid>https://arxiv.org/abs/2506.11073</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models, Multilingual, Object Hallucination, Attention, Language Shift<br />
Summary:<br />
- Large Vision-Language Models (LVLMs) show impressive multimodal abilities but struggle with multilingual object hallucination.<br />
- Existing solutions rely on resource-intensive pretraining or fine-tuning.<br />
- Cross-Lingual Attention Intervention for Mitigating (CLAIM) proposes a near training-free method by aligning attention patterns across languages.<br />
- CLAIM identifies language-specific attention heads, estimates language shift vectors, and intervenes during inference to align cross-lingual visual perception.<br />
- Experiments show CLAIM improves performance on multilingual benchmarks, with up to 30% improvement in Spanish.<br /> <div>
arXiv:2506.11073v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal abilities but remain prone to multilingual object hallucination, with a higher likelihood of generating responses inconsistent with the visual input when utilizing queries in non-English languages compared to English. Most existing approaches to address these rely on pretraining or fine-tuning, which are resource-intensive. In this paper, inspired by observing the disparities in cross-modal attention patterns across languages, we propose Cross-Lingual Attention Intervention for Mitigating multilingual object hallucination (CLAIM) in LVLMs, a novel near training-free method by aligning attention patterns. CLAIM first identifies language-specific cross-modal attention heads, then estimates language shift vectors from English to the target language, and finally intervenes in the attention outputs during inference to facilitate cross-lingual visual perception capability alignment. Extensive experiments demonstrate that CLAIM achieves an average improvement of 13.56% (up to 30% in Spanish) on the POPE and 21.75% on the hallucination subsets of the MME benchmark across various languages. Further analysis reveals that multilingual attention divergence is most prominent in intermediate layers, highlighting their critical role in multilingual scenarios.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CyclicReflex: Improving Large Reasoning Models via Cyclical Reflection Token Scheduling</title>
<link>https://arxiv.org/abs/2506.11077</link>
<guid>https://arxiv.org/abs/2506.11077</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning models, reflection tokens, resource allocation, test-time compute performance, dynamic modulation

Summary:
Large reasoning models (LRMs) like OpenAI's o1 and DeepSeek-R1 use reflection tokens to guide the reasoning process before providing final answers. However, improper use of reflection tokens can lead to decreased model performance. This study introduces the concept of resource allocation for reflection tokens to enhance test-time compute performance by regulating their frequency and placement. The researchers propose CyclicReflex, a decoding strategy that dynamically adjusts reflection token logits using a position-dependent triangular waveform. Experimental results on various tasks show that CyclicReflex consistently improves model performance across different sizes, surpassing standard decoding methods and recent approaches like TIP and S1. This research sheds light on the importance of balancing reflection token usage and provides a practical solution for optimizing model performance during reasoning tasks. The codes for CyclicReflex are available on GitHub for further exploration and application. 

<br /><br />Summary: <div>
arXiv:2506.11077v1 Announce Type: new 
Abstract: Large reasoning models (LRMs), such as OpenAI's o1 and DeepSeek-R1, harness test-time scaling to perform multi-step reasoning for complex problem-solving. This reasoning process, executed before producing final answers, is often guided by special juncture tokens or textual segments that prompt self-evaluative reflection. We refer to these transition markers and reflective cues as "reflection tokens" (e.g., "wait", "but", "alternatively"). In this work, we treat reflection tokens as a "resource" and introduce the problem of resource allocation, aimed at improving the test-time compute performance of LRMs by adaptively regulating the frequency and placement of reflection tokens. Through empirical analysis, we show that both excessive and insufficient use of reflection tokens, referred to as over-reflection and under-reflection, can degrade model performance. To better understand and manage this trade-off, we draw an analogy between reflection token usage and learning rate scheduling in optimization. Building on this insight, we propose cyclical reflection token scheduling (termed CyclicReflex), a decoding strategy that dynamically modulates reflection token logits using a position-dependent triangular waveform. Experiments on MATH500, AIME2024/2025, and AMC2023 demonstrate that CyclicReflex consistently improves performance across model sizes (1.5B-8B), outperforming standard decoding and more recent approaches such as TIP (thought switching penalty) and S1. Codes are available at https://github.com/OPTML-Group/CyclicReflex.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoE-FND: A Case-Based Reasoning Approach with Dual Verification for Fake News Detection via LLMs</title>
<link>https://arxiv.org/abs/2506.11078</link>
<guid>https://arxiv.org/abs/2506.11078</guid>
<content:encoded><![CDATA[
<div> Keywords: Fake News Detection, Large Language Models, Logical Deduction, Experiential Learning, Case-Based Reasoning

Summary: 
RoE-FND, a novel framework for Fake News Detection, combines Large Language Models with experiential learning to address existing challenges in evidence-based detection methods. The framework reframes FND as a logical deduction task, improving the decision-making process. It consists of two stages: self-reflective knowledge building and dynamic criterion retrieval, enabling adaptation to evolving situations without the need for training. RoE-FND incorporates a case-based reasoning framework that cross-checks rationales against internal experience, leading to superior generalization and effectiveness over state-of-the-art methods across multiple datasets. Empirical validation demonstrates the framework's ability to address noisy evidence selection, generalization bottlenecks, hallucinated rationales, and conclusion bias, making it a promising approach for combating deceptive content online. 

<br /><br />Summary: <div>
arXiv:2506.11078v1 Announce Type: new 
Abstract: The proliferation of deceptive content online necessitates robust Fake News Detection (FND) systems. While evidence-based approaches leverage external knowledge to verify claims, existing methods face critical limitations: noisy evidence selection, generalization bottlenecks, and unclear decision-making processes. Recent efforts to harness Large Language Models (LLMs) for FND introduce new challenges, including hallucinated rationales and conclusion bias. To address these issues, we propose \textbf{RoE-FND} (\textbf{\underline{R}}eason \textbf{\underline{o}}n \textbf{\underline{E}}xperiences FND), a framework that reframes evidence-based FND as a logical deduction task by synergizing LLMs with experiential learning. RoE-FND encompasses two stages: (1) \textit{self-reflective knowledge building}, where a knowledge base is curated by analyzing past reasoning errors, namely the exploration stage, and (2) \textit{dynamic criterion retrieval}, which synthesizes task-specific reasoning guidelines from historical cases as experiences during deployment. It further cross-checks rationales against internal experience through a devised dual-channel procedure. Key contributions include: a case-based reasoning framework for FND that addresses multiple existing challenges, a training-free approach enabling adaptation to evolving situations, and empirical validation of the framework's superior generalization and effectiveness over state-of-the-art methods across three datasets.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MANBench: Is Your Multimodal Model Smarter than Human?</title>
<link>https://arxiv.org/abs/2506.11080</link>
<guid>https://arxiv.org/abs/2506.11080</guid>
<content:encoded><![CDATA[
<div> benchmark, Multimodal Large Language Models, MANBench, cross-modal reasoning, human performance<br />
Summary:<br />
The article introduces MANBench, a bilingual benchmark assessing Multimodal Large Language Models (MLLMs) across various tasks. Through human experiments, it was found that while MLLMs excel in tasks like Knowledge and Text-Image Understanding, they struggle with deeper cross-modal reasoning tasks. Both humans and MLLMs face challenges in complex tasks such as Puzzles and Spatial Imagination. The study highlights the strengths and limitations of MLLMs, showing that advanced models still fall short of human-level performance in many domains. MANBench aims to inspire efforts to bridge the gap between MLLMs and human multimodal capabilities. The code and dataset are available for access on GitHub for further research and development. <br /> <div>
arXiv:2506.11080v1 Announce Type: new 
Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has ignited discussions regarding their potential to surpass human performance in multimodal tasks. In response, we introduce MANBench (Multimodal Ability Norms Benchmark), a bilingual benchmark (English and Chinese) comprising 1,314 questions across nine tasks, spanning knowledge-based and non-knowledge-based domains. MANBench emphasizes intuitive reasoning, seamless cross-modal integration, and real-world complexity, providing a rigorous evaluation framework.
  Through extensive human experiments involving diverse participants, we compared human performance against state-of-the-art MLLMs. The results indicate that while MLLMs excel in tasks like Knowledge and Text-Image Understanding, they struggle with deeper cross-modal reasoning tasks such as Transmorphic Understanding, Image Consistency, and Multi-image Understanding. Moreover, both humans and MLLMs face challenges in highly complex tasks like Puzzles and Spatial Imagination.
  MANBench highlights the strengths and limitations of MLLMs, revealing that even advanced models fall short of achieving human-level performance across many domains. We hope MANBench will inspire efforts to bridge the gap between MLLMs and human multimodal capabilities. The code and dataset are available at https://github.com/micdz/MANBench.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGE:Specification-Aware Grammar Extraction for Automated Test Case Generation with LLMs</title>
<link>https://arxiv.org/abs/2506.11081</link>
<guid>https://arxiv.org/abs/2506.11081</guid>
<content:encoded><![CDATA[
<div> Keywords: Grammar-based test case generation, Context-Free Grammars with Counters (CCFGs), large language models (LLMs), reinforcement learning, test effectiveness

Summary: 
Grammar-based test case generation has been successful in competitive programming but generating valid and general grammars from natural language specifications remains a challenge. The use of Context-Free Grammars with Counters (CCFGs) has been proposed to address this issue by representing specifications with logical constraints. In this study, the authors explore the use of large language models (LLMs) to induce CCFGs from specifications with limited labeled examples and reinforcement learning. They fine-tune an open-source LLM for specification-to-grammar translation and apply Group Relative Policy Optimization (GRPO) to enhance grammar quality. The approach, named SAGE, shows improved generalization and outperforms existing LLMs in grammar validity and test effectiveness. The experimental results demonstrate a significant enhancement over the state-of-the-art methods. The implementation and dataset are available on an anonymous repository for further exploration and research. 

<br /><br />Summary: <div>
arXiv:2506.11081v1 Announce Type: new 
Abstract: Grammar-based test case generation has proven effective for competitive programming problems, but generating valid and general grammars from natural language specifications remains a key challenge, especially under limited supervision. Context-Free Grammars with Counters (CCFGs) have recently been introduced as a formalism to represent such specifications with logical constraints by storing and reusing counter values during derivation. In this work, we explore the use of open-source large language models (LLMs) to induce CCFGs from specifications using a small number of labeled examples and verifiable reward-guided reinforcement learning. Our approach first fine-tunes an open-source LLM to perform specification-to-grammar translation, and further applies Group Relative Policy Optimization (GRPO) to enhance grammar validity and generality. We also examine the effectiveness of iterative feedback for open and closed-source LLMs in correcting syntactic and semantic errors in generated grammars.
  Experimental results show that our approach SAGE achieves stronger generalization and outperforms 17 open and closed-source LLMs in both grammar quality and test effectiveness, improving over the state-of-the-art by 15.92%p in grammar validity and 12.34%p in test effectiveness. We provide our implementation and dataset at the following anonymous repository:https://anonymous.4open.science/r/SAGE-5714
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISM: A Transformer-based Language Model of Structured Clinical Event Data</title>
<link>https://arxiv.org/abs/2506.11082</link>
<guid>https://arxiv.org/abs/2506.11082</guid>
<content:encoded><![CDATA[
<div> transformer-based architecture, clinical decision-making, predictive reasoning, sequential medicine, generative language modeling

Summary:
PRISM, a transformer-based model, is introduced for understanding sequential clinical decision-making processes. It represents clinical trajectories as sequences of events and predicts future steps in patient diagnoses. The model utilizes a large clinical vocabulary and autoregressive training to capture complex dependencies in patient timelines. Experimental results show significant improvement in predicting next tokens compared to random baselines. The generated sequences reflect realistic diagnostic pathways, laboratory result progressions, and clinician ordering behaviors. This approach demonstrates the application of generative language modeling in structured medical event data for clinical decision support, simulation, and education. PRISM paves the way for advancements in sequence-based healthcare modeling by bridging machine learning architectures with real-world diagnostic reasoning. 

Summary: <div>
arXiv:2506.11082v1 Announce Type: new 
Abstract: We introduce PRISM (Predictive Reasoning in Sequential Medicine), a transformer-based architecture designed to model the sequential progression of clinical decision-making processes. Unlike traditional approaches that rely on isolated diagnostic classification, PRISM frames clinical trajectories as tokenized sequences of events - including diagnostic tests, laboratory results, and diagnoses - and learns to predict the most probable next steps in the patient diagnostic journey. Leveraging a large custom clinical vocabulary and an autoregressive training objective, PRISM demonstrates the ability to capture complex dependencies across longitudinal patient timelines. Experimental results show substantial improvements over random baselines in next-token prediction tasks, with generated sequences reflecting realistic diagnostic pathways, laboratory result progressions, and clinician ordering behaviors. These findings highlight the feasibility of applying generative language modeling techniques to structured medical event data, enabling applications in clinical decision support, simulation, and education. PRISM establishes a foundation for future advancements in sequence-based healthcare modeling, bridging the gap between machine learning architectures and real-world diagnostic reasoning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RedDebate: Safer Responses through Multi-Agent Red Teaming Debates</title>
<link>https://arxiv.org/abs/2506.11083</link>
<guid>https://arxiv.org/abs/2506.11083</guid>
<content:encoded><![CDATA[
<div> framework, RedDebate, multi-agent debate, Large Language Models, AI safety<br />
Summary:<br />
The article introduces RedDebate, a unique framework that utilizes adversarial argumentation among Large Language Models (LLMs) to proactively identify and address unsafe behaviors. Unlike traditional AI safety methods that rely on human evaluations or single-model assessments, RedDebate promotes collaborative disagreement among multiple LLMs to uncover blind spots and enhance responses through automated red-teaming. By integrating different long-term memory capacities for retaining safety insights from debates, RedDebate demonstrates a significant reduction in unsafe behaviors on existing benchmarks. Specifically, debate alone can reduce such behaviors by 17.7%, which further improves to over 23.5% when combined with long-term memory modules. Notably, RedDebate represents the first automated framework that leverages multi-agent debates and red-teaming to enhance AI safety without direct human intervention.<br /><br />Summary: <div>
arXiv:2506.11083v1 Announce Type: new 
Abstract: We propose RedDebate, a novel multi-agent debate framework that leverages adversarial argumentation among Large Language Models (LLMs) to proactively identify and mitigate their own unsafe behaviours. Existing AI safety methods often depend heavily on costly human evaluations or isolated single-model assessment, both subject to scalability constraints and oversight risks. RedDebate instead embraces collaborative disagreement, enabling multiple LLMs to critically examine one another's reasoning, and systematically uncovering unsafe blind spots through automated red-teaming, and iteratively improve their responses. We further integrate distinct types of long-term memory that retain learned safety insights from debate interactions. Evaluating on established safety benchmarks such as HarmBench, we demonstrate the proposed method's effectiveness. Debate alone can reduce unsafe behaviours by 17.7%, and when combined with long-term memory modules, achieves reductions exceeding 23.5%. To our knowledge, RedDebate constitutes the first fully automated framework that combines multi-agent debates with red-teaming to progressively enhance AI safety without direct human intervention.(Github Repository: https://github.com/aliasad059/RedDebate)
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Birds with One Stone: Improving Factuality and Faithfulness of LLMs via Dynamic Interactive Subspace Editing</title>
<link>https://arxiv.org/abs/2506.11088</link>
<guid>https://arxiv.org/abs/2506.11088</guid>
<content:encoded><![CDATA[
<div> framework, factuality, faithfulness, LLMs, hallucination
<br />
SPACE is a new approach that addresses factuality and faithfulness hallucinations in Large Language Models (LLMs) by editing shared activation subspaces. It identifies overlapping subspaces within neural representations, allowing for concurrent mitigation of both types of hallucinations. The framework incorporates dual-task feature modeling to establish a geometric foundation for shared subspace existence and utilizes a hybrid probe strategy for subspace identification and editing. Experimental results across various datasets demonstrate the effectiveness of SPACE in enhancing both factuality and faithfulness in LLMs. 
<br /><br />Summary: 
- SPACE framework enhances factuality and faithfulness in LLMs.
- It addresses factuality and faithfulness hallucinations simultaneously.
- Shared activation subspaces within neural representations are edited to mitigate hallucinations.
- Dual-task feature modeling establishes a geometric foundation for shared subspace existence.
- A hybrid probe strategy combining spectral clustering and attention head saliency scoring is used for subspace identification and editing. <div>
arXiv:2506.11088v1 Announce Type: new 
Abstract: LLMs have demonstrated unprecedented capabilities in natural language processing, yet their practical deployment remains hindered by persistent factuality and faithfulness hallucinations. While existing methods address these hallucination types independently, they inadvertently induce performance trade-offs, as interventions targeting one type often exacerbate the other. Through empirical and theoretical analysis of activation space dynamics in LLMs, we reveal that these hallucination categories share overlapping subspaces within neural representations, presenting an opportunity for concurrent mitigation. To harness this insight, we propose SPACE, a unified framework that jointly enhances factuality and faithfulness by editing shared activation subspaces. SPACE establishes a geometric foundation for shared subspace existence through dual-task feature modeling, then identifies and edits these subspaces via a hybrid probe strategy combining spectral clustering and attention head saliency scoring. Experimental results across multiple benchmark datasets demonstrate the superiority of our approach.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customizing Speech Recognition Model with Large Language Model Feedback</title>
<link>https://arxiv.org/abs/2506.11091</link>
<guid>https://arxiv.org/abs/2506.11091</guid>
<content:encoded><![CDATA[
<div> named entities, automatic speech recognition, domain adaptation, large language models, reinforcement learning

Summary:
Named entities are a challenge for automatic speech recognition (ASR) systems, particularly in domain mismatches. This study proposes a reinforcement learning approach for unsupervised domain adaptation in ASR, utilizing large language models (LLMs) to improve transcription quality. By using contextual information and LLM as a reward model to score ASR hypotheses, the framework fine-tunes the ASR model through reinforcement learning. The method significantly enhances named entity recognition, achieving a 21% reduction in entity word error rate compared to conventional self-training methods. This approach demonstrates the effectiveness of leveraging LLMs and reinforcement learning for improving ASR performance, particularly in handling rare named entities and adapting to diverse domains. <div>
arXiv:2506.11091v1 Announce Type: new 
Abstract: Automatic speech recognition (ASR) systems have achieved strong performance on general transcription tasks. However, they continue to struggle with recognizing rare named entities and adapting to domain mismatches. In contrast, large language models (LLMs), trained on massive internet-scale datasets, are often more effective across a wide range of domains. In this work, we propose a reinforcement learning based approach for unsupervised domain adaptation, leveraging unlabeled data to enhance transcription quality, particularly the named entities affected by domain mismatch, through feedback from a LLM. Given contextual information, our framework employs a LLM as the reward model to score the hypotheses from the ASR model. These scores serve as reward signals to fine-tune the ASR model via reinforcement learning. Our method achieves a 21\% improvement on entity word error rate over conventional self-training methods.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation</title>
<link>https://arxiv.org/abs/2506.11092</link>
<guid>https://arxiv.org/abs/2506.11092</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, large language models, healthcare, smart homes, dynamic environments

Summary:
Dynamic Context Tuning (DCT) is a lightweight framework that enhances Retrieval-Augmented Generation (RAG) systems to support multi-turn dialogue and dynamic environments without the need for retraining. DCT integrates an attention-based context cache, LoRA-based retrieval, and efficient context compression to enable flexible adaptation to evolving tool environments. Experiments demonstrate that DCT improves plan accuracy by 14% and reduces hallucinations by 37% compared to existing systems, while achieving comparable performance to GPT-4 at lower cost. DCT's ability to generalize to new tools enables the development of scalable and adaptable AI assistants for dynamic domains like healthcare and smart homes. This framework addresses the limitations of current RAG systems by offering a more versatile and efficient approach to handling changing user intent, available tools, and contextual factors in real-time interactions. 

<br /><br />Summary: <div>
arXiv:2506.11092v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has significantly advanced large language models (LLMs) by grounding their outputs in external tools and knowledge sources. However, existing RAG systems are typically constrained to static, single-turn interactions with fixed toolsets, making them ill-suited for dynamic domains such as healthcare and smart homes, where user intent, available tools, and contextual factors evolve over time. We present Dynamic Context Tuning (DCT), a lightweight framework that extends RAG to support multi-turn dialogue and evolving tool environments without requiring retraining. DCT integrates an attention-based context cache to track relevant past information, LoRA-based retrieval to dynamically select domain-specific tools, and efficient context compression to maintain inputs within LLM context limits. Experiments on both synthetic and real-world benchmarks show that DCT improves plan accuracy by 14% and reduces hallucinations by 37%, while matching GPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to previously unseen tools, enabling scalable and adaptable AI assistants across a wide range of dynamic environments.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs</title>
<link>https://arxiv.org/abs/2506.11094</link>
<guid>https://arxiv.org/abs/2506.11094</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Safety Evaluation, Toxicity, Bias, Ethical

Summary:
With the rapid advancement of artificial intelligence technology, Large Language Models (LLMs) have shown immense potential in Natural Language Processing. However, their deployment has raised safety concerns, especially regarding toxicity and bias in generated content. This survey provides a systematic overview of recent advancements in LLMs safety evaluation. It covers the background and significance of safety evaluation, categorizes evaluation tasks based on key capabilities like toxicity, robustness, ethics, bias, and fairness, summarizes evaluation metrics and datasets, and reviews evaluation methods and toolkits. The survey also highlights the challenges in LLMs safety evaluation and proposes future research directions to ensure the safe deployment of these models in real-world applications.<br /><br />Summary: <div>
arXiv:2506.11094v1 Announce Type: new 
Abstract: With the rapid advancement of artificial intelligence technology, Large Language Models (LLMs) have demonstrated remarkable potential in the field of Natural Language Processing (NLP), including areas such as content generation, human-computer interaction, machine translation, and code generation, among others. However, their widespread deployment has also raised significant safety concerns. In recent years, LLM-generated content has occasionally exhibited unsafe elements like toxicity and bias, particularly in adversarial scenarios, which has garnered extensive attention from both academia and industry. While numerous efforts have been made to evaluate the safety risks associated with LLMs, there remains a lack of systematic reviews summarizing these research endeavors. This survey aims to provide a comprehensive and systematic overview of recent advancements in LLMs safety evaluation, focusing on several key aspects: (1) "Why evaluate" that explores the background of LLMs safety evaluation, how they differ from general LLMs evaluation, and the significance of such evaluation; (2) "What to evaluate" that examines and categorizes existing safety evaluation tasks based on key capabilities, including dimensions such as toxicity, robustness, ethics, bias and fairness, truthfulness, and so on; (3) "Where to evaluate" that summarizes the evaluation metrics, datasets and benchmarks currently used in safety evaluations; (4) "How to evaluate" that reviews existing evaluation toolkit, and categorizing mainstream evaluation methods based on the roles of the evaluators. Finally, we identify the challenges in LLMs safety evaluation and propose potential research directions to promote further advancement in this field. We emphasize the importance of prioritizing LLMs safety evaluation to ensure the safe deployment of these models in real-world applications.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persistent Homology of Topic Networks for the Prediction of Reader Curiosity</title>
<link>https://arxiv.org/abs/2506.11095</link>
<guid>https://arxiv.org/abs/2506.11095</guid>
<content:encoded><![CDATA[
<div> Keywords: reader curiosity, NLP, information gap theory, BERTopic, semantic network

Summary:<br />
- The study focuses on exploring reader curiosity in NLP and introduces a framework based on Loewenstein's Information Gap Theory.
- A pipeline is developed using BERTopic-inspired topic modeling and persistent homology to analyze the topology of a semantic network derived from text segments.
- Topological features of the dynamic semantic network are used as proxies for information gaps to quantify reader curiosity.
- The pipeline is empirically evaluated using reader curiosity ratings obtained from participants reading ''The Hunger Games'' novel.
- Results show a significant improvement in predicting reader curiosity ratings using the topological features compared to a baseline model, validating the approach.

Summary: <div>
arXiv:2506.11095v1 Announce Type: new 
Abstract: Reader curiosity, the drive to seek information, is crucial for textual engagement, yet remains relatively underexplored in NLP. Building on Loewenstein's Information Gap Theory, we introduce a framework that models reader curiosity by quantifying semantic information gaps within a text's semantic structure. Our approach leverages BERTopic-inspired topic modeling and persistent homology to analyze the evolving topology (connected components, cycles, voids) of a dynamic semantic network derived from text segments, treating these features as proxies for information gaps. To empirically evaluate this pipeline, we collect reader curiosity ratings from participants (n = 49) as they read S. Collins's ''The Hunger Games'' novel. We then use the topological features from our pipeline as independent variables to predict these ratings, and experimentally show that they significantly improve curiosity prediction compared to a baseline model (73% vs. 30% explained deviance), validating our approach. This pipeline offers a new computational method for analyzing text structure and its relation to reader engagement.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-SEO Bench: Does Conversational SEO Work?</title>
<link>https://arxiv.org/abs/2506.11097</link>
<guid>https://arxiv.org/abs/2506.11097</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Conversational Search Engines, Conversational Search Engine Optimization, C-SEO methods, C-SEO Bench<br />
<br />
Summary: <br />
Large Language Models are changing search engines into Conversational Search Engines, leading to the emergence of Conversational Search Engine Optimization (C-SEO). However, current C-SEO methods are found to be largely ineffective across various tasks, domains, and number of actors. The study introduces C-SEO Bench, a benchmark to evaluate C-SEO methods in different scenarios. Traditional SEO strategies are shown to be more effective in improving visibility in Conversational Search Engines compared to cutting-edge C-SEO techniques. Additionally, increasing the number of C-SEO adopters leads to diminishing overall gains, indicating a competitive and zero-sum nature of the problem. The research provides valuable insights into the challenges and dynamics of optimizing web documents for Conversational Search Engines. <div>
arXiv:2506.11097v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are transforming search engines into Conversational Search Engines (CSE). Consequently, Search Engine Optimization (SEO) is being shifted into Conversational Search Engine Optimization (C-SEO). We are beginning to see dedicated C-SEO methods for modifying web documents to increase their visibility in CSE responses. However, they are often tested only for a limited breadth of application domains; we do not understand whether certain C-SEO methods would be effective for a broad range of domains. Moreover, existing evaluations consider only a single-actor scenario where only one web document adopts a C-SEO method; in reality, multiple players are likely to competitively adopt the cutting-edge C-SEO techniques, drawing an analogy from the dynamics we have seen in SEO. We present C-SEO Bench, the first benchmark designed to evaluate C-SEO methods across multiple tasks, domains, and number of actors. We consider two search tasks, question answering and product recommendation, with three domains each. We also formalize a new evaluation protocol with varying adoption rates among involved actors. Our experiments reveal that most current C-SEO methods are largely ineffective, contrary to reported results in the literature. Instead, traditional SEO strategies, those aiming to improve the ranking of the source in the LLM context, are significantly more effective. We also observe that as we increase the number of C-SEO adopters, the overall gains decrease, depicting a congested and zero-sum nature of the problem. Our code and data are available at https://github.com/parameterlab/c-seo-bench and https://huggingface.co/datasets/parameterlab/c-seo-bench.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2506.11102</link>
<guid>https://arxiv.org/abs/2506.11102</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, chatbots, AI agents, evaluation, benchmarks

Summary:
This paper discusses the evolution from traditional large language model (LLM) chatbots to advanced AI agents and the importance of distinguishing between the two in evaluation frameworks. It introduces a systematic analysis of current evaluation approaches, highlighting five key aspects that differentiate AI agents from LLM chatbots. The paper categorizes existing evaluation benchmarks based on external environments and internal capabilities, providing practical reference tables for researchers. It also outlines future evaluation methodologies through four critical lenses: environment, agent, evaluator, and metrics. This analysis offers guidance for researchers in selecting appropriate benchmarks for evaluating AI agents, aiding in the continued advancement of this rapidly evolving research domain.<br /><br />Summary: <div>
arXiv:2506.11102v1 Announce Type: new 
Abstract: The advent of large language models (LLMs), such as GPT, Gemini, and DeepSeek, has significantly advanced natural language processing, giving rise to sophisticated chatbots capable of diverse language-related tasks. The transition from these traditional LLM chatbots to more advanced AI agents represents a pivotal evolutionary step. However, existing evaluation frameworks often blur the distinctions between LLM chatbots and AI agents, leading to confusion among researchers selecting appropriate benchmarks. To bridge this gap, this paper introduces a systematic analysis of current evaluation approaches, grounded in an evolutionary perspective. We provide a detailed analytical framework that clearly differentiates AI agents from LLM chatbots along five key aspects: complex environment, multi-source instructor, dynamic feedback, multi-modal perception, and advanced capability. Further, we categorize existing evaluation benchmarks based on external environments driving forces, and resulting advanced internal capabilities. For each category, we delineate relevant evaluation attributes, presented comprehensively in practical reference tables. Finally, we synthesize current trends and outline future evaluation methodologies through four critical lenses: environment, agent, evaluator, and metrics. Our findings offer actionable guidance for researchers, facilitating the informed selection and application of benchmarks in AI agent evaluation, thus fostering continued advancement in this rapidly evolving research domain.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model</title>
<link>https://arxiv.org/abs/2506.11103</link>
<guid>https://arxiv.org/abs/2506.11103</guid>
<content:encoded><![CDATA[
<div> ICL, ManyICL, LLMs, in-context learning, fine-tuning <br />
Summary: 
The paper introduces Many-Shot In-Context Fine-tuning (ManyICL) as a new approach to enhance the performance of large language models (LLMs) in handling multiple downstream tasks simultaneously. ManyICL utilizes in-context learning principles in a many-shot setting, where each answer within the context is treated as a supervised training target to improve autoregressive learning. Experimental results across various tasks show that ManyICL outperforms zero/few-shot fine-tuning and approaches the performance of dedicated fine-tuning while also mitigating catastrophic forgetting issues. The proposed training objective significantly narrows the performance gap between in-context fine-tuning and dedicated fine-tuning methods. The code for ManyICL will be publicly available upon publication. <br /> <div>
arXiv:2506.11103v1 Announce Type: new 
Abstract: Large language models (LLMs) possess a remarkable ability to perform in-context learning (ICL), which enables them to handle multiple downstream tasks simultaneously without requiring task-specific fine-tuning. Recent studies have shown that even moderately sized LLMs, such as Mistral 7B, Gemma 7B and Llama-3 8B, can achieve ICL through few-shot in-context fine-tuning of all tasks at once. However, this approach still lags behind dedicated fine-tuning, where a separate model is trained for each individual task.
  In this paper, we propose a novel approach, Many-Shot In-Context Fine-tuning (ManyICL), which significantly narrows this performance gap by extending the principles of ICL to a many-shot setting. To unlock the full potential of ManyICL and address the inherent inefficiency of processing long sequences with numerous in-context examples, we propose a novel training objective. Instead of solely predicting the final answer, our approach treats every answer within the context as a supervised training target. This effectively shifts the role of many-shot examples from prompts to targets for autoregressive learning. Through extensive experiments on diverse downstream tasks, including classification, summarization, question answering, natural language inference, and math, we demonstrate that ManyICL substantially outperforms zero/few-shot fine-tuning and approaches the performance of dedicated fine-tuning. Furthermore, ManyICL significantly mitigates catastrophic forgetting issues observed in zero/few-shot fine-tuning. The code will be made publicly available upon publication.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAM: Dynamic Attention Mask for Long-Context Large Language Model Inference Acceleration</title>
<link>https://arxiv.org/abs/2506.11104</link>
<guid>https://arxiv.org/abs/2506.11104</guid>
<content:encoded><![CDATA[
<div> Keywords: NLP, transformers, sparse attention, dynamic attention, Large Language Models (LLMs)

Summary:
Dynamic sparse attention mechanisms have been introduced to address the efficiency issues faced by transformers in handling long-context understanding tasks in Natural Language Processing (NLP) applications. These methods adaptively assign masks at the attention-map level, allowing for heterogeneous patterns to be captured across layers and heads. Unlike existing approaches, this method does not require fine-tuning or predefined mask structures, while maintaining computational efficiency. By learning context-aware attention structures, the dynamic sparse attention mechanism achieves high alignment with full-attention models, ensuring minimal performance degradation while reducing memory and compute overhead. This scalable alternative to full attention enables the practical deployment of large-scale Large Language Models (LLMs) without compromising retrieval performance.

<br /><br />Summary: <div>
arXiv:2506.11104v1 Announce Type: new 
Abstract: Long-context understanding is crucial for many NLP applications, yet transformers struggle with efficiency due to the quadratic complexity of self-attention. Sparse attention methods alleviate this cost but often impose static, predefined masks, failing to capture heterogeneous attention patterns. This results in suboptimal token interactions, limiting adaptability and retrieval accuracy in long-sequence tasks. This work introduces a dynamic sparse attention mechanism that assigns adaptive masks at the attention-map level, preserving heterogeneous patterns across layers and heads. Unlike existing approaches, our method eliminates the need for fine-tuning and predefined mask structures while maintaining computational efficiency. By learning context-aware attention structures, it achieves high alignment with full-attention models, ensuring minimal performance degradation while reducing memory and compute overhead. This approach provides a scalable alternative to full attention, enabling the practical deployment of large-scale Large Language Models (LLMs) without sacrificing retrieval performance. DAM is available at: https://github.com/HanzhiZhang-Ulrica/DAM.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation</title>
<link>https://arxiv.org/abs/2506.11105</link>
<guid>https://arxiv.org/abs/2506.11105</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, healthcare scenarios, compression framework, neuron saliency, real-time deployment

Summary:
Large Language Models (LLMs) have a significant impact on healthcare scenarios but are often too large for deployment on resource-constrained edge devices. This work introduces a novel medical assistant system optimized through a compression framework tailored for specialized domains. By measuring neuron saliency on domain-specific data, irrelevant neurons are pruned to reduce model size while maintaining performance. Post-training quantization further reduces memory footprint. The compressed models, Gemma and LLaMA3, were evaluated across medical benchmarks such as MedMCQA, MedQA, and PubMedQA. Deployment on Jetson Orin Nano and Raspberry Pi 5 demonstrated real-time, energy-efficient inference under hardware constraints. This approach enables the efficient deployment of LLMs in real-time healthcare applications on edge devices. 

<br /><br />Summary: 
- Introduction of a medical assistant system optimized for specialized domains through a compression framework
- Pruning of irrelevant neurons based on domain-specific data to reduce model size without sacrificing performance
- Post-training quantization further reduces memory footprint
- Evaluation of compressed models on medical benchmarks showcasing their effectiveness
- Successful deployment on Jetson Orin Nano and Raspberry Pi 5 for real-time, energy-efficient inference in healthcare scenarios <div>
arXiv:2506.11105v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have significant impact on the healthcare scenarios but remain prohibitively large for deployment in real-time, resource-constrained environments such as edge devices. In this work, we introduce a novel medical assistant system, optimized through our general-purpose compression framework, which tailors Large Language Models (LLMs) for deployment in specialized domains. By measuring neuron saliency on domain-specific data, our method can aggressively prune irrelevant neurons, reducing model size while preserving performance. Following pruning, we apply post-training quantization to further reduce the memory footprint, and evaluate the compressed model across medical benchmarks including MedMCQA, MedQA, and PubMedQA. We also deploy the 50\% compressed Gemma and the 67\% compressed LLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak), achieving real-time, energy-efficient inference under hardware constraints.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-based RAG Enhancement via Global Query Disambiguation and Dependency-Aware Reranking</title>
<link>https://arxiv.org/abs/2506.11106</link>
<guid>https://arxiv.org/abs/2506.11106</guid>
<content:encoded><![CDATA[
<div> framework, PankRAG, graph-based retrieval-augmented generation, hierarchical query-resolution strategy, dependency-aware reranking mechanism, large language models

Summary: 
The article introduces PankRAG, a novel framework for graph-based retrieval-augmented generation. PankRAG addresses limitations in existing methods by incorporating a hierarchical query-resolution strategy that captures interdependencies in user queries. It also utilizes a dependency-aware reranking mechanism to enrich and validate retrieval results. Empirical evaluations show PankRAG outperforms state-of-the-art approaches on multiple benchmarks, demonstrating its robustness and generalizability. The framework's globally aware resolution path and structured reasoning guide large language models through complex queries, leading to more accurate and relevant retrieved content. By considering parallel and sequential dependencies within queries, PankRAG mitigates the risk of misinterpretation and omission of critical information, enhancing the fidelity of generated responses. Overall, PankRAG offers a comprehensive solution for improving information retrieval and generation tasks in natural language processing. 

<br /><br />Summary: <div>
arXiv:2506.11106v1 Announce Type: new 
Abstract: Contemporary graph-based retrieval-augmented generation (RAG) methods typically begin by extracting entities from user queries and then leverage pre-constructed knowledge graphs to retrieve related relationships and metadata. However, this pipeline's exclusive reliance on entity-level extraction can lead to the misinterpretation or omission of latent yet critical information and relations. As a result, retrieved content may be irrelevant or contradictory, and essential knowledge may be excluded, exacerbating hallucination risks and degrading the fidelity of generated responses. To address these limitations, we introduce PankRAG, a framework that combines a globally aware, hierarchical query-resolution strategy with a novel dependency-aware reranking mechanism. PankRAG first constructs a multi-level resolution path that captures both parallel and sequential interdependencies within a query, guiding large language models (LLMs) through structured reasoning. It then applies its dependency-aware reranker to exploit the dependency structure among resolved sub-questions, enriching and validating retrieval results for subsequent sub-questions. Empirical evaluations demonstrate that PankRAG consistently outperforms state-of-the-art approaches across multiple benchmarks, underscoring its robustness and generalizability.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>History-Aware Cross-Attention Reinforcement: Self-Supervised Multi Turn and Chain-of-Thought Fine-Tuning with vLLM</title>
<link>https://arxiv.org/abs/2506.11108</link>
<guid>https://arxiv.org/abs/2506.11108</guid>
<content:encoded><![CDATA[
<div> extension, Self-Supervised, Cross-Attention, Reinforcement, multi-turn dialogue

Summary:
The article introduces CAGSR-vLLM-MTC, an extension of the CAGSR framework implemented on the high-performance vLLM runtime. It addresses multi-turn dialogue and chain-of-thought reasoning by capturing cross-attention weights during generation and accumulating attention signals over conversation histories and chain-of-thought steps. Practical trade-offs such as an entropy-based clamping mechanism to prevent attention collapse are discussed. Future directions include exploring multi-party dialogues and hierarchical reasoning. <div>
arXiv:2506.11108v1 Announce Type: new 
Abstract: We present CAGSR-vLLM-MTC, an extension of our Self-Supervised Cross-Attention-Guided Reinforcement (CAGSR) framework, now implemented on the high-performance vLLM runtime, to address both multi-turn dialogue and chain-of-thought reasoning. Building upon our original single-turn approach, we first instrumented vLLM's C++/CUDA kernels to asynchronously capture per-layer, per-head cross-attention weights during generation. We then generalized our self-supervised reward function to accumulate attention signals over entire conversation histories and intermediate chain-of-thought steps. We discuss practical trade-offs, including an entropy-based clamping mechanism to prevent attention collapse on early context, and outline future directions for multi-party dialogues and hierarchical reasoning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Large Language Models for Mobility Analytics with Semantic Location Tokenization</title>
<link>https://arxiv.org/abs/2506.11109</link>
<guid>https://arxiv.org/abs/2506.11109</guid>
<content:encoded><![CDATA[
<div> Keywords: mobility data, Large Language Models, QT-Mob, location tokenization, fine-tuning objectives

Summary:
The article introduces QT-Mob, a novel framework designed to enhance Large Language Models (LLMs) for mobility analytics by addressing key limitations. QT-Mob incorporates a location tokenization module to create semantically rich location tokens, improving contextual representation while maintaining compatibility with LLMs. Additionally, the framework utilizes fine-tuning objectives to align learned tokens with internal LLM representations, enhancing the model's understanding of sequential movement patterns and location semantics. Experimental results on real-world datasets demonstrate QT-Mob's superior performance in next-location prediction and mobility recovery tasks compared to existing deep learning and LLM-based methods. This framework not only improves LLMs' interpretation of mobility data but also offers a more generalizable approach for various mobility analytics tasks.<br /><br />Summary: <div>
arXiv:2506.11109v1 Announce Type: new 
Abstract: The widespread adoption of location-based services has led to the generation of vast amounts of mobility data, providing significant opportunities to model user movement dynamics within urban environments. Recent advancements have focused on adapting Large Language Models (LLMs) for mobility analytics. However, existing methods face two primary limitations: inadequate semantic representation of locations (i.e., discrete IDs) and insufficient modeling of mobility signals within LLMs (i.e., single templated instruction fine-tuning). To address these issues, we propose QT-Mob, a novel framework that significantly enhances LLMs for mobility analytics. QT-Mob introduces a location tokenization module that learns compact, semantically rich tokens to represent locations, preserving contextual information while ensuring compatibility with LLMs. Furthermore, QT-Mob incorporates a series of complementary fine-tuning objectives that align the learned tokens with the internal representations in LLMs, improving the model's comprehension of sequential movement patterns and location semantics. The proposed QT-Mob framework not only enhances LLMs' ability to interpret mobility data but also provides a more generalizable approach for various mobility analytics tasks. Experiments on three real-world dataset demonstrate the superior performance in both next-location prediction and mobility recovery tasks, outperforming existing deep learning and LLM-based methods.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models</title>
<link>https://arxiv.org/abs/2506.11110</link>
<guid>https://arxiv.org/abs/2506.11110</guid>
<content:encoded><![CDATA[
<div> factuality, framing, Large Language Models, AssertBench, FEVEROUS

Summary:
The article introduces AssertBench, a benchmark designed to investigate how the framing of factually true statements influences the agreement of Large Language Models (LLMs). AssertBench samples evidence-supported facts from the FEVEROUS dataset and presents two framing prompts for each fact: one where the user claims the statement is correct and another where the user claims it is incorrect. The goal is to measure whether the LLM maintains consistent truth evaluation across different framings or switches its evaluation to agree with the user. By stratifying results based on the model's accuracy on neutral claims, AssertBench aims to isolate framing-induced variability from the model's underlying factual knowledge. The benchmark evaluates the LLM's ability to remain consistent in its assessment when faced with contradictory user assertions about the same fact. The complete source code for AssertBench is available on GitHub. 

Summary: <div>
arXiv:2506.11110v1 Announce Type: new 
Abstract: Recent benchmarks have probed factual consistency and rhetorical robustness in Large Language Models (LLMs). However, a knowledge gap exists regarding how directional framing of factually true statements influences model agreement, a common scenario for LLM users. AssertBench addresses this by sampling evidence-supported facts from FEVEROUS, a fact verification dataset. For each (evidence-backed) fact, we construct two framing prompts: one where the user claims the statement is factually correct, and another where the user claims it is incorrect. We then record the model's agreement and reasoning. The desired outcome is that the model asserts itself, maintaining consistent truth evaluation across both framings, rather than switching its evaluation to agree with the user. AssertBench isolates framing-induced variability from the model's underlying factual knowledge by stratifying results based on the model's accuracy on the same claims when presented neutrally. In doing so, this benchmark aims to measure an LLM's ability to "stick to its guns" when presented with contradictory user assertions about the same fact. The complete source code is available at https://github.com/achowd32/assert-bench.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions</title>
<link>https://arxiv.org/abs/2506.11111</link>
<guid>https://arxiv.org/abs/2506.11111</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Robustness, Adversarial Robustness, OOD Robustness, Evaluation

Summary: 
Large Language Models (LLMs) have become integral to various AI applications, necessitating a focus on their robustness. This survey paper comprehensively explores LLM robustness, categorizing it into three main perspectives. Firstly, Adversarial Robustness addresses intentional manipulation in prompts, such as noise prompts and data attacks. Secondly, OOD Robustness handles unforeseen real-world scenarios like OOD detection and hallucinations. Lastly, the Evaluation of Robustness section reviews new datasets, metrics, and tools for assessing LLM robustness. The paper discusses key research areas and future directions, aiming to provide a valuable resource for the community. Additionally, a GitHub project is organized to facilitate easy access to related works and foster collaboration. <br /><br />Summary: <div>
arXiv:2506.11111v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have gained enormous attention in recent years due to their capability of understanding and generating natural languages. With the rapid development and wild-range applications (e.g., Agents, Embodied Intelligence), the robustness of LLMs has received increased attention. As the core brain of many AI applications, the robustness of LLMs requires that models should not only generate consistent contents, but also ensure the correctness and stability of generated content when dealing with unexpeted application scenarios (e.g., toxic prompts, limited noise domain data, outof-distribution (OOD) applications, etc). In this survey paper, we conduct a thorough review of the robustness of LLMs, aiming to provide a comprehensive terminology of concepts and methods around this field and facilitate the community. Specifically, we first give a formal definition of LLM robustness and present the collection protocol of this survey paper. Then, based on the types of perturbated inputs, we organize this survey from the following perspectives: 1) Adversarial Robustness: tackling the problem that prompts are manipulated intentionally, such as noise prompts, long context, data attack, etc; 2) OOD Robustness: dealing with the unexpected real-world application scenarios, such as OOD detection, zero-shot transferring, hallucinations, etc; 3) Evaluation of Robustness: summarizing the new evaluation datasets, metrics, and tools for verifying the robustness of LLMs. After reviewing the representative work from each perspective, we discuss and highlight future opportunities and research directions in this field. Meanwhile, we also organize related works and provide an easy-to-search project (https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers) to support the community.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manifesto from Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE)</title>
<link>https://arxiv.org/abs/2506.11112</link>
<guid>https://arxiv.org/abs/2506.11112</guid>
<content:encoded><![CDATA[
<div> Keywords: CONversational Information ACcess, CONIAC, world model, Conversational Agents Framework for Evaluation, evaluation methodology

Summary:
During the workshop, the concept of CONversational Information ACcess (CONIAC) was thoroughly discussed, focusing on its unique features and proposing a world model to abstract it. The Conversational Agents Framework for Evaluation (CAFE) was introduced as a tool for evaluating CONIAC systems, comprising six key components. These components include defining the goals of the system's stakeholders, identifying user tasks for evaluation, analyzing user aspects, determining evaluation criteria, selecting appropriate evaluation methodologies, and establishing measures for quantitative criteria. This comprehensive framework aims to provide a systematic approach to evaluating CONIAC systems, ensuring that they meet the needs and expectations of their users. <div>
arXiv:2506.11112v1 Announce Type: new 
Abstract: During the workshop, we deeply discussed what CONversational Information ACcess (CONIAC) is and its unique features, proposing a world model abstracting it, and defined the Conversational Agents Framework for Evaluation (CAFE) for the evaluation of CONIAC systems, consisting of six major components: 1) goals of the system's stakeholders, 2) user tasks to be studied in the evaluation, 3) aspects of the users carrying out the tasks, 4) evaluation criteria to be considered, 5) evaluation methodology to be applied, and 6) measures for the quantitative criteria chosen.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks</title>
<link>https://arxiv.org/abs/2506.11113</link>
<guid>https://arxiv.org/abs/2506.11113</guid>
<content:encoded><![CDATA[
<div> Keywords: peer review, large language models, adversarial attacks, reliability, mitigation strategies

Summary:
Large language models (LLMs) are being explored as potential automated reviewers to alleviate the burden on human reviewers. However, their susceptibility to adversarial attacks raises concerns about the reliability of their generated reviews. This study evaluates the effectiveness of LLMs compared to human reviewers and explores the impact of adversarial attacks on the reliability of LLM-generated reviews. The research highlights significant vulnerabilities in LLM assessments due to text manipulations, emphasizing the need to address adversarial risks. The study also discusses challenges and potential mitigation strategies for using LLMs in automated peer reviewing. It is crucial to ensure that AI strengthens rather than compromises the integrity of scholarly communication. 

Summary: <div>
arXiv:2506.11113v1 Announce Type: new 
Abstract: Peer review is essential for maintaining academic quality, but the increasing volume of submissions places a significant burden on reviewers. Large language models (LLMs) offer potential assistance in this process, yet their susceptibility to textual adversarial attacks raises reliability concerns. This paper investigates the robustness of LLMs used as automated reviewers in the presence of such attacks. We focus on three key questions: (1) The effectiveness of LLMs in generating reviews compared to human reviewers. (2) The impact of adversarial attacks on the reliability of LLM-generated reviews. (3) Challenges and potential mitigation strategies for LLM-based review. Our evaluation reveals significant vulnerabilities, as text manipulations can distort LLM assessments. We offer a comprehensive evaluation of LLM performance in automated peer reviewing and analyze its robustness against adversarial attacks. Our findings emphasize the importance of addressing adversarial risks to ensure AI strengthens, rather than compromises, the integrity of scholarly communication.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations</title>
<link>https://arxiv.org/abs/2506.11114</link>
<guid>https://arxiv.org/abs/2506.11114</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, KokushiMD-10, healthcare licensing exams, multimodal benchmark, medical AI

Summary:
The article introduces KokushiMD-10, a multimodal benchmark derived from Japanese national healthcare licensing exams to assess LLMs in high-stakes clinical scenarios. It consists of over 11588 real exam questions covering various healthcare roles and includes clinical images for evaluating textual and visual reasoning. Thirty state-of-the-art LLMs are benchmarked, but none consistently meet passing thresholds across domains, highlighting challenges in medical AI. KokushiMD-10 aims to provide a comprehensive resource for advancing reasoning-centric medical AI in multilingual and multimodal clinical tasks. <div>
arXiv:2506.11114v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have demonstrated notable performance in medical licensing exams. However, comprehensive evaluation of LLMs across various healthcare roles, particularly in high-stakes clinical scenarios, remains a challenge. Existing benchmarks are typically text-based, English-centric, and focus primarily on medicines, which limits their ability to assess broader healthcare knowledge and multimodal reasoning. To address these gaps, we introduce KokushiMD-10, the first multimodal benchmark constructed from ten Japanese national healthcare licensing exams. This benchmark spans multiple fields, including Medicine, Dentistry, Nursing, Pharmacy, and allied health professions. It contains over 11588 real exam questions, incorporating clinical images and expert-annotated rationales to evaluate both textual and visual reasoning. We benchmark over 30 state-of-the-art LLMs, including GPT-4o, Claude 3.5, and Gemini, across both text and image-based settings. Despite promising results, no model consistently meets passing thresholds across domains, highlighting the ongoing challenges in medical AI. KokushiMD-10 provides a comprehensive and linguistically grounded resource for evaluating and advancing reasoning-centric medical AI across multilingual and multimodal clinical tasks.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating Domain Knowledge into Materials Tokenization</title>
<link>https://arxiv.org/abs/2506.11115</link>
<guid>https://arxiv.org/abs/2506.11115</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, materials science, tokenization, MATTER, domain knowledge

Summary:
MATTER is introduced as a novel tokenization approach for language models in materials science. Traditional tokenization methods often lead to fragmentation and semantic loss when applied to scientific text. MATTER addresses this issue by incorporating material knowledge into tokenization. By utilizing MatDetector and a re-ranking method, MATTER maintains the structural and semantic integrity of material concepts during token merging. Experimental results show that MATTER outperforms existing tokenization methods, with a performance gain of 4% and 2% in generation and classification tasks, respectively. The study highlights the significance of domain knowledge in developing effective tokenization strategies for scientific text processing.

<br /><br />Summary: <div>
arXiv:2506.11115v1 Announce Type: new 
Abstract: While language models are increasingly utilized in materials science, typical models rely on frequency-centric tokenization methods originally developed for natural language processing. However, these methods frequently produce excessive fragmentation and semantic loss, failing to maintain the structural and semantic integrity of material concepts. To address this issue, we propose MATTER, a novel tokenization approach that integrates material knowledge into tokenization. Based on MatDetector trained on our materials knowledge base and a re-ranking method prioritizing material concepts in token merging, MATTER maintains the structural integrity of identified material concepts and prevents fragmentation during tokenization, ensuring their semantic meaning remains intact. The experimental results demonstrate that MATTER outperforms existing tokenization methods, achieving an average performance gain of $4\%$ and $2\%$ in the generation and classification tasks, respectively. These results underscore the importance of domain knowledge for tokenization strategies in scientific text processing. Our code is available at https://github.com/yerimoh/MATTER
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models</title>
<link>https://arxiv.org/abs/2506.11116</link>
<guid>https://arxiv.org/abs/2506.11116</guid>
<content:encoded><![CDATA[
<div> Instruction dataset, Large Language Models, Fine-tuning, Performance gains, Benchmarking <br />
<br />
Summary: <br />
The article introduces Infinity-Instruct, a high-quality instruction dataset aiming to enhance the capabilities of Large Language Models (LLMs) in foundational and chat tasks. The dataset is developed in two phases: Phase 1 involves curating 7.4M foundational instructions, while Phase 2 focuses on synthesizing 1.5M chat instructions. By fine-tuning open-source models with Infinity-Instruct, significant performance improvements are observed in both foundational and instruction-following benchmarks. Notably, the InfInstruct-LLaMA3.1-70B model outperforms GPT-4-0314 by 8.6% on instruction-following tasks while maintaining comparable foundational performance. These results highlight the importance of synergizing foundational and chat training for comprehensive LLM development. The dataset and codes have been publicly released for further research and application. <div>
arXiv:2506.11116v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate strong performance in real-world applications, yet existing open-source instruction datasets often concentrate on narrow domains, such as mathematics or coding, limiting generalization and widening the gap with proprietary models. To bridge this gap, we introduce Infinity-Instruct, a high-quality instruction dataset designed to enhance both foundational and chat capabilities of LLMs through a two-phase pipeline. In Phase 1, we curate 7.4M high-quality foundational instructions (InfInstruct-F-7.4M) from over 100M samples using hybrid data selection techniques. In Phase 2, we synthesize 1.5M high-quality chat instructions (InfInstruct-G-1.5M) through a two-stage process involving instruction selection, evolution, and diagnostic filtering. We empirically evaluate Infinity-Instruct by fine-tuning several open-source models, including Mistral, LLaMA, Qwen, and Yi, and observe substantial performance gains across both foundational and instruction following benchmarks, consistently surpassing official instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B outperforms GPT-4-0314 by 8.6\% on instruction following tasks while achieving comparable foundational performance. These results underscore the synergy between foundational and chat training and offer new insights into holistic LLM development. Our dataset\footnote{https://huggingface.co/datasets/BAAI/Infinity-Instruct} and codes\footnote{https://gitee.com/li-touch/infinity-instruct} have been publicly released.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScIRGen: Synthesize Realistic and Large-Scale RAG Dataset for Scientific Research</title>
<link>https://arxiv.org/abs/2506.11117</link>
<guid>https://arxiv.org/abs/2506.11117</guid>
<content:encoded><![CDATA[
<div> information needs, dataset, scientific QA, retrieval, ScIRGen-Geo

Summary:
ScIRGen introduces a dataset generation framework for scientific QA & retrieval that reflects the information needs of professional science researchers. It creates a large-scale scientific retrieval-augmented generation (RAG) dataset with realistic queries, datasets, and papers. Utilizing dataset-oriented information extraction from academic papers and a question generation framework based on cognitive taxonomy, the dataset ScIRGen-Geo is created. A method is developed to filter synthetic answers based on the perplexity shift of LLMs for validity. Benchmarking shows current methods struggle with complex question reasoning. This work advances tools to support the intricate information needs of the scientific community. 

Summary: <div>
arXiv:2506.11117v1 Announce Type: new 
Abstract: Scientific researchers need intensive information about datasets to effectively evaluate and develop theories and methodologies. The information needs regarding datasets are implicitly embedded in particular research tasks, rather than explicitly expressed in search queries. However, existing scientific retrieval and question-answering (QA) datasets typically address straightforward questions, which do not align with the distribution of real-world research inquiries. To bridge this gap, we developed ScIRGen, a dataset generation framework for scientific QA \& retrieval that more accurately reflects the information needs of professional science researchers, and uses it to create a large-scale scientific retrieval-augmented generation (RAG) dataset with realistic queries, datasets and papers. Technically, we designed a dataset-oriented information extraction method that leverages academic papers to augment the dataset representation. We then proposed a question generation framework by employing cognitive taxonomy to ensure the quality of synthesized questions. We also design a method to automatically filter synthetic answers based on the perplexity shift of LLMs, which is highly aligned with human judgment of answers' validity. Collectively, these methodologies culminated in the creation of the 61k QA dataset, ScIRGen-Geo. We benchmarked representative methods on the ScIRGen-Geo dataset for their question-answering and retrieval capabilities, finding out that current methods still suffer from reasoning from complex questions. This work advances the development of more sophisticated tools to support the intricate information needs of the scientific community.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Foundation Speech and Language Models for Alzheimer's Disease and Related Dementia Detection from Spontaneous Speech</title>
<link>https://arxiv.org/abs/2506.11119</link>
<guid>https://arxiv.org/abs/2506.11119</guid>
<content:encoded><![CDATA[
<div> Keywords: Alzheimer's disease, dementia, speech analysis, cognitive decline, early detection 

Summary: 
- The study focused on using speech analysis as a non-invasive biomarker for cognitive decline in Alzheimer's disease and related dementias (ADRD).
- The researchers used the PREPARE Challenge dataset, which included audio recordings from individuals with different cognitive statuses.
- They explored the performance of various open-source foundation speech and language models in classifying cognitive status into healthy control, mild cognitive impairment, and Alzheimer's Disease categories.
- The Whisper-medium model for speech analysis and BERT with pause annotation for language models showed the best performance.
- The study found that incorporating non-semantic features like pause patterns improved the text-based classification, and automatic speech recognition-generated audio embeddings outperformed others. Acoustic-based approaches, particularly ASR-derived embeddings, have the potential for scalable and cost-effective early detection of ADRD. 

<br /><br />Summary: <div>
arXiv:2506.11119v1 Announce Type: new 
Abstract: Background: Alzheimer's disease and related dementias (ADRD) are progressive neurodegenerative conditions where early detection is vital for timely intervention and care. Spontaneous speech contains rich acoustic and linguistic markers that may serve as non-invasive biomarkers for cognitive decline. Foundation models, pre-trained on large-scale audio or text data, produce high-dimensional embeddings encoding contextual and acoustic features.
  Methods: We used the PREPARE Challenge dataset, which includes audio recordings from over 1,600 participants with three cognitive statuses: healthy control (HC), mild cognitive impairment (MCI), and Alzheimer's Disease (AD). We excluded non-English, non-spontaneous, or poor-quality recordings. The final dataset included 703 (59.13%) HC, 81 (6.81%) MCI, and 405 (34.06%) AD cases. We benchmarked a range of open-source foundation speech and language models to classify cognitive status into the three categories.
  Results: The Whisper-medium model achieved the highest performance among speech models (accuracy = 0.731, AUC = 0.802). Among language models, BERT with pause annotation performed best (accuracy = 0.662, AUC = 0.744). ADRD detection using state-of-the-art automatic speech recognition (ASR) model-generated audio embeddings outperformed others. Including non-semantic features like pause patterns consistently improved text-based classification.
  Conclusion: This study introduces a benchmarking framework using foundation models and a clinically relevant dataset. Acoustic-based approaches -- particularly ASR-derived embeddings -- demonstrate strong potential for scalable, non-invasive, and cost-effective early detection of ADRD.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models</title>
<link>https://arxiv.org/abs/2506.11120</link>
<guid>https://arxiv.org/abs/2506.11120</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, gradient-based pruning, self-distillation loss, MLP modules, compression

Summary: 
This study addresses the high deployment costs of large language models (LLMs) by proposing a novel gradient-based pruning method. By introducing a self-distillation loss during the pruning phase, the model can fully exploit the predictions of the original LLM, leading to more accurate gradient information for compression. The focus is on pruning MLP modules, which have a significant parameter footprint in LLMs, contributing to efficient compression without noticeable performance degradation. Experimental results on zero-shot benchmarks demonstrate the superior performance of the proposed method compared to existing pruning techniques. Additionally, the method achieves competitive results among 1B-scale open source LLMs. The source code and trained weights are available for further exploration. 

<br /><br />Summary: <div>
arXiv:2506.11120v1 Announce Type: new 
Abstract: In spite of strong performance achieved by LLMs, the costs of their deployment are unaffordable. For the compression of LLMs, gradient-based pruning methods present promising effectiveness. However, in these methods, the gradient computation with one-hot labels ignore the potential predictions on other words, thus missing key information for generative capability of the original model. To address this issue, we introduce a self-distillation loss during the pruning phase (rather than post-training) to fully exploit the predictions of the original model, thereby obtaining more accurate gradient information for pruning. Moreover, we find that, compared to attention modules, the predictions of LLM are less sensitive to multilayer perceptron (MLP) modules, which take up more than $5 \times$ parameters (LLaMA3.2-1.2B). To this end, we focus on the pruning of MLP modules, to significantly compress LLM without obvious performance degradation. Experimental results on extensive zero-shot benchmarks demonstrate that our method significantly outperforms existing pruning methods. Furthermore, our method achieves very competitive performance among 1B-scale open source LLMs. The source code and trained weights are available at https://github.com/visresearch/SDMPrune.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR</title>
<link>https://arxiv.org/abs/2506.11121</link>
<guid>https://arxiv.org/abs/2506.11121</guid>
<content:encoded><![CDATA[
<div> adaptation, ASR, language model, test-time, entropy minimization 

Summary: 
The article discusses the challenges of combining Test-Time Adaptation (TTA) with language model rescoring in automatic speech recognition (ASR) systems. While TTA aims to improve ASR performance in real-world domain mismatches, it can interfere with language model rescoring. The proposed solution, SUTA-LM, extends the entropy-minimization-based TTA approach by incorporating a language model rescoring step. SUTA-LM employs a controlled adaptation process guided by an auto-step selection mechanism that considers both acoustic and linguistic information. Experiments conducted on 18 diverse ASR datasets demonstrate that SUTA-LM provides robust results across various domains. The combination of TTA and language model rescoring in SUTA-LM showcases an effective approach to improving ASR performance in challenging domain mismatches. 

<br /><br />Summary: <div>
arXiv:2506.11121v1 Announce Type: new 
Abstract: Despite progress in end-to-end ASR, real-world domain mismatches still cause performance drops, which Test-Time Adaptation (TTA) aims to mitigate by adjusting models during inference. Recent work explores combining TTA with external language models, using techniques like beam search rescoring or generative error correction. In this work, we identify a previously overlooked challenge: TTA can interfere with language model rescoring, revealing the nontrivial nature of effectively combining the two methods. Based on this insight, we propose SUTA-LM, a simple yet effective extension of SUTA, an entropy-minimization-based TTA approach, with language model rescoring. SUTA-LM first applies a controlled adaptation process guided by an auto-step selection mechanism leveraging both acoustic and linguistic information, followed by language model rescoring to refine the outputs. Experiments on 18 diverse ASR datasets show that SUTA-LM achieves robust results across a wide range of domains.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone Scams</title>
<link>https://arxiv.org/abs/2506.11125</link>
<guid>https://arxiv.org/abs/2506.11125</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Voice phishing, Text-to-Speech, Automatic Speech Recognition, Adversarial perturbations

Summary:
Large Language Models (LLMs) are being used in voice phishing scams, with Automatic Speech Recognition (ASR) being a vulnerable point. ASRJam is a defense framework that disrupts attackers' ASR by injecting adversarial perturbations into victim audio. EchoGuard is a jammer that disrupts ASR using natural distortions like reverberation, while still being tolerable to humans. A user study compared EchoGuard with three state-of-the-art attacks, showing it to be the most effective in both disrupting ASR and maintaining a positive human listening experience. This proactive defense approach aims to break the scam feedback loop without hindering genuine callers' understanding of the conversation. <br /><br />Summary: Large Language Models combined with Text-to-Speech are used in voice phishing scams, with the vulnerable ASR being targeted by ASRJam and the EchoGuard jammer. A user study confirmed EchoGuard's effectiveness in disrupting ASR while providing a positive listening experience for humans, offering a proactive defense against voice phishing scams. <div>
arXiv:2506.11125v1 Announce Type: new 
Abstract: Large Language Models (LLMs), combined with Text-to-Speech (TTS) and Automatic Speech Recognition (ASR), are increasingly used to automate voice phishing (vishing) scams. These systems are scalable and convincing, posing a significant security threat. We identify the ASR transcription step as the most vulnerable link in the scam pipeline and introduce ASRJam, a proactive defence framework that injects adversarial perturbations into the victim's audio to disrupt the attacker's ASR. This breaks the scam's feedback loop without affecting human callers, who can still understand the conversation. While prior adversarial audio techniques are often unpleasant and impractical for real-time use, we also propose EchoGuard, a novel jammer that leverages natural distortions, such as reverberation and echo, that are disruptive to ASR but tolerable to humans. To evaluate EchoGuard's effectiveness and usability, we conducted a 39-person user study comparing it with three state-of-the-art attacks. Results show that EchoGuard achieved the highest overall utility, offering the best combination of ASR disruption and human listening experience.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech Instructions</title>
<link>https://arxiv.org/abs/2506.11127</link>
<guid>https://arxiv.org/abs/2506.11127</guid>
<content:encoded><![CDATA[
<div> GUIRoboTron-Speech, autonomous agents, Graphical User Interfaces, speech instructions, training strategy<br />
Summary:<br />
Autonomous agents for Graphical User Interfaces (GUIs) are advancing human-computer interaction, with GUIRoboTron-Speech being the first agent to accept speech instructions and on-device screenshots. High-quality speech instructions were generated using a random timbre text-to-speech (TTS) model. GUIRoboTron-Speech's capabilities were developed through grounding and planning training stages, with a heuristic mixed-instruction training strategy utilized to address modality imbalance. Experiments on benchmark datasets confirmed the superior performance of GUIRoboTron-Speech, highlighting speech as an effective instruction modality for GUI agents. The code and datasets are available at https://github.com/GUIRoboTron/GUIRoboTron-Speech. <br /><br /> <div>
arXiv:2506.11127v1 Announce Type: new 
Abstract: Autonomous agents for Graphical User Interfaces (GUIs) are revolutionizing human-computer interaction, yet their reliance on text-based instructions imposes limitations on accessibility and convenience, particularly in hands-free scenarios. To address this gap, we propose GUIRoboTron-Speech, the first end-to-end autonomous GUI agent that directly accepts speech instructions and on-device screenshots to predict actions. Confronted with the scarcity of speech-based GUI agent datasets, we initially generated high-quality speech instructions for training by leveraging a random timbre text-to-speech (TTS) model to convert existing text instructions. We then develop GUIRoboTron-Speech's capabilities through progressive grounding and planning training stages. A key contribution is a heuristic mixed-instruction training strategy designed to mitigate the modality imbalance inherent in pre-trained foundation models. Comprehensive experiments on several benchmark datasets validate the robust and superior performance of GUIRoboTron-Speech, demonstrating the significant potential and widespread applicability of speech as an effective instruction modality for driving GUI agents. Our code and datasets are available at https://github.com/GUIRoboTron/GUIRoboTron-Speech.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stronger Language Models Produce More Human-Like Errors</title>
<link>https://arxiv.org/abs/2506.11128</link>
<guid>https://arxiv.org/abs/2506.11128</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, reasoning capabilities, human fallacies, logical correctness, cognitive framework

Summary: 
Language models, as they advance in sophistication, show improved reasoning capabilities. However, a study using the Erotetic Theory of Reasoning found that errors made by these models increasingly resemble common human reasoning fallacies. This inverse scaling phenomenon indicates a convergence towards human-like cognition, including biases and limitations. Despite the overall improvement in reasoning abilities, there is a shift in error patterns towards aligning with human fallacies as models become more advanced. This shift is independent of error rate and suggests that scaling language models may not naturally achieve normative rationality. The study also demonstrated order effects in language model reasoning, further supporting the idea of a convergence towards human-like cognitive patterns. <div>
arXiv:2506.11128v1 Announce Type: new 
Abstract: Do language models converge toward human-like reasoning patterns as they improve? We provide surprising evidence that while overall reasoning capabilities increase with model sophistication, the nature of errors increasingly mirrors predictable human reasoning fallacies: a previously unobserved inverse scaling phenomenon. To investigate this question, we apply the Erotetic Theory of Reasoning (ETR), a formal cognitive framework with empirical support for predicting human reasoning outcomes. Using the open-source package PyETR, we generate logical reasoning problems where humans predictably err, evaluating responses from 38 language models across 383 reasoning tasks. Our analysis indicates that as models advance in general capability (as measured by Chatbot Arena scores), the proportion of their incorrect answers that align with ETR-predicted human fallacies tends to increase ($\rho = 0.360, p = 0.0265$). Notably, as we observe no correlation between model sophistication and logical correctness on these tasks, this shift in error patterns toward human-likeness occurs independently of error rate. These findings challenge the prevailing view that scaling language models naturally obtains normative rationality, suggesting instead a convergence toward human-like cognition inclusive of our characteristic biases and limitations, as we further confirm by demonstrating order-effects in language model reasoning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trustworthy AI for Medicine: Continuous Hallucination Detection and Elimination with CHECK</title>
<link>https://arxiv.org/abs/2506.11129</link>
<guid>https://arxiv.org/abs/2506.11129</guid>
<content:encoded><![CDATA[
<div> framework, structured, classifier, hallucinations, healthcare
Summary:
1. The article introduces the CHECK framework, designed to reduce hallucination rates in large language models (LLMs) used in healthcare.
2. CHECK integrates structured clinical databases with a classifier grounded in information theory to detect factual and reasoning-based hallucinations.
3. Evaluated on 1500 questions from clinical trials, CHECK significantly reduced hallucination rates, making an open source model state of the art.
4. Its classifier showed strong generalization across medical benchmarks, achieving high AUC scores.
5. By leveraging hallucination probabilities to guide refinement, CHECK improved the USMLE passing rate of the GPT-4o model and offers a scalable solution for safe LLM deployment in high-stakes domains.<br /><br />Summary: <div>
arXiv:2506.11129v1 Announce Type: new 
Abstract: Large language models (LLMs) show promise in healthcare, but hallucinations remain a major barrier to clinical use. We present CHECK, a continuous-learning framework that integrates structured clinical databases with a classifier grounded in information theory to detect both factual and reasoning-based hallucinations. Evaluated on 1500 questions from 100 pivotal clinical trials, CHECK reduced LLama3.3-70B-Instruct hallucination rates from 31% to 0.3% - making an open source model state of the art. Its classifier generalized across medical benchmarks, achieving AUCs of 0.95-0.96, including on the MedQA (USMLE) benchmark and HealthBench realistic multi-turn medical questioning. By leveraging hallucination probabilities to guide GPT-4o's refinement and judiciously escalate compute, CHECK boosted its USMLE passing rate by 5 percentage points, achieving a state-of-the-art 92.1%. By suppressing hallucinations below accepted clinical error thresholds, CHECK offers a scalable foundation for safe LLM deployment in medicine and other high-stakes domains.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data</title>
<link>https://arxiv.org/abs/2506.11130</link>
<guid>https://arxiv.org/abs/2506.11130</guid>
<content:encoded><![CDATA[
<div> Framework, ASR, unlabeled data, self-improvement, Mandarin <br />
Summary:<br />
The article proposes a self-refining framework to enhance Automatic Speech Recognition (ASR) performance using only unlabeled datasets. In this framework, an existing ASR model generates pseudo-labels on unannotated speech, which are then used to train a high-fidelity text-to-speech (TTS) system. The synthesized speech text pairs are then integrated back into the original ASR system, completing a closed-loop self-improvement cycle. The framework was tested on Taiwanese Mandarin speech with 6,000 hours of unlabeled data and synthetic content. The adapted model, Twister, showed a significant reduction in error rates on Mandarin and Mandarin-English code-switching benchmarks compared to the original Whisper model. This approach offers a practical pathway for improving ASR performance in low-resource or domain-specific settings. <br /> <div>
arXiv:2506.11130v1 Announce Type: new 
Abstract: We propose a self-refining framework that enhances ASR performance with only unlabeled datasets. The process starts with an existing ASR model generating pseudo-labels on unannotated speech, which are then used to train a high-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs are bootstrapped into the original ASR system, completing the closed-loop self-improvement cycle. We demonstrated the effectiveness of the framework on Taiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a moderate amount of text data, and synthetic content from the AI models, we adapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error rates by up to 20% on Mandarin and 50% on Mandarin-English code-switching benchmarks compared to Whisper. Results highlight the framework as a compelling alternative to pseudo-labeling self-distillation approaches and provides a practical pathway for improving ASR performance in low-resource or domain-specific settings.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models and Emergence: A Complex Systems Perspective</title>
<link>https://arxiv.org/abs/2506.11135</link>
<guid>https://arxiv.org/abs/2506.11135</guid>
<content:encoded><![CDATA[
<div> Keywords: Emergence, Complexity science, Large Language Models, Intelligence, Novel higher-level properties

Summary: 
Emergence in complexity science elucidates how systems exhibit new properties at higher levels, often described through replacement of high-dimensional mechanisms with lower-dimensional effective variables. This concept is exemplified by the notion "more is different," where novel characteristics arise when combined elements interact. Large Language Models (LLMs) have been examined for exhibiting emergent capabilities, prompting investigations into quantifying this phenomenon. Additionally, the concept of emergent intelligence, characterized by the adage "less is more," emphasizes the efficient utilization of emergent capabilities to solve problems with increased effectiveness and speed. The paper delves into whether LLMs possess emergent intelligence, highlighting the importance of analyzing the system's ability to harness emergent properties for intelligent problem-solving strategies. <div>
arXiv:2506.11135v1 Announce Type: new 
Abstract: Emergence is a concept in complexity science that describes how many-body systems manifest novel higher-level properties, properties that can be described by replacing high-dimensional mechanisms with lower-dimensional effective variables and theories. This is captured by the idea "more is different". Intelligence is a consummate emergent property manifesting increasingly efficient -- cheaper and faster -- uses of emergent capabilities to solve problems. This is captured by the idea "less is more". In this paper, we first examine claims that Large Language Models exhibit emergent capabilities, reviewing several approaches to quantifying emergence, and secondly ask whether LLMs possess emergent intelligence.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Medication Extraction and Discontinuation Identification from Electronic Health Records Using Large Language Models</title>
<link>https://arxiv.org/abs/2506.11137</link>
<guid>https://arxiv.org/abs/2506.11137</guid>
<content:encoded><![CDATA[
<div> medication extraction, medication status classification, electronic health records, large language models, discontinuation identification <br />
Summary:<br />
- Study evaluates capabilities of advanced open-sourced and proprietary large language models (LLMs) in medication extraction and status classification from EHR notes <br />
- GPT-4o had highest average F1 scores in all tasks under zero-shot setting <br />
- Open-sourced model Llama-3.1-70B-Instruct excelled in medication status classification and joint task on specific datasets <br />
- Medical-specific LLMs showed lower performance compared to general-domain LLMs <br />
- Few-shot learning improved performance, with CoT reasoning showing inconsistent gains <br />
- LLMs demonstrate strong potential for medication extraction and discontinuation identification on EHR notes, with open-sourced models providing scalable solutions and few-shot learning further enhancing LLMs' capabilities. <br /> 
Summary: <div>
arXiv:2506.11137v1 Announce Type: new 
Abstract: Identifying medication discontinuations in electronic health records (EHRs) is vital for patient safety but is often hindered by information being buried in unstructured notes. This study aims to evaluate the capabilities of advanced open-sourced and proprietary large language models (LLMs) in extracting medications and classifying their medication status from EHR notes, focusing on their scalability on medication information extraction without human annotation. We collected three EHR datasets from diverse sources to build the evaluation benchmark. We evaluated 12 advanced LLMs and explored multiple LLM prompting strategies. Performance on medication extraction, medication status classification, and their joint task (extraction then classification) was systematically compared across all experiments. We found that LLMs showed promising performance on the medication extraction and discontinuation classification from EHR notes. GPT-4o consistently achieved the highest average F1 scores in all tasks under zero-shot setting - 94.0% for medication extraction, 78.1% for discontinuation classification, and 72.7% for the joint task. Open-sourced models followed closely, Llama-3.1-70B-Instruct achieved the highest performance in medication status classification on the MIV-Med dataset (68.7%) and in the joint task on both the Re-CASI (76.2%) and MIV-Med (60.2%) datasets. Medical-specific LLMs demonstrated lower performance compared to advanced general-domain LLMs. Few-shot learning generally improved performance, while CoT reasoning showed inconsistent gains. LLMs demonstrate strong potential for medication extraction and discontinuation identification on EHR notes, with open-sourced models offering scalable alternatives to proprietary systems and few-shot can further improve LLMs' capability.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RETUYT-INCO at BEA 2025 Shared Task: How Far Can Lightweight Models Go in AI-powered Tutor Evaluation?</title>
<link>https://arxiv.org/abs/2506.11243</link>
<guid>https://arxiv.org/abs/2506.11243</guid>
<content:encoded><![CDATA[
<div> small models, computational power, Global South, shared task, competitive 

Summary:
In this paper, the RETUYT-INCO participation in the BEA 2025 shared task is discussed. The decision to use small models with fewer than 1B parameters was made to reflect the limited access to computational power in the Global South. Despite this limitation, the models performed competitively with other teams in the shared task. The $exact\ F_1$ scores showed performance gaps ranging from 6.46 to 13.13 points compared to the winning teams. This demonstrates that models with a size smaller than 1B parameters can be competitive for tasks that do not require high computational resources, making them accessible for labs or institutions with low-budget GPUs or even without GPUs.  <br /><br />Summary: <div>
arXiv:2506.11243v1 Announce Type: new 
Abstract: In this paper, we present the RETUYT-INCO participation at the BEA 2025 shared task. Our participation was characterized by the decision of using relatively small models, with fewer than 1B parameters. This self-imposed restriction tries to represent the conditions in which many research labs or institutions are in the Global South, where computational power is not easily accessible due to its prohibitive cost. Even under this restrictive self-imposed setting, our models managed to stay competitive with the rest of teams that participated in the shared task. According to the $exact\ F_1$ scores published by the organizers, the performance gaps between our models and the winners were as follows: $6.46$ in Track 1; $10.24$ in Track 2; $7.85$ in Track 3; $9.56$ in Track 4; and $13.13$ in Track 5. Considering that the minimum difference with a winner team is $6.46$ points -- and the maximum difference is $13.13$ -- according to the $exact\ F_1$ score, we find that models with a size smaller than 1B parameters are competitive for these tasks, all of which can be run on computers with a low-budget GPU or even without a GPU.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Multilingual Spectral Attribute Erasure</title>
<link>https://arxiv.org/abs/2506.11244</link>
<guid>https://arxiv.org/abs/2506.11244</guid>
<content:encoded><![CDATA[
<div> Keywords: Multilingual representations, debiasing, IMSAE, spectral attribute erasure, cross-lingual approaches

Summary:
Multilingual representations aim to embed words with similar meanings into a common semantic space across multiple languages. Existing debiasing methods are limited to individual languages and do not utilize the potential for transferring debiasing effects between languages. The Iterative Multilingual Spectral Attribute Erasure (IMSAE) technique presented in this study identifies and mitigates joint bias subspaces across multiple languages through iterative SVD-based truncation. Evaluation across eight languages and five demographic dimensions demonstrates the effectiveness of IMSAE in both standard and zero-shot settings. The method leverages linguistically similar languages for debiasing even when target language data is unavailable. Comprehensive experiments with diverse language models, including BERT, LLaMA, and Mistral, show that IMSAE outperforms traditional monolingual and cross-lingual approaches while maintaining model utility. <br /><br />Summary: Multilingual representations are harnessed through IMSAE to jointly address biases across languages, demonstrating superior performance in debiasing and cross-lingual transfer tasks. <div>
arXiv:2506.11244v1 Announce Type: new 
Abstract: Multilingual representations embed words with similar meanings to share a common semantic space across languages, creating opportunities to transfer debiasing effects between languages. However, existing methods for debiasing are unable to exploit this opportunity because they operate on individual languages. We present Iterative Multilingual Spectral Attribute Erasure (IMSAE), which identifies and mitigates joint bias subspaces across multiple languages through iterative SVD-based truncation. Evaluating IMSAE across eight languages and five demographic dimensions, we demonstrate its effectiveness in both standard and zero-shot settings, where target language data is unavailable, but linguistically similar languages can be used for debiasing. Our comprehensive experiments across diverse language models (BERT, LLaMA, Mistral) show that IMSAE outperforms traditional monolingual and cross-lingual approaches while maintaining model utility.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning</title>
<link>https://arxiv.org/abs/2506.11246</link>
<guid>https://arxiv.org/abs/2506.11246</guid>
<content:encoded><![CDATA[
<div> Keywords: Temporal Table Reasoning, Large Language Models, Prompting Techniques, SEAR, Table Structure Refactoring

Summary:
This research paper explores the challenge of Temporal Table Reasoning for Large Language Models (LLMs) and the impact of different prompting techniques on model performance. The study investigates various prompting methods across diverse table types and context structures to determine optimal approaches for different scenarios. Results show that performance varies based on entity type, table structure, additional context requirements, and question complexity, with no single method consistently outperforming others. To address these challenges, the researchers introduce SEAR, an adaptive prompting framework inspired by human reasoning that dynamically adjusts based on context characteristics. SEAR achieves superior performance across all table types compared to baseline prompting techniques. The study also examines the impact of table structure refactoring and finds that a unified representation enhances model reasoning. Overall, this work highlights the importance of effective prompting techniques and context adaptation in improving temporal table reasoning for Large Language Models.<br /><br />Summary: <div>
arXiv:2506.11246v1 Announce Type: new 
Abstract: Temporal Table Reasoning is a critical challenge for Large Language Models (LLMs), requiring effective prompting techniques to extract relevant insights. Despite existence of multiple prompting methods, their impact on table reasoning remains largely unexplored. Furthermore, the performance of these models varies drastically across different table and context structures, making it difficult to determine an optimal approach. This work investigates multiple prompting technique across diverse table types to determine optimal approaches for different scenarios. We find that performance varies based on entity type, table structure, requirement of additional context and question complexity, with NO single method consistently outperforming others. To mitigate these challenges, we introduce SEAR, an adaptive prompting framework inspired by human reasoning that dynamically adjusts based on context characteristics and integrates a structured reasoning. Our results demonstrate that SEAR achieves superior performance across all table types compared to other baseline prompting techniques. Additionally, we explore the impact of table structure refactoring, finding that a unified representation enhances model's reasoning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning a Continue-Thinking Token for Enhanced Test-Time Scaling</title>
<link>https://arxiv.org/abs/2506.11274</link>
<guid>https://arxiv.org/abs/2506.11274</guid>
<content:encoded><![CDATA[
<div> test-time scaling, language model, deep learning, reinforcement learning, reasoning

Summary:
Test-time scaling is an effective method for enhancing language model performance by utilizing additional compute during inference. This study investigates the use of a learned "<|continue-thinking|>" token to trigger extended reasoning in a distilled DeepSeek-R1 model. By training only the token's embedding via reinforcement learning, the model achieves improved accuracy on standard math benchmarks compared to both the baseline model and a fixed token approach for budget forcing. The learned token outperforms the fixed-token method, particularly in cases where the fixed token enhances the base model's accuracy. For instance, on the GSM8K benchmark, the fixed-token approach produces a 1.3% accuracy improvement, while the learned-token method achieves a significant 4.2% improvement over the base model without budget forcing. <div>
arXiv:2506.11274v1 Announce Type: new 
Abstract: Test-time scaling has emerged as an effective approach for improving language model performance by utilizing additional compute at inference time. Recent studies have shown that overriding end-of-thinking tokens (e.g., replacing "" with "Wait") can extend reasoning steps and improve accuracy. In this work, we explore whether a dedicated continue-thinking token can be learned to trigger extended reasoning. We augment a distilled version of DeepSeek-R1 with a single learned "<|continue-thinking|>" token, training only its embedding via reinforcement learning while keeping the model weights frozen. Our experiments show that this learned token achieves improved accuracy on standard math benchmarks compared to both the baseline model and a test-time scaling approach that uses a fixed token (e.g., "Wait") for budget forcing. In particular, we observe that in cases where the fixed-token approach enhances the base model's accuracy, our method achieves a markedly greater improvement. For example, on the GSM8K benchmark, the fixed-token approach yields a 1.3% absolute improvement in accuracy, whereas our learned-token method achieves a 4.2% improvement over the base model that does not use budget forcing.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning</title>
<link>https://arxiv.org/abs/2506.11300</link>
<guid>https://arxiv.org/abs/2506.11300</guid>
<content:encoded><![CDATA[
<div> Keywords: curriculum learning, language models, training efficiency, generalization, difficulty metrics

Summary: 
Curriculum learning was investigated for pretraining language models, with experiments conducted using various settings such as vanilla curriculum learning and pacing-based sampling. Six difficulty metrics were employed to guide the curricula, leading to improved convergence during early and mid-training phases. Notably, compression ratio, lexical diversity, and readability were identified as effective difficulty signals. The experiments demonstrated that curriculum learning can result in up to a 3.5% improvement when used as a warmup strategy. The study emphasizes the significance of data ordering in large-scale pretraining and provides valuable insights for developing scalable and data-efficient models under realistic training scenarios.<br /><br />Summary: <div>
arXiv:2506.11300v1 Announce Type: new 
Abstract: Curriculum learning has shown promise in improving training efficiency and generalization in various machine learning domains, yet its potential in pretraining language models remains underexplored, prompting our work as the first systematic investigation in this area. We experimented with different settings, including vanilla curriculum learning, pacing-based sampling, and interleaved curricula-guided by six difficulty metrics spanning linguistic and information-theoretic perspectives. We train models under these settings and evaluate their performance on eight diverse benchmarks. Our experiments reveal that curriculum learning consistently improves convergence in early and mid-training phases, and can yield lasting gains when used as a warmup strategy with up to $3.5\%$ improvement. Notably, we identify compression ratio, lexical diversity, and readability as effective difficulty signals across settings. Our findings highlight the importance of data ordering in large-scale pretraining and provide actionable insights for scalable, data-efficient model development under realistic training scenarios.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Pay Attention</title>
<link>https://arxiv.org/abs/2506.11305</link>
<guid>https://arxiv.org/abs/2506.11305</guid>
<content:encoded><![CDATA[
<div> Transformer, large language models, RNN-like architectures, Avey, long-range dependencies <br />
Summary: Avey is introduced as a new neural foundational architecture that aims to overcome the limitations of the Transformer model by breaking away from both attention and recurrence mechanisms. Avey consists of a ranker and an autoregressive neural processor that work together to identify and contextualize the most relevant tokens for a given token, irrespective of their position in the sequence. This decoupling of sequence length from context width allows Avey to effectively process arbitrarily long sequences, addressing the challenges faced by the Transformer in handling long-range dependencies. Experimental results demonstrate that Avey outperforms the Transformer on various standard short-range NLP benchmarks while excelling at capturing long-range dependencies. The proposed architecture presents a promising alternative to existing models for processing sequences and offers a potential solution to the limitations faced by current state-of-the-art language models. <br /><br />Summary: <div>
arXiv:2506.11305v1 Announce Type: new 
Abstract: The Transformer has become the de facto standard for large language models and a wide range of downstream tasks across various domains. Despite its numerous advantages like inherent training parallelism, the Transformer still faces key challenges due to its inability to effectively process sequences beyond a fixed context window and the quadratic complexity of its attention mechanism. These challenges have renewed interest in RNN-like architectures, which offer linear scaling with sequence length and improved handling of long-range dependencies, albeit with limited parallelism due to their inherently recurrent nature. In this paper, we propose Avey, a new neural foundational architecture that breaks away from both attention and recurrence. Avey comprises a ranker and an autoregressive neural processor, which collaboratively identify and contextualize only the most relevant tokens for any given token, regardless of their positions in the sequence. Specifically, Avey decouples sequence length from context width, thus enabling effective processing of arbitrarily long sequences. Experimental results show that Avey compares favorably to the Transformer across a variety of standard short-range NLP benchmarks, while notably excelling at capturing long-range dependencies.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surprisal from Larger Transformer-based Language Models Predicts fMRI Data More Poorly</title>
<link>https://arxiv.org/abs/2506.11338</link>
<guid>https://arxiv.org/abs/2506.11338</guid>
<content:encoded><![CDATA[
arXiv:2506.11338v1 Announce Type: new 
Abstract: As Transformers become more widely incorporated into natural language processing tasks, there has been considerable interest in using surprisal from these models as predictors of human sentence processing difficulty. Recent work has observed a positive relationship between Transformer-based models' perplexity and the predictive power of their surprisal estimates on reading times, showing that language models with more parameters and trained on more data are less predictive of human reading times. However, these studies focus on predicting latency-based measures (i.e., self-paced reading times and eye-gaze durations) with surprisal estimates from Transformer-based language models. This trend has not been tested on brain imaging data. This study therefore evaluates the predictive power of surprisal estimates from 17 pre-trained Transformer-based models across three different language families on two functional magnetic resonance imaging datasets. Results show that the positive relationship between model perplexity and model fit still obtains, suggesting that this trend is not specific to latency-based measures and can be generalized to neural measures.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review</title>
<link>https://arxiv.org/abs/2506.11343</link>
<guid>https://arxiv.org/abs/2506.11343</guid>
<content:encoded><![CDATA[
arXiv:2506.11343v1 Announce Type: new 
Abstract: The advent of large language models (LLMs) offers unprecedented opportunities to reimagine peer review beyond the constraints of traditional workflows. Despite these opportunities, prior efforts have largely focused on replicating traditional review workflows with LLMs serving as direct substitutes for human reviewers, while limited attention has been given to exploring new paradigms that fundamentally rethink how LLMs can participate in the academic review process. In this paper, we introduce and explore a novel mechanism that employs LLM agents to perform pairwise comparisons among manuscripts instead of individual scoring. By aggregating outcomes from substantial pairwise evaluations, this approach enables a more accurate and robust measure of relative manuscript quality. Our experiments demonstrate that this comparative approach significantly outperforms traditional rating-based methods in identifying high-impact papers. However, our analysis also reveals emergent biases in the selection process, notably a reduced novelty in research topics and an increased institutional imbalance. These findings highlight both the transformative potential of rethinking peer review with LLMs and critical challenges that future systems must address to ensure equity and diversity.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do We Still Need Audio? Rethinking Speaker Diarization with a Text-Based Approach Using Multiple Prediction Models</title>
<link>https://arxiv.org/abs/2506.11344</link>
<guid>https://arxiv.org/abs/2506.11344</guid>
<content:encoded><![CDATA[
arXiv:2506.11344v1 Announce Type: new 
Abstract: We present a novel approach to Speaker Diarization (SD) by leveraging text-based methods focused on Sentence-level Speaker Change Detection within dialogues. Unlike audio-based SD systems, which are often challenged by audio quality and speaker similarity, our approach utilizes the dialogue transcript alone. Two models are developed: the Single Prediction Model (SPM) and the Multiple Prediction Model (MPM), both of which demonstrate significant improvements in identifying speaker changes, particularly in short conversations. Our findings, based on a curated dataset encompassing diverse conversational scenarios, reveal that the text-based SD approach, especially the MPM, performs competitively against state-of-the-art audio-based SD systems, with superior performance in short conversational contexts. This paper not only showcases the potential of leveraging linguistic features for SD but also highlights the importance of integrating semantic understanding into SD systems, opening avenues for future research in multimodal and semantic feature-based diarization.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Biased Samaritan: LLM biases in Perceived Kindness</title>
<link>https://arxiv.org/abs/2506.11361</link>
<guid>https://arxiv.org/abs/2506.11361</guid>
<content:encoded><![CDATA[
arXiv:2506.11361v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) have become ubiquitous in many fields, understanding and mitigating LLM biases is an ongoing issue. This paper provides a novel method for evaluating the demographic biases of various generative AI models. By prompting models to assess a moral patient's willingness to intervene constructively, we aim to quantitatively evaluate different LLMs' biases towards various genders, races, and ages. Our work differs from existing work by aiming to determine the baseline demographic identities for various commercial models and the relationship between the baseline and other demographics. We strive to understand if these biases are positive, neutral, or negative, and the strength of these biases. This paper can contribute to the objective assessment of bias in Large Language Models and give the user or developer the power to account for these biases in LLM output or in training future LLMs. Our analysis suggested two key findings: that models view the baseline demographic as a white middle-aged or young adult male; however, a general trend across models suggested that non-baseline demographics are more willing to help than the baseline. These methodologies allowed us to distinguish these two biases that are often tangled together.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Variational Approach for Mitigating Entity Bias in Relation Extraction</title>
<link>https://arxiv.org/abs/2506.11381</link>
<guid>https://arxiv.org/abs/2506.11381</guid>
<content:encoded><![CDATA[
arXiv:2506.11381v1 Announce Type: new 
Abstract: Mitigating entity bias is a critical challenge in Relation Extraction (RE), where models often rely excessively on entities, resulting in poor generalization. This paper presents a novel approach to address this issue by adapting a Variational Information Bottleneck (VIB) framework. Our method compresses entity-specific information while preserving task-relevant features. It achieves state-of-the-art performance on relation extraction datasets across general, financial, and biomedical domains, in both indomain (original test sets) and out-of-domain (modified test sets with type-constrained entity replacements) settings. Our approach offers a robust, interpretable, and theoretically grounded methodology.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curriculum-Guided Layer Scaling for Language Model Pretraining</title>
<link>https://arxiv.org/abs/2506.11389</link>
<guid>https://arxiv.org/abs/2506.11389</guid>
<content:encoded><![CDATA[
arXiv:2506.11389v1 Announce Type: new 
Abstract: As the cost of pretraining large language models grows, there is continued interest in strategies to improve learning efficiency during this core training stage. Motivated by cognitive development, where humans gradually build knowledge as their brains mature, we propose Curriculum-Guided Layer Scaling (CGLS), a framework for compute-efficient pretraining that synchronizes increasing data difficulty with model growth through progressive layer stacking (i.e. gradually adding layers during training). At the 100M parameter scale, using a curriculum transitioning from synthetic short stories to general web data, CGLS outperforms baseline methods on the question-answering benchmarks PIQA and ARC. Pretraining at the 1.2B scale, we stratify the DataComp-LM corpus with a DistilBERT-based classifier and progress from general text to highly technical or specialized content. Our results show that progressively increasing model depth alongside sample difficulty leads to better generalization and zero-shot performance on various downstream benchmarks. Altogether, our findings demonstrate that CGLS unlocks the potential of progressive stacking, offering a simple yet effective strategy for improving generalization on knowledge-intensive and reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Early-Onset Colorectal Cancer with Large Language Models</title>
<link>https://arxiv.org/abs/2506.11410</link>
<guid>https://arxiv.org/abs/2506.11410</guid>
<content:encoded><![CDATA[
arXiv:2506.11410v1 Announce Type: new 
Abstract: The incidence rate of early-onset colorectal cancer (EoCRC, age < 45) has increased every year, but this population is younger than the recommended age established by national guidelines for cancer screening. In this paper, we applied 10 different machine learning models to predict EoCRC, and compared their performance with advanced large language models (LLM), using patient conditions, lab results, and observations within 6 months of patient journey prior to the CRC diagnoses. We retrospectively identified 1,953 CRC patients from multiple health systems across the United States. The results demonstrated that the fine-tuned LLM achieved an average of 73% sensitivity and 91% specificity.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Long-Context LLM Inference via KV Cache Clustering</title>
<link>https://arxiv.org/abs/2506.11418</link>
<guid>https://arxiv.org/abs/2506.11418</guid>
<content:encoded><![CDATA[
arXiv:2506.11418v1 Announce Type: new 
Abstract: Large language models (LLMs) with extended context windows have become increasingly prevalent for tackling complex tasks. However, the substantial Key-Value (KV) cache required for long-context LLMs poses significant deployment challenges. Existing approaches either discard potentially critical information needed for future generations or offer limited efficiency gains due to high computational overhead. In this paper, we introduce Chelsea, a simple yet effective framework for online KV cache clustering. Our approach is based on the observation that key states exhibit high similarity along the sequence dimension. To enable efficient clustering, we divide the sequence into chunks and propose Chunked Soft Matching, which employs an alternating partition strategy within each chunk and identifies clusters based on similarity. Chelsea then merges the KV cache within each cluster into a single centroid. Additionally, we provide a theoretical analysis of the computational complexity and the optimality of the intra-chunk partitioning strategy. Extensive experiments across various models and long-context benchmarks demonstrate that Chelsea achieves up to 80% reduction in KV cache memory usage while maintaining comparable model performance. Moreover, with minimal computational overhead, Chelsea accelerates the decoding stage of inference by up to 3.19$\times$ and reduces end-to-end latency by up to 2.72$\times$.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>